{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.006\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet_50_cosine') #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformer_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    #transforms.ColorJitter(brightness=(0.2, 2), contrast=(0.3, 2), saturation=(0.2, 2), hue=(-0.3, 0.3)),\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_2 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_3 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "validation_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_2 = torch.utils.data.DataLoader(dataset=training_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_3 = torch.utils.data.DataLoader(dataset=training_dataset_3, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "       \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #self.dropblock.step()\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        #out = self.dropblock(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        #out = self.dropblock(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BottleNeck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 1 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
      "             ReLU-13          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-14          [-1, 256, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "           Conv2d-18           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
      "             ReLU-20           [-1, 64, 32, 32]               0\n",
      "           Conv2d-21          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-22          [-1, 256, 32, 32]             512\n",
      "             ReLU-23          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
      "             ReLU-27           [-1, 64, 32, 32]               0\n",
      "           Conv2d-28           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 32, 32]             128\n",
      "             ReLU-30           [-1, 64, 32, 32]               0\n",
      "           Conv2d-31          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
      "             ReLU-33          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "             ReLU-40          [-1, 128, 16, 16]               0\n",
      "           Conv2d-41          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-42          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-43          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-45          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-46          [-1, 512, 16, 16]               0\n",
      "           Conv2d-47          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-48          [-1, 128, 16, 16]             256\n",
      "             ReLU-49          [-1, 128, 16, 16]               0\n",
      "           Conv2d-50          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-51          [-1, 128, 16, 16]             256\n",
      "             ReLU-52          [-1, 128, 16, 16]               0\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-55          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-56          [-1, 512, 16, 16]               0\n",
      "           Conv2d-57          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-58          [-1, 128, 16, 16]             256\n",
      "             ReLU-59          [-1, 128, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-66          [-1, 512, 16, 16]               0\n",
      "           Conv2d-67          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-68          [-1, 128, 16, 16]             256\n",
      "             ReLU-69          [-1, 128, 16, 16]               0\n",
      "           Conv2d-70          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-71          [-1, 128, 16, 16]             256\n",
      "             ReLU-72          [-1, 128, 16, 16]               0\n",
      "           Conv2d-73          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-74          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-75          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-76          [-1, 512, 16, 16]               0\n",
      "           Conv2d-77          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-78          [-1, 256, 16, 16]             512\n",
      "             ReLU-79          [-1, 256, 16, 16]               0\n",
      "           Conv2d-80            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-81            [-1, 256, 8, 8]             512\n",
      "             ReLU-82            [-1, 256, 8, 8]               0\n",
      "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-85           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-87           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-88           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-89            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-90            [-1, 256, 8, 8]             512\n",
      "             ReLU-91            [-1, 256, 8, 8]               0\n",
      "           Conv2d-92            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-93            [-1, 256, 8, 8]             512\n",
      "             ReLU-94            [-1, 256, 8, 8]               0\n",
      "           Conv2d-95           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-96           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-97           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-98           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-99            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-100            [-1, 256, 8, 8]             512\n",
      "            ReLU-101            [-1, 256, 8, 8]               0\n",
      "          Conv2d-102            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-103            [-1, 256, 8, 8]             512\n",
      "            ReLU-104            [-1, 256, 8, 8]               0\n",
      "          Conv2d-105           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-106           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-107           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-108           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-109            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-110            [-1, 256, 8, 8]             512\n",
      "            ReLU-111            [-1, 256, 8, 8]               0\n",
      "          Conv2d-112            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-113            [-1, 256, 8, 8]             512\n",
      "            ReLU-114            [-1, 256, 8, 8]               0\n",
      "          Conv2d-115           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-116           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-117           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-118           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-119            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-120            [-1, 256, 8, 8]             512\n",
      "            ReLU-121            [-1, 256, 8, 8]               0\n",
      "          Conv2d-122            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "            ReLU-124            [-1, 256, 8, 8]               0\n",
      "          Conv2d-125           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-126           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-127           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-128           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-129            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-130            [-1, 256, 8, 8]             512\n",
      "            ReLU-131            [-1, 256, 8, 8]               0\n",
      "          Conv2d-132            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-133            [-1, 256, 8, 8]             512\n",
      "            ReLU-134            [-1, 256, 8, 8]               0\n",
      "          Conv2d-135           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-136           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-137           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-138           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-139            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-140            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-141            [-1, 512, 8, 8]               0\n",
      "          Conv2d-142            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-143            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-144            [-1, 512, 4, 4]               0\n",
      "          Conv2d-145           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-146           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-147           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-148           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-149           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-150           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-151            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-152            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-153            [-1, 512, 4, 4]               0\n",
      "          Conv2d-154            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-155            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-156            [-1, 512, 4, 4]               0\n",
      "          Conv2d-157           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-158           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-159           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-160           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-161            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-162            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-163            [-1, 512, 4, 4]               0\n",
      "          Conv2d-164            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-165            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-166            [-1, 512, 4, 4]               0\n",
      "          Conv2d-167           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-168           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-169           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-170           [-1, 2048, 4, 4]               0\n",
      "          Linear-171                   [-1, 10]          20,490\n",
      "          ResNet-172                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 23,520,842\n",
      "Trainable params: 23,520,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 88.06\n",
      "Params size (MB): 89.72\n",
      "Estimated Total Size (MB): 177.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(training_loader)\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(training_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(validation_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(validation_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (2.4336) | Acc_1: (13.28%) (17/128)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (2.6887) | Acc_1: (12.07%) (170/1408)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (2.6024) | Acc_1: (14.47%) (389/2688)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (2.5314) | Acc_1: (16.56%) (657/3968)\n",
      "Epoch: 0 | Batch_idx: 40 |  Loss_1: (2.4246) | Acc_1: (18.12%) (951/5248)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss_1: (2.3563) | Acc_1: (19.47%) (1271/6528)\n",
      "Epoch: 0 | Batch_idx: 60 |  Loss_1: (2.3061) | Acc_1: (20.75%) (1620/7808)\n",
      "Epoch: 0 | Batch_idx: 70 |  Loss_1: (2.2658) | Acc_1: (21.83%) (1984/9088)\n",
      "Epoch: 0 | Batch_idx: 80 |  Loss_1: (2.2105) | Acc_1: (23.20%) (2405/10368)\n",
      "Epoch: 0 | Batch_idx: 90 |  Loss_1: (2.1681) | Acc_1: (24.26%) (2826/11648)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss_1: (2.1349) | Acc_1: (25.02%) (3235/12928)\n",
      "Epoch: 0 | Batch_idx: 110 |  Loss_1: (2.1036) | Acc_1: (26.07%) (3704/14208)\n",
      "Epoch: 0 | Batch_idx: 120 |  Loss_1: (2.0704) | Acc_1: (26.96%) (4175/15488)\n",
      "Epoch: 0 | Batch_idx: 130 |  Loss_1: (2.0456) | Acc_1: (27.65%) (4636/16768)\n",
      "Epoch: 0 | Batch_idx: 140 |  Loss_1: (2.0210) | Acc_1: (28.42%) (5129/18048)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss_1: (1.9953) | Acc_1: (29.03%) (5610/19328)\n",
      "Epoch: 0 | Batch_idx: 160 |  Loss_1: (1.9748) | Acc_1: (29.65%) (6111/20608)\n",
      "Epoch: 0 | Batch_idx: 170 |  Loss_1: (1.9566) | Acc_1: (30.18%) (6605/21888)\n",
      "Epoch: 0 | Batch_idx: 180 |  Loss_1: (1.9373) | Acc_1: (30.74%) (7121/23168)\n",
      "Epoch: 0 | Batch_idx: 190 |  Loss_1: (1.9185) | Acc_1: (31.34%) (7662/24448)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss_1: (1.9025) | Acc_1: (31.84%) (8191/25728)\n",
      "Epoch: 0 | Batch_idx: 210 |  Loss_1: (1.8905) | Acc_1: (32.25%) (8710/27008)\n",
      "Epoch: 0 | Batch_idx: 220 |  Loss_1: (1.8751) | Acc_1: (32.75%) (9265/28288)\n",
      "Epoch: 0 | Batch_idx: 230 |  Loss_1: (1.8608) | Acc_1: (33.19%) (9813/29568)\n",
      "Epoch: 0 | Batch_idx: 240 |  Loss_1: (1.8473) | Acc_1: (33.66%) (10382/30848)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss_1: (1.8327) | Acc_1: (34.20%) (10988/32128)\n",
      "Epoch: 0 | Batch_idx: 260 |  Loss_1: (1.8177) | Acc_1: (34.65%) (11575/33408)\n",
      "Epoch: 0 | Batch_idx: 270 |  Loss_1: (1.8034) | Acc_1: (35.14%) (12190/34688)\n",
      "Epoch: 0 | Batch_idx: 280 |  Loss_1: (1.7920) | Acc_1: (35.53%) (12780/35968)\n",
      "Epoch: 0 | Batch_idx: 290 |  Loss_1: (1.7778) | Acc_1: (36.02%) (13418/37248)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss_1: (1.7659) | Acc_1: (36.46%) (14048/38528)\n",
      "Epoch: 0 | Batch_idx: 310 |  Loss_1: (1.7550) | Acc_1: (36.85%) (14669/39808)\n",
      "Epoch: 0 | Batch_idx: 320 |  Loss_1: (1.7443) | Acc_1: (37.21%) (15290/41088)\n",
      "Epoch: 0 | Batch_idx: 330 |  Loss_1: (1.7339) | Acc_1: (37.56%) (15915/42368)\n",
      "Epoch: 0 | Batch_idx: 340 |  Loss_1: (1.7224) | Acc_1: (37.91%) (16545/43648)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss_1: (1.7107) | Acc_1: (38.29%) (17203/44928)\n",
      "Epoch: 0 | Batch_idx: 360 |  Loss_1: (1.7012) | Acc_1: (38.63%) (17852/46208)\n",
      "Epoch: 0 | Batch_idx: 370 |  Loss_1: (1.6906) | Acc_1: (39.01%) (18523/47488)\n",
      "Epoch: 0 | Batch_idx: 380 |  Loss_1: (1.6800) | Acc_1: (39.35%) (19192/48768)\n",
      "Epoch: 0 | Batch_idx: 390 |  Loss_1: (1.6704) | Acc_1: (39.71%) (19854/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2784) | Acc: (53.52%) (5352/10000)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.4096) | Acc_1: (50.00%) (64/128)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.3511) | Acc_1: (52.13%) (734/1408)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.3380) | Acc_1: (52.90%) (1422/2688)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.3194) | Acc_1: (53.58%) (2126/3968)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss_1: (1.3132) | Acc_1: (53.37%) (2801/5248)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss_1: (1.3051) | Acc_1: (53.48%) (3491/6528)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss_1: (1.2996) | Acc_1: (53.68%) (4191/7808)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss_1: (1.2819) | Acc_1: (54.34%) (4938/9088)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss_1: (1.2736) | Acc_1: (54.68%) (5669/10368)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss_1: (1.2674) | Acc_1: (54.90%) (6395/11648)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss_1: (1.2661) | Acc_1: (55.01%) (7112/12928)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss_1: (1.2617) | Acc_1: (55.15%) (7836/14208)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss_1: (1.2526) | Acc_1: (55.46%) (8590/15488)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss_1: (1.2478) | Acc_1: (55.63%) (9328/16768)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss_1: (1.2448) | Acc_1: (55.76%) (10063/18048)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss_1: (1.2398) | Acc_1: (55.92%) (10809/19328)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss_1: (1.2325) | Acc_1: (56.21%) (11583/20608)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss_1: (1.2279) | Acc_1: (56.43%) (12352/21888)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss_1: (1.2208) | Acc_1: (56.64%) (13122/23168)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss_1: (1.2149) | Acc_1: (56.85%) (13899/24448)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss_1: (1.2082) | Acc_1: (57.02%) (14670/25728)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss_1: (1.2014) | Acc_1: (57.28%) (15470/27008)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss_1: (1.1960) | Acc_1: (57.45%) (16252/28288)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss_1: (1.1900) | Acc_1: (57.59%) (17029/29568)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss_1: (1.1856) | Acc_1: (57.73%) (17810/30848)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss_1: (1.1794) | Acc_1: (58.01%) (18637/32128)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss_1: (1.1747) | Acc_1: (58.19%) (19441/33408)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss_1: (1.1697) | Acc_1: (58.39%) (20255/34688)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss_1: (1.1666) | Acc_1: (58.56%) (21063/35968)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss_1: (1.1618) | Acc_1: (58.74%) (21878/37248)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss_1: (1.1571) | Acc_1: (58.90%) (22692/38528)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss_1: (1.1547) | Acc_1: (58.96%) (23471/39808)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss_1: (1.1487) | Acc_1: (59.18%) (24317/41088)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss_1: (1.1448) | Acc_1: (59.30%) (25124/42368)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss_1: (1.1416) | Acc_1: (59.38%) (25916/43648)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss_1: (1.1378) | Acc_1: (59.52%) (26741/44928)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss_1: (1.1322) | Acc_1: (59.72%) (27597/46208)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss_1: (1.1288) | Acc_1: (59.81%) (28402/47488)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss_1: (1.1233) | Acc_1: (60.01%) (29268/48768)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss_1: (1.1180) | Acc_1: (60.19%) (30097/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0283) | Acc: (64.98%) (6498/10000)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (0.9592) | Acc_1: (64.84%) (83/128)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (0.9395) | Acc_1: (67.40%) (949/1408)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (0.9183) | Acc_1: (68.12%) (1831/2688)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (0.9010) | Acc_1: (68.27%) (2709/3968)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss_1: (0.9073) | Acc_1: (67.97%) (3567/5248)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss_1: (0.9112) | Acc_1: (67.65%) (4416/6528)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss_1: (0.9111) | Acc_1: (67.70%) (5286/7808)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss_1: (0.9090) | Acc_1: (67.84%) (6165/9088)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss_1: (0.9072) | Acc_1: (67.82%) (7032/10368)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss_1: (0.9019) | Acc_1: (68.23%) (7948/11648)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss_1: (0.9078) | Acc_1: (68.09%) (8803/12928)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss_1: (0.9052) | Acc_1: (68.17%) (9686/14208)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss_1: (0.8977) | Acc_1: (68.43%) (10599/15488)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss_1: (0.8946) | Acc_1: (68.61%) (11505/16768)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss_1: (0.8912) | Acc_1: (68.76%) (12409/18048)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss_1: (0.8879) | Acc_1: (68.84%) (13305/19328)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss_1: (0.8863) | Acc_1: (68.89%) (14197/20608)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss_1: (0.8847) | Acc_1: (68.97%) (15097/21888)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss_1: (0.8803) | Acc_1: (69.11%) (16012/23168)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss_1: (0.8809) | Acc_1: (69.11%) (16895/24448)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss_1: (0.8785) | Acc_1: (69.20%) (17805/25728)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss_1: (0.8755) | Acc_1: (69.34%) (18727/27008)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss_1: (0.8742) | Acc_1: (69.36%) (19620/28288)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss_1: (0.8734) | Acc_1: (69.39%) (20516/29568)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss_1: (0.8710) | Acc_1: (69.40%) (21410/30848)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss_1: (0.8652) | Acc_1: (69.67%) (22382/32128)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss_1: (0.8624) | Acc_1: (69.76%) (23304/33408)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss_1: (0.8618) | Acc_1: (69.79%) (24209/34688)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss_1: (0.8604) | Acc_1: (69.86%) (25128/35968)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss_1: (0.8572) | Acc_1: (69.92%) (26044/37248)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss_1: (0.8537) | Acc_1: (70.06%) (26992/38528)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss_1: (0.8504) | Acc_1: (70.16%) (27929/39808)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss_1: (0.8458) | Acc_1: (70.30%) (28886/41088)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss_1: (0.8469) | Acc_1: (70.31%) (29788/42368)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss_1: (0.8439) | Acc_1: (70.44%) (30745/43648)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss_1: (0.8420) | Acc_1: (70.50%) (31672/44928)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss_1: (0.8387) | Acc_1: (70.60%) (32623/46208)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss_1: (0.8375) | Acc_1: (70.66%) (33557/47488)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss_1: (0.8352) | Acc_1: (70.74%) (34498/48768)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss_1: (0.8342) | Acc_1: (70.76%) (35379/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9514) | Acc: (68.61%) (6861/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (0.6529) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (0.6996) | Acc_1: (76.14%) (1072/1408)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (0.6799) | Acc_1: (76.41%) (2054/2688)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (0.6837) | Acc_1: (75.88%) (3011/3968)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss_1: (0.6832) | Acc_1: (76.12%) (3995/5248)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss_1: (0.6836) | Acc_1: (76.15%) (4971/6528)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss_1: (0.6942) | Acc_1: (75.79%) (5918/7808)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss_1: (0.7007) | Acc_1: (75.59%) (6870/9088)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss_1: (0.7017) | Acc_1: (75.51%) (7829/10368)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss_1: (0.7019) | Acc_1: (75.46%) (8790/11648)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss_1: (0.6988) | Acc_1: (75.58%) (9771/12928)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss_1: (0.6983) | Acc_1: (75.61%) (10743/14208)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss_1: (0.6995) | Acc_1: (75.57%) (11704/15488)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss_1: (0.6965) | Acc_1: (75.78%) (12706/16768)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss_1: (0.6987) | Acc_1: (75.71%) (13664/18048)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss_1: (0.6952) | Acc_1: (75.82%) (14654/19328)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss_1: (0.6935) | Acc_1: (75.92%) (15646/20608)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss_1: (0.6930) | Acc_1: (75.95%) (16624/21888)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss_1: (0.6941) | Acc_1: (75.92%) (17588/23168)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss_1: (0.6949) | Acc_1: (75.84%) (18541/24448)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss_1: (0.6957) | Acc_1: (75.77%) (19495/25728)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss_1: (0.6937) | Acc_1: (75.84%) (20484/27008)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss_1: (0.6928) | Acc_1: (75.87%) (21463/28288)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss_1: (0.6911) | Acc_1: (75.94%) (22454/29568)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss_1: (0.6918) | Acc_1: (75.92%) (23420/30848)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss_1: (0.6917) | Acc_1: (75.96%) (24403/32128)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss_1: (0.6900) | Acc_1: (75.98%) (25385/33408)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss_1: (0.6883) | Acc_1: (76.05%) (26381/34688)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss_1: (0.6855) | Acc_1: (76.11%) (27377/35968)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss_1: (0.6844) | Acc_1: (76.14%) (28361/37248)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss_1: (0.6843) | Acc_1: (76.16%) (29341/38528)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss_1: (0.6836) | Acc_1: (76.16%) (30316/39808)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss_1: (0.6835) | Acc_1: (76.19%) (31306/41088)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss_1: (0.6828) | Acc_1: (76.25%) (32304/42368)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss_1: (0.6815) | Acc_1: (76.28%) (33296/43648)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss_1: (0.6798) | Acc_1: (76.35%) (34302/44928)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss_1: (0.6793) | Acc_1: (76.38%) (35293/46208)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss_1: (0.6783) | Acc_1: (76.40%) (36283/47488)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss_1: (0.6774) | Acc_1: (76.41%) (37263/48768)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss_1: (0.6754) | Acc_1: (76.44%) (38221/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7035) | Acc: (76.20%) (7620/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (0.5857) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (0.5861) | Acc_1: (80.04%) (1127/1408)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (0.5798) | Acc_1: (80.02%) (2151/2688)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (0.5825) | Acc_1: (80.07%) (3177/3968)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss_1: (0.5901) | Acc_1: (80.01%) (4199/5248)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss_1: (0.5800) | Acc_1: (80.30%) (5242/6528)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss_1: (0.5821) | Acc_1: (80.05%) (6250/7808)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss_1: (0.5860) | Acc_1: (79.78%) (7250/9088)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss_1: (0.5833) | Acc_1: (79.83%) (8277/10368)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss_1: (0.5829) | Acc_1: (79.81%) (9296/11648)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss_1: (0.5835) | Acc_1: (79.71%) (10305/12928)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss_1: (0.5818) | Acc_1: (79.84%) (11343/14208)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss_1: (0.5831) | Acc_1: (79.76%) (12354/15488)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss_1: (0.5855) | Acc_1: (79.73%) (13369/16768)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss_1: (0.5855) | Acc_1: (79.69%) (14382/18048)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss_1: (0.5855) | Acc_1: (79.69%) (15402/19328)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss_1: (0.5862) | Acc_1: (79.68%) (16421/20608)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss_1: (0.5865) | Acc_1: (79.70%) (17445/21888)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss_1: (0.5844) | Acc_1: (79.82%) (18493/23168)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss_1: (0.5833) | Acc_1: (79.87%) (19527/24448)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss_1: (0.5816) | Acc_1: (79.96%) (20573/25728)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss_1: (0.5823) | Acc_1: (79.89%) (21578/27008)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss_1: (0.5811) | Acc_1: (79.93%) (22610/28288)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss_1: (0.5797) | Acc_1: (79.97%) (23645/29568)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss_1: (0.5811) | Acc_1: (79.87%) (24639/30848)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss_1: (0.5796) | Acc_1: (79.93%) (25679/32128)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss_1: (0.5787) | Acc_1: (79.99%) (26723/33408)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss_1: (0.5784) | Acc_1: (80.03%) (27762/34688)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss_1: (0.5783) | Acc_1: (80.06%) (28797/35968)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss_1: (0.5766) | Acc_1: (80.10%) (29834/37248)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss_1: (0.5761) | Acc_1: (80.09%) (30857/38528)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss_1: (0.5760) | Acc_1: (80.08%) (31878/39808)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss_1: (0.5761) | Acc_1: (80.07%) (32899/41088)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss_1: (0.5765) | Acc_1: (80.05%) (33917/42368)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss_1: (0.5768) | Acc_1: (80.03%) (34932/43648)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss_1: (0.5762) | Acc_1: (80.03%) (35958/44928)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss_1: (0.5745) | Acc_1: (80.11%) (37015/46208)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss_1: (0.5732) | Acc_1: (80.18%) (38076/47488)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss_1: (0.5730) | Acc_1: (80.18%) (39102/48768)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss_1: (0.5743) | Acc_1: (80.20%) (40100/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6202) | Acc: (79.46%) (7946/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (0.4786) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (0.4803) | Acc_1: (83.88%) (1181/1408)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (0.5146) | Acc_1: (82.85%) (2227/2688)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (0.5124) | Acc_1: (82.46%) (3272/3968)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss_1: (0.5021) | Acc_1: (82.89%) (4350/5248)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss_1: (0.5050) | Acc_1: (82.84%) (5408/6528)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss_1: (0.4982) | Acc_1: (83.07%) (6486/7808)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss_1: (0.4964) | Acc_1: (83.08%) (7550/9088)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss_1: (0.4965) | Acc_1: (83.08%) (8614/10368)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss_1: (0.4946) | Acc_1: (83.04%) (9672/11648)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss_1: (0.4960) | Acc_1: (83.01%) (10731/12928)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss_1: (0.4982) | Acc_1: (82.90%) (11778/14208)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss_1: (0.4981) | Acc_1: (82.85%) (12832/15488)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss_1: (0.4995) | Acc_1: (82.84%) (13890/16768)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss_1: (0.5006) | Acc_1: (82.80%) (14944/18048)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss_1: (0.4998) | Acc_1: (82.87%) (16018/19328)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss_1: (0.5002) | Acc_1: (82.87%) (17077/20608)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss_1: (0.5017) | Acc_1: (82.81%) (18125/21888)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss_1: (0.5009) | Acc_1: (82.79%) (19180/23168)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss_1: (0.5010) | Acc_1: (82.78%) (20237/24448)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss_1: (0.5006) | Acc_1: (82.72%) (21281/25728)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss_1: (0.5021) | Acc_1: (82.63%) (22318/27008)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss_1: (0.5042) | Acc_1: (82.57%) (23358/28288)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss_1: (0.5047) | Acc_1: (82.55%) (24408/29568)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss_1: (0.5034) | Acc_1: (82.61%) (25483/30848)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss_1: (0.5016) | Acc_1: (82.66%) (26558/32128)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss_1: (0.5012) | Acc_1: (82.68%) (27623/33408)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss_1: (0.5026) | Acc_1: (82.67%) (28676/34688)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss_1: (0.5018) | Acc_1: (82.71%) (29748/35968)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss_1: (0.5018) | Acc_1: (82.72%) (30811/37248)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss_1: (0.5030) | Acc_1: (82.65%) (31842/38528)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss_1: (0.5036) | Acc_1: (82.64%) (32896/39808)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss_1: (0.5030) | Acc_1: (82.67%) (33968/41088)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss_1: (0.5025) | Acc_1: (82.67%) (35024/42368)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss_1: (0.5024) | Acc_1: (82.69%) (36093/43648)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss_1: (0.5019) | Acc_1: (82.69%) (37150/44928)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss_1: (0.5004) | Acc_1: (82.75%) (38237/46208)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss_1: (0.5006) | Acc_1: (82.75%) (39294/47488)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss_1: (0.5011) | Acc_1: (82.72%) (40341/48768)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss_1: (0.5001) | Acc_1: (82.74%) (41368/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5428) | Acc: (81.97%) (8197/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (0.5199) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (0.4308) | Acc_1: (85.30%) (1201/1408)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (0.4297) | Acc_1: (85.01%) (2285/2688)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (0.4309) | Acc_1: (84.85%) (3367/3968)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss_1: (0.4308) | Acc_1: (85.02%) (4462/5248)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss_1: (0.4385) | Acc_1: (84.77%) (5534/6528)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss_1: (0.4433) | Acc_1: (84.57%) (6603/7808)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss_1: (0.4420) | Acc_1: (84.58%) (7687/9088)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss_1: (0.4467) | Acc_1: (84.48%) (8759/10368)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss_1: (0.4411) | Acc_1: (84.63%) (9858/11648)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss_1: (0.4408) | Acc_1: (84.67%) (10946/12928)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss_1: (0.4424) | Acc_1: (84.61%) (12021/14208)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss_1: (0.4447) | Acc_1: (84.62%) (13106/15488)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss_1: (0.4434) | Acc_1: (84.64%) (14193/16768)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss_1: (0.4451) | Acc_1: (84.57%) (15264/18048)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss_1: (0.4443) | Acc_1: (84.63%) (16358/19328)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss_1: (0.4439) | Acc_1: (84.66%) (17446/20608)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss_1: (0.4431) | Acc_1: (84.70%) (18540/21888)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss_1: (0.4429) | Acc_1: (84.72%) (19629/23168)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss_1: (0.4434) | Acc_1: (84.66%) (20697/24448)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss_1: (0.4463) | Acc_1: (84.53%) (21748/25728)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss_1: (0.4473) | Acc_1: (84.44%) (22805/27008)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss_1: (0.4469) | Acc_1: (84.46%) (23893/28288)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss_1: (0.4461) | Acc_1: (84.50%) (24986/29568)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss_1: (0.4459) | Acc_1: (84.51%) (26070/30848)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss_1: (0.4455) | Acc_1: (84.54%) (27161/32128)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss_1: (0.4473) | Acc_1: (84.45%) (28213/33408)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss_1: (0.4473) | Acc_1: (84.43%) (29288/34688)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss_1: (0.4462) | Acc_1: (84.47%) (30381/35968)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss_1: (0.4474) | Acc_1: (84.45%) (31457/37248)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss_1: (0.4470) | Acc_1: (84.48%) (32547/38528)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss_1: (0.4473) | Acc_1: (84.48%) (33630/39808)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss_1: (0.4466) | Acc_1: (84.49%) (34716/41088)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss_1: (0.4462) | Acc_1: (84.56%) (35825/42368)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss_1: (0.4456) | Acc_1: (84.60%) (36928/43648)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss_1: (0.4449) | Acc_1: (84.63%) (38023/44928)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss_1: (0.4448) | Acc_1: (84.64%) (39110/46208)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss_1: (0.4452) | Acc_1: (84.63%) (40191/47488)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss_1: (0.4454) | Acc_1: (84.64%) (41277/48768)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss_1: (0.4443) | Acc_1: (84.67%) (42334/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5582) | Acc: (81.44%) (8144/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (0.4071) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (0.4336) | Acc_1: (84.80%) (1194/1408)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (0.4262) | Acc_1: (85.04%) (2286/2688)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (0.4053) | Acc_1: (85.81%) (3405/3968)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss_1: (0.3969) | Acc_1: (86.22%) (4525/5248)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss_1: (0.3921) | Acc_1: (86.52%) (5648/6528)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss_1: (0.3921) | Acc_1: (86.51%) (6755/7808)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss_1: (0.3956) | Acc_1: (86.37%) (7849/9088)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss_1: (0.3991) | Acc_1: (86.22%) (8939/10368)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss_1: (0.4039) | Acc_1: (86.05%) (10023/11648)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss_1: (0.4018) | Acc_1: (86.19%) (11142/12928)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss_1: (0.4016) | Acc_1: (86.20%) (12248/14208)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss_1: (0.4040) | Acc_1: (86.07%) (13331/15488)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss_1: (0.4010) | Acc_1: (86.13%) (14443/16768)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss_1: (0.4014) | Acc_1: (86.11%) (15541/18048)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss_1: (0.4018) | Acc_1: (85.96%) (16614/19328)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss_1: (0.4025) | Acc_1: (85.98%) (17719/20608)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss_1: (0.4035) | Acc_1: (85.90%) (18801/21888)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss_1: (0.4044) | Acc_1: (85.87%) (19895/23168)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss_1: (0.4055) | Acc_1: (85.88%) (20995/24448)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss_1: (0.4036) | Acc_1: (85.94%) (22110/25728)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss_1: (0.4062) | Acc_1: (85.88%) (23194/27008)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss_1: (0.4068) | Acc_1: (85.86%) (24288/28288)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss_1: (0.4070) | Acc_1: (85.83%) (25378/29568)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss_1: (0.4067) | Acc_1: (85.84%) (26479/30848)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss_1: (0.4048) | Acc_1: (85.89%) (27594/32128)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss_1: (0.4033) | Acc_1: (85.97%) (28720/33408)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss_1: (0.4024) | Acc_1: (86.00%) (29830/34688)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss_1: (0.4021) | Acc_1: (86.03%) (30943/35968)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss_1: (0.4027) | Acc_1: (86.03%) (32046/37248)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss_1: (0.4017) | Acc_1: (86.08%) (33166/38528)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss_1: (0.4018) | Acc_1: (86.10%) (34276/39808)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss_1: (0.4016) | Acc_1: (86.12%) (35386/41088)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss_1: (0.4020) | Acc_1: (86.10%) (36479/42368)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss_1: (0.4028) | Acc_1: (86.07%) (37567/43648)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss_1: (0.4046) | Acc_1: (85.98%) (38628/44928)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss_1: (0.4041) | Acc_1: (86.02%) (39746/46208)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss_1: (0.4051) | Acc_1: (86.00%) (40839/47488)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss_1: (0.4042) | Acc_1: (86.03%) (41953/48768)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss_1: (0.4035) | Acc_1: (86.05%) (43026/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5152) | Acc: (83.16%) (8316/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (0.3312) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (0.3655) | Acc_1: (87.36%) (1230/1408)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (0.3842) | Acc_1: (86.24%) (2318/2688)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.3789) | Acc_1: (86.27%) (3423/3968)\n",
      "Epoch: 8 | Batch_idx: 40 |  Loss_1: (0.3761) | Acc_1: (86.68%) (4549/5248)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss_1: (0.3728) | Acc_1: (87.09%) (5685/6528)\n",
      "Epoch: 8 | Batch_idx: 60 |  Loss_1: (0.3651) | Acc_1: (87.46%) (6829/7808)\n",
      "Epoch: 8 | Batch_idx: 70 |  Loss_1: (0.3650) | Acc_1: (87.58%) (7959/9088)\n",
      "Epoch: 8 | Batch_idx: 80 |  Loss_1: (0.3703) | Acc_1: (87.28%) (9049/10368)\n",
      "Epoch: 8 | Batch_idx: 90 |  Loss_1: (0.3718) | Acc_1: (87.28%) (10166/11648)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss_1: (0.3721) | Acc_1: (87.21%) (11275/12928)\n",
      "Epoch: 8 | Batch_idx: 110 |  Loss_1: (0.3663) | Acc_1: (87.41%) (12419/14208)\n",
      "Epoch: 8 | Batch_idx: 120 |  Loss_1: (0.3678) | Acc_1: (87.36%) (13530/15488)\n",
      "Epoch: 8 | Batch_idx: 130 |  Loss_1: (0.3658) | Acc_1: (87.45%) (14663/16768)\n",
      "Epoch: 8 | Batch_idx: 140 |  Loss_1: (0.3712) | Acc_1: (87.19%) (15736/18048)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss_1: (0.3698) | Acc_1: (87.17%) (16849/19328)\n",
      "Epoch: 8 | Batch_idx: 160 |  Loss_1: (0.3692) | Acc_1: (87.19%) (17969/20608)\n",
      "Epoch: 8 | Batch_idx: 170 |  Loss_1: (0.3693) | Acc_1: (87.19%) (19085/21888)\n",
      "Epoch: 8 | Batch_idx: 180 |  Loss_1: (0.3690) | Acc_1: (87.21%) (20205/23168)\n",
      "Epoch: 8 | Batch_idx: 190 |  Loss_1: (0.3707) | Acc_1: (87.18%) (21314/24448)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss_1: (0.3693) | Acc_1: (87.25%) (22447/25728)\n",
      "Epoch: 8 | Batch_idx: 210 |  Loss_1: (0.3660) | Acc_1: (87.34%) (23590/27008)\n",
      "Epoch: 8 | Batch_idx: 220 |  Loss_1: (0.3659) | Acc_1: (87.30%) (24696/28288)\n",
      "Epoch: 8 | Batch_idx: 230 |  Loss_1: (0.3688) | Acc_1: (87.22%) (25788/29568)\n",
      "Epoch: 8 | Batch_idx: 240 |  Loss_1: (0.3716) | Acc_1: (87.14%) (26880/30848)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss_1: (0.3697) | Acc_1: (87.18%) (28010/32128)\n",
      "Epoch: 8 | Batch_idx: 260 |  Loss_1: (0.3686) | Acc_1: (87.21%) (29135/33408)\n",
      "Epoch: 8 | Batch_idx: 270 |  Loss_1: (0.3681) | Acc_1: (87.20%) (30248/34688)\n",
      "Epoch: 8 | Batch_idx: 280 |  Loss_1: (0.3681) | Acc_1: (87.22%) (31372/35968)\n",
      "Epoch: 8 | Batch_idx: 290 |  Loss_1: (0.3685) | Acc_1: (87.22%) (32487/37248)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss_1: (0.3677) | Acc_1: (87.26%) (33620/38528)\n",
      "Epoch: 8 | Batch_idx: 310 |  Loss_1: (0.3664) | Acc_1: (87.28%) (34745/39808)\n",
      "Epoch: 8 | Batch_idx: 320 |  Loss_1: (0.3664) | Acc_1: (87.28%) (35861/41088)\n",
      "Epoch: 8 | Batch_idx: 330 |  Loss_1: (0.3664) | Acc_1: (87.28%) (36977/42368)\n",
      "Epoch: 8 | Batch_idx: 340 |  Loss_1: (0.3658) | Acc_1: (87.25%) (38083/43648)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss_1: (0.3657) | Acc_1: (87.25%) (39201/44928)\n",
      "Epoch: 8 | Batch_idx: 360 |  Loss_1: (0.3649) | Acc_1: (87.26%) (40321/46208)\n",
      "Epoch: 8 | Batch_idx: 370 |  Loss_1: (0.3653) | Acc_1: (87.23%) (41426/47488)\n",
      "Epoch: 8 | Batch_idx: 380 |  Loss_1: (0.3649) | Acc_1: (87.26%) (42556/48768)\n",
      "Epoch: 8 | Batch_idx: 390 |  Loss_1: (0.3636) | Acc_1: (87.28%) (43642/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4503) | Acc: (85.58%) (8558/10000)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (0.2704) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (0.3262) | Acc_1: (89.06%) (1254/1408)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (0.3180) | Acc_1: (89.14%) (2396/2688)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (0.3158) | Acc_1: (89.29%) (3543/3968)\n",
      "Epoch: 9 | Batch_idx: 40 |  Loss_1: (0.3320) | Acc_1: (88.68%) (4654/5248)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss_1: (0.3284) | Acc_1: (88.83%) (5799/6528)\n",
      "Epoch: 9 | Batch_idx: 60 |  Loss_1: (0.3264) | Acc_1: (88.79%) (6933/7808)\n",
      "Epoch: 9 | Batch_idx: 70 |  Loss_1: (0.3301) | Acc_1: (88.61%) (8053/9088)\n",
      "Epoch: 9 | Batch_idx: 80 |  Loss_1: (0.3332) | Acc_1: (88.44%) (9169/10368)\n",
      "Epoch: 9 | Batch_idx: 90 |  Loss_1: (0.3336) | Acc_1: (88.43%) (10300/11648)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss_1: (0.3336) | Acc_1: (88.46%) (11436/12928)\n",
      "Epoch: 9 | Batch_idx: 110 |  Loss_1: (0.3305) | Acc_1: (88.58%) (12585/14208)\n",
      "Epoch: 9 | Batch_idx: 120 |  Loss_1: (0.3314) | Acc_1: (88.52%) (13710/15488)\n",
      "Epoch: 9 | Batch_idx: 130 |  Loss_1: (0.3329) | Acc_1: (88.45%) (14832/16768)\n",
      "Epoch: 9 | Batch_idx: 140 |  Loss_1: (0.3339) | Acc_1: (88.43%) (15960/18048)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss_1: (0.3352) | Acc_1: (88.42%) (17089/19328)\n",
      "Epoch: 9 | Batch_idx: 160 |  Loss_1: (0.3351) | Acc_1: (88.39%) (18215/20608)\n",
      "Epoch: 9 | Batch_idx: 170 |  Loss_1: (0.3356) | Acc_1: (88.33%) (19333/21888)\n",
      "Epoch: 9 | Batch_idx: 180 |  Loss_1: (0.3334) | Acc_1: (88.42%) (20486/23168)\n",
      "Epoch: 9 | Batch_idx: 190 |  Loss_1: (0.3327) | Acc_1: (88.47%) (21628/24448)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss_1: (0.3338) | Acc_1: (88.41%) (22745/25728)\n",
      "Epoch: 9 | Batch_idx: 210 |  Loss_1: (0.3345) | Acc_1: (88.35%) (23862/27008)\n",
      "Epoch: 9 | Batch_idx: 220 |  Loss_1: (0.3338) | Acc_1: (88.37%) (24999/28288)\n",
      "Epoch: 9 | Batch_idx: 230 |  Loss_1: (0.3340) | Acc_1: (88.39%) (26134/29568)\n",
      "Epoch: 9 | Batch_idx: 240 |  Loss_1: (0.3340) | Acc_1: (88.40%) (27269/30848)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss_1: (0.3345) | Acc_1: (88.37%) (28391/32128)\n",
      "Epoch: 9 | Batch_idx: 260 |  Loss_1: (0.3342) | Acc_1: (88.39%) (29529/33408)\n",
      "Epoch: 9 | Batch_idx: 270 |  Loss_1: (0.3342) | Acc_1: (88.39%) (30662/34688)\n",
      "Epoch: 9 | Batch_idx: 280 |  Loss_1: (0.3346) | Acc_1: (88.35%) (31776/35968)\n",
      "Epoch: 9 | Batch_idx: 290 |  Loss_1: (0.3349) | Acc_1: (88.35%) (32909/37248)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss_1: (0.3332) | Acc_1: (88.43%) (34069/38528)\n",
      "Epoch: 9 | Batch_idx: 310 |  Loss_1: (0.3322) | Acc_1: (88.46%) (35214/39808)\n",
      "Epoch: 9 | Batch_idx: 320 |  Loss_1: (0.3333) | Acc_1: (88.40%) (36320/41088)\n",
      "Epoch: 9 | Batch_idx: 330 |  Loss_1: (0.3328) | Acc_1: (88.41%) (37459/42368)\n",
      "Epoch: 9 | Batch_idx: 340 |  Loss_1: (0.3327) | Acc_1: (88.43%) (38597/43648)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss_1: (0.3330) | Acc_1: (88.41%) (39719/44928)\n",
      "Epoch: 9 | Batch_idx: 360 |  Loss_1: (0.3330) | Acc_1: (88.43%) (40860/46208)\n",
      "Epoch: 9 | Batch_idx: 370 |  Loss_1: (0.3332) | Acc_1: (88.44%) (41998/47488)\n",
      "Epoch: 9 | Batch_idx: 380 |  Loss_1: (0.3319) | Acc_1: (88.48%) (43148/48768)\n",
      "Epoch: 9 | Batch_idx: 390 |  Loss_1: (0.3333) | Acc_1: (88.43%) (44214/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4307) | Acc: (85.89%) (8589/10000)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (0.2589) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.2990) | Acc_1: (89.20%) (1256/1408)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.2915) | Acc_1: (89.66%) (2410/2688)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.2975) | Acc_1: (89.49%) (3551/3968)\n",
      "Epoch: 10 | Batch_idx: 40 |  Loss_1: (0.2975) | Acc_1: (89.52%) (4698/5248)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss_1: (0.2961) | Acc_1: (89.57%) (5847/6528)\n",
      "Epoch: 10 | Batch_idx: 60 |  Loss_1: (0.2908) | Acc_1: (89.86%) (7016/7808)\n",
      "Epoch: 10 | Batch_idx: 70 |  Loss_1: (0.2958) | Acc_1: (89.72%) (8154/9088)\n",
      "Epoch: 10 | Batch_idx: 80 |  Loss_1: (0.3017) | Acc_1: (89.52%) (9281/10368)\n",
      "Epoch: 10 | Batch_idx: 90 |  Loss_1: (0.2995) | Acc_1: (89.62%) (10439/11648)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss_1: (0.2984) | Acc_1: (89.67%) (11592/12928)\n",
      "Epoch: 10 | Batch_idx: 110 |  Loss_1: (0.2974) | Acc_1: (89.68%) (12742/14208)\n",
      "Epoch: 10 | Batch_idx: 120 |  Loss_1: (0.2993) | Acc_1: (89.53%) (13867/15488)\n",
      "Epoch: 10 | Batch_idx: 130 |  Loss_1: (0.3028) | Acc_1: (89.46%) (15000/16768)\n",
      "Epoch: 10 | Batch_idx: 140 |  Loss_1: (0.3088) | Acc_1: (89.28%) (16113/18048)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss_1: (0.3099) | Acc_1: (89.27%) (17255/19328)\n",
      "Epoch: 10 | Batch_idx: 160 |  Loss_1: (0.3109) | Acc_1: (89.26%) (18395/20608)\n",
      "Epoch: 10 | Batch_idx: 170 |  Loss_1: (0.3121) | Acc_1: (89.24%) (19532/21888)\n",
      "Epoch: 10 | Batch_idx: 180 |  Loss_1: (0.3113) | Acc_1: (89.27%) (20682/23168)\n",
      "Epoch: 10 | Batch_idx: 190 |  Loss_1: (0.3094) | Acc_1: (89.34%) (21841/24448)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss_1: (0.3097) | Acc_1: (89.27%) (22968/25728)\n",
      "Epoch: 10 | Batch_idx: 210 |  Loss_1: (0.3086) | Acc_1: (89.34%) (24128/27008)\n",
      "Epoch: 10 | Batch_idx: 220 |  Loss_1: (0.3069) | Acc_1: (89.41%) (25291/28288)\n",
      "Epoch: 10 | Batch_idx: 230 |  Loss_1: (0.3052) | Acc_1: (89.48%) (26458/29568)\n",
      "Epoch: 10 | Batch_idx: 240 |  Loss_1: (0.3034) | Acc_1: (89.54%) (27621/30848)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss_1: (0.3030) | Acc_1: (89.56%) (28774/32128)\n",
      "Epoch: 10 | Batch_idx: 260 |  Loss_1: (0.3039) | Acc_1: (89.54%) (29914/33408)\n",
      "Epoch: 10 | Batch_idx: 270 |  Loss_1: (0.3039) | Acc_1: (89.53%) (31056/34688)\n",
      "Epoch: 10 | Batch_idx: 280 |  Loss_1: (0.3038) | Acc_1: (89.54%) (32204/35968)\n",
      "Epoch: 10 | Batch_idx: 290 |  Loss_1: (0.3026) | Acc_1: (89.55%) (33356/37248)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss_1: (0.3019) | Acc_1: (89.59%) (34517/38528)\n",
      "Epoch: 10 | Batch_idx: 310 |  Loss_1: (0.3034) | Acc_1: (89.56%) (35653/39808)\n",
      "Epoch: 10 | Batch_idx: 320 |  Loss_1: (0.3026) | Acc_1: (89.56%) (36799/41088)\n",
      "Epoch: 10 | Batch_idx: 330 |  Loss_1: (0.3019) | Acc_1: (89.58%) (37952/42368)\n",
      "Epoch: 10 | Batch_idx: 340 |  Loss_1: (0.3020) | Acc_1: (89.57%) (39097/43648)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss_1: (0.3025) | Acc_1: (89.55%) (40234/44928)\n",
      "Epoch: 10 | Batch_idx: 360 |  Loss_1: (0.3022) | Acc_1: (89.56%) (41384/46208)\n",
      "Epoch: 10 | Batch_idx: 370 |  Loss_1: (0.3016) | Acc_1: (89.59%) (42546/47488)\n",
      "Epoch: 10 | Batch_idx: 380 |  Loss_1: (0.3032) | Acc_1: (89.55%) (43670/48768)\n",
      "Epoch: 10 | Batch_idx: 390 |  Loss_1: (0.3032) | Acc_1: (89.55%) (44777/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4310) | Acc: (85.60%) (8560/10000)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.4053) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.2724) | Acc_1: (90.55%) (1275/1408)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.2589) | Acc_1: (91.03%) (2447/2688)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.2727) | Acc_1: (90.40%) (3587/3968)\n",
      "Epoch: 11 | Batch_idx: 40 |  Loss_1: (0.2672) | Acc_1: (90.40%) (4744/5248)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss_1: (0.2700) | Acc_1: (90.49%) (5907/6528)\n",
      "Epoch: 11 | Batch_idx: 60 |  Loss_1: (0.2623) | Acc_1: (90.78%) (7088/7808)\n",
      "Epoch: 11 | Batch_idx: 70 |  Loss_1: (0.2637) | Acc_1: (90.69%) (8242/9088)\n",
      "Epoch: 11 | Batch_idx: 80 |  Loss_1: (0.2652) | Acc_1: (90.58%) (9391/10368)\n",
      "Epoch: 11 | Batch_idx: 90 |  Loss_1: (0.2673) | Acc_1: (90.51%) (10543/11648)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss_1: (0.2654) | Acc_1: (90.65%) (11719/12928)\n",
      "Epoch: 11 | Batch_idx: 110 |  Loss_1: (0.2659) | Acc_1: (90.62%) (12876/14208)\n",
      "Epoch: 11 | Batch_idx: 120 |  Loss_1: (0.2671) | Acc_1: (90.55%) (14024/15488)\n",
      "Epoch: 11 | Batch_idx: 130 |  Loss_1: (0.2704) | Acc_1: (90.40%) (15158/16768)\n",
      "Epoch: 11 | Batch_idx: 140 |  Loss_1: (0.2699) | Acc_1: (90.43%) (16321/18048)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss_1: (0.2726) | Acc_1: (90.38%) (17468/19328)\n",
      "Epoch: 11 | Batch_idx: 160 |  Loss_1: (0.2748) | Acc_1: (90.31%) (18611/20608)\n",
      "Epoch: 11 | Batch_idx: 170 |  Loss_1: (0.2768) | Acc_1: (90.27%) (19758/21888)\n",
      "Epoch: 11 | Batch_idx: 180 |  Loss_1: (0.2780) | Acc_1: (90.26%) (20912/23168)\n",
      "Epoch: 11 | Batch_idx: 190 |  Loss_1: (0.2775) | Acc_1: (90.24%) (22063/24448)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss_1: (0.2768) | Acc_1: (90.27%) (23225/25728)\n",
      "Epoch: 11 | Batch_idx: 210 |  Loss_1: (0.2757) | Acc_1: (90.31%) (24390/27008)\n",
      "Epoch: 11 | Batch_idx: 220 |  Loss_1: (0.2769) | Acc_1: (90.29%) (25540/28288)\n",
      "Epoch: 11 | Batch_idx: 230 |  Loss_1: (0.2759) | Acc_1: (90.33%) (26708/29568)\n",
      "Epoch: 11 | Batch_idx: 240 |  Loss_1: (0.2764) | Acc_1: (90.33%) (27864/30848)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss_1: (0.2763) | Acc_1: (90.30%) (29010/32128)\n",
      "Epoch: 11 | Batch_idx: 260 |  Loss_1: (0.2763) | Acc_1: (90.29%) (30164/33408)\n",
      "Epoch: 11 | Batch_idx: 270 |  Loss_1: (0.2769) | Acc_1: (90.28%) (31317/34688)\n",
      "Epoch: 11 | Batch_idx: 280 |  Loss_1: (0.2767) | Acc_1: (90.27%) (32468/35968)\n",
      "Epoch: 11 | Batch_idx: 290 |  Loss_1: (0.2768) | Acc_1: (90.28%) (33627/37248)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss_1: (0.2766) | Acc_1: (90.29%) (34788/38528)\n",
      "Epoch: 11 | Batch_idx: 310 |  Loss_1: (0.2752) | Acc_1: (90.32%) (35955/39808)\n",
      "Epoch: 11 | Batch_idx: 320 |  Loss_1: (0.2751) | Acc_1: (90.35%) (37123/41088)\n",
      "Epoch: 11 | Batch_idx: 330 |  Loss_1: (0.2755) | Acc_1: (90.34%) (38274/42368)\n",
      "Epoch: 11 | Batch_idx: 340 |  Loss_1: (0.2750) | Acc_1: (90.36%) (39439/43648)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss_1: (0.2756) | Acc_1: (90.33%) (40585/44928)\n",
      "Epoch: 11 | Batch_idx: 360 |  Loss_1: (0.2766) | Acc_1: (90.32%) (41733/46208)\n",
      "Epoch: 11 | Batch_idx: 370 |  Loss_1: (0.2769) | Acc_1: (90.30%) (42881/47488)\n",
      "Epoch: 11 | Batch_idx: 380 |  Loss_1: (0.2776) | Acc_1: (90.24%) (44009/48768)\n",
      "Epoch: 11 | Batch_idx: 390 |  Loss_1: (0.2797) | Acc_1: (90.17%) (45087/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4232) | Acc: (86.63%) (8663/10000)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.1565) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.2334) | Acc_1: (91.76%) (1292/1408)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.2327) | Acc_1: (91.29%) (2454/2688)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.2442) | Acc_1: (91.15%) (3617/3968)\n",
      "Epoch: 12 | Batch_idx: 40 |  Loss_1: (0.2423) | Acc_1: (91.22%) (4787/5248)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss_1: (0.2453) | Acc_1: (91.12%) (5948/6528)\n",
      "Epoch: 12 | Batch_idx: 60 |  Loss_1: (0.2377) | Acc_1: (91.29%) (7128/7808)\n",
      "Epoch: 12 | Batch_idx: 70 |  Loss_1: (0.2370) | Acc_1: (91.35%) (8302/9088)\n",
      "Epoch: 12 | Batch_idx: 80 |  Loss_1: (0.2432) | Acc_1: (91.26%) (9462/10368)\n",
      "Epoch: 12 | Batch_idx: 90 |  Loss_1: (0.2438) | Acc_1: (91.31%) (10636/11648)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss_1: (0.2489) | Acc_1: (91.21%) (11792/12928)\n",
      "Epoch: 12 | Batch_idx: 110 |  Loss_1: (0.2485) | Acc_1: (91.17%) (12953/14208)\n",
      "Epoch: 12 | Batch_idx: 120 |  Loss_1: (0.2522) | Acc_1: (91.08%) (14107/15488)\n",
      "Epoch: 12 | Batch_idx: 130 |  Loss_1: (0.2543) | Acc_1: (90.97%) (15254/16768)\n",
      "Epoch: 12 | Batch_idx: 140 |  Loss_1: (0.2558) | Acc_1: (90.92%) (16409/18048)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss_1: (0.2565) | Acc_1: (90.94%) (17577/19328)\n",
      "Epoch: 12 | Batch_idx: 160 |  Loss_1: (0.2584) | Acc_1: (90.89%) (18731/20608)\n",
      "Epoch: 12 | Batch_idx: 170 |  Loss_1: (0.2578) | Acc_1: (90.92%) (19901/21888)\n",
      "Epoch: 12 | Batch_idx: 180 |  Loss_1: (0.2578) | Acc_1: (90.89%) (21057/23168)\n",
      "Epoch: 12 | Batch_idx: 190 |  Loss_1: (0.2568) | Acc_1: (90.91%) (22225/24448)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss_1: (0.2552) | Acc_1: (91.00%) (23412/25728)\n",
      "Epoch: 12 | Batch_idx: 210 |  Loss_1: (0.2563) | Acc_1: (90.97%) (24570/27008)\n",
      "Epoch: 12 | Batch_idx: 220 |  Loss_1: (0.2556) | Acc_1: (90.96%) (25732/28288)\n",
      "Epoch: 12 | Batch_idx: 230 |  Loss_1: (0.2564) | Acc_1: (90.93%) (26885/29568)\n",
      "Epoch: 12 | Batch_idx: 240 |  Loss_1: (0.2559) | Acc_1: (90.98%) (28064/30848)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss_1: (0.2549) | Acc_1: (91.01%) (29239/32128)\n",
      "Epoch: 12 | Batch_idx: 260 |  Loss_1: (0.2543) | Acc_1: (91.04%) (30413/33408)\n",
      "Epoch: 12 | Batch_idx: 270 |  Loss_1: (0.2535) | Acc_1: (91.05%) (31585/34688)\n",
      "Epoch: 12 | Batch_idx: 280 |  Loss_1: (0.2538) | Acc_1: (91.06%) (32753/35968)\n",
      "Epoch: 12 | Batch_idx: 290 |  Loss_1: (0.2549) | Acc_1: (91.02%) (33903/37248)\n",
      "Epoch: 12 | Batch_idx: 300 |  Loss_1: (0.2561) | Acc_1: (90.99%) (35058/38528)\n",
      "Epoch: 12 | Batch_idx: 310 |  Loss_1: (0.2564) | Acc_1: (90.98%) (36218/39808)\n",
      "Epoch: 12 | Batch_idx: 320 |  Loss_1: (0.2559) | Acc_1: (90.99%) (37387/41088)\n",
      "Epoch: 12 | Batch_idx: 330 |  Loss_1: (0.2571) | Acc_1: (90.93%) (38526/42368)\n",
      "Epoch: 12 | Batch_idx: 340 |  Loss_1: (0.2569) | Acc_1: (90.94%) (39692/43648)\n",
      "Epoch: 12 | Batch_idx: 350 |  Loss_1: (0.2574) | Acc_1: (90.91%) (40845/44928)\n",
      "Epoch: 12 | Batch_idx: 360 |  Loss_1: (0.2580) | Acc_1: (90.88%) (41995/46208)\n",
      "Epoch: 12 | Batch_idx: 370 |  Loss_1: (0.2578) | Acc_1: (90.90%) (43166/47488)\n",
      "Epoch: 12 | Batch_idx: 380 |  Loss_1: (0.2583) | Acc_1: (90.88%) (44320/48768)\n",
      "Epoch: 12 | Batch_idx: 390 |  Loss_1: (0.2579) | Acc_1: (90.90%) (45448/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4035) | Acc: (87.30%) (8730/10000)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.2890) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.2269) | Acc_1: (91.83%) (1293/1408)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.2162) | Acc_1: (92.19%) (2478/2688)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.2187) | Acc_1: (92.31%) (3663/3968)\n",
      "Epoch: 13 | Batch_idx: 40 |  Loss_1: (0.2148) | Acc_1: (92.59%) (4859/5248)\n",
      "Epoch: 13 | Batch_idx: 50 |  Loss_1: (0.2173) | Acc_1: (92.56%) (6042/6528)\n",
      "Epoch: 13 | Batch_idx: 60 |  Loss_1: (0.2179) | Acc_1: (92.41%) (7215/7808)\n",
      "Epoch: 13 | Batch_idx: 70 |  Loss_1: (0.2248) | Acc_1: (92.17%) (8376/9088)\n",
      "Epoch: 13 | Batch_idx: 80 |  Loss_1: (0.2273) | Acc_1: (92.03%) (9542/10368)\n",
      "Epoch: 13 | Batch_idx: 90 |  Loss_1: (0.2314) | Acc_1: (91.84%) (10697/11648)\n",
      "Epoch: 13 | Batch_idx: 100 |  Loss_1: (0.2295) | Acc_1: (91.84%) (11873/12928)\n",
      "Epoch: 13 | Batch_idx: 110 |  Loss_1: (0.2329) | Acc_1: (91.72%) (13031/14208)\n",
      "Epoch: 13 | Batch_idx: 120 |  Loss_1: (0.2325) | Acc_1: (91.77%) (14213/15488)\n",
      "Epoch: 13 | Batch_idx: 130 |  Loss_1: (0.2300) | Acc_1: (91.86%) (15403/16768)\n",
      "Epoch: 13 | Batch_idx: 140 |  Loss_1: (0.2318) | Acc_1: (91.80%) (16568/18048)\n",
      "Epoch: 13 | Batch_idx: 150 |  Loss_1: (0.2319) | Acc_1: (91.83%) (17748/19328)\n",
      "Epoch: 13 | Batch_idx: 160 |  Loss_1: (0.2333) | Acc_1: (91.76%) (18909/20608)\n",
      "Epoch: 13 | Batch_idx: 170 |  Loss_1: (0.2332) | Acc_1: (91.76%) (20085/21888)\n",
      "Epoch: 13 | Batch_idx: 180 |  Loss_1: (0.2326) | Acc_1: (91.77%) (21261/23168)\n",
      "Epoch: 13 | Batch_idx: 190 |  Loss_1: (0.2333) | Acc_1: (91.77%) (22435/24448)\n",
      "Epoch: 13 | Batch_idx: 200 |  Loss_1: (0.2338) | Acc_1: (91.74%) (23603/25728)\n",
      "Epoch: 13 | Batch_idx: 210 |  Loss_1: (0.2342) | Acc_1: (91.75%) (24779/27008)\n",
      "Epoch: 13 | Batch_idx: 220 |  Loss_1: (0.2335) | Acc_1: (91.81%) (25970/28288)\n",
      "Epoch: 13 | Batch_idx: 230 |  Loss_1: (0.2335) | Acc_1: (91.83%) (27152/29568)\n",
      "Epoch: 13 | Batch_idx: 240 |  Loss_1: (0.2338) | Acc_1: (91.83%) (28329/30848)\n",
      "Epoch: 13 | Batch_idx: 250 |  Loss_1: (0.2348) | Acc_1: (91.82%) (29499/32128)\n",
      "Epoch: 13 | Batch_idx: 260 |  Loss_1: (0.2339) | Acc_1: (91.86%) (30690/33408)\n",
      "Epoch: 13 | Batch_idx: 270 |  Loss_1: (0.2364) | Acc_1: (91.77%) (31833/34688)\n",
      "Epoch: 13 | Batch_idx: 280 |  Loss_1: (0.2384) | Acc_1: (91.71%) (32986/35968)\n",
      "Epoch: 13 | Batch_idx: 290 |  Loss_1: (0.2385) | Acc_1: (91.69%) (34154/37248)\n",
      "Epoch: 13 | Batch_idx: 300 |  Loss_1: (0.2371) | Acc_1: (91.75%) (35351/38528)\n",
      "Epoch: 13 | Batch_idx: 310 |  Loss_1: (0.2370) | Acc_1: (91.75%) (36525/39808)\n",
      "Epoch: 13 | Batch_idx: 320 |  Loss_1: (0.2370) | Acc_1: (91.76%) (37701/41088)\n",
      "Epoch: 13 | Batch_idx: 330 |  Loss_1: (0.2365) | Acc_1: (91.78%) (38884/42368)\n",
      "Epoch: 13 | Batch_idx: 340 |  Loss_1: (0.2370) | Acc_1: (91.74%) (40043/43648)\n",
      "Epoch: 13 | Batch_idx: 350 |  Loss_1: (0.2376) | Acc_1: (91.73%) (41211/44928)\n",
      "Epoch: 13 | Batch_idx: 360 |  Loss_1: (0.2383) | Acc_1: (91.71%) (42376/46208)\n",
      "Epoch: 13 | Batch_idx: 370 |  Loss_1: (0.2388) | Acc_1: (91.67%) (43534/47488)\n",
      "Epoch: 13 | Batch_idx: 380 |  Loss_1: (0.2393) | Acc_1: (91.64%) (44692/48768)\n",
      "Epoch: 13 | Batch_idx: 390 |  Loss_1: (0.2396) | Acc_1: (91.63%) (45816/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3796) | Acc: (87.96%) (8796/10000)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.2866) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.2228) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.2039) | Acc_1: (93.38%) (2510/2688)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.1974) | Acc_1: (93.32%) (3703/3968)\n",
      "Epoch: 14 | Batch_idx: 40 |  Loss_1: (0.1926) | Acc_1: (93.46%) (4905/5248)\n",
      "Epoch: 14 | Batch_idx: 50 |  Loss_1: (0.1933) | Acc_1: (93.34%) (6093/6528)\n",
      "Epoch: 14 | Batch_idx: 60 |  Loss_1: (0.1927) | Acc_1: (93.35%) (7289/7808)\n",
      "Epoch: 14 | Batch_idx: 70 |  Loss_1: (0.1966) | Acc_1: (93.29%) (8478/9088)\n",
      "Epoch: 14 | Batch_idx: 80 |  Loss_1: (0.1968) | Acc_1: (93.33%) (9676/10368)\n",
      "Epoch: 14 | Batch_idx: 90 |  Loss_1: (0.1992) | Acc_1: (93.17%) (10853/11648)\n",
      "Epoch: 14 | Batch_idx: 100 |  Loss_1: (0.2000) | Acc_1: (93.11%) (12037/12928)\n",
      "Epoch: 14 | Batch_idx: 110 |  Loss_1: (0.2027) | Acc_1: (93.02%) (13216/14208)\n",
      "Epoch: 14 | Batch_idx: 120 |  Loss_1: (0.2052) | Acc_1: (92.91%) (14390/15488)\n",
      "Epoch: 14 | Batch_idx: 130 |  Loss_1: (0.2062) | Acc_1: (92.89%) (15576/16768)\n",
      "Epoch: 14 | Batch_idx: 140 |  Loss_1: (0.2052) | Acc_1: (92.89%) (16765/18048)\n",
      "Epoch: 14 | Batch_idx: 150 |  Loss_1: (0.2078) | Acc_1: (92.73%) (17923/19328)\n",
      "Epoch: 14 | Batch_idx: 160 |  Loss_1: (0.2070) | Acc_1: (92.76%) (19117/20608)\n",
      "Epoch: 14 | Batch_idx: 170 |  Loss_1: (0.2084) | Acc_1: (92.70%) (20291/21888)\n",
      "Epoch: 14 | Batch_idx: 180 |  Loss_1: (0.2092) | Acc_1: (92.69%) (21475/23168)\n",
      "Epoch: 14 | Batch_idx: 190 |  Loss_1: (0.2120) | Acc_1: (92.62%) (22644/24448)\n",
      "Epoch: 14 | Batch_idx: 200 |  Loss_1: (0.2120) | Acc_1: (92.62%) (23830/25728)\n",
      "Epoch: 14 | Batch_idx: 210 |  Loss_1: (0.2136) | Acc_1: (92.58%) (25005/27008)\n",
      "Epoch: 14 | Batch_idx: 220 |  Loss_1: (0.2130) | Acc_1: (92.57%) (26187/28288)\n",
      "Epoch: 14 | Batch_idx: 230 |  Loss_1: (0.2130) | Acc_1: (92.55%) (27364/29568)\n",
      "Epoch: 14 | Batch_idx: 240 |  Loss_1: (0.2128) | Acc_1: (92.54%) (28547/30848)\n",
      "Epoch: 14 | Batch_idx: 250 |  Loss_1: (0.2121) | Acc_1: (92.55%) (29733/32128)\n",
      "Epoch: 14 | Batch_idx: 260 |  Loss_1: (0.2126) | Acc_1: (92.54%) (30915/33408)\n",
      "Epoch: 14 | Batch_idx: 270 |  Loss_1: (0.2140) | Acc_1: (92.50%) (32088/34688)\n",
      "Epoch: 14 | Batch_idx: 280 |  Loss_1: (0.2140) | Acc_1: (92.50%) (33269/35968)\n",
      "Epoch: 14 | Batch_idx: 290 |  Loss_1: (0.2139) | Acc_1: (92.46%) (34440/37248)\n",
      "Epoch: 14 | Batch_idx: 300 |  Loss_1: (0.2141) | Acc_1: (92.45%) (35621/38528)\n",
      "Epoch: 14 | Batch_idx: 310 |  Loss_1: (0.2147) | Acc_1: (92.43%) (36794/39808)\n",
      "Epoch: 14 | Batch_idx: 320 |  Loss_1: (0.2156) | Acc_1: (92.42%) (37975/41088)\n",
      "Epoch: 14 | Batch_idx: 330 |  Loss_1: (0.2152) | Acc_1: (92.44%) (39165/42368)\n",
      "Epoch: 14 | Batch_idx: 340 |  Loss_1: (0.2146) | Acc_1: (92.47%) (40361/43648)\n",
      "Epoch: 14 | Batch_idx: 350 |  Loss_1: (0.2153) | Acc_1: (92.45%) (41536/44928)\n",
      "Epoch: 14 | Batch_idx: 360 |  Loss_1: (0.2151) | Acc_1: (92.47%) (42727/46208)\n",
      "Epoch: 14 | Batch_idx: 370 |  Loss_1: (0.2156) | Acc_1: (92.45%) (43904/47488)\n",
      "Epoch: 14 | Batch_idx: 380 |  Loss_1: (0.2159) | Acc_1: (92.45%) (45086/48768)\n",
      "Epoch: 14 | Batch_idx: 390 |  Loss_1: (0.2156) | Acc_1: (92.47%) (46234/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3947) | Acc: (88.02%) (8802/10000)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.2733) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.1840) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.1930) | Acc_1: (93.38%) (2510/2688)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.1947) | Acc_1: (93.15%) (3696/3968)\n",
      "Epoch: 15 | Batch_idx: 40 |  Loss_1: (0.1971) | Acc_1: (93.20%) (4891/5248)\n",
      "Epoch: 15 | Batch_idx: 50 |  Loss_1: (0.1918) | Acc_1: (93.35%) (6094/6528)\n",
      "Epoch: 15 | Batch_idx: 60 |  Loss_1: (0.1881) | Acc_1: (93.57%) (7306/7808)\n",
      "Epoch: 15 | Batch_idx: 70 |  Loss_1: (0.1878) | Acc_1: (93.55%) (8502/9088)\n",
      "Epoch: 15 | Batch_idx: 80 |  Loss_1: (0.1862) | Acc_1: (93.60%) (9704/10368)\n",
      "Epoch: 15 | Batch_idx: 90 |  Loss_1: (0.1871) | Acc_1: (93.61%) (10904/11648)\n",
      "Epoch: 15 | Batch_idx: 100 |  Loss_1: (0.1880) | Acc_1: (93.53%) (12092/12928)\n",
      "Epoch: 15 | Batch_idx: 110 |  Loss_1: (0.1886) | Acc_1: (93.55%) (13292/14208)\n",
      "Epoch: 15 | Batch_idx: 120 |  Loss_1: (0.1903) | Acc_1: (93.48%) (14478/15488)\n",
      "Epoch: 15 | Batch_idx: 130 |  Loss_1: (0.1904) | Acc_1: (93.48%) (15675/16768)\n",
      "Epoch: 15 | Batch_idx: 140 |  Loss_1: (0.1925) | Acc_1: (93.42%) (16860/18048)\n",
      "Epoch: 15 | Batch_idx: 150 |  Loss_1: (0.1946) | Acc_1: (93.28%) (18030/19328)\n",
      "Epoch: 15 | Batch_idx: 160 |  Loss_1: (0.1939) | Acc_1: (93.30%) (19227/20608)\n",
      "Epoch: 15 | Batch_idx: 170 |  Loss_1: (0.1957) | Acc_1: (93.22%) (20405/21888)\n",
      "Epoch: 15 | Batch_idx: 180 |  Loss_1: (0.1965) | Acc_1: (93.24%) (21601/23168)\n",
      "Epoch: 15 | Batch_idx: 190 |  Loss_1: (0.1956) | Acc_1: (93.25%) (22798/24448)\n",
      "Epoch: 15 | Batch_idx: 200 |  Loss_1: (0.1946) | Acc_1: (93.29%) (24001/25728)\n",
      "Epoch: 15 | Batch_idx: 210 |  Loss_1: (0.1949) | Acc_1: (93.30%) (25199/27008)\n",
      "Epoch: 15 | Batch_idx: 220 |  Loss_1: (0.1953) | Acc_1: (93.29%) (26391/28288)\n",
      "Epoch: 15 | Batch_idx: 230 |  Loss_1: (0.1951) | Acc_1: (93.28%) (27582/29568)\n",
      "Epoch: 15 | Batch_idx: 240 |  Loss_1: (0.1957) | Acc_1: (93.27%) (28771/30848)\n",
      "Epoch: 15 | Batch_idx: 250 |  Loss_1: (0.1966) | Acc_1: (93.22%) (29950/32128)\n",
      "Epoch: 15 | Batch_idx: 260 |  Loss_1: (0.1966) | Acc_1: (93.21%) (31138/33408)\n",
      "Epoch: 15 | Batch_idx: 270 |  Loss_1: (0.1980) | Acc_1: (93.14%) (32309/34688)\n",
      "Epoch: 15 | Batch_idx: 280 |  Loss_1: (0.1996) | Acc_1: (93.07%) (33476/35968)\n",
      "Epoch: 15 | Batch_idx: 290 |  Loss_1: (0.1994) | Acc_1: (93.09%) (34675/37248)\n",
      "Epoch: 15 | Batch_idx: 300 |  Loss_1: (0.1992) | Acc_1: (93.08%) (35862/38528)\n",
      "Epoch: 15 | Batch_idx: 310 |  Loss_1: (0.1987) | Acc_1: (93.12%) (37068/39808)\n",
      "Epoch: 15 | Batch_idx: 320 |  Loss_1: (0.1982) | Acc_1: (93.12%) (38263/41088)\n",
      "Epoch: 15 | Batch_idx: 330 |  Loss_1: (0.1991) | Acc_1: (93.10%) (39443/42368)\n",
      "Epoch: 15 | Batch_idx: 340 |  Loss_1: (0.1997) | Acc_1: (93.05%) (40613/43648)\n",
      "Epoch: 15 | Batch_idx: 350 |  Loss_1: (0.2013) | Acc_1: (92.99%) (41777/44928)\n",
      "Epoch: 15 | Batch_idx: 360 |  Loss_1: (0.2012) | Acc_1: (92.99%) (42971/46208)\n",
      "Epoch: 15 | Batch_idx: 370 |  Loss_1: (0.2010) | Acc_1: (92.99%) (44157/47488)\n",
      "Epoch: 15 | Batch_idx: 380 |  Loss_1: (0.2008) | Acc_1: (92.99%) (45349/48768)\n",
      "Epoch: 15 | Batch_idx: 390 |  Loss_1: (0.2013) | Acc_1: (92.97%) (46485/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3957) | Acc: (88.41%) (8841/10000)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.1472) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.1661) | Acc_1: (93.82%) (1321/1408)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.1721) | Acc_1: (93.94%) (2525/2688)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.1664) | Acc_1: (93.95%) (3728/3968)\n",
      "Epoch: 16 | Batch_idx: 40 |  Loss_1: (0.1651) | Acc_1: (94.02%) (4934/5248)\n",
      "Epoch: 16 | Batch_idx: 50 |  Loss_1: (0.1705) | Acc_1: (93.86%) (6127/6528)\n",
      "Epoch: 16 | Batch_idx: 60 |  Loss_1: (0.1731) | Acc_1: (93.80%) (7324/7808)\n",
      "Epoch: 16 | Batch_idx: 70 |  Loss_1: (0.1732) | Acc_1: (93.82%) (8526/9088)\n",
      "Epoch: 16 | Batch_idx: 80 |  Loss_1: (0.1741) | Acc_1: (93.79%) (9724/10368)\n",
      "Epoch: 16 | Batch_idx: 90 |  Loss_1: (0.1710) | Acc_1: (93.86%) (10933/11648)\n",
      "Epoch: 16 | Batch_idx: 100 |  Loss_1: (0.1699) | Acc_1: (93.94%) (12144/12928)\n",
      "Epoch: 16 | Batch_idx: 110 |  Loss_1: (0.1700) | Acc_1: (93.93%) (13345/14208)\n",
      "Epoch: 16 | Batch_idx: 120 |  Loss_1: (0.1711) | Acc_1: (93.90%) (14544/15488)\n",
      "Epoch: 16 | Batch_idx: 130 |  Loss_1: (0.1725) | Acc_1: (93.86%) (15738/16768)\n",
      "Epoch: 16 | Batch_idx: 140 |  Loss_1: (0.1749) | Acc_1: (93.74%) (16918/18048)\n",
      "Epoch: 16 | Batch_idx: 150 |  Loss_1: (0.1764) | Acc_1: (93.66%) (18103/19328)\n",
      "Epoch: 16 | Batch_idx: 160 |  Loss_1: (0.1780) | Acc_1: (93.60%) (19289/20608)\n",
      "Epoch: 16 | Batch_idx: 170 |  Loss_1: (0.1799) | Acc_1: (93.57%) (20481/21888)\n",
      "Epoch: 16 | Batch_idx: 180 |  Loss_1: (0.1810) | Acc_1: (93.54%) (21671/23168)\n",
      "Epoch: 16 | Batch_idx: 190 |  Loss_1: (0.1818) | Acc_1: (93.54%) (22868/24448)\n",
      "Epoch: 16 | Batch_idx: 200 |  Loss_1: (0.1823) | Acc_1: (93.52%) (24062/25728)\n",
      "Epoch: 16 | Batch_idx: 210 |  Loss_1: (0.1828) | Acc_1: (93.50%) (25253/27008)\n",
      "Epoch: 16 | Batch_idx: 220 |  Loss_1: (0.1824) | Acc_1: (93.54%) (26462/28288)\n",
      "Epoch: 16 | Batch_idx: 230 |  Loss_1: (0.1828) | Acc_1: (93.51%) (27649/29568)\n",
      "Epoch: 16 | Batch_idx: 240 |  Loss_1: (0.1832) | Acc_1: (93.50%) (28844/30848)\n",
      "Epoch: 16 | Batch_idx: 250 |  Loss_1: (0.1835) | Acc_1: (93.49%) (30036/32128)\n",
      "Epoch: 16 | Batch_idx: 260 |  Loss_1: (0.1829) | Acc_1: (93.49%) (31234/33408)\n",
      "Epoch: 16 | Batch_idx: 270 |  Loss_1: (0.1844) | Acc_1: (93.44%) (32413/34688)\n",
      "Epoch: 16 | Batch_idx: 280 |  Loss_1: (0.1841) | Acc_1: (93.46%) (33616/35968)\n",
      "Epoch: 16 | Batch_idx: 290 |  Loss_1: (0.1840) | Acc_1: (93.46%) (34811/37248)\n",
      "Epoch: 16 | Batch_idx: 300 |  Loss_1: (0.1842) | Acc_1: (93.45%) (36004/38528)\n",
      "Epoch: 16 | Batch_idx: 310 |  Loss_1: (0.1853) | Acc_1: (93.42%) (37189/39808)\n",
      "Epoch: 16 | Batch_idx: 320 |  Loss_1: (0.1856) | Acc_1: (93.38%) (38368/41088)\n",
      "Epoch: 16 | Batch_idx: 330 |  Loss_1: (0.1868) | Acc_1: (93.36%) (39556/42368)\n",
      "Epoch: 16 | Batch_idx: 340 |  Loss_1: (0.1875) | Acc_1: (93.34%) (40739/43648)\n",
      "Epoch: 16 | Batch_idx: 350 |  Loss_1: (0.1869) | Acc_1: (93.37%) (41950/44928)\n",
      "Epoch: 16 | Batch_idx: 360 |  Loss_1: (0.1869) | Acc_1: (93.36%) (43141/46208)\n",
      "Epoch: 16 | Batch_idx: 370 |  Loss_1: (0.1870) | Acc_1: (93.35%) (44332/47488)\n",
      "Epoch: 16 | Batch_idx: 380 |  Loss_1: (0.1870) | Acc_1: (93.36%) (45531/48768)\n",
      "Epoch: 16 | Batch_idx: 390 |  Loss_1: (0.1866) | Acc_1: (93.37%) (46686/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3889) | Acc: (88.34%) (8834/10000)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.2065) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.1504) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.1509) | Acc_1: (94.61%) (2543/2688)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.1547) | Acc_1: (94.33%) (3743/3968)\n",
      "Epoch: 17 | Batch_idx: 40 |  Loss_1: (0.1623) | Acc_1: (94.00%) (4933/5248)\n",
      "Epoch: 17 | Batch_idx: 50 |  Loss_1: (0.1600) | Acc_1: (93.93%) (6132/6528)\n",
      "Epoch: 17 | Batch_idx: 60 |  Loss_1: (0.1639) | Acc_1: (93.85%) (7328/7808)\n",
      "Epoch: 17 | Batch_idx: 70 |  Loss_1: (0.1672) | Acc_1: (93.77%) (8522/9088)\n",
      "Epoch: 17 | Batch_idx: 80 |  Loss_1: (0.1667) | Acc_1: (93.75%) (9720/10368)\n",
      "Epoch: 17 | Batch_idx: 90 |  Loss_1: (0.1676) | Acc_1: (93.80%) (10926/11648)\n",
      "Epoch: 17 | Batch_idx: 100 |  Loss_1: (0.1646) | Acc_1: (93.92%) (12142/12928)\n",
      "Epoch: 17 | Batch_idx: 110 |  Loss_1: (0.1658) | Acc_1: (93.95%) (13348/14208)\n",
      "Epoch: 17 | Batch_idx: 120 |  Loss_1: (0.1664) | Acc_1: (93.95%) (14551/15488)\n",
      "Epoch: 17 | Batch_idx: 130 |  Loss_1: (0.1660) | Acc_1: (93.95%) (15754/16768)\n",
      "Epoch: 17 | Batch_idx: 140 |  Loss_1: (0.1641) | Acc_1: (93.99%) (16963/18048)\n",
      "Epoch: 17 | Batch_idx: 150 |  Loss_1: (0.1645) | Acc_1: (94.00%) (18168/19328)\n",
      "Epoch: 17 | Batch_idx: 160 |  Loss_1: (0.1647) | Acc_1: (93.99%) (19369/20608)\n",
      "Epoch: 17 | Batch_idx: 170 |  Loss_1: (0.1641) | Acc_1: (94.00%) (20574/21888)\n",
      "Epoch: 17 | Batch_idx: 180 |  Loss_1: (0.1646) | Acc_1: (93.98%) (21773/23168)\n",
      "Epoch: 17 | Batch_idx: 190 |  Loss_1: (0.1648) | Acc_1: (94.01%) (22984/24448)\n",
      "Epoch: 17 | Batch_idx: 200 |  Loss_1: (0.1650) | Acc_1: (94.00%) (24184/25728)\n",
      "Epoch: 17 | Batch_idx: 210 |  Loss_1: (0.1643) | Acc_1: (94.02%) (25393/27008)\n",
      "Epoch: 17 | Batch_idx: 220 |  Loss_1: (0.1642) | Acc_1: (94.08%) (26612/28288)\n",
      "Epoch: 17 | Batch_idx: 230 |  Loss_1: (0.1628) | Acc_1: (94.12%) (27829/29568)\n",
      "Epoch: 17 | Batch_idx: 240 |  Loss_1: (0.1625) | Acc_1: (94.15%) (29044/30848)\n",
      "Epoch: 17 | Batch_idx: 250 |  Loss_1: (0.1629) | Acc_1: (94.13%) (30243/32128)\n",
      "Epoch: 17 | Batch_idx: 260 |  Loss_1: (0.1637) | Acc_1: (94.13%) (31448/33408)\n",
      "Epoch: 17 | Batch_idx: 270 |  Loss_1: (0.1642) | Acc_1: (94.15%) (32658/34688)\n",
      "Epoch: 17 | Batch_idx: 280 |  Loss_1: (0.1654) | Acc_1: (94.12%) (33852/35968)\n",
      "Epoch: 17 | Batch_idx: 290 |  Loss_1: (0.1663) | Acc_1: (94.11%) (35054/37248)\n",
      "Epoch: 17 | Batch_idx: 300 |  Loss_1: (0.1661) | Acc_1: (94.13%) (36265/38528)\n",
      "Epoch: 17 | Batch_idx: 310 |  Loss_1: (0.1660) | Acc_1: (94.13%) (37470/39808)\n",
      "Epoch: 17 | Batch_idx: 320 |  Loss_1: (0.1662) | Acc_1: (94.10%) (38664/41088)\n",
      "Epoch: 17 | Batch_idx: 330 |  Loss_1: (0.1665) | Acc_1: (94.08%) (39859/42368)\n",
      "Epoch: 17 | Batch_idx: 340 |  Loss_1: (0.1666) | Acc_1: (94.06%) (41054/43648)\n",
      "Epoch: 17 | Batch_idx: 350 |  Loss_1: (0.1667) | Acc_1: (94.07%) (42264/44928)\n",
      "Epoch: 17 | Batch_idx: 360 |  Loss_1: (0.1663) | Acc_1: (94.10%) (43481/46208)\n",
      "Epoch: 17 | Batch_idx: 370 |  Loss_1: (0.1670) | Acc_1: (94.06%) (44669/47488)\n",
      "Epoch: 17 | Batch_idx: 380 |  Loss_1: (0.1680) | Acc_1: (94.03%) (45855/48768)\n",
      "Epoch: 17 | Batch_idx: 390 |  Loss_1: (0.1685) | Acc_1: (94.01%) (47003/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4083) | Acc: (88.45%) (8845/10000)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.1169) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.1492) | Acc_1: (95.17%) (1340/1408)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.1480) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.1436) | Acc_1: (94.98%) (3769/3968)\n",
      "Epoch: 18 | Batch_idx: 40 |  Loss_1: (0.1424) | Acc_1: (94.89%) (4980/5248)\n",
      "Epoch: 18 | Batch_idx: 50 |  Loss_1: (0.1438) | Acc_1: (94.73%) (6184/6528)\n",
      "Epoch: 18 | Batch_idx: 60 |  Loss_1: (0.1432) | Acc_1: (94.81%) (7403/7808)\n",
      "Epoch: 18 | Batch_idx: 70 |  Loss_1: (0.1421) | Acc_1: (94.88%) (8623/9088)\n",
      "Epoch: 18 | Batch_idx: 80 |  Loss_1: (0.1437) | Acc_1: (94.91%) (9840/10368)\n",
      "Epoch: 18 | Batch_idx: 90 |  Loss_1: (0.1430) | Acc_1: (94.92%) (11056/11648)\n",
      "Epoch: 18 | Batch_idx: 100 |  Loss_1: (0.1446) | Acc_1: (94.86%) (12264/12928)\n",
      "Epoch: 18 | Batch_idx: 110 |  Loss_1: (0.1443) | Acc_1: (94.90%) (13483/14208)\n",
      "Epoch: 18 | Batch_idx: 120 |  Loss_1: (0.1461) | Acc_1: (94.81%) (14684/15488)\n",
      "Epoch: 18 | Batch_idx: 130 |  Loss_1: (0.1474) | Acc_1: (94.81%) (15897/16768)\n",
      "Epoch: 18 | Batch_idx: 140 |  Loss_1: (0.1480) | Acc_1: (94.81%) (17111/18048)\n",
      "Epoch: 18 | Batch_idx: 150 |  Loss_1: (0.1476) | Acc_1: (94.83%) (18328/19328)\n",
      "Epoch: 18 | Batch_idx: 160 |  Loss_1: (0.1466) | Acc_1: (94.88%) (19553/20608)\n",
      "Epoch: 18 | Batch_idx: 170 |  Loss_1: (0.1467) | Acc_1: (94.88%) (20768/21888)\n",
      "Epoch: 18 | Batch_idx: 180 |  Loss_1: (0.1484) | Acc_1: (94.84%) (21973/23168)\n",
      "Epoch: 18 | Batch_idx: 190 |  Loss_1: (0.1502) | Acc_1: (94.80%) (23176/24448)\n",
      "Epoch: 18 | Batch_idx: 200 |  Loss_1: (0.1504) | Acc_1: (94.76%) (24381/25728)\n",
      "Epoch: 18 | Batch_idx: 210 |  Loss_1: (0.1501) | Acc_1: (94.77%) (25596/27008)\n",
      "Epoch: 18 | Batch_idx: 220 |  Loss_1: (0.1505) | Acc_1: (94.77%) (26809/28288)\n",
      "Epoch: 18 | Batch_idx: 230 |  Loss_1: (0.1510) | Acc_1: (94.75%) (28015/29568)\n",
      "Epoch: 18 | Batch_idx: 240 |  Loss_1: (0.1525) | Acc_1: (94.70%) (29213/30848)\n",
      "Epoch: 18 | Batch_idx: 250 |  Loss_1: (0.1528) | Acc_1: (94.67%) (30416/32128)\n",
      "Epoch: 18 | Batch_idx: 260 |  Loss_1: (0.1535) | Acc_1: (94.64%) (31619/33408)\n",
      "Epoch: 18 | Batch_idx: 270 |  Loss_1: (0.1530) | Acc_1: (94.65%) (32832/34688)\n",
      "Epoch: 18 | Batch_idx: 280 |  Loss_1: (0.1530) | Acc_1: (94.64%) (34039/35968)\n",
      "Epoch: 18 | Batch_idx: 290 |  Loss_1: (0.1529) | Acc_1: (94.63%) (35246/37248)\n",
      "Epoch: 18 | Batch_idx: 300 |  Loss_1: (0.1538) | Acc_1: (94.61%) (36451/38528)\n",
      "Epoch: 18 | Batch_idx: 310 |  Loss_1: (0.1542) | Acc_1: (94.59%) (37653/39808)\n",
      "Epoch: 18 | Batch_idx: 320 |  Loss_1: (0.1554) | Acc_1: (94.55%) (38847/41088)\n",
      "Epoch: 18 | Batch_idx: 330 |  Loss_1: (0.1561) | Acc_1: (94.52%) (40046/42368)\n",
      "Epoch: 18 | Batch_idx: 340 |  Loss_1: (0.1567) | Acc_1: (94.48%) (41238/43648)\n",
      "Epoch: 18 | Batch_idx: 350 |  Loss_1: (0.1566) | Acc_1: (94.48%) (42447/44928)\n",
      "Epoch: 18 | Batch_idx: 360 |  Loss_1: (0.1566) | Acc_1: (94.49%) (43660/46208)\n",
      "Epoch: 18 | Batch_idx: 370 |  Loss_1: (0.1568) | Acc_1: (94.48%) (44869/47488)\n",
      "Epoch: 18 | Batch_idx: 380 |  Loss_1: (0.1574) | Acc_1: (94.46%) (46065/48768)\n",
      "Epoch: 18 | Batch_idx: 390 |  Loss_1: (0.1574) | Acc_1: (94.47%) (47234/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4049) | Acc: (88.24%) (8824/10000)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.1245) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.1264) | Acc_1: (95.38%) (1343/1408)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.1279) | Acc_1: (95.46%) (2566/2688)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.1214) | Acc_1: (95.74%) (3799/3968)\n",
      "Epoch: 19 | Batch_idx: 40 |  Loss_1: (0.1213) | Acc_1: (95.71%) (5023/5248)\n",
      "Epoch: 19 | Batch_idx: 50 |  Loss_1: (0.1268) | Acc_1: (95.48%) (6233/6528)\n",
      "Epoch: 19 | Batch_idx: 60 |  Loss_1: (0.1280) | Acc_1: (95.27%) (7439/7808)\n",
      "Epoch: 19 | Batch_idx: 70 |  Loss_1: (0.1248) | Acc_1: (95.40%) (8670/9088)\n",
      "Epoch: 19 | Batch_idx: 80 |  Loss_1: (0.1284) | Acc_1: (95.29%) (9880/10368)\n",
      "Epoch: 19 | Batch_idx: 90 |  Loss_1: (0.1308) | Acc_1: (95.18%) (11086/11648)\n",
      "Epoch: 19 | Batch_idx: 100 |  Loss_1: (0.1314) | Acc_1: (95.20%) (12307/12928)\n",
      "Epoch: 19 | Batch_idx: 110 |  Loss_1: (0.1310) | Acc_1: (95.23%) (13530/14208)\n",
      "Epoch: 19 | Batch_idx: 120 |  Loss_1: (0.1321) | Acc_1: (95.18%) (14741/15488)\n",
      "Epoch: 19 | Batch_idx: 130 |  Loss_1: (0.1326) | Acc_1: (95.21%) (15964/16768)\n",
      "Epoch: 19 | Batch_idx: 140 |  Loss_1: (0.1322) | Acc_1: (95.21%) (17184/18048)\n",
      "Epoch: 19 | Batch_idx: 150 |  Loss_1: (0.1341) | Acc_1: (95.17%) (18394/19328)\n",
      "Epoch: 19 | Batch_idx: 160 |  Loss_1: (0.1362) | Acc_1: (95.09%) (19596/20608)\n",
      "Epoch: 19 | Batch_idx: 170 |  Loss_1: (0.1382) | Acc_1: (95.06%) (20806/21888)\n",
      "Epoch: 19 | Batch_idx: 180 |  Loss_1: (0.1378) | Acc_1: (95.10%) (22033/23168)\n",
      "Epoch: 19 | Batch_idx: 190 |  Loss_1: (0.1384) | Acc_1: (95.04%) (23235/24448)\n",
      "Epoch: 19 | Batch_idx: 200 |  Loss_1: (0.1387) | Acc_1: (95.01%) (24445/25728)\n",
      "Epoch: 19 | Batch_idx: 210 |  Loss_1: (0.1392) | Acc_1: (94.98%) (25653/27008)\n",
      "Epoch: 19 | Batch_idx: 220 |  Loss_1: (0.1395) | Acc_1: (94.96%) (26862/28288)\n",
      "Epoch: 19 | Batch_idx: 230 |  Loss_1: (0.1403) | Acc_1: (94.93%) (28068/29568)\n",
      "Epoch: 19 | Batch_idx: 240 |  Loss_1: (0.1411) | Acc_1: (94.89%) (29273/30848)\n",
      "Epoch: 19 | Batch_idx: 250 |  Loss_1: (0.1407) | Acc_1: (94.90%) (30489/32128)\n",
      "Epoch: 19 | Batch_idx: 260 |  Loss_1: (0.1403) | Acc_1: (94.91%) (31709/33408)\n",
      "Epoch: 19 | Batch_idx: 270 |  Loss_1: (0.1407) | Acc_1: (94.89%) (32915/34688)\n",
      "Epoch: 19 | Batch_idx: 280 |  Loss_1: (0.1410) | Acc_1: (94.86%) (34121/35968)\n",
      "Epoch: 19 | Batch_idx: 290 |  Loss_1: (0.1413) | Acc_1: (94.86%) (35332/37248)\n",
      "Epoch: 19 | Batch_idx: 300 |  Loss_1: (0.1408) | Acc_1: (94.90%) (36562/38528)\n",
      "Epoch: 19 | Batch_idx: 310 |  Loss_1: (0.1408) | Acc_1: (94.89%) (37774/39808)\n",
      "Epoch: 19 | Batch_idx: 320 |  Loss_1: (0.1415) | Acc_1: (94.87%) (38979/41088)\n",
      "Epoch: 19 | Batch_idx: 330 |  Loss_1: (0.1418) | Acc_1: (94.87%) (40193/42368)\n",
      "Epoch: 19 | Batch_idx: 340 |  Loss_1: (0.1423) | Acc_1: (94.85%) (41402/43648)\n",
      "Epoch: 19 | Batch_idx: 350 |  Loss_1: (0.1424) | Acc_1: (94.85%) (42613/44928)\n",
      "Epoch: 19 | Batch_idx: 360 |  Loss_1: (0.1426) | Acc_1: (94.85%) (43829/46208)\n",
      "Epoch: 19 | Batch_idx: 370 |  Loss_1: (0.1434) | Acc_1: (94.84%) (45036/47488)\n",
      "Epoch: 19 | Batch_idx: 380 |  Loss_1: (0.1440) | Acc_1: (94.83%) (46248/48768)\n",
      "Epoch: 19 | Batch_idx: 390 |  Loss_1: (0.1434) | Acc_1: (94.86%) (47429/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3706) | Acc: (89.03%) (8903/10000)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.0509) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.1081) | Acc_1: (96.38%) (1357/1408)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.1072) | Acc_1: (96.43%) (2592/2688)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.1104) | Acc_1: (96.30%) (3821/3968)\n",
      "Epoch: 20 | Batch_idx: 40 |  Loss_1: (0.1119) | Acc_1: (96.27%) (5052/5248)\n",
      "Epoch: 20 | Batch_idx: 50 |  Loss_1: (0.1141) | Acc_1: (96.16%) (6277/6528)\n",
      "Epoch: 20 | Batch_idx: 60 |  Loss_1: (0.1174) | Acc_1: (96.03%) (7498/7808)\n",
      "Epoch: 20 | Batch_idx: 70 |  Loss_1: (0.1202) | Acc_1: (95.96%) (8721/9088)\n",
      "Epoch: 20 | Batch_idx: 80 |  Loss_1: (0.1197) | Acc_1: (95.98%) (9951/10368)\n",
      "Epoch: 20 | Batch_idx: 90 |  Loss_1: (0.1192) | Acc_1: (96.02%) (11184/11648)\n",
      "Epoch: 20 | Batch_idx: 100 |  Loss_1: (0.1214) | Acc_1: (95.92%) (12400/12928)\n",
      "Epoch: 20 | Batch_idx: 110 |  Loss_1: (0.1204) | Acc_1: (95.94%) (13631/14208)\n",
      "Epoch: 20 | Batch_idx: 120 |  Loss_1: (0.1200) | Acc_1: (95.88%) (14850/15488)\n",
      "Epoch: 20 | Batch_idx: 130 |  Loss_1: (0.1215) | Acc_1: (95.83%) (16069/16768)\n",
      "Epoch: 20 | Batch_idx: 140 |  Loss_1: (0.1222) | Acc_1: (95.76%) (17282/18048)\n",
      "Epoch: 20 | Batch_idx: 150 |  Loss_1: (0.1231) | Acc_1: (95.73%) (18503/19328)\n",
      "Epoch: 20 | Batch_idx: 160 |  Loss_1: (0.1241) | Acc_1: (95.70%) (19722/20608)\n",
      "Epoch: 20 | Batch_idx: 170 |  Loss_1: (0.1246) | Acc_1: (95.69%) (20945/21888)\n",
      "Epoch: 20 | Batch_idx: 180 |  Loss_1: (0.1246) | Acc_1: (95.71%) (22174/23168)\n",
      "Epoch: 20 | Batch_idx: 190 |  Loss_1: (0.1245) | Acc_1: (95.71%) (23399/24448)\n",
      "Epoch: 20 | Batch_idx: 200 |  Loss_1: (0.1247) | Acc_1: (95.68%) (24616/25728)\n",
      "Epoch: 20 | Batch_idx: 210 |  Loss_1: (0.1254) | Acc_1: (95.64%) (25831/27008)\n",
      "Epoch: 20 | Batch_idx: 220 |  Loss_1: (0.1264) | Acc_1: (95.62%) (27048/28288)\n",
      "Epoch: 20 | Batch_idx: 230 |  Loss_1: (0.1266) | Acc_1: (95.60%) (28266/29568)\n",
      "Epoch: 20 | Batch_idx: 240 |  Loss_1: (0.1268) | Acc_1: (95.58%) (29485/30848)\n",
      "Epoch: 20 | Batch_idx: 250 |  Loss_1: (0.1261) | Acc_1: (95.62%) (30722/32128)\n",
      "Epoch: 20 | Batch_idx: 260 |  Loss_1: (0.1262) | Acc_1: (95.65%) (31956/33408)\n",
      "Epoch: 20 | Batch_idx: 270 |  Loss_1: (0.1272) | Acc_1: (95.61%) (33164/34688)\n",
      "Epoch: 20 | Batch_idx: 280 |  Loss_1: (0.1276) | Acc_1: (95.60%) (34385/35968)\n",
      "Epoch: 20 | Batch_idx: 290 |  Loss_1: (0.1275) | Acc_1: (95.60%) (35609/37248)\n",
      "Epoch: 20 | Batch_idx: 300 |  Loss_1: (0.1278) | Acc_1: (95.57%) (36823/38528)\n",
      "Epoch: 20 | Batch_idx: 310 |  Loss_1: (0.1279) | Acc_1: (95.57%) (38046/39808)\n",
      "Epoch: 20 | Batch_idx: 320 |  Loss_1: (0.1280) | Acc_1: (95.57%) (39268/41088)\n",
      "Epoch: 20 | Batch_idx: 330 |  Loss_1: (0.1283) | Acc_1: (95.58%) (40495/42368)\n",
      "Epoch: 20 | Batch_idx: 340 |  Loss_1: (0.1286) | Acc_1: (95.56%) (41708/43648)\n",
      "Epoch: 20 | Batch_idx: 350 |  Loss_1: (0.1287) | Acc_1: (95.55%) (42928/44928)\n",
      "Epoch: 20 | Batch_idx: 360 |  Loss_1: (0.1292) | Acc_1: (95.52%) (44140/46208)\n",
      "Epoch: 20 | Batch_idx: 370 |  Loss_1: (0.1294) | Acc_1: (95.51%) (45354/47488)\n",
      "Epoch: 20 | Batch_idx: 380 |  Loss_1: (0.1287) | Acc_1: (95.53%) (46587/48768)\n",
      "Epoch: 20 | Batch_idx: 390 |  Loss_1: (0.1292) | Acc_1: (95.50%) (47748/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3784) | Acc: (89.51%) (8951/10000)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.0867) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.1153) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.1026) | Acc_1: (95.87%) (2577/2688)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.1136) | Acc_1: (95.51%) (3790/3968)\n",
      "Epoch: 21 | Batch_idx: 40 |  Loss_1: (0.1137) | Acc_1: (95.41%) (5007/5248)\n",
      "Epoch: 21 | Batch_idx: 50 |  Loss_1: (0.1138) | Acc_1: (95.47%) (6232/6528)\n",
      "Epoch: 21 | Batch_idx: 60 |  Loss_1: (0.1142) | Acc_1: (95.48%) (7455/7808)\n",
      "Epoch: 21 | Batch_idx: 70 |  Loss_1: (0.1133) | Acc_1: (95.55%) (8684/9088)\n",
      "Epoch: 21 | Batch_idx: 80 |  Loss_1: (0.1137) | Acc_1: (95.51%) (9902/10368)\n",
      "Epoch: 21 | Batch_idx: 90 |  Loss_1: (0.1122) | Acc_1: (95.60%) (11136/11648)\n",
      "Epoch: 21 | Batch_idx: 100 |  Loss_1: (0.1118) | Acc_1: (95.68%) (12370/12928)\n",
      "Epoch: 21 | Batch_idx: 110 |  Loss_1: (0.1134) | Acc_1: (95.62%) (13586/14208)\n",
      "Epoch: 21 | Batch_idx: 120 |  Loss_1: (0.1130) | Acc_1: (95.64%) (14812/15488)\n",
      "Epoch: 21 | Batch_idx: 130 |  Loss_1: (0.1157) | Acc_1: (95.57%) (16025/16768)\n",
      "Epoch: 21 | Batch_idx: 140 |  Loss_1: (0.1157) | Acc_1: (95.57%) (17249/18048)\n",
      "Epoch: 21 | Batch_idx: 150 |  Loss_1: (0.1147) | Acc_1: (95.63%) (18484/19328)\n",
      "Epoch: 21 | Batch_idx: 160 |  Loss_1: (0.1152) | Acc_1: (95.64%) (19709/20608)\n",
      "Epoch: 21 | Batch_idx: 170 |  Loss_1: (0.1153) | Acc_1: (95.66%) (20938/21888)\n",
      "Epoch: 21 | Batch_idx: 180 |  Loss_1: (0.1149) | Acc_1: (95.67%) (22164/23168)\n",
      "Epoch: 21 | Batch_idx: 190 |  Loss_1: (0.1142) | Acc_1: (95.72%) (23402/24448)\n",
      "Epoch: 21 | Batch_idx: 200 |  Loss_1: (0.1145) | Acc_1: (95.72%) (24628/25728)\n",
      "Epoch: 21 | Batch_idx: 210 |  Loss_1: (0.1145) | Acc_1: (95.75%) (25861/27008)\n",
      "Epoch: 21 | Batch_idx: 220 |  Loss_1: (0.1137) | Acc_1: (95.80%) (27099/28288)\n",
      "Epoch: 21 | Batch_idx: 230 |  Loss_1: (0.1137) | Acc_1: (95.80%) (28325/29568)\n",
      "Epoch: 21 | Batch_idx: 240 |  Loss_1: (0.1135) | Acc_1: (95.81%) (29554/30848)\n",
      "Epoch: 21 | Batch_idx: 250 |  Loss_1: (0.1132) | Acc_1: (95.82%) (30785/32128)\n",
      "Epoch: 21 | Batch_idx: 260 |  Loss_1: (0.1135) | Acc_1: (95.81%) (32007/33408)\n",
      "Epoch: 21 | Batch_idx: 270 |  Loss_1: (0.1133) | Acc_1: (95.83%) (33240/34688)\n",
      "Epoch: 21 | Batch_idx: 280 |  Loss_1: (0.1138) | Acc_1: (95.84%) (34471/35968)\n",
      "Epoch: 21 | Batch_idx: 290 |  Loss_1: (0.1152) | Acc_1: (95.79%) (35681/37248)\n",
      "Epoch: 21 | Batch_idx: 300 |  Loss_1: (0.1163) | Acc_1: (95.80%) (36908/38528)\n",
      "Epoch: 21 | Batch_idx: 310 |  Loss_1: (0.1171) | Acc_1: (95.77%) (38123/39808)\n",
      "Epoch: 21 | Batch_idx: 320 |  Loss_1: (0.1177) | Acc_1: (95.76%) (39344/41088)\n",
      "Epoch: 21 | Batch_idx: 330 |  Loss_1: (0.1181) | Acc_1: (95.75%) (40569/42368)\n",
      "Epoch: 21 | Batch_idx: 340 |  Loss_1: (0.1182) | Acc_1: (95.76%) (41796/43648)\n",
      "Epoch: 21 | Batch_idx: 350 |  Loss_1: (0.1180) | Acc_1: (95.77%) (43028/44928)\n",
      "Epoch: 21 | Batch_idx: 360 |  Loss_1: (0.1190) | Acc_1: (95.73%) (44233/46208)\n",
      "Epoch: 21 | Batch_idx: 370 |  Loss_1: (0.1198) | Acc_1: (95.70%) (45444/47488)\n",
      "Epoch: 21 | Batch_idx: 380 |  Loss_1: (0.1200) | Acc_1: (95.68%) (46663/48768)\n",
      "Epoch: 21 | Batch_idx: 390 |  Loss_1: (0.1197) | Acc_1: (95.69%) (47844/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3800) | Acc: (89.73%) (8973/10000)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.0930) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.0840) | Acc_1: (97.09%) (1367/1408)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.0864) | Acc_1: (97.02%) (2608/2688)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.0898) | Acc_1: (97.10%) (3853/3968)\n",
      "Epoch: 22 | Batch_idx: 40 |  Loss_1: (0.0970) | Acc_1: (96.72%) (5076/5248)\n",
      "Epoch: 22 | Batch_idx: 50 |  Loss_1: (0.1019) | Acc_1: (96.69%) (6312/6528)\n",
      "Epoch: 22 | Batch_idx: 60 |  Loss_1: (0.1049) | Acc_1: (96.55%) (7539/7808)\n",
      "Epoch: 22 | Batch_idx: 70 |  Loss_1: (0.1037) | Acc_1: (96.57%) (8776/9088)\n",
      "Epoch: 22 | Batch_idx: 80 |  Loss_1: (0.1040) | Acc_1: (96.58%) (10013/10368)\n",
      "Epoch: 22 | Batch_idx: 90 |  Loss_1: (0.1035) | Acc_1: (96.54%) (11245/11648)\n",
      "Epoch: 22 | Batch_idx: 100 |  Loss_1: (0.1040) | Acc_1: (96.48%) (12473/12928)\n",
      "Epoch: 22 | Batch_idx: 110 |  Loss_1: (0.1058) | Acc_1: (96.39%) (13695/14208)\n",
      "Epoch: 22 | Batch_idx: 120 |  Loss_1: (0.1075) | Acc_1: (96.30%) (14915/15488)\n",
      "Epoch: 22 | Batch_idx: 130 |  Loss_1: (0.1071) | Acc_1: (96.28%) (16145/16768)\n",
      "Epoch: 22 | Batch_idx: 140 |  Loss_1: (0.1077) | Acc_1: (96.28%) (17377/18048)\n",
      "Epoch: 22 | Batch_idx: 150 |  Loss_1: (0.1093) | Acc_1: (96.23%) (18600/19328)\n",
      "Epoch: 22 | Batch_idx: 160 |  Loss_1: (0.1107) | Acc_1: (96.21%) (19826/20608)\n",
      "Epoch: 22 | Batch_idx: 170 |  Loss_1: (0.1115) | Acc_1: (96.19%) (21054/21888)\n",
      "Epoch: 22 | Batch_idx: 180 |  Loss_1: (0.1117) | Acc_1: (96.17%) (22280/23168)\n",
      "Epoch: 22 | Batch_idx: 190 |  Loss_1: (0.1118) | Acc_1: (96.18%) (23515/24448)\n",
      "Epoch: 22 | Batch_idx: 200 |  Loss_1: (0.1116) | Acc_1: (96.19%) (24747/25728)\n",
      "Epoch: 22 | Batch_idx: 210 |  Loss_1: (0.1108) | Acc_1: (96.18%) (25976/27008)\n",
      "Epoch: 22 | Batch_idx: 220 |  Loss_1: (0.1104) | Acc_1: (96.19%) (27210/28288)\n",
      "Epoch: 22 | Batch_idx: 230 |  Loss_1: (0.1107) | Acc_1: (96.17%) (28437/29568)\n",
      "Epoch: 22 | Batch_idx: 240 |  Loss_1: (0.1114) | Acc_1: (96.17%) (29665/30848)\n",
      "Epoch: 22 | Batch_idx: 250 |  Loss_1: (0.1119) | Acc_1: (96.17%) (30896/32128)\n",
      "Epoch: 22 | Batch_idx: 260 |  Loss_1: (0.1127) | Acc_1: (96.14%) (32119/33408)\n",
      "Epoch: 22 | Batch_idx: 270 |  Loss_1: (0.1129) | Acc_1: (96.15%) (33352/34688)\n",
      "Epoch: 22 | Batch_idx: 280 |  Loss_1: (0.1124) | Acc_1: (96.15%) (34585/35968)\n",
      "Epoch: 22 | Batch_idx: 290 |  Loss_1: (0.1120) | Acc_1: (96.19%) (35828/37248)\n",
      "Epoch: 22 | Batch_idx: 300 |  Loss_1: (0.1115) | Acc_1: (96.22%) (37071/38528)\n",
      "Epoch: 22 | Batch_idx: 310 |  Loss_1: (0.1106) | Acc_1: (96.25%) (38314/39808)\n",
      "Epoch: 22 | Batch_idx: 320 |  Loss_1: (0.1106) | Acc_1: (96.24%) (39544/41088)\n",
      "Epoch: 22 | Batch_idx: 330 |  Loss_1: (0.1107) | Acc_1: (96.23%) (40769/42368)\n",
      "Epoch: 22 | Batch_idx: 340 |  Loss_1: (0.1106) | Acc_1: (96.24%) (42006/43648)\n",
      "Epoch: 22 | Batch_idx: 350 |  Loss_1: (0.1100) | Acc_1: (96.25%) (43245/44928)\n",
      "Epoch: 22 | Batch_idx: 360 |  Loss_1: (0.1095) | Acc_1: (96.27%) (44483/46208)\n",
      "Epoch: 22 | Batch_idx: 370 |  Loss_1: (0.1094) | Acc_1: (96.27%) (45716/47488)\n",
      "Epoch: 22 | Batch_idx: 380 |  Loss_1: (0.1093) | Acc_1: (96.27%) (46947/48768)\n",
      "Epoch: 22 | Batch_idx: 390 |  Loss_1: (0.1094) | Acc_1: (96.25%) (48127/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3722) | Acc: (90.24%) (9024/10000)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.1228) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.0833) | Acc_1: (97.44%) (1372/1408)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.0870) | Acc_1: (97.10%) (2610/2688)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.0859) | Acc_1: (97.08%) (3852/3968)\n",
      "Epoch: 23 | Batch_idx: 40 |  Loss_1: (0.0893) | Acc_1: (96.99%) (5090/5248)\n",
      "Epoch: 23 | Batch_idx: 50 |  Loss_1: (0.0925) | Acc_1: (96.78%) (6318/6528)\n",
      "Epoch: 23 | Batch_idx: 60 |  Loss_1: (0.0949) | Acc_1: (96.72%) (7552/7808)\n",
      "Epoch: 23 | Batch_idx: 70 |  Loss_1: (0.0923) | Acc_1: (96.85%) (8802/9088)\n",
      "Epoch: 23 | Batch_idx: 80 |  Loss_1: (0.0920) | Acc_1: (96.80%) (10036/10368)\n",
      "Epoch: 23 | Batch_idx: 90 |  Loss_1: (0.0923) | Acc_1: (96.81%) (11276/11648)\n",
      "Epoch: 23 | Batch_idx: 100 |  Loss_1: (0.0923) | Acc_1: (96.78%) (12512/12928)\n",
      "Epoch: 23 | Batch_idx: 110 |  Loss_1: (0.0951) | Acc_1: (96.70%) (13739/14208)\n",
      "Epoch: 23 | Batch_idx: 120 |  Loss_1: (0.0944) | Acc_1: (96.73%) (14981/15488)\n",
      "Epoch: 23 | Batch_idx: 130 |  Loss_1: (0.0952) | Acc_1: (96.73%) (16219/16768)\n",
      "Epoch: 23 | Batch_idx: 140 |  Loss_1: (0.0959) | Acc_1: (96.69%) (17450/18048)\n",
      "Epoch: 23 | Batch_idx: 150 |  Loss_1: (0.0968) | Acc_1: (96.65%) (18681/19328)\n",
      "Epoch: 23 | Batch_idx: 160 |  Loss_1: (0.0966) | Acc_1: (96.65%) (19917/20608)\n",
      "Epoch: 23 | Batch_idx: 170 |  Loss_1: (0.0979) | Acc_1: (96.57%) (21138/21888)\n",
      "Epoch: 23 | Batch_idx: 180 |  Loss_1: (0.0969) | Acc_1: (96.58%) (22376/23168)\n",
      "Epoch: 23 | Batch_idx: 190 |  Loss_1: (0.0970) | Acc_1: (96.58%) (23612/24448)\n",
      "Epoch: 23 | Batch_idx: 200 |  Loss_1: (0.0966) | Acc_1: (96.60%) (24854/25728)\n",
      "Epoch: 23 | Batch_idx: 210 |  Loss_1: (0.0962) | Acc_1: (96.62%) (26094/27008)\n",
      "Epoch: 23 | Batch_idx: 220 |  Loss_1: (0.0963) | Acc_1: (96.60%) (27326/28288)\n",
      "Epoch: 23 | Batch_idx: 230 |  Loss_1: (0.0969) | Acc_1: (96.59%) (28560/29568)\n",
      "Epoch: 23 | Batch_idx: 240 |  Loss_1: (0.0975) | Acc_1: (96.55%) (29785/30848)\n",
      "Epoch: 23 | Batch_idx: 250 |  Loss_1: (0.0977) | Acc_1: (96.55%) (31019/32128)\n",
      "Epoch: 23 | Batch_idx: 260 |  Loss_1: (0.0974) | Acc_1: (96.54%) (32253/33408)\n",
      "Epoch: 23 | Batch_idx: 270 |  Loss_1: (0.0977) | Acc_1: (96.53%) (33485/34688)\n",
      "Epoch: 23 | Batch_idx: 280 |  Loss_1: (0.0977) | Acc_1: (96.52%) (34717/35968)\n",
      "Epoch: 23 | Batch_idx: 290 |  Loss_1: (0.0976) | Acc_1: (96.53%) (35955/37248)\n",
      "Epoch: 23 | Batch_idx: 300 |  Loss_1: (0.0975) | Acc_1: (96.52%) (37188/38528)\n",
      "Epoch: 23 | Batch_idx: 310 |  Loss_1: (0.0970) | Acc_1: (96.55%) (38433/39808)\n",
      "Epoch: 23 | Batch_idx: 320 |  Loss_1: (0.0967) | Acc_1: (96.55%) (39671/41088)\n",
      "Epoch: 23 | Batch_idx: 330 |  Loss_1: (0.0968) | Acc_1: (96.55%) (40905/42368)\n",
      "Epoch: 23 | Batch_idx: 340 |  Loss_1: (0.0965) | Acc_1: (96.56%) (42147/43648)\n",
      "Epoch: 23 | Batch_idx: 350 |  Loss_1: (0.0960) | Acc_1: (96.59%) (43396/44928)\n",
      "Epoch: 23 | Batch_idx: 360 |  Loss_1: (0.0962) | Acc_1: (96.58%) (44627/46208)\n",
      "Epoch: 23 | Batch_idx: 370 |  Loss_1: (0.0959) | Acc_1: (96.58%) (45865/47488)\n",
      "Epoch: 23 | Batch_idx: 380 |  Loss_1: (0.0962) | Acc_1: (96.57%) (47095/48768)\n",
      "Epoch: 23 | Batch_idx: 390 |  Loss_1: (0.0968) | Acc_1: (96.55%) (48274/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3745) | Acc: (90.09%) (9009/10000)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.0757) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.0830) | Acc_1: (96.95%) (1365/1408)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.0864) | Acc_1: (96.76%) (2601/2688)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.0873) | Acc_1: (96.75%) (3839/3968)\n",
      "Epoch: 24 | Batch_idx: 40 |  Loss_1: (0.0873) | Acc_1: (96.84%) (5082/5248)\n",
      "Epoch: 24 | Batch_idx: 50 |  Loss_1: (0.0852) | Acc_1: (96.95%) (6329/6528)\n",
      "Epoch: 24 | Batch_idx: 60 |  Loss_1: (0.0848) | Acc_1: (96.93%) (7568/7808)\n",
      "Epoch: 24 | Batch_idx: 70 |  Loss_1: (0.0856) | Acc_1: (96.92%) (8808/9088)\n",
      "Epoch: 24 | Batch_idx: 80 |  Loss_1: (0.0842) | Acc_1: (96.95%) (10052/10368)\n",
      "Epoch: 24 | Batch_idx: 90 |  Loss_1: (0.0854) | Acc_1: (96.91%) (11288/11648)\n",
      "Epoch: 24 | Batch_idx: 100 |  Loss_1: (0.0865) | Acc_1: (96.91%) (12528/12928)\n",
      "Epoch: 24 | Batch_idx: 110 |  Loss_1: (0.0871) | Acc_1: (96.88%) (13765/14208)\n",
      "Epoch: 24 | Batch_idx: 120 |  Loss_1: (0.0877) | Acc_1: (96.86%) (15002/15488)\n",
      "Epoch: 24 | Batch_idx: 130 |  Loss_1: (0.0877) | Acc_1: (96.86%) (16242/16768)\n",
      "Epoch: 24 | Batch_idx: 140 |  Loss_1: (0.0885) | Acc_1: (96.82%) (17474/18048)\n",
      "Epoch: 24 | Batch_idx: 150 |  Loss_1: (0.0885) | Acc_1: (96.83%) (18715/19328)\n",
      "Epoch: 24 | Batch_idx: 160 |  Loss_1: (0.0887) | Acc_1: (96.82%) (19953/20608)\n",
      "Epoch: 24 | Batch_idx: 170 |  Loss_1: (0.0882) | Acc_1: (96.85%) (21199/21888)\n",
      "Epoch: 24 | Batch_idx: 180 |  Loss_1: (0.0886) | Acc_1: (96.85%) (22438/23168)\n",
      "Epoch: 24 | Batch_idx: 190 |  Loss_1: (0.0895) | Acc_1: (96.81%) (23667/24448)\n",
      "Epoch: 24 | Batch_idx: 200 |  Loss_1: (0.0892) | Acc_1: (96.82%) (24910/25728)\n",
      "Epoch: 24 | Batch_idx: 210 |  Loss_1: (0.0884) | Acc_1: (96.86%) (26161/27008)\n",
      "Epoch: 24 | Batch_idx: 220 |  Loss_1: (0.0876) | Acc_1: (96.88%) (27406/28288)\n",
      "Epoch: 24 | Batch_idx: 230 |  Loss_1: (0.0875) | Acc_1: (96.88%) (28646/29568)\n",
      "Epoch: 24 | Batch_idx: 240 |  Loss_1: (0.0880) | Acc_1: (96.88%) (29884/30848)\n",
      "Epoch: 24 | Batch_idx: 250 |  Loss_1: (0.0882) | Acc_1: (96.86%) (31120/32128)\n",
      "Epoch: 24 | Batch_idx: 260 |  Loss_1: (0.0880) | Acc_1: (96.88%) (32366/33408)\n",
      "Epoch: 24 | Batch_idx: 270 |  Loss_1: (0.0882) | Acc_1: (96.89%) (33609/34688)\n",
      "Epoch: 24 | Batch_idx: 280 |  Loss_1: (0.0887) | Acc_1: (96.87%) (34841/35968)\n",
      "Epoch: 24 | Batch_idx: 290 |  Loss_1: (0.0892) | Acc_1: (96.85%) (36075/37248)\n",
      "Epoch: 24 | Batch_idx: 300 |  Loss_1: (0.0892) | Acc_1: (96.88%) (37324/38528)\n",
      "Epoch: 24 | Batch_idx: 310 |  Loss_1: (0.0890) | Acc_1: (96.88%) (38565/39808)\n",
      "Epoch: 24 | Batch_idx: 320 |  Loss_1: (0.0893) | Acc_1: (96.86%) (39799/41088)\n",
      "Epoch: 24 | Batch_idx: 330 |  Loss_1: (0.0892) | Acc_1: (96.88%) (41044/42368)\n",
      "Epoch: 24 | Batch_idx: 340 |  Loss_1: (0.0894) | Acc_1: (96.88%) (42286/43648)\n",
      "Epoch: 24 | Batch_idx: 350 |  Loss_1: (0.0895) | Acc_1: (96.89%) (43530/44928)\n",
      "Epoch: 24 | Batch_idx: 360 |  Loss_1: (0.0890) | Acc_1: (96.92%) (44783/46208)\n",
      "Epoch: 24 | Batch_idx: 370 |  Loss_1: (0.0891) | Acc_1: (96.93%) (46028/47488)\n",
      "Epoch: 24 | Batch_idx: 380 |  Loss_1: (0.0890) | Acc_1: (96.93%) (47273/48768)\n",
      "Epoch: 24 | Batch_idx: 390 |  Loss_1: (0.0892) | Acc_1: (96.93%) (48465/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4301) | Acc: (89.42%) (8942/10000)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.0641) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.0806) | Acc_1: (97.16%) (1368/1408)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.0844) | Acc_1: (97.02%) (2608/2688)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.0761) | Acc_1: (97.45%) (3867/3968)\n",
      "Epoch: 25 | Batch_idx: 40 |  Loss_1: (0.0735) | Acc_1: (97.54%) (5119/5248)\n",
      "Epoch: 25 | Batch_idx: 50 |  Loss_1: (0.0678) | Acc_1: (97.70%) (6378/6528)\n",
      "Epoch: 25 | Batch_idx: 60 |  Loss_1: (0.0671) | Acc_1: (97.67%) (7626/7808)\n",
      "Epoch: 25 | Batch_idx: 70 |  Loss_1: (0.0691) | Acc_1: (97.55%) (8865/9088)\n",
      "Epoch: 25 | Batch_idx: 80 |  Loss_1: (0.0701) | Acc_1: (97.50%) (10109/10368)\n",
      "Epoch: 25 | Batch_idx: 90 |  Loss_1: (0.0725) | Acc_1: (97.42%) (11348/11648)\n",
      "Epoch: 25 | Batch_idx: 100 |  Loss_1: (0.0737) | Acc_1: (97.39%) (12590/12928)\n",
      "Epoch: 25 | Batch_idx: 110 |  Loss_1: (0.0734) | Acc_1: (97.39%) (13837/14208)\n",
      "Epoch: 25 | Batch_idx: 120 |  Loss_1: (0.0767) | Acc_1: (97.30%) (15070/15488)\n",
      "Epoch: 25 | Batch_idx: 130 |  Loss_1: (0.0767) | Acc_1: (97.29%) (16313/16768)\n",
      "Epoch: 25 | Batch_idx: 140 |  Loss_1: (0.0769) | Acc_1: (97.27%) (17556/18048)\n",
      "Epoch: 25 | Batch_idx: 150 |  Loss_1: (0.0761) | Acc_1: (97.31%) (18808/19328)\n",
      "Epoch: 25 | Batch_idx: 160 |  Loss_1: (0.0757) | Acc_1: (97.31%) (20053/20608)\n",
      "Epoch: 25 | Batch_idx: 170 |  Loss_1: (0.0753) | Acc_1: (97.31%) (21300/21888)\n",
      "Epoch: 25 | Batch_idx: 180 |  Loss_1: (0.0764) | Acc_1: (97.28%) (22537/23168)\n",
      "Epoch: 25 | Batch_idx: 190 |  Loss_1: (0.0762) | Acc_1: (97.29%) (23785/24448)\n",
      "Epoch: 25 | Batch_idx: 200 |  Loss_1: (0.0761) | Acc_1: (97.29%) (25030/25728)\n",
      "Epoch: 25 | Batch_idx: 210 |  Loss_1: (0.0759) | Acc_1: (97.30%) (26280/27008)\n",
      "Epoch: 25 | Batch_idx: 220 |  Loss_1: (0.0752) | Acc_1: (97.33%) (27534/28288)\n",
      "Epoch: 25 | Batch_idx: 230 |  Loss_1: (0.0749) | Acc_1: (97.35%) (28784/29568)\n",
      "Epoch: 25 | Batch_idx: 240 |  Loss_1: (0.0759) | Acc_1: (97.32%) (30020/30848)\n",
      "Epoch: 25 | Batch_idx: 250 |  Loss_1: (0.0770) | Acc_1: (97.29%) (31258/32128)\n",
      "Epoch: 25 | Batch_idx: 260 |  Loss_1: (0.0776) | Acc_1: (97.29%) (32501/33408)\n",
      "Epoch: 25 | Batch_idx: 270 |  Loss_1: (0.0774) | Acc_1: (97.28%) (33745/34688)\n",
      "Epoch: 25 | Batch_idx: 280 |  Loss_1: (0.0778) | Acc_1: (97.26%) (34983/35968)\n",
      "Epoch: 25 | Batch_idx: 290 |  Loss_1: (0.0785) | Acc_1: (97.24%) (36220/37248)\n",
      "Epoch: 25 | Batch_idx: 300 |  Loss_1: (0.0787) | Acc_1: (97.25%) (37469/38528)\n",
      "Epoch: 25 | Batch_idx: 310 |  Loss_1: (0.0787) | Acc_1: (97.25%) (38713/39808)\n",
      "Epoch: 25 | Batch_idx: 320 |  Loss_1: (0.0786) | Acc_1: (97.25%) (39959/41088)\n",
      "Epoch: 25 | Batch_idx: 330 |  Loss_1: (0.0782) | Acc_1: (97.26%) (41208/42368)\n",
      "Epoch: 25 | Batch_idx: 340 |  Loss_1: (0.0778) | Acc_1: (97.28%) (42460/43648)\n",
      "Epoch: 25 | Batch_idx: 350 |  Loss_1: (0.0778) | Acc_1: (97.28%) (43706/44928)\n",
      "Epoch: 25 | Batch_idx: 360 |  Loss_1: (0.0777) | Acc_1: (97.29%) (44954/46208)\n",
      "Epoch: 25 | Batch_idx: 370 |  Loss_1: (0.0776) | Acc_1: (97.28%) (46198/47488)\n",
      "Epoch: 25 | Batch_idx: 380 |  Loss_1: (0.0777) | Acc_1: (97.27%) (47437/48768)\n",
      "Epoch: 25 | Batch_idx: 390 |  Loss_1: (0.0784) | Acc_1: (97.26%) (48628/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4052) | Acc: (89.98%) (8998/10000)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.0522) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.0649) | Acc_1: (97.73%) (1376/1408)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.0690) | Acc_1: (97.62%) (2624/2688)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.0710) | Acc_1: (97.53%) (3870/3968)\n",
      "Epoch: 26 | Batch_idx: 40 |  Loss_1: (0.0667) | Acc_1: (97.71%) (5128/5248)\n",
      "Epoch: 26 | Batch_idx: 50 |  Loss_1: (0.0641) | Acc_1: (97.70%) (6378/6528)\n",
      "Epoch: 26 | Batch_idx: 60 |  Loss_1: (0.0666) | Acc_1: (97.57%) (7618/7808)\n",
      "Epoch: 26 | Batch_idx: 70 |  Loss_1: (0.0668) | Acc_1: (97.52%) (8863/9088)\n",
      "Epoch: 26 | Batch_idx: 80 |  Loss_1: (0.0656) | Acc_1: (97.57%) (10116/10368)\n",
      "Epoch: 26 | Batch_idx: 90 |  Loss_1: (0.0690) | Acc_1: (97.42%) (11347/11648)\n",
      "Epoch: 26 | Batch_idx: 100 |  Loss_1: (0.0678) | Acc_1: (97.45%) (12598/12928)\n",
      "Epoch: 26 | Batch_idx: 110 |  Loss_1: (0.0676) | Acc_1: (97.49%) (13851/14208)\n",
      "Epoch: 26 | Batch_idx: 120 |  Loss_1: (0.0673) | Acc_1: (97.53%) (15105/15488)\n",
      "Epoch: 26 | Batch_idx: 130 |  Loss_1: (0.0672) | Acc_1: (97.51%) (16350/16768)\n",
      "Epoch: 26 | Batch_idx: 140 |  Loss_1: (0.0662) | Acc_1: (97.53%) (17602/18048)\n",
      "Epoch: 26 | Batch_idx: 150 |  Loss_1: (0.0670) | Acc_1: (97.51%) (18847/19328)\n",
      "Epoch: 26 | Batch_idx: 160 |  Loss_1: (0.0676) | Acc_1: (97.50%) (20092/20608)\n",
      "Epoch: 26 | Batch_idx: 170 |  Loss_1: (0.0690) | Acc_1: (97.46%) (21333/21888)\n",
      "Epoch: 26 | Batch_idx: 180 |  Loss_1: (0.0697) | Acc_1: (97.44%) (22576/23168)\n",
      "Epoch: 26 | Batch_idx: 190 |  Loss_1: (0.0694) | Acc_1: (97.46%) (23828/24448)\n",
      "Epoch: 26 | Batch_idx: 200 |  Loss_1: (0.0701) | Acc_1: (97.45%) (25072/25728)\n",
      "Epoch: 26 | Batch_idx: 210 |  Loss_1: (0.0711) | Acc_1: (97.42%) (26312/27008)\n",
      "Epoch: 26 | Batch_idx: 220 |  Loss_1: (0.0708) | Acc_1: (97.44%) (27565/28288)\n",
      "Epoch: 26 | Batch_idx: 230 |  Loss_1: (0.0724) | Acc_1: (97.44%) (28810/29568)\n",
      "Epoch: 26 | Batch_idx: 240 |  Loss_1: (0.0729) | Acc_1: (97.42%) (30052/30848)\n",
      "Epoch: 26 | Batch_idx: 250 |  Loss_1: (0.0734) | Acc_1: (97.40%) (31292/32128)\n",
      "Epoch: 26 | Batch_idx: 260 |  Loss_1: (0.0738) | Acc_1: (97.39%) (32535/33408)\n",
      "Epoch: 26 | Batch_idx: 270 |  Loss_1: (0.0736) | Acc_1: (97.39%) (33783/34688)\n",
      "Epoch: 26 | Batch_idx: 280 |  Loss_1: (0.0737) | Acc_1: (97.40%) (35033/35968)\n",
      "Epoch: 26 | Batch_idx: 290 |  Loss_1: (0.0737) | Acc_1: (97.39%) (36277/37248)\n",
      "Epoch: 26 | Batch_idx: 300 |  Loss_1: (0.0738) | Acc_1: (97.41%) (37532/38528)\n",
      "Epoch: 26 | Batch_idx: 310 |  Loss_1: (0.0739) | Acc_1: (97.42%) (38780/39808)\n",
      "Epoch: 26 | Batch_idx: 320 |  Loss_1: (0.0739) | Acc_1: (97.42%) (40029/41088)\n",
      "Epoch: 26 | Batch_idx: 330 |  Loss_1: (0.0741) | Acc_1: (97.40%) (41266/42368)\n",
      "Epoch: 26 | Batch_idx: 340 |  Loss_1: (0.0743) | Acc_1: (97.38%) (42506/43648)\n",
      "Epoch: 26 | Batch_idx: 350 |  Loss_1: (0.0750) | Acc_1: (97.36%) (43741/44928)\n",
      "Epoch: 26 | Batch_idx: 360 |  Loss_1: (0.0749) | Acc_1: (97.37%) (44992/46208)\n",
      "Epoch: 26 | Batch_idx: 370 |  Loss_1: (0.0756) | Acc_1: (97.35%) (46230/47488)\n",
      "Epoch: 26 | Batch_idx: 380 |  Loss_1: (0.0751) | Acc_1: (97.37%) (47486/48768)\n",
      "Epoch: 26 | Batch_idx: 390 |  Loss_1: (0.0755) | Acc_1: (97.36%) (48682/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4026) | Acc: (90.12%) (9012/10000)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.0910) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.0527) | Acc_1: (98.22%) (1383/1408)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.0544) | Acc_1: (98.10%) (2637/2688)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.0644) | Acc_1: (97.78%) (3880/3968)\n",
      "Epoch: 27 | Batch_idx: 40 |  Loss_1: (0.0690) | Acc_1: (97.64%) (5124/5248)\n",
      "Epoch: 27 | Batch_idx: 50 |  Loss_1: (0.0655) | Acc_1: (97.79%) (6384/6528)\n",
      "Epoch: 27 | Batch_idx: 60 |  Loss_1: (0.0681) | Acc_1: (97.69%) (7628/7808)\n",
      "Epoch: 27 | Batch_idx: 70 |  Loss_1: (0.0678) | Acc_1: (97.73%) (8882/9088)\n",
      "Epoch: 27 | Batch_idx: 80 |  Loss_1: (0.0671) | Acc_1: (97.78%) (10138/10368)\n",
      "Epoch: 27 | Batch_idx: 90 |  Loss_1: (0.0656) | Acc_1: (97.78%) (11389/11648)\n",
      "Epoch: 27 | Batch_idx: 100 |  Loss_1: (0.0645) | Acc_1: (97.82%) (12646/12928)\n",
      "Epoch: 27 | Batch_idx: 110 |  Loss_1: (0.0642) | Acc_1: (97.81%) (13897/14208)\n",
      "Epoch: 27 | Batch_idx: 120 |  Loss_1: (0.0647) | Acc_1: (97.81%) (15149/15488)\n",
      "Epoch: 27 | Batch_idx: 130 |  Loss_1: (0.0637) | Acc_1: (97.83%) (16404/16768)\n",
      "Epoch: 27 | Batch_idx: 140 |  Loss_1: (0.0645) | Acc_1: (97.79%) (17650/18048)\n",
      "Epoch: 27 | Batch_idx: 150 |  Loss_1: (0.0644) | Acc_1: (97.78%) (18898/19328)\n",
      "Epoch: 27 | Batch_idx: 160 |  Loss_1: (0.0646) | Acc_1: (97.77%) (20148/20608)\n",
      "Epoch: 27 | Batch_idx: 170 |  Loss_1: (0.0642) | Acc_1: (97.77%) (21400/21888)\n",
      "Epoch: 27 | Batch_idx: 180 |  Loss_1: (0.0647) | Acc_1: (97.76%) (22648/23168)\n",
      "Epoch: 27 | Batch_idx: 190 |  Loss_1: (0.0646) | Acc_1: (97.75%) (23897/24448)\n",
      "Epoch: 27 | Batch_idx: 200 |  Loss_1: (0.0647) | Acc_1: (97.75%) (25148/25728)\n",
      "Epoch: 27 | Batch_idx: 210 |  Loss_1: (0.0652) | Acc_1: (97.73%) (26394/27008)\n",
      "Epoch: 27 | Batch_idx: 220 |  Loss_1: (0.0659) | Acc_1: (97.72%) (27644/28288)\n",
      "Epoch: 27 | Batch_idx: 230 |  Loss_1: (0.0661) | Acc_1: (97.73%) (28896/29568)\n",
      "Epoch: 27 | Batch_idx: 240 |  Loss_1: (0.0661) | Acc_1: (97.71%) (30142/30848)\n",
      "Epoch: 27 | Batch_idx: 250 |  Loss_1: (0.0662) | Acc_1: (97.70%) (31389/32128)\n",
      "Epoch: 27 | Batch_idx: 260 |  Loss_1: (0.0662) | Acc_1: (97.70%) (32641/33408)\n",
      "Epoch: 27 | Batch_idx: 270 |  Loss_1: (0.0666) | Acc_1: (97.69%) (33886/34688)\n",
      "Epoch: 27 | Batch_idx: 280 |  Loss_1: (0.0667) | Acc_1: (97.69%) (35137/35968)\n",
      "Epoch: 27 | Batch_idx: 290 |  Loss_1: (0.0661) | Acc_1: (97.72%) (36397/37248)\n",
      "Epoch: 27 | Batch_idx: 300 |  Loss_1: (0.0662) | Acc_1: (97.71%) (37646/38528)\n",
      "Epoch: 27 | Batch_idx: 310 |  Loss_1: (0.0667) | Acc_1: (97.70%) (38893/39808)\n",
      "Epoch: 27 | Batch_idx: 320 |  Loss_1: (0.0675) | Acc_1: (97.65%) (40124/41088)\n",
      "Epoch: 27 | Batch_idx: 330 |  Loss_1: (0.0675) | Acc_1: (97.64%) (41370/42368)\n",
      "Epoch: 27 | Batch_idx: 340 |  Loss_1: (0.0678) | Acc_1: (97.63%) (42613/43648)\n",
      "Epoch: 27 | Batch_idx: 350 |  Loss_1: (0.0682) | Acc_1: (97.60%) (43850/44928)\n",
      "Epoch: 27 | Batch_idx: 360 |  Loss_1: (0.0684) | Acc_1: (97.60%) (45097/46208)\n",
      "Epoch: 27 | Batch_idx: 370 |  Loss_1: (0.0684) | Acc_1: (97.60%) (46350/47488)\n",
      "Epoch: 27 | Batch_idx: 380 |  Loss_1: (0.0687) | Acc_1: (97.58%) (47590/48768)\n",
      "Epoch: 27 | Batch_idx: 390 |  Loss_1: (0.0688) | Acc_1: (97.58%) (48790/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3846) | Acc: (90.60%) (9060/10000)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.0446) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.0713) | Acc_1: (97.80%) (1377/1408)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.0756) | Acc_1: (97.51%) (2621/2688)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.0709) | Acc_1: (97.56%) (3871/3968)\n",
      "Epoch: 28 | Batch_idx: 40 |  Loss_1: (0.0683) | Acc_1: (97.56%) (5120/5248)\n",
      "Epoch: 28 | Batch_idx: 50 |  Loss_1: (0.0676) | Acc_1: (97.58%) (6370/6528)\n",
      "Epoch: 28 | Batch_idx: 60 |  Loss_1: (0.0666) | Acc_1: (97.64%) (7624/7808)\n",
      "Epoch: 28 | Batch_idx: 70 |  Loss_1: (0.0659) | Acc_1: (97.72%) (8881/9088)\n",
      "Epoch: 28 | Batch_idx: 80 |  Loss_1: (0.0643) | Acc_1: (97.76%) (10136/10368)\n",
      "Epoch: 28 | Batch_idx: 90 |  Loss_1: (0.0622) | Acc_1: (97.78%) (11389/11648)\n",
      "Epoch: 28 | Batch_idx: 100 |  Loss_1: (0.0626) | Acc_1: (97.80%) (12644/12928)\n",
      "Epoch: 28 | Batch_idx: 110 |  Loss_1: (0.0612) | Acc_1: (97.87%) (13905/14208)\n",
      "Epoch: 28 | Batch_idx: 120 |  Loss_1: (0.0620) | Acc_1: (97.82%) (15150/15488)\n",
      "Epoch: 28 | Batch_idx: 130 |  Loss_1: (0.0621) | Acc_1: (97.80%) (16399/16768)\n",
      "Epoch: 28 | Batch_idx: 140 |  Loss_1: (0.0604) | Acc_1: (97.87%) (17664/18048)\n",
      "Epoch: 28 | Batch_idx: 150 |  Loss_1: (0.0600) | Acc_1: (97.89%) (18920/19328)\n",
      "Epoch: 28 | Batch_idx: 160 |  Loss_1: (0.0593) | Acc_1: (97.90%) (20175/20608)\n",
      "Epoch: 28 | Batch_idx: 170 |  Loss_1: (0.0589) | Acc_1: (97.93%) (21436/21888)\n",
      "Epoch: 28 | Batch_idx: 180 |  Loss_1: (0.0596) | Acc_1: (97.89%) (22679/23168)\n",
      "Epoch: 28 | Batch_idx: 190 |  Loss_1: (0.0597) | Acc_1: (97.89%) (23932/24448)\n",
      "Epoch: 28 | Batch_idx: 200 |  Loss_1: (0.0599) | Acc_1: (97.85%) (25176/25728)\n",
      "Epoch: 28 | Batch_idx: 210 |  Loss_1: (0.0598) | Acc_1: (97.87%) (26432/27008)\n",
      "Epoch: 28 | Batch_idx: 220 |  Loss_1: (0.0590) | Acc_1: (97.91%) (27696/28288)\n",
      "Epoch: 28 | Batch_idx: 230 |  Loss_1: (0.0590) | Acc_1: (97.89%) (28944/29568)\n",
      "Epoch: 28 | Batch_idx: 240 |  Loss_1: (0.0587) | Acc_1: (97.91%) (30202/30848)\n",
      "Epoch: 28 | Batch_idx: 250 |  Loss_1: (0.0592) | Acc_1: (97.89%) (31450/32128)\n",
      "Epoch: 28 | Batch_idx: 260 |  Loss_1: (0.0596) | Acc_1: (97.87%) (32696/33408)\n",
      "Epoch: 28 | Batch_idx: 270 |  Loss_1: (0.0596) | Acc_1: (97.87%) (33949/34688)\n",
      "Epoch: 28 | Batch_idx: 280 |  Loss_1: (0.0594) | Acc_1: (97.90%) (35211/35968)\n",
      "Epoch: 28 | Batch_idx: 290 |  Loss_1: (0.0592) | Acc_1: (97.91%) (36468/37248)\n",
      "Epoch: 28 | Batch_idx: 300 |  Loss_1: (0.0586) | Acc_1: (97.93%) (37729/38528)\n",
      "Epoch: 28 | Batch_idx: 310 |  Loss_1: (0.0586) | Acc_1: (97.93%) (38983/39808)\n",
      "Epoch: 28 | Batch_idx: 320 |  Loss_1: (0.0593) | Acc_1: (97.90%) (40225/41088)\n",
      "Epoch: 28 | Batch_idx: 330 |  Loss_1: (0.0597) | Acc_1: (97.89%) (41475/42368)\n",
      "Epoch: 28 | Batch_idx: 340 |  Loss_1: (0.0596) | Acc_1: (97.89%) (42726/43648)\n",
      "Epoch: 28 | Batch_idx: 350 |  Loss_1: (0.0599) | Acc_1: (97.87%) (43972/44928)\n",
      "Epoch: 28 | Batch_idx: 360 |  Loss_1: (0.0597) | Acc_1: (97.87%) (45226/46208)\n",
      "Epoch: 28 | Batch_idx: 370 |  Loss_1: (0.0596) | Acc_1: (97.87%) (46478/47488)\n",
      "Epoch: 28 | Batch_idx: 380 |  Loss_1: (0.0601) | Acc_1: (97.86%) (47725/48768)\n",
      "Epoch: 28 | Batch_idx: 390 |  Loss_1: (0.0598) | Acc_1: (97.87%) (48935/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4391) | Acc: (90.06%) (9006/10000)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.0436) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.0506) | Acc_1: (98.65%) (1389/1408)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.0459) | Acc_1: (98.62%) (2651/2688)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.0471) | Acc_1: (98.59%) (3912/3968)\n",
      "Epoch: 29 | Batch_idx: 40 |  Loss_1: (0.0461) | Acc_1: (98.55%) (5172/5248)\n",
      "Epoch: 29 | Batch_idx: 50 |  Loss_1: (0.0461) | Acc_1: (98.48%) (6429/6528)\n",
      "Epoch: 29 | Batch_idx: 60 |  Loss_1: (0.0447) | Acc_1: (98.54%) (7694/7808)\n",
      "Epoch: 29 | Batch_idx: 70 |  Loss_1: (0.0429) | Acc_1: (98.59%) (8960/9088)\n",
      "Epoch: 29 | Batch_idx: 80 |  Loss_1: (0.0433) | Acc_1: (98.57%) (10220/10368)\n",
      "Epoch: 29 | Batch_idx: 90 |  Loss_1: (0.0415) | Acc_1: (98.63%) (11489/11648)\n",
      "Epoch: 29 | Batch_idx: 100 |  Loss_1: (0.0416) | Acc_1: (98.64%) (12752/12928)\n",
      "Epoch: 29 | Batch_idx: 110 |  Loss_1: (0.0425) | Acc_1: (98.60%) (14009/14208)\n",
      "Epoch: 29 | Batch_idx: 120 |  Loss_1: (0.0426) | Acc_1: (98.61%) (15273/15488)\n",
      "Epoch: 29 | Batch_idx: 130 |  Loss_1: (0.0437) | Acc_1: (98.57%) (16528/16768)\n",
      "Epoch: 29 | Batch_idx: 140 |  Loss_1: (0.0435) | Acc_1: (98.57%) (17790/18048)\n",
      "Epoch: 29 | Batch_idx: 150 |  Loss_1: (0.0447) | Acc_1: (98.53%) (19044/19328)\n",
      "Epoch: 29 | Batch_idx: 160 |  Loss_1: (0.0449) | Acc_1: (98.51%) (20300/20608)\n",
      "Epoch: 29 | Batch_idx: 170 |  Loss_1: (0.0451) | Acc_1: (98.48%) (21555/21888)\n",
      "Epoch: 29 | Batch_idx: 180 |  Loss_1: (0.0448) | Acc_1: (98.48%) (22817/23168)\n",
      "Epoch: 29 | Batch_idx: 190 |  Loss_1: (0.0452) | Acc_1: (98.47%) (24074/24448)\n",
      "Epoch: 29 | Batch_idx: 200 |  Loss_1: (0.0450) | Acc_1: (98.47%) (25335/25728)\n",
      "Epoch: 29 | Batch_idx: 210 |  Loss_1: (0.0446) | Acc_1: (98.50%) (26604/27008)\n",
      "Epoch: 29 | Batch_idx: 220 |  Loss_1: (0.0450) | Acc_1: (98.49%) (27860/28288)\n",
      "Epoch: 29 | Batch_idx: 230 |  Loss_1: (0.0452) | Acc_1: (98.48%) (29118/29568)\n",
      "Epoch: 29 | Batch_idx: 240 |  Loss_1: (0.0459) | Acc_1: (98.44%) (30367/30848)\n",
      "Epoch: 29 | Batch_idx: 250 |  Loss_1: (0.0463) | Acc_1: (98.42%) (31619/32128)\n",
      "Epoch: 29 | Batch_idx: 260 |  Loss_1: (0.0471) | Acc_1: (98.40%) (32872/33408)\n",
      "Epoch: 29 | Batch_idx: 270 |  Loss_1: (0.0474) | Acc_1: (98.39%) (34129/34688)\n",
      "Epoch: 29 | Batch_idx: 280 |  Loss_1: (0.0476) | Acc_1: (98.39%) (35389/35968)\n",
      "Epoch: 29 | Batch_idx: 290 |  Loss_1: (0.0479) | Acc_1: (98.38%) (36645/37248)\n",
      "Epoch: 29 | Batch_idx: 300 |  Loss_1: (0.0475) | Acc_1: (98.39%) (37906/38528)\n",
      "Epoch: 29 | Batch_idx: 310 |  Loss_1: (0.0477) | Acc_1: (98.37%) (39161/39808)\n",
      "Epoch: 29 | Batch_idx: 320 |  Loss_1: (0.0481) | Acc_1: (98.35%) (40409/41088)\n",
      "Epoch: 29 | Batch_idx: 330 |  Loss_1: (0.0482) | Acc_1: (98.33%) (41662/42368)\n",
      "Epoch: 29 | Batch_idx: 340 |  Loss_1: (0.0488) | Acc_1: (98.30%) (42904/43648)\n",
      "Epoch: 29 | Batch_idx: 350 |  Loss_1: (0.0492) | Acc_1: (98.28%) (44155/44928)\n",
      "Epoch: 29 | Batch_idx: 360 |  Loss_1: (0.0494) | Acc_1: (98.27%) (45410/46208)\n",
      "Epoch: 29 | Batch_idx: 370 |  Loss_1: (0.0495) | Acc_1: (98.26%) (46660/47488)\n",
      "Epoch: 29 | Batch_idx: 380 |  Loss_1: (0.0494) | Acc_1: (98.26%) (47920/48768)\n",
      "Epoch: 29 | Batch_idx: 390 |  Loss_1: (0.0497) | Acc_1: (98.25%) (49127/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3873) | Acc: (91.21%) (9121/10000)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.0098) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.0459) | Acc_1: (98.37%) (1385/1408)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.0496) | Acc_1: (98.03%) (2635/2688)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.0506) | Acc_1: (98.11%) (3893/3968)\n",
      "Epoch: 30 | Batch_idx: 40 |  Loss_1: (0.0463) | Acc_1: (98.32%) (5160/5248)\n",
      "Epoch: 30 | Batch_idx: 50 |  Loss_1: (0.0458) | Acc_1: (98.41%) (6424/6528)\n",
      "Epoch: 30 | Batch_idx: 60 |  Loss_1: (0.0430) | Acc_1: (98.49%) (7690/7808)\n",
      "Epoch: 30 | Batch_idx: 70 |  Loss_1: (0.0444) | Acc_1: (98.39%) (8942/9088)\n",
      "Epoch: 30 | Batch_idx: 80 |  Loss_1: (0.0471) | Acc_1: (98.36%) (10198/10368)\n",
      "Epoch: 30 | Batch_idx: 90 |  Loss_1: (0.0463) | Acc_1: (98.41%) (11463/11648)\n",
      "Epoch: 30 | Batch_idx: 100 |  Loss_1: (0.0462) | Acc_1: (98.39%) (12720/12928)\n",
      "Epoch: 30 | Batch_idx: 110 |  Loss_1: (0.0460) | Acc_1: (98.45%) (13988/14208)\n",
      "Epoch: 30 | Batch_idx: 120 |  Loss_1: (0.0467) | Acc_1: (98.43%) (15245/15488)\n",
      "Epoch: 30 | Batch_idx: 130 |  Loss_1: (0.0475) | Acc_1: (98.41%) (16502/16768)\n",
      "Epoch: 30 | Batch_idx: 140 |  Loss_1: (0.0475) | Acc_1: (98.42%) (17762/18048)\n",
      "Epoch: 30 | Batch_idx: 150 |  Loss_1: (0.0475) | Acc_1: (98.39%) (19017/19328)\n",
      "Epoch: 30 | Batch_idx: 160 |  Loss_1: (0.0483) | Acc_1: (98.36%) (20269/20608)\n",
      "Epoch: 30 | Batch_idx: 170 |  Loss_1: (0.0475) | Acc_1: (98.38%) (21534/21888)\n",
      "Epoch: 30 | Batch_idx: 180 |  Loss_1: (0.0477) | Acc_1: (98.35%) (22785/23168)\n",
      "Epoch: 30 | Batch_idx: 190 |  Loss_1: (0.0479) | Acc_1: (98.34%) (24041/24448)\n",
      "Epoch: 30 | Batch_idx: 200 |  Loss_1: (0.0479) | Acc_1: (98.33%) (25298/25728)\n",
      "Epoch: 30 | Batch_idx: 210 |  Loss_1: (0.0484) | Acc_1: (98.29%) (26547/27008)\n",
      "Epoch: 30 | Batch_idx: 220 |  Loss_1: (0.0488) | Acc_1: (98.27%) (27799/28288)\n",
      "Epoch: 30 | Batch_idx: 230 |  Loss_1: (0.0484) | Acc_1: (98.28%) (29060/29568)\n",
      "Epoch: 30 | Batch_idx: 240 |  Loss_1: (0.0482) | Acc_1: (98.30%) (30324/30848)\n",
      "Epoch: 30 | Batch_idx: 250 |  Loss_1: (0.0484) | Acc_1: (98.29%) (31579/32128)\n",
      "Epoch: 30 | Batch_idx: 260 |  Loss_1: (0.0482) | Acc_1: (98.30%) (32841/33408)\n",
      "Epoch: 30 | Batch_idx: 270 |  Loss_1: (0.0483) | Acc_1: (98.32%) (34104/34688)\n",
      "Epoch: 30 | Batch_idx: 280 |  Loss_1: (0.0482) | Acc_1: (98.33%) (35367/35968)\n",
      "Epoch: 30 | Batch_idx: 290 |  Loss_1: (0.0482) | Acc_1: (98.33%) (36625/37248)\n",
      "Epoch: 30 | Batch_idx: 300 |  Loss_1: (0.0484) | Acc_1: (98.32%) (37879/38528)\n",
      "Epoch: 30 | Batch_idx: 310 |  Loss_1: (0.0484) | Acc_1: (98.32%) (39138/39808)\n",
      "Epoch: 30 | Batch_idx: 320 |  Loss_1: (0.0486) | Acc_1: (98.32%) (40397/41088)\n",
      "Epoch: 30 | Batch_idx: 330 |  Loss_1: (0.0485) | Acc_1: (98.32%) (41656/42368)\n",
      "Epoch: 30 | Batch_idx: 340 |  Loss_1: (0.0482) | Acc_1: (98.33%) (42919/43648)\n",
      "Epoch: 30 | Batch_idx: 350 |  Loss_1: (0.0485) | Acc_1: (98.32%) (44173/44928)\n",
      "Epoch: 30 | Batch_idx: 360 |  Loss_1: (0.0483) | Acc_1: (98.32%) (45432/46208)\n",
      "Epoch: 30 | Batch_idx: 370 |  Loss_1: (0.0484) | Acc_1: (98.31%) (46686/47488)\n",
      "Epoch: 30 | Batch_idx: 380 |  Loss_1: (0.0483) | Acc_1: (98.31%) (47944/48768)\n",
      "Epoch: 30 | Batch_idx: 390 |  Loss_1: (0.0486) | Acc_1: (98.30%) (49149/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4217) | Acc: (90.60%) (9060/10000)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.0480) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.0331) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.0379) | Acc_1: (98.70%) (2653/2688)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.0417) | Acc_1: (98.59%) (3912/3968)\n",
      "Epoch: 31 | Batch_idx: 40 |  Loss_1: (0.0418) | Acc_1: (98.59%) (5174/5248)\n",
      "Epoch: 31 | Batch_idx: 50 |  Loss_1: (0.0476) | Acc_1: (98.38%) (6422/6528)\n",
      "Epoch: 31 | Batch_idx: 60 |  Loss_1: (0.0450) | Acc_1: (98.46%) (7688/7808)\n",
      "Epoch: 31 | Batch_idx: 70 |  Loss_1: (0.0440) | Acc_1: (98.49%) (8951/9088)\n",
      "Epoch: 31 | Batch_idx: 80 |  Loss_1: (0.0426) | Acc_1: (98.53%) (10216/10368)\n",
      "Epoch: 31 | Batch_idx: 90 |  Loss_1: (0.0414) | Acc_1: (98.56%) (11480/11648)\n",
      "Epoch: 31 | Batch_idx: 100 |  Loss_1: (0.0408) | Acc_1: (98.58%) (12745/12928)\n",
      "Epoch: 31 | Batch_idx: 110 |  Loss_1: (0.0406) | Acc_1: (98.59%) (14008/14208)\n",
      "Epoch: 31 | Batch_idx: 120 |  Loss_1: (0.0396) | Acc_1: (98.61%) (15273/15488)\n",
      "Epoch: 31 | Batch_idx: 130 |  Loss_1: (0.0391) | Acc_1: (98.61%) (16535/16768)\n",
      "Epoch: 31 | Batch_idx: 140 |  Loss_1: (0.0389) | Acc_1: (98.61%) (17798/18048)\n",
      "Epoch: 31 | Batch_idx: 150 |  Loss_1: (0.0391) | Acc_1: (98.59%) (19056/19328)\n",
      "Epoch: 31 | Batch_idx: 160 |  Loss_1: (0.0387) | Acc_1: (98.59%) (20318/20608)\n",
      "Epoch: 31 | Batch_idx: 170 |  Loss_1: (0.0398) | Acc_1: (98.55%) (21570/21888)\n",
      "Epoch: 31 | Batch_idx: 180 |  Loss_1: (0.0409) | Acc_1: (98.52%) (22825/23168)\n",
      "Epoch: 31 | Batch_idx: 190 |  Loss_1: (0.0412) | Acc_1: (98.52%) (24085/24448)\n",
      "Epoch: 31 | Batch_idx: 200 |  Loss_1: (0.0416) | Acc_1: (98.51%) (25345/25728)\n",
      "Epoch: 31 | Batch_idx: 210 |  Loss_1: (0.0430) | Acc_1: (98.44%) (26588/27008)\n",
      "Epoch: 31 | Batch_idx: 220 |  Loss_1: (0.0435) | Acc_1: (98.42%) (27841/28288)\n",
      "Epoch: 31 | Batch_idx: 230 |  Loss_1: (0.0435) | Acc_1: (98.41%) (29099/29568)\n",
      "Epoch: 31 | Batch_idx: 240 |  Loss_1: (0.0435) | Acc_1: (98.41%) (30356/30848)\n",
      "Epoch: 31 | Batch_idx: 250 |  Loss_1: (0.0436) | Acc_1: (98.39%) (31610/32128)\n",
      "Epoch: 31 | Batch_idx: 260 |  Loss_1: (0.0438) | Acc_1: (98.38%) (32866/33408)\n",
      "Epoch: 31 | Batch_idx: 270 |  Loss_1: (0.0444) | Acc_1: (98.36%) (34119/34688)\n",
      "Epoch: 31 | Batch_idx: 280 |  Loss_1: (0.0445) | Acc_1: (98.36%) (35379/35968)\n",
      "Epoch: 31 | Batch_idx: 290 |  Loss_1: (0.0445) | Acc_1: (98.37%) (36639/37248)\n",
      "Epoch: 31 | Batch_idx: 300 |  Loss_1: (0.0443) | Acc_1: (98.37%) (37901/38528)\n",
      "Epoch: 31 | Batch_idx: 310 |  Loss_1: (0.0439) | Acc_1: (98.39%) (39167/39808)\n",
      "Epoch: 31 | Batch_idx: 320 |  Loss_1: (0.0439) | Acc_1: (98.39%) (40426/41088)\n",
      "Epoch: 31 | Batch_idx: 330 |  Loss_1: (0.0441) | Acc_1: (98.38%) (41680/42368)\n",
      "Epoch: 31 | Batch_idx: 340 |  Loss_1: (0.0440) | Acc_1: (98.39%) (42944/43648)\n",
      "Epoch: 31 | Batch_idx: 350 |  Loss_1: (0.0437) | Acc_1: (98.40%) (44209/44928)\n",
      "Epoch: 31 | Batch_idx: 360 |  Loss_1: (0.0435) | Acc_1: (98.41%) (45473/46208)\n",
      "Epoch: 31 | Batch_idx: 370 |  Loss_1: (0.0435) | Acc_1: (98.41%) (46733/47488)\n",
      "Epoch: 31 | Batch_idx: 380 |  Loss_1: (0.0431) | Acc_1: (98.42%) (47999/48768)\n",
      "Epoch: 31 | Batch_idx: 390 |  Loss_1: (0.0435) | Acc_1: (98.41%) (49207/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4149) | Acc: (90.71%) (9071/10000)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.0407) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.0417) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.0452) | Acc_1: (98.55%) (2649/2688)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.0394) | Acc_1: (98.71%) (3917/3968)\n",
      "Epoch: 32 | Batch_idx: 40 |  Loss_1: (0.0348) | Acc_1: (98.88%) (5189/5248)\n",
      "Epoch: 32 | Batch_idx: 50 |  Loss_1: (0.0364) | Acc_1: (98.84%) (6452/6528)\n",
      "Epoch: 32 | Batch_idx: 60 |  Loss_1: (0.0358) | Acc_1: (98.82%) (7716/7808)\n",
      "Epoch: 32 | Batch_idx: 70 |  Loss_1: (0.0351) | Acc_1: (98.80%) (8979/9088)\n",
      "Epoch: 32 | Batch_idx: 80 |  Loss_1: (0.0343) | Acc_1: (98.81%) (10245/10368)\n",
      "Epoch: 32 | Batch_idx: 90 |  Loss_1: (0.0345) | Acc_1: (98.78%) (11506/11648)\n",
      "Epoch: 32 | Batch_idx: 100 |  Loss_1: (0.0344) | Acc_1: (98.77%) (12769/12928)\n",
      "Epoch: 32 | Batch_idx: 110 |  Loss_1: (0.0342) | Acc_1: (98.78%) (14035/14208)\n",
      "Epoch: 32 | Batch_idx: 120 |  Loss_1: (0.0342) | Acc_1: (98.79%) (15301/15488)\n",
      "Epoch: 32 | Batch_idx: 130 |  Loss_1: (0.0338) | Acc_1: (98.80%) (16567/16768)\n",
      "Epoch: 32 | Batch_idx: 140 |  Loss_1: (0.0339) | Acc_1: (98.80%) (17832/18048)\n",
      "Epoch: 32 | Batch_idx: 150 |  Loss_1: (0.0344) | Acc_1: (98.78%) (19093/19328)\n",
      "Epoch: 32 | Batch_idx: 160 |  Loss_1: (0.0345) | Acc_1: (98.77%) (20355/20608)\n",
      "Epoch: 32 | Batch_idx: 170 |  Loss_1: (0.0345) | Acc_1: (98.78%) (21621/21888)\n",
      "Epoch: 32 | Batch_idx: 180 |  Loss_1: (0.0344) | Acc_1: (98.77%) (22884/23168)\n",
      "Epoch: 32 | Batch_idx: 190 |  Loss_1: (0.0341) | Acc_1: (98.80%) (24154/24448)\n",
      "Epoch: 32 | Batch_idx: 200 |  Loss_1: (0.0336) | Acc_1: (98.81%) (25423/25728)\n",
      "Epoch: 32 | Batch_idx: 210 |  Loss_1: (0.0341) | Acc_1: (98.79%) (26682/27008)\n",
      "Epoch: 32 | Batch_idx: 220 |  Loss_1: (0.0338) | Acc_1: (98.79%) (27947/28288)\n",
      "Epoch: 32 | Batch_idx: 230 |  Loss_1: (0.0347) | Acc_1: (98.78%) (29207/29568)\n",
      "Epoch: 32 | Batch_idx: 240 |  Loss_1: (0.0347) | Acc_1: (98.77%) (30470/30848)\n",
      "Epoch: 32 | Batch_idx: 250 |  Loss_1: (0.0352) | Acc_1: (98.76%) (31729/32128)\n",
      "Epoch: 32 | Batch_idx: 260 |  Loss_1: (0.0352) | Acc_1: (98.77%) (32996/33408)\n",
      "Epoch: 32 | Batch_idx: 270 |  Loss_1: (0.0357) | Acc_1: (98.75%) (34254/34688)\n",
      "Epoch: 32 | Batch_idx: 280 |  Loss_1: (0.0361) | Acc_1: (98.72%) (35509/35968)\n",
      "Epoch: 32 | Batch_idx: 290 |  Loss_1: (0.0363) | Acc_1: (98.73%) (36774/37248)\n",
      "Epoch: 32 | Batch_idx: 300 |  Loss_1: (0.0362) | Acc_1: (98.73%) (38037/38528)\n",
      "Epoch: 32 | Batch_idx: 310 |  Loss_1: (0.0362) | Acc_1: (98.73%) (39301/39808)\n",
      "Epoch: 32 | Batch_idx: 320 |  Loss_1: (0.0362) | Acc_1: (98.73%) (40565/41088)\n",
      "Epoch: 32 | Batch_idx: 330 |  Loss_1: (0.0359) | Acc_1: (98.73%) (41832/42368)\n",
      "Epoch: 32 | Batch_idx: 340 |  Loss_1: (0.0359) | Acc_1: (98.73%) (43092/43648)\n",
      "Epoch: 32 | Batch_idx: 350 |  Loss_1: (0.0364) | Acc_1: (98.70%) (44344/44928)\n",
      "Epoch: 32 | Batch_idx: 360 |  Loss_1: (0.0366) | Acc_1: (98.70%) (45608/46208)\n",
      "Epoch: 32 | Batch_idx: 370 |  Loss_1: (0.0364) | Acc_1: (98.71%) (46875/47488)\n",
      "Epoch: 32 | Batch_idx: 380 |  Loss_1: (0.0362) | Acc_1: (98.71%) (48141/48768)\n",
      "Epoch: 32 | Batch_idx: 390 |  Loss_1: (0.0363) | Acc_1: (98.71%) (49355/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4261) | Acc: (90.74%) (9074/10000)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.0831) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.0338) | Acc_1: (99.01%) (1394/1408)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.0356) | Acc_1: (98.66%) (2652/2688)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.0363) | Acc_1: (98.66%) (3915/3968)\n",
      "Epoch: 33 | Batch_idx: 40 |  Loss_1: (0.0340) | Acc_1: (98.78%) (5184/5248)\n",
      "Epoch: 33 | Batch_idx: 50 |  Loss_1: (0.0339) | Acc_1: (98.82%) (6451/6528)\n",
      "Epoch: 33 | Batch_idx: 60 |  Loss_1: (0.0340) | Acc_1: (98.81%) (7715/7808)\n",
      "Epoch: 33 | Batch_idx: 70 |  Loss_1: (0.0343) | Acc_1: (98.82%) (8981/9088)\n",
      "Epoch: 33 | Batch_idx: 80 |  Loss_1: (0.0337) | Acc_1: (98.86%) (10250/10368)\n",
      "Epoch: 33 | Batch_idx: 90 |  Loss_1: (0.0339) | Acc_1: (98.87%) (11516/11648)\n",
      "Epoch: 33 | Batch_idx: 100 |  Loss_1: (0.0330) | Acc_1: (98.88%) (12783/12928)\n",
      "Epoch: 33 | Batch_idx: 110 |  Loss_1: (0.0333) | Acc_1: (98.85%) (14044/14208)\n",
      "Epoch: 33 | Batch_idx: 120 |  Loss_1: (0.0329) | Acc_1: (98.85%) (15310/15488)\n",
      "Epoch: 33 | Batch_idx: 130 |  Loss_1: (0.0334) | Acc_1: (98.83%) (16572/16768)\n",
      "Epoch: 33 | Batch_idx: 140 |  Loss_1: (0.0337) | Acc_1: (98.81%) (17833/18048)\n",
      "Epoch: 33 | Batch_idx: 150 |  Loss_1: (0.0337) | Acc_1: (98.80%) (19097/19328)\n",
      "Epoch: 33 | Batch_idx: 160 |  Loss_1: (0.0342) | Acc_1: (98.78%) (20356/20608)\n",
      "Epoch: 33 | Batch_idx: 170 |  Loss_1: (0.0338) | Acc_1: (98.79%) (21624/21888)\n",
      "Epoch: 33 | Batch_idx: 180 |  Loss_1: (0.0338) | Acc_1: (98.79%) (22888/23168)\n",
      "Epoch: 33 | Batch_idx: 190 |  Loss_1: (0.0335) | Acc_1: (98.81%) (24157/24448)\n",
      "Epoch: 33 | Batch_idx: 200 |  Loss_1: (0.0339) | Acc_1: (98.80%) (25418/25728)\n",
      "Epoch: 33 | Batch_idx: 210 |  Loss_1: (0.0344) | Acc_1: (98.77%) (26677/27008)\n",
      "Epoch: 33 | Batch_idx: 220 |  Loss_1: (0.0341) | Acc_1: (98.78%) (27944/28288)\n",
      "Epoch: 33 | Batch_idx: 230 |  Loss_1: (0.0340) | Acc_1: (98.79%) (29210/29568)\n",
      "Epoch: 33 | Batch_idx: 240 |  Loss_1: (0.0339) | Acc_1: (98.80%) (30479/30848)\n",
      "Epoch: 33 | Batch_idx: 250 |  Loss_1: (0.0335) | Acc_1: (98.81%) (31745/32128)\n",
      "Epoch: 33 | Batch_idx: 260 |  Loss_1: (0.0336) | Acc_1: (98.81%) (33010/33408)\n",
      "Epoch: 33 | Batch_idx: 270 |  Loss_1: (0.0337) | Acc_1: (98.81%) (34274/34688)\n",
      "Epoch: 33 | Batch_idx: 280 |  Loss_1: (0.0332) | Acc_1: (98.83%) (35547/35968)\n",
      "Epoch: 33 | Batch_idx: 290 |  Loss_1: (0.0330) | Acc_1: (98.83%) (36813/37248)\n",
      "Epoch: 33 | Batch_idx: 300 |  Loss_1: (0.0329) | Acc_1: (98.84%) (38081/38528)\n",
      "Epoch: 33 | Batch_idx: 310 |  Loss_1: (0.0328) | Acc_1: (98.85%) (39352/39808)\n",
      "Epoch: 33 | Batch_idx: 320 |  Loss_1: (0.0327) | Acc_1: (98.86%) (40619/41088)\n",
      "Epoch: 33 | Batch_idx: 330 |  Loss_1: (0.0329) | Acc_1: (98.86%) (41883/42368)\n",
      "Epoch: 33 | Batch_idx: 340 |  Loss_1: (0.0327) | Acc_1: (98.87%) (43153/43648)\n",
      "Epoch: 33 | Batch_idx: 350 |  Loss_1: (0.0330) | Acc_1: (98.85%) (44411/44928)\n",
      "Epoch: 33 | Batch_idx: 360 |  Loss_1: (0.0332) | Acc_1: (98.85%) (45676/46208)\n",
      "Epoch: 33 | Batch_idx: 370 |  Loss_1: (0.0331) | Acc_1: (98.85%) (46944/47488)\n",
      "Epoch: 33 | Batch_idx: 380 |  Loss_1: (0.0333) | Acc_1: (98.85%) (48208/48768)\n",
      "Epoch: 33 | Batch_idx: 390 |  Loss_1: (0.0335) | Acc_1: (98.84%) (49422/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4207) | Acc: (91.29%) (9129/10000)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.0542) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.0284) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.0305) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.0267) | Acc_1: (99.07%) (3931/3968)\n",
      "Epoch: 34 | Batch_idx: 40 |  Loss_1: (0.0268) | Acc_1: (99.03%) (5197/5248)\n",
      "Epoch: 34 | Batch_idx: 50 |  Loss_1: (0.0284) | Acc_1: (98.97%) (6461/6528)\n",
      "Epoch: 34 | Batch_idx: 60 |  Loss_1: (0.0294) | Acc_1: (98.96%) (7727/7808)\n",
      "Epoch: 34 | Batch_idx: 70 |  Loss_1: (0.0281) | Acc_1: (98.99%) (8996/9088)\n",
      "Epoch: 34 | Batch_idx: 80 |  Loss_1: (0.0281) | Acc_1: (98.98%) (10262/10368)\n",
      "Epoch: 34 | Batch_idx: 90 |  Loss_1: (0.0297) | Acc_1: (98.93%) (11523/11648)\n",
      "Epoch: 34 | Batch_idx: 100 |  Loss_1: (0.0297) | Acc_1: (98.96%) (12793/12928)\n",
      "Epoch: 34 | Batch_idx: 110 |  Loss_1: (0.0284) | Acc_1: (98.99%) (14064/14208)\n",
      "Epoch: 34 | Batch_idx: 120 |  Loss_1: (0.0288) | Acc_1: (98.97%) (15329/15488)\n",
      "Epoch: 34 | Batch_idx: 130 |  Loss_1: (0.0286) | Acc_1: (98.99%) (16598/16768)\n",
      "Epoch: 34 | Batch_idx: 140 |  Loss_1: (0.0291) | Acc_1: (98.97%) (17863/18048)\n",
      "Epoch: 34 | Batch_idx: 150 |  Loss_1: (0.0293) | Acc_1: (98.96%) (19127/19328)\n",
      "Epoch: 34 | Batch_idx: 160 |  Loss_1: (0.0290) | Acc_1: (98.95%) (20392/20608)\n",
      "Epoch: 34 | Batch_idx: 170 |  Loss_1: (0.0290) | Acc_1: (98.95%) (21659/21888)\n",
      "Epoch: 34 | Batch_idx: 180 |  Loss_1: (0.0291) | Acc_1: (98.93%) (22921/23168)\n",
      "Epoch: 34 | Batch_idx: 190 |  Loss_1: (0.0294) | Acc_1: (98.92%) (24185/24448)\n",
      "Epoch: 34 | Batch_idx: 200 |  Loss_1: (0.0289) | Acc_1: (98.95%) (25457/25728)\n",
      "Epoch: 34 | Batch_idx: 210 |  Loss_1: (0.0286) | Acc_1: (98.96%) (26726/27008)\n",
      "Epoch: 34 | Batch_idx: 220 |  Loss_1: (0.0286) | Acc_1: (98.95%) (27992/28288)\n",
      "Epoch: 34 | Batch_idx: 230 |  Loss_1: (0.0284) | Acc_1: (98.97%) (29262/29568)\n",
      "Epoch: 34 | Batch_idx: 240 |  Loss_1: (0.0286) | Acc_1: (98.95%) (30525/30848)\n",
      "Epoch: 34 | Batch_idx: 250 |  Loss_1: (0.0283) | Acc_1: (98.97%) (31797/32128)\n",
      "Epoch: 34 | Batch_idx: 260 |  Loss_1: (0.0281) | Acc_1: (98.98%) (33067/33408)\n",
      "Epoch: 34 | Batch_idx: 270 |  Loss_1: (0.0279) | Acc_1: (98.99%) (34337/34688)\n",
      "Epoch: 34 | Batch_idx: 280 |  Loss_1: (0.0278) | Acc_1: (98.99%) (35606/35968)\n",
      "Epoch: 34 | Batch_idx: 290 |  Loss_1: (0.0276) | Acc_1: (99.00%) (36874/37248)\n",
      "Epoch: 34 | Batch_idx: 300 |  Loss_1: (0.0276) | Acc_1: (99.00%) (38141/38528)\n",
      "Epoch: 34 | Batch_idx: 310 |  Loss_1: (0.0278) | Acc_1: (98.99%) (39406/39808)\n",
      "Epoch: 34 | Batch_idx: 320 |  Loss_1: (0.0277) | Acc_1: (98.99%) (40673/41088)\n",
      "Epoch: 34 | Batch_idx: 330 |  Loss_1: (0.0281) | Acc_1: (98.97%) (41933/42368)\n",
      "Epoch: 34 | Batch_idx: 340 |  Loss_1: (0.0286) | Acc_1: (98.96%) (43193/43648)\n",
      "Epoch: 34 | Batch_idx: 350 |  Loss_1: (0.0286) | Acc_1: (98.96%) (44460/44928)\n",
      "Epoch: 34 | Batch_idx: 360 |  Loss_1: (0.0288) | Acc_1: (98.95%) (45721/46208)\n",
      "Epoch: 34 | Batch_idx: 370 |  Loss_1: (0.0290) | Acc_1: (98.94%) (46986/47488)\n",
      "Epoch: 34 | Batch_idx: 380 |  Loss_1: (0.0290) | Acc_1: (98.94%) (48252/48768)\n",
      "Epoch: 34 | Batch_idx: 390 |  Loss_1: (0.0289) | Acc_1: (98.95%) (49474/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4369) | Acc: (90.94%) (9094/10000)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.0689) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.0224) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.0244) | Acc_1: (99.18%) (2666/2688)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.0262) | Acc_1: (99.09%) (3932/3968)\n",
      "Epoch: 35 | Batch_idx: 40 |  Loss_1: (0.0238) | Acc_1: (99.18%) (5205/5248)\n",
      "Epoch: 35 | Batch_idx: 50 |  Loss_1: (0.0240) | Acc_1: (99.17%) (6474/6528)\n",
      "Epoch: 35 | Batch_idx: 60 |  Loss_1: (0.0250) | Acc_1: (99.19%) (7745/7808)\n",
      "Epoch: 35 | Batch_idx: 70 |  Loss_1: (0.0248) | Acc_1: (99.22%) (9017/9088)\n",
      "Epoch: 35 | Batch_idx: 80 |  Loss_1: (0.0249) | Acc_1: (99.19%) (10284/10368)\n",
      "Epoch: 35 | Batch_idx: 90 |  Loss_1: (0.0236) | Acc_1: (99.25%) (11561/11648)\n",
      "Epoch: 35 | Batch_idx: 100 |  Loss_1: (0.0233) | Acc_1: (99.25%) (12831/12928)\n",
      "Epoch: 35 | Batch_idx: 110 |  Loss_1: (0.0231) | Acc_1: (99.25%) (14101/14208)\n",
      "Epoch: 35 | Batch_idx: 120 |  Loss_1: (0.0233) | Acc_1: (99.24%) (15370/15488)\n",
      "Epoch: 35 | Batch_idx: 130 |  Loss_1: (0.0243) | Acc_1: (99.21%) (16635/16768)\n",
      "Epoch: 35 | Batch_idx: 140 |  Loss_1: (0.0244) | Acc_1: (99.19%) (17901/18048)\n",
      "Epoch: 35 | Batch_idx: 150 |  Loss_1: (0.0239) | Acc_1: (99.20%) (19174/19328)\n",
      "Epoch: 35 | Batch_idx: 160 |  Loss_1: (0.0237) | Acc_1: (99.21%) (20445/20608)\n",
      "Epoch: 35 | Batch_idx: 170 |  Loss_1: (0.0238) | Acc_1: (99.21%) (21716/21888)\n",
      "Epoch: 35 | Batch_idx: 180 |  Loss_1: (0.0238) | Acc_1: (99.21%) (22986/23168)\n",
      "Epoch: 35 | Batch_idx: 190 |  Loss_1: (0.0234) | Acc_1: (99.24%) (24261/24448)\n",
      "Epoch: 35 | Batch_idx: 200 |  Loss_1: (0.0236) | Acc_1: (99.21%) (25525/25728)\n",
      "Epoch: 35 | Batch_idx: 210 |  Loss_1: (0.0238) | Acc_1: (99.19%) (26790/27008)\n",
      "Epoch: 35 | Batch_idx: 220 |  Loss_1: (0.0235) | Acc_1: (99.21%) (28064/28288)\n",
      "Epoch: 35 | Batch_idx: 230 |  Loss_1: (0.0238) | Acc_1: (99.19%) (29328/29568)\n",
      "Epoch: 35 | Batch_idx: 240 |  Loss_1: (0.0238) | Acc_1: (99.18%) (30595/30848)\n",
      "Epoch: 35 | Batch_idx: 250 |  Loss_1: (0.0241) | Acc_1: (99.17%) (31860/32128)\n",
      "Epoch: 35 | Batch_idx: 260 |  Loss_1: (0.0239) | Acc_1: (99.17%) (33131/33408)\n",
      "Epoch: 35 | Batch_idx: 270 |  Loss_1: (0.0237) | Acc_1: (99.18%) (34403/34688)\n",
      "Epoch: 35 | Batch_idx: 280 |  Loss_1: (0.0240) | Acc_1: (99.16%) (35666/35968)\n",
      "Epoch: 35 | Batch_idx: 290 |  Loss_1: (0.0238) | Acc_1: (99.17%) (36938/37248)\n",
      "Epoch: 35 | Batch_idx: 300 |  Loss_1: (0.0240) | Acc_1: (99.16%) (38203/38528)\n",
      "Epoch: 35 | Batch_idx: 310 |  Loss_1: (0.0244) | Acc_1: (99.14%) (39464/39808)\n",
      "Epoch: 35 | Batch_idx: 320 |  Loss_1: (0.0247) | Acc_1: (99.12%) (40726/41088)\n",
      "Epoch: 35 | Batch_idx: 330 |  Loss_1: (0.0245) | Acc_1: (99.13%) (41998/42368)\n",
      "Epoch: 35 | Batch_idx: 340 |  Loss_1: (0.0247) | Acc_1: (99.12%) (43262/43648)\n",
      "Epoch: 35 | Batch_idx: 350 |  Loss_1: (0.0247) | Acc_1: (99.11%) (44530/44928)\n",
      "Epoch: 35 | Batch_idx: 360 |  Loss_1: (0.0251) | Acc_1: (99.09%) (45787/46208)\n",
      "Epoch: 35 | Batch_idx: 370 |  Loss_1: (0.0253) | Acc_1: (99.09%) (47054/47488)\n",
      "Epoch: 35 | Batch_idx: 380 |  Loss_1: (0.0254) | Acc_1: (99.08%) (48320/48768)\n",
      "Epoch: 35 | Batch_idx: 390 |  Loss_1: (0.0256) | Acc_1: (99.07%) (49534/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4395) | Acc: (91.23%) (9123/10000)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.0124) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.0316) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.0299) | Acc_1: (99.03%) (2662/2688)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.0277) | Acc_1: (99.14%) (3934/3968)\n",
      "Epoch: 36 | Batch_idx: 40 |  Loss_1: (0.0280) | Acc_1: (99.09%) (5200/5248)\n",
      "Epoch: 36 | Batch_idx: 50 |  Loss_1: (0.0276) | Acc_1: (99.10%) (6469/6528)\n",
      "Epoch: 36 | Batch_idx: 60 |  Loss_1: (0.0273) | Acc_1: (99.09%) (7737/7808)\n",
      "Epoch: 36 | Batch_idx: 70 |  Loss_1: (0.0262) | Acc_1: (99.12%) (9008/9088)\n",
      "Epoch: 36 | Batch_idx: 80 |  Loss_1: (0.0250) | Acc_1: (99.16%) (10281/10368)\n",
      "Epoch: 36 | Batch_idx: 90 |  Loss_1: (0.0245) | Acc_1: (99.19%) (11554/11648)\n",
      "Epoch: 36 | Batch_idx: 100 |  Loss_1: (0.0246) | Acc_1: (99.18%) (12822/12928)\n",
      "Epoch: 36 | Batch_idx: 110 |  Loss_1: (0.0245) | Acc_1: (99.17%) (14090/14208)\n",
      "Epoch: 36 | Batch_idx: 120 |  Loss_1: (0.0245) | Acc_1: (99.18%) (15361/15488)\n",
      "Epoch: 36 | Batch_idx: 130 |  Loss_1: (0.0244) | Acc_1: (99.18%) (16631/16768)\n",
      "Epoch: 36 | Batch_idx: 140 |  Loss_1: (0.0240) | Acc_1: (99.20%) (17903/18048)\n",
      "Epoch: 36 | Batch_idx: 150 |  Loss_1: (0.0238) | Acc_1: (99.20%) (19174/19328)\n",
      "Epoch: 36 | Batch_idx: 160 |  Loss_1: (0.0237) | Acc_1: (99.21%) (20445/20608)\n",
      "Epoch: 36 | Batch_idx: 170 |  Loss_1: (0.0238) | Acc_1: (99.21%) (21716/21888)\n",
      "Epoch: 36 | Batch_idx: 180 |  Loss_1: (0.0236) | Acc_1: (99.22%) (22987/23168)\n",
      "Epoch: 36 | Batch_idx: 190 |  Loss_1: (0.0236) | Acc_1: (99.22%) (24258/24448)\n",
      "Epoch: 36 | Batch_idx: 200 |  Loss_1: (0.0232) | Acc_1: (99.24%) (25532/25728)\n",
      "Epoch: 36 | Batch_idx: 210 |  Loss_1: (0.0234) | Acc_1: (99.23%) (26799/27008)\n",
      "Epoch: 36 | Batch_idx: 220 |  Loss_1: (0.0232) | Acc_1: (99.22%) (28068/28288)\n",
      "Epoch: 36 | Batch_idx: 230 |  Loss_1: (0.0231) | Acc_1: (99.22%) (29338/29568)\n",
      "Epoch: 36 | Batch_idx: 240 |  Loss_1: (0.0233) | Acc_1: (99.21%) (30604/30848)\n",
      "Epoch: 36 | Batch_idx: 250 |  Loss_1: (0.0230) | Acc_1: (99.22%) (31878/32128)\n",
      "Epoch: 36 | Batch_idx: 260 |  Loss_1: (0.0229) | Acc_1: (99.22%) (33148/33408)\n",
      "Epoch: 36 | Batch_idx: 270 |  Loss_1: (0.0230) | Acc_1: (99.22%) (34417/34688)\n",
      "Epoch: 36 | Batch_idx: 280 |  Loss_1: (0.0230) | Acc_1: (99.22%) (35687/35968)\n",
      "Epoch: 36 | Batch_idx: 290 |  Loss_1: (0.0227) | Acc_1: (99.23%) (36960/37248)\n",
      "Epoch: 36 | Batch_idx: 300 |  Loss_1: (0.0224) | Acc_1: (99.24%) (38234/38528)\n",
      "Epoch: 36 | Batch_idx: 310 |  Loss_1: (0.0227) | Acc_1: (99.22%) (39497/39808)\n",
      "Epoch: 36 | Batch_idx: 320 |  Loss_1: (0.0225) | Acc_1: (99.23%) (40772/41088)\n",
      "Epoch: 36 | Batch_idx: 330 |  Loss_1: (0.0227) | Acc_1: (99.23%) (42041/42368)\n",
      "Epoch: 36 | Batch_idx: 340 |  Loss_1: (0.0224) | Acc_1: (99.24%) (43318/43648)\n",
      "Epoch: 36 | Batch_idx: 350 |  Loss_1: (0.0225) | Acc_1: (99.25%) (44589/44928)\n",
      "Epoch: 36 | Batch_idx: 360 |  Loss_1: (0.0226) | Acc_1: (99.24%) (45858/46208)\n",
      "Epoch: 36 | Batch_idx: 370 |  Loss_1: (0.0228) | Acc_1: (99.23%) (47121/47488)\n",
      "Epoch: 36 | Batch_idx: 380 |  Loss_1: (0.0228) | Acc_1: (99.22%) (48389/48768)\n",
      "Epoch: 36 | Batch_idx: 390 |  Loss_1: (0.0228) | Acc_1: (99.22%) (49608/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4286) | Acc: (91.71%) (9171/10000)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.0395) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.0192) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.0182) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.0164) | Acc_1: (99.47%) (3947/3968)\n",
      "Epoch: 37 | Batch_idx: 40 |  Loss_1: (0.0167) | Acc_1: (99.45%) (5219/5248)\n",
      "Epoch: 37 | Batch_idx: 50 |  Loss_1: (0.0178) | Acc_1: (99.40%) (6489/6528)\n",
      "Epoch: 37 | Batch_idx: 60 |  Loss_1: (0.0179) | Acc_1: (99.40%) (7761/7808)\n",
      "Epoch: 37 | Batch_idx: 70 |  Loss_1: (0.0184) | Acc_1: (99.38%) (9032/9088)\n",
      "Epoch: 37 | Batch_idx: 80 |  Loss_1: (0.0188) | Acc_1: (99.37%) (10303/10368)\n",
      "Epoch: 37 | Batch_idx: 90 |  Loss_1: (0.0184) | Acc_1: (99.38%) (11576/11648)\n",
      "Epoch: 37 | Batch_idx: 100 |  Loss_1: (0.0193) | Acc_1: (99.33%) (12842/12928)\n",
      "Epoch: 37 | Batch_idx: 110 |  Loss_1: (0.0186) | Acc_1: (99.36%) (14117/14208)\n",
      "Epoch: 37 | Batch_idx: 120 |  Loss_1: (0.0184) | Acc_1: (99.37%) (15390/15488)\n",
      "Epoch: 37 | Batch_idx: 130 |  Loss_1: (0.0188) | Acc_1: (99.35%) (16659/16768)\n",
      "Epoch: 37 | Batch_idx: 140 |  Loss_1: (0.0191) | Acc_1: (99.35%) (17930/18048)\n",
      "Epoch: 37 | Batch_idx: 150 |  Loss_1: (0.0194) | Acc_1: (99.32%) (19196/19328)\n",
      "Epoch: 37 | Batch_idx: 160 |  Loss_1: (0.0195) | Acc_1: (99.30%) (20464/20608)\n",
      "Epoch: 37 | Batch_idx: 170 |  Loss_1: (0.0195) | Acc_1: (99.30%) (21735/21888)\n",
      "Epoch: 37 | Batch_idx: 180 |  Loss_1: (0.0192) | Acc_1: (99.31%) (23008/23168)\n",
      "Epoch: 37 | Batch_idx: 190 |  Loss_1: (0.0191) | Acc_1: (99.31%) (24280/24448)\n",
      "Epoch: 37 | Batch_idx: 200 |  Loss_1: (0.0189) | Acc_1: (99.32%) (25552/25728)\n",
      "Epoch: 37 | Batch_idx: 210 |  Loss_1: (0.0185) | Acc_1: (99.33%) (26826/27008)\n",
      "Epoch: 37 | Batch_idx: 220 |  Loss_1: (0.0184) | Acc_1: (99.33%) (28098/28288)\n",
      "Epoch: 37 | Batch_idx: 230 |  Loss_1: (0.0183) | Acc_1: (99.33%) (29370/29568)\n",
      "Epoch: 37 | Batch_idx: 240 |  Loss_1: (0.0182) | Acc_1: (99.33%) (30641/30848)\n",
      "Epoch: 37 | Batch_idx: 250 |  Loss_1: (0.0181) | Acc_1: (99.34%) (31916/32128)\n",
      "Epoch: 37 | Batch_idx: 260 |  Loss_1: (0.0179) | Acc_1: (99.34%) (33187/33408)\n",
      "Epoch: 37 | Batch_idx: 270 |  Loss_1: (0.0181) | Acc_1: (99.34%) (34459/34688)\n",
      "Epoch: 37 | Batch_idx: 280 |  Loss_1: (0.0181) | Acc_1: (99.33%) (35727/35968)\n",
      "Epoch: 37 | Batch_idx: 290 |  Loss_1: (0.0181) | Acc_1: (99.33%) (36999/37248)\n",
      "Epoch: 37 | Batch_idx: 300 |  Loss_1: (0.0181) | Acc_1: (99.33%) (38271/38528)\n",
      "Epoch: 37 | Batch_idx: 310 |  Loss_1: (0.0182) | Acc_1: (99.33%) (39540/39808)\n",
      "Epoch: 37 | Batch_idx: 320 |  Loss_1: (0.0184) | Acc_1: (99.32%) (40808/41088)\n",
      "Epoch: 37 | Batch_idx: 330 |  Loss_1: (0.0184) | Acc_1: (99.32%) (42082/42368)\n",
      "Epoch: 37 | Batch_idx: 340 |  Loss_1: (0.0186) | Acc_1: (99.32%) (43351/43648)\n",
      "Epoch: 37 | Batch_idx: 350 |  Loss_1: (0.0188) | Acc_1: (99.32%) (44622/44928)\n",
      "Epoch: 37 | Batch_idx: 360 |  Loss_1: (0.0190) | Acc_1: (99.31%) (45889/46208)\n",
      "Epoch: 37 | Batch_idx: 370 |  Loss_1: (0.0194) | Acc_1: (99.29%) (47153/47488)\n",
      "Epoch: 37 | Batch_idx: 380 |  Loss_1: (0.0195) | Acc_1: (99.29%) (48421/48768)\n",
      "Epoch: 37 | Batch_idx: 390 |  Loss_1: (0.0194) | Acc_1: (99.29%) (49646/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4504) | Acc: (91.17%) (9117/10000)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.0072) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.0185) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.0169) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.0181) | Acc_1: (99.32%) (3941/3968)\n",
      "Epoch: 38 | Batch_idx: 40 |  Loss_1: (0.0177) | Acc_1: (99.35%) (5214/5248)\n",
      "Epoch: 38 | Batch_idx: 50 |  Loss_1: (0.0170) | Acc_1: (99.37%) (6487/6528)\n",
      "Epoch: 38 | Batch_idx: 60 |  Loss_1: (0.0175) | Acc_1: (99.37%) (7759/7808)\n",
      "Epoch: 38 | Batch_idx: 70 |  Loss_1: (0.0166) | Acc_1: (99.41%) (9034/9088)\n",
      "Epoch: 38 | Batch_idx: 80 |  Loss_1: (0.0163) | Acc_1: (99.41%) (10307/10368)\n",
      "Epoch: 38 | Batch_idx: 90 |  Loss_1: (0.0165) | Acc_1: (99.38%) (11576/11648)\n",
      "Epoch: 38 | Batch_idx: 100 |  Loss_1: (0.0160) | Acc_1: (99.40%) (12851/12928)\n",
      "Epoch: 38 | Batch_idx: 110 |  Loss_1: (0.0155) | Acc_1: (99.44%) (14128/14208)\n",
      "Epoch: 38 | Batch_idx: 120 |  Loss_1: (0.0160) | Acc_1: (99.43%) (15400/15488)\n",
      "Epoch: 38 | Batch_idx: 130 |  Loss_1: (0.0158) | Acc_1: (99.42%) (16671/16768)\n",
      "Epoch: 38 | Batch_idx: 140 |  Loss_1: (0.0158) | Acc_1: (99.43%) (17945/18048)\n",
      "Epoch: 38 | Batch_idx: 150 |  Loss_1: (0.0163) | Acc_1: (99.42%) (19215/19328)\n",
      "Epoch: 38 | Batch_idx: 160 |  Loss_1: (0.0161) | Acc_1: (99.42%) (20489/20608)\n",
      "Epoch: 38 | Batch_idx: 170 |  Loss_1: (0.0163) | Acc_1: (99.42%) (21762/21888)\n",
      "Epoch: 38 | Batch_idx: 180 |  Loss_1: (0.0160) | Acc_1: (99.44%) (23038/23168)\n",
      "Epoch: 38 | Batch_idx: 190 |  Loss_1: (0.0161) | Acc_1: (99.44%) (24310/24448)\n",
      "Epoch: 38 | Batch_idx: 200 |  Loss_1: (0.0162) | Acc_1: (99.44%) (25584/25728)\n",
      "Epoch: 38 | Batch_idx: 210 |  Loss_1: (0.0165) | Acc_1: (99.42%) (26851/27008)\n",
      "Epoch: 38 | Batch_idx: 220 |  Loss_1: (0.0166) | Acc_1: (99.42%) (28125/28288)\n",
      "Epoch: 38 | Batch_idx: 230 |  Loss_1: (0.0167) | Acc_1: (99.42%) (29396/29568)\n",
      "Epoch: 38 | Batch_idx: 240 |  Loss_1: (0.0171) | Acc_1: (99.41%) (30667/30848)\n",
      "Epoch: 38 | Batch_idx: 250 |  Loss_1: (0.0175) | Acc_1: (99.40%) (31934/32128)\n",
      "Epoch: 38 | Batch_idx: 260 |  Loss_1: (0.0174) | Acc_1: (99.40%) (33206/33408)\n",
      "Epoch: 38 | Batch_idx: 270 |  Loss_1: (0.0173) | Acc_1: (99.40%) (34479/34688)\n",
      "Epoch: 38 | Batch_idx: 280 |  Loss_1: (0.0172) | Acc_1: (99.41%) (35755/35968)\n",
      "Epoch: 38 | Batch_idx: 290 |  Loss_1: (0.0170) | Acc_1: (99.41%) (37030/37248)\n",
      "Epoch: 38 | Batch_idx: 300 |  Loss_1: (0.0169) | Acc_1: (99.43%) (38307/38528)\n",
      "Epoch: 38 | Batch_idx: 310 |  Loss_1: (0.0170) | Acc_1: (99.43%) (39580/39808)\n",
      "Epoch: 38 | Batch_idx: 320 |  Loss_1: (0.0169) | Acc_1: (99.44%) (40856/41088)\n",
      "Epoch: 38 | Batch_idx: 330 |  Loss_1: (0.0173) | Acc_1: (99.42%) (42121/42368)\n",
      "Epoch: 38 | Batch_idx: 340 |  Loss_1: (0.0172) | Acc_1: (99.42%) (43395/43648)\n",
      "Epoch: 38 | Batch_idx: 350 |  Loss_1: (0.0173) | Acc_1: (99.42%) (44666/44928)\n",
      "Epoch: 38 | Batch_idx: 360 |  Loss_1: (0.0174) | Acc_1: (99.41%) (45937/46208)\n",
      "Epoch: 38 | Batch_idx: 370 |  Loss_1: (0.0173) | Acc_1: (99.41%) (47209/47488)\n",
      "Epoch: 38 | Batch_idx: 380 |  Loss_1: (0.0174) | Acc_1: (99.41%) (48479/48768)\n",
      "Epoch: 38 | Batch_idx: 390 |  Loss_1: (0.0174) | Acc_1: (99.41%) (49703/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4307) | Acc: (91.61%) (9161/10000)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.0249) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.0222) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.0181) | Acc_1: (99.37%) (2671/2688)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.0148) | Acc_1: (99.55%) (3950/3968)\n",
      "Epoch: 39 | Batch_idx: 40 |  Loss_1: (0.0165) | Acc_1: (99.52%) (5223/5248)\n",
      "Epoch: 39 | Batch_idx: 50 |  Loss_1: (0.0154) | Acc_1: (99.54%) (6498/6528)\n",
      "Epoch: 39 | Batch_idx: 60 |  Loss_1: (0.0149) | Acc_1: (99.55%) (7773/7808)\n",
      "Epoch: 39 | Batch_idx: 70 |  Loss_1: (0.0147) | Acc_1: (99.58%) (9050/9088)\n",
      "Epoch: 39 | Batch_idx: 80 |  Loss_1: (0.0142) | Acc_1: (99.60%) (10327/10368)\n",
      "Epoch: 39 | Batch_idx: 90 |  Loss_1: (0.0144) | Acc_1: (99.59%) (11600/11648)\n",
      "Epoch: 39 | Batch_idx: 100 |  Loss_1: (0.0149) | Acc_1: (99.57%) (12872/12928)\n",
      "Epoch: 39 | Batch_idx: 110 |  Loss_1: (0.0154) | Acc_1: (99.56%) (14145/14208)\n",
      "Epoch: 39 | Batch_idx: 120 |  Loss_1: (0.0149) | Acc_1: (99.59%) (15424/15488)\n",
      "Epoch: 39 | Batch_idx: 130 |  Loss_1: (0.0148) | Acc_1: (99.58%) (16697/16768)\n",
      "Epoch: 39 | Batch_idx: 140 |  Loss_1: (0.0147) | Acc_1: (99.58%) (17973/18048)\n",
      "Epoch: 39 | Batch_idx: 150 |  Loss_1: (0.0147) | Acc_1: (99.58%) (19247/19328)\n",
      "Epoch: 39 | Batch_idx: 160 |  Loss_1: (0.0146) | Acc_1: (99.58%) (20522/20608)\n",
      "Epoch: 39 | Batch_idx: 170 |  Loss_1: (0.0145) | Acc_1: (99.57%) (21794/21888)\n",
      "Epoch: 39 | Batch_idx: 180 |  Loss_1: (0.0148) | Acc_1: (99.56%) (23067/23168)\n",
      "Epoch: 39 | Batch_idx: 190 |  Loss_1: (0.0148) | Acc_1: (99.56%) (24340/24448)\n",
      "Epoch: 39 | Batch_idx: 200 |  Loss_1: (0.0145) | Acc_1: (99.56%) (25615/25728)\n",
      "Epoch: 39 | Batch_idx: 210 |  Loss_1: (0.0143) | Acc_1: (99.57%) (26892/27008)\n",
      "Epoch: 39 | Batch_idx: 220 |  Loss_1: (0.0143) | Acc_1: (99.56%) (28163/28288)\n",
      "Epoch: 39 | Batch_idx: 230 |  Loss_1: (0.0145) | Acc_1: (99.55%) (29435/29568)\n",
      "Epoch: 39 | Batch_idx: 240 |  Loss_1: (0.0149) | Acc_1: (99.53%) (30704/30848)\n",
      "Epoch: 39 | Batch_idx: 250 |  Loss_1: (0.0150) | Acc_1: (99.52%) (31975/32128)\n",
      "Epoch: 39 | Batch_idx: 260 |  Loss_1: (0.0149) | Acc_1: (99.52%) (33248/33408)\n",
      "Epoch: 39 | Batch_idx: 270 |  Loss_1: (0.0148) | Acc_1: (99.52%) (34521/34688)\n",
      "Epoch: 39 | Batch_idx: 280 |  Loss_1: (0.0146) | Acc_1: (99.52%) (35796/35968)\n",
      "Epoch: 39 | Batch_idx: 290 |  Loss_1: (0.0146) | Acc_1: (99.53%) (37072/37248)\n",
      "Epoch: 39 | Batch_idx: 300 |  Loss_1: (0.0147) | Acc_1: (99.52%) (38344/38528)\n",
      "Epoch: 39 | Batch_idx: 310 |  Loss_1: (0.0145) | Acc_1: (99.53%) (39621/39808)\n",
      "Epoch: 39 | Batch_idx: 320 |  Loss_1: (0.0145) | Acc_1: (99.53%) (40893/41088)\n",
      "Epoch: 39 | Batch_idx: 330 |  Loss_1: (0.0145) | Acc_1: (99.53%) (42168/42368)\n",
      "Epoch: 39 | Batch_idx: 340 |  Loss_1: (0.0145) | Acc_1: (99.53%) (43441/43648)\n",
      "Epoch: 39 | Batch_idx: 350 |  Loss_1: (0.0144) | Acc_1: (99.53%) (44716/44928)\n",
      "Epoch: 39 | Batch_idx: 360 |  Loss_1: (0.0146) | Acc_1: (99.52%) (45988/46208)\n",
      "Epoch: 39 | Batch_idx: 370 |  Loss_1: (0.0146) | Acc_1: (99.53%) (47263/47488)\n",
      "Epoch: 39 | Batch_idx: 380 |  Loss_1: (0.0145) | Acc_1: (99.53%) (48538/48768)\n",
      "Epoch: 39 | Batch_idx: 390 |  Loss_1: (0.0144) | Acc_1: (99.53%) (49764/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4565) | Acc: (91.39%) (9139/10000)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.0168) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.0111) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.0117) | Acc_1: (99.52%) (2675/2688)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.0109) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 40 | Batch_idx: 40 |  Loss_1: (0.0122) | Acc_1: (99.62%) (5228/5248)\n",
      "Epoch: 40 | Batch_idx: 50 |  Loss_1: (0.0113) | Acc_1: (99.63%) (6504/6528)\n",
      "Epoch: 40 | Batch_idx: 60 |  Loss_1: (0.0118) | Acc_1: (99.63%) (7779/7808)\n",
      "Epoch: 40 | Batch_idx: 70 |  Loss_1: (0.0120) | Acc_1: (99.63%) (9054/9088)\n",
      "Epoch: 40 | Batch_idx: 80 |  Loss_1: (0.0123) | Acc_1: (99.62%) (10329/10368)\n",
      "Epoch: 40 | Batch_idx: 90 |  Loss_1: (0.0127) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 40 | Batch_idx: 100 |  Loss_1: (0.0131) | Acc_1: (99.60%) (12876/12928)\n",
      "Epoch: 40 | Batch_idx: 110 |  Loss_1: (0.0127) | Acc_1: (99.61%) (14153/14208)\n",
      "Epoch: 40 | Batch_idx: 120 |  Loss_1: (0.0133) | Acc_1: (99.58%) (15423/15488)\n",
      "Epoch: 40 | Batch_idx: 130 |  Loss_1: (0.0133) | Acc_1: (99.59%) (16699/16768)\n",
      "Epoch: 40 | Batch_idx: 140 |  Loss_1: (0.0132) | Acc_1: (99.59%) (17974/18048)\n",
      "Epoch: 40 | Batch_idx: 150 |  Loss_1: (0.0130) | Acc_1: (99.60%) (19250/19328)\n",
      "Epoch: 40 | Batch_idx: 160 |  Loss_1: (0.0128) | Acc_1: (99.60%) (20526/20608)\n",
      "Epoch: 40 | Batch_idx: 170 |  Loss_1: (0.0136) | Acc_1: (99.58%) (21796/21888)\n",
      "Epoch: 40 | Batch_idx: 180 |  Loss_1: (0.0136) | Acc_1: (99.58%) (23071/23168)\n",
      "Epoch: 40 | Batch_idx: 190 |  Loss_1: (0.0138) | Acc_1: (99.57%) (24343/24448)\n",
      "Epoch: 40 | Batch_idx: 200 |  Loss_1: (0.0141) | Acc_1: (99.56%) (25614/25728)\n",
      "Epoch: 40 | Batch_idx: 210 |  Loss_1: (0.0140) | Acc_1: (99.56%) (26888/27008)\n",
      "Epoch: 40 | Batch_idx: 220 |  Loss_1: (0.0140) | Acc_1: (99.57%) (28165/28288)\n",
      "Epoch: 40 | Batch_idx: 230 |  Loss_1: (0.0141) | Acc_1: (99.56%) (29437/29568)\n",
      "Epoch: 40 | Batch_idx: 240 |  Loss_1: (0.0140) | Acc_1: (99.56%) (30711/30848)\n",
      "Epoch: 40 | Batch_idx: 250 |  Loss_1: (0.0142) | Acc_1: (99.55%) (31985/32128)\n",
      "Epoch: 40 | Batch_idx: 260 |  Loss_1: (0.0138) | Acc_1: (99.57%) (33264/33408)\n",
      "Epoch: 40 | Batch_idx: 270 |  Loss_1: (0.0138) | Acc_1: (99.57%) (34540/34688)\n",
      "Epoch: 40 | Batch_idx: 280 |  Loss_1: (0.0136) | Acc_1: (99.58%) (35817/35968)\n",
      "Epoch: 40 | Batch_idx: 290 |  Loss_1: (0.0137) | Acc_1: (99.57%) (37088/37248)\n",
      "Epoch: 40 | Batch_idx: 300 |  Loss_1: (0.0136) | Acc_1: (99.57%) (38362/38528)\n",
      "Epoch: 40 | Batch_idx: 310 |  Loss_1: (0.0137) | Acc_1: (99.56%) (39633/39808)\n",
      "Epoch: 40 | Batch_idx: 320 |  Loss_1: (0.0138) | Acc_1: (99.55%) (40902/41088)\n",
      "Epoch: 40 | Batch_idx: 330 |  Loss_1: (0.0139) | Acc_1: (99.54%) (42175/42368)\n",
      "Epoch: 40 | Batch_idx: 340 |  Loss_1: (0.0140) | Acc_1: (99.55%) (43451/43648)\n",
      "Epoch: 40 | Batch_idx: 350 |  Loss_1: (0.0141) | Acc_1: (99.54%) (44723/44928)\n",
      "Epoch: 40 | Batch_idx: 360 |  Loss_1: (0.0139) | Acc_1: (99.55%) (45998/46208)\n",
      "Epoch: 40 | Batch_idx: 370 |  Loss_1: (0.0139) | Acc_1: (99.54%) (47271/47488)\n",
      "Epoch: 40 | Batch_idx: 380 |  Loss_1: (0.0140) | Acc_1: (99.55%) (48547/48768)\n",
      "Epoch: 40 | Batch_idx: 390 |  Loss_1: (0.0142) | Acc_1: (99.53%) (49766/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4665) | Acc: (91.57%) (9157/10000)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.0262) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.0098) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.0108) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.0106) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 41 | Batch_idx: 40 |  Loss_1: (0.0129) | Acc_1: (99.49%) (5221/5248)\n",
      "Epoch: 41 | Batch_idx: 50 |  Loss_1: (0.0123) | Acc_1: (99.53%) (6497/6528)\n",
      "Epoch: 41 | Batch_idx: 60 |  Loss_1: (0.0132) | Acc_1: (99.54%) (7772/7808)\n",
      "Epoch: 41 | Batch_idx: 70 |  Loss_1: (0.0148) | Acc_1: (99.49%) (9042/9088)\n",
      "Epoch: 41 | Batch_idx: 80 |  Loss_1: (0.0161) | Acc_1: (99.48%) (10314/10368)\n",
      "Epoch: 41 | Batch_idx: 90 |  Loss_1: (0.0165) | Acc_1: (99.47%) (11586/11648)\n",
      "Epoch: 41 | Batch_idx: 100 |  Loss_1: (0.0165) | Acc_1: (99.46%) (12858/12928)\n",
      "Epoch: 41 | Batch_idx: 110 |  Loss_1: (0.0164) | Acc_1: (99.44%) (14128/14208)\n",
      "Epoch: 41 | Batch_idx: 120 |  Loss_1: (0.0162) | Acc_1: (99.43%) (15400/15488)\n",
      "Epoch: 41 | Batch_idx: 130 |  Loss_1: (0.0159) | Acc_1: (99.43%) (16673/16768)\n",
      "Epoch: 41 | Batch_idx: 140 |  Loss_1: (0.0158) | Acc_1: (99.43%) (17946/18048)\n",
      "Epoch: 41 | Batch_idx: 150 |  Loss_1: (0.0153) | Acc_1: (99.45%) (19222/19328)\n",
      "Epoch: 41 | Batch_idx: 160 |  Loss_1: (0.0151) | Acc_1: (99.46%) (20497/20608)\n",
      "Epoch: 41 | Batch_idx: 170 |  Loss_1: (0.0148) | Acc_1: (99.48%) (21775/21888)\n",
      "Epoch: 41 | Batch_idx: 180 |  Loss_1: (0.0145) | Acc_1: (99.50%) (23052/23168)\n",
      "Epoch: 41 | Batch_idx: 190 |  Loss_1: (0.0142) | Acc_1: (99.51%) (24328/24448)\n",
      "Epoch: 41 | Batch_idx: 200 |  Loss_1: (0.0142) | Acc_1: (99.51%) (25602/25728)\n",
      "Epoch: 41 | Batch_idx: 210 |  Loss_1: (0.0142) | Acc_1: (99.51%) (26875/27008)\n",
      "Epoch: 41 | Batch_idx: 220 |  Loss_1: (0.0140) | Acc_1: (99.51%) (28149/28288)\n",
      "Epoch: 41 | Batch_idx: 230 |  Loss_1: (0.0137) | Acc_1: (99.51%) (29424/29568)\n",
      "Epoch: 41 | Batch_idx: 240 |  Loss_1: (0.0136) | Acc_1: (99.51%) (30698/30848)\n",
      "Epoch: 41 | Batch_idx: 250 |  Loss_1: (0.0134) | Acc_1: (99.52%) (31973/32128)\n",
      "Epoch: 41 | Batch_idx: 260 |  Loss_1: (0.0134) | Acc_1: (99.51%) (33245/33408)\n",
      "Epoch: 41 | Batch_idx: 270 |  Loss_1: (0.0133) | Acc_1: (99.52%) (34520/34688)\n",
      "Epoch: 41 | Batch_idx: 280 |  Loss_1: (0.0132) | Acc_1: (99.52%) (35795/35968)\n",
      "Epoch: 41 | Batch_idx: 290 |  Loss_1: (0.0130) | Acc_1: (99.53%) (37073/37248)\n",
      "Epoch: 41 | Batch_idx: 300 |  Loss_1: (0.0128) | Acc_1: (99.54%) (38350/38528)\n",
      "Epoch: 41 | Batch_idx: 310 |  Loss_1: (0.0128) | Acc_1: (99.54%) (39624/39808)\n",
      "Epoch: 41 | Batch_idx: 320 |  Loss_1: (0.0126) | Acc_1: (99.55%) (40903/41088)\n",
      "Epoch: 41 | Batch_idx: 330 |  Loss_1: (0.0126) | Acc_1: (99.55%) (42176/42368)\n",
      "Epoch: 41 | Batch_idx: 340 |  Loss_1: (0.0127) | Acc_1: (99.55%) (43450/43648)\n",
      "Epoch: 41 | Batch_idx: 350 |  Loss_1: (0.0126) | Acc_1: (99.55%) (44726/44928)\n",
      "Epoch: 41 | Batch_idx: 360 |  Loss_1: (0.0125) | Acc_1: (99.56%) (46004/46208)\n",
      "Epoch: 41 | Batch_idx: 370 |  Loss_1: (0.0123) | Acc_1: (99.56%) (47279/47488)\n",
      "Epoch: 41 | Batch_idx: 380 |  Loss_1: (0.0122) | Acc_1: (99.57%) (48557/48768)\n",
      "Epoch: 41 | Batch_idx: 390 |  Loss_1: (0.0124) | Acc_1: (99.56%) (49782/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4549) | Acc: (92.02%) (9202/10000)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.0074) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.0085) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.0086) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.0083) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 42 | Batch_idx: 40 |  Loss_1: (0.0090) | Acc_1: (99.71%) (5233/5248)\n",
      "Epoch: 42 | Batch_idx: 50 |  Loss_1: (0.0099) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 42 | Batch_idx: 60 |  Loss_1: (0.0101) | Acc_1: (99.63%) (7779/7808)\n",
      "Epoch: 42 | Batch_idx: 70 |  Loss_1: (0.0105) | Acc_1: (99.61%) (9053/9088)\n",
      "Epoch: 42 | Batch_idx: 80 |  Loss_1: (0.0104) | Acc_1: (99.62%) (10329/10368)\n",
      "Epoch: 42 | Batch_idx: 90 |  Loss_1: (0.0108) | Acc_1: (99.59%) (11600/11648)\n",
      "Epoch: 42 | Batch_idx: 100 |  Loss_1: (0.0111) | Acc_1: (99.58%) (12874/12928)\n",
      "Epoch: 42 | Batch_idx: 110 |  Loss_1: (0.0113) | Acc_1: (99.58%) (14148/14208)\n",
      "Epoch: 42 | Batch_idx: 120 |  Loss_1: (0.0108) | Acc_1: (99.61%) (15427/15488)\n",
      "Epoch: 42 | Batch_idx: 130 |  Loss_1: (0.0109) | Acc_1: (99.60%) (16701/16768)\n",
      "Epoch: 42 | Batch_idx: 140 |  Loss_1: (0.0116) | Acc_1: (99.60%) (17975/18048)\n",
      "Epoch: 42 | Batch_idx: 150 |  Loss_1: (0.0114) | Acc_1: (99.60%) (19251/19328)\n",
      "Epoch: 42 | Batch_idx: 160 |  Loss_1: (0.0114) | Acc_1: (99.60%) (20525/20608)\n",
      "Epoch: 42 | Batch_idx: 170 |  Loss_1: (0.0111) | Acc_1: (99.61%) (21802/21888)\n",
      "Epoch: 42 | Batch_idx: 180 |  Loss_1: (0.0114) | Acc_1: (99.59%) (23074/23168)\n",
      "Epoch: 42 | Batch_idx: 190 |  Loss_1: (0.0112) | Acc_1: (99.61%) (24353/24448)\n",
      "Epoch: 42 | Batch_idx: 200 |  Loss_1: (0.0110) | Acc_1: (99.62%) (25631/25728)\n",
      "Epoch: 42 | Batch_idx: 210 |  Loss_1: (0.0111) | Acc_1: (99.61%) (26902/27008)\n",
      "Epoch: 42 | Batch_idx: 220 |  Loss_1: (0.0113) | Acc_1: (99.60%) (28175/28288)\n",
      "Epoch: 42 | Batch_idx: 230 |  Loss_1: (0.0112) | Acc_1: (99.60%) (29450/29568)\n",
      "Epoch: 42 | Batch_idx: 240 |  Loss_1: (0.0112) | Acc_1: (99.61%) (30728/30848)\n",
      "Epoch: 42 | Batch_idx: 250 |  Loss_1: (0.0113) | Acc_1: (99.61%) (32004/32128)\n",
      "Epoch: 42 | Batch_idx: 260 |  Loss_1: (0.0113) | Acc_1: (99.61%) (33279/33408)\n",
      "Epoch: 42 | Batch_idx: 270 |  Loss_1: (0.0116) | Acc_1: (99.61%) (34551/34688)\n",
      "Epoch: 42 | Batch_idx: 280 |  Loss_1: (0.0116) | Acc_1: (99.60%) (35825/35968)\n",
      "Epoch: 42 | Batch_idx: 290 |  Loss_1: (0.0116) | Acc_1: (99.60%) (37100/37248)\n",
      "Epoch: 42 | Batch_idx: 300 |  Loss_1: (0.0117) | Acc_1: (99.59%) (38371/38528)\n",
      "Epoch: 42 | Batch_idx: 310 |  Loss_1: (0.0120) | Acc_1: (99.58%) (39641/39808)\n",
      "Epoch: 42 | Batch_idx: 320 |  Loss_1: (0.0119) | Acc_1: (99.58%) (40917/41088)\n",
      "Epoch: 42 | Batch_idx: 330 |  Loss_1: (0.0120) | Acc_1: (99.58%) (42190/42368)\n",
      "Epoch: 42 | Batch_idx: 340 |  Loss_1: (0.0120) | Acc_1: (99.58%) (43465/43648)\n",
      "Epoch: 42 | Batch_idx: 350 |  Loss_1: (0.0119) | Acc_1: (99.58%) (44740/44928)\n",
      "Epoch: 42 | Batch_idx: 360 |  Loss_1: (0.0118) | Acc_1: (99.58%) (46016/46208)\n",
      "Epoch: 42 | Batch_idx: 370 |  Loss_1: (0.0117) | Acc_1: (99.59%) (47292/47488)\n",
      "Epoch: 42 | Batch_idx: 380 |  Loss_1: (0.0115) | Acc_1: (99.59%) (48570/48768)\n",
      "Epoch: 42 | Batch_idx: 390 |  Loss_1: (0.0115) | Acc_1: (99.60%) (49798/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4583) | Acc: (91.79%) (9179/10000)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.0017) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.0151) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.0123) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.0108) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 43 | Batch_idx: 40 |  Loss_1: (0.0107) | Acc_1: (99.64%) (5229/5248)\n",
      "Epoch: 43 | Batch_idx: 50 |  Loss_1: (0.0107) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 43 | Batch_idx: 60 |  Loss_1: (0.0111) | Acc_1: (99.65%) (7781/7808)\n",
      "Epoch: 43 | Batch_idx: 70 |  Loss_1: (0.0104) | Acc_1: (99.67%) (9058/9088)\n",
      "Epoch: 43 | Batch_idx: 80 |  Loss_1: (0.0099) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 43 | Batch_idx: 90 |  Loss_1: (0.0097) | Acc_1: (99.71%) (11614/11648)\n",
      "Epoch: 43 | Batch_idx: 100 |  Loss_1: (0.0096) | Acc_1: (99.71%) (12890/12928)\n",
      "Epoch: 43 | Batch_idx: 110 |  Loss_1: (0.0094) | Acc_1: (99.72%) (14168/14208)\n",
      "Epoch: 43 | Batch_idx: 120 |  Loss_1: (0.0092) | Acc_1: (99.72%) (15445/15488)\n",
      "Epoch: 43 | Batch_idx: 130 |  Loss_1: (0.0091) | Acc_1: (99.72%) (16721/16768)\n",
      "Epoch: 43 | Batch_idx: 140 |  Loss_1: (0.0094) | Acc_1: (99.72%) (17997/18048)\n",
      "Epoch: 43 | Batch_idx: 150 |  Loss_1: (0.0095) | Acc_1: (99.71%) (19272/19328)\n",
      "Epoch: 43 | Batch_idx: 160 |  Loss_1: (0.0094) | Acc_1: (99.71%) (20548/20608)\n",
      "Epoch: 43 | Batch_idx: 170 |  Loss_1: (0.0096) | Acc_1: (99.70%) (21822/21888)\n",
      "Epoch: 43 | Batch_idx: 180 |  Loss_1: (0.0095) | Acc_1: (99.70%) (23099/23168)\n",
      "Epoch: 43 | Batch_idx: 190 |  Loss_1: (0.0095) | Acc_1: (99.70%) (24374/24448)\n",
      "Epoch: 43 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.71%) (25654/25728)\n",
      "Epoch: 43 | Batch_idx: 210 |  Loss_1: (0.0093) | Acc_1: (99.69%) (26925/27008)\n",
      "Epoch: 43 | Batch_idx: 220 |  Loss_1: (0.0093) | Acc_1: (99.70%) (28202/28288)\n",
      "Epoch: 43 | Batch_idx: 230 |  Loss_1: (0.0093) | Acc_1: (99.69%) (29477/29568)\n",
      "Epoch: 43 | Batch_idx: 240 |  Loss_1: (0.0093) | Acc_1: (99.69%) (30752/30848)\n",
      "Epoch: 43 | Batch_idx: 250 |  Loss_1: (0.0092) | Acc_1: (99.69%) (32030/32128)\n",
      "Epoch: 43 | Batch_idx: 260 |  Loss_1: (0.0091) | Acc_1: (99.70%) (33307/33408)\n",
      "Epoch: 43 | Batch_idx: 270 |  Loss_1: (0.0092) | Acc_1: (99.70%) (34583/34688)\n",
      "Epoch: 43 | Batch_idx: 280 |  Loss_1: (0.0092) | Acc_1: (99.69%) (35857/35968)\n",
      "Epoch: 43 | Batch_idx: 290 |  Loss_1: (0.0094) | Acc_1: (99.68%) (37129/37248)\n",
      "Epoch: 43 | Batch_idx: 300 |  Loss_1: (0.0093) | Acc_1: (99.68%) (38406/38528)\n",
      "Epoch: 43 | Batch_idx: 310 |  Loss_1: (0.0098) | Acc_1: (99.67%) (39677/39808)\n",
      "Epoch: 43 | Batch_idx: 320 |  Loss_1: (0.0100) | Acc_1: (99.66%) (40949/41088)\n",
      "Epoch: 43 | Batch_idx: 330 |  Loss_1: (0.0099) | Acc_1: (99.66%) (42226/42368)\n",
      "Epoch: 43 | Batch_idx: 340 |  Loss_1: (0.0102) | Acc_1: (99.66%) (43499/43648)\n",
      "Epoch: 43 | Batch_idx: 350 |  Loss_1: (0.0102) | Acc_1: (99.66%) (44774/44928)\n",
      "Epoch: 43 | Batch_idx: 360 |  Loss_1: (0.0102) | Acc_1: (99.66%) (46050/46208)\n",
      "Epoch: 43 | Batch_idx: 370 |  Loss_1: (0.0101) | Acc_1: (99.67%) (47329/47488)\n",
      "Epoch: 43 | Batch_idx: 380 |  Loss_1: (0.0101) | Acc_1: (99.67%) (48606/48768)\n",
      "Epoch: 43 | Batch_idx: 390 |  Loss_1: (0.0101) | Acc_1: (99.67%) (49834/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4567) | Acc: (91.95%) (9195/10000)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.0203) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.0120) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.0138) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.0111) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 44 | Batch_idx: 40 |  Loss_1: (0.0096) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 44 | Batch_idx: 50 |  Loss_1: (0.0100) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 44 | Batch_idx: 60 |  Loss_1: (0.0097) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 44 | Batch_idx: 70 |  Loss_1: (0.0091) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 44 | Batch_idx: 80 |  Loss_1: (0.0085) | Acc_1: (99.73%) (10340/10368)\n",
      "Epoch: 44 | Batch_idx: 90 |  Loss_1: (0.0085) | Acc_1: (99.73%) (11617/11648)\n",
      "Epoch: 44 | Batch_idx: 100 |  Loss_1: (0.0082) | Acc_1: (99.74%) (12895/12928)\n",
      "Epoch: 44 | Batch_idx: 110 |  Loss_1: (0.0092) | Acc_1: (99.72%) (14168/14208)\n",
      "Epoch: 44 | Batch_idx: 120 |  Loss_1: (0.0089) | Acc_1: (99.72%) (15444/15488)\n",
      "Epoch: 44 | Batch_idx: 130 |  Loss_1: (0.0089) | Acc_1: (99.71%) (16719/16768)\n",
      "Epoch: 44 | Batch_idx: 140 |  Loss_1: (0.0090) | Acc_1: (99.70%) (17994/18048)\n",
      "Epoch: 44 | Batch_idx: 150 |  Loss_1: (0.0092) | Acc_1: (99.69%) (19269/19328)\n",
      "Epoch: 44 | Batch_idx: 160 |  Loss_1: (0.0091) | Acc_1: (99.70%) (20546/20608)\n",
      "Epoch: 44 | Batch_idx: 170 |  Loss_1: (0.0090) | Acc_1: (99.71%) (21824/21888)\n",
      "Epoch: 44 | Batch_idx: 180 |  Loss_1: (0.0091) | Acc_1: (99.70%) (23099/23168)\n",
      "Epoch: 44 | Batch_idx: 190 |  Loss_1: (0.0093) | Acc_1: (99.70%) (24374/24448)\n",
      "Epoch: 44 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.70%) (25650/25728)\n",
      "Epoch: 44 | Batch_idx: 210 |  Loss_1: (0.0093) | Acc_1: (99.69%) (26923/27008)\n",
      "Epoch: 44 | Batch_idx: 220 |  Loss_1: (0.0097) | Acc_1: (99.67%) (28195/28288)\n",
      "Epoch: 44 | Batch_idx: 230 |  Loss_1: (0.0097) | Acc_1: (99.67%) (29471/29568)\n",
      "Epoch: 44 | Batch_idx: 240 |  Loss_1: (0.0096) | Acc_1: (99.67%) (30746/30848)\n",
      "Epoch: 44 | Batch_idx: 250 |  Loss_1: (0.0096) | Acc_1: (99.67%) (32023/32128)\n",
      "Epoch: 44 | Batch_idx: 260 |  Loss_1: (0.0096) | Acc_1: (99.68%) (33300/33408)\n",
      "Epoch: 44 | Batch_idx: 270 |  Loss_1: (0.0096) | Acc_1: (99.67%) (34575/34688)\n",
      "Epoch: 44 | Batch_idx: 280 |  Loss_1: (0.0100) | Acc_1: (99.66%) (35845/35968)\n",
      "Epoch: 44 | Batch_idx: 290 |  Loss_1: (0.0101) | Acc_1: (99.66%) (37120/37248)\n",
      "Epoch: 44 | Batch_idx: 300 |  Loss_1: (0.0101) | Acc_1: (99.65%) (38395/38528)\n",
      "Epoch: 44 | Batch_idx: 310 |  Loss_1: (0.0102) | Acc_1: (99.65%) (39669/39808)\n",
      "Epoch: 44 | Batch_idx: 320 |  Loss_1: (0.0103) | Acc_1: (99.65%) (40944/41088)\n",
      "Epoch: 44 | Batch_idx: 330 |  Loss_1: (0.0103) | Acc_1: (99.65%) (42220/42368)\n",
      "Epoch: 44 | Batch_idx: 340 |  Loss_1: (0.0103) | Acc_1: (99.65%) (43495/43648)\n",
      "Epoch: 44 | Batch_idx: 350 |  Loss_1: (0.0104) | Acc_1: (99.65%) (44769/44928)\n",
      "Epoch: 44 | Batch_idx: 360 |  Loss_1: (0.0103) | Acc_1: (99.64%) (46043/46208)\n",
      "Epoch: 44 | Batch_idx: 370 |  Loss_1: (0.0102) | Acc_1: (99.65%) (47321/47488)\n",
      "Epoch: 44 | Batch_idx: 380 |  Loss_1: (0.0102) | Acc_1: (99.65%) (48597/48768)\n",
      "Epoch: 44 | Batch_idx: 390 |  Loss_1: (0.0102) | Acc_1: (99.65%) (49826/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4753) | Acc: (91.90%) (9190/10000)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.0031) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.0089) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.0114) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.0118) | Acc_1: (99.52%) (3949/3968)\n",
      "Epoch: 45 | Batch_idx: 40 |  Loss_1: (0.0117) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 45 | Batch_idx: 50 |  Loss_1: (0.0107) | Acc_1: (99.62%) (6503/6528)\n",
      "Epoch: 45 | Batch_idx: 60 |  Loss_1: (0.0107) | Acc_1: (99.63%) (7779/7808)\n",
      "Epoch: 45 | Batch_idx: 70 |  Loss_1: (0.0103) | Acc_1: (99.66%) (9057/9088)\n",
      "Epoch: 45 | Batch_idx: 80 |  Loss_1: (0.0109) | Acc_1: (99.62%) (10329/10368)\n",
      "Epoch: 45 | Batch_idx: 90 |  Loss_1: (0.0101) | Acc_1: (99.66%) (11608/11648)\n",
      "Epoch: 45 | Batch_idx: 100 |  Loss_1: (0.0098) | Acc_1: (99.67%) (12885/12928)\n",
      "Epoch: 45 | Batch_idx: 110 |  Loss_1: (0.0097) | Acc_1: (99.67%) (14161/14208)\n",
      "Epoch: 45 | Batch_idx: 120 |  Loss_1: (0.0105) | Acc_1: (99.64%) (15432/15488)\n",
      "Epoch: 45 | Batch_idx: 130 |  Loss_1: (0.0103) | Acc_1: (99.65%) (16709/16768)\n",
      "Epoch: 45 | Batch_idx: 140 |  Loss_1: (0.0104) | Acc_1: (99.66%) (17986/18048)\n",
      "Epoch: 45 | Batch_idx: 150 |  Loss_1: (0.0100) | Acc_1: (99.67%) (19265/19328)\n",
      "Epoch: 45 | Batch_idx: 160 |  Loss_1: (0.0099) | Acc_1: (99.67%) (20540/20608)\n",
      "Epoch: 45 | Batch_idx: 170 |  Loss_1: (0.0097) | Acc_1: (99.67%) (21816/21888)\n",
      "Epoch: 45 | Batch_idx: 180 |  Loss_1: (0.0096) | Acc_1: (99.68%) (23094/23168)\n",
      "Epoch: 45 | Batch_idx: 190 |  Loss_1: (0.0094) | Acc_1: (99.69%) (24372/24448)\n",
      "Epoch: 45 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.69%) (25649/25728)\n",
      "Epoch: 45 | Batch_idx: 210 |  Loss_1: (0.0093) | Acc_1: (99.70%) (26927/27008)\n",
      "Epoch: 45 | Batch_idx: 220 |  Loss_1: (0.0090) | Acc_1: (99.71%) (28205/28288)\n",
      "Epoch: 45 | Batch_idx: 230 |  Loss_1: (0.0089) | Acc_1: (99.72%) (29484/29568)\n",
      "Epoch: 45 | Batch_idx: 240 |  Loss_1: (0.0089) | Acc_1: (99.71%) (30759/30848)\n",
      "Epoch: 45 | Batch_idx: 250 |  Loss_1: (0.0089) | Acc_1: (99.71%) (32035/32128)\n",
      "Epoch: 45 | Batch_idx: 260 |  Loss_1: (0.0087) | Acc_1: (99.72%) (33315/33408)\n",
      "Epoch: 45 | Batch_idx: 270 |  Loss_1: (0.0087) | Acc_1: (99.72%) (34590/34688)\n",
      "Epoch: 45 | Batch_idx: 280 |  Loss_1: (0.0086) | Acc_1: (99.72%) (35868/35968)\n",
      "Epoch: 45 | Batch_idx: 290 |  Loss_1: (0.0085) | Acc_1: (99.73%) (37147/37248)\n",
      "Epoch: 45 | Batch_idx: 300 |  Loss_1: (0.0084) | Acc_1: (99.73%) (38425/38528)\n",
      "Epoch: 45 | Batch_idx: 310 |  Loss_1: (0.0083) | Acc_1: (99.73%) (39702/39808)\n",
      "Epoch: 45 | Batch_idx: 320 |  Loss_1: (0.0084) | Acc_1: (99.73%) (40977/41088)\n",
      "Epoch: 45 | Batch_idx: 330 |  Loss_1: (0.0084) | Acc_1: (99.73%) (42254/42368)\n",
      "Epoch: 45 | Batch_idx: 340 |  Loss_1: (0.0084) | Acc_1: (99.73%) (43529/43648)\n",
      "Epoch: 45 | Batch_idx: 350 |  Loss_1: (0.0086) | Acc_1: (99.72%) (44804/44928)\n",
      "Epoch: 45 | Batch_idx: 360 |  Loss_1: (0.0085) | Acc_1: (99.73%) (46081/46208)\n",
      "Epoch: 45 | Batch_idx: 370 |  Loss_1: (0.0084) | Acc_1: (99.73%) (47359/47488)\n",
      "Epoch: 45 | Batch_idx: 380 |  Loss_1: (0.0083) | Acc_1: (99.73%) (48638/48768)\n",
      "Epoch: 45 | Batch_idx: 390 |  Loss_1: (0.0083) | Acc_1: (99.73%) (49867/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4690) | Acc: (92.13%) (9213/10000)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.0078) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.0063) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.0063) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.0084) | Acc_1: (99.62%) (3953/3968)\n",
      "Epoch: 46 | Batch_idx: 40 |  Loss_1: (0.0092) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 46 | Batch_idx: 50 |  Loss_1: (0.0084) | Acc_1: (99.71%) (6509/6528)\n",
      "Epoch: 46 | Batch_idx: 60 |  Loss_1: (0.0080) | Acc_1: (99.73%) (7787/7808)\n",
      "Epoch: 46 | Batch_idx: 70 |  Loss_1: (0.0084) | Acc_1: (99.74%) (9064/9088)\n",
      "Epoch: 46 | Batch_idx: 80 |  Loss_1: (0.0083) | Acc_1: (99.73%) (10340/10368)\n",
      "Epoch: 46 | Batch_idx: 90 |  Loss_1: (0.0093) | Acc_1: (99.70%) (11613/11648)\n",
      "Epoch: 46 | Batch_idx: 100 |  Loss_1: (0.0091) | Acc_1: (99.71%) (12890/12928)\n",
      "Epoch: 46 | Batch_idx: 110 |  Loss_1: (0.0096) | Acc_1: (99.70%) (14165/14208)\n",
      "Epoch: 46 | Batch_idx: 120 |  Loss_1: (0.0100) | Acc_1: (99.67%) (15437/15488)\n",
      "Epoch: 46 | Batch_idx: 130 |  Loss_1: (0.0096) | Acc_1: (99.69%) (16716/16768)\n",
      "Epoch: 46 | Batch_idx: 140 |  Loss_1: (0.0095) | Acc_1: (99.69%) (17992/18048)\n",
      "Epoch: 46 | Batch_idx: 150 |  Loss_1: (0.0095) | Acc_1: (99.69%) (19268/19328)\n",
      "Epoch: 46 | Batch_idx: 160 |  Loss_1: (0.0092) | Acc_1: (99.70%) (20547/20608)\n",
      "Epoch: 46 | Batch_idx: 170 |  Loss_1: (0.0092) | Acc_1: (99.70%) (21823/21888)\n",
      "Epoch: 46 | Batch_idx: 180 |  Loss_1: (0.0089) | Acc_1: (99.72%) (23102/23168)\n",
      "Epoch: 46 | Batch_idx: 190 |  Loss_1: (0.0089) | Acc_1: (99.71%) (24376/24448)\n",
      "Epoch: 46 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.70%) (25651/25728)\n",
      "Epoch: 46 | Batch_idx: 210 |  Loss_1: (0.0091) | Acc_1: (99.70%) (26928/27008)\n",
      "Epoch: 46 | Batch_idx: 220 |  Loss_1: (0.0092) | Acc_1: (99.69%) (28201/28288)\n",
      "Epoch: 46 | Batch_idx: 230 |  Loss_1: (0.0092) | Acc_1: (99.70%) (29479/29568)\n",
      "Epoch: 46 | Batch_idx: 240 |  Loss_1: (0.0091) | Acc_1: (99.70%) (30756/30848)\n",
      "Epoch: 46 | Batch_idx: 250 |  Loss_1: (0.0090) | Acc_1: (99.70%) (32032/32128)\n",
      "Epoch: 46 | Batch_idx: 260 |  Loss_1: (0.0089) | Acc_1: (99.71%) (33310/33408)\n",
      "Epoch: 46 | Batch_idx: 270 |  Loss_1: (0.0089) | Acc_1: (99.71%) (34586/34688)\n",
      "Epoch: 46 | Batch_idx: 280 |  Loss_1: (0.0087) | Acc_1: (99.71%) (35862/35968)\n",
      "Epoch: 46 | Batch_idx: 290 |  Loss_1: (0.0086) | Acc_1: (99.71%) (37140/37248)\n",
      "Epoch: 46 | Batch_idx: 300 |  Loss_1: (0.0085) | Acc_1: (99.72%) (38420/38528)\n",
      "Epoch: 46 | Batch_idx: 310 |  Loss_1: (0.0085) | Acc_1: (99.72%) (39697/39808)\n",
      "Epoch: 46 | Batch_idx: 320 |  Loss_1: (0.0084) | Acc_1: (99.72%) (40975/41088)\n",
      "Epoch: 46 | Batch_idx: 330 |  Loss_1: (0.0084) | Acc_1: (99.73%) (42252/42368)\n",
      "Epoch: 46 | Batch_idx: 340 |  Loss_1: (0.0084) | Acc_1: (99.72%) (43526/43648)\n",
      "Epoch: 46 | Batch_idx: 350 |  Loss_1: (0.0084) | Acc_1: (99.72%) (44803/44928)\n",
      "Epoch: 46 | Batch_idx: 360 |  Loss_1: (0.0083) | Acc_1: (99.73%) (46081/46208)\n",
      "Epoch: 46 | Batch_idx: 370 |  Loss_1: (0.0082) | Acc_1: (99.73%) (47360/47488)\n",
      "Epoch: 46 | Batch_idx: 380 |  Loss_1: (0.0081) | Acc_1: (99.73%) (48638/48768)\n",
      "Epoch: 46 | Batch_idx: 390 |  Loss_1: (0.0081) | Acc_1: (99.73%) (49866/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4645) | Acc: (92.18%) (9218/10000)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.0773) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.0105) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.0112) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.0109) | Acc_1: (99.62%) (3953/3968)\n",
      "Epoch: 47 | Batch_idx: 40 |  Loss_1: (0.0105) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 47 | Batch_idx: 50 |  Loss_1: (0.0109) | Acc_1: (99.66%) (6506/6528)\n",
      "Epoch: 47 | Batch_idx: 60 |  Loss_1: (0.0103) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 47 | Batch_idx: 70 |  Loss_1: (0.0096) | Acc_1: (99.69%) (9060/9088)\n",
      "Epoch: 47 | Batch_idx: 80 |  Loss_1: (0.0095) | Acc_1: (99.70%) (10337/10368)\n",
      "Epoch: 47 | Batch_idx: 90 |  Loss_1: (0.0088) | Acc_1: (99.73%) (11617/11648)\n",
      "Epoch: 47 | Batch_idx: 100 |  Loss_1: (0.0084) | Acc_1: (99.74%) (12894/12928)\n",
      "Epoch: 47 | Batch_idx: 110 |  Loss_1: (0.0081) | Acc_1: (99.75%) (14173/14208)\n",
      "Epoch: 47 | Batch_idx: 120 |  Loss_1: (0.0083) | Acc_1: (99.74%) (15448/15488)\n",
      "Epoch: 47 | Batch_idx: 130 |  Loss_1: (0.0082) | Acc_1: (99.74%) (16725/16768)\n",
      "Epoch: 47 | Batch_idx: 140 |  Loss_1: (0.0081) | Acc_1: (99.75%) (18002/18048)\n",
      "Epoch: 47 | Batch_idx: 150 |  Loss_1: (0.0083) | Acc_1: (99.75%) (19280/19328)\n",
      "Epoch: 47 | Batch_idx: 160 |  Loss_1: (0.0084) | Acc_1: (99.74%) (20555/20608)\n",
      "Epoch: 47 | Batch_idx: 170 |  Loss_1: (0.0086) | Acc_1: (99.74%) (21831/21888)\n",
      "Epoch: 47 | Batch_idx: 180 |  Loss_1: (0.0086) | Acc_1: (99.73%) (23106/23168)\n",
      "Epoch: 47 | Batch_idx: 190 |  Loss_1: (0.0084) | Acc_1: (99.74%) (24384/24448)\n",
      "Epoch: 47 | Batch_idx: 200 |  Loss_1: (0.0084) | Acc_1: (99.74%) (25661/25728)\n",
      "Epoch: 47 | Batch_idx: 210 |  Loss_1: (0.0084) | Acc_1: (99.74%) (26937/27008)\n",
      "Epoch: 47 | Batch_idx: 220 |  Loss_1: (0.0085) | Acc_1: (99.74%) (28214/28288)\n",
      "Epoch: 47 | Batch_idx: 230 |  Loss_1: (0.0084) | Acc_1: (99.75%) (29493/29568)\n",
      "Epoch: 47 | Batch_idx: 240 |  Loss_1: (0.0084) | Acc_1: (99.74%) (30767/30848)\n",
      "Epoch: 47 | Batch_idx: 250 |  Loss_1: (0.0085) | Acc_1: (99.73%) (32042/32128)\n",
      "Epoch: 47 | Batch_idx: 260 |  Loss_1: (0.0085) | Acc_1: (99.73%) (33319/33408)\n",
      "Epoch: 47 | Batch_idx: 270 |  Loss_1: (0.0087) | Acc_1: (99.73%) (34595/34688)\n",
      "Epoch: 47 | Batch_idx: 280 |  Loss_1: (0.0087) | Acc_1: (99.74%) (35873/35968)\n",
      "Epoch: 47 | Batch_idx: 290 |  Loss_1: (0.0087) | Acc_1: (99.74%) (37150/37248)\n",
      "Epoch: 47 | Batch_idx: 300 |  Loss_1: (0.0087) | Acc_1: (99.73%) (38425/38528)\n",
      "Epoch: 47 | Batch_idx: 310 |  Loss_1: (0.0087) | Acc_1: (99.73%) (39701/39808)\n",
      "Epoch: 47 | Batch_idx: 320 |  Loss_1: (0.0086) | Acc_1: (99.73%) (40979/41088)\n",
      "Epoch: 47 | Batch_idx: 330 |  Loss_1: (0.0085) | Acc_1: (99.74%) (42257/42368)\n",
      "Epoch: 47 | Batch_idx: 340 |  Loss_1: (0.0084) | Acc_1: (99.74%) (43535/43648)\n",
      "Epoch: 47 | Batch_idx: 350 |  Loss_1: (0.0084) | Acc_1: (99.74%) (44812/44928)\n",
      "Epoch: 47 | Batch_idx: 360 |  Loss_1: (0.0085) | Acc_1: (99.74%) (46090/46208)\n",
      "Epoch: 47 | Batch_idx: 370 |  Loss_1: (0.0084) | Acc_1: (99.75%) (47367/47488)\n",
      "Epoch: 47 | Batch_idx: 380 |  Loss_1: (0.0083) | Acc_1: (99.75%) (48644/48768)\n",
      "Epoch: 47 | Batch_idx: 390 |  Loss_1: (0.0082) | Acc_1: (99.75%) (49875/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4535) | Acc: (92.32%) (9232/10000)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.0019) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.0064) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.0078) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.0081) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 48 | Batch_idx: 40 |  Loss_1: (0.0079) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 48 | Batch_idx: 50 |  Loss_1: (0.0074) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 48 | Batch_idx: 60 |  Loss_1: (0.0075) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 48 | Batch_idx: 70 |  Loss_1: (0.0074) | Acc_1: (99.75%) (9065/9088)\n",
      "Epoch: 48 | Batch_idx: 80 |  Loss_1: (0.0075) | Acc_1: (99.75%) (10342/10368)\n",
      "Epoch: 48 | Batch_idx: 90 |  Loss_1: (0.0073) | Acc_1: (99.76%) (11620/11648)\n",
      "Epoch: 48 | Batch_idx: 100 |  Loss_1: (0.0076) | Acc_1: (99.74%) (12894/12928)\n",
      "Epoch: 48 | Batch_idx: 110 |  Loss_1: (0.0072) | Acc_1: (99.75%) (14173/14208)\n",
      "Epoch: 48 | Batch_idx: 120 |  Loss_1: (0.0076) | Acc_1: (99.74%) (15448/15488)\n",
      "Epoch: 48 | Batch_idx: 130 |  Loss_1: (0.0074) | Acc_1: (99.76%) (16727/16768)\n",
      "Epoch: 48 | Batch_idx: 140 |  Loss_1: (0.0072) | Acc_1: (99.77%) (18007/18048)\n",
      "Epoch: 48 | Batch_idx: 150 |  Loss_1: (0.0070) | Acc_1: (99.77%) (19284/19328)\n",
      "Epoch: 48 | Batch_idx: 160 |  Loss_1: (0.0067) | Acc_1: (99.79%) (20564/20608)\n",
      "Epoch: 48 | Batch_idx: 170 |  Loss_1: (0.0066) | Acc_1: (99.79%) (21843/21888)\n",
      "Epoch: 48 | Batch_idx: 180 |  Loss_1: (0.0066) | Acc_1: (99.79%) (23120/23168)\n",
      "Epoch: 48 | Batch_idx: 190 |  Loss_1: (0.0067) | Acc_1: (99.79%) (24396/24448)\n",
      "Epoch: 48 | Batch_idx: 200 |  Loss_1: (0.0068) | Acc_1: (99.78%) (25672/25728)\n",
      "Epoch: 48 | Batch_idx: 210 |  Loss_1: (0.0071) | Acc_1: (99.78%) (26948/27008)\n",
      "Epoch: 48 | Batch_idx: 220 |  Loss_1: (0.0072) | Acc_1: (99.77%) (28224/28288)\n",
      "Epoch: 48 | Batch_idx: 230 |  Loss_1: (0.0071) | Acc_1: (99.78%) (29502/29568)\n",
      "Epoch: 48 | Batch_idx: 240 |  Loss_1: (0.0070) | Acc_1: (99.78%) (30780/30848)\n",
      "Epoch: 48 | Batch_idx: 250 |  Loss_1: (0.0071) | Acc_1: (99.78%) (32057/32128)\n",
      "Epoch: 48 | Batch_idx: 260 |  Loss_1: (0.0070) | Acc_1: (99.78%) (33336/33408)\n",
      "Epoch: 48 | Batch_idx: 270 |  Loss_1: (0.0072) | Acc_1: (99.77%) (34609/34688)\n",
      "Epoch: 48 | Batch_idx: 280 |  Loss_1: (0.0071) | Acc_1: (99.78%) (35888/35968)\n",
      "Epoch: 48 | Batch_idx: 290 |  Loss_1: (0.0073) | Acc_1: (99.78%) (37165/37248)\n",
      "Epoch: 48 | Batch_idx: 300 |  Loss_1: (0.0072) | Acc_1: (99.78%) (38444/38528)\n",
      "Epoch: 48 | Batch_idx: 310 |  Loss_1: (0.0073) | Acc_1: (99.78%) (39719/39808)\n",
      "Epoch: 48 | Batch_idx: 320 |  Loss_1: (0.0073) | Acc_1: (99.77%) (40994/41088)\n",
      "Epoch: 48 | Batch_idx: 330 |  Loss_1: (0.0073) | Acc_1: (99.77%) (42271/42368)\n",
      "Epoch: 48 | Batch_idx: 340 |  Loss_1: (0.0072) | Acc_1: (99.78%) (43550/43648)\n",
      "Epoch: 48 | Batch_idx: 350 |  Loss_1: (0.0072) | Acc_1: (99.78%) (44828/44928)\n",
      "Epoch: 48 | Batch_idx: 360 |  Loss_1: (0.0072) | Acc_1: (99.78%) (46106/46208)\n",
      "Epoch: 48 | Batch_idx: 370 |  Loss_1: (0.0072) | Acc_1: (99.78%) (47383/47488)\n",
      "Epoch: 48 | Batch_idx: 380 |  Loss_1: (0.0072) | Acc_1: (99.77%) (48658/48768)\n",
      "Epoch: 48 | Batch_idx: 390 |  Loss_1: (0.0072) | Acc_1: (99.78%) (49888/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4585) | Acc: (92.04%) (9204/10000)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.0012) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.0082) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.0073) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.0090) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 49 | Batch_idx: 40 |  Loss_1: (0.0080) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 49 | Batch_idx: 50 |  Loss_1: (0.0076) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 49 | Batch_idx: 60 |  Loss_1: (0.0070) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 49 | Batch_idx: 70 |  Loss_1: (0.0063) | Acc_1: (99.80%) (9070/9088)\n",
      "Epoch: 49 | Batch_idx: 80 |  Loss_1: (0.0065) | Acc_1: (99.81%) (10348/10368)\n",
      "Epoch: 49 | Batch_idx: 90 |  Loss_1: (0.0062) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 49 | Batch_idx: 100 |  Loss_1: (0.0062) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 49 | Batch_idx: 110 |  Loss_1: (0.0065) | Acc_1: (99.80%) (14180/14208)\n",
      "Epoch: 49 | Batch_idx: 120 |  Loss_1: (0.0061) | Acc_1: (99.82%) (15460/15488)\n",
      "Epoch: 49 | Batch_idx: 130 |  Loss_1: (0.0062) | Acc_1: (99.80%) (16735/16768)\n",
      "Epoch: 49 | Batch_idx: 140 |  Loss_1: (0.0061) | Acc_1: (99.81%) (18013/18048)\n",
      "Epoch: 49 | Batch_idx: 150 |  Loss_1: (0.0061) | Acc_1: (99.81%) (19291/19328)\n",
      "Epoch: 49 | Batch_idx: 160 |  Loss_1: (0.0061) | Acc_1: (99.81%) (20569/20608)\n",
      "Epoch: 49 | Batch_idx: 170 |  Loss_1: (0.0061) | Acc_1: (99.82%) (21848/21888)\n",
      "Epoch: 49 | Batch_idx: 180 |  Loss_1: (0.0063) | Acc_1: (99.81%) (23123/23168)\n",
      "Epoch: 49 | Batch_idx: 190 |  Loss_1: (0.0063) | Acc_1: (99.80%) (24400/24448)\n",
      "Epoch: 49 | Batch_idx: 200 |  Loss_1: (0.0062) | Acc_1: (99.81%) (25678/25728)\n",
      "Epoch: 49 | Batch_idx: 210 |  Loss_1: (0.0062) | Acc_1: (99.81%) (26957/27008)\n",
      "Epoch: 49 | Batch_idx: 220 |  Loss_1: (0.0062) | Acc_1: (99.81%) (28234/28288)\n",
      "Epoch: 49 | Batch_idx: 230 |  Loss_1: (0.0063) | Acc_1: (99.81%) (29511/29568)\n",
      "Epoch: 49 | Batch_idx: 240 |  Loss_1: (0.0062) | Acc_1: (99.81%) (30789/30848)\n",
      "Epoch: 49 | Batch_idx: 250 |  Loss_1: (0.0062) | Acc_1: (99.81%) (32067/32128)\n",
      "Epoch: 49 | Batch_idx: 260 |  Loss_1: (0.0062) | Acc_1: (99.81%) (33345/33408)\n",
      "Epoch: 49 | Batch_idx: 270 |  Loss_1: (0.0061) | Acc_1: (99.82%) (34625/34688)\n",
      "Epoch: 49 | Batch_idx: 280 |  Loss_1: (0.0061) | Acc_1: (99.82%) (35902/35968)\n",
      "Epoch: 49 | Batch_idx: 290 |  Loss_1: (0.0062) | Acc_1: (99.81%) (37176/37248)\n",
      "Epoch: 49 | Batch_idx: 300 |  Loss_1: (0.0064) | Acc_1: (99.80%) (38451/38528)\n",
      "Epoch: 49 | Batch_idx: 310 |  Loss_1: (0.0063) | Acc_1: (99.81%) (39731/39808)\n",
      "Epoch: 49 | Batch_idx: 320 |  Loss_1: (0.0063) | Acc_1: (99.81%) (41008/41088)\n",
      "Epoch: 49 | Batch_idx: 330 |  Loss_1: (0.0063) | Acc_1: (99.80%) (42285/42368)\n",
      "Epoch: 49 | Batch_idx: 340 |  Loss_1: (0.0064) | Acc_1: (99.80%) (43561/43648)\n",
      "Epoch: 49 | Batch_idx: 350 |  Loss_1: (0.0063) | Acc_1: (99.80%) (44839/44928)\n",
      "Epoch: 49 | Batch_idx: 360 |  Loss_1: (0.0063) | Acc_1: (99.80%) (46117/46208)\n",
      "Epoch: 49 | Batch_idx: 370 |  Loss_1: (0.0063) | Acc_1: (99.80%) (47393/47488)\n",
      "Epoch: 49 | Batch_idx: 380 |  Loss_1: (0.0064) | Acc_1: (99.80%) (48670/48768)\n",
      "Epoch: 49 | Batch_idx: 390 |  Loss_1: (0.0065) | Acc_1: (99.80%) (49898/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4597) | Acc: (92.13%) (9213/10000)\n",
      "Epoch: 50 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 50 | Batch_idx: 10 |  Loss_1: (0.0047) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 50 | Batch_idx: 20 |  Loss_1: (0.0120) | Acc_1: (99.52%) (2675/2688)\n",
      "Epoch: 50 | Batch_idx: 30 |  Loss_1: (0.0193) | Acc_1: (99.32%) (3941/3968)\n",
      "Epoch: 50 | Batch_idx: 40 |  Loss_1: (0.0328) | Acc_1: (98.95%) (5193/5248)\n",
      "Epoch: 50 | Batch_idx: 50 |  Loss_1: (0.0536) | Acc_1: (98.39%) (6423/6528)\n",
      "Epoch: 50 | Batch_idx: 60 |  Loss_1: (0.0746) | Acc_1: (97.81%) (7637/7808)\n",
      "Epoch: 50 | Batch_idx: 70 |  Loss_1: (0.0878) | Acc_1: (97.46%) (8857/9088)\n",
      "Epoch: 50 | Batch_idx: 80 |  Loss_1: (0.1021) | Acc_1: (96.99%) (10056/10368)\n",
      "Epoch: 50 | Batch_idx: 90 |  Loss_1: (0.1187) | Acc_1: (96.44%) (11233/11648)\n",
      "Epoch: 50 | Batch_idx: 100 |  Loss_1: (0.1290) | Acc_1: (96.09%) (12422/12928)\n",
      "Epoch: 50 | Batch_idx: 110 |  Loss_1: (0.1364) | Acc_1: (95.77%) (13607/14208)\n",
      "Epoch: 50 | Batch_idx: 120 |  Loss_1: (0.1411) | Acc_1: (95.60%) (14807/15488)\n",
      "Epoch: 50 | Batch_idx: 130 |  Loss_1: (0.1445) | Acc_1: (95.42%) (16000/16768)\n",
      "Epoch: 50 | Batch_idx: 140 |  Loss_1: (0.1452) | Acc_1: (95.34%) (17207/18048)\n",
      "Epoch: 50 | Batch_idx: 150 |  Loss_1: (0.1480) | Acc_1: (95.24%) (18408/19328)\n",
      "Epoch: 50 | Batch_idx: 160 |  Loss_1: (0.1475) | Acc_1: (95.20%) (19619/20608)\n",
      "Epoch: 50 | Batch_idx: 170 |  Loss_1: (0.1463) | Acc_1: (95.19%) (20835/21888)\n",
      "Epoch: 50 | Batch_idx: 180 |  Loss_1: (0.1457) | Acc_1: (95.21%) (22058/23168)\n",
      "Epoch: 50 | Batch_idx: 190 |  Loss_1: (0.1438) | Acc_1: (95.24%) (23285/24448)\n",
      "Epoch: 50 | Batch_idx: 200 |  Loss_1: (0.1440) | Acc_1: (95.22%) (24498/25728)\n",
      "Epoch: 50 | Batch_idx: 210 |  Loss_1: (0.1429) | Acc_1: (95.22%) (25718/27008)\n",
      "Epoch: 50 | Batch_idx: 220 |  Loss_1: (0.1422) | Acc_1: (95.21%) (26933/28288)\n",
      "Epoch: 50 | Batch_idx: 230 |  Loss_1: (0.1411) | Acc_1: (95.24%) (28161/29568)\n",
      "Epoch: 50 | Batch_idx: 240 |  Loss_1: (0.1410) | Acc_1: (95.24%) (29380/30848)\n",
      "Epoch: 50 | Batch_idx: 250 |  Loss_1: (0.1411) | Acc_1: (95.23%) (30596/32128)\n",
      "Epoch: 50 | Batch_idx: 260 |  Loss_1: (0.1406) | Acc_1: (95.23%) (31816/33408)\n",
      "Epoch: 50 | Batch_idx: 270 |  Loss_1: (0.1390) | Acc_1: (95.28%) (33051/34688)\n",
      "Epoch: 50 | Batch_idx: 280 |  Loss_1: (0.1385) | Acc_1: (95.29%) (34273/35968)\n",
      "Epoch: 50 | Batch_idx: 290 |  Loss_1: (0.1377) | Acc_1: (95.30%) (35499/37248)\n",
      "Epoch: 50 | Batch_idx: 300 |  Loss_1: (0.1362) | Acc_1: (95.36%) (36739/38528)\n",
      "Epoch: 50 | Batch_idx: 310 |  Loss_1: (0.1360) | Acc_1: (95.35%) (37955/39808)\n",
      "Epoch: 50 | Batch_idx: 320 |  Loss_1: (0.1344) | Acc_1: (95.40%) (39196/41088)\n",
      "Epoch: 50 | Batch_idx: 330 |  Loss_1: (0.1334) | Acc_1: (95.42%) (40429/42368)\n",
      "Epoch: 50 | Batch_idx: 340 |  Loss_1: (0.1326) | Acc_1: (95.43%) (41655/43648)\n",
      "Epoch: 50 | Batch_idx: 350 |  Loss_1: (0.1321) | Acc_1: (95.43%) (42873/44928)\n",
      "Epoch: 50 | Batch_idx: 360 |  Loss_1: (0.1312) | Acc_1: (95.45%) (44107/46208)\n",
      "Epoch: 50 | Batch_idx: 370 |  Loss_1: (0.1310) | Acc_1: (95.46%) (45332/47488)\n",
      "Epoch: 50 | Batch_idx: 380 |  Loss_1: (0.1315) | Acc_1: (95.45%) (46551/48768)\n",
      "Epoch: 50 | Batch_idx: 390 |  Loss_1: (0.1310) | Acc_1: (95.47%) (47735/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4994) | Acc: (88.13%) (8813/10000)\n",
      "Epoch: 51 | Batch_idx: 0 |  Loss_1: (0.0626) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 51 | Batch_idx: 10 |  Loss_1: (0.1115) | Acc_1: (96.59%) (1360/1408)\n",
      "Epoch: 51 | Batch_idx: 20 |  Loss_1: (0.1069) | Acc_1: (96.43%) (2592/2688)\n",
      "Epoch: 51 | Batch_idx: 30 |  Loss_1: (0.1136) | Acc_1: (96.12%) (3814/3968)\n",
      "Epoch: 51 | Batch_idx: 40 |  Loss_1: (0.1129) | Acc_1: (96.21%) (5049/5248)\n",
      "Epoch: 51 | Batch_idx: 50 |  Loss_1: (0.1171) | Acc_1: (96.02%) (6268/6528)\n",
      "Epoch: 51 | Batch_idx: 60 |  Loss_1: (0.1156) | Acc_1: (95.95%) (7492/7808)\n",
      "Epoch: 51 | Batch_idx: 70 |  Loss_1: (0.1165) | Acc_1: (95.91%) (8716/9088)\n",
      "Epoch: 51 | Batch_idx: 80 |  Loss_1: (0.1133) | Acc_1: (96.06%) (9959/10368)\n",
      "Epoch: 51 | Batch_idx: 90 |  Loss_1: (0.1139) | Acc_1: (96.04%) (11187/11648)\n",
      "Epoch: 51 | Batch_idx: 100 |  Loss_1: (0.1143) | Acc_1: (96.02%) (12413/12928)\n",
      "Epoch: 51 | Batch_idx: 110 |  Loss_1: (0.1136) | Acc_1: (96.07%) (13650/14208)\n",
      "Epoch: 51 | Batch_idx: 120 |  Loss_1: (0.1126) | Acc_1: (96.06%) (14878/15488)\n",
      "Epoch: 51 | Batch_idx: 130 |  Loss_1: (0.1107) | Acc_1: (96.09%) (16112/16768)\n",
      "Epoch: 51 | Batch_idx: 140 |  Loss_1: (0.1108) | Acc_1: (96.09%) (17343/18048)\n",
      "Epoch: 51 | Batch_idx: 150 |  Loss_1: (0.1110) | Acc_1: (96.08%) (18570/19328)\n",
      "Epoch: 51 | Batch_idx: 160 |  Loss_1: (0.1112) | Acc_1: (96.06%) (19797/20608)\n",
      "Epoch: 51 | Batch_idx: 170 |  Loss_1: (0.1118) | Acc_1: (96.09%) (21032/21888)\n",
      "Epoch: 51 | Batch_idx: 180 |  Loss_1: (0.1120) | Acc_1: (96.05%) (22254/23168)\n",
      "Epoch: 51 | Batch_idx: 190 |  Loss_1: (0.1114) | Acc_1: (96.06%) (23485/24448)\n",
      "Epoch: 51 | Batch_idx: 200 |  Loss_1: (0.1107) | Acc_1: (96.09%) (24723/25728)\n",
      "Epoch: 51 | Batch_idx: 210 |  Loss_1: (0.1124) | Acc_1: (96.02%) (25933/27008)\n",
      "Epoch: 51 | Batch_idx: 220 |  Loss_1: (0.1129) | Acc_1: (96.04%) (27169/28288)\n",
      "Epoch: 51 | Batch_idx: 230 |  Loss_1: (0.1131) | Acc_1: (96.04%) (28398/29568)\n",
      "Epoch: 51 | Batch_idx: 240 |  Loss_1: (0.1125) | Acc_1: (96.07%) (29635/30848)\n",
      "Epoch: 51 | Batch_idx: 250 |  Loss_1: (0.1135) | Acc_1: (96.04%) (30856/32128)\n",
      "Epoch: 51 | Batch_idx: 260 |  Loss_1: (0.1138) | Acc_1: (96.04%) (32085/33408)\n",
      "Epoch: 51 | Batch_idx: 270 |  Loss_1: (0.1138) | Acc_1: (96.02%) (33308/34688)\n",
      "Epoch: 51 | Batch_idx: 280 |  Loss_1: (0.1139) | Acc_1: (96.01%) (34533/35968)\n",
      "Epoch: 51 | Batch_idx: 290 |  Loss_1: (0.1144) | Acc_1: (95.98%) (35751/37248)\n",
      "Epoch: 51 | Batch_idx: 300 |  Loss_1: (0.1152) | Acc_1: (95.95%) (36969/38528)\n",
      "Epoch: 51 | Batch_idx: 310 |  Loss_1: (0.1150) | Acc_1: (95.95%) (38194/39808)\n",
      "Epoch: 51 | Batch_idx: 320 |  Loss_1: (0.1148) | Acc_1: (95.96%) (39428/41088)\n",
      "Epoch: 51 | Batch_idx: 330 |  Loss_1: (0.1153) | Acc_1: (95.95%) (40650/42368)\n",
      "Epoch: 51 | Batch_idx: 340 |  Loss_1: (0.1155) | Acc_1: (95.92%) (41868/43648)\n",
      "Epoch: 51 | Batch_idx: 350 |  Loss_1: (0.1151) | Acc_1: (95.93%) (43099/44928)\n",
      "Epoch: 51 | Batch_idx: 360 |  Loss_1: (0.1156) | Acc_1: (95.92%) (44323/46208)\n",
      "Epoch: 51 | Batch_idx: 370 |  Loss_1: (0.1159) | Acc_1: (95.91%) (45545/47488)\n",
      "Epoch: 51 | Batch_idx: 380 |  Loss_1: (0.1168) | Acc_1: (95.88%) (46759/48768)\n",
      "Epoch: 51 | Batch_idx: 390 |  Loss_1: (0.1168) | Acc_1: (95.88%) (47940/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4720) | Acc: (88.27%) (8827/10000)\n",
      "Epoch: 52 | Batch_idx: 0 |  Loss_1: (0.0235) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 52 | Batch_idx: 10 |  Loss_1: (0.0969) | Acc_1: (95.88%) (1350/1408)\n",
      "Epoch: 52 | Batch_idx: 20 |  Loss_1: (0.0987) | Acc_1: (96.28%) (2588/2688)\n",
      "Epoch: 52 | Batch_idx: 30 |  Loss_1: (0.0992) | Acc_1: (96.22%) (3818/3968)\n",
      "Epoch: 52 | Batch_idx: 40 |  Loss_1: (0.0954) | Acc_1: (96.32%) (5055/5248)\n",
      "Epoch: 52 | Batch_idx: 50 |  Loss_1: (0.1009) | Acc_1: (96.19%) (6279/6528)\n",
      "Epoch: 52 | Batch_idx: 60 |  Loss_1: (0.1032) | Acc_1: (96.12%) (7505/7808)\n",
      "Epoch: 52 | Batch_idx: 70 |  Loss_1: (0.1032) | Acc_1: (96.16%) (8739/9088)\n",
      "Epoch: 52 | Batch_idx: 80 |  Loss_1: (0.1044) | Acc_1: (96.09%) (9963/10368)\n",
      "Epoch: 52 | Batch_idx: 90 |  Loss_1: (0.1043) | Acc_1: (96.15%) (11200/11648)\n",
      "Epoch: 52 | Batch_idx: 100 |  Loss_1: (0.1064) | Acc_1: (96.09%) (12423/12928)\n",
      "Epoch: 52 | Batch_idx: 110 |  Loss_1: (0.1046) | Acc_1: (96.21%) (13670/14208)\n",
      "Epoch: 52 | Batch_idx: 120 |  Loss_1: (0.1025) | Acc_1: (96.33%) (14920/15488)\n",
      "Epoch: 52 | Batch_idx: 130 |  Loss_1: (0.1011) | Acc_1: (96.37%) (16159/16768)\n",
      "Epoch: 52 | Batch_idx: 140 |  Loss_1: (0.1022) | Acc_1: (96.33%) (17385/18048)\n",
      "Epoch: 52 | Batch_idx: 150 |  Loss_1: (0.1021) | Acc_1: (96.36%) (18625/19328)\n",
      "Epoch: 52 | Batch_idx: 160 |  Loss_1: (0.1029) | Acc_1: (96.31%) (19847/20608)\n",
      "Epoch: 52 | Batch_idx: 170 |  Loss_1: (0.1011) | Acc_1: (96.37%) (21094/21888)\n",
      "Epoch: 52 | Batch_idx: 180 |  Loss_1: (0.1009) | Acc_1: (96.36%) (22325/23168)\n",
      "Epoch: 52 | Batch_idx: 190 |  Loss_1: (0.1007) | Acc_1: (96.36%) (23559/24448)\n",
      "Epoch: 52 | Batch_idx: 200 |  Loss_1: (0.1016) | Acc_1: (96.34%) (24787/25728)\n",
      "Epoch: 52 | Batch_idx: 210 |  Loss_1: (0.1020) | Acc_1: (96.33%) (26017/27008)\n",
      "Epoch: 52 | Batch_idx: 220 |  Loss_1: (0.1029) | Acc_1: (96.27%) (27234/28288)\n",
      "Epoch: 52 | Batch_idx: 230 |  Loss_1: (0.1037) | Acc_1: (96.25%) (28460/29568)\n",
      "Epoch: 52 | Batch_idx: 240 |  Loss_1: (0.1033) | Acc_1: (96.27%) (29697/30848)\n",
      "Epoch: 52 | Batch_idx: 250 |  Loss_1: (0.1034) | Acc_1: (96.27%) (30930/32128)\n",
      "Epoch: 52 | Batch_idx: 260 |  Loss_1: (0.1039) | Acc_1: (96.28%) (32164/33408)\n",
      "Epoch: 52 | Batch_idx: 270 |  Loss_1: (0.1053) | Acc_1: (96.23%) (33379/34688)\n",
      "Epoch: 52 | Batch_idx: 280 |  Loss_1: (0.1062) | Acc_1: (96.20%) (34603/35968)\n",
      "Epoch: 52 | Batch_idx: 290 |  Loss_1: (0.1065) | Acc_1: (96.21%) (35835/37248)\n",
      "Epoch: 52 | Batch_idx: 300 |  Loss_1: (0.1057) | Acc_1: (96.24%) (37079/38528)\n",
      "Epoch: 52 | Batch_idx: 310 |  Loss_1: (0.1071) | Acc_1: (96.21%) (38298/39808)\n",
      "Epoch: 52 | Batch_idx: 320 |  Loss_1: (0.1073) | Acc_1: (96.18%) (39520/41088)\n",
      "Epoch: 52 | Batch_idx: 330 |  Loss_1: (0.1074) | Acc_1: (96.17%) (40747/42368)\n",
      "Epoch: 52 | Batch_idx: 340 |  Loss_1: (0.1074) | Acc_1: (96.16%) (41972/43648)\n",
      "Epoch: 52 | Batch_idx: 350 |  Loss_1: (0.1077) | Acc_1: (96.16%) (43202/44928)\n",
      "Epoch: 52 | Batch_idx: 360 |  Loss_1: (0.1084) | Acc_1: (96.13%) (44421/46208)\n",
      "Epoch: 52 | Batch_idx: 370 |  Loss_1: (0.1083) | Acc_1: (96.13%) (45652/47488)\n",
      "Epoch: 52 | Batch_idx: 380 |  Loss_1: (0.1087) | Acc_1: (96.12%) (46877/48768)\n",
      "Epoch: 52 | Batch_idx: 390 |  Loss_1: (0.1091) | Acc_1: (96.12%) (48061/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4389) | Acc: (89.14%) (8914/10000)\n",
      "Epoch: 53 | Batch_idx: 0 |  Loss_1: (0.0969) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 53 | Batch_idx: 10 |  Loss_1: (0.1136) | Acc_1: (96.31%) (1356/1408)\n",
      "Epoch: 53 | Batch_idx: 20 |  Loss_1: (0.1078) | Acc_1: (96.24%) (2587/2688)\n",
      "Epoch: 53 | Batch_idx: 30 |  Loss_1: (0.0940) | Acc_1: (96.70%) (3837/3968)\n",
      "Epoch: 53 | Batch_idx: 40 |  Loss_1: (0.0943) | Acc_1: (96.68%) (5074/5248)\n",
      "Epoch: 53 | Batch_idx: 50 |  Loss_1: (0.0951) | Acc_1: (96.60%) (6306/6528)\n",
      "Epoch: 53 | Batch_idx: 60 |  Loss_1: (0.0923) | Acc_1: (96.68%) (7549/7808)\n",
      "Epoch: 53 | Batch_idx: 70 |  Loss_1: (0.0918) | Acc_1: (96.69%) (8787/9088)\n",
      "Epoch: 53 | Batch_idx: 80 |  Loss_1: (0.0930) | Acc_1: (96.61%) (10017/10368)\n",
      "Epoch: 53 | Batch_idx: 90 |  Loss_1: (0.0917) | Acc_1: (96.66%) (11259/11648)\n",
      "Epoch: 53 | Batch_idx: 100 |  Loss_1: (0.0926) | Acc_1: (96.58%) (12486/12928)\n",
      "Epoch: 53 | Batch_idx: 110 |  Loss_1: (0.0930) | Acc_1: (96.56%) (13719/14208)\n",
      "Epoch: 53 | Batch_idx: 120 |  Loss_1: (0.0942) | Acc_1: (96.53%) (14951/15488)\n",
      "Epoch: 53 | Batch_idx: 130 |  Loss_1: (0.0939) | Acc_1: (96.58%) (16194/16768)\n",
      "Epoch: 53 | Batch_idx: 140 |  Loss_1: (0.0962) | Acc_1: (96.52%) (17420/18048)\n",
      "Epoch: 53 | Batch_idx: 150 |  Loss_1: (0.0959) | Acc_1: (96.55%) (18662/19328)\n",
      "Epoch: 53 | Batch_idx: 160 |  Loss_1: (0.0962) | Acc_1: (96.55%) (19897/20608)\n",
      "Epoch: 53 | Batch_idx: 170 |  Loss_1: (0.0961) | Acc_1: (96.56%) (21134/21888)\n",
      "Epoch: 53 | Batch_idx: 180 |  Loss_1: (0.0968) | Acc_1: (96.54%) (22366/23168)\n",
      "Epoch: 53 | Batch_idx: 190 |  Loss_1: (0.0976) | Acc_1: (96.50%) (23592/24448)\n",
      "Epoch: 53 | Batch_idx: 200 |  Loss_1: (0.0978) | Acc_1: (96.50%) (24828/25728)\n",
      "Epoch: 53 | Batch_idx: 210 |  Loss_1: (0.0983) | Acc_1: (96.49%) (26061/27008)\n",
      "Epoch: 53 | Batch_idx: 220 |  Loss_1: (0.0974) | Acc_1: (96.54%) (27308/28288)\n",
      "Epoch: 53 | Batch_idx: 230 |  Loss_1: (0.0980) | Acc_1: (96.51%) (28535/29568)\n",
      "Epoch: 53 | Batch_idx: 240 |  Loss_1: (0.0982) | Acc_1: (96.51%) (29772/30848)\n",
      "Epoch: 53 | Batch_idx: 250 |  Loss_1: (0.0983) | Acc_1: (96.51%) (31008/32128)\n",
      "Epoch: 53 | Batch_idx: 260 |  Loss_1: (0.0991) | Acc_1: (96.49%) (32235/33408)\n",
      "Epoch: 53 | Batch_idx: 270 |  Loss_1: (0.0994) | Acc_1: (96.46%) (33460/34688)\n",
      "Epoch: 53 | Batch_idx: 280 |  Loss_1: (0.0999) | Acc_1: (96.45%) (34692/35968)\n",
      "Epoch: 53 | Batch_idx: 290 |  Loss_1: (0.0997) | Acc_1: (96.46%) (35929/37248)\n",
      "Epoch: 53 | Batch_idx: 300 |  Loss_1: (0.1005) | Acc_1: (96.43%) (37152/38528)\n",
      "Epoch: 53 | Batch_idx: 310 |  Loss_1: (0.1009) | Acc_1: (96.42%) (38383/39808)\n",
      "Epoch: 53 | Batch_idx: 320 |  Loss_1: (0.1007) | Acc_1: (96.41%) (39612/41088)\n",
      "Epoch: 53 | Batch_idx: 330 |  Loss_1: (0.1011) | Acc_1: (96.40%) (40841/42368)\n",
      "Epoch: 53 | Batch_idx: 340 |  Loss_1: (0.1010) | Acc_1: (96.39%) (42072/43648)\n",
      "Epoch: 53 | Batch_idx: 350 |  Loss_1: (0.1013) | Acc_1: (96.39%) (43304/44928)\n",
      "Epoch: 53 | Batch_idx: 360 |  Loss_1: (0.1012) | Acc_1: (96.39%) (44539/46208)\n",
      "Epoch: 53 | Batch_idx: 370 |  Loss_1: (0.1016) | Acc_1: (96.38%) (45767/47488)\n",
      "Epoch: 53 | Batch_idx: 380 |  Loss_1: (0.1021) | Acc_1: (96.35%) (46989/48768)\n",
      "Epoch: 53 | Batch_idx: 390 |  Loss_1: (0.1021) | Acc_1: (96.35%) (48177/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5219) | Acc: (88.14%) (8814/10000)\n",
      "Epoch: 54 | Batch_idx: 0 |  Loss_1: (0.1105) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 54 | Batch_idx: 10 |  Loss_1: (0.0989) | Acc_1: (96.52%) (1359/1408)\n",
      "Epoch: 54 | Batch_idx: 20 |  Loss_1: (0.0964) | Acc_1: (96.76%) (2601/2688)\n",
      "Epoch: 54 | Batch_idx: 30 |  Loss_1: (0.0979) | Acc_1: (96.65%) (3835/3968)\n",
      "Epoch: 54 | Batch_idx: 40 |  Loss_1: (0.0894) | Acc_1: (97.01%) (5091/5248)\n",
      "Epoch: 54 | Batch_idx: 50 |  Loss_1: (0.0937) | Acc_1: (96.92%) (6327/6528)\n",
      "Epoch: 54 | Batch_idx: 60 |  Loss_1: (0.0944) | Acc_1: (96.88%) (7564/7808)\n",
      "Epoch: 54 | Batch_idx: 70 |  Loss_1: (0.0905) | Acc_1: (97.00%) (8815/9088)\n",
      "Epoch: 54 | Batch_idx: 80 |  Loss_1: (0.0890) | Acc_1: (97.06%) (10063/10368)\n",
      "Epoch: 54 | Batch_idx: 90 |  Loss_1: (0.0867) | Acc_1: (97.11%) (11311/11648)\n",
      "Epoch: 54 | Batch_idx: 100 |  Loss_1: (0.0871) | Acc_1: (97.05%) (12547/12928)\n",
      "Epoch: 54 | Batch_idx: 110 |  Loss_1: (0.0860) | Acc_1: (97.05%) (13789/14208)\n",
      "Epoch: 54 | Batch_idx: 120 |  Loss_1: (0.0869) | Acc_1: (97.06%) (15032/15488)\n",
      "Epoch: 54 | Batch_idx: 130 |  Loss_1: (0.0879) | Acc_1: (97.01%) (16267/16768)\n",
      "Epoch: 54 | Batch_idx: 140 |  Loss_1: (0.0872) | Acc_1: (97.06%) (17517/18048)\n",
      "Epoch: 54 | Batch_idx: 150 |  Loss_1: (0.0873) | Acc_1: (97.01%) (18751/19328)\n",
      "Epoch: 54 | Batch_idx: 160 |  Loss_1: (0.0874) | Acc_1: (97.02%) (19993/20608)\n",
      "Epoch: 54 | Batch_idx: 170 |  Loss_1: (0.0887) | Acc_1: (96.97%) (21225/21888)\n",
      "Epoch: 54 | Batch_idx: 180 |  Loss_1: (0.0893) | Acc_1: (96.93%) (22457/23168)\n",
      "Epoch: 54 | Batch_idx: 190 |  Loss_1: (0.0894) | Acc_1: (96.91%) (23693/24448)\n",
      "Epoch: 54 | Batch_idx: 200 |  Loss_1: (0.0891) | Acc_1: (96.90%) (24930/25728)\n",
      "Epoch: 54 | Batch_idx: 210 |  Loss_1: (0.0890) | Acc_1: (96.88%) (26164/27008)\n",
      "Epoch: 54 | Batch_idx: 220 |  Loss_1: (0.0894) | Acc_1: (96.88%) (27404/28288)\n",
      "Epoch: 54 | Batch_idx: 230 |  Loss_1: (0.0905) | Acc_1: (96.84%) (28633/29568)\n",
      "Epoch: 54 | Batch_idx: 240 |  Loss_1: (0.0905) | Acc_1: (96.85%) (29875/30848)\n",
      "Epoch: 54 | Batch_idx: 250 |  Loss_1: (0.0905) | Acc_1: (96.83%) (31109/32128)\n",
      "Epoch: 54 | Batch_idx: 260 |  Loss_1: (0.0906) | Acc_1: (96.82%) (32346/33408)\n",
      "Epoch: 54 | Batch_idx: 270 |  Loss_1: (0.0913) | Acc_1: (96.80%) (33578/34688)\n",
      "Epoch: 54 | Batch_idx: 280 |  Loss_1: (0.0916) | Acc_1: (96.79%) (34815/35968)\n",
      "Epoch: 54 | Batch_idx: 290 |  Loss_1: (0.0925) | Acc_1: (96.77%) (36045/37248)\n",
      "Epoch: 54 | Batch_idx: 300 |  Loss_1: (0.0925) | Acc_1: (96.77%) (37283/38528)\n",
      "Epoch: 54 | Batch_idx: 310 |  Loss_1: (0.0925) | Acc_1: (96.77%) (38522/39808)\n",
      "Epoch: 54 | Batch_idx: 320 |  Loss_1: (0.0924) | Acc_1: (96.77%) (39759/41088)\n",
      "Epoch: 54 | Batch_idx: 330 |  Loss_1: (0.0921) | Acc_1: (96.78%) (41003/42368)\n",
      "Epoch: 54 | Batch_idx: 340 |  Loss_1: (0.0919) | Acc_1: (96.78%) (42244/43648)\n",
      "Epoch: 54 | Batch_idx: 350 |  Loss_1: (0.0923) | Acc_1: (96.76%) (43473/44928)\n",
      "Epoch: 54 | Batch_idx: 360 |  Loss_1: (0.0926) | Acc_1: (96.76%) (44710/46208)\n",
      "Epoch: 54 | Batch_idx: 370 |  Loss_1: (0.0926) | Acc_1: (96.76%) (45950/47488)\n",
      "Epoch: 54 | Batch_idx: 380 |  Loss_1: (0.0936) | Acc_1: (96.73%) (47173/48768)\n",
      "Epoch: 54 | Batch_idx: 390 |  Loss_1: (0.0939) | Acc_1: (96.72%) (48360/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4649) | Acc: (88.73%) (8873/10000)\n",
      "Epoch: 55 | Batch_idx: 0 |  Loss_1: (0.1066) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 55 | Batch_idx: 10 |  Loss_1: (0.0928) | Acc_1: (96.88%) (1364/1408)\n",
      "Epoch: 55 | Batch_idx: 20 |  Loss_1: (0.0917) | Acc_1: (96.73%) (2600/2688)\n",
      "Epoch: 55 | Batch_idx: 30 |  Loss_1: (0.0957) | Acc_1: (96.62%) (3834/3968)\n",
      "Epoch: 55 | Batch_idx: 40 |  Loss_1: (0.0933) | Acc_1: (96.70%) (5075/5248)\n",
      "Epoch: 55 | Batch_idx: 50 |  Loss_1: (0.0922) | Acc_1: (96.74%) (6315/6528)\n",
      "Epoch: 55 | Batch_idx: 60 |  Loss_1: (0.0906) | Acc_1: (96.84%) (7561/7808)\n",
      "Epoch: 55 | Batch_idx: 70 |  Loss_1: (0.0894) | Acc_1: (96.90%) (8806/9088)\n",
      "Epoch: 55 | Batch_idx: 80 |  Loss_1: (0.0906) | Acc_1: (96.86%) (10042/10368)\n",
      "Epoch: 55 | Batch_idx: 90 |  Loss_1: (0.0892) | Acc_1: (96.91%) (11288/11648)\n",
      "Epoch: 55 | Batch_idx: 100 |  Loss_1: (0.0876) | Acc_1: (96.97%) (12536/12928)\n",
      "Epoch: 55 | Batch_idx: 110 |  Loss_1: (0.0879) | Acc_1: (96.95%) (13774/14208)\n",
      "Epoch: 55 | Batch_idx: 120 |  Loss_1: (0.0883) | Acc_1: (96.90%) (15008/15488)\n",
      "Epoch: 55 | Batch_idx: 130 |  Loss_1: (0.0872) | Acc_1: (96.94%) (16255/16768)\n",
      "Epoch: 55 | Batch_idx: 140 |  Loss_1: (0.0882) | Acc_1: (96.95%) (17498/18048)\n",
      "Epoch: 55 | Batch_idx: 150 |  Loss_1: (0.0884) | Acc_1: (96.92%) (18733/19328)\n",
      "Epoch: 55 | Batch_idx: 160 |  Loss_1: (0.0884) | Acc_1: (96.90%) (19969/20608)\n",
      "Epoch: 55 | Batch_idx: 170 |  Loss_1: (0.0882) | Acc_1: (96.90%) (21209/21888)\n",
      "Epoch: 55 | Batch_idx: 180 |  Loss_1: (0.0890) | Acc_1: (96.89%) (22447/23168)\n",
      "Epoch: 55 | Batch_idx: 190 |  Loss_1: (0.0880) | Acc_1: (96.93%) (23698/24448)\n",
      "Epoch: 55 | Batch_idx: 200 |  Loss_1: (0.0881) | Acc_1: (96.93%) (24938/25728)\n",
      "Epoch: 55 | Batch_idx: 210 |  Loss_1: (0.0897) | Acc_1: (96.88%) (26164/27008)\n",
      "Epoch: 55 | Batch_idx: 220 |  Loss_1: (0.0903) | Acc_1: (96.85%) (27398/28288)\n",
      "Epoch: 55 | Batch_idx: 230 |  Loss_1: (0.0908) | Acc_1: (96.84%) (28633/29568)\n",
      "Epoch: 55 | Batch_idx: 240 |  Loss_1: (0.0910) | Acc_1: (96.82%) (29868/30848)\n",
      "Epoch: 55 | Batch_idx: 250 |  Loss_1: (0.0916) | Acc_1: (96.80%) (31101/32128)\n",
      "Epoch: 55 | Batch_idx: 260 |  Loss_1: (0.0910) | Acc_1: (96.82%) (32347/33408)\n",
      "Epoch: 55 | Batch_idx: 270 |  Loss_1: (0.0914) | Acc_1: (96.80%) (33578/34688)\n",
      "Epoch: 55 | Batch_idx: 280 |  Loss_1: (0.0912) | Acc_1: (96.79%) (34815/35968)\n",
      "Epoch: 55 | Batch_idx: 290 |  Loss_1: (0.0908) | Acc_1: (96.80%) (36056/37248)\n",
      "Epoch: 55 | Batch_idx: 300 |  Loss_1: (0.0908) | Acc_1: (96.81%) (37298/38528)\n",
      "Epoch: 55 | Batch_idx: 310 |  Loss_1: (0.0906) | Acc_1: (96.82%) (38542/39808)\n",
      "Epoch: 55 | Batch_idx: 320 |  Loss_1: (0.0906) | Acc_1: (96.82%) (39781/41088)\n",
      "Epoch: 55 | Batch_idx: 330 |  Loss_1: (0.0913) | Acc_1: (96.81%) (41015/42368)\n",
      "Epoch: 55 | Batch_idx: 340 |  Loss_1: (0.0912) | Acc_1: (96.80%) (42253/43648)\n",
      "Epoch: 55 | Batch_idx: 350 |  Loss_1: (0.0919) | Acc_1: (96.77%) (43477/44928)\n",
      "Epoch: 55 | Batch_idx: 360 |  Loss_1: (0.0918) | Acc_1: (96.76%) (44712/46208)\n",
      "Epoch: 55 | Batch_idx: 370 |  Loss_1: (0.0921) | Acc_1: (96.74%) (45942/47488)\n",
      "Epoch: 55 | Batch_idx: 380 |  Loss_1: (0.0924) | Acc_1: (96.74%) (47176/48768)\n",
      "Epoch: 55 | Batch_idx: 390 |  Loss_1: (0.0924) | Acc_1: (96.74%) (48371/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4118) | Acc: (90.15%) (9015/10000)\n",
      "Epoch: 56 | Batch_idx: 0 |  Loss_1: (0.0990) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 56 | Batch_idx: 10 |  Loss_1: (0.0692) | Acc_1: (97.44%) (1372/1408)\n",
      "Epoch: 56 | Batch_idx: 20 |  Loss_1: (0.0774) | Acc_1: (97.28%) (2615/2688)\n",
      "Epoch: 56 | Batch_idx: 30 |  Loss_1: (0.0797) | Acc_1: (97.18%) (3856/3968)\n",
      "Epoch: 56 | Batch_idx: 40 |  Loss_1: (0.0785) | Acc_1: (97.20%) (5101/5248)\n",
      "Epoch: 56 | Batch_idx: 50 |  Loss_1: (0.0792) | Acc_1: (97.24%) (6348/6528)\n",
      "Epoch: 56 | Batch_idx: 60 |  Loss_1: (0.0805) | Acc_1: (97.20%) (7589/7808)\n",
      "Epoch: 56 | Batch_idx: 70 |  Loss_1: (0.0823) | Acc_1: (97.16%) (8830/9088)\n",
      "Epoch: 56 | Batch_idx: 80 |  Loss_1: (0.0831) | Acc_1: (97.15%) (10073/10368)\n",
      "Epoch: 56 | Batch_idx: 90 |  Loss_1: (0.0821) | Acc_1: (97.17%) (11318/11648)\n",
      "Epoch: 56 | Batch_idx: 100 |  Loss_1: (0.0828) | Acc_1: (97.15%) (12559/12928)\n",
      "Epoch: 56 | Batch_idx: 110 |  Loss_1: (0.0822) | Acc_1: (97.16%) (13805/14208)\n",
      "Epoch: 56 | Batch_idx: 120 |  Loss_1: (0.0836) | Acc_1: (97.13%) (15043/15488)\n",
      "Epoch: 56 | Batch_idx: 130 |  Loss_1: (0.0830) | Acc_1: (97.14%) (16288/16768)\n",
      "Epoch: 56 | Batch_idx: 140 |  Loss_1: (0.0831) | Acc_1: (97.13%) (17530/18048)\n",
      "Epoch: 56 | Batch_idx: 150 |  Loss_1: (0.0828) | Acc_1: (97.13%) (18773/19328)\n",
      "Epoch: 56 | Batch_idx: 160 |  Loss_1: (0.0836) | Acc_1: (97.09%) (20009/20608)\n",
      "Epoch: 56 | Batch_idx: 170 |  Loss_1: (0.0834) | Acc_1: (97.13%) (21260/21888)\n",
      "Epoch: 56 | Batch_idx: 180 |  Loss_1: (0.0836) | Acc_1: (97.13%) (22504/23168)\n",
      "Epoch: 56 | Batch_idx: 190 |  Loss_1: (0.0844) | Acc_1: (97.12%) (23744/24448)\n",
      "Epoch: 56 | Batch_idx: 200 |  Loss_1: (0.0834) | Acc_1: (97.15%) (24996/25728)\n",
      "Epoch: 56 | Batch_idx: 210 |  Loss_1: (0.0833) | Acc_1: (97.18%) (26246/27008)\n",
      "Epoch: 56 | Batch_idx: 220 |  Loss_1: (0.0838) | Acc_1: (97.15%) (27483/28288)\n",
      "Epoch: 56 | Batch_idx: 230 |  Loss_1: (0.0846) | Acc_1: (97.14%) (28721/29568)\n",
      "Epoch: 56 | Batch_idx: 240 |  Loss_1: (0.0849) | Acc_1: (97.12%) (29960/30848)\n",
      "Epoch: 56 | Batch_idx: 250 |  Loss_1: (0.0854) | Acc_1: (97.09%) (31194/32128)\n",
      "Epoch: 56 | Batch_idx: 260 |  Loss_1: (0.0854) | Acc_1: (97.10%) (32440/33408)\n",
      "Epoch: 56 | Batch_idx: 270 |  Loss_1: (0.0854) | Acc_1: (97.09%) (33679/34688)\n",
      "Epoch: 56 | Batch_idx: 280 |  Loss_1: (0.0847) | Acc_1: (97.11%) (34927/35968)\n",
      "Epoch: 56 | Batch_idx: 290 |  Loss_1: (0.0849) | Acc_1: (97.10%) (36167/37248)\n",
      "Epoch: 56 | Batch_idx: 300 |  Loss_1: (0.0853) | Acc_1: (97.08%) (37403/38528)\n",
      "Epoch: 56 | Batch_idx: 310 |  Loss_1: (0.0853) | Acc_1: (97.08%) (38645/39808)\n",
      "Epoch: 56 | Batch_idx: 320 |  Loss_1: (0.0850) | Acc_1: (97.08%) (39887/41088)\n",
      "Epoch: 56 | Batch_idx: 330 |  Loss_1: (0.0856) | Acc_1: (97.06%) (41121/42368)\n",
      "Epoch: 56 | Batch_idx: 340 |  Loss_1: (0.0856) | Acc_1: (97.05%) (42360/43648)\n",
      "Epoch: 56 | Batch_idx: 350 |  Loss_1: (0.0855) | Acc_1: (97.05%) (43604/44928)\n",
      "Epoch: 56 | Batch_idx: 360 |  Loss_1: (0.0857) | Acc_1: (97.04%) (44839/46208)\n",
      "Epoch: 56 | Batch_idx: 370 |  Loss_1: (0.0852) | Acc_1: (97.05%) (46089/47488)\n",
      "Epoch: 56 | Batch_idx: 380 |  Loss_1: (0.0861) | Acc_1: (97.01%) (47308/48768)\n",
      "Epoch: 56 | Batch_idx: 390 |  Loss_1: (0.0863) | Acc_1: (97.00%) (48498/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4471) | Acc: (89.44%) (8944/10000)\n",
      "Epoch: 57 | Batch_idx: 0 |  Loss_1: (0.0515) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 57 | Batch_idx: 10 |  Loss_1: (0.0848) | Acc_1: (96.88%) (1364/1408)\n",
      "Epoch: 57 | Batch_idx: 20 |  Loss_1: (0.0880) | Acc_1: (96.69%) (2599/2688)\n",
      "Epoch: 57 | Batch_idx: 30 |  Loss_1: (0.0784) | Acc_1: (96.88%) (3844/3968)\n",
      "Epoch: 57 | Batch_idx: 40 |  Loss_1: (0.0760) | Acc_1: (97.03%) (5092/5248)\n",
      "Epoch: 57 | Batch_idx: 50 |  Loss_1: (0.0778) | Acc_1: (97.00%) (6332/6528)\n",
      "Epoch: 57 | Batch_idx: 60 |  Loss_1: (0.0809) | Acc_1: (96.93%) (7568/7808)\n",
      "Epoch: 57 | Batch_idx: 70 |  Loss_1: (0.0791) | Acc_1: (97.10%) (8824/9088)\n",
      "Epoch: 57 | Batch_idx: 80 |  Loss_1: (0.0798) | Acc_1: (97.05%) (10062/10368)\n",
      "Epoch: 57 | Batch_idx: 90 |  Loss_1: (0.0822) | Acc_1: (97.00%) (11298/11648)\n",
      "Epoch: 57 | Batch_idx: 100 |  Loss_1: (0.0823) | Acc_1: (97.02%) (12543/12928)\n",
      "Epoch: 57 | Batch_idx: 110 |  Loss_1: (0.0817) | Acc_1: (97.05%) (13789/14208)\n",
      "Epoch: 57 | Batch_idx: 120 |  Loss_1: (0.0816) | Acc_1: (97.06%) (15033/15488)\n",
      "Epoch: 57 | Batch_idx: 130 |  Loss_1: (0.0810) | Acc_1: (97.06%) (16275/16768)\n",
      "Epoch: 57 | Batch_idx: 140 |  Loss_1: (0.0798) | Acc_1: (97.09%) (17522/18048)\n",
      "Epoch: 57 | Batch_idx: 150 |  Loss_1: (0.0815) | Acc_1: (97.03%) (18753/19328)\n",
      "Epoch: 57 | Batch_idx: 160 |  Loss_1: (0.0809) | Acc_1: (97.04%) (19997/20608)\n",
      "Epoch: 57 | Batch_idx: 170 |  Loss_1: (0.0809) | Acc_1: (97.06%) (21245/21888)\n",
      "Epoch: 57 | Batch_idx: 180 |  Loss_1: (0.0815) | Acc_1: (97.06%) (22488/23168)\n",
      "Epoch: 57 | Batch_idx: 190 |  Loss_1: (0.0821) | Acc_1: (97.03%) (23723/24448)\n",
      "Epoch: 57 | Batch_idx: 200 |  Loss_1: (0.0819) | Acc_1: (97.05%) (24969/25728)\n",
      "Epoch: 57 | Batch_idx: 210 |  Loss_1: (0.0814) | Acc_1: (97.06%) (26214/27008)\n",
      "Epoch: 57 | Batch_idx: 220 |  Loss_1: (0.0822) | Acc_1: (97.02%) (27445/28288)\n",
      "Epoch: 57 | Batch_idx: 230 |  Loss_1: (0.0821) | Acc_1: (97.01%) (28684/29568)\n",
      "Epoch: 57 | Batch_idx: 240 |  Loss_1: (0.0818) | Acc_1: (97.02%) (29930/30848)\n",
      "Epoch: 57 | Batch_idx: 250 |  Loss_1: (0.0827) | Acc_1: (97.00%) (31165/32128)\n",
      "Epoch: 57 | Batch_idx: 260 |  Loss_1: (0.0833) | Acc_1: (96.98%) (32400/33408)\n",
      "Epoch: 57 | Batch_idx: 270 |  Loss_1: (0.0839) | Acc_1: (96.96%) (33635/34688)\n",
      "Epoch: 57 | Batch_idx: 280 |  Loss_1: (0.0839) | Acc_1: (96.95%) (34872/35968)\n",
      "Epoch: 57 | Batch_idx: 290 |  Loss_1: (0.0834) | Acc_1: (96.96%) (36117/37248)\n",
      "Epoch: 57 | Batch_idx: 300 |  Loss_1: (0.0834) | Acc_1: (96.99%) (37367/38528)\n",
      "Epoch: 57 | Batch_idx: 310 |  Loss_1: (0.0840) | Acc_1: (96.97%) (38600/39808)\n",
      "Epoch: 57 | Batch_idx: 320 |  Loss_1: (0.0845) | Acc_1: (96.95%) (39835/41088)\n",
      "Epoch: 57 | Batch_idx: 330 |  Loss_1: (0.0851) | Acc_1: (96.93%) (41069/42368)\n",
      "Epoch: 57 | Batch_idx: 340 |  Loss_1: (0.0848) | Acc_1: (96.94%) (42313/43648)\n",
      "Epoch: 57 | Batch_idx: 350 |  Loss_1: (0.0843) | Acc_1: (96.95%) (43556/44928)\n",
      "Epoch: 57 | Batch_idx: 360 |  Loss_1: (0.0845) | Acc_1: (96.93%) (44788/46208)\n",
      "Epoch: 57 | Batch_idx: 370 |  Loss_1: (0.0846) | Acc_1: (96.93%) (46029/47488)\n",
      "Epoch: 57 | Batch_idx: 380 |  Loss_1: (0.0849) | Acc_1: (96.93%) (47271/48768)\n",
      "Epoch: 57 | Batch_idx: 390 |  Loss_1: (0.0849) | Acc_1: (96.92%) (48459/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4866) | Acc: (88.80%) (8880/10000)\n",
      "Epoch: 58 | Batch_idx: 0 |  Loss_1: (0.0864) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 58 | Batch_idx: 10 |  Loss_1: (0.0760) | Acc_1: (97.37%) (1371/1408)\n",
      "Epoch: 58 | Batch_idx: 20 |  Loss_1: (0.0825) | Acc_1: (97.17%) (2612/2688)\n",
      "Epoch: 58 | Batch_idx: 30 |  Loss_1: (0.0786) | Acc_1: (97.15%) (3855/3968)\n",
      "Epoch: 58 | Batch_idx: 40 |  Loss_1: (0.0740) | Acc_1: (97.35%) (5109/5248)\n",
      "Epoch: 58 | Batch_idx: 50 |  Loss_1: (0.0699) | Acc_1: (97.46%) (6362/6528)\n",
      "Epoch: 58 | Batch_idx: 60 |  Loss_1: (0.0696) | Acc_1: (97.46%) (7610/7808)\n",
      "Epoch: 58 | Batch_idx: 70 |  Loss_1: (0.0671) | Acc_1: (97.58%) (8868/9088)\n",
      "Epoch: 58 | Batch_idx: 80 |  Loss_1: (0.0662) | Acc_1: (97.58%) (10117/10368)\n",
      "Epoch: 58 | Batch_idx: 90 |  Loss_1: (0.0668) | Acc_1: (97.58%) (11366/11648)\n",
      "Epoch: 58 | Batch_idx: 100 |  Loss_1: (0.0663) | Acc_1: (97.62%) (12620/12928)\n",
      "Epoch: 58 | Batch_idx: 110 |  Loss_1: (0.0666) | Acc_1: (97.58%) (13864/14208)\n",
      "Epoch: 58 | Batch_idx: 120 |  Loss_1: (0.0670) | Acc_1: (97.53%) (15106/15488)\n",
      "Epoch: 58 | Batch_idx: 130 |  Loss_1: (0.0672) | Acc_1: (97.51%) (16351/16768)\n",
      "Epoch: 58 | Batch_idx: 140 |  Loss_1: (0.0681) | Acc_1: (97.49%) (17595/18048)\n",
      "Epoch: 58 | Batch_idx: 150 |  Loss_1: (0.0674) | Acc_1: (97.51%) (18846/19328)\n",
      "Epoch: 58 | Batch_idx: 160 |  Loss_1: (0.0678) | Acc_1: (97.51%) (20095/20608)\n",
      "Epoch: 58 | Batch_idx: 170 |  Loss_1: (0.0687) | Acc_1: (97.48%) (21336/21888)\n",
      "Epoch: 58 | Batch_idx: 180 |  Loss_1: (0.0694) | Acc_1: (97.47%) (22582/23168)\n",
      "Epoch: 58 | Batch_idx: 190 |  Loss_1: (0.0697) | Acc_1: (97.45%) (23824/24448)\n",
      "Epoch: 58 | Batch_idx: 200 |  Loss_1: (0.0689) | Acc_1: (97.47%) (25078/25728)\n",
      "Epoch: 58 | Batch_idx: 210 |  Loss_1: (0.0689) | Acc_1: (97.46%) (26322/27008)\n",
      "Epoch: 58 | Batch_idx: 220 |  Loss_1: (0.0695) | Acc_1: (97.44%) (27565/28288)\n",
      "Epoch: 58 | Batch_idx: 230 |  Loss_1: (0.0703) | Acc_1: (97.42%) (28804/29568)\n",
      "Epoch: 58 | Batch_idx: 240 |  Loss_1: (0.0717) | Acc_1: (97.35%) (30032/30848)\n",
      "Epoch: 58 | Batch_idx: 250 |  Loss_1: (0.0725) | Acc_1: (97.33%) (31270/32128)\n",
      "Epoch: 58 | Batch_idx: 260 |  Loss_1: (0.0742) | Acc_1: (97.27%) (32497/33408)\n",
      "Epoch: 58 | Batch_idx: 270 |  Loss_1: (0.0741) | Acc_1: (97.27%) (33740/34688)\n",
      "Epoch: 58 | Batch_idx: 280 |  Loss_1: (0.0739) | Acc_1: (97.27%) (34986/35968)\n",
      "Epoch: 58 | Batch_idx: 290 |  Loss_1: (0.0744) | Acc_1: (97.27%) (36232/37248)\n",
      "Epoch: 58 | Batch_idx: 300 |  Loss_1: (0.0744) | Acc_1: (97.26%) (37474/38528)\n",
      "Epoch: 58 | Batch_idx: 310 |  Loss_1: (0.0742) | Acc_1: (97.29%) (38729/39808)\n",
      "Epoch: 58 | Batch_idx: 320 |  Loss_1: (0.0745) | Acc_1: (97.27%) (39965/41088)\n",
      "Epoch: 58 | Batch_idx: 330 |  Loss_1: (0.0753) | Acc_1: (97.24%) (41198/42368)\n",
      "Epoch: 58 | Batch_idx: 340 |  Loss_1: (0.0759) | Acc_1: (97.21%) (42430/43648)\n",
      "Epoch: 58 | Batch_idx: 350 |  Loss_1: (0.0760) | Acc_1: (97.21%) (43673/44928)\n",
      "Epoch: 58 | Batch_idx: 360 |  Loss_1: (0.0766) | Acc_1: (97.20%) (44912/46208)\n",
      "Epoch: 58 | Batch_idx: 370 |  Loss_1: (0.0774) | Acc_1: (97.16%) (46139/47488)\n",
      "Epoch: 58 | Batch_idx: 380 |  Loss_1: (0.0774) | Acc_1: (97.16%) (47383/48768)\n",
      "Epoch: 58 | Batch_idx: 390 |  Loss_1: (0.0775) | Acc_1: (97.16%) (48581/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4330) | Acc: (89.68%) (8968/10000)\n",
      "Epoch: 59 | Batch_idx: 0 |  Loss_1: (0.0320) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 59 | Batch_idx: 10 |  Loss_1: (0.0711) | Acc_1: (97.59%) (1374/1408)\n",
      "Epoch: 59 | Batch_idx: 20 |  Loss_1: (0.0747) | Acc_1: (97.36%) (2617/2688)\n",
      "Epoch: 59 | Batch_idx: 30 |  Loss_1: (0.0750) | Acc_1: (97.45%) (3867/3968)\n",
      "Epoch: 59 | Batch_idx: 40 |  Loss_1: (0.0773) | Acc_1: (97.45%) (5114/5248)\n",
      "Epoch: 59 | Batch_idx: 50 |  Loss_1: (0.0754) | Acc_1: (97.44%) (6361/6528)\n",
      "Epoch: 59 | Batch_idx: 60 |  Loss_1: (0.0719) | Acc_1: (97.55%) (7617/7808)\n",
      "Epoch: 59 | Batch_idx: 70 |  Loss_1: (0.0734) | Acc_1: (97.45%) (8856/9088)\n",
      "Epoch: 59 | Batch_idx: 80 |  Loss_1: (0.0709) | Acc_1: (97.55%) (10114/10368)\n",
      "Epoch: 59 | Batch_idx: 90 |  Loss_1: (0.0704) | Acc_1: (97.55%) (11363/11648)\n",
      "Epoch: 59 | Batch_idx: 100 |  Loss_1: (0.0707) | Acc_1: (97.56%) (12612/12928)\n",
      "Epoch: 59 | Batch_idx: 110 |  Loss_1: (0.0707) | Acc_1: (97.56%) (13862/14208)\n",
      "Epoch: 59 | Batch_idx: 120 |  Loss_1: (0.0700) | Acc_1: (97.56%) (15110/15488)\n",
      "Epoch: 59 | Batch_idx: 130 |  Loss_1: (0.0701) | Acc_1: (97.55%) (16357/16768)\n",
      "Epoch: 59 | Batch_idx: 140 |  Loss_1: (0.0696) | Acc_1: (97.54%) (17604/18048)\n",
      "Epoch: 59 | Batch_idx: 150 |  Loss_1: (0.0697) | Acc_1: (97.55%) (18855/19328)\n",
      "Epoch: 59 | Batch_idx: 160 |  Loss_1: (0.0698) | Acc_1: (97.56%) (20105/20608)\n",
      "Epoch: 59 | Batch_idx: 170 |  Loss_1: (0.0704) | Acc_1: (97.54%) (21349/21888)\n",
      "Epoch: 59 | Batch_idx: 180 |  Loss_1: (0.0707) | Acc_1: (97.52%) (22593/23168)\n",
      "Epoch: 59 | Batch_idx: 190 |  Loss_1: (0.0706) | Acc_1: (97.54%) (23847/24448)\n",
      "Epoch: 59 | Batch_idx: 200 |  Loss_1: (0.0719) | Acc_1: (97.52%) (25091/25728)\n",
      "Epoch: 59 | Batch_idx: 210 |  Loss_1: (0.0726) | Acc_1: (97.49%) (26331/27008)\n",
      "Epoch: 59 | Batch_idx: 220 |  Loss_1: (0.0739) | Acc_1: (97.45%) (27567/28288)\n",
      "Epoch: 59 | Batch_idx: 230 |  Loss_1: (0.0734) | Acc_1: (97.47%) (28821/29568)\n",
      "Epoch: 59 | Batch_idx: 240 |  Loss_1: (0.0725) | Acc_1: (97.49%) (30073/30848)\n",
      "Epoch: 59 | Batch_idx: 250 |  Loss_1: (0.0725) | Acc_1: (97.49%) (31322/32128)\n",
      "Epoch: 59 | Batch_idx: 260 |  Loss_1: (0.0728) | Acc_1: (97.49%) (32569/33408)\n",
      "Epoch: 59 | Batch_idx: 270 |  Loss_1: (0.0722) | Acc_1: (97.50%) (33821/34688)\n",
      "Epoch: 59 | Batch_idx: 280 |  Loss_1: (0.0714) | Acc_1: (97.53%) (35081/35968)\n",
      "Epoch: 59 | Batch_idx: 290 |  Loss_1: (0.0711) | Acc_1: (97.55%) (36335/37248)\n",
      "Epoch: 59 | Batch_idx: 300 |  Loss_1: (0.0704) | Acc_1: (97.58%) (37595/38528)\n",
      "Epoch: 59 | Batch_idx: 310 |  Loss_1: (0.0701) | Acc_1: (97.60%) (38851/39808)\n",
      "Epoch: 59 | Batch_idx: 320 |  Loss_1: (0.0698) | Acc_1: (97.59%) (40097/41088)\n",
      "Epoch: 59 | Batch_idx: 330 |  Loss_1: (0.0695) | Acc_1: (97.59%) (41348/42368)\n",
      "Epoch: 59 | Batch_idx: 340 |  Loss_1: (0.0695) | Acc_1: (97.59%) (42595/43648)\n",
      "Epoch: 59 | Batch_idx: 350 |  Loss_1: (0.0692) | Acc_1: (97.58%) (43840/44928)\n",
      "Epoch: 59 | Batch_idx: 360 |  Loss_1: (0.0694) | Acc_1: (97.57%) (45083/46208)\n",
      "Epoch: 59 | Batch_idx: 370 |  Loss_1: (0.0693) | Acc_1: (97.57%) (46333/47488)\n",
      "Epoch: 59 | Batch_idx: 380 |  Loss_1: (0.0694) | Acc_1: (97.57%) (47581/48768)\n",
      "Epoch: 59 | Batch_idx: 390 |  Loss_1: (0.0693) | Acc_1: (97.57%) (48784/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4532) | Acc: (89.50%) (8950/10000)\n",
      "Epoch: 60 | Batch_idx: 0 |  Loss_1: (0.0298) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 60 | Batch_idx: 10 |  Loss_1: (0.0553) | Acc_1: (98.30%) (1384/1408)\n",
      "Epoch: 60 | Batch_idx: 20 |  Loss_1: (0.0578) | Acc_1: (98.21%) (2640/2688)\n",
      "Epoch: 60 | Batch_idx: 30 |  Loss_1: (0.0629) | Acc_1: (97.98%) (3888/3968)\n",
      "Epoch: 60 | Batch_idx: 40 |  Loss_1: (0.0576) | Acc_1: (98.02%) (5144/5248)\n",
      "Epoch: 60 | Batch_idx: 50 |  Loss_1: (0.0571) | Acc_1: (98.01%) (6398/6528)\n",
      "Epoch: 60 | Batch_idx: 60 |  Loss_1: (0.0580) | Acc_1: (97.94%) (7647/7808)\n",
      "Epoch: 60 | Batch_idx: 70 |  Loss_1: (0.0569) | Acc_1: (98.01%) (8907/9088)\n",
      "Epoch: 60 | Batch_idx: 80 |  Loss_1: (0.0577) | Acc_1: (97.93%) (10153/10368)\n",
      "Epoch: 60 | Batch_idx: 90 |  Loss_1: (0.0604) | Acc_1: (97.87%) (11400/11648)\n",
      "Epoch: 60 | Batch_idx: 100 |  Loss_1: (0.0605) | Acc_1: (97.86%) (12651/12928)\n",
      "Epoch: 60 | Batch_idx: 110 |  Loss_1: (0.0600) | Acc_1: (97.90%) (13909/14208)\n",
      "Epoch: 60 | Batch_idx: 120 |  Loss_1: (0.0586) | Acc_1: (97.91%) (15164/15488)\n",
      "Epoch: 60 | Batch_idx: 130 |  Loss_1: (0.0589) | Acc_1: (97.92%) (16419/16768)\n",
      "Epoch: 60 | Batch_idx: 140 |  Loss_1: (0.0592) | Acc_1: (97.90%) (17669/18048)\n",
      "Epoch: 60 | Batch_idx: 150 |  Loss_1: (0.0601) | Acc_1: (97.89%) (18920/19328)\n",
      "Epoch: 60 | Batch_idx: 160 |  Loss_1: (0.0608) | Acc_1: (97.87%) (20170/20608)\n",
      "Epoch: 60 | Batch_idx: 170 |  Loss_1: (0.0617) | Acc_1: (97.84%) (21415/21888)\n",
      "Epoch: 60 | Batch_idx: 180 |  Loss_1: (0.0630) | Acc_1: (97.81%) (22661/23168)\n",
      "Epoch: 60 | Batch_idx: 190 |  Loss_1: (0.0631) | Acc_1: (97.80%) (23910/24448)\n",
      "Epoch: 60 | Batch_idx: 200 |  Loss_1: (0.0636) | Acc_1: (97.78%) (25158/25728)\n",
      "Epoch: 60 | Batch_idx: 210 |  Loss_1: (0.0639) | Acc_1: (97.76%) (26403/27008)\n",
      "Epoch: 60 | Batch_idx: 220 |  Loss_1: (0.0640) | Acc_1: (97.74%) (27648/28288)\n",
      "Epoch: 60 | Batch_idx: 230 |  Loss_1: (0.0631) | Acc_1: (97.77%) (28910/29568)\n",
      "Epoch: 60 | Batch_idx: 240 |  Loss_1: (0.0627) | Acc_1: (97.79%) (30167/30848)\n",
      "Epoch: 60 | Batch_idx: 250 |  Loss_1: (0.0633) | Acc_1: (97.79%) (31417/32128)\n",
      "Epoch: 60 | Batch_idx: 260 |  Loss_1: (0.0636) | Acc_1: (97.79%) (32669/33408)\n",
      "Epoch: 60 | Batch_idx: 270 |  Loss_1: (0.0631) | Acc_1: (97.81%) (33928/34688)\n",
      "Epoch: 60 | Batch_idx: 280 |  Loss_1: (0.0629) | Acc_1: (97.82%) (35183/35968)\n",
      "Epoch: 60 | Batch_idx: 290 |  Loss_1: (0.0628) | Acc_1: (97.83%) (36438/37248)\n",
      "Epoch: 60 | Batch_idx: 300 |  Loss_1: (0.0632) | Acc_1: (97.81%) (37685/38528)\n",
      "Epoch: 60 | Batch_idx: 310 |  Loss_1: (0.0634) | Acc_1: (97.79%) (38928/39808)\n",
      "Epoch: 60 | Batch_idx: 320 |  Loss_1: (0.0635) | Acc_1: (97.79%) (40179/41088)\n",
      "Epoch: 60 | Batch_idx: 330 |  Loss_1: (0.0634) | Acc_1: (97.79%) (41430/42368)\n",
      "Epoch: 60 | Batch_idx: 340 |  Loss_1: (0.0640) | Acc_1: (97.78%) (42679/43648)\n",
      "Epoch: 60 | Batch_idx: 350 |  Loss_1: (0.0644) | Acc_1: (97.77%) (43926/44928)\n",
      "Epoch: 60 | Batch_idx: 360 |  Loss_1: (0.0641) | Acc_1: (97.78%) (45183/46208)\n",
      "Epoch: 60 | Batch_idx: 370 |  Loss_1: (0.0640) | Acc_1: (97.78%) (46433/47488)\n",
      "Epoch: 60 | Batch_idx: 380 |  Loss_1: (0.0642) | Acc_1: (97.77%) (47682/48768)\n",
      "Epoch: 60 | Batch_idx: 390 |  Loss_1: (0.0643) | Acc_1: (97.77%) (48886/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4632) | Acc: (89.70%) (8970/10000)\n",
      "Epoch: 61 | Batch_idx: 0 |  Loss_1: (0.0175) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 61 | Batch_idx: 10 |  Loss_1: (0.0404) | Acc_1: (98.37%) (1385/1408)\n",
      "Epoch: 61 | Batch_idx: 20 |  Loss_1: (0.0489) | Acc_1: (98.33%) (2643/2688)\n",
      "Epoch: 61 | Batch_idx: 30 |  Loss_1: (0.0553) | Acc_1: (98.06%) (3891/3968)\n",
      "Epoch: 61 | Batch_idx: 40 |  Loss_1: (0.0533) | Acc_1: (98.15%) (5151/5248)\n",
      "Epoch: 61 | Batch_idx: 50 |  Loss_1: (0.0536) | Acc_1: (98.18%) (6409/6528)\n",
      "Epoch: 61 | Batch_idx: 60 |  Loss_1: (0.0528) | Acc_1: (98.19%) (7667/7808)\n",
      "Epoch: 61 | Batch_idx: 70 |  Loss_1: (0.0536) | Acc_1: (98.20%) (8924/9088)\n",
      "Epoch: 61 | Batch_idx: 80 |  Loss_1: (0.0577) | Acc_1: (98.03%) (10164/10368)\n",
      "Epoch: 61 | Batch_idx: 90 |  Loss_1: (0.0581) | Acc_1: (97.98%) (11413/11648)\n",
      "Epoch: 61 | Batch_idx: 100 |  Loss_1: (0.0567) | Acc_1: (98.04%) (12675/12928)\n",
      "Epoch: 61 | Batch_idx: 110 |  Loss_1: (0.0585) | Acc_1: (97.98%) (13921/14208)\n",
      "Epoch: 61 | Batch_idx: 120 |  Loss_1: (0.0583) | Acc_1: (97.94%) (15169/15488)\n",
      "Epoch: 61 | Batch_idx: 130 |  Loss_1: (0.0582) | Acc_1: (97.91%) (16418/16768)\n",
      "Epoch: 61 | Batch_idx: 140 |  Loss_1: (0.0597) | Acc_1: (97.85%) (17660/18048)\n",
      "Epoch: 61 | Batch_idx: 150 |  Loss_1: (0.0600) | Acc_1: (97.85%) (18913/19328)\n",
      "Epoch: 61 | Batch_idx: 160 |  Loss_1: (0.0597) | Acc_1: (97.86%) (20167/20608)\n",
      "Epoch: 61 | Batch_idx: 170 |  Loss_1: (0.0605) | Acc_1: (97.83%) (21412/21888)\n",
      "Epoch: 61 | Batch_idx: 180 |  Loss_1: (0.0605) | Acc_1: (97.85%) (22669/23168)\n",
      "Epoch: 61 | Batch_idx: 190 |  Loss_1: (0.0611) | Acc_1: (97.81%) (23913/24448)\n",
      "Epoch: 61 | Batch_idx: 200 |  Loss_1: (0.0615) | Acc_1: (97.81%) (25165/25728)\n",
      "Epoch: 61 | Batch_idx: 210 |  Loss_1: (0.0619) | Acc_1: (97.80%) (26414/27008)\n",
      "Epoch: 61 | Batch_idx: 220 |  Loss_1: (0.0615) | Acc_1: (97.85%) (27679/28288)\n",
      "Epoch: 61 | Batch_idx: 230 |  Loss_1: (0.0622) | Acc_1: (97.84%) (28929/29568)\n",
      "Epoch: 61 | Batch_idx: 240 |  Loss_1: (0.0623) | Acc_1: (97.82%) (30175/30848)\n",
      "Epoch: 61 | Batch_idx: 250 |  Loss_1: (0.0639) | Acc_1: (97.76%) (31407/32128)\n",
      "Epoch: 61 | Batch_idx: 260 |  Loss_1: (0.0640) | Acc_1: (97.76%) (32660/33408)\n",
      "Epoch: 61 | Batch_idx: 270 |  Loss_1: (0.0641) | Acc_1: (97.74%) (33904/34688)\n",
      "Epoch: 61 | Batch_idx: 280 |  Loss_1: (0.0648) | Acc_1: (97.73%) (35151/35968)\n",
      "Epoch: 61 | Batch_idx: 290 |  Loss_1: (0.0655) | Acc_1: (97.72%) (36398/37248)\n",
      "Epoch: 61 | Batch_idx: 300 |  Loss_1: (0.0664) | Acc_1: (97.70%) (37642/38528)\n",
      "Epoch: 61 | Batch_idx: 310 |  Loss_1: (0.0662) | Acc_1: (97.70%) (38894/39808)\n",
      "Epoch: 61 | Batch_idx: 320 |  Loss_1: (0.0664) | Acc_1: (97.70%) (40142/41088)\n",
      "Epoch: 61 | Batch_idx: 330 |  Loss_1: (0.0661) | Acc_1: (97.71%) (41396/42368)\n",
      "Epoch: 61 | Batch_idx: 340 |  Loss_1: (0.0660) | Acc_1: (97.70%) (42646/43648)\n",
      "Epoch: 61 | Batch_idx: 350 |  Loss_1: (0.0655) | Acc_1: (97.71%) (43901/44928)\n",
      "Epoch: 61 | Batch_idx: 360 |  Loss_1: (0.0652) | Acc_1: (97.73%) (45158/46208)\n",
      "Epoch: 61 | Batch_idx: 370 |  Loss_1: (0.0655) | Acc_1: (97.72%) (46403/47488)\n",
      "Epoch: 61 | Batch_idx: 380 |  Loss_1: (0.0665) | Acc_1: (97.68%) (47635/48768)\n",
      "Epoch: 61 | Batch_idx: 390 |  Loss_1: (0.0672) | Acc_1: (97.66%) (48829/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5309) | Acc: (88.94%) (8894/10000)\n",
      "Epoch: 62 | Batch_idx: 0 |  Loss_1: (0.0674) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 62 | Batch_idx: 10 |  Loss_1: (0.0551) | Acc_1: (98.30%) (1384/1408)\n",
      "Epoch: 62 | Batch_idx: 20 |  Loss_1: (0.0702) | Acc_1: (97.54%) (2622/2688)\n",
      "Epoch: 62 | Batch_idx: 30 |  Loss_1: (0.0672) | Acc_1: (97.68%) (3876/3968)\n",
      "Epoch: 62 | Batch_idx: 40 |  Loss_1: (0.0629) | Acc_1: (97.79%) (5132/5248)\n",
      "Epoch: 62 | Batch_idx: 50 |  Loss_1: (0.0596) | Acc_1: (97.93%) (6393/6528)\n",
      "Epoch: 62 | Batch_idx: 60 |  Loss_1: (0.0589) | Acc_1: (97.91%) (7645/7808)\n",
      "Epoch: 62 | Batch_idx: 70 |  Loss_1: (0.0569) | Acc_1: (98.00%) (8906/9088)\n",
      "Epoch: 62 | Batch_idx: 80 |  Loss_1: (0.0566) | Acc_1: (97.99%) (10160/10368)\n",
      "Epoch: 62 | Batch_idx: 90 |  Loss_1: (0.0576) | Acc_1: (98.02%) (11417/11648)\n",
      "Epoch: 62 | Batch_idx: 100 |  Loss_1: (0.0565) | Acc_1: (98.04%) (12674/12928)\n",
      "Epoch: 62 | Batch_idx: 110 |  Loss_1: (0.0555) | Acc_1: (98.08%) (13935/14208)\n",
      "Epoch: 62 | Batch_idx: 120 |  Loss_1: (0.0561) | Acc_1: (98.06%) (15188/15488)\n",
      "Epoch: 62 | Batch_idx: 130 |  Loss_1: (0.0561) | Acc_1: (98.01%) (16435/16768)\n",
      "Epoch: 62 | Batch_idx: 140 |  Loss_1: (0.0551) | Acc_1: (98.07%) (17700/18048)\n",
      "Epoch: 62 | Batch_idx: 150 |  Loss_1: (0.0563) | Acc_1: (98.01%) (18943/19328)\n",
      "Epoch: 62 | Batch_idx: 160 |  Loss_1: (0.0567) | Acc_1: (98.00%) (20195/20608)\n",
      "Epoch: 62 | Batch_idx: 170 |  Loss_1: (0.0575) | Acc_1: (97.97%) (21444/21888)\n",
      "Epoch: 62 | Batch_idx: 180 |  Loss_1: (0.0577) | Acc_1: (97.95%) (22694/23168)\n",
      "Epoch: 62 | Batch_idx: 190 |  Loss_1: (0.0578) | Acc_1: (97.95%) (23948/24448)\n",
      "Epoch: 62 | Batch_idx: 200 |  Loss_1: (0.0574) | Acc_1: (97.95%) (25200/25728)\n",
      "Epoch: 62 | Batch_idx: 210 |  Loss_1: (0.0584) | Acc_1: (97.90%) (26440/27008)\n",
      "Epoch: 62 | Batch_idx: 220 |  Loss_1: (0.0589) | Acc_1: (97.88%) (27689/28288)\n",
      "Epoch: 62 | Batch_idx: 230 |  Loss_1: (0.0588) | Acc_1: (97.89%) (28945/29568)\n",
      "Epoch: 62 | Batch_idx: 240 |  Loss_1: (0.0595) | Acc_1: (97.87%) (30191/30848)\n",
      "Epoch: 62 | Batch_idx: 250 |  Loss_1: (0.0593) | Acc_1: (97.87%) (31445/32128)\n",
      "Epoch: 62 | Batch_idx: 260 |  Loss_1: (0.0594) | Acc_1: (97.87%) (32697/33408)\n",
      "Epoch: 62 | Batch_idx: 270 |  Loss_1: (0.0597) | Acc_1: (97.84%) (33939/34688)\n",
      "Epoch: 62 | Batch_idx: 280 |  Loss_1: (0.0600) | Acc_1: (97.82%) (35185/35968)\n",
      "Epoch: 62 | Batch_idx: 290 |  Loss_1: (0.0602) | Acc_1: (97.82%) (36435/37248)\n",
      "Epoch: 62 | Batch_idx: 300 |  Loss_1: (0.0606) | Acc_1: (97.81%) (37683/38528)\n",
      "Epoch: 62 | Batch_idx: 310 |  Loss_1: (0.0613) | Acc_1: (97.78%) (38925/39808)\n",
      "Epoch: 62 | Batch_idx: 320 |  Loss_1: (0.0612) | Acc_1: (97.79%) (40179/41088)\n",
      "Epoch: 62 | Batch_idx: 330 |  Loss_1: (0.0610) | Acc_1: (97.80%) (41437/42368)\n",
      "Epoch: 62 | Batch_idx: 340 |  Loss_1: (0.0614) | Acc_1: (97.80%) (42686/43648)\n",
      "Epoch: 62 | Batch_idx: 350 |  Loss_1: (0.0610) | Acc_1: (97.81%) (43944/44928)\n",
      "Epoch: 62 | Batch_idx: 360 |  Loss_1: (0.0610) | Acc_1: (97.80%) (45193/46208)\n",
      "Epoch: 62 | Batch_idx: 370 |  Loss_1: (0.0610) | Acc_1: (97.80%) (46445/47488)\n",
      "Epoch: 62 | Batch_idx: 380 |  Loss_1: (0.0610) | Acc_1: (97.80%) (47693/48768)\n",
      "Epoch: 62 | Batch_idx: 390 |  Loss_1: (0.0615) | Acc_1: (97.77%) (48884/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4083) | Acc: (90.28%) (9028/10000)\n",
      "Epoch: 63 | Batch_idx: 0 |  Loss_1: (0.0146) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 63 | Batch_idx: 10 |  Loss_1: (0.0498) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 63 | Batch_idx: 20 |  Loss_1: (0.0602) | Acc_1: (98.14%) (2638/2688)\n",
      "Epoch: 63 | Batch_idx: 30 |  Loss_1: (0.0570) | Acc_1: (98.21%) (3897/3968)\n",
      "Epoch: 63 | Batch_idx: 40 |  Loss_1: (0.0600) | Acc_1: (98.09%) (5148/5248)\n",
      "Epoch: 63 | Batch_idx: 50 |  Loss_1: (0.0606) | Acc_1: (98.09%) (6403/6528)\n",
      "Epoch: 63 | Batch_idx: 60 |  Loss_1: (0.0588) | Acc_1: (98.10%) (7660/7808)\n",
      "Epoch: 63 | Batch_idx: 70 |  Loss_1: (0.0581) | Acc_1: (98.09%) (8914/9088)\n",
      "Epoch: 63 | Batch_idx: 80 |  Loss_1: (0.0580) | Acc_1: (98.06%) (10167/10368)\n",
      "Epoch: 63 | Batch_idx: 90 |  Loss_1: (0.0582) | Acc_1: (98.03%) (11419/11648)\n",
      "Epoch: 63 | Batch_idx: 100 |  Loss_1: (0.0574) | Acc_1: (98.03%) (12673/12928)\n",
      "Epoch: 63 | Batch_idx: 110 |  Loss_1: (0.0580) | Acc_1: (98.01%) (13925/14208)\n",
      "Epoch: 63 | Batch_idx: 120 |  Loss_1: (0.0588) | Acc_1: (97.94%) (15169/15488)\n",
      "Epoch: 63 | Batch_idx: 130 |  Loss_1: (0.0615) | Acc_1: (97.88%) (16413/16768)\n",
      "Epoch: 63 | Batch_idx: 140 |  Loss_1: (0.0610) | Acc_1: (97.88%) (17665/18048)\n",
      "Epoch: 63 | Batch_idx: 150 |  Loss_1: (0.0601) | Acc_1: (97.92%) (18926/19328)\n",
      "Epoch: 63 | Batch_idx: 160 |  Loss_1: (0.0599) | Acc_1: (97.93%) (20182/20608)\n",
      "Epoch: 63 | Batch_idx: 170 |  Loss_1: (0.0606) | Acc_1: (97.91%) (21431/21888)\n",
      "Epoch: 63 | Batch_idx: 180 |  Loss_1: (0.0605) | Acc_1: (97.92%) (22686/23168)\n",
      "Epoch: 63 | Batch_idx: 190 |  Loss_1: (0.0609) | Acc_1: (97.93%) (23942/24448)\n",
      "Epoch: 63 | Batch_idx: 200 |  Loss_1: (0.0611) | Acc_1: (97.92%) (25193/25728)\n",
      "Epoch: 63 | Batch_idx: 210 |  Loss_1: (0.0612) | Acc_1: (97.92%) (26445/27008)\n",
      "Epoch: 63 | Batch_idx: 220 |  Loss_1: (0.0615) | Acc_1: (97.90%) (27694/28288)\n",
      "Epoch: 63 | Batch_idx: 230 |  Loss_1: (0.0611) | Acc_1: (97.91%) (28950/29568)\n",
      "Epoch: 63 | Batch_idx: 240 |  Loss_1: (0.0609) | Acc_1: (97.91%) (30203/30848)\n",
      "Epoch: 63 | Batch_idx: 250 |  Loss_1: (0.0605) | Acc_1: (97.93%) (31464/32128)\n",
      "Epoch: 63 | Batch_idx: 260 |  Loss_1: (0.0602) | Acc_1: (97.94%) (32720/33408)\n",
      "Epoch: 63 | Batch_idx: 270 |  Loss_1: (0.0596) | Acc_1: (97.95%) (33978/34688)\n",
      "Epoch: 63 | Batch_idx: 280 |  Loss_1: (0.0600) | Acc_1: (97.94%) (35227/35968)\n",
      "Epoch: 63 | Batch_idx: 290 |  Loss_1: (0.0606) | Acc_1: (97.91%) (36468/37248)\n",
      "Epoch: 63 | Batch_idx: 300 |  Loss_1: (0.0604) | Acc_1: (97.90%) (37718/38528)\n",
      "Epoch: 63 | Batch_idx: 310 |  Loss_1: (0.0603) | Acc_1: (97.90%) (38973/39808)\n",
      "Epoch: 63 | Batch_idx: 320 |  Loss_1: (0.0603) | Acc_1: (97.90%) (40227/41088)\n",
      "Epoch: 63 | Batch_idx: 330 |  Loss_1: (0.0599) | Acc_1: (97.92%) (41486/42368)\n",
      "Epoch: 63 | Batch_idx: 340 |  Loss_1: (0.0606) | Acc_1: (97.89%) (42729/43648)\n",
      "Epoch: 63 | Batch_idx: 350 |  Loss_1: (0.0606) | Acc_1: (97.90%) (43984/44928)\n",
      "Epoch: 63 | Batch_idx: 360 |  Loss_1: (0.0601) | Acc_1: (97.91%) (45241/46208)\n",
      "Epoch: 63 | Batch_idx: 370 |  Loss_1: (0.0602) | Acc_1: (97.91%) (46497/47488)\n",
      "Epoch: 63 | Batch_idx: 380 |  Loss_1: (0.0607) | Acc_1: (97.90%) (47743/48768)\n",
      "Epoch: 63 | Batch_idx: 390 |  Loss_1: (0.0606) | Acc_1: (97.91%) (48954/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4433) | Acc: (90.08%) (9008/10000)\n",
      "Epoch: 64 | Batch_idx: 0 |  Loss_1: (0.0200) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 64 | Batch_idx: 10 |  Loss_1: (0.0430) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 64 | Batch_idx: 20 |  Loss_1: (0.0472) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 64 | Batch_idx: 30 |  Loss_1: (0.0550) | Acc_1: (98.08%) (3892/3968)\n",
      "Epoch: 64 | Batch_idx: 40 |  Loss_1: (0.0548) | Acc_1: (98.11%) (5149/5248)\n",
      "Epoch: 64 | Batch_idx: 50 |  Loss_1: (0.0526) | Acc_1: (98.16%) (6408/6528)\n",
      "Epoch: 64 | Batch_idx: 60 |  Loss_1: (0.0498) | Acc_1: (98.21%) (7668/7808)\n",
      "Epoch: 64 | Batch_idx: 70 |  Loss_1: (0.0500) | Acc_1: (98.24%) (8928/9088)\n",
      "Epoch: 64 | Batch_idx: 80 |  Loss_1: (0.0486) | Acc_1: (98.26%) (10188/10368)\n",
      "Epoch: 64 | Batch_idx: 90 |  Loss_1: (0.0493) | Acc_1: (98.25%) (11444/11648)\n",
      "Epoch: 64 | Batch_idx: 100 |  Loss_1: (0.0485) | Acc_1: (98.28%) (12706/12928)\n",
      "Epoch: 64 | Batch_idx: 110 |  Loss_1: (0.0487) | Acc_1: (98.29%) (13965/14208)\n",
      "Epoch: 64 | Batch_idx: 120 |  Loss_1: (0.0505) | Acc_1: (98.23%) (15214/15488)\n",
      "Epoch: 64 | Batch_idx: 130 |  Loss_1: (0.0507) | Acc_1: (98.23%) (16471/16768)\n",
      "Epoch: 64 | Batch_idx: 140 |  Loss_1: (0.0518) | Acc_1: (98.21%) (17725/18048)\n",
      "Epoch: 64 | Batch_idx: 150 |  Loss_1: (0.0521) | Acc_1: (98.17%) (18975/19328)\n",
      "Epoch: 64 | Batch_idx: 160 |  Loss_1: (0.0547) | Acc_1: (98.06%) (20209/20608)\n",
      "Epoch: 64 | Batch_idx: 170 |  Loss_1: (0.0558) | Acc_1: (98.04%) (21459/21888)\n",
      "Epoch: 64 | Batch_idx: 180 |  Loss_1: (0.0564) | Acc_1: (98.00%) (22705/23168)\n",
      "Epoch: 64 | Batch_idx: 190 |  Loss_1: (0.0570) | Acc_1: (97.99%) (23956/24448)\n",
      "Epoch: 64 | Batch_idx: 200 |  Loss_1: (0.0580) | Acc_1: (97.97%) (25205/25728)\n",
      "Epoch: 64 | Batch_idx: 210 |  Loss_1: (0.0584) | Acc_1: (97.97%) (26459/27008)\n",
      "Epoch: 64 | Batch_idx: 220 |  Loss_1: (0.0581) | Acc_1: (97.98%) (27717/28288)\n",
      "Epoch: 64 | Batch_idx: 230 |  Loss_1: (0.0581) | Acc_1: (98.01%) (28979/29568)\n",
      "Epoch: 64 | Batch_idx: 240 |  Loss_1: (0.0583) | Acc_1: (98.01%) (30234/30848)\n",
      "Epoch: 64 | Batch_idx: 250 |  Loss_1: (0.0581) | Acc_1: (97.99%) (31482/32128)\n",
      "Epoch: 64 | Batch_idx: 260 |  Loss_1: (0.0585) | Acc_1: (97.98%) (32732/33408)\n",
      "Epoch: 64 | Batch_idx: 270 |  Loss_1: (0.0583) | Acc_1: (97.97%) (33985/34688)\n",
      "Epoch: 64 | Batch_idx: 280 |  Loss_1: (0.0576) | Acc_1: (98.00%) (35249/35968)\n",
      "Epoch: 64 | Batch_idx: 290 |  Loss_1: (0.0571) | Acc_1: (98.02%) (36510/37248)\n",
      "Epoch: 64 | Batch_idx: 300 |  Loss_1: (0.0569) | Acc_1: (98.02%) (37765/38528)\n",
      "Epoch: 64 | Batch_idx: 310 |  Loss_1: (0.0566) | Acc_1: (98.04%) (39026/39808)\n",
      "Epoch: 64 | Batch_idx: 320 |  Loss_1: (0.0570) | Acc_1: (98.01%) (40271/41088)\n",
      "Epoch: 64 | Batch_idx: 330 |  Loss_1: (0.0569) | Acc_1: (98.02%) (41529/42368)\n",
      "Epoch: 64 | Batch_idx: 340 |  Loss_1: (0.0566) | Acc_1: (98.04%) (42794/43648)\n",
      "Epoch: 64 | Batch_idx: 350 |  Loss_1: (0.0568) | Acc_1: (98.04%) (44046/44928)\n",
      "Epoch: 64 | Batch_idx: 360 |  Loss_1: (0.0568) | Acc_1: (98.03%) (45299/46208)\n",
      "Epoch: 64 | Batch_idx: 370 |  Loss_1: (0.0568) | Acc_1: (98.04%) (46555/47488)\n",
      "Epoch: 64 | Batch_idx: 380 |  Loss_1: (0.0566) | Acc_1: (98.06%) (47820/48768)\n",
      "Epoch: 64 | Batch_idx: 390 |  Loss_1: (0.0567) | Acc_1: (98.05%) (49025/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4888) | Acc: (89.74%) (8974/10000)\n",
      "Epoch: 65 | Batch_idx: 0 |  Loss_1: (0.0516) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 65 | Batch_idx: 10 |  Loss_1: (0.0639) | Acc_1: (97.80%) (1377/1408)\n",
      "Epoch: 65 | Batch_idx: 20 |  Loss_1: (0.0525) | Acc_1: (98.18%) (2639/2688)\n",
      "Epoch: 65 | Batch_idx: 30 |  Loss_1: (0.0547) | Acc_1: (97.98%) (3888/3968)\n",
      "Epoch: 65 | Batch_idx: 40 |  Loss_1: (0.0496) | Acc_1: (98.17%) (5152/5248)\n",
      "Epoch: 65 | Batch_idx: 50 |  Loss_1: (0.0484) | Acc_1: (98.19%) (6410/6528)\n",
      "Epoch: 65 | Batch_idx: 60 |  Loss_1: (0.0466) | Acc_1: (98.30%) (7675/7808)\n",
      "Epoch: 65 | Batch_idx: 70 |  Loss_1: (0.0485) | Acc_1: (98.23%) (8927/9088)\n",
      "Epoch: 65 | Batch_idx: 80 |  Loss_1: (0.0494) | Acc_1: (98.16%) (10177/10368)\n",
      "Epoch: 65 | Batch_idx: 90 |  Loss_1: (0.0511) | Acc_1: (98.09%) (11425/11648)\n",
      "Epoch: 65 | Batch_idx: 100 |  Loss_1: (0.0500) | Acc_1: (98.13%) (12686/12928)\n",
      "Epoch: 65 | Batch_idx: 110 |  Loss_1: (0.0490) | Acc_1: (98.17%) (13948/14208)\n",
      "Epoch: 65 | Batch_idx: 120 |  Loss_1: (0.0496) | Acc_1: (98.14%) (15200/15488)\n",
      "Epoch: 65 | Batch_idx: 130 |  Loss_1: (0.0489) | Acc_1: (98.17%) (16461/16768)\n",
      "Epoch: 65 | Batch_idx: 140 |  Loss_1: (0.0492) | Acc_1: (98.18%) (17719/18048)\n",
      "Epoch: 65 | Batch_idx: 150 |  Loss_1: (0.0514) | Acc_1: (98.14%) (18968/19328)\n",
      "Epoch: 65 | Batch_idx: 160 |  Loss_1: (0.0513) | Acc_1: (98.14%) (20224/20608)\n",
      "Epoch: 65 | Batch_idx: 170 |  Loss_1: (0.0514) | Acc_1: (98.12%) (21477/21888)\n",
      "Epoch: 65 | Batch_idx: 180 |  Loss_1: (0.0513) | Acc_1: (98.14%) (22736/23168)\n",
      "Epoch: 65 | Batch_idx: 190 |  Loss_1: (0.0513) | Acc_1: (98.15%) (23995/24448)\n",
      "Epoch: 65 | Batch_idx: 200 |  Loss_1: (0.0514) | Acc_1: (98.13%) (25247/25728)\n",
      "Epoch: 65 | Batch_idx: 210 |  Loss_1: (0.0522) | Acc_1: (98.10%) (26496/27008)\n",
      "Epoch: 65 | Batch_idx: 220 |  Loss_1: (0.0525) | Acc_1: (98.09%) (27747/28288)\n",
      "Epoch: 65 | Batch_idx: 230 |  Loss_1: (0.0523) | Acc_1: (98.09%) (29003/29568)\n",
      "Epoch: 65 | Batch_idx: 240 |  Loss_1: (0.0523) | Acc_1: (98.09%) (30260/30848)\n",
      "Epoch: 65 | Batch_idx: 250 |  Loss_1: (0.0523) | Acc_1: (98.10%) (31519/32128)\n",
      "Epoch: 65 | Batch_idx: 260 |  Loss_1: (0.0524) | Acc_1: (98.10%) (32772/33408)\n",
      "Epoch: 65 | Batch_idx: 270 |  Loss_1: (0.0522) | Acc_1: (98.09%) (34026/34688)\n",
      "Epoch: 65 | Batch_idx: 280 |  Loss_1: (0.0519) | Acc_1: (98.09%) (35282/35968)\n",
      "Epoch: 65 | Batch_idx: 290 |  Loss_1: (0.0519) | Acc_1: (98.09%) (36538/37248)\n",
      "Epoch: 65 | Batch_idx: 300 |  Loss_1: (0.0522) | Acc_1: (98.08%) (37790/38528)\n",
      "Epoch: 65 | Batch_idx: 310 |  Loss_1: (0.0522) | Acc_1: (98.08%) (39044/39808)\n",
      "Epoch: 65 | Batch_idx: 320 |  Loss_1: (0.0527) | Acc_1: (98.07%) (40295/41088)\n",
      "Epoch: 65 | Batch_idx: 330 |  Loss_1: (0.0523) | Acc_1: (98.09%) (41560/42368)\n",
      "Epoch: 65 | Batch_idx: 340 |  Loss_1: (0.0521) | Acc_1: (98.11%) (42821/43648)\n",
      "Epoch: 65 | Batch_idx: 350 |  Loss_1: (0.0530) | Acc_1: (98.07%) (44063/44928)\n",
      "Epoch: 65 | Batch_idx: 360 |  Loss_1: (0.0537) | Acc_1: (98.03%) (45300/46208)\n",
      "Epoch: 65 | Batch_idx: 370 |  Loss_1: (0.0536) | Acc_1: (98.05%) (46561/47488)\n",
      "Epoch: 65 | Batch_idx: 380 |  Loss_1: (0.0538) | Acc_1: (98.05%) (47817/48768)\n",
      "Epoch: 65 | Batch_idx: 390 |  Loss_1: (0.0538) | Acc_1: (98.05%) (49025/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4338) | Acc: (90.70%) (9070/10000)\n",
      "Epoch: 66 | Batch_idx: 0 |  Loss_1: (0.0491) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 66 | Batch_idx: 10 |  Loss_1: (0.0383) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 66 | Batch_idx: 20 |  Loss_1: (0.0465) | Acc_1: (98.40%) (2645/2688)\n",
      "Epoch: 66 | Batch_idx: 30 |  Loss_1: (0.0542) | Acc_1: (98.21%) (3897/3968)\n",
      "Epoch: 66 | Batch_idx: 40 |  Loss_1: (0.0529) | Acc_1: (98.21%) (5154/5248)\n",
      "Epoch: 66 | Batch_idx: 50 |  Loss_1: (0.0514) | Acc_1: (98.25%) (6414/6528)\n",
      "Epoch: 66 | Batch_idx: 60 |  Loss_1: (0.0523) | Acc_1: (98.26%) (7672/7808)\n",
      "Epoch: 66 | Batch_idx: 70 |  Loss_1: (0.0509) | Acc_1: (98.28%) (8932/9088)\n",
      "Epoch: 66 | Batch_idx: 80 |  Loss_1: (0.0500) | Acc_1: (98.27%) (10189/10368)\n",
      "Epoch: 66 | Batch_idx: 90 |  Loss_1: (0.0503) | Acc_1: (98.28%) (11448/11648)\n",
      "Epoch: 66 | Batch_idx: 100 |  Loss_1: (0.0525) | Acc_1: (98.19%) (12694/12928)\n",
      "Epoch: 66 | Batch_idx: 110 |  Loss_1: (0.0528) | Acc_1: (98.16%) (13947/14208)\n",
      "Epoch: 66 | Batch_idx: 120 |  Loss_1: (0.0533) | Acc_1: (98.18%) (15206/15488)\n",
      "Epoch: 66 | Batch_idx: 130 |  Loss_1: (0.0531) | Acc_1: (98.18%) (16463/16768)\n",
      "Epoch: 66 | Batch_idx: 140 |  Loss_1: (0.0537) | Acc_1: (98.18%) (17719/18048)\n",
      "Epoch: 66 | Batch_idx: 150 |  Loss_1: (0.0537) | Acc_1: (98.18%) (18976/19328)\n",
      "Epoch: 66 | Batch_idx: 160 |  Loss_1: (0.0536) | Acc_1: (98.18%) (20233/20608)\n",
      "Epoch: 66 | Batch_idx: 170 |  Loss_1: (0.0528) | Acc_1: (98.22%) (21498/21888)\n",
      "Epoch: 66 | Batch_idx: 180 |  Loss_1: (0.0525) | Acc_1: (98.21%) (22754/23168)\n",
      "Epoch: 66 | Batch_idx: 190 |  Loss_1: (0.0529) | Acc_1: (98.21%) (24011/24448)\n",
      "Epoch: 66 | Batch_idx: 200 |  Loss_1: (0.0529) | Acc_1: (98.20%) (25264/25728)\n",
      "Epoch: 66 | Batch_idx: 210 |  Loss_1: (0.0530) | Acc_1: (98.17%) (26515/27008)\n",
      "Epoch: 66 | Batch_idx: 220 |  Loss_1: (0.0524) | Acc_1: (98.19%) (27775/28288)\n",
      "Epoch: 66 | Batch_idx: 230 |  Loss_1: (0.0521) | Acc_1: (98.20%) (29035/29568)\n",
      "Epoch: 66 | Batch_idx: 240 |  Loss_1: (0.0518) | Acc_1: (98.20%) (30293/30848)\n",
      "Epoch: 66 | Batch_idx: 250 |  Loss_1: (0.0510) | Acc_1: (98.23%) (31558/32128)\n",
      "Epoch: 66 | Batch_idx: 260 |  Loss_1: (0.0511) | Acc_1: (98.22%) (32814/33408)\n",
      "Epoch: 66 | Batch_idx: 270 |  Loss_1: (0.0513) | Acc_1: (98.23%) (34073/34688)\n",
      "Epoch: 66 | Batch_idx: 280 |  Loss_1: (0.0515) | Acc_1: (98.21%) (35324/35968)\n",
      "Epoch: 66 | Batch_idx: 290 |  Loss_1: (0.0520) | Acc_1: (98.20%) (36576/37248)\n",
      "Epoch: 66 | Batch_idx: 300 |  Loss_1: (0.0518) | Acc_1: (98.19%) (37829/38528)\n",
      "Epoch: 66 | Batch_idx: 310 |  Loss_1: (0.0518) | Acc_1: (98.19%) (39087/39808)\n",
      "Epoch: 66 | Batch_idx: 320 |  Loss_1: (0.0517) | Acc_1: (98.19%) (40345/41088)\n",
      "Epoch: 66 | Batch_idx: 330 |  Loss_1: (0.0519) | Acc_1: (98.18%) (41599/42368)\n",
      "Epoch: 66 | Batch_idx: 340 |  Loss_1: (0.0518) | Acc_1: (98.18%) (42855/43648)\n",
      "Epoch: 66 | Batch_idx: 350 |  Loss_1: (0.0514) | Acc_1: (98.19%) (44116/44928)\n",
      "Epoch: 66 | Batch_idx: 360 |  Loss_1: (0.0514) | Acc_1: (98.20%) (45375/46208)\n",
      "Epoch: 66 | Batch_idx: 370 |  Loss_1: (0.0511) | Acc_1: (98.20%) (46635/47488)\n",
      "Epoch: 66 | Batch_idx: 380 |  Loss_1: (0.0507) | Acc_1: (98.21%) (47897/48768)\n",
      "Epoch: 66 | Batch_idx: 390 |  Loss_1: (0.0505) | Acc_1: (98.22%) (49109/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4229) | Acc: (91.11%) (9111/10000)\n",
      "Epoch: 67 | Batch_idx: 0 |  Loss_1: (0.0141) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 67 | Batch_idx: 10 |  Loss_1: (0.0402) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 67 | Batch_idx: 20 |  Loss_1: (0.0478) | Acc_1: (98.25%) (2641/2688)\n",
      "Epoch: 67 | Batch_idx: 30 |  Loss_1: (0.0457) | Acc_1: (98.29%) (3900/3968)\n",
      "Epoch: 67 | Batch_idx: 40 |  Loss_1: (0.0420) | Acc_1: (98.42%) (5165/5248)\n",
      "Epoch: 67 | Batch_idx: 50 |  Loss_1: (0.0403) | Acc_1: (98.54%) (6433/6528)\n",
      "Epoch: 67 | Batch_idx: 60 |  Loss_1: (0.0437) | Acc_1: (98.48%) (7689/7808)\n",
      "Epoch: 67 | Batch_idx: 70 |  Loss_1: (0.0412) | Acc_1: (98.59%) (8960/9088)\n",
      "Epoch: 67 | Batch_idx: 80 |  Loss_1: (0.0407) | Acc_1: (98.63%) (10226/10368)\n",
      "Epoch: 67 | Batch_idx: 90 |  Loss_1: (0.0424) | Acc_1: (98.59%) (11484/11648)\n",
      "Epoch: 67 | Batch_idx: 100 |  Loss_1: (0.0423) | Acc_1: (98.57%) (12743/12928)\n",
      "Epoch: 67 | Batch_idx: 110 |  Loss_1: (0.0429) | Acc_1: (98.55%) (14002/14208)\n",
      "Epoch: 67 | Batch_idx: 120 |  Loss_1: (0.0437) | Acc_1: (98.51%) (15258/15488)\n",
      "Epoch: 67 | Batch_idx: 130 |  Loss_1: (0.0433) | Acc_1: (98.54%) (16524/16768)\n",
      "Epoch: 67 | Batch_idx: 140 |  Loss_1: (0.0433) | Acc_1: (98.54%) (17784/18048)\n",
      "Epoch: 67 | Batch_idx: 150 |  Loss_1: (0.0431) | Acc_1: (98.53%) (19043/19328)\n",
      "Epoch: 67 | Batch_idx: 160 |  Loss_1: (0.0437) | Acc_1: (98.50%) (20299/20608)\n",
      "Epoch: 67 | Batch_idx: 170 |  Loss_1: (0.0436) | Acc_1: (98.48%) (21555/21888)\n",
      "Epoch: 67 | Batch_idx: 180 |  Loss_1: (0.0438) | Acc_1: (98.46%) (22812/23168)\n",
      "Epoch: 67 | Batch_idx: 190 |  Loss_1: (0.0446) | Acc_1: (98.44%) (24067/24448)\n",
      "Epoch: 67 | Batch_idx: 200 |  Loss_1: (0.0450) | Acc_1: (98.41%) (25318/25728)\n",
      "Epoch: 67 | Batch_idx: 210 |  Loss_1: (0.0463) | Acc_1: (98.36%) (26565/27008)\n",
      "Epoch: 67 | Batch_idx: 220 |  Loss_1: (0.0468) | Acc_1: (98.32%) (27813/28288)\n",
      "Epoch: 67 | Batch_idx: 230 |  Loss_1: (0.0469) | Acc_1: (98.33%) (29074/29568)\n",
      "Epoch: 67 | Batch_idx: 240 |  Loss_1: (0.0481) | Acc_1: (98.29%) (30322/30848)\n",
      "Epoch: 67 | Batch_idx: 250 |  Loss_1: (0.0489) | Acc_1: (98.27%) (31573/32128)\n",
      "Epoch: 67 | Batch_idx: 260 |  Loss_1: (0.0493) | Acc_1: (98.24%) (32821/33408)\n",
      "Epoch: 67 | Batch_idx: 270 |  Loss_1: (0.0495) | Acc_1: (98.25%) (34081/34688)\n",
      "Epoch: 67 | Batch_idx: 280 |  Loss_1: (0.0502) | Acc_1: (98.22%) (35327/35968)\n",
      "Epoch: 67 | Batch_idx: 290 |  Loss_1: (0.0514) | Acc_1: (98.19%) (36572/37248)\n",
      "Epoch: 67 | Batch_idx: 300 |  Loss_1: (0.0513) | Acc_1: (98.17%) (37824/38528)\n",
      "Epoch: 67 | Batch_idx: 310 |  Loss_1: (0.0512) | Acc_1: (98.17%) (39080/39808)\n",
      "Epoch: 67 | Batch_idx: 320 |  Loss_1: (0.0512) | Acc_1: (98.17%) (40335/41088)\n",
      "Epoch: 67 | Batch_idx: 330 |  Loss_1: (0.0517) | Acc_1: (98.14%) (41581/42368)\n",
      "Epoch: 67 | Batch_idx: 340 |  Loss_1: (0.0514) | Acc_1: (98.15%) (42842/43648)\n",
      "Epoch: 67 | Batch_idx: 350 |  Loss_1: (0.0513) | Acc_1: (98.15%) (44099/44928)\n",
      "Epoch: 67 | Batch_idx: 360 |  Loss_1: (0.0509) | Acc_1: (98.17%) (45363/46208)\n",
      "Epoch: 67 | Batch_idx: 370 |  Loss_1: (0.0509) | Acc_1: (98.18%) (46623/47488)\n",
      "Epoch: 67 | Batch_idx: 380 |  Loss_1: (0.0508) | Acc_1: (98.18%) (47880/48768)\n",
      "Epoch: 67 | Batch_idx: 390 |  Loss_1: (0.0505) | Acc_1: (98.20%) (49098/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4680) | Acc: (90.00%) (9000/10000)\n",
      "Epoch: 68 | Batch_idx: 0 |  Loss_1: (0.0500) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 68 | Batch_idx: 10 |  Loss_1: (0.0376) | Acc_1: (98.65%) (1389/1408)\n",
      "Epoch: 68 | Batch_idx: 20 |  Loss_1: (0.0465) | Acc_1: (98.36%) (2644/2688)\n",
      "Epoch: 68 | Batch_idx: 30 |  Loss_1: (0.0464) | Acc_1: (98.26%) (3899/3968)\n",
      "Epoch: 68 | Batch_idx: 40 |  Loss_1: (0.0458) | Acc_1: (98.29%) (5158/5248)\n",
      "Epoch: 68 | Batch_idx: 50 |  Loss_1: (0.0430) | Acc_1: (98.41%) (6424/6528)\n",
      "Epoch: 68 | Batch_idx: 60 |  Loss_1: (0.0417) | Acc_1: (98.46%) (7688/7808)\n",
      "Epoch: 68 | Batch_idx: 70 |  Loss_1: (0.0423) | Acc_1: (98.46%) (8948/9088)\n",
      "Epoch: 68 | Batch_idx: 80 |  Loss_1: (0.0418) | Acc_1: (98.46%) (10208/10368)\n",
      "Epoch: 68 | Batch_idx: 90 |  Loss_1: (0.0425) | Acc_1: (98.45%) (11468/11648)\n",
      "Epoch: 68 | Batch_idx: 100 |  Loss_1: (0.0429) | Acc_1: (98.42%) (12724/12928)\n",
      "Epoch: 68 | Batch_idx: 110 |  Loss_1: (0.0437) | Acc_1: (98.41%) (13982/14208)\n",
      "Epoch: 68 | Batch_idx: 120 |  Loss_1: (0.0431) | Acc_1: (98.44%) (15247/15488)\n",
      "Epoch: 68 | Batch_idx: 130 |  Loss_1: (0.0426) | Acc_1: (98.45%) (16508/16768)\n",
      "Epoch: 68 | Batch_idx: 140 |  Loss_1: (0.0421) | Acc_1: (98.47%) (17771/18048)\n",
      "Epoch: 68 | Batch_idx: 150 |  Loss_1: (0.0423) | Acc_1: (98.44%) (19026/19328)\n",
      "Epoch: 68 | Batch_idx: 160 |  Loss_1: (0.0421) | Acc_1: (98.46%) (20290/20608)\n",
      "Epoch: 68 | Batch_idx: 170 |  Loss_1: (0.0436) | Acc_1: (98.41%) (21539/21888)\n",
      "Epoch: 68 | Batch_idx: 180 |  Loss_1: (0.0439) | Acc_1: (98.41%) (22799/23168)\n",
      "Epoch: 68 | Batch_idx: 190 |  Loss_1: (0.0447) | Acc_1: (98.40%) (24056/24448)\n",
      "Epoch: 68 | Batch_idx: 200 |  Loss_1: (0.0442) | Acc_1: (98.42%) (25322/25728)\n",
      "Epoch: 68 | Batch_idx: 210 |  Loss_1: (0.0447) | Acc_1: (98.42%) (26581/27008)\n",
      "Epoch: 68 | Batch_idx: 220 |  Loss_1: (0.0459) | Acc_1: (98.37%) (27828/28288)\n",
      "Epoch: 68 | Batch_idx: 230 |  Loss_1: (0.0464) | Acc_1: (98.35%) (29081/29568)\n",
      "Epoch: 68 | Batch_idx: 240 |  Loss_1: (0.0467) | Acc_1: (98.35%) (30340/30848)\n",
      "Epoch: 68 | Batch_idx: 250 |  Loss_1: (0.0465) | Acc_1: (98.36%) (31600/32128)\n",
      "Epoch: 68 | Batch_idx: 260 |  Loss_1: (0.0465) | Acc_1: (98.36%) (32860/33408)\n",
      "Epoch: 68 | Batch_idx: 270 |  Loss_1: (0.0464) | Acc_1: (98.35%) (34115/34688)\n",
      "Epoch: 68 | Batch_idx: 280 |  Loss_1: (0.0462) | Acc_1: (98.36%) (35379/35968)\n",
      "Epoch: 68 | Batch_idx: 290 |  Loss_1: (0.0465) | Acc_1: (98.36%) (36636/37248)\n",
      "Epoch: 68 | Batch_idx: 300 |  Loss_1: (0.0462) | Acc_1: (98.36%) (37896/38528)\n",
      "Epoch: 68 | Batch_idx: 310 |  Loss_1: (0.0460) | Acc_1: (98.37%) (39159/39808)\n",
      "Epoch: 68 | Batch_idx: 320 |  Loss_1: (0.0463) | Acc_1: (98.38%) (40421/41088)\n",
      "Epoch: 68 | Batch_idx: 330 |  Loss_1: (0.0461) | Acc_1: (98.37%) (41678/42368)\n",
      "Epoch: 68 | Batch_idx: 340 |  Loss_1: (0.0465) | Acc_1: (98.35%) (42929/43648)\n",
      "Epoch: 68 | Batch_idx: 350 |  Loss_1: (0.0465) | Acc_1: (98.36%) (44189/44928)\n",
      "Epoch: 68 | Batch_idx: 360 |  Loss_1: (0.0468) | Acc_1: (98.35%) (45444/46208)\n",
      "Epoch: 68 | Batch_idx: 370 |  Loss_1: (0.0467) | Acc_1: (98.36%) (46709/47488)\n",
      "Epoch: 68 | Batch_idx: 380 |  Loss_1: (0.0466) | Acc_1: (98.36%) (47967/48768)\n",
      "Epoch: 68 | Batch_idx: 390 |  Loss_1: (0.0465) | Acc_1: (98.36%) (49182/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4181) | Acc: (91.00%) (9100/10000)\n",
      "Epoch: 69 | Batch_idx: 0 |  Loss_1: (0.0148) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 69 | Batch_idx: 10 |  Loss_1: (0.0307) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 69 | Batch_idx: 20 |  Loss_1: (0.0360) | Acc_1: (98.62%) (2651/2688)\n",
      "Epoch: 69 | Batch_idx: 30 |  Loss_1: (0.0411) | Acc_1: (98.44%) (3906/3968)\n",
      "Epoch: 69 | Batch_idx: 40 |  Loss_1: (0.0406) | Acc_1: (98.49%) (5169/5248)\n",
      "Epoch: 69 | Batch_idx: 50 |  Loss_1: (0.0395) | Acc_1: (98.54%) (6433/6528)\n",
      "Epoch: 69 | Batch_idx: 60 |  Loss_1: (0.0407) | Acc_1: (98.51%) (7692/7808)\n",
      "Epoch: 69 | Batch_idx: 70 |  Loss_1: (0.0400) | Acc_1: (98.53%) (8954/9088)\n",
      "Epoch: 69 | Batch_idx: 80 |  Loss_1: (0.0386) | Acc_1: (98.57%) (10220/10368)\n",
      "Epoch: 69 | Batch_idx: 90 |  Loss_1: (0.0392) | Acc_1: (98.53%) (11477/11648)\n",
      "Epoch: 69 | Batch_idx: 100 |  Loss_1: (0.0396) | Acc_1: (98.52%) (12737/12928)\n",
      "Epoch: 69 | Batch_idx: 110 |  Loss_1: (0.0410) | Acc_1: (98.47%) (13991/14208)\n",
      "Epoch: 69 | Batch_idx: 120 |  Loss_1: (0.0418) | Acc_1: (98.46%) (15250/15488)\n",
      "Epoch: 69 | Batch_idx: 130 |  Loss_1: (0.0427) | Acc_1: (98.44%) (16506/16768)\n",
      "Epoch: 69 | Batch_idx: 140 |  Loss_1: (0.0425) | Acc_1: (98.43%) (17765/18048)\n",
      "Epoch: 69 | Batch_idx: 150 |  Loss_1: (0.0432) | Acc_1: (98.43%) (19024/19328)\n",
      "Epoch: 69 | Batch_idx: 160 |  Loss_1: (0.0429) | Acc_1: (98.43%) (20285/20608)\n",
      "Epoch: 69 | Batch_idx: 170 |  Loss_1: (0.0443) | Acc_1: (98.41%) (21540/21888)\n",
      "Epoch: 69 | Batch_idx: 180 |  Loss_1: (0.0438) | Acc_1: (98.42%) (22803/23168)\n",
      "Epoch: 69 | Batch_idx: 190 |  Loss_1: (0.0438) | Acc_1: (98.41%) (24060/24448)\n",
      "Epoch: 69 | Batch_idx: 200 |  Loss_1: (0.0437) | Acc_1: (98.41%) (25319/25728)\n",
      "Epoch: 69 | Batch_idx: 210 |  Loss_1: (0.0440) | Acc_1: (98.40%) (26577/27008)\n",
      "Epoch: 69 | Batch_idx: 220 |  Loss_1: (0.0449) | Acc_1: (98.38%) (27829/28288)\n",
      "Epoch: 69 | Batch_idx: 230 |  Loss_1: (0.0448) | Acc_1: (98.38%) (29088/29568)\n",
      "Epoch: 69 | Batch_idx: 240 |  Loss_1: (0.0445) | Acc_1: (98.39%) (30352/30848)\n",
      "Epoch: 69 | Batch_idx: 250 |  Loss_1: (0.0446) | Acc_1: (98.40%) (31614/32128)\n",
      "Epoch: 69 | Batch_idx: 260 |  Loss_1: (0.0442) | Acc_1: (98.41%) (32878/33408)\n",
      "Epoch: 69 | Batch_idx: 270 |  Loss_1: (0.0446) | Acc_1: (98.41%) (34135/34688)\n",
      "Epoch: 69 | Batch_idx: 280 |  Loss_1: (0.0453) | Acc_1: (98.38%) (35385/35968)\n",
      "Epoch: 69 | Batch_idx: 290 |  Loss_1: (0.0453) | Acc_1: (98.37%) (36642/37248)\n",
      "Epoch: 69 | Batch_idx: 300 |  Loss_1: (0.0457) | Acc_1: (98.38%) (37905/38528)\n",
      "Epoch: 69 | Batch_idx: 310 |  Loss_1: (0.0458) | Acc_1: (98.39%) (39166/39808)\n",
      "Epoch: 69 | Batch_idx: 320 |  Loss_1: (0.0455) | Acc_1: (98.40%) (40429/41088)\n",
      "Epoch: 69 | Batch_idx: 330 |  Loss_1: (0.0455) | Acc_1: (98.40%) (41691/42368)\n",
      "Epoch: 69 | Batch_idx: 340 |  Loss_1: (0.0463) | Acc_1: (98.38%) (42941/43648)\n",
      "Epoch: 69 | Batch_idx: 350 |  Loss_1: (0.0463) | Acc_1: (98.37%) (44197/44928)\n",
      "Epoch: 69 | Batch_idx: 360 |  Loss_1: (0.0465) | Acc_1: (98.37%) (45456/46208)\n",
      "Epoch: 69 | Batch_idx: 370 |  Loss_1: (0.0463) | Acc_1: (98.37%) (46716/47488)\n",
      "Epoch: 69 | Batch_idx: 380 |  Loss_1: (0.0459) | Acc_1: (98.38%) (47980/48768)\n",
      "Epoch: 69 | Batch_idx: 390 |  Loss_1: (0.0460) | Acc_1: (98.40%) (49198/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4162) | Acc: (91.29%) (9129/10000)\n",
      "Epoch: 70 | Batch_idx: 0 |  Loss_1: (0.0157) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 70 | Batch_idx: 10 |  Loss_1: (0.0323) | Acc_1: (98.58%) (1388/1408)\n",
      "Epoch: 70 | Batch_idx: 20 |  Loss_1: (0.0339) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 70 | Batch_idx: 30 |  Loss_1: (0.0321) | Acc_1: (98.66%) (3915/3968)\n",
      "Epoch: 70 | Batch_idx: 40 |  Loss_1: (0.0337) | Acc_1: (98.74%) (5182/5248)\n",
      "Epoch: 70 | Batch_idx: 50 |  Loss_1: (0.0354) | Acc_1: (98.62%) (6438/6528)\n",
      "Epoch: 70 | Batch_idx: 60 |  Loss_1: (0.0349) | Acc_1: (98.64%) (7702/7808)\n",
      "Epoch: 70 | Batch_idx: 70 |  Loss_1: (0.0363) | Acc_1: (98.54%) (8955/9088)\n",
      "Epoch: 70 | Batch_idx: 80 |  Loss_1: (0.0375) | Acc_1: (98.51%) (10214/10368)\n",
      "Epoch: 70 | Batch_idx: 90 |  Loss_1: (0.0367) | Acc_1: (98.56%) (11480/11648)\n",
      "Epoch: 70 | Batch_idx: 100 |  Loss_1: (0.0369) | Acc_1: (98.56%) (12742/12928)\n",
      "Epoch: 70 | Batch_idx: 110 |  Loss_1: (0.0369) | Acc_1: (98.56%) (14003/14208)\n",
      "Epoch: 70 | Batch_idx: 120 |  Loss_1: (0.0377) | Acc_1: (98.55%) (15263/15488)\n",
      "Epoch: 70 | Batch_idx: 130 |  Loss_1: (0.0386) | Acc_1: (98.54%) (16523/16768)\n",
      "Epoch: 70 | Batch_idx: 140 |  Loss_1: (0.0388) | Acc_1: (98.53%) (17783/18048)\n",
      "Epoch: 70 | Batch_idx: 150 |  Loss_1: (0.0387) | Acc_1: (98.54%) (19046/19328)\n",
      "Epoch: 70 | Batch_idx: 160 |  Loss_1: (0.0386) | Acc_1: (98.54%) (20308/20608)\n",
      "Epoch: 70 | Batch_idx: 170 |  Loss_1: (0.0390) | Acc_1: (98.55%) (21570/21888)\n",
      "Epoch: 70 | Batch_idx: 180 |  Loss_1: (0.0391) | Acc_1: (98.56%) (22834/23168)\n",
      "Epoch: 70 | Batch_idx: 190 |  Loss_1: (0.0395) | Acc_1: (98.54%) (24092/24448)\n",
      "Epoch: 70 | Batch_idx: 200 |  Loss_1: (0.0397) | Acc_1: (98.53%) (25350/25728)\n",
      "Epoch: 70 | Batch_idx: 210 |  Loss_1: (0.0393) | Acc_1: (98.55%) (26617/27008)\n",
      "Epoch: 70 | Batch_idx: 220 |  Loss_1: (0.0392) | Acc_1: (98.56%) (27882/28288)\n",
      "Epoch: 70 | Batch_idx: 230 |  Loss_1: (0.0394) | Acc_1: (98.57%) (29144/29568)\n",
      "Epoch: 70 | Batch_idx: 240 |  Loss_1: (0.0404) | Acc_1: (98.54%) (30398/30848)\n",
      "Epoch: 70 | Batch_idx: 250 |  Loss_1: (0.0411) | Acc_1: (98.50%) (31646/32128)\n",
      "Epoch: 70 | Batch_idx: 260 |  Loss_1: (0.0411) | Acc_1: (98.50%) (32907/33408)\n",
      "Epoch: 70 | Batch_idx: 270 |  Loss_1: (0.0409) | Acc_1: (98.51%) (34170/34688)\n",
      "Epoch: 70 | Batch_idx: 280 |  Loss_1: (0.0412) | Acc_1: (98.51%) (35432/35968)\n",
      "Epoch: 70 | Batch_idx: 290 |  Loss_1: (0.0418) | Acc_1: (98.49%) (36685/37248)\n",
      "Epoch: 70 | Batch_idx: 300 |  Loss_1: (0.0420) | Acc_1: (98.48%) (37942/38528)\n",
      "Epoch: 70 | Batch_idx: 310 |  Loss_1: (0.0426) | Acc_1: (98.46%) (39194/39808)\n",
      "Epoch: 70 | Batch_idx: 320 |  Loss_1: (0.0432) | Acc_1: (98.44%) (40447/41088)\n",
      "Epoch: 70 | Batch_idx: 330 |  Loss_1: (0.0431) | Acc_1: (98.45%) (41710/42368)\n",
      "Epoch: 70 | Batch_idx: 340 |  Loss_1: (0.0438) | Acc_1: (98.44%) (42966/43648)\n",
      "Epoch: 70 | Batch_idx: 350 |  Loss_1: (0.0437) | Acc_1: (98.44%) (44227/44928)\n",
      "Epoch: 70 | Batch_idx: 360 |  Loss_1: (0.0441) | Acc_1: (98.42%) (45478/46208)\n",
      "Epoch: 70 | Batch_idx: 370 |  Loss_1: (0.0439) | Acc_1: (98.42%) (46739/47488)\n",
      "Epoch: 70 | Batch_idx: 380 |  Loss_1: (0.0441) | Acc_1: (98.41%) (47995/48768)\n",
      "Epoch: 70 | Batch_idx: 390 |  Loss_1: (0.0440) | Acc_1: (98.42%) (49210/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5422) | Acc: (89.57%) (8957/10000)\n",
      "Epoch: 71 | Batch_idx: 0 |  Loss_1: (0.0163) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 71 | Batch_idx: 10 |  Loss_1: (0.0514) | Acc_1: (98.01%) (1380/1408)\n",
      "Epoch: 71 | Batch_idx: 20 |  Loss_1: (0.0440) | Acc_1: (98.33%) (2643/2688)\n",
      "Epoch: 71 | Batch_idx: 30 |  Loss_1: (0.0420) | Acc_1: (98.36%) (3903/3968)\n",
      "Epoch: 71 | Batch_idx: 40 |  Loss_1: (0.0405) | Acc_1: (98.42%) (5165/5248)\n",
      "Epoch: 71 | Batch_idx: 50 |  Loss_1: (0.0389) | Acc_1: (98.48%) (6429/6528)\n",
      "Epoch: 71 | Batch_idx: 60 |  Loss_1: (0.0387) | Acc_1: (98.57%) (7696/7808)\n",
      "Epoch: 71 | Batch_idx: 70 |  Loss_1: (0.0424) | Acc_1: (98.50%) (8952/9088)\n",
      "Epoch: 71 | Batch_idx: 80 |  Loss_1: (0.0430) | Acc_1: (98.53%) (10216/10368)\n",
      "Epoch: 71 | Batch_idx: 90 |  Loss_1: (0.0454) | Acc_1: (98.51%) (11475/11648)\n",
      "Epoch: 71 | Batch_idx: 100 |  Loss_1: (0.0450) | Acc_1: (98.54%) (12739/12928)\n",
      "Epoch: 71 | Batch_idx: 110 |  Loss_1: (0.0441) | Acc_1: (98.54%) (14001/14208)\n",
      "Epoch: 71 | Batch_idx: 120 |  Loss_1: (0.0440) | Acc_1: (98.54%) (15262/15488)\n",
      "Epoch: 71 | Batch_idx: 130 |  Loss_1: (0.0435) | Acc_1: (98.57%) (16528/16768)\n",
      "Epoch: 71 | Batch_idx: 140 |  Loss_1: (0.0427) | Acc_1: (98.56%) (17789/18048)\n",
      "Epoch: 71 | Batch_idx: 150 |  Loss_1: (0.0433) | Acc_1: (98.54%) (19046/19328)\n",
      "Epoch: 71 | Batch_idx: 160 |  Loss_1: (0.0426) | Acc_1: (98.56%) (20312/20608)\n",
      "Epoch: 71 | Batch_idx: 170 |  Loss_1: (0.0416) | Acc_1: (98.60%) (21581/21888)\n",
      "Epoch: 71 | Batch_idx: 180 |  Loss_1: (0.0412) | Acc_1: (98.61%) (22846/23168)\n",
      "Epoch: 71 | Batch_idx: 190 |  Loss_1: (0.0407) | Acc_1: (98.62%) (24110/24448)\n",
      "Epoch: 71 | Batch_idx: 200 |  Loss_1: (0.0409) | Acc_1: (98.61%) (25370/25728)\n",
      "Epoch: 71 | Batch_idx: 210 |  Loss_1: (0.0411) | Acc_1: (98.58%) (26624/27008)\n",
      "Epoch: 71 | Batch_idx: 220 |  Loss_1: (0.0410) | Acc_1: (98.58%) (27887/28288)\n",
      "Epoch: 71 | Batch_idx: 230 |  Loss_1: (0.0412) | Acc_1: (98.57%) (29144/29568)\n",
      "Epoch: 71 | Batch_idx: 240 |  Loss_1: (0.0415) | Acc_1: (98.55%) (30401/30848)\n",
      "Epoch: 71 | Batch_idx: 250 |  Loss_1: (0.0413) | Acc_1: (98.56%) (31666/32128)\n",
      "Epoch: 71 | Batch_idx: 260 |  Loss_1: (0.0412) | Acc_1: (98.56%) (32928/33408)\n",
      "Epoch: 71 | Batch_idx: 270 |  Loss_1: (0.0411) | Acc_1: (98.57%) (34191/34688)\n",
      "Epoch: 71 | Batch_idx: 280 |  Loss_1: (0.0408) | Acc_1: (98.57%) (35455/35968)\n",
      "Epoch: 71 | Batch_idx: 290 |  Loss_1: (0.0406) | Acc_1: (98.59%) (36721/37248)\n",
      "Epoch: 71 | Batch_idx: 300 |  Loss_1: (0.0405) | Acc_1: (98.58%) (37982/38528)\n",
      "Epoch: 71 | Batch_idx: 310 |  Loss_1: (0.0408) | Acc_1: (98.58%) (39242/39808)\n",
      "Epoch: 71 | Batch_idx: 320 |  Loss_1: (0.0412) | Acc_1: (98.57%) (40500/41088)\n",
      "Epoch: 71 | Batch_idx: 330 |  Loss_1: (0.0411) | Acc_1: (98.57%) (41761/42368)\n",
      "Epoch: 71 | Batch_idx: 340 |  Loss_1: (0.0411) | Acc_1: (98.57%) (43023/43648)\n",
      "Epoch: 71 | Batch_idx: 350 |  Loss_1: (0.0409) | Acc_1: (98.57%) (44287/44928)\n",
      "Epoch: 71 | Batch_idx: 360 |  Loss_1: (0.0407) | Acc_1: (98.58%) (45551/46208)\n",
      "Epoch: 71 | Batch_idx: 370 |  Loss_1: (0.0409) | Acc_1: (98.57%) (46811/47488)\n",
      "Epoch: 71 | Batch_idx: 380 |  Loss_1: (0.0411) | Acc_1: (98.57%) (48072/48768)\n",
      "Epoch: 71 | Batch_idx: 390 |  Loss_1: (0.0413) | Acc_1: (98.57%) (49284/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4713) | Acc: (90.63%) (9063/10000)\n",
      "Epoch: 72 | Batch_idx: 0 |  Loss_1: (0.0657) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 72 | Batch_idx: 10 |  Loss_1: (0.0501) | Acc_1: (98.22%) (1383/1408)\n",
      "Epoch: 72 | Batch_idx: 20 |  Loss_1: (0.0411) | Acc_1: (98.36%) (2644/2688)\n",
      "Epoch: 72 | Batch_idx: 30 |  Loss_1: (0.0389) | Acc_1: (98.54%) (3910/3968)\n",
      "Epoch: 72 | Batch_idx: 40 |  Loss_1: (0.0413) | Acc_1: (98.40%) (5164/5248)\n",
      "Epoch: 72 | Batch_idx: 50 |  Loss_1: (0.0414) | Acc_1: (98.44%) (6426/6528)\n",
      "Epoch: 72 | Batch_idx: 60 |  Loss_1: (0.0401) | Acc_1: (98.49%) (7690/7808)\n",
      "Epoch: 72 | Batch_idx: 70 |  Loss_1: (0.0412) | Acc_1: (98.46%) (8948/9088)\n",
      "Epoch: 72 | Batch_idx: 80 |  Loss_1: (0.0395) | Acc_1: (98.52%) (10215/10368)\n",
      "Epoch: 72 | Batch_idx: 90 |  Loss_1: (0.0409) | Acc_1: (98.49%) (11472/11648)\n",
      "Epoch: 72 | Batch_idx: 100 |  Loss_1: (0.0396) | Acc_1: (98.55%) (12741/12928)\n",
      "Epoch: 72 | Batch_idx: 110 |  Loss_1: (0.0402) | Acc_1: (98.54%) (14000/14208)\n",
      "Epoch: 72 | Batch_idx: 120 |  Loss_1: (0.0394) | Acc_1: (98.57%) (15267/15488)\n",
      "Epoch: 72 | Batch_idx: 130 |  Loss_1: (0.0395) | Acc_1: (98.58%) (16530/16768)\n",
      "Epoch: 72 | Batch_idx: 140 |  Loss_1: (0.0393) | Acc_1: (98.59%) (17793/18048)\n",
      "Epoch: 72 | Batch_idx: 150 |  Loss_1: (0.0391) | Acc_1: (98.61%) (19060/19328)\n",
      "Epoch: 72 | Batch_idx: 160 |  Loss_1: (0.0392) | Acc_1: (98.63%) (20325/20608)\n",
      "Epoch: 72 | Batch_idx: 170 |  Loss_1: (0.0393) | Acc_1: (98.62%) (21587/21888)\n",
      "Epoch: 72 | Batch_idx: 180 |  Loss_1: (0.0399) | Acc_1: (98.61%) (22847/23168)\n",
      "Epoch: 72 | Batch_idx: 190 |  Loss_1: (0.0404) | Acc_1: (98.59%) (24103/24448)\n",
      "Epoch: 72 | Batch_idx: 200 |  Loss_1: (0.0403) | Acc_1: (98.59%) (25365/25728)\n",
      "Epoch: 72 | Batch_idx: 210 |  Loss_1: (0.0411) | Acc_1: (98.55%) (26616/27008)\n",
      "Epoch: 72 | Batch_idx: 220 |  Loss_1: (0.0408) | Acc_1: (98.54%) (27875/28288)\n",
      "Epoch: 72 | Batch_idx: 230 |  Loss_1: (0.0410) | Acc_1: (98.53%) (29133/29568)\n",
      "Epoch: 72 | Batch_idx: 240 |  Loss_1: (0.0409) | Acc_1: (98.52%) (30392/30848)\n",
      "Epoch: 72 | Batch_idx: 250 |  Loss_1: (0.0406) | Acc_1: (98.53%) (31657/32128)\n",
      "Epoch: 72 | Batch_idx: 260 |  Loss_1: (0.0406) | Acc_1: (98.54%) (32921/33408)\n",
      "Epoch: 72 | Batch_idx: 270 |  Loss_1: (0.0400) | Acc_1: (98.57%) (34191/34688)\n",
      "Epoch: 72 | Batch_idx: 280 |  Loss_1: (0.0404) | Acc_1: (98.53%) (35439/35968)\n",
      "Epoch: 72 | Batch_idx: 290 |  Loss_1: (0.0404) | Acc_1: (98.53%) (36699/37248)\n",
      "Epoch: 72 | Batch_idx: 300 |  Loss_1: (0.0408) | Acc_1: (98.51%) (37953/38528)\n",
      "Epoch: 72 | Batch_idx: 310 |  Loss_1: (0.0412) | Acc_1: (98.51%) (39214/39808)\n",
      "Epoch: 72 | Batch_idx: 320 |  Loss_1: (0.0416) | Acc_1: (98.49%) (40467/41088)\n",
      "Epoch: 72 | Batch_idx: 330 |  Loss_1: (0.0414) | Acc_1: (98.49%) (41728/42368)\n",
      "Epoch: 72 | Batch_idx: 340 |  Loss_1: (0.0418) | Acc_1: (98.49%) (42987/43648)\n",
      "Epoch: 72 | Batch_idx: 350 |  Loss_1: (0.0420) | Acc_1: (98.47%) (44242/44928)\n",
      "Epoch: 72 | Batch_idx: 360 |  Loss_1: (0.0423) | Acc_1: (98.46%) (45497/46208)\n",
      "Epoch: 72 | Batch_idx: 370 |  Loss_1: (0.0428) | Acc_1: (98.44%) (46749/47488)\n",
      "Epoch: 72 | Batch_idx: 380 |  Loss_1: (0.0429) | Acc_1: (98.44%) (48006/48768)\n",
      "Epoch: 72 | Batch_idx: 390 |  Loss_1: (0.0429) | Acc_1: (98.44%) (49220/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4108) | Acc: (91.27%) (9127/10000)\n",
      "Epoch: 73 | Batch_idx: 0 |  Loss_1: (0.0361) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 73 | Batch_idx: 10 |  Loss_1: (0.0216) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 73 | Batch_idx: 20 |  Loss_1: (0.0249) | Acc_1: (99.26%) (2668/2688)\n",
      "Epoch: 73 | Batch_idx: 30 |  Loss_1: (0.0285) | Acc_1: (99.09%) (3932/3968)\n",
      "Epoch: 73 | Batch_idx: 40 |  Loss_1: (0.0289) | Acc_1: (99.14%) (5203/5248)\n",
      "Epoch: 73 | Batch_idx: 50 |  Loss_1: (0.0302) | Acc_1: (99.10%) (6469/6528)\n",
      "Epoch: 73 | Batch_idx: 60 |  Loss_1: (0.0289) | Acc_1: (99.12%) (7739/7808)\n",
      "Epoch: 73 | Batch_idx: 70 |  Loss_1: (0.0289) | Acc_1: (99.09%) (9005/9088)\n",
      "Epoch: 73 | Batch_idx: 80 |  Loss_1: (0.0284) | Acc_1: (99.09%) (10274/10368)\n",
      "Epoch: 73 | Batch_idx: 90 |  Loss_1: (0.0283) | Acc_1: (99.08%) (11541/11648)\n",
      "Epoch: 73 | Batch_idx: 100 |  Loss_1: (0.0296) | Acc_1: (99.03%) (12803/12928)\n",
      "Epoch: 73 | Batch_idx: 110 |  Loss_1: (0.0296) | Acc_1: (99.01%) (14068/14208)\n",
      "Epoch: 73 | Batch_idx: 120 |  Loss_1: (0.0294) | Acc_1: (99.01%) (15335/15488)\n",
      "Epoch: 73 | Batch_idx: 130 |  Loss_1: (0.0313) | Acc_1: (98.96%) (16593/16768)\n",
      "Epoch: 73 | Batch_idx: 140 |  Loss_1: (0.0315) | Acc_1: (98.96%) (17861/18048)\n",
      "Epoch: 73 | Batch_idx: 150 |  Loss_1: (0.0325) | Acc_1: (98.94%) (19123/19328)\n",
      "Epoch: 73 | Batch_idx: 160 |  Loss_1: (0.0331) | Acc_1: (98.91%) (20384/20608)\n",
      "Epoch: 73 | Batch_idx: 170 |  Loss_1: (0.0336) | Acc_1: (98.87%) (21640/21888)\n",
      "Epoch: 73 | Batch_idx: 180 |  Loss_1: (0.0331) | Acc_1: (98.87%) (22907/23168)\n",
      "Epoch: 73 | Batch_idx: 190 |  Loss_1: (0.0337) | Acc_1: (98.85%) (24166/24448)\n",
      "Epoch: 73 | Batch_idx: 200 |  Loss_1: (0.0336) | Acc_1: (98.85%) (25431/25728)\n",
      "Epoch: 73 | Batch_idx: 210 |  Loss_1: (0.0345) | Acc_1: (98.82%) (26689/27008)\n",
      "Epoch: 73 | Batch_idx: 220 |  Loss_1: (0.0345) | Acc_1: (98.81%) (27951/28288)\n",
      "Epoch: 73 | Batch_idx: 230 |  Loss_1: (0.0353) | Acc_1: (98.79%) (29210/29568)\n",
      "Epoch: 73 | Batch_idx: 240 |  Loss_1: (0.0352) | Acc_1: (98.79%) (30475/30848)\n",
      "Epoch: 73 | Batch_idx: 250 |  Loss_1: (0.0360) | Acc_1: (98.76%) (31730/32128)\n",
      "Epoch: 73 | Batch_idx: 260 |  Loss_1: (0.0367) | Acc_1: (98.74%) (32988/33408)\n",
      "Epoch: 73 | Batch_idx: 270 |  Loss_1: (0.0371) | Acc_1: (98.72%) (34245/34688)\n",
      "Epoch: 73 | Batch_idx: 280 |  Loss_1: (0.0376) | Acc_1: (98.70%) (35499/35968)\n",
      "Epoch: 73 | Batch_idx: 290 |  Loss_1: (0.0385) | Acc_1: (98.68%) (36755/37248)\n",
      "Epoch: 73 | Batch_idx: 300 |  Loss_1: (0.0386) | Acc_1: (98.67%) (38014/38528)\n",
      "Epoch: 73 | Batch_idx: 310 |  Loss_1: (0.0393) | Acc_1: (98.64%) (39268/39808)\n",
      "Epoch: 73 | Batch_idx: 320 |  Loss_1: (0.0396) | Acc_1: (98.63%) (40525/41088)\n",
      "Epoch: 73 | Batch_idx: 330 |  Loss_1: (0.0393) | Acc_1: (98.65%) (41794/42368)\n",
      "Epoch: 73 | Batch_idx: 340 |  Loss_1: (0.0393) | Acc_1: (98.65%) (43058/43648)\n",
      "Epoch: 73 | Batch_idx: 350 |  Loss_1: (0.0395) | Acc_1: (98.63%) (44314/44928)\n",
      "Epoch: 73 | Batch_idx: 360 |  Loss_1: (0.0394) | Acc_1: (98.63%) (45576/46208)\n",
      "Epoch: 73 | Batch_idx: 370 |  Loss_1: (0.0395) | Acc_1: (98.63%) (46836/47488)\n",
      "Epoch: 73 | Batch_idx: 380 |  Loss_1: (0.0394) | Acc_1: (98.63%) (48100/48768)\n",
      "Epoch: 73 | Batch_idx: 390 |  Loss_1: (0.0396) | Acc_1: (98.63%) (49315/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4002) | Acc: (91.41%) (9141/10000)\n",
      "Epoch: 74 | Batch_idx: 0 |  Loss_1: (0.0194) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 74 | Batch_idx: 10 |  Loss_1: (0.0204) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 74 | Batch_idx: 20 |  Loss_1: (0.0279) | Acc_1: (99.11%) (2664/2688)\n",
      "Epoch: 74 | Batch_idx: 30 |  Loss_1: (0.0331) | Acc_1: (98.89%) (3924/3968)\n",
      "Epoch: 74 | Batch_idx: 40 |  Loss_1: (0.0362) | Acc_1: (98.80%) (5185/5248)\n",
      "Epoch: 74 | Batch_idx: 50 |  Loss_1: (0.0360) | Acc_1: (98.84%) (6452/6528)\n",
      "Epoch: 74 | Batch_idx: 60 |  Loss_1: (0.0359) | Acc_1: (98.77%) (7712/7808)\n",
      "Epoch: 74 | Batch_idx: 70 |  Loss_1: (0.0344) | Acc_1: (98.84%) (8983/9088)\n",
      "Epoch: 74 | Batch_idx: 80 |  Loss_1: (0.0337) | Acc_1: (98.88%) (10252/10368)\n",
      "Epoch: 74 | Batch_idx: 90 |  Loss_1: (0.0338) | Acc_1: (98.91%) (11521/11648)\n",
      "Epoch: 74 | Batch_idx: 100 |  Loss_1: (0.0331) | Acc_1: (98.93%) (12790/12928)\n",
      "Epoch: 74 | Batch_idx: 110 |  Loss_1: (0.0343) | Acc_1: (98.90%) (14052/14208)\n",
      "Epoch: 74 | Batch_idx: 120 |  Loss_1: (0.0339) | Acc_1: (98.91%) (15319/15488)\n",
      "Epoch: 74 | Batch_idx: 130 |  Loss_1: (0.0340) | Acc_1: (98.89%) (16582/16768)\n",
      "Epoch: 74 | Batch_idx: 140 |  Loss_1: (0.0335) | Acc_1: (98.91%) (17851/18048)\n",
      "Epoch: 74 | Batch_idx: 150 |  Loss_1: (0.0328) | Acc_1: (98.92%) (19120/19328)\n",
      "Epoch: 74 | Batch_idx: 160 |  Loss_1: (0.0331) | Acc_1: (98.90%) (20382/20608)\n",
      "Epoch: 74 | Batch_idx: 170 |  Loss_1: (0.0335) | Acc_1: (98.89%) (21645/21888)\n",
      "Epoch: 74 | Batch_idx: 180 |  Loss_1: (0.0334) | Acc_1: (98.87%) (22907/23168)\n",
      "Epoch: 74 | Batch_idx: 190 |  Loss_1: (0.0344) | Acc_1: (98.86%) (24169/24448)\n",
      "Epoch: 74 | Batch_idx: 200 |  Loss_1: (0.0350) | Acc_1: (98.85%) (25433/25728)\n",
      "Epoch: 74 | Batch_idx: 210 |  Loss_1: (0.0347) | Acc_1: (98.86%) (26701/27008)\n",
      "Epoch: 74 | Batch_idx: 220 |  Loss_1: (0.0346) | Acc_1: (98.87%) (27969/28288)\n",
      "Epoch: 74 | Batch_idx: 230 |  Loss_1: (0.0345) | Acc_1: (98.87%) (29234/29568)\n",
      "Epoch: 74 | Batch_idx: 240 |  Loss_1: (0.0344) | Acc_1: (98.88%) (30501/30848)\n",
      "Epoch: 74 | Batch_idx: 250 |  Loss_1: (0.0344) | Acc_1: (98.87%) (31766/32128)\n",
      "Epoch: 74 | Batch_idx: 260 |  Loss_1: (0.0345) | Acc_1: (98.86%) (33028/33408)\n",
      "Epoch: 74 | Batch_idx: 270 |  Loss_1: (0.0343) | Acc_1: (98.87%) (34295/34688)\n",
      "Epoch: 74 | Batch_idx: 280 |  Loss_1: (0.0343) | Acc_1: (98.86%) (35558/35968)\n",
      "Epoch: 74 | Batch_idx: 290 |  Loss_1: (0.0349) | Acc_1: (98.83%) (36814/37248)\n",
      "Epoch: 74 | Batch_idx: 300 |  Loss_1: (0.0355) | Acc_1: (98.81%) (38069/38528)\n",
      "Epoch: 74 | Batch_idx: 310 |  Loss_1: (0.0354) | Acc_1: (98.80%) (39330/39808)\n",
      "Epoch: 74 | Batch_idx: 320 |  Loss_1: (0.0354) | Acc_1: (98.80%) (40594/41088)\n",
      "Epoch: 74 | Batch_idx: 330 |  Loss_1: (0.0351) | Acc_1: (98.81%) (41862/42368)\n",
      "Epoch: 74 | Batch_idx: 340 |  Loss_1: (0.0350) | Acc_1: (98.82%) (43131/43648)\n",
      "Epoch: 74 | Batch_idx: 350 |  Loss_1: (0.0350) | Acc_1: (98.81%) (44395/44928)\n",
      "Epoch: 74 | Batch_idx: 360 |  Loss_1: (0.0352) | Acc_1: (98.80%) (45652/46208)\n",
      "Epoch: 74 | Batch_idx: 370 |  Loss_1: (0.0351) | Acc_1: (98.80%) (46918/47488)\n",
      "Epoch: 74 | Batch_idx: 380 |  Loss_1: (0.0353) | Acc_1: (98.80%) (48181/48768)\n",
      "Epoch: 74 | Batch_idx: 390 |  Loss_1: (0.0357) | Acc_1: (98.78%) (49392/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4440) | Acc: (90.79%) (9079/10000)\n",
      "Epoch: 75 | Batch_idx: 0 |  Loss_1: (0.0639) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 75 | Batch_idx: 10 |  Loss_1: (0.0434) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 75 | Batch_idx: 20 |  Loss_1: (0.0402) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 75 | Batch_idx: 30 |  Loss_1: (0.0357) | Acc_1: (98.99%) (3928/3968)\n",
      "Epoch: 75 | Batch_idx: 40 |  Loss_1: (0.0329) | Acc_1: (99.01%) (5196/5248)\n",
      "Epoch: 75 | Batch_idx: 50 |  Loss_1: (0.0302) | Acc_1: (99.13%) (6471/6528)\n",
      "Epoch: 75 | Batch_idx: 60 |  Loss_1: (0.0287) | Acc_1: (99.15%) (7742/7808)\n",
      "Epoch: 75 | Batch_idx: 70 |  Loss_1: (0.0279) | Acc_1: (99.20%) (9015/9088)\n",
      "Epoch: 75 | Batch_idx: 80 |  Loss_1: (0.0276) | Acc_1: (99.18%) (10283/10368)\n",
      "Epoch: 75 | Batch_idx: 90 |  Loss_1: (0.0282) | Acc_1: (99.15%) (11549/11648)\n",
      "Epoch: 75 | Batch_idx: 100 |  Loss_1: (0.0282) | Acc_1: (99.15%) (12818/12928)\n",
      "Epoch: 75 | Batch_idx: 110 |  Loss_1: (0.0273) | Acc_1: (99.16%) (14089/14208)\n",
      "Epoch: 75 | Batch_idx: 120 |  Loss_1: (0.0272) | Acc_1: (99.17%) (15359/15488)\n",
      "Epoch: 75 | Batch_idx: 130 |  Loss_1: (0.0270) | Acc_1: (99.17%) (16628/16768)\n",
      "Epoch: 75 | Batch_idx: 140 |  Loss_1: (0.0263) | Acc_1: (99.19%) (17901/18048)\n",
      "Epoch: 75 | Batch_idx: 150 |  Loss_1: (0.0268) | Acc_1: (99.16%) (19166/19328)\n",
      "Epoch: 75 | Batch_idx: 160 |  Loss_1: (0.0269) | Acc_1: (99.16%) (20435/20608)\n",
      "Epoch: 75 | Batch_idx: 170 |  Loss_1: (0.0276) | Acc_1: (99.11%) (21693/21888)\n",
      "Epoch: 75 | Batch_idx: 180 |  Loss_1: (0.0277) | Acc_1: (99.12%) (22963/23168)\n",
      "Epoch: 75 | Batch_idx: 190 |  Loss_1: (0.0280) | Acc_1: (99.10%) (24228/24448)\n",
      "Epoch: 75 | Batch_idx: 200 |  Loss_1: (0.0290) | Acc_1: (99.06%) (25487/25728)\n",
      "Epoch: 75 | Batch_idx: 210 |  Loss_1: (0.0296) | Acc_1: (99.05%) (26751/27008)\n",
      "Epoch: 75 | Batch_idx: 220 |  Loss_1: (0.0300) | Acc_1: (99.04%) (28017/28288)\n",
      "Epoch: 75 | Batch_idx: 230 |  Loss_1: (0.0300) | Acc_1: (99.04%) (29284/29568)\n",
      "Epoch: 75 | Batch_idx: 240 |  Loss_1: (0.0305) | Acc_1: (99.02%) (30547/30848)\n",
      "Epoch: 75 | Batch_idx: 250 |  Loss_1: (0.0309) | Acc_1: (99.01%) (31810/32128)\n",
      "Epoch: 75 | Batch_idx: 260 |  Loss_1: (0.0316) | Acc_1: (98.99%) (33070/33408)\n",
      "Epoch: 75 | Batch_idx: 270 |  Loss_1: (0.0318) | Acc_1: (98.99%) (34337/34688)\n",
      "Epoch: 75 | Batch_idx: 280 |  Loss_1: (0.0322) | Acc_1: (98.97%) (35596/35968)\n",
      "Epoch: 75 | Batch_idx: 290 |  Loss_1: (0.0324) | Acc_1: (98.94%) (36855/37248)\n",
      "Epoch: 75 | Batch_idx: 300 |  Loss_1: (0.0328) | Acc_1: (98.93%) (38117/38528)\n",
      "Epoch: 75 | Batch_idx: 310 |  Loss_1: (0.0328) | Acc_1: (98.92%) (39378/39808)\n",
      "Epoch: 75 | Batch_idx: 320 |  Loss_1: (0.0327) | Acc_1: (98.92%) (40644/41088)\n",
      "Epoch: 75 | Batch_idx: 330 |  Loss_1: (0.0325) | Acc_1: (98.93%) (41913/42368)\n",
      "Epoch: 75 | Batch_idx: 340 |  Loss_1: (0.0326) | Acc_1: (98.93%) (43181/43648)\n",
      "Epoch: 75 | Batch_idx: 350 |  Loss_1: (0.0326) | Acc_1: (98.93%) (44447/44928)\n",
      "Epoch: 75 | Batch_idx: 360 |  Loss_1: (0.0324) | Acc_1: (98.93%) (45715/46208)\n",
      "Epoch: 75 | Batch_idx: 370 |  Loss_1: (0.0325) | Acc_1: (98.94%) (46983/47488)\n",
      "Epoch: 75 | Batch_idx: 380 |  Loss_1: (0.0327) | Acc_1: (98.93%) (48246/48768)\n",
      "Epoch: 75 | Batch_idx: 390 |  Loss_1: (0.0326) | Acc_1: (98.93%) (49464/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4818) | Acc: (90.43%) (9043/10000)\n",
      "Epoch: 76 | Batch_idx: 0 |  Loss_1: (0.0140) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 76 | Batch_idx: 10 |  Loss_1: (0.0429) | Acc_1: (98.44%) (1386/1408)\n",
      "Epoch: 76 | Batch_idx: 20 |  Loss_1: (0.0423) | Acc_1: (98.59%) (2650/2688)\n",
      "Epoch: 76 | Batch_idx: 30 |  Loss_1: (0.0413) | Acc_1: (98.69%) (3916/3968)\n",
      "Epoch: 76 | Batch_idx: 40 |  Loss_1: (0.0399) | Acc_1: (98.74%) (5182/5248)\n",
      "Epoch: 76 | Batch_idx: 50 |  Loss_1: (0.0370) | Acc_1: (98.87%) (6454/6528)\n",
      "Epoch: 76 | Batch_idx: 60 |  Loss_1: (0.0362) | Acc_1: (98.90%) (7722/7808)\n",
      "Epoch: 76 | Batch_idx: 70 |  Loss_1: (0.0374) | Acc_1: (98.84%) (8983/9088)\n",
      "Epoch: 76 | Batch_idx: 80 |  Loss_1: (0.0358) | Acc_1: (98.89%) (10253/10368)\n",
      "Epoch: 76 | Batch_idx: 90 |  Loss_1: (0.0350) | Acc_1: (98.90%) (11520/11648)\n",
      "Epoch: 76 | Batch_idx: 100 |  Loss_1: (0.0336) | Acc_1: (98.92%) (12789/12928)\n",
      "Epoch: 76 | Batch_idx: 110 |  Loss_1: (0.0329) | Acc_1: (98.94%) (14058/14208)\n",
      "Epoch: 76 | Batch_idx: 120 |  Loss_1: (0.0346) | Acc_1: (98.90%) (15317/15488)\n",
      "Epoch: 76 | Batch_idx: 130 |  Loss_1: (0.0352) | Acc_1: (98.87%) (16579/16768)\n",
      "Epoch: 76 | Batch_idx: 140 |  Loss_1: (0.0358) | Acc_1: (98.86%) (17842/18048)\n",
      "Epoch: 76 | Batch_idx: 150 |  Loss_1: (0.0364) | Acc_1: (98.85%) (19105/19328)\n",
      "Epoch: 76 | Batch_idx: 160 |  Loss_1: (0.0366) | Acc_1: (98.82%) (20364/20608)\n",
      "Epoch: 76 | Batch_idx: 170 |  Loss_1: (0.0364) | Acc_1: (98.83%) (21631/21888)\n",
      "Epoch: 76 | Batch_idx: 180 |  Loss_1: (0.0361) | Acc_1: (98.85%) (22901/23168)\n",
      "Epoch: 76 | Batch_idx: 190 |  Loss_1: (0.0362) | Acc_1: (98.84%) (24164/24448)\n",
      "Epoch: 76 | Batch_idx: 200 |  Loss_1: (0.0357) | Acc_1: (98.86%) (25434/25728)\n",
      "Epoch: 76 | Batch_idx: 210 |  Loss_1: (0.0355) | Acc_1: (98.87%) (26703/27008)\n",
      "Epoch: 76 | Batch_idx: 220 |  Loss_1: (0.0358) | Acc_1: (98.86%) (27966/28288)\n",
      "Epoch: 76 | Batch_idx: 230 |  Loss_1: (0.0358) | Acc_1: (98.84%) (29226/29568)\n",
      "Epoch: 76 | Batch_idx: 240 |  Loss_1: (0.0353) | Acc_1: (98.86%) (30495/30848)\n",
      "Epoch: 76 | Batch_idx: 250 |  Loss_1: (0.0352) | Acc_1: (98.85%) (31759/32128)\n",
      "Epoch: 76 | Batch_idx: 260 |  Loss_1: (0.0350) | Acc_1: (98.86%) (33027/33408)\n",
      "Epoch: 76 | Batch_idx: 270 |  Loss_1: (0.0352) | Acc_1: (98.84%) (34286/34688)\n",
      "Epoch: 76 | Batch_idx: 280 |  Loss_1: (0.0352) | Acc_1: (98.84%) (35552/35968)\n",
      "Epoch: 76 | Batch_idx: 290 |  Loss_1: (0.0351) | Acc_1: (98.84%) (36817/37248)\n",
      "Epoch: 76 | Batch_idx: 300 |  Loss_1: (0.0354) | Acc_1: (98.84%) (38080/38528)\n",
      "Epoch: 76 | Batch_idx: 310 |  Loss_1: (0.0356) | Acc_1: (98.82%) (39339/39808)\n",
      "Epoch: 76 | Batch_idx: 320 |  Loss_1: (0.0354) | Acc_1: (98.82%) (40602/41088)\n",
      "Epoch: 76 | Batch_idx: 330 |  Loss_1: (0.0355) | Acc_1: (98.81%) (41865/42368)\n",
      "Epoch: 76 | Batch_idx: 340 |  Loss_1: (0.0357) | Acc_1: (98.80%) (43126/43648)\n",
      "Epoch: 76 | Batch_idx: 350 |  Loss_1: (0.0355) | Acc_1: (98.82%) (44396/44928)\n",
      "Epoch: 76 | Batch_idx: 360 |  Loss_1: (0.0354) | Acc_1: (98.82%) (45662/46208)\n",
      "Epoch: 76 | Batch_idx: 370 |  Loss_1: (0.0353) | Acc_1: (98.82%) (46929/47488)\n",
      "Epoch: 76 | Batch_idx: 380 |  Loss_1: (0.0352) | Acc_1: (98.83%) (48197/48768)\n",
      "Epoch: 76 | Batch_idx: 390 |  Loss_1: (0.0352) | Acc_1: (98.83%) (49413/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4626) | Acc: (90.57%) (9057/10000)\n",
      "Epoch: 77 | Batch_idx: 0 |  Loss_1: (0.0387) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 77 | Batch_idx: 10 |  Loss_1: (0.0375) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 77 | Batch_idx: 20 |  Loss_1: (0.0359) | Acc_1: (98.85%) (2657/2688)\n",
      "Epoch: 77 | Batch_idx: 30 |  Loss_1: (0.0365) | Acc_1: (98.82%) (3921/3968)\n",
      "Epoch: 77 | Batch_idx: 40 |  Loss_1: (0.0355) | Acc_1: (98.74%) (5182/5248)\n",
      "Epoch: 77 | Batch_idx: 50 |  Loss_1: (0.0369) | Acc_1: (98.74%) (6446/6528)\n",
      "Epoch: 77 | Batch_idx: 60 |  Loss_1: (0.0352) | Acc_1: (98.76%) (7711/7808)\n",
      "Epoch: 77 | Batch_idx: 70 |  Loss_1: (0.0359) | Acc_1: (98.71%) (8971/9088)\n",
      "Epoch: 77 | Batch_idx: 80 |  Loss_1: (0.0358) | Acc_1: (98.71%) (10234/10368)\n",
      "Epoch: 77 | Batch_idx: 90 |  Loss_1: (0.0353) | Acc_1: (98.76%) (11503/11648)\n",
      "Epoch: 77 | Batch_idx: 100 |  Loss_1: (0.0355) | Acc_1: (98.77%) (12769/12928)\n",
      "Epoch: 77 | Batch_idx: 110 |  Loss_1: (0.0348) | Acc_1: (98.80%) (14037/14208)\n",
      "Epoch: 77 | Batch_idx: 120 |  Loss_1: (0.0346) | Acc_1: (98.82%) (15305/15488)\n",
      "Epoch: 77 | Batch_idx: 130 |  Loss_1: (0.0348) | Acc_1: (98.82%) (16570/16768)\n",
      "Epoch: 77 | Batch_idx: 140 |  Loss_1: (0.0342) | Acc_1: (98.83%) (17836/18048)\n",
      "Epoch: 77 | Batch_idx: 150 |  Loss_1: (0.0344) | Acc_1: (98.80%) (19097/19328)\n",
      "Epoch: 77 | Batch_idx: 160 |  Loss_1: (0.0339) | Acc_1: (98.81%) (20363/20608)\n",
      "Epoch: 77 | Batch_idx: 170 |  Loss_1: (0.0332) | Acc_1: (98.83%) (21633/21888)\n",
      "Epoch: 77 | Batch_idx: 180 |  Loss_1: (0.0335) | Acc_1: (98.83%) (22897/23168)\n",
      "Epoch: 77 | Batch_idx: 190 |  Loss_1: (0.0330) | Acc_1: (98.85%) (24168/24448)\n",
      "Epoch: 77 | Batch_idx: 200 |  Loss_1: (0.0331) | Acc_1: (98.85%) (25431/25728)\n",
      "Epoch: 77 | Batch_idx: 210 |  Loss_1: (0.0344) | Acc_1: (98.81%) (26687/27008)\n",
      "Epoch: 77 | Batch_idx: 220 |  Loss_1: (0.0339) | Acc_1: (98.83%) (27956/28288)\n",
      "Epoch: 77 | Batch_idx: 230 |  Loss_1: (0.0339) | Acc_1: (98.81%) (29215/29568)\n",
      "Epoch: 77 | Batch_idx: 240 |  Loss_1: (0.0333) | Acc_1: (98.83%) (30486/30848)\n",
      "Epoch: 77 | Batch_idx: 250 |  Loss_1: (0.0337) | Acc_1: (98.81%) (31745/32128)\n",
      "Epoch: 77 | Batch_idx: 260 |  Loss_1: (0.0337) | Acc_1: (98.80%) (33006/33408)\n",
      "Epoch: 77 | Batch_idx: 270 |  Loss_1: (0.0337) | Acc_1: (98.80%) (34271/34688)\n",
      "Epoch: 77 | Batch_idx: 280 |  Loss_1: (0.0333) | Acc_1: (98.82%) (35543/35968)\n",
      "Epoch: 77 | Batch_idx: 290 |  Loss_1: (0.0333) | Acc_1: (98.82%) (36807/37248)\n",
      "Epoch: 77 | Batch_idx: 300 |  Loss_1: (0.0333) | Acc_1: (98.81%) (38071/38528)\n",
      "Epoch: 77 | Batch_idx: 310 |  Loss_1: (0.0331) | Acc_1: (98.81%) (39336/39808)\n",
      "Epoch: 77 | Batch_idx: 320 |  Loss_1: (0.0330) | Acc_1: (98.82%) (40602/41088)\n",
      "Epoch: 77 | Batch_idx: 330 |  Loss_1: (0.0327) | Acc_1: (98.82%) (41869/42368)\n",
      "Epoch: 77 | Batch_idx: 340 |  Loss_1: (0.0327) | Acc_1: (98.83%) (43136/43648)\n",
      "Epoch: 77 | Batch_idx: 350 |  Loss_1: (0.0328) | Acc_1: (98.81%) (44395/44928)\n",
      "Epoch: 77 | Batch_idx: 360 |  Loss_1: (0.0328) | Acc_1: (98.82%) (45663/46208)\n",
      "Epoch: 77 | Batch_idx: 370 |  Loss_1: (0.0328) | Acc_1: (98.83%) (46931/47488)\n",
      "Epoch: 77 | Batch_idx: 380 |  Loss_1: (0.0326) | Acc_1: (98.83%) (48198/48768)\n",
      "Epoch: 77 | Batch_idx: 390 |  Loss_1: (0.0328) | Acc_1: (98.82%) (49412/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5035) | Acc: (90.18%) (9018/10000)\n",
      "Epoch: 78 | Batch_idx: 0 |  Loss_1: (0.0078) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 78 | Batch_idx: 10 |  Loss_1: (0.0235) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 78 | Batch_idx: 20 |  Loss_1: (0.0216) | Acc_1: (99.26%) (2668/2688)\n",
      "Epoch: 78 | Batch_idx: 30 |  Loss_1: (0.0260) | Acc_1: (99.12%) (3933/3968)\n",
      "Epoch: 78 | Batch_idx: 40 |  Loss_1: (0.0253) | Acc_1: (99.12%) (5202/5248)\n",
      "Epoch: 78 | Batch_idx: 50 |  Loss_1: (0.0256) | Acc_1: (99.13%) (6471/6528)\n",
      "Epoch: 78 | Batch_idx: 60 |  Loss_1: (0.0246) | Acc_1: (99.14%) (7741/7808)\n",
      "Epoch: 78 | Batch_idx: 70 |  Loss_1: (0.0259) | Acc_1: (99.12%) (9008/9088)\n",
      "Epoch: 78 | Batch_idx: 80 |  Loss_1: (0.0272) | Acc_1: (99.10%) (10275/10368)\n",
      "Epoch: 78 | Batch_idx: 90 |  Loss_1: (0.0266) | Acc_1: (99.11%) (11544/11648)\n",
      "Epoch: 78 | Batch_idx: 100 |  Loss_1: (0.0274) | Acc_1: (99.07%) (12808/12928)\n",
      "Epoch: 78 | Batch_idx: 110 |  Loss_1: (0.0283) | Acc_1: (99.06%) (14075/14208)\n",
      "Epoch: 78 | Batch_idx: 120 |  Loss_1: (0.0289) | Acc_1: (99.06%) (15342/15488)\n",
      "Epoch: 78 | Batch_idx: 130 |  Loss_1: (0.0287) | Acc_1: (99.06%) (16610/16768)\n",
      "Epoch: 78 | Batch_idx: 140 |  Loss_1: (0.0299) | Acc_1: (99.01%) (17869/18048)\n",
      "Epoch: 78 | Batch_idx: 150 |  Loss_1: (0.0305) | Acc_1: (98.99%) (19132/19328)\n",
      "Epoch: 78 | Batch_idx: 160 |  Loss_1: (0.0307) | Acc_1: (98.99%) (20399/20608)\n",
      "Epoch: 78 | Batch_idx: 170 |  Loss_1: (0.0312) | Acc_1: (98.98%) (21664/21888)\n",
      "Epoch: 78 | Batch_idx: 180 |  Loss_1: (0.0311) | Acc_1: (98.96%) (22927/23168)\n",
      "Epoch: 78 | Batch_idx: 190 |  Loss_1: (0.0308) | Acc_1: (98.95%) (24192/24448)\n",
      "Epoch: 78 | Batch_idx: 200 |  Loss_1: (0.0309) | Acc_1: (98.94%) (25456/25728)\n",
      "Epoch: 78 | Batch_idx: 210 |  Loss_1: (0.0305) | Acc_1: (98.96%) (26727/27008)\n",
      "Epoch: 78 | Batch_idx: 220 |  Loss_1: (0.0312) | Acc_1: (98.92%) (27982/28288)\n",
      "Epoch: 78 | Batch_idx: 230 |  Loss_1: (0.0311) | Acc_1: (98.92%) (29249/29568)\n",
      "Epoch: 78 | Batch_idx: 240 |  Loss_1: (0.0308) | Acc_1: (98.93%) (30519/30848)\n",
      "Epoch: 78 | Batch_idx: 250 |  Loss_1: (0.0306) | Acc_1: (98.94%) (31789/32128)\n",
      "Epoch: 78 | Batch_idx: 260 |  Loss_1: (0.0305) | Acc_1: (98.94%) (33055/33408)\n",
      "Epoch: 78 | Batch_idx: 270 |  Loss_1: (0.0305) | Acc_1: (98.94%) (34321/34688)\n",
      "Epoch: 78 | Batch_idx: 280 |  Loss_1: (0.0306) | Acc_1: (98.93%) (35583/35968)\n",
      "Epoch: 78 | Batch_idx: 290 |  Loss_1: (0.0305) | Acc_1: (98.92%) (36846/37248)\n",
      "Epoch: 78 | Batch_idx: 300 |  Loss_1: (0.0302) | Acc_1: (98.93%) (38116/38528)\n",
      "Epoch: 78 | Batch_idx: 310 |  Loss_1: (0.0300) | Acc_1: (98.93%) (39384/39808)\n",
      "Epoch: 78 | Batch_idx: 320 |  Loss_1: (0.0302) | Acc_1: (98.94%) (40653/41088)\n",
      "Epoch: 78 | Batch_idx: 330 |  Loss_1: (0.0300) | Acc_1: (98.94%) (41919/42368)\n",
      "Epoch: 78 | Batch_idx: 340 |  Loss_1: (0.0299) | Acc_1: (98.95%) (43188/43648)\n",
      "Epoch: 78 | Batch_idx: 350 |  Loss_1: (0.0299) | Acc_1: (98.95%) (44455/44928)\n",
      "Epoch: 78 | Batch_idx: 360 |  Loss_1: (0.0300) | Acc_1: (98.94%) (45720/46208)\n",
      "Epoch: 78 | Batch_idx: 370 |  Loss_1: (0.0301) | Acc_1: (98.94%) (46983/47488)\n",
      "Epoch: 78 | Batch_idx: 380 |  Loss_1: (0.0301) | Acc_1: (98.93%) (48247/48768)\n",
      "Epoch: 78 | Batch_idx: 390 |  Loss_1: (0.0300) | Acc_1: (98.94%) (49469/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4561) | Acc: (91.03%) (9103/10000)\n",
      "Epoch: 79 | Batch_idx: 0 |  Loss_1: (0.0014) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 79 | Batch_idx: 10 |  Loss_1: (0.0256) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 79 | Batch_idx: 20 |  Loss_1: (0.0194) | Acc_1: (99.37%) (2671/2688)\n",
      "Epoch: 79 | Batch_idx: 30 |  Loss_1: (0.0190) | Acc_1: (99.34%) (3942/3968)\n",
      "Epoch: 79 | Batch_idx: 40 |  Loss_1: (0.0219) | Acc_1: (99.26%) (5209/5248)\n",
      "Epoch: 79 | Batch_idx: 50 |  Loss_1: (0.0220) | Acc_1: (99.22%) (6477/6528)\n",
      "Epoch: 79 | Batch_idx: 60 |  Loss_1: (0.0236) | Acc_1: (99.21%) (7746/7808)\n",
      "Epoch: 79 | Batch_idx: 70 |  Loss_1: (0.0236) | Acc_1: (99.21%) (9016/9088)\n",
      "Epoch: 79 | Batch_idx: 80 |  Loss_1: (0.0244) | Acc_1: (99.19%) (10284/10368)\n",
      "Epoch: 79 | Batch_idx: 90 |  Loss_1: (0.0250) | Acc_1: (99.15%) (11549/11648)\n",
      "Epoch: 79 | Batch_idx: 100 |  Loss_1: (0.0245) | Acc_1: (99.17%) (12821/12928)\n",
      "Epoch: 79 | Batch_idx: 110 |  Loss_1: (0.0241) | Acc_1: (99.18%) (14092/14208)\n",
      "Epoch: 79 | Batch_idx: 120 |  Loss_1: (0.0247) | Acc_1: (99.15%) (15357/15488)\n",
      "Epoch: 79 | Batch_idx: 130 |  Loss_1: (0.0249) | Acc_1: (99.14%) (16623/16768)\n",
      "Epoch: 79 | Batch_idx: 140 |  Loss_1: (0.0261) | Acc_1: (99.10%) (17885/18048)\n",
      "Epoch: 79 | Batch_idx: 150 |  Loss_1: (0.0268) | Acc_1: (99.08%) (19151/19328)\n",
      "Epoch: 79 | Batch_idx: 160 |  Loss_1: (0.0268) | Acc_1: (99.08%) (20419/20608)\n",
      "Epoch: 79 | Batch_idx: 170 |  Loss_1: (0.0267) | Acc_1: (99.08%) (21686/21888)\n",
      "Epoch: 79 | Batch_idx: 180 |  Loss_1: (0.0276) | Acc_1: (99.06%) (22950/23168)\n",
      "Epoch: 79 | Batch_idx: 190 |  Loss_1: (0.0274) | Acc_1: (99.06%) (24217/24448)\n",
      "Epoch: 79 | Batch_idx: 200 |  Loss_1: (0.0285) | Acc_1: (99.01%) (25473/25728)\n",
      "Epoch: 79 | Batch_idx: 210 |  Loss_1: (0.0291) | Acc_1: (98.99%) (26734/27008)\n",
      "Epoch: 79 | Batch_idx: 220 |  Loss_1: (0.0292) | Acc_1: (98.98%) (28000/28288)\n",
      "Epoch: 79 | Batch_idx: 230 |  Loss_1: (0.0302) | Acc_1: (98.95%) (29259/29568)\n",
      "Epoch: 79 | Batch_idx: 240 |  Loss_1: (0.0301) | Acc_1: (98.95%) (30523/30848)\n",
      "Epoch: 79 | Batch_idx: 250 |  Loss_1: (0.0304) | Acc_1: (98.93%) (31785/32128)\n",
      "Epoch: 79 | Batch_idx: 260 |  Loss_1: (0.0308) | Acc_1: (98.91%) (33045/33408)\n",
      "Epoch: 79 | Batch_idx: 270 |  Loss_1: (0.0308) | Acc_1: (98.92%) (34313/34688)\n",
      "Epoch: 79 | Batch_idx: 280 |  Loss_1: (0.0312) | Acc_1: (98.89%) (35569/35968)\n",
      "Epoch: 79 | Batch_idx: 290 |  Loss_1: (0.0314) | Acc_1: (98.88%) (36832/37248)\n",
      "Epoch: 79 | Batch_idx: 300 |  Loss_1: (0.0317) | Acc_1: (98.88%) (38095/38528)\n",
      "Epoch: 79 | Batch_idx: 310 |  Loss_1: (0.0318) | Acc_1: (98.87%) (39358/39808)\n",
      "Epoch: 79 | Batch_idx: 320 |  Loss_1: (0.0317) | Acc_1: (98.87%) (40625/41088)\n",
      "Epoch: 79 | Batch_idx: 330 |  Loss_1: (0.0319) | Acc_1: (98.87%) (41891/42368)\n",
      "Epoch: 79 | Batch_idx: 340 |  Loss_1: (0.0319) | Acc_1: (98.88%) (43158/43648)\n",
      "Epoch: 79 | Batch_idx: 350 |  Loss_1: (0.0316) | Acc_1: (98.89%) (44431/44928)\n",
      "Epoch: 79 | Batch_idx: 360 |  Loss_1: (0.0315) | Acc_1: (98.90%) (45699/46208)\n",
      "Epoch: 79 | Batch_idx: 370 |  Loss_1: (0.0312) | Acc_1: (98.91%) (46970/47488)\n",
      "Epoch: 79 | Batch_idx: 380 |  Loss_1: (0.0312) | Acc_1: (98.90%) (48233/48768)\n",
      "Epoch: 79 | Batch_idx: 390 |  Loss_1: (0.0310) | Acc_1: (98.91%) (49455/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4752) | Acc: (90.81%) (9081/10000)\n",
      "Epoch: 80 | Batch_idx: 0 |  Loss_1: (0.0329) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 80 | Batch_idx: 10 |  Loss_1: (0.0239) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 80 | Batch_idx: 20 |  Loss_1: (0.0241) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 80 | Batch_idx: 30 |  Loss_1: (0.0216) | Acc_1: (99.34%) (3942/3968)\n",
      "Epoch: 80 | Batch_idx: 40 |  Loss_1: (0.0216) | Acc_1: (99.29%) (5211/5248)\n",
      "Epoch: 80 | Batch_idx: 50 |  Loss_1: (0.0211) | Acc_1: (99.30%) (6482/6528)\n",
      "Epoch: 80 | Batch_idx: 60 |  Loss_1: (0.0198) | Acc_1: (99.32%) (7755/7808)\n",
      "Epoch: 80 | Batch_idx: 70 |  Loss_1: (0.0202) | Acc_1: (99.33%) (9027/9088)\n",
      "Epoch: 80 | Batch_idx: 80 |  Loss_1: (0.0201) | Acc_1: (99.32%) (10298/10368)\n",
      "Epoch: 80 | Batch_idx: 90 |  Loss_1: (0.0200) | Acc_1: (99.34%) (11571/11648)\n",
      "Epoch: 80 | Batch_idx: 100 |  Loss_1: (0.0201) | Acc_1: (99.33%) (12842/12928)\n",
      "Epoch: 80 | Batch_idx: 110 |  Loss_1: (0.0201) | Acc_1: (99.33%) (14113/14208)\n",
      "Epoch: 80 | Batch_idx: 120 |  Loss_1: (0.0207) | Acc_1: (99.31%) (15381/15488)\n",
      "Epoch: 80 | Batch_idx: 130 |  Loss_1: (0.0210) | Acc_1: (99.30%) (16650/16768)\n",
      "Epoch: 80 | Batch_idx: 140 |  Loss_1: (0.0211) | Acc_1: (99.29%) (17919/18048)\n",
      "Epoch: 80 | Batch_idx: 150 |  Loss_1: (0.0218) | Acc_1: (99.26%) (19185/19328)\n",
      "Epoch: 80 | Batch_idx: 160 |  Loss_1: (0.0222) | Acc_1: (99.26%) (20455/20608)\n",
      "Epoch: 80 | Batch_idx: 170 |  Loss_1: (0.0227) | Acc_1: (99.24%) (21722/21888)\n",
      "Epoch: 80 | Batch_idx: 180 |  Loss_1: (0.0225) | Acc_1: (99.25%) (22994/23168)\n",
      "Epoch: 80 | Batch_idx: 190 |  Loss_1: (0.0230) | Acc_1: (99.24%) (24263/24448)\n",
      "Epoch: 80 | Batch_idx: 200 |  Loss_1: (0.0234) | Acc_1: (99.23%) (25529/25728)\n",
      "Epoch: 80 | Batch_idx: 210 |  Loss_1: (0.0242) | Acc_1: (99.21%) (26795/27008)\n",
      "Epoch: 80 | Batch_idx: 220 |  Loss_1: (0.0256) | Acc_1: (99.18%) (28055/28288)\n",
      "Epoch: 80 | Batch_idx: 230 |  Loss_1: (0.0262) | Acc_1: (99.15%) (29317/29568)\n",
      "Epoch: 80 | Batch_idx: 240 |  Loss_1: (0.0263) | Acc_1: (99.14%) (30582/30848)\n",
      "Epoch: 80 | Batch_idx: 250 |  Loss_1: (0.0269) | Acc_1: (99.12%) (31846/32128)\n",
      "Epoch: 80 | Batch_idx: 260 |  Loss_1: (0.0270) | Acc_1: (99.11%) (33112/33408)\n",
      "Epoch: 80 | Batch_idx: 270 |  Loss_1: (0.0274) | Acc_1: (99.09%) (34372/34688)\n",
      "Epoch: 80 | Batch_idx: 280 |  Loss_1: (0.0277) | Acc_1: (99.08%) (35637/35968)\n",
      "Epoch: 80 | Batch_idx: 290 |  Loss_1: (0.0283) | Acc_1: (99.06%) (36898/37248)\n",
      "Epoch: 80 | Batch_idx: 300 |  Loss_1: (0.0281) | Acc_1: (99.07%) (38168/38528)\n",
      "Epoch: 80 | Batch_idx: 310 |  Loss_1: (0.0279) | Acc_1: (99.07%) (39439/39808)\n",
      "Epoch: 80 | Batch_idx: 320 |  Loss_1: (0.0278) | Acc_1: (99.07%) (40707/41088)\n",
      "Epoch: 80 | Batch_idx: 330 |  Loss_1: (0.0278) | Acc_1: (99.07%) (41972/42368)\n",
      "Epoch: 80 | Batch_idx: 340 |  Loss_1: (0.0278) | Acc_1: (99.06%) (43238/43648)\n",
      "Epoch: 80 | Batch_idx: 350 |  Loss_1: (0.0275) | Acc_1: (99.07%) (44510/44928)\n",
      "Epoch: 80 | Batch_idx: 360 |  Loss_1: (0.0281) | Acc_1: (99.05%) (45769/46208)\n",
      "Epoch: 80 | Batch_idx: 370 |  Loss_1: (0.0284) | Acc_1: (99.04%) (47032/47488)\n",
      "Epoch: 80 | Batch_idx: 380 |  Loss_1: (0.0285) | Acc_1: (99.03%) (48295/48768)\n",
      "Epoch: 80 | Batch_idx: 390 |  Loss_1: (0.0286) | Acc_1: (99.03%) (49514/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4669) | Acc: (90.79%) (9079/10000)\n",
      "Epoch: 81 | Batch_idx: 0 |  Loss_1: (0.0148) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 81 | Batch_idx: 10 |  Loss_1: (0.0330) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 81 | Batch_idx: 20 |  Loss_1: (0.0308) | Acc_1: (98.81%) (2656/2688)\n",
      "Epoch: 81 | Batch_idx: 30 |  Loss_1: (0.0364) | Acc_1: (98.69%) (3916/3968)\n",
      "Epoch: 81 | Batch_idx: 40 |  Loss_1: (0.0372) | Acc_1: (98.69%) (5179/5248)\n",
      "Epoch: 81 | Batch_idx: 50 |  Loss_1: (0.0363) | Acc_1: (98.71%) (6444/6528)\n",
      "Epoch: 81 | Batch_idx: 60 |  Loss_1: (0.0357) | Acc_1: (98.73%) (7709/7808)\n",
      "Epoch: 81 | Batch_idx: 70 |  Loss_1: (0.0355) | Acc_1: (98.73%) (8973/9088)\n",
      "Epoch: 81 | Batch_idx: 80 |  Loss_1: (0.0339) | Acc_1: (98.81%) (10245/10368)\n",
      "Epoch: 81 | Batch_idx: 90 |  Loss_1: (0.0332) | Acc_1: (98.84%) (11513/11648)\n",
      "Epoch: 81 | Batch_idx: 100 |  Loss_1: (0.0321) | Acc_1: (98.88%) (12783/12928)\n",
      "Epoch: 81 | Batch_idx: 110 |  Loss_1: (0.0312) | Acc_1: (98.92%) (14054/14208)\n",
      "Epoch: 81 | Batch_idx: 120 |  Loss_1: (0.0305) | Acc_1: (98.95%) (15325/15488)\n",
      "Epoch: 81 | Batch_idx: 130 |  Loss_1: (0.0299) | Acc_1: (98.94%) (16591/16768)\n",
      "Epoch: 81 | Batch_idx: 140 |  Loss_1: (0.0292) | Acc_1: (98.97%) (17862/18048)\n",
      "Epoch: 81 | Batch_idx: 150 |  Loss_1: (0.0294) | Acc_1: (98.98%) (19130/19328)\n",
      "Epoch: 81 | Batch_idx: 160 |  Loss_1: (0.0292) | Acc_1: (98.97%) (20396/20608)\n",
      "Epoch: 81 | Batch_idx: 170 |  Loss_1: (0.0286) | Acc_1: (98.99%) (21667/21888)\n",
      "Epoch: 81 | Batch_idx: 180 |  Loss_1: (0.0281) | Acc_1: (99.02%) (22941/23168)\n",
      "Epoch: 81 | Batch_idx: 190 |  Loss_1: (0.0283) | Acc_1: (99.03%) (24210/24448)\n",
      "Epoch: 81 | Batch_idx: 200 |  Loss_1: (0.0281) | Acc_1: (99.04%) (25481/25728)\n",
      "Epoch: 81 | Batch_idx: 210 |  Loss_1: (0.0279) | Acc_1: (99.04%) (26749/27008)\n",
      "Epoch: 81 | Batch_idx: 220 |  Loss_1: (0.0277) | Acc_1: (99.04%) (28017/28288)\n",
      "Epoch: 81 | Batch_idx: 230 |  Loss_1: (0.0277) | Acc_1: (99.05%) (29288/29568)\n",
      "Epoch: 81 | Batch_idx: 240 |  Loss_1: (0.0277) | Acc_1: (99.04%) (30553/30848)\n",
      "Epoch: 81 | Batch_idx: 250 |  Loss_1: (0.0278) | Acc_1: (99.03%) (31816/32128)\n",
      "Epoch: 81 | Batch_idx: 260 |  Loss_1: (0.0279) | Acc_1: (99.02%) (33082/33408)\n",
      "Epoch: 81 | Batch_idx: 270 |  Loss_1: (0.0275) | Acc_1: (99.04%) (34356/34688)\n",
      "Epoch: 81 | Batch_idx: 280 |  Loss_1: (0.0272) | Acc_1: (99.05%) (35626/35968)\n",
      "Epoch: 81 | Batch_idx: 290 |  Loss_1: (0.0272) | Acc_1: (99.04%) (36891/37248)\n",
      "Epoch: 81 | Batch_idx: 300 |  Loss_1: (0.0270) | Acc_1: (99.04%) (38160/38528)\n",
      "Epoch: 81 | Batch_idx: 310 |  Loss_1: (0.0266) | Acc_1: (99.06%) (39432/39808)\n",
      "Epoch: 81 | Batch_idx: 320 |  Loss_1: (0.0263) | Acc_1: (99.06%) (40702/41088)\n",
      "Epoch: 81 | Batch_idx: 330 |  Loss_1: (0.0262) | Acc_1: (99.07%) (41972/42368)\n",
      "Epoch: 81 | Batch_idx: 340 |  Loss_1: (0.0264) | Acc_1: (99.07%) (43240/43648)\n",
      "Epoch: 81 | Batch_idx: 350 |  Loss_1: (0.0265) | Acc_1: (99.06%) (44507/44928)\n",
      "Epoch: 81 | Batch_idx: 360 |  Loss_1: (0.0265) | Acc_1: (99.06%) (45775/46208)\n",
      "Epoch: 81 | Batch_idx: 370 |  Loss_1: (0.0264) | Acc_1: (99.07%) (47047/47488)\n",
      "Epoch: 81 | Batch_idx: 380 |  Loss_1: (0.0266) | Acc_1: (99.07%) (48314/48768)\n",
      "Epoch: 81 | Batch_idx: 390 |  Loss_1: (0.0269) | Acc_1: (99.06%) (49529/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5169) | Acc: (90.68%) (9068/10000)\n",
      "Epoch: 82 | Batch_idx: 0 |  Loss_1: (0.0110) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 82 | Batch_idx: 10 |  Loss_1: (0.0330) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 82 | Batch_idx: 20 |  Loss_1: (0.0302) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 82 | Batch_idx: 30 |  Loss_1: (0.0292) | Acc_1: (98.97%) (3927/3968)\n",
      "Epoch: 82 | Batch_idx: 40 |  Loss_1: (0.0271) | Acc_1: (99.05%) (5198/5248)\n",
      "Epoch: 82 | Batch_idx: 50 |  Loss_1: (0.0252) | Acc_1: (99.14%) (6472/6528)\n",
      "Epoch: 82 | Batch_idx: 60 |  Loss_1: (0.0245) | Acc_1: (99.19%) (7745/7808)\n",
      "Epoch: 82 | Batch_idx: 70 |  Loss_1: (0.0244) | Acc_1: (99.16%) (9012/9088)\n",
      "Epoch: 82 | Batch_idx: 80 |  Loss_1: (0.0253) | Acc_1: (99.12%) (10277/10368)\n",
      "Epoch: 82 | Batch_idx: 90 |  Loss_1: (0.0273) | Acc_1: (99.04%) (11536/11648)\n",
      "Epoch: 82 | Batch_idx: 100 |  Loss_1: (0.0278) | Acc_1: (98.99%) (12797/12928)\n",
      "Epoch: 82 | Batch_idx: 110 |  Loss_1: (0.0280) | Acc_1: (98.99%) (14065/14208)\n",
      "Epoch: 82 | Batch_idx: 120 |  Loss_1: (0.0284) | Acc_1: (98.98%) (15330/15488)\n",
      "Epoch: 82 | Batch_idx: 130 |  Loss_1: (0.0285) | Acc_1: (99.00%) (16600/16768)\n",
      "Epoch: 82 | Batch_idx: 140 |  Loss_1: (0.0285) | Acc_1: (98.97%) (17863/18048)\n",
      "Epoch: 82 | Batch_idx: 150 |  Loss_1: (0.0286) | Acc_1: (98.97%) (19129/19328)\n",
      "Epoch: 82 | Batch_idx: 160 |  Loss_1: (0.0296) | Acc_1: (98.96%) (20394/20608)\n",
      "Epoch: 82 | Batch_idx: 170 |  Loss_1: (0.0294) | Acc_1: (98.97%) (21662/21888)\n",
      "Epoch: 82 | Batch_idx: 180 |  Loss_1: (0.0300) | Acc_1: (98.96%) (22927/23168)\n",
      "Epoch: 82 | Batch_idx: 190 |  Loss_1: (0.0303) | Acc_1: (98.95%) (24192/24448)\n",
      "Epoch: 82 | Batch_idx: 200 |  Loss_1: (0.0303) | Acc_1: (98.96%) (25460/25728)\n",
      "Epoch: 82 | Batch_idx: 210 |  Loss_1: (0.0301) | Acc_1: (98.97%) (26729/27008)\n",
      "Epoch: 82 | Batch_idx: 220 |  Loss_1: (0.0304) | Acc_1: (98.97%) (27996/28288)\n",
      "Epoch: 82 | Batch_idx: 230 |  Loss_1: (0.0299) | Acc_1: (98.99%) (29270/29568)\n",
      "Epoch: 82 | Batch_idx: 240 |  Loss_1: (0.0300) | Acc_1: (98.99%) (30535/30848)\n",
      "Epoch: 82 | Batch_idx: 250 |  Loss_1: (0.0297) | Acc_1: (98.99%) (31803/32128)\n",
      "Epoch: 82 | Batch_idx: 260 |  Loss_1: (0.0293) | Acc_1: (99.00%) (33074/33408)\n",
      "Epoch: 82 | Batch_idx: 270 |  Loss_1: (0.0289) | Acc_1: (99.02%) (34348/34688)\n",
      "Epoch: 82 | Batch_idx: 280 |  Loss_1: (0.0286) | Acc_1: (99.03%) (35619/35968)\n",
      "Epoch: 82 | Batch_idx: 290 |  Loss_1: (0.0285) | Acc_1: (99.03%) (36886/37248)\n",
      "Epoch: 82 | Batch_idx: 300 |  Loss_1: (0.0281) | Acc_1: (99.04%) (38160/38528)\n",
      "Epoch: 82 | Batch_idx: 310 |  Loss_1: (0.0282) | Acc_1: (99.03%) (39423/39808)\n",
      "Epoch: 82 | Batch_idx: 320 |  Loss_1: (0.0280) | Acc_1: (99.04%) (40695/41088)\n",
      "Epoch: 82 | Batch_idx: 330 |  Loss_1: (0.0282) | Acc_1: (99.04%) (41960/42368)\n",
      "Epoch: 82 | Batch_idx: 340 |  Loss_1: (0.0281) | Acc_1: (99.04%) (43230/43648)\n",
      "Epoch: 82 | Batch_idx: 350 |  Loss_1: (0.0280) | Acc_1: (99.04%) (44497/44928)\n",
      "Epoch: 82 | Batch_idx: 360 |  Loss_1: (0.0281) | Acc_1: (99.04%) (45765/46208)\n",
      "Epoch: 82 | Batch_idx: 370 |  Loss_1: (0.0283) | Acc_1: (99.03%) (47029/47488)\n",
      "Epoch: 82 | Batch_idx: 380 |  Loss_1: (0.0281) | Acc_1: (99.03%) (48296/48768)\n",
      "Epoch: 82 | Batch_idx: 390 |  Loss_1: (0.0283) | Acc_1: (99.02%) (49512/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4963) | Acc: (90.79%) (9079/10000)\n",
      "Epoch: 83 | Batch_idx: 0 |  Loss_1: (0.0030) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 83 | Batch_idx: 10 |  Loss_1: (0.0254) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 83 | Batch_idx: 20 |  Loss_1: (0.0226) | Acc_1: (99.14%) (2665/2688)\n",
      "Epoch: 83 | Batch_idx: 30 |  Loss_1: (0.0245) | Acc_1: (99.02%) (3929/3968)\n",
      "Epoch: 83 | Batch_idx: 40 |  Loss_1: (0.0256) | Acc_1: (99.05%) (5198/5248)\n",
      "Epoch: 83 | Batch_idx: 50 |  Loss_1: (0.0233) | Acc_1: (99.13%) (6471/6528)\n",
      "Epoch: 83 | Batch_idx: 60 |  Loss_1: (0.0228) | Acc_1: (99.15%) (7742/7808)\n",
      "Epoch: 83 | Batch_idx: 70 |  Loss_1: (0.0221) | Acc_1: (99.15%) (9011/9088)\n",
      "Epoch: 83 | Batch_idx: 80 |  Loss_1: (0.0235) | Acc_1: (99.14%) (10279/10368)\n",
      "Epoch: 83 | Batch_idx: 90 |  Loss_1: (0.0225) | Acc_1: (99.16%) (11550/11648)\n",
      "Epoch: 83 | Batch_idx: 100 |  Loss_1: (0.0226) | Acc_1: (99.16%) (12820/12928)\n",
      "Epoch: 83 | Batch_idx: 110 |  Loss_1: (0.0228) | Acc_1: (99.18%) (14092/14208)\n",
      "Epoch: 83 | Batch_idx: 120 |  Loss_1: (0.0233) | Acc_1: (99.15%) (15357/15488)\n",
      "Epoch: 83 | Batch_idx: 130 |  Loss_1: (0.0230) | Acc_1: (99.17%) (16628/16768)\n",
      "Epoch: 83 | Batch_idx: 140 |  Loss_1: (0.0226) | Acc_1: (99.18%) (17900/18048)\n",
      "Epoch: 83 | Batch_idx: 150 |  Loss_1: (0.0228) | Acc_1: (99.19%) (19171/19328)\n",
      "Epoch: 83 | Batch_idx: 160 |  Loss_1: (0.0229) | Acc_1: (99.16%) (20434/20608)\n",
      "Epoch: 83 | Batch_idx: 170 |  Loss_1: (0.0229) | Acc_1: (99.15%) (21702/21888)\n",
      "Epoch: 83 | Batch_idx: 180 |  Loss_1: (0.0232) | Acc_1: (99.13%) (22967/23168)\n",
      "Epoch: 83 | Batch_idx: 190 |  Loss_1: (0.0228) | Acc_1: (99.15%) (24241/24448)\n",
      "Epoch: 83 | Batch_idx: 200 |  Loss_1: (0.0226) | Acc_1: (99.17%) (25514/25728)\n",
      "Epoch: 83 | Batch_idx: 210 |  Loss_1: (0.0228) | Acc_1: (99.15%) (26779/27008)\n",
      "Epoch: 83 | Batch_idx: 220 |  Loss_1: (0.0225) | Acc_1: (99.17%) (28054/28288)\n",
      "Epoch: 83 | Batch_idx: 230 |  Loss_1: (0.0226) | Acc_1: (99.17%) (29324/29568)\n",
      "Epoch: 83 | Batch_idx: 240 |  Loss_1: (0.0228) | Acc_1: (99.16%) (30590/30848)\n",
      "Epoch: 83 | Batch_idx: 250 |  Loss_1: (0.0232) | Acc_1: (99.15%) (31855/32128)\n",
      "Epoch: 83 | Batch_idx: 260 |  Loss_1: (0.0235) | Acc_1: (99.13%) (33118/33408)\n",
      "Epoch: 83 | Batch_idx: 270 |  Loss_1: (0.0235) | Acc_1: (99.13%) (34387/34688)\n",
      "Epoch: 83 | Batch_idx: 280 |  Loss_1: (0.0240) | Acc_1: (99.11%) (35649/35968)\n",
      "Epoch: 83 | Batch_idx: 290 |  Loss_1: (0.0241) | Acc_1: (99.11%) (36917/37248)\n",
      "Epoch: 83 | Batch_idx: 300 |  Loss_1: (0.0244) | Acc_1: (99.10%) (38181/38528)\n",
      "Epoch: 83 | Batch_idx: 310 |  Loss_1: (0.0242) | Acc_1: (99.11%) (39455/39808)\n",
      "Epoch: 83 | Batch_idx: 320 |  Loss_1: (0.0243) | Acc_1: (99.11%) (40722/41088)\n",
      "Epoch: 83 | Batch_idx: 330 |  Loss_1: (0.0242) | Acc_1: (99.11%) (41990/42368)\n",
      "Epoch: 83 | Batch_idx: 340 |  Loss_1: (0.0240) | Acc_1: (99.12%) (43263/43648)\n",
      "Epoch: 83 | Batch_idx: 350 |  Loss_1: (0.0241) | Acc_1: (99.13%) (44535/44928)\n",
      "Epoch: 83 | Batch_idx: 360 |  Loss_1: (0.0240) | Acc_1: (99.13%) (45804/46208)\n",
      "Epoch: 83 | Batch_idx: 370 |  Loss_1: (0.0241) | Acc_1: (99.12%) (47072/47488)\n",
      "Epoch: 83 | Batch_idx: 380 |  Loss_1: (0.0239) | Acc_1: (99.13%) (48346/48768)\n",
      "Epoch: 83 | Batch_idx: 390 |  Loss_1: (0.0240) | Acc_1: (99.13%) (49567/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4103) | Acc: (91.82%) (9182/10000)\n",
      "Epoch: 84 | Batch_idx: 0 |  Loss_1: (0.0226) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 84 | Batch_idx: 10 |  Loss_1: (0.0175) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 84 | Batch_idx: 20 |  Loss_1: (0.0168) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 84 | Batch_idx: 30 |  Loss_1: (0.0167) | Acc_1: (99.42%) (3945/3968)\n",
      "Epoch: 84 | Batch_idx: 40 |  Loss_1: (0.0182) | Acc_1: (99.29%) (5211/5248)\n",
      "Epoch: 84 | Batch_idx: 50 |  Loss_1: (0.0189) | Acc_1: (99.26%) (6480/6528)\n",
      "Epoch: 84 | Batch_idx: 60 |  Loss_1: (0.0182) | Acc_1: (99.30%) (7753/7808)\n",
      "Epoch: 84 | Batch_idx: 70 |  Loss_1: (0.0179) | Acc_1: (99.34%) (9028/9088)\n",
      "Epoch: 84 | Batch_idx: 80 |  Loss_1: (0.0169) | Acc_1: (99.38%) (10304/10368)\n",
      "Epoch: 84 | Batch_idx: 90 |  Loss_1: (0.0171) | Acc_1: (99.40%) (11578/11648)\n",
      "Epoch: 84 | Batch_idx: 100 |  Loss_1: (0.0170) | Acc_1: (99.37%) (12847/12928)\n",
      "Epoch: 84 | Batch_idx: 110 |  Loss_1: (0.0174) | Acc_1: (99.36%) (14117/14208)\n",
      "Epoch: 84 | Batch_idx: 120 |  Loss_1: (0.0174) | Acc_1: (99.33%) (15385/15488)\n",
      "Epoch: 84 | Batch_idx: 130 |  Loss_1: (0.0178) | Acc_1: (99.33%) (16655/16768)\n",
      "Epoch: 84 | Batch_idx: 140 |  Loss_1: (0.0181) | Acc_1: (99.32%) (17926/18048)\n",
      "Epoch: 84 | Batch_idx: 150 |  Loss_1: (0.0184) | Acc_1: (99.31%) (19194/19328)\n",
      "Epoch: 84 | Batch_idx: 160 |  Loss_1: (0.0184) | Acc_1: (99.31%) (20465/20608)\n",
      "Epoch: 84 | Batch_idx: 170 |  Loss_1: (0.0189) | Acc_1: (99.31%) (21736/21888)\n",
      "Epoch: 84 | Batch_idx: 180 |  Loss_1: (0.0197) | Acc_1: (99.30%) (23005/23168)\n",
      "Epoch: 84 | Batch_idx: 190 |  Loss_1: (0.0201) | Acc_1: (99.28%) (24273/24448)\n",
      "Epoch: 84 | Batch_idx: 200 |  Loss_1: (0.0203) | Acc_1: (99.28%) (25543/25728)\n",
      "Epoch: 84 | Batch_idx: 210 |  Loss_1: (0.0207) | Acc_1: (99.27%) (26811/27008)\n",
      "Epoch: 84 | Batch_idx: 220 |  Loss_1: (0.0212) | Acc_1: (99.27%) (28081/28288)\n",
      "Epoch: 84 | Batch_idx: 230 |  Loss_1: (0.0214) | Acc_1: (99.27%) (29352/29568)\n",
      "Epoch: 84 | Batch_idx: 240 |  Loss_1: (0.0217) | Acc_1: (99.25%) (30618/30848)\n",
      "Epoch: 84 | Batch_idx: 250 |  Loss_1: (0.0218) | Acc_1: (99.24%) (31885/32128)\n",
      "Epoch: 84 | Batch_idx: 260 |  Loss_1: (0.0219) | Acc_1: (99.24%) (33155/33408)\n",
      "Epoch: 84 | Batch_idx: 270 |  Loss_1: (0.0218) | Acc_1: (99.25%) (34427/34688)\n",
      "Epoch: 84 | Batch_idx: 280 |  Loss_1: (0.0222) | Acc_1: (99.23%) (35691/35968)\n",
      "Epoch: 84 | Batch_idx: 290 |  Loss_1: (0.0222) | Acc_1: (99.23%) (36960/37248)\n",
      "Epoch: 84 | Batch_idx: 300 |  Loss_1: (0.0221) | Acc_1: (99.22%) (38228/38528)\n",
      "Epoch: 84 | Batch_idx: 310 |  Loss_1: (0.0222) | Acc_1: (99.22%) (39497/39808)\n",
      "Epoch: 84 | Batch_idx: 320 |  Loss_1: (0.0221) | Acc_1: (99.23%) (40770/41088)\n",
      "Epoch: 84 | Batch_idx: 330 |  Loss_1: (0.0223) | Acc_1: (99.22%) (42038/42368)\n",
      "Epoch: 84 | Batch_idx: 340 |  Loss_1: (0.0224) | Acc_1: (99.22%) (43309/43648)\n",
      "Epoch: 84 | Batch_idx: 350 |  Loss_1: (0.0225) | Acc_1: (99.22%) (44579/44928)\n",
      "Epoch: 84 | Batch_idx: 360 |  Loss_1: (0.0225) | Acc_1: (99.23%) (45850/46208)\n",
      "Epoch: 84 | Batch_idx: 370 |  Loss_1: (0.0223) | Acc_1: (99.23%) (47122/47488)\n",
      "Epoch: 84 | Batch_idx: 380 |  Loss_1: (0.0222) | Acc_1: (99.24%) (48395/48768)\n",
      "Epoch: 84 | Batch_idx: 390 |  Loss_1: (0.0222) | Acc_1: (99.23%) (49616/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4386) | Acc: (91.69%) (9169/10000)\n",
      "Epoch: 85 | Batch_idx: 0 |  Loss_1: (0.0205) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 85 | Batch_idx: 10 |  Loss_1: (0.0133) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 85 | Batch_idx: 20 |  Loss_1: (0.0204) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 85 | Batch_idx: 30 |  Loss_1: (0.0226) | Acc_1: (99.47%) (3947/3968)\n",
      "Epoch: 85 | Batch_idx: 40 |  Loss_1: (0.0231) | Acc_1: (99.39%) (5216/5248)\n",
      "Epoch: 85 | Batch_idx: 50 |  Loss_1: (0.0243) | Acc_1: (99.30%) (6482/6528)\n",
      "Epoch: 85 | Batch_idx: 60 |  Loss_1: (0.0239) | Acc_1: (99.30%) (7753/7808)\n",
      "Epoch: 85 | Batch_idx: 70 |  Loss_1: (0.0247) | Acc_1: (99.27%) (9022/9088)\n",
      "Epoch: 85 | Batch_idx: 80 |  Loss_1: (0.0266) | Acc_1: (99.14%) (10279/10368)\n",
      "Epoch: 85 | Batch_idx: 90 |  Loss_1: (0.0262) | Acc_1: (99.18%) (11552/11648)\n",
      "Epoch: 85 | Batch_idx: 100 |  Loss_1: (0.0252) | Acc_1: (99.22%) (12827/12928)\n",
      "Epoch: 85 | Batch_idx: 110 |  Loss_1: (0.0256) | Acc_1: (99.21%) (14096/14208)\n",
      "Epoch: 85 | Batch_idx: 120 |  Loss_1: (0.0252) | Acc_1: (99.23%) (15368/15488)\n",
      "Epoch: 85 | Batch_idx: 130 |  Loss_1: (0.0246) | Acc_1: (99.25%) (16643/16768)\n",
      "Epoch: 85 | Batch_idx: 140 |  Loss_1: (0.0244) | Acc_1: (99.25%) (17912/18048)\n",
      "Epoch: 85 | Batch_idx: 150 |  Loss_1: (0.0249) | Acc_1: (99.22%) (19177/19328)\n",
      "Epoch: 85 | Batch_idx: 160 |  Loss_1: (0.0246) | Acc_1: (99.22%) (20448/20608)\n",
      "Epoch: 85 | Batch_idx: 170 |  Loss_1: (0.0246) | Acc_1: (99.22%) (21717/21888)\n",
      "Epoch: 85 | Batch_idx: 180 |  Loss_1: (0.0247) | Acc_1: (99.21%) (22984/23168)\n",
      "Epoch: 85 | Batch_idx: 190 |  Loss_1: (0.0250) | Acc_1: (99.20%) (24253/24448)\n",
      "Epoch: 85 | Batch_idx: 200 |  Loss_1: (0.0248) | Acc_1: (99.21%) (25525/25728)\n",
      "Epoch: 85 | Batch_idx: 210 |  Loss_1: (0.0248) | Acc_1: (99.20%) (26792/27008)\n",
      "Epoch: 85 | Batch_idx: 220 |  Loss_1: (0.0252) | Acc_1: (99.19%) (28060/28288)\n",
      "Epoch: 85 | Batch_idx: 230 |  Loss_1: (0.0248) | Acc_1: (99.20%) (29332/29568)\n",
      "Epoch: 85 | Batch_idx: 240 |  Loss_1: (0.0247) | Acc_1: (99.19%) (30598/30848)\n",
      "Epoch: 85 | Batch_idx: 250 |  Loss_1: (0.0252) | Acc_1: (99.17%) (31861/32128)\n",
      "Epoch: 85 | Batch_idx: 260 |  Loss_1: (0.0250) | Acc_1: (99.16%) (33129/33408)\n",
      "Epoch: 85 | Batch_idx: 270 |  Loss_1: (0.0248) | Acc_1: (99.18%) (34402/34688)\n",
      "Epoch: 85 | Batch_idx: 280 |  Loss_1: (0.0244) | Acc_1: (99.19%) (35675/35968)\n",
      "Epoch: 85 | Batch_idx: 290 |  Loss_1: (0.0243) | Acc_1: (99.19%) (36947/37248)\n",
      "Epoch: 85 | Batch_idx: 300 |  Loss_1: (0.0244) | Acc_1: (99.19%) (38217/38528)\n",
      "Epoch: 85 | Batch_idx: 310 |  Loss_1: (0.0243) | Acc_1: (99.20%) (39488/39808)\n",
      "Epoch: 85 | Batch_idx: 320 |  Loss_1: (0.0245) | Acc_1: (99.19%) (40756/41088)\n",
      "Epoch: 85 | Batch_idx: 330 |  Loss_1: (0.0245) | Acc_1: (99.19%) (42025/42368)\n",
      "Epoch: 85 | Batch_idx: 340 |  Loss_1: (0.0242) | Acc_1: (99.20%) (43299/43648)\n",
      "Epoch: 85 | Batch_idx: 350 |  Loss_1: (0.0244) | Acc_1: (99.20%) (44567/44928)\n",
      "Epoch: 85 | Batch_idx: 360 |  Loss_1: (0.0245) | Acc_1: (99.19%) (45835/46208)\n",
      "Epoch: 85 | Batch_idx: 370 |  Loss_1: (0.0246) | Acc_1: (99.18%) (47100/47488)\n",
      "Epoch: 85 | Batch_idx: 380 |  Loss_1: (0.0248) | Acc_1: (99.17%) (48361/48768)\n",
      "Epoch: 85 | Batch_idx: 390 |  Loss_1: (0.0248) | Acc_1: (99.16%) (49580/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4564) | Acc: (91.73%) (9173/10000)\n",
      "Epoch: 86 | Batch_idx: 0 |  Loss_1: (0.0034) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 86 | Batch_idx: 10 |  Loss_1: (0.0146) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 86 | Batch_idx: 20 |  Loss_1: (0.0182) | Acc_1: (99.37%) (2671/2688)\n",
      "Epoch: 86 | Batch_idx: 30 |  Loss_1: (0.0151) | Acc_1: (99.50%) (3948/3968)\n",
      "Epoch: 86 | Batch_idx: 40 |  Loss_1: (0.0158) | Acc_1: (99.45%) (5219/5248)\n",
      "Epoch: 86 | Batch_idx: 50 |  Loss_1: (0.0157) | Acc_1: (99.48%) (6494/6528)\n",
      "Epoch: 86 | Batch_idx: 60 |  Loss_1: (0.0179) | Acc_1: (99.39%) (7760/7808)\n",
      "Epoch: 86 | Batch_idx: 70 |  Loss_1: (0.0179) | Acc_1: (99.36%) (9030/9088)\n",
      "Epoch: 86 | Batch_idx: 80 |  Loss_1: (0.0191) | Acc_1: (99.32%) (10298/10368)\n",
      "Epoch: 86 | Batch_idx: 90 |  Loss_1: (0.0186) | Acc_1: (99.34%) (11571/11648)\n",
      "Epoch: 86 | Batch_idx: 100 |  Loss_1: (0.0199) | Acc_1: (99.29%) (12836/12928)\n",
      "Epoch: 86 | Batch_idx: 110 |  Loss_1: (0.0194) | Acc_1: (99.32%) (14111/14208)\n",
      "Epoch: 86 | Batch_idx: 120 |  Loss_1: (0.0192) | Acc_1: (99.32%) (15382/15488)\n",
      "Epoch: 86 | Batch_idx: 130 |  Loss_1: (0.0197) | Acc_1: (99.31%) (16653/16768)\n",
      "Epoch: 86 | Batch_idx: 140 |  Loss_1: (0.0205) | Acc_1: (99.30%) (17921/18048)\n",
      "Epoch: 86 | Batch_idx: 150 |  Loss_1: (0.0208) | Acc_1: (99.28%) (19189/19328)\n",
      "Epoch: 86 | Batch_idx: 160 |  Loss_1: (0.0207) | Acc_1: (99.29%) (20461/20608)\n",
      "Epoch: 86 | Batch_idx: 170 |  Loss_1: (0.0205) | Acc_1: (99.29%) (21733/21888)\n",
      "Epoch: 86 | Batch_idx: 180 |  Loss_1: (0.0209) | Acc_1: (99.28%) (23002/23168)\n",
      "Epoch: 86 | Batch_idx: 190 |  Loss_1: (0.0217) | Acc_1: (99.26%) (24267/24448)\n",
      "Epoch: 86 | Batch_idx: 200 |  Loss_1: (0.0220) | Acc_1: (99.24%) (25533/25728)\n",
      "Epoch: 86 | Batch_idx: 210 |  Loss_1: (0.0223) | Acc_1: (99.23%) (26801/27008)\n",
      "Epoch: 86 | Batch_idx: 220 |  Loss_1: (0.0222) | Acc_1: (99.23%) (28070/28288)\n",
      "Epoch: 86 | Batch_idx: 230 |  Loss_1: (0.0227) | Acc_1: (99.21%) (29333/29568)\n",
      "Epoch: 86 | Batch_idx: 240 |  Loss_1: (0.0228) | Acc_1: (99.20%) (30602/30848)\n",
      "Epoch: 86 | Batch_idx: 250 |  Loss_1: (0.0229) | Acc_1: (99.19%) (31868/32128)\n",
      "Epoch: 86 | Batch_idx: 260 |  Loss_1: (0.0227) | Acc_1: (99.20%) (33140/33408)\n",
      "Epoch: 86 | Batch_idx: 270 |  Loss_1: (0.0226) | Acc_1: (99.20%) (34411/34688)\n",
      "Epoch: 86 | Batch_idx: 280 |  Loss_1: (0.0226) | Acc_1: (99.20%) (35682/35968)\n",
      "Epoch: 86 | Batch_idx: 290 |  Loss_1: (0.0230) | Acc_1: (99.20%) (36949/37248)\n",
      "Epoch: 86 | Batch_idx: 300 |  Loss_1: (0.0235) | Acc_1: (99.19%) (38214/38528)\n",
      "Epoch: 86 | Batch_idx: 310 |  Loss_1: (0.0237) | Acc_1: (99.18%) (39481/39808)\n",
      "Epoch: 86 | Batch_idx: 320 |  Loss_1: (0.0246) | Acc_1: (99.17%) (40746/41088)\n",
      "Epoch: 86 | Batch_idx: 330 |  Loss_1: (0.0248) | Acc_1: (99.15%) (42009/42368)\n",
      "Epoch: 86 | Batch_idx: 340 |  Loss_1: (0.0250) | Acc_1: (99.15%) (43277/43648)\n",
      "Epoch: 86 | Batch_idx: 350 |  Loss_1: (0.0250) | Acc_1: (99.15%) (44548/44928)\n",
      "Epoch: 86 | Batch_idx: 360 |  Loss_1: (0.0249) | Acc_1: (99.16%) (45821/46208)\n",
      "Epoch: 86 | Batch_idx: 370 |  Loss_1: (0.0249) | Acc_1: (99.16%) (47091/47488)\n",
      "Epoch: 86 | Batch_idx: 380 |  Loss_1: (0.0246) | Acc_1: (99.17%) (48365/48768)\n",
      "Epoch: 86 | Batch_idx: 390 |  Loss_1: (0.0246) | Acc_1: (99.17%) (49586/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4461) | Acc: (91.95%) (9195/10000)\n",
      "Epoch: 87 | Batch_idx: 0 |  Loss_1: (0.0264) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 87 | Batch_idx: 10 |  Loss_1: (0.0191) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 87 | Batch_idx: 20 |  Loss_1: (0.0185) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 87 | Batch_idx: 30 |  Loss_1: (0.0176) | Acc_1: (99.40%) (3944/3968)\n",
      "Epoch: 87 | Batch_idx: 40 |  Loss_1: (0.0176) | Acc_1: (99.39%) (5216/5248)\n",
      "Epoch: 87 | Batch_idx: 50 |  Loss_1: (0.0178) | Acc_1: (99.36%) (6486/6528)\n",
      "Epoch: 87 | Batch_idx: 60 |  Loss_1: (0.0176) | Acc_1: (99.36%) (7758/7808)\n",
      "Epoch: 87 | Batch_idx: 70 |  Loss_1: (0.0181) | Acc_1: (99.36%) (9030/9088)\n",
      "Epoch: 87 | Batch_idx: 80 |  Loss_1: (0.0191) | Acc_1: (99.32%) (10297/10368)\n",
      "Epoch: 87 | Batch_idx: 90 |  Loss_1: (0.0191) | Acc_1: (99.31%) (11568/11648)\n",
      "Epoch: 87 | Batch_idx: 100 |  Loss_1: (0.0207) | Acc_1: (99.27%) (12834/12928)\n",
      "Epoch: 87 | Batch_idx: 110 |  Loss_1: (0.0215) | Acc_1: (99.25%) (14102/14208)\n",
      "Epoch: 87 | Batch_idx: 120 |  Loss_1: (0.0215) | Acc_1: (99.25%) (15372/15488)\n",
      "Epoch: 87 | Batch_idx: 130 |  Loss_1: (0.0213) | Acc_1: (99.25%) (16643/16768)\n",
      "Epoch: 87 | Batch_idx: 140 |  Loss_1: (0.0218) | Acc_1: (99.23%) (17909/18048)\n",
      "Epoch: 87 | Batch_idx: 150 |  Loss_1: (0.0219) | Acc_1: (99.23%) (19179/19328)\n",
      "Epoch: 87 | Batch_idx: 160 |  Loss_1: (0.0219) | Acc_1: (99.21%) (20446/20608)\n",
      "Epoch: 87 | Batch_idx: 170 |  Loss_1: (0.0219) | Acc_1: (99.21%) (21716/21888)\n",
      "Epoch: 87 | Batch_idx: 180 |  Loss_1: (0.0216) | Acc_1: (99.22%) (22988/23168)\n",
      "Epoch: 87 | Batch_idx: 190 |  Loss_1: (0.0221) | Acc_1: (99.24%) (24261/24448)\n",
      "Epoch: 87 | Batch_idx: 200 |  Loss_1: (0.0216) | Acc_1: (99.26%) (25538/25728)\n",
      "Epoch: 87 | Batch_idx: 210 |  Loss_1: (0.0214) | Acc_1: (99.27%) (26811/27008)\n",
      "Epoch: 87 | Batch_idx: 220 |  Loss_1: (0.0212) | Acc_1: (99.28%) (28085/28288)\n",
      "Epoch: 87 | Batch_idx: 230 |  Loss_1: (0.0212) | Acc_1: (99.29%) (29357/29568)\n",
      "Epoch: 87 | Batch_idx: 240 |  Loss_1: (0.0210) | Acc_1: (99.29%) (30630/30848)\n",
      "Epoch: 87 | Batch_idx: 250 |  Loss_1: (0.0211) | Acc_1: (99.29%) (31899/32128)\n",
      "Epoch: 87 | Batch_idx: 260 |  Loss_1: (0.0212) | Acc_1: (99.28%) (33166/33408)\n",
      "Epoch: 87 | Batch_idx: 270 |  Loss_1: (0.0211) | Acc_1: (99.28%) (34437/34688)\n",
      "Epoch: 87 | Batch_idx: 280 |  Loss_1: (0.0209) | Acc_1: (99.28%) (35709/35968)\n",
      "Epoch: 87 | Batch_idx: 290 |  Loss_1: (0.0211) | Acc_1: (99.26%) (36974/37248)\n",
      "Epoch: 87 | Batch_idx: 300 |  Loss_1: (0.0211) | Acc_1: (99.27%) (38246/38528)\n",
      "Epoch: 87 | Batch_idx: 310 |  Loss_1: (0.0209) | Acc_1: (99.28%) (39520/39808)\n",
      "Epoch: 87 | Batch_idx: 320 |  Loss_1: (0.0206) | Acc_1: (99.29%) (40797/41088)\n",
      "Epoch: 87 | Batch_idx: 330 |  Loss_1: (0.0207) | Acc_1: (99.29%) (42068/42368)\n",
      "Epoch: 87 | Batch_idx: 340 |  Loss_1: (0.0206) | Acc_1: (99.29%) (43340/43648)\n",
      "Epoch: 87 | Batch_idx: 350 |  Loss_1: (0.0209) | Acc_1: (99.28%) (44604/44928)\n",
      "Epoch: 87 | Batch_idx: 360 |  Loss_1: (0.0208) | Acc_1: (99.29%) (45878/46208)\n",
      "Epoch: 87 | Batch_idx: 370 |  Loss_1: (0.0208) | Acc_1: (99.29%) (47151/47488)\n",
      "Epoch: 87 | Batch_idx: 380 |  Loss_1: (0.0208) | Acc_1: (99.29%) (48421/48768)\n",
      "Epoch: 87 | Batch_idx: 390 |  Loss_1: (0.0208) | Acc_1: (99.29%) (49643/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4396) | Acc: (91.97%) (9197/10000)\n",
      "Epoch: 88 | Batch_idx: 0 |  Loss_1: (0.0039) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 88 | Batch_idx: 10 |  Loss_1: (0.0153) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 88 | Batch_idx: 20 |  Loss_1: (0.0219) | Acc_1: (99.29%) (2669/2688)\n",
      "Epoch: 88 | Batch_idx: 30 |  Loss_1: (0.0195) | Acc_1: (99.29%) (3940/3968)\n",
      "Epoch: 88 | Batch_idx: 40 |  Loss_1: (0.0225) | Acc_1: (99.20%) (5206/5248)\n",
      "Epoch: 88 | Batch_idx: 50 |  Loss_1: (0.0241) | Acc_1: (99.13%) (6471/6528)\n",
      "Epoch: 88 | Batch_idx: 60 |  Loss_1: (0.0230) | Acc_1: (99.15%) (7742/7808)\n",
      "Epoch: 88 | Batch_idx: 70 |  Loss_1: (0.0226) | Acc_1: (99.16%) (9012/9088)\n",
      "Epoch: 88 | Batch_idx: 80 |  Loss_1: (0.0224) | Acc_1: (99.21%) (10286/10368)\n",
      "Epoch: 88 | Batch_idx: 90 |  Loss_1: (0.0235) | Acc_1: (99.17%) (11551/11648)\n",
      "Epoch: 88 | Batch_idx: 100 |  Loss_1: (0.0227) | Acc_1: (99.21%) (12826/12928)\n",
      "Epoch: 88 | Batch_idx: 110 |  Loss_1: (0.0217) | Acc_1: (99.25%) (14101/14208)\n",
      "Epoch: 88 | Batch_idx: 120 |  Loss_1: (0.0214) | Acc_1: (99.26%) (15374/15488)\n",
      "Epoch: 88 | Batch_idx: 130 |  Loss_1: (0.0207) | Acc_1: (99.28%) (16647/16768)\n",
      "Epoch: 88 | Batch_idx: 140 |  Loss_1: (0.0206) | Acc_1: (99.30%) (17921/18048)\n",
      "Epoch: 88 | Batch_idx: 150 |  Loss_1: (0.0203) | Acc_1: (99.31%) (19195/19328)\n",
      "Epoch: 88 | Batch_idx: 160 |  Loss_1: (0.0203) | Acc_1: (99.31%) (20465/20608)\n",
      "Epoch: 88 | Batch_idx: 170 |  Loss_1: (0.0204) | Acc_1: (99.31%) (21736/21888)\n",
      "Epoch: 88 | Batch_idx: 180 |  Loss_1: (0.0201) | Acc_1: (99.32%) (23010/23168)\n",
      "Epoch: 88 | Batch_idx: 190 |  Loss_1: (0.0199) | Acc_1: (99.32%) (24281/24448)\n",
      "Epoch: 88 | Batch_idx: 200 |  Loss_1: (0.0198) | Acc_1: (99.32%) (25554/25728)\n",
      "Epoch: 88 | Batch_idx: 210 |  Loss_1: (0.0197) | Acc_1: (99.33%) (26828/27008)\n",
      "Epoch: 88 | Batch_idx: 220 |  Loss_1: (0.0205) | Acc_1: (99.30%) (28090/28288)\n",
      "Epoch: 88 | Batch_idx: 230 |  Loss_1: (0.0210) | Acc_1: (99.28%) (29355/29568)\n",
      "Epoch: 88 | Batch_idx: 240 |  Loss_1: (0.0210) | Acc_1: (99.28%) (30625/30848)\n",
      "Epoch: 88 | Batch_idx: 250 |  Loss_1: (0.0213) | Acc_1: (99.27%) (31894/32128)\n",
      "Epoch: 88 | Batch_idx: 260 |  Loss_1: (0.0223) | Acc_1: (99.25%) (33158/33408)\n",
      "Epoch: 88 | Batch_idx: 270 |  Loss_1: (0.0220) | Acc_1: (99.26%) (34431/34688)\n",
      "Epoch: 88 | Batch_idx: 280 |  Loss_1: (0.0221) | Acc_1: (99.25%) (35699/35968)\n",
      "Epoch: 88 | Batch_idx: 290 |  Loss_1: (0.0220) | Acc_1: (99.25%) (36969/37248)\n",
      "Epoch: 88 | Batch_idx: 300 |  Loss_1: (0.0218) | Acc_1: (99.25%) (38239/38528)\n",
      "Epoch: 88 | Batch_idx: 310 |  Loss_1: (0.0215) | Acc_1: (99.26%) (39513/39808)\n",
      "Epoch: 88 | Batch_idx: 320 |  Loss_1: (0.0215) | Acc_1: (99.26%) (40783/41088)\n",
      "Epoch: 88 | Batch_idx: 330 |  Loss_1: (0.0212) | Acc_1: (99.27%) (42060/42368)\n",
      "Epoch: 88 | Batch_idx: 340 |  Loss_1: (0.0211) | Acc_1: (99.28%) (43333/43648)\n",
      "Epoch: 88 | Batch_idx: 350 |  Loss_1: (0.0211) | Acc_1: (99.28%) (44603/44928)\n",
      "Epoch: 88 | Batch_idx: 360 |  Loss_1: (0.0209) | Acc_1: (99.29%) (45878/46208)\n",
      "Epoch: 88 | Batch_idx: 370 |  Loss_1: (0.0210) | Acc_1: (99.28%) (47148/47488)\n",
      "Epoch: 88 | Batch_idx: 380 |  Loss_1: (0.0210) | Acc_1: (99.28%) (48418/48768)\n",
      "Epoch: 88 | Batch_idx: 390 |  Loss_1: (0.0209) | Acc_1: (99.28%) (49642/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4786) | Acc: (91.43%) (9143/10000)\n",
      "Epoch: 89 | Batch_idx: 0 |  Loss_1: (0.0122) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 89 | Batch_idx: 10 |  Loss_1: (0.0102) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 89 | Batch_idx: 20 |  Loss_1: (0.0207) | Acc_1: (99.29%) (2669/2688)\n",
      "Epoch: 89 | Batch_idx: 30 |  Loss_1: (0.0227) | Acc_1: (99.22%) (3937/3968)\n",
      "Epoch: 89 | Batch_idx: 40 |  Loss_1: (0.0203) | Acc_1: (99.29%) (5211/5248)\n",
      "Epoch: 89 | Batch_idx: 50 |  Loss_1: (0.0203) | Acc_1: (99.30%) (6482/6528)\n",
      "Epoch: 89 | Batch_idx: 60 |  Loss_1: (0.0200) | Acc_1: (99.31%) (7754/7808)\n",
      "Epoch: 89 | Batch_idx: 70 |  Loss_1: (0.0216) | Acc_1: (99.30%) (9024/9088)\n",
      "Epoch: 89 | Batch_idx: 80 |  Loss_1: (0.0210) | Acc_1: (99.32%) (10297/10368)\n",
      "Epoch: 89 | Batch_idx: 90 |  Loss_1: (0.0207) | Acc_1: (99.33%) (11570/11648)\n",
      "Epoch: 89 | Batch_idx: 100 |  Loss_1: (0.0205) | Acc_1: (99.34%) (12843/12928)\n",
      "Epoch: 89 | Batch_idx: 110 |  Loss_1: (0.0208) | Acc_1: (99.32%) (14112/14208)\n",
      "Epoch: 89 | Batch_idx: 120 |  Loss_1: (0.0204) | Acc_1: (99.34%) (15386/15488)\n",
      "Epoch: 89 | Batch_idx: 130 |  Loss_1: (0.0204) | Acc_1: (99.33%) (16656/16768)\n",
      "Epoch: 89 | Batch_idx: 140 |  Loss_1: (0.0199) | Acc_1: (99.34%) (17928/18048)\n",
      "Epoch: 89 | Batch_idx: 150 |  Loss_1: (0.0200) | Acc_1: (99.34%) (19200/19328)\n",
      "Epoch: 89 | Batch_idx: 160 |  Loss_1: (0.0193) | Acc_1: (99.35%) (20475/20608)\n",
      "Epoch: 89 | Batch_idx: 170 |  Loss_1: (0.0188) | Acc_1: (99.38%) (21752/21888)\n",
      "Epoch: 89 | Batch_idx: 180 |  Loss_1: (0.0192) | Acc_1: (99.37%) (23022/23168)\n",
      "Epoch: 89 | Batch_idx: 190 |  Loss_1: (0.0190) | Acc_1: (99.38%) (24296/24448)\n",
      "Epoch: 89 | Batch_idx: 200 |  Loss_1: (0.0191) | Acc_1: (99.37%) (25567/25728)\n",
      "Epoch: 89 | Batch_idx: 210 |  Loss_1: (0.0193) | Acc_1: (99.38%) (26840/27008)\n",
      "Epoch: 89 | Batch_idx: 220 |  Loss_1: (0.0193) | Acc_1: (99.38%) (28112/28288)\n",
      "Epoch: 89 | Batch_idx: 230 |  Loss_1: (0.0190) | Acc_1: (99.38%) (29386/29568)\n",
      "Epoch: 89 | Batch_idx: 240 |  Loss_1: (0.0192) | Acc_1: (99.38%) (30657/30848)\n",
      "Epoch: 89 | Batch_idx: 250 |  Loss_1: (0.0192) | Acc_1: (99.37%) (31927/32128)\n",
      "Epoch: 89 | Batch_idx: 260 |  Loss_1: (0.0190) | Acc_1: (99.38%) (33201/33408)\n",
      "Epoch: 89 | Batch_idx: 270 |  Loss_1: (0.0191) | Acc_1: (99.38%) (34472/34688)\n",
      "Epoch: 89 | Batch_idx: 280 |  Loss_1: (0.0188) | Acc_1: (99.39%) (35748/35968)\n",
      "Epoch: 89 | Batch_idx: 290 |  Loss_1: (0.0188) | Acc_1: (99.39%) (37021/37248)\n",
      "Epoch: 89 | Batch_idx: 300 |  Loss_1: (0.0187) | Acc_1: (99.40%) (38295/38528)\n",
      "Epoch: 89 | Batch_idx: 310 |  Loss_1: (0.0185) | Acc_1: (99.40%) (39571/39808)\n",
      "Epoch: 89 | Batch_idx: 320 |  Loss_1: (0.0183) | Acc_1: (99.42%) (40848/41088)\n",
      "Epoch: 89 | Batch_idx: 330 |  Loss_1: (0.0184) | Acc_1: (99.41%) (42120/42368)\n",
      "Epoch: 89 | Batch_idx: 340 |  Loss_1: (0.0185) | Acc_1: (99.41%) (43392/43648)\n",
      "Epoch: 89 | Batch_idx: 350 |  Loss_1: (0.0183) | Acc_1: (99.42%) (44666/44928)\n",
      "Epoch: 89 | Batch_idx: 360 |  Loss_1: (0.0183) | Acc_1: (99.41%) (45937/46208)\n",
      "Epoch: 89 | Batch_idx: 370 |  Loss_1: (0.0182) | Acc_1: (99.42%) (47211/47488)\n",
      "Epoch: 89 | Batch_idx: 380 |  Loss_1: (0.0182) | Acc_1: (99.42%) (48483/48768)\n",
      "Epoch: 89 | Batch_idx: 390 |  Loss_1: (0.0183) | Acc_1: (99.41%) (49706/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4891) | Acc: (91.26%) (9126/10000)\n",
      "Epoch: 90 | Batch_idx: 0 |  Loss_1: (0.0141) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 90 | Batch_idx: 10 |  Loss_1: (0.0223) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 90 | Batch_idx: 20 |  Loss_1: (0.0222) | Acc_1: (99.18%) (2666/2688)\n",
      "Epoch: 90 | Batch_idx: 30 |  Loss_1: (0.0208) | Acc_1: (99.22%) (3937/3968)\n",
      "Epoch: 90 | Batch_idx: 40 |  Loss_1: (0.0190) | Acc_1: (99.28%) (5210/5248)\n",
      "Epoch: 90 | Batch_idx: 50 |  Loss_1: (0.0186) | Acc_1: (99.31%) (6483/6528)\n",
      "Epoch: 90 | Batch_idx: 60 |  Loss_1: (0.0196) | Acc_1: (99.28%) (7752/7808)\n",
      "Epoch: 90 | Batch_idx: 70 |  Loss_1: (0.0192) | Acc_1: (99.30%) (9024/9088)\n",
      "Epoch: 90 | Batch_idx: 80 |  Loss_1: (0.0186) | Acc_1: (99.32%) (10297/10368)\n",
      "Epoch: 90 | Batch_idx: 90 |  Loss_1: (0.0188) | Acc_1: (99.31%) (11568/11648)\n",
      "Epoch: 90 | Batch_idx: 100 |  Loss_1: (0.0184) | Acc_1: (99.33%) (12841/12928)\n",
      "Epoch: 90 | Batch_idx: 110 |  Loss_1: (0.0183) | Acc_1: (99.32%) (14112/14208)\n",
      "Epoch: 90 | Batch_idx: 120 |  Loss_1: (0.0188) | Acc_1: (99.31%) (15381/15488)\n",
      "Epoch: 90 | Batch_idx: 130 |  Loss_1: (0.0189) | Acc_1: (99.33%) (16655/16768)\n",
      "Epoch: 90 | Batch_idx: 140 |  Loss_1: (0.0188) | Acc_1: (99.33%) (17927/18048)\n",
      "Epoch: 90 | Batch_idx: 150 |  Loss_1: (0.0194) | Acc_1: (99.32%) (19196/19328)\n",
      "Epoch: 90 | Batch_idx: 160 |  Loss_1: (0.0193) | Acc_1: (99.32%) (20468/20608)\n",
      "Epoch: 90 | Batch_idx: 170 |  Loss_1: (0.0192) | Acc_1: (99.32%) (21739/21888)\n",
      "Epoch: 90 | Batch_idx: 180 |  Loss_1: (0.0194) | Acc_1: (99.33%) (23012/23168)\n",
      "Epoch: 90 | Batch_idx: 190 |  Loss_1: (0.0196) | Acc_1: (99.32%) (24282/24448)\n",
      "Epoch: 90 | Batch_idx: 200 |  Loss_1: (0.0198) | Acc_1: (99.30%) (25549/25728)\n",
      "Epoch: 90 | Batch_idx: 210 |  Loss_1: (0.0193) | Acc_1: (99.32%) (26825/27008)\n",
      "Epoch: 90 | Batch_idx: 220 |  Loss_1: (0.0189) | Acc_1: (99.34%) (28100/28288)\n",
      "Epoch: 90 | Batch_idx: 230 |  Loss_1: (0.0191) | Acc_1: (99.33%) (29369/29568)\n",
      "Epoch: 90 | Batch_idx: 240 |  Loss_1: (0.0193) | Acc_1: (99.31%) (30636/30848)\n",
      "Epoch: 90 | Batch_idx: 250 |  Loss_1: (0.0194) | Acc_1: (99.31%) (31907/32128)\n",
      "Epoch: 90 | Batch_idx: 260 |  Loss_1: (0.0199) | Acc_1: (99.30%) (33173/33408)\n",
      "Epoch: 90 | Batch_idx: 270 |  Loss_1: (0.0200) | Acc_1: (99.29%) (34441/34688)\n",
      "Epoch: 90 | Batch_idx: 280 |  Loss_1: (0.0202) | Acc_1: (99.28%) (35708/35968)\n",
      "Epoch: 90 | Batch_idx: 290 |  Loss_1: (0.0206) | Acc_1: (99.27%) (36977/37248)\n",
      "Epoch: 90 | Batch_idx: 300 |  Loss_1: (0.0209) | Acc_1: (99.26%) (38242/38528)\n",
      "Epoch: 90 | Batch_idx: 310 |  Loss_1: (0.0212) | Acc_1: (99.25%) (39511/39808)\n",
      "Epoch: 90 | Batch_idx: 320 |  Loss_1: (0.0213) | Acc_1: (99.26%) (40783/41088)\n",
      "Epoch: 90 | Batch_idx: 330 |  Loss_1: (0.0211) | Acc_1: (99.26%) (42053/42368)\n",
      "Epoch: 90 | Batch_idx: 340 |  Loss_1: (0.0217) | Acc_1: (99.24%) (43318/43648)\n",
      "Epoch: 90 | Batch_idx: 350 |  Loss_1: (0.0217) | Acc_1: (99.25%) (44589/44928)\n",
      "Epoch: 90 | Batch_idx: 360 |  Loss_1: (0.0216) | Acc_1: (99.26%) (45864/46208)\n",
      "Epoch: 90 | Batch_idx: 370 |  Loss_1: (0.0216) | Acc_1: (99.25%) (47133/47488)\n",
      "Epoch: 90 | Batch_idx: 380 |  Loss_1: (0.0213) | Acc_1: (99.26%) (48407/48768)\n",
      "Epoch: 90 | Batch_idx: 390 |  Loss_1: (0.0211) | Acc_1: (99.26%) (49632/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4559) | Acc: (91.83%) (9183/10000)\n",
      "Epoch: 91 | Batch_idx: 0 |  Loss_1: (0.0059) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 91 | Batch_idx: 10 |  Loss_1: (0.0156) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 91 | Batch_idx: 20 |  Loss_1: (0.0223) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 91 | Batch_idx: 30 |  Loss_1: (0.0218) | Acc_1: (99.34%) (3942/3968)\n",
      "Epoch: 91 | Batch_idx: 40 |  Loss_1: (0.0217) | Acc_1: (99.37%) (5215/5248)\n",
      "Epoch: 91 | Batch_idx: 50 |  Loss_1: (0.0221) | Acc_1: (99.33%) (6484/6528)\n",
      "Epoch: 91 | Batch_idx: 60 |  Loss_1: (0.0231) | Acc_1: (99.23%) (7748/7808)\n",
      "Epoch: 91 | Batch_idx: 70 |  Loss_1: (0.0229) | Acc_1: (99.24%) (9019/9088)\n",
      "Epoch: 91 | Batch_idx: 80 |  Loss_1: (0.0233) | Acc_1: (99.21%) (10286/10368)\n",
      "Epoch: 91 | Batch_idx: 90 |  Loss_1: (0.0246) | Acc_1: (99.14%) (11548/11648)\n",
      "Epoch: 91 | Batch_idx: 100 |  Loss_1: (0.0240) | Acc_1: (99.18%) (12822/12928)\n",
      "Epoch: 91 | Batch_idx: 110 |  Loss_1: (0.0251) | Acc_1: (99.12%) (14083/14208)\n",
      "Epoch: 91 | Batch_idx: 120 |  Loss_1: (0.0247) | Acc_1: (99.14%) (15355/15488)\n",
      "Epoch: 91 | Batch_idx: 130 |  Loss_1: (0.0244) | Acc_1: (99.14%) (16623/16768)\n",
      "Epoch: 91 | Batch_idx: 140 |  Loss_1: (0.0237) | Acc_1: (99.16%) (17897/18048)\n",
      "Epoch: 91 | Batch_idx: 150 |  Loss_1: (0.0241) | Acc_1: (99.15%) (19163/19328)\n",
      "Epoch: 91 | Batch_idx: 160 |  Loss_1: (0.0237) | Acc_1: (99.15%) (20433/20608)\n",
      "Epoch: 91 | Batch_idx: 170 |  Loss_1: (0.0242) | Acc_1: (99.13%) (21697/21888)\n",
      "Epoch: 91 | Batch_idx: 180 |  Loss_1: (0.0244) | Acc_1: (99.12%) (22965/23168)\n",
      "Epoch: 91 | Batch_idx: 190 |  Loss_1: (0.0247) | Acc_1: (99.12%) (24233/24448)\n",
      "Epoch: 91 | Batch_idx: 200 |  Loss_1: (0.0246) | Acc_1: (99.13%) (25504/25728)\n",
      "Epoch: 91 | Batch_idx: 210 |  Loss_1: (0.0241) | Acc_1: (99.13%) (26774/27008)\n",
      "Epoch: 91 | Batch_idx: 220 |  Loss_1: (0.0241) | Acc_1: (99.13%) (28043/28288)\n",
      "Epoch: 91 | Batch_idx: 230 |  Loss_1: (0.0236) | Acc_1: (99.14%) (29315/29568)\n",
      "Epoch: 91 | Batch_idx: 240 |  Loss_1: (0.0240) | Acc_1: (99.13%) (30579/30848)\n",
      "Epoch: 91 | Batch_idx: 250 |  Loss_1: (0.0243) | Acc_1: (99.13%) (31847/32128)\n",
      "Epoch: 91 | Batch_idx: 260 |  Loss_1: (0.0239) | Acc_1: (99.14%) (33121/33408)\n",
      "Epoch: 91 | Batch_idx: 270 |  Loss_1: (0.0238) | Acc_1: (99.14%) (34391/34688)\n",
      "Epoch: 91 | Batch_idx: 280 |  Loss_1: (0.0237) | Acc_1: (99.15%) (35661/35968)\n",
      "Epoch: 91 | Batch_idx: 290 |  Loss_1: (0.0234) | Acc_1: (99.17%) (36937/37248)\n",
      "Epoch: 91 | Batch_idx: 300 |  Loss_1: (0.0231) | Acc_1: (99.17%) (38208/38528)\n",
      "Epoch: 91 | Batch_idx: 310 |  Loss_1: (0.0231) | Acc_1: (99.17%) (39477/39808)\n",
      "Epoch: 91 | Batch_idx: 320 |  Loss_1: (0.0228) | Acc_1: (99.18%) (40751/41088)\n",
      "Epoch: 91 | Batch_idx: 330 |  Loss_1: (0.0225) | Acc_1: (99.20%) (42027/42368)\n",
      "Epoch: 91 | Batch_idx: 340 |  Loss_1: (0.0224) | Acc_1: (99.20%) (43298/43648)\n",
      "Epoch: 91 | Batch_idx: 350 |  Loss_1: (0.0222) | Acc_1: (99.21%) (44572/44928)\n",
      "Epoch: 91 | Batch_idx: 360 |  Loss_1: (0.0221) | Acc_1: (99.21%) (45844/46208)\n",
      "Epoch: 91 | Batch_idx: 370 |  Loss_1: (0.0218) | Acc_1: (99.23%) (47120/47488)\n",
      "Epoch: 91 | Batch_idx: 380 |  Loss_1: (0.0217) | Acc_1: (99.24%) (48395/48768)\n",
      "Epoch: 91 | Batch_idx: 390 |  Loss_1: (0.0216) | Acc_1: (99.24%) (49620/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4560) | Acc: (91.63%) (9163/10000)\n",
      "Epoch: 92 | Batch_idx: 0 |  Loss_1: (0.0078) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 92 | Batch_idx: 10 |  Loss_1: (0.0101) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 92 | Batch_idx: 20 |  Loss_1: (0.0137) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 92 | Batch_idx: 30 |  Loss_1: (0.0135) | Acc_1: (99.42%) (3945/3968)\n",
      "Epoch: 92 | Batch_idx: 40 |  Loss_1: (0.0131) | Acc_1: (99.50%) (5222/5248)\n",
      "Epoch: 92 | Batch_idx: 50 |  Loss_1: (0.0131) | Acc_1: (99.51%) (6496/6528)\n",
      "Epoch: 92 | Batch_idx: 60 |  Loss_1: (0.0125) | Acc_1: (99.51%) (7770/7808)\n",
      "Epoch: 92 | Batch_idx: 70 |  Loss_1: (0.0123) | Acc_1: (99.52%) (9044/9088)\n",
      "Epoch: 92 | Batch_idx: 80 |  Loss_1: (0.0128) | Acc_1: (99.50%) (10316/10368)\n",
      "Epoch: 92 | Batch_idx: 90 |  Loss_1: (0.0129) | Acc_1: (99.51%) (11591/11648)\n",
      "Epoch: 92 | Batch_idx: 100 |  Loss_1: (0.0131) | Acc_1: (99.49%) (12862/12928)\n",
      "Epoch: 92 | Batch_idx: 110 |  Loss_1: (0.0145) | Acc_1: (99.46%) (14131/14208)\n",
      "Epoch: 92 | Batch_idx: 120 |  Loss_1: (0.0150) | Acc_1: (99.45%) (15403/15488)\n",
      "Epoch: 92 | Batch_idx: 130 |  Loss_1: (0.0154) | Acc_1: (99.43%) (16673/16768)\n",
      "Epoch: 92 | Batch_idx: 140 |  Loss_1: (0.0161) | Acc_1: (99.42%) (17943/18048)\n",
      "Epoch: 92 | Batch_idx: 150 |  Loss_1: (0.0169) | Acc_1: (99.39%) (19211/19328)\n",
      "Epoch: 92 | Batch_idx: 160 |  Loss_1: (0.0170) | Acc_1: (99.40%) (20484/20608)\n",
      "Epoch: 92 | Batch_idx: 170 |  Loss_1: (0.0172) | Acc_1: (99.38%) (21753/21888)\n",
      "Epoch: 92 | Batch_idx: 180 |  Loss_1: (0.0173) | Acc_1: (99.38%) (23024/23168)\n",
      "Epoch: 92 | Batch_idx: 190 |  Loss_1: (0.0173) | Acc_1: (99.37%) (24295/24448)\n",
      "Epoch: 92 | Batch_idx: 200 |  Loss_1: (0.0177) | Acc_1: (99.36%) (25563/25728)\n",
      "Epoch: 92 | Batch_idx: 210 |  Loss_1: (0.0185) | Acc_1: (99.33%) (26827/27008)\n",
      "Epoch: 92 | Batch_idx: 220 |  Loss_1: (0.0186) | Acc_1: (99.31%) (28094/28288)\n",
      "Epoch: 92 | Batch_idx: 230 |  Loss_1: (0.0190) | Acc_1: (99.30%) (29360/29568)\n",
      "Epoch: 92 | Batch_idx: 240 |  Loss_1: (0.0189) | Acc_1: (99.30%) (30631/30848)\n",
      "Epoch: 92 | Batch_idx: 250 |  Loss_1: (0.0188) | Acc_1: (99.29%) (31901/32128)\n",
      "Epoch: 92 | Batch_idx: 260 |  Loss_1: (0.0191) | Acc_1: (99.29%) (33170/33408)\n",
      "Epoch: 92 | Batch_idx: 270 |  Loss_1: (0.0193) | Acc_1: (99.29%) (34440/34688)\n",
      "Epoch: 92 | Batch_idx: 280 |  Loss_1: (0.0191) | Acc_1: (99.29%) (35711/35968)\n",
      "Epoch: 92 | Batch_idx: 290 |  Loss_1: (0.0194) | Acc_1: (99.28%) (36979/37248)\n",
      "Epoch: 92 | Batch_idx: 300 |  Loss_1: (0.0193) | Acc_1: (99.28%) (38252/38528)\n",
      "Epoch: 92 | Batch_idx: 310 |  Loss_1: (0.0195) | Acc_1: (99.28%) (39522/39808)\n",
      "Epoch: 92 | Batch_idx: 320 |  Loss_1: (0.0195) | Acc_1: (99.27%) (40790/41088)\n",
      "Epoch: 92 | Batch_idx: 330 |  Loss_1: (0.0195) | Acc_1: (99.28%) (42063/42368)\n",
      "Epoch: 92 | Batch_idx: 340 |  Loss_1: (0.0197) | Acc_1: (99.28%) (43333/43648)\n",
      "Epoch: 92 | Batch_idx: 350 |  Loss_1: (0.0199) | Acc_1: (99.27%) (44600/44928)\n",
      "Epoch: 92 | Batch_idx: 360 |  Loss_1: (0.0202) | Acc_1: (99.26%) (45867/46208)\n",
      "Epoch: 92 | Batch_idx: 370 |  Loss_1: (0.0202) | Acc_1: (99.26%) (47137/47488)\n",
      "Epoch: 92 | Batch_idx: 380 |  Loss_1: (0.0206) | Acc_1: (99.25%) (48403/48768)\n",
      "Epoch: 92 | Batch_idx: 390 |  Loss_1: (0.0204) | Acc_1: (99.26%) (49630/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4588) | Acc: (91.46%) (9146/10000)\n",
      "Epoch: 93 | Batch_idx: 0 |  Loss_1: (0.0190) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 93 | Batch_idx: 10 |  Loss_1: (0.0204) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 93 | Batch_idx: 20 |  Loss_1: (0.0165) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 93 | Batch_idx: 30 |  Loss_1: (0.0182) | Acc_1: (99.45%) (3946/3968)\n",
      "Epoch: 93 | Batch_idx: 40 |  Loss_1: (0.0172) | Acc_1: (99.50%) (5222/5248)\n",
      "Epoch: 93 | Batch_idx: 50 |  Loss_1: (0.0159) | Acc_1: (99.54%) (6498/6528)\n",
      "Epoch: 93 | Batch_idx: 60 |  Loss_1: (0.0170) | Acc_1: (99.53%) (7771/7808)\n",
      "Epoch: 93 | Batch_idx: 70 |  Loss_1: (0.0158) | Acc_1: (99.55%) (9047/9088)\n",
      "Epoch: 93 | Batch_idx: 80 |  Loss_1: (0.0165) | Acc_1: (99.52%) (10318/10368)\n",
      "Epoch: 93 | Batch_idx: 90 |  Loss_1: (0.0173) | Acc_1: (99.48%) (11588/11648)\n",
      "Epoch: 93 | Batch_idx: 100 |  Loss_1: (0.0165) | Acc_1: (99.50%) (12864/12928)\n",
      "Epoch: 93 | Batch_idx: 110 |  Loss_1: (0.0179) | Acc_1: (99.47%) (14132/14208)\n",
      "Epoch: 93 | Batch_idx: 120 |  Loss_1: (0.0184) | Acc_1: (99.45%) (15403/15488)\n",
      "Epoch: 93 | Batch_idx: 130 |  Loss_1: (0.0186) | Acc_1: (99.43%) (16673/16768)\n",
      "Epoch: 93 | Batch_idx: 140 |  Loss_1: (0.0181) | Acc_1: (99.44%) (17947/18048)\n",
      "Epoch: 93 | Batch_idx: 150 |  Loss_1: (0.0183) | Acc_1: (99.45%) (19221/19328)\n",
      "Epoch: 93 | Batch_idx: 160 |  Loss_1: (0.0185) | Acc_1: (99.44%) (20492/20608)\n",
      "Epoch: 93 | Batch_idx: 170 |  Loss_1: (0.0185) | Acc_1: (99.43%) (21764/21888)\n",
      "Epoch: 93 | Batch_idx: 180 |  Loss_1: (0.0185) | Acc_1: (99.43%) (23036/23168)\n",
      "Epoch: 93 | Batch_idx: 190 |  Loss_1: (0.0185) | Acc_1: (99.43%) (24308/24448)\n",
      "Epoch: 93 | Batch_idx: 200 |  Loss_1: (0.0184) | Acc_1: (99.44%) (25583/25728)\n",
      "Epoch: 93 | Batch_idx: 210 |  Loss_1: (0.0189) | Acc_1: (99.44%) (26856/27008)\n",
      "Epoch: 93 | Batch_idx: 220 |  Loss_1: (0.0185) | Acc_1: (99.44%) (28130/28288)\n",
      "Epoch: 93 | Batch_idx: 230 |  Loss_1: (0.0182) | Acc_1: (99.45%) (29405/29568)\n",
      "Epoch: 93 | Batch_idx: 240 |  Loss_1: (0.0180) | Acc_1: (99.44%) (30676/30848)\n",
      "Epoch: 93 | Batch_idx: 250 |  Loss_1: (0.0178) | Acc_1: (99.45%) (31950/32128)\n",
      "Epoch: 93 | Batch_idx: 260 |  Loss_1: (0.0179) | Acc_1: (99.45%) (33223/33408)\n",
      "Epoch: 93 | Batch_idx: 270 |  Loss_1: (0.0178) | Acc_1: (99.45%) (34496/34688)\n",
      "Epoch: 93 | Batch_idx: 280 |  Loss_1: (0.0181) | Acc_1: (99.44%) (35766/35968)\n",
      "Epoch: 93 | Batch_idx: 290 |  Loss_1: (0.0183) | Acc_1: (99.43%) (37036/37248)\n",
      "Epoch: 93 | Batch_idx: 300 |  Loss_1: (0.0182) | Acc_1: (99.43%) (38308/38528)\n",
      "Epoch: 93 | Batch_idx: 310 |  Loss_1: (0.0181) | Acc_1: (99.43%) (39582/39808)\n",
      "Epoch: 93 | Batch_idx: 320 |  Loss_1: (0.0180) | Acc_1: (99.43%) (40854/41088)\n",
      "Epoch: 93 | Batch_idx: 330 |  Loss_1: (0.0178) | Acc_1: (99.44%) (42131/42368)\n",
      "Epoch: 93 | Batch_idx: 340 |  Loss_1: (0.0178) | Acc_1: (99.44%) (43404/43648)\n",
      "Epoch: 93 | Batch_idx: 350 |  Loss_1: (0.0177) | Acc_1: (99.44%) (44676/44928)\n",
      "Epoch: 93 | Batch_idx: 360 |  Loss_1: (0.0178) | Acc_1: (99.44%) (45948/46208)\n",
      "Epoch: 93 | Batch_idx: 370 |  Loss_1: (0.0176) | Acc_1: (99.44%) (47223/47488)\n",
      "Epoch: 93 | Batch_idx: 380 |  Loss_1: (0.0176) | Acc_1: (99.44%) (48493/48768)\n",
      "Epoch: 93 | Batch_idx: 390 |  Loss_1: (0.0174) | Acc_1: (99.44%) (49720/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4781) | Acc: (91.54%) (9154/10000)\n",
      "Epoch: 94 | Batch_idx: 0 |  Loss_1: (0.0363) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 94 | Batch_idx: 10 |  Loss_1: (0.0263) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 94 | Batch_idx: 20 |  Loss_1: (0.0225) | Acc_1: (99.29%) (2669/2688)\n",
      "Epoch: 94 | Batch_idx: 30 |  Loss_1: (0.0234) | Acc_1: (99.19%) (3936/3968)\n",
      "Epoch: 94 | Batch_idx: 40 |  Loss_1: (0.0213) | Acc_1: (99.26%) (5209/5248)\n",
      "Epoch: 94 | Batch_idx: 50 |  Loss_1: (0.0199) | Acc_1: (99.31%) (6483/6528)\n",
      "Epoch: 94 | Batch_idx: 60 |  Loss_1: (0.0208) | Acc_1: (99.28%) (7752/7808)\n",
      "Epoch: 94 | Batch_idx: 70 |  Loss_1: (0.0206) | Acc_1: (99.30%) (9024/9088)\n",
      "Epoch: 94 | Batch_idx: 80 |  Loss_1: (0.0204) | Acc_1: (99.31%) (10296/10368)\n",
      "Epoch: 94 | Batch_idx: 90 |  Loss_1: (0.0197) | Acc_1: (99.32%) (11569/11648)\n",
      "Epoch: 94 | Batch_idx: 100 |  Loss_1: (0.0191) | Acc_1: (99.34%) (12843/12928)\n",
      "Epoch: 94 | Batch_idx: 110 |  Loss_1: (0.0184) | Acc_1: (99.36%) (14117/14208)\n",
      "Epoch: 94 | Batch_idx: 120 |  Loss_1: (0.0182) | Acc_1: (99.35%) (15387/15488)\n",
      "Epoch: 94 | Batch_idx: 130 |  Loss_1: (0.0180) | Acc_1: (99.37%) (16662/16768)\n",
      "Epoch: 94 | Batch_idx: 140 |  Loss_1: (0.0186) | Acc_1: (99.34%) (17929/18048)\n",
      "Epoch: 94 | Batch_idx: 150 |  Loss_1: (0.0188) | Acc_1: (99.34%) (19201/19328)\n",
      "Epoch: 94 | Batch_idx: 160 |  Loss_1: (0.0183) | Acc_1: (99.36%) (20476/20608)\n",
      "Epoch: 94 | Batch_idx: 170 |  Loss_1: (0.0185) | Acc_1: (99.36%) (21748/21888)\n",
      "Epoch: 94 | Batch_idx: 180 |  Loss_1: (0.0191) | Acc_1: (99.33%) (23013/23168)\n",
      "Epoch: 94 | Batch_idx: 190 |  Loss_1: (0.0195) | Acc_1: (99.31%) (24279/24448)\n",
      "Epoch: 94 | Batch_idx: 200 |  Loss_1: (0.0193) | Acc_1: (99.31%) (25551/25728)\n",
      "Epoch: 94 | Batch_idx: 210 |  Loss_1: (0.0189) | Acc_1: (99.33%) (26826/27008)\n",
      "Epoch: 94 | Batch_idx: 220 |  Loss_1: (0.0188) | Acc_1: (99.32%) (28096/28288)\n",
      "Epoch: 94 | Batch_idx: 230 |  Loss_1: (0.0190) | Acc_1: (99.32%) (29368/29568)\n",
      "Epoch: 94 | Batch_idx: 240 |  Loss_1: (0.0185) | Acc_1: (99.34%) (30644/30848)\n",
      "Epoch: 94 | Batch_idx: 250 |  Loss_1: (0.0187) | Acc_1: (99.33%) (31914/32128)\n",
      "Epoch: 94 | Batch_idx: 260 |  Loss_1: (0.0190) | Acc_1: (99.32%) (33181/33408)\n",
      "Epoch: 94 | Batch_idx: 270 |  Loss_1: (0.0188) | Acc_1: (99.32%) (34452/34688)\n",
      "Epoch: 94 | Batch_idx: 280 |  Loss_1: (0.0189) | Acc_1: (99.32%) (35723/35968)\n",
      "Epoch: 94 | Batch_idx: 290 |  Loss_1: (0.0187) | Acc_1: (99.33%) (36997/37248)\n",
      "Epoch: 94 | Batch_idx: 300 |  Loss_1: (0.0187) | Acc_1: (99.32%) (38267/38528)\n",
      "Epoch: 94 | Batch_idx: 310 |  Loss_1: (0.0185) | Acc_1: (99.33%) (39542/39808)\n",
      "Epoch: 94 | Batch_idx: 320 |  Loss_1: (0.0184) | Acc_1: (99.34%) (40815/41088)\n",
      "Epoch: 94 | Batch_idx: 330 |  Loss_1: (0.0181) | Acc_1: (99.35%) (42093/42368)\n",
      "Epoch: 94 | Batch_idx: 340 |  Loss_1: (0.0178) | Acc_1: (99.37%) (43371/43648)\n",
      "Epoch: 94 | Batch_idx: 350 |  Loss_1: (0.0175) | Acc_1: (99.37%) (44647/44928)\n",
      "Epoch: 94 | Batch_idx: 360 |  Loss_1: (0.0175) | Acc_1: (99.38%) (45922/46208)\n",
      "Epoch: 94 | Batch_idx: 370 |  Loss_1: (0.0177) | Acc_1: (99.37%) (47191/47488)\n",
      "Epoch: 94 | Batch_idx: 380 |  Loss_1: (0.0177) | Acc_1: (99.38%) (48465/48768)\n",
      "Epoch: 94 | Batch_idx: 390 |  Loss_1: (0.0176) | Acc_1: (99.38%) (49689/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4633) | Acc: (91.80%) (9180/10000)\n",
      "Epoch: 95 | Batch_idx: 0 |  Loss_1: (0.0088) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 95 | Batch_idx: 10 |  Loss_1: (0.0136) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 95 | Batch_idx: 20 |  Loss_1: (0.0123) | Acc_1: (99.52%) (2675/2688)\n",
      "Epoch: 95 | Batch_idx: 30 |  Loss_1: (0.0129) | Acc_1: (99.50%) (3948/3968)\n",
      "Epoch: 95 | Batch_idx: 40 |  Loss_1: (0.0119) | Acc_1: (99.54%) (5224/5248)\n",
      "Epoch: 95 | Batch_idx: 50 |  Loss_1: (0.0118) | Acc_1: (99.53%) (6497/6528)\n",
      "Epoch: 95 | Batch_idx: 60 |  Loss_1: (0.0114) | Acc_1: (99.55%) (7773/7808)\n",
      "Epoch: 95 | Batch_idx: 70 |  Loss_1: (0.0108) | Acc_1: (99.57%) (9049/9088)\n",
      "Epoch: 95 | Batch_idx: 80 |  Loss_1: (0.0108) | Acc_1: (99.59%) (10325/10368)\n",
      "Epoch: 95 | Batch_idx: 90 |  Loss_1: (0.0110) | Acc_1: (99.59%) (11600/11648)\n",
      "Epoch: 95 | Batch_idx: 100 |  Loss_1: (0.0113) | Acc_1: (99.57%) (12872/12928)\n",
      "Epoch: 95 | Batch_idx: 110 |  Loss_1: (0.0118) | Acc_1: (99.56%) (14145/14208)\n",
      "Epoch: 95 | Batch_idx: 120 |  Loss_1: (0.0124) | Acc_1: (99.54%) (15417/15488)\n",
      "Epoch: 95 | Batch_idx: 130 |  Loss_1: (0.0122) | Acc_1: (99.55%) (16693/16768)\n",
      "Epoch: 95 | Batch_idx: 140 |  Loss_1: (0.0127) | Acc_1: (99.54%) (17965/18048)\n",
      "Epoch: 95 | Batch_idx: 150 |  Loss_1: (0.0128) | Acc_1: (99.53%) (19238/19328)\n",
      "Epoch: 95 | Batch_idx: 160 |  Loss_1: (0.0133) | Acc_1: (99.51%) (20506/20608)\n",
      "Epoch: 95 | Batch_idx: 170 |  Loss_1: (0.0137) | Acc_1: (99.51%) (21781/21888)\n",
      "Epoch: 95 | Batch_idx: 180 |  Loss_1: (0.0137) | Acc_1: (99.50%) (23052/23168)\n",
      "Epoch: 95 | Batch_idx: 190 |  Loss_1: (0.0140) | Acc_1: (99.50%) (24325/24448)\n",
      "Epoch: 95 | Batch_idx: 200 |  Loss_1: (0.0138) | Acc_1: (99.51%) (25601/25728)\n",
      "Epoch: 95 | Batch_idx: 210 |  Loss_1: (0.0138) | Acc_1: (99.51%) (26876/27008)\n",
      "Epoch: 95 | Batch_idx: 220 |  Loss_1: (0.0140) | Acc_1: (99.51%) (28148/28288)\n",
      "Epoch: 95 | Batch_idx: 230 |  Loss_1: (0.0144) | Acc_1: (99.49%) (29417/29568)\n",
      "Epoch: 95 | Batch_idx: 240 |  Loss_1: (0.0145) | Acc_1: (99.48%) (30689/30848)\n",
      "Epoch: 95 | Batch_idx: 250 |  Loss_1: (0.0147) | Acc_1: (99.48%) (31960/32128)\n",
      "Epoch: 95 | Batch_idx: 260 |  Loss_1: (0.0151) | Acc_1: (99.47%) (33231/33408)\n",
      "Epoch: 95 | Batch_idx: 270 |  Loss_1: (0.0149) | Acc_1: (99.48%) (34508/34688)\n",
      "Epoch: 95 | Batch_idx: 280 |  Loss_1: (0.0151) | Acc_1: (99.47%) (35778/35968)\n",
      "Epoch: 95 | Batch_idx: 290 |  Loss_1: (0.0152) | Acc_1: (99.47%) (37051/37248)\n",
      "Epoch: 95 | Batch_idx: 300 |  Loss_1: (0.0153) | Acc_1: (99.47%) (38322/38528)\n",
      "Epoch: 95 | Batch_idx: 310 |  Loss_1: (0.0154) | Acc_1: (99.46%) (39595/39808)\n",
      "Epoch: 95 | Batch_idx: 320 |  Loss_1: (0.0157) | Acc_1: (99.45%) (40862/41088)\n",
      "Epoch: 95 | Batch_idx: 330 |  Loss_1: (0.0160) | Acc_1: (99.45%) (42134/42368)\n",
      "Epoch: 95 | Batch_idx: 340 |  Loss_1: (0.0162) | Acc_1: (99.44%) (43405/43648)\n",
      "Epoch: 95 | Batch_idx: 350 |  Loss_1: (0.0166) | Acc_1: (99.43%) (44670/44928)\n",
      "Epoch: 95 | Batch_idx: 360 |  Loss_1: (0.0170) | Acc_1: (99.41%) (45934/46208)\n",
      "Epoch: 95 | Batch_idx: 370 |  Loss_1: (0.0172) | Acc_1: (99.41%) (47206/47488)\n",
      "Epoch: 95 | Batch_idx: 380 |  Loss_1: (0.0171) | Acc_1: (99.41%) (48482/48768)\n",
      "Epoch: 95 | Batch_idx: 390 |  Loss_1: (0.0172) | Acc_1: (99.41%) (49705/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4190) | Acc: (91.91%) (9191/10000)\n",
      "Epoch: 96 | Batch_idx: 0 |  Loss_1: (0.0311) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 96 | Batch_idx: 10 |  Loss_1: (0.0133) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 96 | Batch_idx: 20 |  Loss_1: (0.0140) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 96 | Batch_idx: 30 |  Loss_1: (0.0183) | Acc_1: (99.34%) (3942/3968)\n",
      "Epoch: 96 | Batch_idx: 40 |  Loss_1: (0.0188) | Acc_1: (99.31%) (5212/5248)\n",
      "Epoch: 96 | Batch_idx: 50 |  Loss_1: (0.0183) | Acc_1: (99.33%) (6484/6528)\n",
      "Epoch: 96 | Batch_idx: 60 |  Loss_1: (0.0187) | Acc_1: (99.33%) (7756/7808)\n",
      "Epoch: 96 | Batch_idx: 70 |  Loss_1: (0.0188) | Acc_1: (99.34%) (9028/9088)\n",
      "Epoch: 96 | Batch_idx: 80 |  Loss_1: (0.0184) | Acc_1: (99.35%) (10301/10368)\n",
      "Epoch: 96 | Batch_idx: 90 |  Loss_1: (0.0177) | Acc_1: (99.38%) (11576/11648)\n",
      "Epoch: 96 | Batch_idx: 100 |  Loss_1: (0.0171) | Acc_1: (99.40%) (12850/12928)\n",
      "Epoch: 96 | Batch_idx: 110 |  Loss_1: (0.0165) | Acc_1: (99.43%) (14127/14208)\n",
      "Epoch: 96 | Batch_idx: 120 |  Loss_1: (0.0169) | Acc_1: (99.41%) (15396/15488)\n",
      "Epoch: 96 | Batch_idx: 130 |  Loss_1: (0.0165) | Acc_1: (99.41%) (16669/16768)\n",
      "Epoch: 96 | Batch_idx: 140 |  Loss_1: (0.0168) | Acc_1: (99.40%) (17940/18048)\n",
      "Epoch: 96 | Batch_idx: 150 |  Loss_1: (0.0168) | Acc_1: (99.41%) (19214/19328)\n",
      "Epoch: 96 | Batch_idx: 160 |  Loss_1: (0.0166) | Acc_1: (99.41%) (20487/20608)\n",
      "Epoch: 96 | Batch_idx: 170 |  Loss_1: (0.0169) | Acc_1: (99.39%) (21755/21888)\n",
      "Epoch: 96 | Batch_idx: 180 |  Loss_1: (0.0168) | Acc_1: (99.40%) (23028/23168)\n",
      "Epoch: 96 | Batch_idx: 190 |  Loss_1: (0.0164) | Acc_1: (99.40%) (24302/24448)\n",
      "Epoch: 96 | Batch_idx: 200 |  Loss_1: (0.0160) | Acc_1: (99.42%) (25578/25728)\n",
      "Epoch: 96 | Batch_idx: 210 |  Loss_1: (0.0158) | Acc_1: (99.43%) (26854/27008)\n",
      "Epoch: 96 | Batch_idx: 220 |  Loss_1: (0.0157) | Acc_1: (99.43%) (28127/28288)\n",
      "Epoch: 96 | Batch_idx: 230 |  Loss_1: (0.0153) | Acc_1: (99.45%) (29405/29568)\n",
      "Epoch: 96 | Batch_idx: 240 |  Loss_1: (0.0152) | Acc_1: (99.46%) (30680/30848)\n",
      "Epoch: 96 | Batch_idx: 250 |  Loss_1: (0.0150) | Acc_1: (99.46%) (31954/32128)\n",
      "Epoch: 96 | Batch_idx: 260 |  Loss_1: (0.0148) | Acc_1: (99.46%) (33229/33408)\n",
      "Epoch: 96 | Batch_idx: 270 |  Loss_1: (0.0148) | Acc_1: (99.47%) (34503/34688)\n",
      "Epoch: 96 | Batch_idx: 280 |  Loss_1: (0.0149) | Acc_1: (99.47%) (35776/35968)\n",
      "Epoch: 96 | Batch_idx: 290 |  Loss_1: (0.0149) | Acc_1: (99.47%) (37049/37248)\n",
      "Epoch: 96 | Batch_idx: 300 |  Loss_1: (0.0146) | Acc_1: (99.48%) (38328/38528)\n",
      "Epoch: 96 | Batch_idx: 310 |  Loss_1: (0.0146) | Acc_1: (99.48%) (39602/39808)\n",
      "Epoch: 96 | Batch_idx: 320 |  Loss_1: (0.0147) | Acc_1: (99.48%) (40873/41088)\n",
      "Epoch: 96 | Batch_idx: 330 |  Loss_1: (0.0146) | Acc_1: (99.48%) (42147/42368)\n",
      "Epoch: 96 | Batch_idx: 340 |  Loss_1: (0.0146) | Acc_1: (99.48%) (43420/43648)\n",
      "Epoch: 96 | Batch_idx: 350 |  Loss_1: (0.0144) | Acc_1: (99.49%) (44697/44928)\n",
      "Epoch: 96 | Batch_idx: 360 |  Loss_1: (0.0142) | Acc_1: (99.49%) (45972/46208)\n",
      "Epoch: 96 | Batch_idx: 370 |  Loss_1: (0.0144) | Acc_1: (99.49%) (47244/47488)\n",
      "Epoch: 96 | Batch_idx: 380 |  Loss_1: (0.0143) | Acc_1: (99.49%) (48521/48768)\n",
      "Epoch: 96 | Batch_idx: 390 |  Loss_1: (0.0142) | Acc_1: (99.50%) (49749/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4477) | Acc: (92.12%) (9212/10000)\n",
      "Epoch: 97 | Batch_idx: 0 |  Loss_1: (0.0013) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 97 | Batch_idx: 10 |  Loss_1: (0.0088) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 97 | Batch_idx: 20 |  Loss_1: (0.0118) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 97 | Batch_idx: 30 |  Loss_1: (0.0133) | Acc_1: (99.55%) (3950/3968)\n",
      "Epoch: 97 | Batch_idx: 40 |  Loss_1: (0.0126) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 97 | Batch_idx: 50 |  Loss_1: (0.0125) | Acc_1: (99.57%) (6500/6528)\n",
      "Epoch: 97 | Batch_idx: 60 |  Loss_1: (0.0129) | Acc_1: (99.55%) (7773/7808)\n",
      "Epoch: 97 | Batch_idx: 70 |  Loss_1: (0.0129) | Acc_1: (99.54%) (9046/9088)\n",
      "Epoch: 97 | Batch_idx: 80 |  Loss_1: (0.0128) | Acc_1: (99.54%) (10320/10368)\n",
      "Epoch: 97 | Batch_idx: 90 |  Loss_1: (0.0123) | Acc_1: (99.56%) (11597/11648)\n",
      "Epoch: 97 | Batch_idx: 100 |  Loss_1: (0.0119) | Acc_1: (99.57%) (12873/12928)\n",
      "Epoch: 97 | Batch_idx: 110 |  Loss_1: (0.0121) | Acc_1: (99.56%) (14146/14208)\n",
      "Epoch: 97 | Batch_idx: 120 |  Loss_1: (0.0126) | Acc_1: (99.54%) (15416/15488)\n",
      "Epoch: 97 | Batch_idx: 130 |  Loss_1: (0.0126) | Acc_1: (99.55%) (16692/16768)\n",
      "Epoch: 97 | Batch_idx: 140 |  Loss_1: (0.0124) | Acc_1: (99.56%) (17968/18048)\n",
      "Epoch: 97 | Batch_idx: 150 |  Loss_1: (0.0122) | Acc_1: (99.56%) (19243/19328)\n",
      "Epoch: 97 | Batch_idx: 160 |  Loss_1: (0.0124) | Acc_1: (99.55%) (20516/20608)\n",
      "Epoch: 97 | Batch_idx: 170 |  Loss_1: (0.0121) | Acc_1: (99.57%) (21794/21888)\n",
      "Epoch: 97 | Batch_idx: 180 |  Loss_1: (0.0123) | Acc_1: (99.56%) (23065/23168)\n",
      "Epoch: 97 | Batch_idx: 190 |  Loss_1: (0.0119) | Acc_1: (99.57%) (24344/24448)\n",
      "Epoch: 97 | Batch_idx: 200 |  Loss_1: (0.0121) | Acc_1: (99.57%) (25618/25728)\n",
      "Epoch: 97 | Batch_idx: 210 |  Loss_1: (0.0122) | Acc_1: (99.57%) (26891/27008)\n",
      "Epoch: 97 | Batch_idx: 220 |  Loss_1: (0.0123) | Acc_1: (99.56%) (28164/28288)\n",
      "Epoch: 97 | Batch_idx: 230 |  Loss_1: (0.0123) | Acc_1: (99.57%) (29442/29568)\n",
      "Epoch: 97 | Batch_idx: 240 |  Loss_1: (0.0120) | Acc_1: (99.58%) (30718/30848)\n",
      "Epoch: 97 | Batch_idx: 250 |  Loss_1: (0.0121) | Acc_1: (99.57%) (31990/32128)\n",
      "Epoch: 97 | Batch_idx: 260 |  Loss_1: (0.0121) | Acc_1: (99.57%) (33264/33408)\n",
      "Epoch: 97 | Batch_idx: 270 |  Loss_1: (0.0121) | Acc_1: (99.57%) (34539/34688)\n",
      "Epoch: 97 | Batch_idx: 280 |  Loss_1: (0.0119) | Acc_1: (99.58%) (35816/35968)\n",
      "Epoch: 97 | Batch_idx: 290 |  Loss_1: (0.0119) | Acc_1: (99.58%) (37092/37248)\n",
      "Epoch: 97 | Batch_idx: 300 |  Loss_1: (0.0121) | Acc_1: (99.57%) (38364/38528)\n",
      "Epoch: 97 | Batch_idx: 310 |  Loss_1: (0.0124) | Acc_1: (99.55%) (39630/39808)\n",
      "Epoch: 97 | Batch_idx: 320 |  Loss_1: (0.0126) | Acc_1: (99.55%) (40902/41088)\n",
      "Epoch: 97 | Batch_idx: 330 |  Loss_1: (0.0128) | Acc_1: (99.54%) (42172/42368)\n",
      "Epoch: 97 | Batch_idx: 340 |  Loss_1: (0.0128) | Acc_1: (99.54%) (43447/43648)\n",
      "Epoch: 97 | Batch_idx: 350 |  Loss_1: (0.0130) | Acc_1: (99.54%) (44721/44928)\n",
      "Epoch: 97 | Batch_idx: 360 |  Loss_1: (0.0130) | Acc_1: (99.54%) (45994/46208)\n",
      "Epoch: 97 | Batch_idx: 370 |  Loss_1: (0.0130) | Acc_1: (99.53%) (47264/47488)\n",
      "Epoch: 97 | Batch_idx: 380 |  Loss_1: (0.0130) | Acc_1: (99.53%) (48538/48768)\n",
      "Epoch: 97 | Batch_idx: 390 |  Loss_1: (0.0129) | Acc_1: (99.53%) (49767/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4720) | Acc: (91.84%) (9184/10000)\n",
      "Epoch: 98 | Batch_idx: 0 |  Loss_1: (0.0086) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 98 | Batch_idx: 10 |  Loss_1: (0.0093) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 98 | Batch_idx: 20 |  Loss_1: (0.0117) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 98 | Batch_idx: 30 |  Loss_1: (0.0153) | Acc_1: (99.32%) (3941/3968)\n",
      "Epoch: 98 | Batch_idx: 40 |  Loss_1: (0.0157) | Acc_1: (99.33%) (5213/5248)\n",
      "Epoch: 98 | Batch_idx: 50 |  Loss_1: (0.0164) | Acc_1: (99.34%) (6485/6528)\n",
      "Epoch: 98 | Batch_idx: 60 |  Loss_1: (0.0161) | Acc_1: (99.37%) (7759/7808)\n",
      "Epoch: 98 | Batch_idx: 70 |  Loss_1: (0.0164) | Acc_1: (99.38%) (9032/9088)\n",
      "Epoch: 98 | Batch_idx: 80 |  Loss_1: (0.0163) | Acc_1: (99.42%) (10308/10368)\n",
      "Epoch: 98 | Batch_idx: 90 |  Loss_1: (0.0151) | Acc_1: (99.48%) (11587/11648)\n",
      "Epoch: 98 | Batch_idx: 100 |  Loss_1: (0.0149) | Acc_1: (99.48%) (12861/12928)\n",
      "Epoch: 98 | Batch_idx: 110 |  Loss_1: (0.0143) | Acc_1: (99.49%) (14135/14208)\n",
      "Epoch: 98 | Batch_idx: 120 |  Loss_1: (0.0144) | Acc_1: (99.48%) (15407/15488)\n",
      "Epoch: 98 | Batch_idx: 130 |  Loss_1: (0.0144) | Acc_1: (99.48%) (16680/16768)\n",
      "Epoch: 98 | Batch_idx: 140 |  Loss_1: (0.0145) | Acc_1: (99.47%) (17953/18048)\n",
      "Epoch: 98 | Batch_idx: 150 |  Loss_1: (0.0148) | Acc_1: (99.47%) (19226/19328)\n",
      "Epoch: 98 | Batch_idx: 160 |  Loss_1: (0.0143) | Acc_1: (99.50%) (20504/20608)\n",
      "Epoch: 98 | Batch_idx: 170 |  Loss_1: (0.0145) | Acc_1: (99.48%) (21775/21888)\n",
      "Epoch: 98 | Batch_idx: 180 |  Loss_1: (0.0141) | Acc_1: (99.51%) (23054/23168)\n",
      "Epoch: 98 | Batch_idx: 190 |  Loss_1: (0.0141) | Acc_1: (99.51%) (24327/24448)\n",
      "Epoch: 98 | Batch_idx: 200 |  Loss_1: (0.0145) | Acc_1: (99.48%) (25595/25728)\n",
      "Epoch: 98 | Batch_idx: 210 |  Loss_1: (0.0143) | Acc_1: (99.50%) (26872/27008)\n",
      "Epoch: 98 | Batch_idx: 220 |  Loss_1: (0.0141) | Acc_1: (99.51%) (28148/28288)\n",
      "Epoch: 98 | Batch_idx: 230 |  Loss_1: (0.0142) | Acc_1: (99.50%) (29419/29568)\n",
      "Epoch: 98 | Batch_idx: 240 |  Loss_1: (0.0140) | Acc_1: (99.50%) (30694/30848)\n",
      "Epoch: 98 | Batch_idx: 250 |  Loss_1: (0.0140) | Acc_1: (99.50%) (31968/32128)\n",
      "Epoch: 98 | Batch_idx: 260 |  Loss_1: (0.0138) | Acc_1: (99.51%) (33244/33408)\n",
      "Epoch: 98 | Batch_idx: 270 |  Loss_1: (0.0136) | Acc_1: (99.52%) (34520/34688)\n",
      "Epoch: 98 | Batch_idx: 280 |  Loss_1: (0.0134) | Acc_1: (99.53%) (35799/35968)\n",
      "Epoch: 98 | Batch_idx: 290 |  Loss_1: (0.0133) | Acc_1: (99.54%) (37077/37248)\n",
      "Epoch: 98 | Batch_idx: 300 |  Loss_1: (0.0131) | Acc_1: (99.54%) (38352/38528)\n",
      "Epoch: 98 | Batch_idx: 310 |  Loss_1: (0.0129) | Acc_1: (99.55%) (39627/39808)\n",
      "Epoch: 98 | Batch_idx: 320 |  Loss_1: (0.0131) | Acc_1: (99.54%) (40900/41088)\n",
      "Epoch: 98 | Batch_idx: 330 |  Loss_1: (0.0131) | Acc_1: (99.54%) (42174/42368)\n",
      "Epoch: 98 | Batch_idx: 340 |  Loss_1: (0.0132) | Acc_1: (99.54%) (43446/43648)\n",
      "Epoch: 98 | Batch_idx: 350 |  Loss_1: (0.0131) | Acc_1: (99.54%) (44721/44928)\n",
      "Epoch: 98 | Batch_idx: 360 |  Loss_1: (0.0129) | Acc_1: (99.54%) (45997/46208)\n",
      "Epoch: 98 | Batch_idx: 370 |  Loss_1: (0.0129) | Acc_1: (99.55%) (47272/47488)\n",
      "Epoch: 98 | Batch_idx: 380 |  Loss_1: (0.0128) | Acc_1: (99.55%) (48548/48768)\n",
      "Epoch: 98 | Batch_idx: 390 |  Loss_1: (0.0128) | Acc_1: (99.55%) (49775/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5047) | Acc: (91.82%) (9182/10000)\n",
      "Epoch: 99 | Batch_idx: 0 |  Loss_1: (0.0058) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 99 | Batch_idx: 10 |  Loss_1: (0.0098) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 99 | Batch_idx: 20 |  Loss_1: (0.0125) | Acc_1: (99.52%) (2675/2688)\n",
      "Epoch: 99 | Batch_idx: 30 |  Loss_1: (0.0122) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 99 | Batch_idx: 40 |  Loss_1: (0.0114) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 99 | Batch_idx: 50 |  Loss_1: (0.0108) | Acc_1: (99.60%) (6502/6528)\n",
      "Epoch: 99 | Batch_idx: 60 |  Loss_1: (0.0108) | Acc_1: (99.58%) (7775/7808)\n",
      "Epoch: 99 | Batch_idx: 70 |  Loss_1: (0.0112) | Acc_1: (99.53%) (9045/9088)\n",
      "Epoch: 99 | Batch_idx: 80 |  Loss_1: (0.0119) | Acc_1: (99.50%) (10316/10368)\n",
      "Epoch: 99 | Batch_idx: 90 |  Loss_1: (0.0117) | Acc_1: (99.53%) (11593/11648)\n",
      "Epoch: 99 | Batch_idx: 100 |  Loss_1: (0.0121) | Acc_1: (99.53%) (12867/12928)\n",
      "Epoch: 99 | Batch_idx: 110 |  Loss_1: (0.0125) | Acc_1: (99.52%) (14140/14208)\n",
      "Epoch: 99 | Batch_idx: 120 |  Loss_1: (0.0122) | Acc_1: (99.55%) (15418/15488)\n",
      "Epoch: 99 | Batch_idx: 130 |  Loss_1: (0.0125) | Acc_1: (99.55%) (16693/16768)\n",
      "Epoch: 99 | Batch_idx: 140 |  Loss_1: (0.0126) | Acc_1: (99.55%) (17967/18048)\n",
      "Epoch: 99 | Batch_idx: 150 |  Loss_1: (0.0125) | Acc_1: (99.55%) (19241/19328)\n",
      "Epoch: 99 | Batch_idx: 160 |  Loss_1: (0.0123) | Acc_1: (99.56%) (20518/20608)\n",
      "Epoch: 99 | Batch_idx: 170 |  Loss_1: (0.0124) | Acc_1: (99.57%) (21793/21888)\n",
      "Epoch: 99 | Batch_idx: 180 |  Loss_1: (0.0132) | Acc_1: (99.55%) (23063/23168)\n",
      "Epoch: 99 | Batch_idx: 190 |  Loss_1: (0.0135) | Acc_1: (99.53%) (24334/24448)\n",
      "Epoch: 99 | Batch_idx: 200 |  Loss_1: (0.0135) | Acc_1: (99.54%) (25609/25728)\n",
      "Epoch: 99 | Batch_idx: 210 |  Loss_1: (0.0133) | Acc_1: (99.55%) (26886/27008)\n",
      "Epoch: 99 | Batch_idx: 220 |  Loss_1: (0.0132) | Acc_1: (99.55%) (28162/28288)\n",
      "Epoch: 99 | Batch_idx: 230 |  Loss_1: (0.0131) | Acc_1: (99.56%) (29437/29568)\n",
      "Epoch: 99 | Batch_idx: 240 |  Loss_1: (0.0128) | Acc_1: (99.57%) (30714/30848)\n",
      "Epoch: 99 | Batch_idx: 250 |  Loss_1: (0.0128) | Acc_1: (99.57%) (31989/32128)\n",
      "Epoch: 99 | Batch_idx: 260 |  Loss_1: (0.0129) | Acc_1: (99.57%) (33264/33408)\n",
      "Epoch: 99 | Batch_idx: 270 |  Loss_1: (0.0129) | Acc_1: (99.56%) (34537/34688)\n",
      "Epoch: 99 | Batch_idx: 280 |  Loss_1: (0.0129) | Acc_1: (99.57%) (35813/35968)\n",
      "Epoch: 99 | Batch_idx: 290 |  Loss_1: (0.0129) | Acc_1: (99.57%) (37086/37248)\n",
      "Epoch: 99 | Batch_idx: 300 |  Loss_1: (0.0129) | Acc_1: (99.56%) (38359/38528)\n",
      "Epoch: 99 | Batch_idx: 310 |  Loss_1: (0.0129) | Acc_1: (99.56%) (39632/39808)\n",
      "Epoch: 99 | Batch_idx: 320 |  Loss_1: (0.0127) | Acc_1: (99.56%) (40909/41088)\n",
      "Epoch: 99 | Batch_idx: 330 |  Loss_1: (0.0125) | Acc_1: (99.57%) (42186/42368)\n",
      "Epoch: 99 | Batch_idx: 340 |  Loss_1: (0.0126) | Acc_1: (99.57%) (43460/43648)\n",
      "Epoch: 99 | Batch_idx: 350 |  Loss_1: (0.0125) | Acc_1: (99.57%) (44735/44928)\n",
      "Epoch: 99 | Batch_idx: 360 |  Loss_1: (0.0126) | Acc_1: (99.57%) (46007/46208)\n",
      "Epoch: 99 | Batch_idx: 370 |  Loss_1: (0.0128) | Acc_1: (99.56%) (47278/47488)\n",
      "Epoch: 99 | Batch_idx: 380 |  Loss_1: (0.0126) | Acc_1: (99.56%) (48554/48768)\n",
      "Epoch: 99 | Batch_idx: 390 |  Loss_1: (0.0128) | Acc_1: (99.56%) (49781/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4883) | Acc: (91.88%) (9188/10000)\n",
      "Epoch: 100 | Batch_idx: 0 |  Loss_1: (0.0019) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 100 | Batch_idx: 10 |  Loss_1: (0.0130) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 100 | Batch_idx: 20 |  Loss_1: (0.0153) | Acc_1: (99.29%) (2669/2688)\n",
      "Epoch: 100 | Batch_idx: 30 |  Loss_1: (0.0130) | Acc_1: (99.47%) (3947/3968)\n",
      "Epoch: 100 | Batch_idx: 40 |  Loss_1: (0.0141) | Acc_1: (99.37%) (5215/5248)\n",
      "Epoch: 100 | Batch_idx: 50 |  Loss_1: (0.0135) | Acc_1: (99.43%) (6491/6528)\n",
      "Epoch: 100 | Batch_idx: 60 |  Loss_1: (0.0132) | Acc_1: (99.44%) (7764/7808)\n",
      "Epoch: 100 | Batch_idx: 70 |  Loss_1: (0.0133) | Acc_1: (99.43%) (9036/9088)\n",
      "Epoch: 100 | Batch_idx: 80 |  Loss_1: (0.0123) | Acc_1: (99.49%) (10315/10368)\n",
      "Epoch: 100 | Batch_idx: 90 |  Loss_1: (0.0128) | Acc_1: (99.48%) (11588/11648)\n",
      "Epoch: 100 | Batch_idx: 100 |  Loss_1: (0.0124) | Acc_1: (99.50%) (12864/12928)\n",
      "Epoch: 100 | Batch_idx: 110 |  Loss_1: (0.0127) | Acc_1: (99.51%) (14138/14208)\n",
      "Epoch: 100 | Batch_idx: 120 |  Loss_1: (0.0134) | Acc_1: (99.50%) (15410/15488)\n",
      "Epoch: 100 | Batch_idx: 130 |  Loss_1: (0.0128) | Acc_1: (99.52%) (16687/16768)\n",
      "Epoch: 100 | Batch_idx: 140 |  Loss_1: (0.0124) | Acc_1: (99.53%) (17963/18048)\n",
      "Epoch: 100 | Batch_idx: 150 |  Loss_1: (0.0119) | Acc_1: (99.56%) (19243/19328)\n",
      "Epoch: 100 | Batch_idx: 160 |  Loss_1: (0.0117) | Acc_1: (99.57%) (20520/20608)\n",
      "Epoch: 100 | Batch_idx: 170 |  Loss_1: (0.0120) | Acc_1: (99.57%) (21793/21888)\n",
      "Epoch: 100 | Batch_idx: 180 |  Loss_1: (0.0116) | Acc_1: (99.58%) (23071/23168)\n",
      "Epoch: 100 | Batch_idx: 190 |  Loss_1: (0.0117) | Acc_1: (99.59%) (24347/24448)\n",
      "Epoch: 100 | Batch_idx: 200 |  Loss_1: (0.0119) | Acc_1: (99.57%) (25618/25728)\n",
      "Epoch: 100 | Batch_idx: 210 |  Loss_1: (0.0120) | Acc_1: (99.57%) (26892/27008)\n",
      "Epoch: 100 | Batch_idx: 220 |  Loss_1: (0.0120) | Acc_1: (99.57%) (28166/28288)\n",
      "Epoch: 100 | Batch_idx: 230 |  Loss_1: (0.0125) | Acc_1: (99.55%) (29436/29568)\n",
      "Epoch: 100 | Batch_idx: 240 |  Loss_1: (0.0125) | Acc_1: (99.55%) (30709/30848)\n",
      "Epoch: 100 | Batch_idx: 250 |  Loss_1: (0.0123) | Acc_1: (99.55%) (31984/32128)\n",
      "Epoch: 100 | Batch_idx: 260 |  Loss_1: (0.0124) | Acc_1: (99.55%) (33259/33408)\n",
      "Epoch: 100 | Batch_idx: 270 |  Loss_1: (0.0125) | Acc_1: (99.55%) (34531/34688)\n",
      "Epoch: 100 | Batch_idx: 280 |  Loss_1: (0.0126) | Acc_1: (99.54%) (35804/35968)\n",
      "Epoch: 100 | Batch_idx: 290 |  Loss_1: (0.0125) | Acc_1: (99.55%) (37079/37248)\n",
      "Epoch: 100 | Batch_idx: 300 |  Loss_1: (0.0125) | Acc_1: (99.55%) (38353/38528)\n",
      "Epoch: 100 | Batch_idx: 310 |  Loss_1: (0.0128) | Acc_1: (99.53%) (39621/39808)\n",
      "Epoch: 100 | Batch_idx: 320 |  Loss_1: (0.0129) | Acc_1: (99.53%) (40894/41088)\n",
      "Epoch: 100 | Batch_idx: 330 |  Loss_1: (0.0131) | Acc_1: (99.52%) (42166/42368)\n",
      "Epoch: 100 | Batch_idx: 340 |  Loss_1: (0.0131) | Acc_1: (99.52%) (43440/43648)\n",
      "Epoch: 100 | Batch_idx: 350 |  Loss_1: (0.0131) | Acc_1: (99.52%) (44714/44928)\n",
      "Epoch: 100 | Batch_idx: 360 |  Loss_1: (0.0132) | Acc_1: (99.52%) (45985/46208)\n",
      "Epoch: 100 | Batch_idx: 370 |  Loss_1: (0.0132) | Acc_1: (99.52%) (47260/47488)\n",
      "Epoch: 100 | Batch_idx: 380 |  Loss_1: (0.0136) | Acc_1: (99.51%) (48529/48768)\n",
      "Epoch: 100 | Batch_idx: 390 |  Loss_1: (0.0136) | Acc_1: (99.51%) (49753/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4743) | Acc: (91.63%) (9163/10000)\n",
      "Epoch: 101 | Batch_idx: 0 |  Loss_1: (0.0049) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 101 | Batch_idx: 10 |  Loss_1: (0.0138) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 101 | Batch_idx: 20 |  Loss_1: (0.0148) | Acc_1: (99.37%) (2671/2688)\n",
      "Epoch: 101 | Batch_idx: 30 |  Loss_1: (0.0124) | Acc_1: (99.55%) (3950/3968)\n",
      "Epoch: 101 | Batch_idx: 40 |  Loss_1: (0.0125) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 101 | Batch_idx: 50 |  Loss_1: (0.0116) | Acc_1: (99.62%) (6503/6528)\n",
      "Epoch: 101 | Batch_idx: 60 |  Loss_1: (0.0123) | Acc_1: (99.58%) (7775/7808)\n",
      "Epoch: 101 | Batch_idx: 70 |  Loss_1: (0.0125) | Acc_1: (99.55%) (9047/9088)\n",
      "Epoch: 101 | Batch_idx: 80 |  Loss_1: (0.0129) | Acc_1: (99.53%) (10319/10368)\n",
      "Epoch: 101 | Batch_idx: 90 |  Loss_1: (0.0123) | Acc_1: (99.54%) (11595/11648)\n",
      "Epoch: 101 | Batch_idx: 100 |  Loss_1: (0.0121) | Acc_1: (99.54%) (12868/12928)\n",
      "Epoch: 101 | Batch_idx: 110 |  Loss_1: (0.0126) | Acc_1: (99.52%) (14140/14208)\n",
      "Epoch: 101 | Batch_idx: 120 |  Loss_1: (0.0121) | Acc_1: (99.54%) (15417/15488)\n",
      "Epoch: 101 | Batch_idx: 130 |  Loss_1: (0.0117) | Acc_1: (99.56%) (16695/16768)\n",
      "Epoch: 101 | Batch_idx: 140 |  Loss_1: (0.0124) | Acc_1: (99.55%) (17967/18048)\n",
      "Epoch: 101 | Batch_idx: 150 |  Loss_1: (0.0123) | Acc_1: (99.55%) (19241/19328)\n",
      "Epoch: 101 | Batch_idx: 160 |  Loss_1: (0.0120) | Acc_1: (99.56%) (20518/20608)\n",
      "Epoch: 101 | Batch_idx: 170 |  Loss_1: (0.0121) | Acc_1: (99.56%) (21792/21888)\n",
      "Epoch: 101 | Batch_idx: 180 |  Loss_1: (0.0120) | Acc_1: (99.56%) (23067/23168)\n",
      "Epoch: 101 | Batch_idx: 190 |  Loss_1: (0.0118) | Acc_1: (99.58%) (24345/24448)\n",
      "Epoch: 101 | Batch_idx: 200 |  Loss_1: (0.0120) | Acc_1: (99.58%) (25620/25728)\n",
      "Epoch: 101 | Batch_idx: 210 |  Loss_1: (0.0117) | Acc_1: (99.60%) (26899/27008)\n",
      "Epoch: 101 | Batch_idx: 220 |  Loss_1: (0.0119) | Acc_1: (99.59%) (28171/28288)\n",
      "Epoch: 101 | Batch_idx: 230 |  Loss_1: (0.0118) | Acc_1: (99.59%) (29446/29568)\n",
      "Epoch: 101 | Batch_idx: 240 |  Loss_1: (0.0115) | Acc_1: (99.60%) (30725/30848)\n",
      "Epoch: 101 | Batch_idx: 250 |  Loss_1: (0.0115) | Acc_1: (99.60%) (31998/32128)\n",
      "Epoch: 101 | Batch_idx: 260 |  Loss_1: (0.0114) | Acc_1: (99.60%) (33274/33408)\n",
      "Epoch: 101 | Batch_idx: 270 |  Loss_1: (0.0114) | Acc_1: (99.60%) (34548/34688)\n",
      "Epoch: 101 | Batch_idx: 280 |  Loss_1: (0.0115) | Acc_1: (99.59%) (35819/35968)\n",
      "Epoch: 101 | Batch_idx: 290 |  Loss_1: (0.0113) | Acc_1: (99.59%) (37096/37248)\n",
      "Epoch: 101 | Batch_idx: 300 |  Loss_1: (0.0112) | Acc_1: (99.60%) (38372/38528)\n",
      "Epoch: 101 | Batch_idx: 310 |  Loss_1: (0.0110) | Acc_1: (99.60%) (39649/39808)\n",
      "Epoch: 101 | Batch_idx: 320 |  Loss_1: (0.0110) | Acc_1: (99.59%) (40920/41088)\n",
      "Epoch: 101 | Batch_idx: 330 |  Loss_1: (0.0110) | Acc_1: (99.59%) (42194/42368)\n",
      "Epoch: 101 | Batch_idx: 340 |  Loss_1: (0.0114) | Acc_1: (99.58%) (43464/43648)\n",
      "Epoch: 101 | Batch_idx: 350 |  Loss_1: (0.0113) | Acc_1: (99.58%) (44739/44928)\n",
      "Epoch: 101 | Batch_idx: 360 |  Loss_1: (0.0111) | Acc_1: (99.59%) (46018/46208)\n",
      "Epoch: 101 | Batch_idx: 370 |  Loss_1: (0.0112) | Acc_1: (99.59%) (47295/47488)\n",
      "Epoch: 101 | Batch_idx: 380 |  Loss_1: (0.0110) | Acc_1: (99.60%) (48574/48768)\n",
      "Epoch: 101 | Batch_idx: 390 |  Loss_1: (0.0110) | Acc_1: (99.60%) (49802/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4757) | Acc: (92.19%) (9219/10000)\n",
      "Epoch: 102 | Batch_idx: 0 |  Loss_1: (0.0024) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 102 | Batch_idx: 10 |  Loss_1: (0.0046) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 102 | Batch_idx: 20 |  Loss_1: (0.0056) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 102 | Batch_idx: 30 |  Loss_1: (0.0076) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 102 | Batch_idx: 40 |  Loss_1: (0.0085) | Acc_1: (99.68%) (5231/5248)\n",
      "Epoch: 102 | Batch_idx: 50 |  Loss_1: (0.0087) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 102 | Batch_idx: 60 |  Loss_1: (0.0085) | Acc_1: (99.67%) (7782/7808)\n",
      "Epoch: 102 | Batch_idx: 70 |  Loss_1: (0.0082) | Acc_1: (99.66%) (9057/9088)\n",
      "Epoch: 102 | Batch_idx: 80 |  Loss_1: (0.0079) | Acc_1: (99.67%) (10334/10368)\n",
      "Epoch: 102 | Batch_idx: 90 |  Loss_1: (0.0076) | Acc_1: (99.70%) (11613/11648)\n",
      "Epoch: 102 | Batch_idx: 100 |  Loss_1: (0.0073) | Acc_1: (99.71%) (12891/12928)\n",
      "Epoch: 102 | Batch_idx: 110 |  Loss_1: (0.0077) | Acc_1: (99.69%) (14164/14208)\n",
      "Epoch: 102 | Batch_idx: 120 |  Loss_1: (0.0079) | Acc_1: (99.68%) (15439/15488)\n",
      "Epoch: 102 | Batch_idx: 130 |  Loss_1: (0.0081) | Acc_1: (99.68%) (16715/16768)\n",
      "Epoch: 102 | Batch_idx: 140 |  Loss_1: (0.0088) | Acc_1: (99.68%) (17990/18048)\n",
      "Epoch: 102 | Batch_idx: 150 |  Loss_1: (0.0095) | Acc_1: (99.66%) (19263/19328)\n",
      "Epoch: 102 | Batch_idx: 160 |  Loss_1: (0.0094) | Acc_1: (99.67%) (20540/20608)\n",
      "Epoch: 102 | Batch_idx: 170 |  Loss_1: (0.0095) | Acc_1: (99.66%) (21814/21888)\n",
      "Epoch: 102 | Batch_idx: 180 |  Loss_1: (0.0095) | Acc_1: (99.65%) (23088/23168)\n",
      "Epoch: 102 | Batch_idx: 190 |  Loss_1: (0.0095) | Acc_1: (99.67%) (24367/24448)\n",
      "Epoch: 102 | Batch_idx: 200 |  Loss_1: (0.0095) | Acc_1: (99.67%) (25642/25728)\n",
      "Epoch: 102 | Batch_idx: 210 |  Loss_1: (0.0097) | Acc_1: (99.65%) (26914/27008)\n",
      "Epoch: 102 | Batch_idx: 220 |  Loss_1: (0.0098) | Acc_1: (99.65%) (28188/28288)\n",
      "Epoch: 102 | Batch_idx: 230 |  Loss_1: (0.0101) | Acc_1: (99.64%) (29463/29568)\n",
      "Epoch: 102 | Batch_idx: 240 |  Loss_1: (0.0102) | Acc_1: (99.65%) (30739/30848)\n",
      "Epoch: 102 | Batch_idx: 250 |  Loss_1: (0.0102) | Acc_1: (99.65%) (32014/32128)\n",
      "Epoch: 102 | Batch_idx: 260 |  Loss_1: (0.0101) | Acc_1: (99.65%) (33291/33408)\n",
      "Epoch: 102 | Batch_idx: 270 |  Loss_1: (0.0101) | Acc_1: (99.65%) (34566/34688)\n",
      "Epoch: 102 | Batch_idx: 280 |  Loss_1: (0.0101) | Acc_1: (99.65%) (35843/35968)\n",
      "Epoch: 102 | Batch_idx: 290 |  Loss_1: (0.0103) | Acc_1: (99.65%) (37118/37248)\n",
      "Epoch: 102 | Batch_idx: 300 |  Loss_1: (0.0103) | Acc_1: (99.65%) (38394/38528)\n",
      "Epoch: 102 | Batch_idx: 310 |  Loss_1: (0.0103) | Acc_1: (99.65%) (39668/39808)\n",
      "Epoch: 102 | Batch_idx: 320 |  Loss_1: (0.0104) | Acc_1: (99.65%) (40943/41088)\n",
      "Epoch: 102 | Batch_idx: 330 |  Loss_1: (0.0102) | Acc_1: (99.65%) (42221/42368)\n",
      "Epoch: 102 | Batch_idx: 340 |  Loss_1: (0.0102) | Acc_1: (99.66%) (43498/43648)\n",
      "Epoch: 102 | Batch_idx: 350 |  Loss_1: (0.0104) | Acc_1: (99.65%) (44769/44928)\n",
      "Epoch: 102 | Batch_idx: 360 |  Loss_1: (0.0105) | Acc_1: (99.64%) (46042/46208)\n",
      "Epoch: 102 | Batch_idx: 370 |  Loss_1: (0.0106) | Acc_1: (99.64%) (47317/47488)\n",
      "Epoch: 102 | Batch_idx: 380 |  Loss_1: (0.0106) | Acc_1: (99.64%) (48593/48768)\n",
      "Epoch: 102 | Batch_idx: 390 |  Loss_1: (0.0109) | Acc_1: (99.64%) (49819/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4503) | Acc: (92.24%) (9224/10000)\n",
      "Epoch: 103 | Batch_idx: 0 |  Loss_1: (0.0025) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 103 | Batch_idx: 10 |  Loss_1: (0.0159) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 103 | Batch_idx: 20 |  Loss_1: (0.0143) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 103 | Batch_idx: 30 |  Loss_1: (0.0131) | Acc_1: (99.52%) (3949/3968)\n",
      "Epoch: 103 | Batch_idx: 40 |  Loss_1: (0.0108) | Acc_1: (99.62%) (5228/5248)\n",
      "Epoch: 103 | Batch_idx: 50 |  Loss_1: (0.0117) | Acc_1: (99.60%) (6502/6528)\n",
      "Epoch: 103 | Batch_idx: 60 |  Loss_1: (0.0111) | Acc_1: (99.62%) (7778/7808)\n",
      "Epoch: 103 | Batch_idx: 70 |  Loss_1: (0.0120) | Acc_1: (99.58%) (9050/9088)\n",
      "Epoch: 103 | Batch_idx: 80 |  Loss_1: (0.0116) | Acc_1: (99.61%) (10328/10368)\n",
      "Epoch: 103 | Batch_idx: 90 |  Loss_1: (0.0110) | Acc_1: (99.64%) (11606/11648)\n",
      "Epoch: 103 | Batch_idx: 100 |  Loss_1: (0.0106) | Acc_1: (99.66%) (12884/12928)\n",
      "Epoch: 103 | Batch_idx: 110 |  Loss_1: (0.0101) | Acc_1: (99.66%) (14160/14208)\n",
      "Epoch: 103 | Batch_idx: 120 |  Loss_1: (0.0102) | Acc_1: (99.66%) (15436/15488)\n",
      "Epoch: 103 | Batch_idx: 130 |  Loss_1: (0.0100) | Acc_1: (99.67%) (16712/16768)\n",
      "Epoch: 103 | Batch_idx: 140 |  Loss_1: (0.0096) | Acc_1: (99.68%) (17990/18048)\n",
      "Epoch: 103 | Batch_idx: 150 |  Loss_1: (0.0096) | Acc_1: (99.68%) (19266/19328)\n",
      "Epoch: 103 | Batch_idx: 160 |  Loss_1: (0.0095) | Acc_1: (99.69%) (20544/20608)\n",
      "Epoch: 103 | Batch_idx: 170 |  Loss_1: (0.0100) | Acc_1: (99.66%) (21814/21888)\n",
      "Epoch: 103 | Batch_idx: 180 |  Loss_1: (0.0104) | Acc_1: (99.65%) (23086/23168)\n",
      "Epoch: 103 | Batch_idx: 190 |  Loss_1: (0.0102) | Acc_1: (99.66%) (24364/24448)\n",
      "Epoch: 103 | Batch_idx: 200 |  Loss_1: (0.0099) | Acc_1: (99.66%) (25641/25728)\n",
      "Epoch: 103 | Batch_idx: 210 |  Loss_1: (0.0097) | Acc_1: (99.67%) (26918/27008)\n",
      "Epoch: 103 | Batch_idx: 220 |  Loss_1: (0.0097) | Acc_1: (99.67%) (28194/28288)\n",
      "Epoch: 103 | Batch_idx: 230 |  Loss_1: (0.0096) | Acc_1: (99.68%) (29472/29568)\n",
      "Epoch: 103 | Batch_idx: 240 |  Loss_1: (0.0099) | Acc_1: (99.67%) (30746/30848)\n",
      "Epoch: 103 | Batch_idx: 250 |  Loss_1: (0.0101) | Acc_1: (99.67%) (32021/32128)\n",
      "Epoch: 103 | Batch_idx: 260 |  Loss_1: (0.0104) | Acc_1: (99.65%) (33292/33408)\n",
      "Epoch: 103 | Batch_idx: 270 |  Loss_1: (0.0106) | Acc_1: (99.64%) (34563/34688)\n",
      "Epoch: 103 | Batch_idx: 280 |  Loss_1: (0.0105) | Acc_1: (99.65%) (35841/35968)\n",
      "Epoch: 103 | Batch_idx: 290 |  Loss_1: (0.0105) | Acc_1: (99.65%) (37117/37248)\n",
      "Epoch: 103 | Batch_idx: 300 |  Loss_1: (0.0106) | Acc_1: (99.64%) (38391/38528)\n",
      "Epoch: 103 | Batch_idx: 310 |  Loss_1: (0.0106) | Acc_1: (99.65%) (39667/39808)\n",
      "Epoch: 103 | Batch_idx: 320 |  Loss_1: (0.0105) | Acc_1: (99.64%) (40942/41088)\n",
      "Epoch: 103 | Batch_idx: 330 |  Loss_1: (0.0105) | Acc_1: (99.64%) (42216/42368)\n",
      "Epoch: 103 | Batch_idx: 340 |  Loss_1: (0.0105) | Acc_1: (99.65%) (43494/43648)\n",
      "Epoch: 103 | Batch_idx: 350 |  Loss_1: (0.0104) | Acc_1: (99.65%) (44770/44928)\n",
      "Epoch: 103 | Batch_idx: 360 |  Loss_1: (0.0104) | Acc_1: (99.65%) (46045/46208)\n",
      "Epoch: 103 | Batch_idx: 370 |  Loss_1: (0.0104) | Acc_1: (99.65%) (47320/47488)\n",
      "Epoch: 103 | Batch_idx: 380 |  Loss_1: (0.0104) | Acc_1: (99.64%) (48594/48768)\n",
      "Epoch: 103 | Batch_idx: 390 |  Loss_1: (0.0104) | Acc_1: (99.65%) (49823/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4688) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 104 | Batch_idx: 0 |  Loss_1: (0.0063) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 104 | Batch_idx: 10 |  Loss_1: (0.0067) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 104 | Batch_idx: 20 |  Loss_1: (0.0094) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 104 | Batch_idx: 30 |  Loss_1: (0.0096) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 104 | Batch_idx: 40 |  Loss_1: (0.0102) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 104 | Batch_idx: 50 |  Loss_1: (0.0091) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 104 | Batch_idx: 60 |  Loss_1: (0.0092) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 104 | Batch_idx: 70 |  Loss_1: (0.0091) | Acc_1: (99.76%) (9066/9088)\n",
      "Epoch: 104 | Batch_idx: 80 |  Loss_1: (0.0085) | Acc_1: (99.78%) (10345/10368)\n",
      "Epoch: 104 | Batch_idx: 90 |  Loss_1: (0.0090) | Acc_1: (99.77%) (11621/11648)\n",
      "Epoch: 104 | Batch_idx: 100 |  Loss_1: (0.0091) | Acc_1: (99.76%) (12897/12928)\n",
      "Epoch: 104 | Batch_idx: 110 |  Loss_1: (0.0097) | Acc_1: (99.74%) (14171/14208)\n",
      "Epoch: 104 | Batch_idx: 120 |  Loss_1: (0.0097) | Acc_1: (99.74%) (15448/15488)\n",
      "Epoch: 104 | Batch_idx: 130 |  Loss_1: (0.0099) | Acc_1: (99.73%) (16722/16768)\n",
      "Epoch: 104 | Batch_idx: 140 |  Loss_1: (0.0107) | Acc_1: (99.69%) (17992/18048)\n",
      "Epoch: 104 | Batch_idx: 150 |  Loss_1: (0.0107) | Acc_1: (99.69%) (19268/19328)\n",
      "Epoch: 104 | Batch_idx: 160 |  Loss_1: (0.0106) | Acc_1: (99.68%) (20543/20608)\n",
      "Epoch: 104 | Batch_idx: 170 |  Loss_1: (0.0115) | Acc_1: (99.66%) (21814/21888)\n",
      "Epoch: 104 | Batch_idx: 180 |  Loss_1: (0.0116) | Acc_1: (99.66%) (23089/23168)\n",
      "Epoch: 104 | Batch_idx: 190 |  Loss_1: (0.0118) | Acc_1: (99.64%) (24361/24448)\n",
      "Epoch: 104 | Batch_idx: 200 |  Loss_1: (0.0118) | Acc_1: (99.64%) (25636/25728)\n",
      "Epoch: 104 | Batch_idx: 210 |  Loss_1: (0.0118) | Acc_1: (99.63%) (26909/27008)\n",
      "Epoch: 104 | Batch_idx: 220 |  Loss_1: (0.0119) | Acc_1: (99.62%) (28181/28288)\n",
      "Epoch: 104 | Batch_idx: 230 |  Loss_1: (0.0119) | Acc_1: (99.62%) (29456/29568)\n",
      "Epoch: 104 | Batch_idx: 240 |  Loss_1: (0.0119) | Acc_1: (99.63%) (30734/30848)\n",
      "Epoch: 104 | Batch_idx: 250 |  Loss_1: (0.0117) | Acc_1: (99.64%) (32012/32128)\n",
      "Epoch: 104 | Batch_idx: 260 |  Loss_1: (0.0119) | Acc_1: (99.64%) (33289/33408)\n",
      "Epoch: 104 | Batch_idx: 270 |  Loss_1: (0.0119) | Acc_1: (99.65%) (34565/34688)\n",
      "Epoch: 104 | Batch_idx: 280 |  Loss_1: (0.0117) | Acc_1: (99.65%) (35841/35968)\n",
      "Epoch: 104 | Batch_idx: 290 |  Loss_1: (0.0118) | Acc_1: (99.64%) (37114/37248)\n",
      "Epoch: 104 | Batch_idx: 300 |  Loss_1: (0.0118) | Acc_1: (99.64%) (38390/38528)\n",
      "Epoch: 104 | Batch_idx: 310 |  Loss_1: (0.0117) | Acc_1: (99.64%) (39666/39808)\n",
      "Epoch: 104 | Batch_idx: 320 |  Loss_1: (0.0119) | Acc_1: (99.63%) (40938/41088)\n",
      "Epoch: 104 | Batch_idx: 330 |  Loss_1: (0.0120) | Acc_1: (99.63%) (42213/42368)\n",
      "Epoch: 104 | Batch_idx: 340 |  Loss_1: (0.0120) | Acc_1: (99.63%) (43486/43648)\n",
      "Epoch: 104 | Batch_idx: 350 |  Loss_1: (0.0119) | Acc_1: (99.63%) (44762/44928)\n",
      "Epoch: 104 | Batch_idx: 360 |  Loss_1: (0.0119) | Acc_1: (99.63%) (46037/46208)\n",
      "Epoch: 104 | Batch_idx: 370 |  Loss_1: (0.0119) | Acc_1: (99.64%) (47315/47488)\n",
      "Epoch: 104 | Batch_idx: 380 |  Loss_1: (0.0120) | Acc_1: (99.63%) (48589/48768)\n",
      "Epoch: 104 | Batch_idx: 390 |  Loss_1: (0.0120) | Acc_1: (99.63%) (49814/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4623) | Acc: (92.35%) (9235/10000)\n",
      "Epoch: 105 | Batch_idx: 0 |  Loss_1: (0.0016) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 105 | Batch_idx: 10 |  Loss_1: (0.0063) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 105 | Batch_idx: 20 |  Loss_1: (0.0060) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 105 | Batch_idx: 30 |  Loss_1: (0.0078) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 105 | Batch_idx: 40 |  Loss_1: (0.0095) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 105 | Batch_idx: 50 |  Loss_1: (0.0090) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 105 | Batch_idx: 60 |  Loss_1: (0.0091) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 105 | Batch_idx: 70 |  Loss_1: (0.0091) | Acc_1: (99.68%) (9059/9088)\n",
      "Epoch: 105 | Batch_idx: 80 |  Loss_1: (0.0090) | Acc_1: (99.68%) (10335/10368)\n",
      "Epoch: 105 | Batch_idx: 90 |  Loss_1: (0.0092) | Acc_1: (99.67%) (11609/11648)\n",
      "Epoch: 105 | Batch_idx: 100 |  Loss_1: (0.0093) | Acc_1: (99.67%) (12885/12928)\n",
      "Epoch: 105 | Batch_idx: 110 |  Loss_1: (0.0096) | Acc_1: (99.65%) (14158/14208)\n",
      "Epoch: 105 | Batch_idx: 120 |  Loss_1: (0.0094) | Acc_1: (99.65%) (15434/15488)\n",
      "Epoch: 105 | Batch_idx: 130 |  Loss_1: (0.0103) | Acc_1: (99.62%) (16705/16768)\n",
      "Epoch: 105 | Batch_idx: 140 |  Loss_1: (0.0100) | Acc_1: (99.64%) (17983/18048)\n",
      "Epoch: 105 | Batch_idx: 150 |  Loss_1: (0.0101) | Acc_1: (99.64%) (19258/19328)\n",
      "Epoch: 105 | Batch_idx: 160 |  Loss_1: (0.0097) | Acc_1: (99.66%) (20537/20608)\n",
      "Epoch: 105 | Batch_idx: 170 |  Loss_1: (0.0096) | Acc_1: (99.65%) (21812/21888)\n",
      "Epoch: 105 | Batch_idx: 180 |  Loss_1: (0.0095) | Acc_1: (99.65%) (23088/23168)\n",
      "Epoch: 105 | Batch_idx: 190 |  Loss_1: (0.0097) | Acc_1: (99.65%) (24362/24448)\n",
      "Epoch: 105 | Batch_idx: 200 |  Loss_1: (0.0096) | Acc_1: (99.65%) (25639/25728)\n",
      "Epoch: 105 | Batch_idx: 210 |  Loss_1: (0.0094) | Acc_1: (99.66%) (26915/27008)\n",
      "Epoch: 105 | Batch_idx: 220 |  Loss_1: (0.0097) | Acc_1: (99.64%) (28186/28288)\n",
      "Epoch: 105 | Batch_idx: 230 |  Loss_1: (0.0097) | Acc_1: (99.63%) (29459/29568)\n",
      "Epoch: 105 | Batch_idx: 240 |  Loss_1: (0.0095) | Acc_1: (99.64%) (30736/30848)\n",
      "Epoch: 105 | Batch_idx: 250 |  Loss_1: (0.0098) | Acc_1: (99.63%) (32008/32128)\n",
      "Epoch: 105 | Batch_idx: 260 |  Loss_1: (0.0099) | Acc_1: (99.63%) (33283/33408)\n",
      "Epoch: 105 | Batch_idx: 270 |  Loss_1: (0.0098) | Acc_1: (99.63%) (34560/34688)\n",
      "Epoch: 105 | Batch_idx: 280 |  Loss_1: (0.0101) | Acc_1: (99.62%) (35832/35968)\n",
      "Epoch: 105 | Batch_idx: 290 |  Loss_1: (0.0105) | Acc_1: (99.61%) (37103/37248)\n",
      "Epoch: 105 | Batch_idx: 300 |  Loss_1: (0.0105) | Acc_1: (99.61%) (38378/38528)\n",
      "Epoch: 105 | Batch_idx: 310 |  Loss_1: (0.0107) | Acc_1: (99.61%) (39651/39808)\n",
      "Epoch: 105 | Batch_idx: 320 |  Loss_1: (0.0107) | Acc_1: (99.60%) (40925/41088)\n",
      "Epoch: 105 | Batch_idx: 330 |  Loss_1: (0.0108) | Acc_1: (99.60%) (42197/42368)\n",
      "Epoch: 105 | Batch_idx: 340 |  Loss_1: (0.0106) | Acc_1: (99.60%) (43475/43648)\n",
      "Epoch: 105 | Batch_idx: 350 |  Loss_1: (0.0106) | Acc_1: (99.60%) (44750/44928)\n",
      "Epoch: 105 | Batch_idx: 360 |  Loss_1: (0.0106) | Acc_1: (99.61%) (46026/46208)\n",
      "Epoch: 105 | Batch_idx: 370 |  Loss_1: (0.0106) | Acc_1: (99.60%) (47299/47488)\n",
      "Epoch: 105 | Batch_idx: 380 |  Loss_1: (0.0107) | Acc_1: (99.60%) (48575/48768)\n",
      "Epoch: 105 | Batch_idx: 390 |  Loss_1: (0.0108) | Acc_1: (99.60%) (49799/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4687) | Acc: (91.96%) (9196/10000)\n",
      "Epoch: 106 | Batch_idx: 0 |  Loss_1: (0.0022) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 106 | Batch_idx: 10 |  Loss_1: (0.0091) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 106 | Batch_idx: 20 |  Loss_1: (0.0098) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 106 | Batch_idx: 30 |  Loss_1: (0.0109) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 106 | Batch_idx: 40 |  Loss_1: (0.0099) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 106 | Batch_idx: 50 |  Loss_1: (0.0107) | Acc_1: (99.66%) (6506/6528)\n",
      "Epoch: 106 | Batch_idx: 60 |  Loss_1: (0.0098) | Acc_1: (99.69%) (7784/7808)\n",
      "Epoch: 106 | Batch_idx: 70 |  Loss_1: (0.0096) | Acc_1: (99.69%) (9060/9088)\n",
      "Epoch: 106 | Batch_idx: 80 |  Loss_1: (0.0099) | Acc_1: (99.68%) (10335/10368)\n",
      "Epoch: 106 | Batch_idx: 90 |  Loss_1: (0.0096) | Acc_1: (99.68%) (11611/11648)\n",
      "Epoch: 106 | Batch_idx: 100 |  Loss_1: (0.0101) | Acc_1: (99.66%) (12884/12928)\n",
      "Epoch: 106 | Batch_idx: 110 |  Loss_1: (0.0102) | Acc_1: (99.66%) (14160/14208)\n",
      "Epoch: 106 | Batch_idx: 120 |  Loss_1: (0.0105) | Acc_1: (99.66%) (15435/15488)\n",
      "Epoch: 106 | Batch_idx: 130 |  Loss_1: (0.0106) | Acc_1: (99.65%) (16710/16768)\n",
      "Epoch: 106 | Batch_idx: 140 |  Loss_1: (0.0107) | Acc_1: (99.65%) (17985/18048)\n",
      "Epoch: 106 | Batch_idx: 150 |  Loss_1: (0.0107) | Acc_1: (99.65%) (19260/19328)\n",
      "Epoch: 106 | Batch_idx: 160 |  Loss_1: (0.0107) | Acc_1: (99.65%) (20536/20608)\n",
      "Epoch: 106 | Batch_idx: 170 |  Loss_1: (0.0102) | Acc_1: (99.67%) (21815/21888)\n",
      "Epoch: 106 | Batch_idx: 180 |  Loss_1: (0.0101) | Acc_1: (99.67%) (23091/23168)\n",
      "Epoch: 106 | Batch_idx: 190 |  Loss_1: (0.0099) | Acc_1: (99.67%) (24367/24448)\n",
      "Epoch: 106 | Batch_idx: 200 |  Loss_1: (0.0097) | Acc_1: (99.67%) (25644/25728)\n",
      "Epoch: 106 | Batch_idx: 210 |  Loss_1: (0.0098) | Acc_1: (99.68%) (26921/27008)\n",
      "Epoch: 106 | Batch_idx: 220 |  Loss_1: (0.0098) | Acc_1: (99.68%) (28197/28288)\n",
      "Epoch: 106 | Batch_idx: 230 |  Loss_1: (0.0096) | Acc_1: (99.68%) (29474/29568)\n",
      "Epoch: 106 | Batch_idx: 240 |  Loss_1: (0.0099) | Acc_1: (99.67%) (30746/30848)\n",
      "Epoch: 106 | Batch_idx: 250 |  Loss_1: (0.0098) | Acc_1: (99.68%) (32024/32128)\n",
      "Epoch: 106 | Batch_idx: 260 |  Loss_1: (0.0095) | Acc_1: (99.69%) (33303/33408)\n",
      "Epoch: 106 | Batch_idx: 270 |  Loss_1: (0.0094) | Acc_1: (99.69%) (34579/34688)\n",
      "Epoch: 106 | Batch_idx: 280 |  Loss_1: (0.0095) | Acc_1: (99.68%) (35854/35968)\n",
      "Epoch: 106 | Batch_idx: 290 |  Loss_1: (0.0096) | Acc_1: (99.69%) (37131/37248)\n",
      "Epoch: 106 | Batch_idx: 300 |  Loss_1: (0.0096) | Acc_1: (99.69%) (38407/38528)\n",
      "Epoch: 106 | Batch_idx: 310 |  Loss_1: (0.0097) | Acc_1: (99.68%) (39682/39808)\n",
      "Epoch: 106 | Batch_idx: 320 |  Loss_1: (0.0099) | Acc_1: (99.68%) (40958/41088)\n",
      "Epoch: 106 | Batch_idx: 330 |  Loss_1: (0.0100) | Acc_1: (99.67%) (42229/42368)\n",
      "Epoch: 106 | Batch_idx: 340 |  Loss_1: (0.0099) | Acc_1: (99.67%) (43505/43648)\n",
      "Epoch: 106 | Batch_idx: 350 |  Loss_1: (0.0100) | Acc_1: (99.67%) (44780/44928)\n",
      "Epoch: 106 | Batch_idx: 360 |  Loss_1: (0.0102) | Acc_1: (99.67%) (46054/46208)\n",
      "Epoch: 106 | Batch_idx: 370 |  Loss_1: (0.0103) | Acc_1: (99.66%) (47328/47488)\n",
      "Epoch: 106 | Batch_idx: 380 |  Loss_1: (0.0102) | Acc_1: (99.66%) (48604/48768)\n",
      "Epoch: 106 | Batch_idx: 390 |  Loss_1: (0.0103) | Acc_1: (99.66%) (49829/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4575) | Acc: (92.11%) (9211/10000)\n",
      "Epoch: 107 | Batch_idx: 0 |  Loss_1: (0.0014) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 107 | Batch_idx: 10 |  Loss_1: (0.0047) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 107 | Batch_idx: 20 |  Loss_1: (0.0045) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 107 | Batch_idx: 30 |  Loss_1: (0.0058) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 107 | Batch_idx: 40 |  Loss_1: (0.0055) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 107 | Batch_idx: 50 |  Loss_1: (0.0063) | Acc_1: (99.82%) (6516/6528)\n",
      "Epoch: 107 | Batch_idx: 60 |  Loss_1: (0.0067) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 107 | Batch_idx: 70 |  Loss_1: (0.0063) | Acc_1: (99.79%) (9069/9088)\n",
      "Epoch: 107 | Batch_idx: 80 |  Loss_1: (0.0075) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 107 | Batch_idx: 90 |  Loss_1: (0.0074) | Acc_1: (99.79%) (11624/11648)\n",
      "Epoch: 107 | Batch_idx: 100 |  Loss_1: (0.0074) | Acc_1: (99.79%) (12901/12928)\n",
      "Epoch: 107 | Batch_idx: 110 |  Loss_1: (0.0074) | Acc_1: (99.79%) (14178/14208)\n",
      "Epoch: 107 | Batch_idx: 120 |  Loss_1: (0.0072) | Acc_1: (99.79%) (15456/15488)\n",
      "Epoch: 107 | Batch_idx: 130 |  Loss_1: (0.0072) | Acc_1: (99.79%) (16732/16768)\n",
      "Epoch: 107 | Batch_idx: 140 |  Loss_1: (0.0072) | Acc_1: (99.79%) (18010/18048)\n",
      "Epoch: 107 | Batch_idx: 150 |  Loss_1: (0.0070) | Acc_1: (99.79%) (19288/19328)\n",
      "Epoch: 107 | Batch_idx: 160 |  Loss_1: (0.0071) | Acc_1: (99.80%) (20566/20608)\n",
      "Epoch: 107 | Batch_idx: 170 |  Loss_1: (0.0071) | Acc_1: (99.79%) (21842/21888)\n",
      "Epoch: 107 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.79%) (23120/23168)\n",
      "Epoch: 107 | Batch_idx: 190 |  Loss_1: (0.0072) | Acc_1: (99.78%) (24395/24448)\n",
      "Epoch: 107 | Batch_idx: 200 |  Loss_1: (0.0073) | Acc_1: (99.78%) (25671/25728)\n",
      "Epoch: 107 | Batch_idx: 210 |  Loss_1: (0.0075) | Acc_1: (99.76%) (26944/27008)\n",
      "Epoch: 107 | Batch_idx: 220 |  Loss_1: (0.0073) | Acc_1: (99.77%) (28224/28288)\n",
      "Epoch: 107 | Batch_idx: 230 |  Loss_1: (0.0074) | Acc_1: (99.77%) (29499/29568)\n",
      "Epoch: 107 | Batch_idx: 240 |  Loss_1: (0.0074) | Acc_1: (99.76%) (30775/30848)\n",
      "Epoch: 107 | Batch_idx: 250 |  Loss_1: (0.0074) | Acc_1: (99.77%) (32053/32128)\n",
      "Epoch: 107 | Batch_idx: 260 |  Loss_1: (0.0074) | Acc_1: (99.76%) (33329/33408)\n",
      "Epoch: 107 | Batch_idx: 270 |  Loss_1: (0.0073) | Acc_1: (99.77%) (34607/34688)\n",
      "Epoch: 107 | Batch_idx: 280 |  Loss_1: (0.0073) | Acc_1: (99.76%) (35882/35968)\n",
      "Epoch: 107 | Batch_idx: 290 |  Loss_1: (0.0073) | Acc_1: (99.76%) (37159/37248)\n",
      "Epoch: 107 | Batch_idx: 300 |  Loss_1: (0.0075) | Acc_1: (99.75%) (38433/38528)\n",
      "Epoch: 107 | Batch_idx: 310 |  Loss_1: (0.0074) | Acc_1: (99.76%) (39711/39808)\n",
      "Epoch: 107 | Batch_idx: 320 |  Loss_1: (0.0075) | Acc_1: (99.76%) (40988/41088)\n",
      "Epoch: 107 | Batch_idx: 330 |  Loss_1: (0.0075) | Acc_1: (99.75%) (42264/42368)\n",
      "Epoch: 107 | Batch_idx: 340 |  Loss_1: (0.0076) | Acc_1: (99.75%) (43539/43648)\n",
      "Epoch: 107 | Batch_idx: 350 |  Loss_1: (0.0075) | Acc_1: (99.75%) (44815/44928)\n",
      "Epoch: 107 | Batch_idx: 360 |  Loss_1: (0.0075) | Acc_1: (99.75%) (46094/46208)\n",
      "Epoch: 107 | Batch_idx: 370 |  Loss_1: (0.0074) | Acc_1: (99.75%) (47370/47488)\n",
      "Epoch: 107 | Batch_idx: 380 |  Loss_1: (0.0074) | Acc_1: (99.75%) (48647/48768)\n",
      "Epoch: 107 | Batch_idx: 390 |  Loss_1: (0.0075) | Acc_1: (99.75%) (49873/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4566) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 108 | Batch_idx: 0 |  Loss_1: (0.0064) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 108 | Batch_idx: 10 |  Loss_1: (0.0139) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 108 | Batch_idx: 20 |  Loss_1: (0.0087) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 108 | Batch_idx: 30 |  Loss_1: (0.0088) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 108 | Batch_idx: 40 |  Loss_1: (0.0084) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 108 | Batch_idx: 50 |  Loss_1: (0.0078) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 108 | Batch_idx: 60 |  Loss_1: (0.0079) | Acc_1: (99.69%) (7784/7808)\n",
      "Epoch: 108 | Batch_idx: 70 |  Loss_1: (0.0072) | Acc_1: (99.72%) (9063/9088)\n",
      "Epoch: 108 | Batch_idx: 80 |  Loss_1: (0.0074) | Acc_1: (99.71%) (10338/10368)\n",
      "Epoch: 108 | Batch_idx: 90 |  Loss_1: (0.0075) | Acc_1: (99.72%) (11615/11648)\n",
      "Epoch: 108 | Batch_idx: 100 |  Loss_1: (0.0074) | Acc_1: (99.72%) (12892/12928)\n",
      "Epoch: 108 | Batch_idx: 110 |  Loss_1: (0.0077) | Acc_1: (99.71%) (14167/14208)\n",
      "Epoch: 108 | Batch_idx: 120 |  Loss_1: (0.0079) | Acc_1: (99.72%) (15445/15488)\n",
      "Epoch: 108 | Batch_idx: 130 |  Loss_1: (0.0075) | Acc_1: (99.74%) (16724/16768)\n",
      "Epoch: 108 | Batch_idx: 140 |  Loss_1: (0.0080) | Acc_1: (99.71%) (17996/18048)\n",
      "Epoch: 108 | Batch_idx: 150 |  Loss_1: (0.0083) | Acc_1: (99.71%) (19272/19328)\n",
      "Epoch: 108 | Batch_idx: 160 |  Loss_1: (0.0084) | Acc_1: (99.71%) (20549/20608)\n",
      "Epoch: 108 | Batch_idx: 170 |  Loss_1: (0.0090) | Acc_1: (99.70%) (21822/21888)\n",
      "Epoch: 108 | Batch_idx: 180 |  Loss_1: (0.0092) | Acc_1: (99.68%) (23095/23168)\n",
      "Epoch: 108 | Batch_idx: 190 |  Loss_1: (0.0091) | Acc_1: (99.69%) (24373/24448)\n",
      "Epoch: 108 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.69%) (25649/25728)\n",
      "Epoch: 108 | Batch_idx: 210 |  Loss_1: (0.0094) | Acc_1: (99.68%) (26922/27008)\n",
      "Epoch: 108 | Batch_idx: 220 |  Loss_1: (0.0093) | Acc_1: (99.69%) (28200/28288)\n",
      "Epoch: 108 | Batch_idx: 230 |  Loss_1: (0.0091) | Acc_1: (99.70%) (29479/29568)\n",
      "Epoch: 108 | Batch_idx: 240 |  Loss_1: (0.0089) | Acc_1: (99.71%) (30758/30848)\n",
      "Epoch: 108 | Batch_idx: 250 |  Loss_1: (0.0089) | Acc_1: (99.70%) (32033/32128)\n",
      "Epoch: 108 | Batch_idx: 260 |  Loss_1: (0.0088) | Acc_1: (99.71%) (33311/33408)\n",
      "Epoch: 108 | Batch_idx: 270 |  Loss_1: (0.0087) | Acc_1: (99.71%) (34589/34688)\n",
      "Epoch: 108 | Batch_idx: 280 |  Loss_1: (0.0087) | Acc_1: (99.71%) (35865/35968)\n",
      "Epoch: 108 | Batch_idx: 290 |  Loss_1: (0.0085) | Acc_1: (99.72%) (37143/37248)\n",
      "Epoch: 108 | Batch_idx: 300 |  Loss_1: (0.0084) | Acc_1: (99.72%) (38420/38528)\n",
      "Epoch: 108 | Batch_idx: 310 |  Loss_1: (0.0084) | Acc_1: (99.73%) (39699/39808)\n",
      "Epoch: 108 | Batch_idx: 320 |  Loss_1: (0.0083) | Acc_1: (99.73%) (40976/41088)\n",
      "Epoch: 108 | Batch_idx: 330 |  Loss_1: (0.0082) | Acc_1: (99.73%) (42254/42368)\n",
      "Epoch: 108 | Batch_idx: 340 |  Loss_1: (0.0082) | Acc_1: (99.73%) (43532/43648)\n",
      "Epoch: 108 | Batch_idx: 350 |  Loss_1: (0.0081) | Acc_1: (99.74%) (44810/44928)\n",
      "Epoch: 108 | Batch_idx: 360 |  Loss_1: (0.0081) | Acc_1: (99.73%) (46085/46208)\n",
      "Epoch: 108 | Batch_idx: 370 |  Loss_1: (0.0082) | Acc_1: (99.73%) (47361/47488)\n",
      "Epoch: 108 | Batch_idx: 380 |  Loss_1: (0.0081) | Acc_1: (99.74%) (48640/48768)\n",
      "Epoch: 108 | Batch_idx: 390 |  Loss_1: (0.0080) | Acc_1: (99.74%) (49870/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4879) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 109 | Batch_idx: 0 |  Loss_1: (0.0013) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 109 | Batch_idx: 10 |  Loss_1: (0.0035) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 109 | Batch_idx: 20 |  Loss_1: (0.0048) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 109 | Batch_idx: 30 |  Loss_1: (0.0088) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 109 | Batch_idx: 40 |  Loss_1: (0.0085) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 109 | Batch_idx: 50 |  Loss_1: (0.0083) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 109 | Batch_idx: 60 |  Loss_1: (0.0087) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 109 | Batch_idx: 70 |  Loss_1: (0.0088) | Acc_1: (99.72%) (9063/9088)\n",
      "Epoch: 109 | Batch_idx: 80 |  Loss_1: (0.0091) | Acc_1: (99.72%) (10339/10368)\n",
      "Epoch: 109 | Batch_idx: 90 |  Loss_1: (0.0095) | Acc_1: (99.73%) (11616/11648)\n",
      "Epoch: 109 | Batch_idx: 100 |  Loss_1: (0.0102) | Acc_1: (99.71%) (12891/12928)\n",
      "Epoch: 109 | Batch_idx: 110 |  Loss_1: (0.0099) | Acc_1: (99.73%) (14169/14208)\n",
      "Epoch: 109 | Batch_idx: 120 |  Loss_1: (0.0099) | Acc_1: (99.71%) (15443/15488)\n",
      "Epoch: 109 | Batch_idx: 130 |  Loss_1: (0.0102) | Acc_1: (99.70%) (16717/16768)\n",
      "Epoch: 109 | Batch_idx: 140 |  Loss_1: (0.0103) | Acc_1: (99.69%) (17992/18048)\n",
      "Epoch: 109 | Batch_idx: 150 |  Loss_1: (0.0103) | Acc_1: (99.69%) (19269/19328)\n",
      "Epoch: 109 | Batch_idx: 160 |  Loss_1: (0.0103) | Acc_1: (99.68%) (20542/20608)\n",
      "Epoch: 109 | Batch_idx: 170 |  Loss_1: (0.0099) | Acc_1: (99.69%) (21821/21888)\n",
      "Epoch: 109 | Batch_idx: 180 |  Loss_1: (0.0101) | Acc_1: (99.67%) (23092/23168)\n",
      "Epoch: 109 | Batch_idx: 190 |  Loss_1: (0.0101) | Acc_1: (99.68%) (24369/24448)\n",
      "Epoch: 109 | Batch_idx: 200 |  Loss_1: (0.0098) | Acc_1: (99.69%) (25647/25728)\n",
      "Epoch: 109 | Batch_idx: 210 |  Loss_1: (0.0095) | Acc_1: (99.69%) (26925/27008)\n",
      "Epoch: 109 | Batch_idx: 220 |  Loss_1: (0.0093) | Acc_1: (99.70%) (28204/28288)\n",
      "Epoch: 109 | Batch_idx: 230 |  Loss_1: (0.0093) | Acc_1: (99.69%) (29477/29568)\n",
      "Epoch: 109 | Batch_idx: 240 |  Loss_1: (0.0094) | Acc_1: (99.69%) (30751/30848)\n",
      "Epoch: 109 | Batch_idx: 250 |  Loss_1: (0.0095) | Acc_1: (99.68%) (32026/32128)\n",
      "Epoch: 109 | Batch_idx: 260 |  Loss_1: (0.0098) | Acc_1: (99.67%) (33299/33408)\n",
      "Epoch: 109 | Batch_idx: 270 |  Loss_1: (0.0100) | Acc_1: (99.67%) (34572/34688)\n",
      "Epoch: 109 | Batch_idx: 280 |  Loss_1: (0.0100) | Acc_1: (99.67%) (35850/35968)\n",
      "Epoch: 109 | Batch_idx: 290 |  Loss_1: (0.0101) | Acc_1: (99.67%) (37124/37248)\n",
      "Epoch: 109 | Batch_idx: 300 |  Loss_1: (0.0101) | Acc_1: (99.67%) (38400/38528)\n",
      "Epoch: 109 | Batch_idx: 310 |  Loss_1: (0.0102) | Acc_1: (99.67%) (39675/39808)\n",
      "Epoch: 109 | Batch_idx: 320 |  Loss_1: (0.0101) | Acc_1: (99.67%) (40953/41088)\n",
      "Epoch: 109 | Batch_idx: 330 |  Loss_1: (0.0101) | Acc_1: (99.67%) (42230/42368)\n",
      "Epoch: 109 | Batch_idx: 340 |  Loss_1: (0.0101) | Acc_1: (99.67%) (43506/43648)\n",
      "Epoch: 109 | Batch_idx: 350 |  Loss_1: (0.0099) | Acc_1: (99.68%) (44785/44928)\n",
      "Epoch: 109 | Batch_idx: 360 |  Loss_1: (0.0098) | Acc_1: (99.69%) (46064/46208)\n",
      "Epoch: 109 | Batch_idx: 370 |  Loss_1: (0.0097) | Acc_1: (99.69%) (47340/47488)\n",
      "Epoch: 109 | Batch_idx: 380 |  Loss_1: (0.0096) | Acc_1: (99.69%) (48618/48768)\n",
      "Epoch: 109 | Batch_idx: 390 |  Loss_1: (0.0096) | Acc_1: (99.69%) (49845/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4559) | Acc: (92.42%) (9242/10000)\n",
      "Epoch: 110 | Batch_idx: 0 |  Loss_1: (0.0033) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 110 | Batch_idx: 10 |  Loss_1: (0.0111) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 110 | Batch_idx: 20 |  Loss_1: (0.0081) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 110 | Batch_idx: 30 |  Loss_1: (0.0066) | Acc_1: (99.82%) (3961/3968)\n",
      "Epoch: 110 | Batch_idx: 40 |  Loss_1: (0.0064) | Acc_1: (99.81%) (5238/5248)\n",
      "Epoch: 110 | Batch_idx: 50 |  Loss_1: (0.0061) | Acc_1: (99.83%) (6517/6528)\n",
      "Epoch: 110 | Batch_idx: 60 |  Loss_1: (0.0056) | Acc_1: (99.85%) (7796/7808)\n",
      "Epoch: 110 | Batch_idx: 70 |  Loss_1: (0.0060) | Acc_1: (99.82%) (9072/9088)\n",
      "Epoch: 110 | Batch_idx: 80 |  Loss_1: (0.0060) | Acc_1: (99.83%) (10350/10368)\n",
      "Epoch: 110 | Batch_idx: 90 |  Loss_1: (0.0063) | Acc_1: (99.82%) (11627/11648)\n",
      "Epoch: 110 | Batch_idx: 100 |  Loss_1: (0.0067) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 110 | Batch_idx: 110 |  Loss_1: (0.0070) | Acc_1: (99.77%) (14176/14208)\n",
      "Epoch: 110 | Batch_idx: 120 |  Loss_1: (0.0067) | Acc_1: (99.78%) (15454/15488)\n",
      "Epoch: 110 | Batch_idx: 130 |  Loss_1: (0.0070) | Acc_1: (99.77%) (16729/16768)\n",
      "Epoch: 110 | Batch_idx: 140 |  Loss_1: (0.0067) | Acc_1: (99.77%) (18007/18048)\n",
      "Epoch: 110 | Batch_idx: 150 |  Loss_1: (0.0065) | Acc_1: (99.78%) (19286/19328)\n",
      "Epoch: 110 | Batch_idx: 160 |  Loss_1: (0.0063) | Acc_1: (99.79%) (20565/20608)\n",
      "Epoch: 110 | Batch_idx: 170 |  Loss_1: (0.0063) | Acc_1: (99.79%) (21842/21888)\n",
      "Epoch: 110 | Batch_idx: 180 |  Loss_1: (0.0063) | Acc_1: (99.78%) (23118/23168)\n",
      "Epoch: 110 | Batch_idx: 190 |  Loss_1: (0.0063) | Acc_1: (99.78%) (24395/24448)\n",
      "Epoch: 110 | Batch_idx: 200 |  Loss_1: (0.0063) | Acc_1: (99.78%) (25672/25728)\n",
      "Epoch: 110 | Batch_idx: 210 |  Loss_1: (0.0064) | Acc_1: (99.78%) (26949/27008)\n",
      "Epoch: 110 | Batch_idx: 220 |  Loss_1: (0.0064) | Acc_1: (99.78%) (28227/28288)\n",
      "Epoch: 110 | Batch_idx: 230 |  Loss_1: (0.0063) | Acc_1: (99.79%) (29505/29568)\n",
      "Epoch: 110 | Batch_idx: 240 |  Loss_1: (0.0064) | Acc_1: (99.78%) (30780/30848)\n",
      "Epoch: 110 | Batch_idx: 250 |  Loss_1: (0.0063) | Acc_1: (99.78%) (32057/32128)\n",
      "Epoch: 110 | Batch_idx: 260 |  Loss_1: (0.0063) | Acc_1: (99.78%) (33336/33408)\n",
      "Epoch: 110 | Batch_idx: 270 |  Loss_1: (0.0062) | Acc_1: (99.79%) (34614/34688)\n",
      "Epoch: 110 | Batch_idx: 280 |  Loss_1: (0.0063) | Acc_1: (99.78%) (35890/35968)\n",
      "Epoch: 110 | Batch_idx: 290 |  Loss_1: (0.0063) | Acc_1: (99.79%) (37168/37248)\n",
      "Epoch: 110 | Batch_idx: 300 |  Loss_1: (0.0066) | Acc_1: (99.78%) (38444/38528)\n",
      "Epoch: 110 | Batch_idx: 310 |  Loss_1: (0.0066) | Acc_1: (99.78%) (39722/39808)\n",
      "Epoch: 110 | Batch_idx: 320 |  Loss_1: (0.0067) | Acc_1: (99.78%) (40998/41088)\n",
      "Epoch: 110 | Batch_idx: 330 |  Loss_1: (0.0066) | Acc_1: (99.79%) (42278/42368)\n",
      "Epoch: 110 | Batch_idx: 340 |  Loss_1: (0.0065) | Acc_1: (99.79%) (43556/43648)\n",
      "Epoch: 110 | Batch_idx: 350 |  Loss_1: (0.0065) | Acc_1: (99.79%) (44834/44928)\n",
      "Epoch: 110 | Batch_idx: 360 |  Loss_1: (0.0066) | Acc_1: (99.79%) (46109/46208)\n",
      "Epoch: 110 | Batch_idx: 370 |  Loss_1: (0.0066) | Acc_1: (99.79%) (47386/47488)\n",
      "Epoch: 110 | Batch_idx: 380 |  Loss_1: (0.0069) | Acc_1: (99.77%) (48658/48768)\n",
      "Epoch: 110 | Batch_idx: 390 |  Loss_1: (0.0069) | Acc_1: (99.78%) (49889/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4687) | Acc: (92.05%) (9205/10000)\n",
      "Epoch: 111 | Batch_idx: 0 |  Loss_1: (0.0016) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 111 | Batch_idx: 10 |  Loss_1: (0.0048) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 111 | Batch_idx: 20 |  Loss_1: (0.0072) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 111 | Batch_idx: 30 |  Loss_1: (0.0088) | Acc_1: (99.62%) (3953/3968)\n",
      "Epoch: 111 | Batch_idx: 40 |  Loss_1: (0.0083) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 111 | Batch_idx: 50 |  Loss_1: (0.0087) | Acc_1: (99.63%) (6504/6528)\n",
      "Epoch: 111 | Batch_idx: 60 |  Loss_1: (0.0090) | Acc_1: (99.63%) (7779/7808)\n",
      "Epoch: 111 | Batch_idx: 70 |  Loss_1: (0.0083) | Acc_1: (99.66%) (9057/9088)\n",
      "Epoch: 111 | Batch_idx: 80 |  Loss_1: (0.0090) | Acc_1: (99.64%) (10331/10368)\n",
      "Epoch: 111 | Batch_idx: 90 |  Loss_1: (0.0097) | Acc_1: (99.63%) (11605/11648)\n",
      "Epoch: 111 | Batch_idx: 100 |  Loss_1: (0.0096) | Acc_1: (99.64%) (12882/12928)\n",
      "Epoch: 111 | Batch_idx: 110 |  Loss_1: (0.0094) | Acc_1: (99.65%) (14158/14208)\n",
      "Epoch: 111 | Batch_idx: 120 |  Loss_1: (0.0101) | Acc_1: (99.62%) (15429/15488)\n",
      "Epoch: 111 | Batch_idx: 130 |  Loss_1: (0.0099) | Acc_1: (99.63%) (16706/16768)\n",
      "Epoch: 111 | Batch_idx: 140 |  Loss_1: (0.0096) | Acc_1: (99.63%) (17982/18048)\n",
      "Epoch: 111 | Batch_idx: 150 |  Loss_1: (0.0093) | Acc_1: (99.65%) (19260/19328)\n",
      "Epoch: 111 | Batch_idx: 160 |  Loss_1: (0.0089) | Acc_1: (99.67%) (20539/20608)\n",
      "Epoch: 111 | Batch_idx: 170 |  Loss_1: (0.0088) | Acc_1: (99.67%) (21815/21888)\n",
      "Epoch: 111 | Batch_idx: 180 |  Loss_1: (0.0086) | Acc_1: (99.68%) (23093/23168)\n",
      "Epoch: 111 | Batch_idx: 190 |  Loss_1: (0.0086) | Acc_1: (99.69%) (24372/24448)\n",
      "Epoch: 111 | Batch_idx: 200 |  Loss_1: (0.0086) | Acc_1: (99.69%) (25647/25728)\n",
      "Epoch: 111 | Batch_idx: 210 |  Loss_1: (0.0087) | Acc_1: (99.68%) (26921/27008)\n",
      "Epoch: 111 | Batch_idx: 220 |  Loss_1: (0.0087) | Acc_1: (99.68%) (28198/28288)\n",
      "Epoch: 111 | Batch_idx: 230 |  Loss_1: (0.0088) | Acc_1: (99.68%) (29473/29568)\n",
      "Epoch: 111 | Batch_idx: 240 |  Loss_1: (0.0088) | Acc_1: (99.68%) (30750/30848)\n",
      "Epoch: 111 | Batch_idx: 250 |  Loss_1: (0.0088) | Acc_1: (99.68%) (32026/32128)\n",
      "Epoch: 111 | Batch_idx: 260 |  Loss_1: (0.0089) | Acc_1: (99.68%) (33302/33408)\n",
      "Epoch: 111 | Batch_idx: 270 |  Loss_1: (0.0091) | Acc_1: (99.67%) (34574/34688)\n",
      "Epoch: 111 | Batch_idx: 280 |  Loss_1: (0.0091) | Acc_1: (99.67%) (35849/35968)\n",
      "Epoch: 111 | Batch_idx: 290 |  Loss_1: (0.0091) | Acc_1: (99.67%) (37124/37248)\n",
      "Epoch: 111 | Batch_idx: 300 |  Loss_1: (0.0092) | Acc_1: (99.67%) (38401/38528)\n",
      "Epoch: 111 | Batch_idx: 310 |  Loss_1: (0.0093) | Acc_1: (99.66%) (39674/39808)\n",
      "Epoch: 111 | Batch_idx: 320 |  Loss_1: (0.0093) | Acc_1: (99.66%) (40950/41088)\n",
      "Epoch: 111 | Batch_idx: 330 |  Loss_1: (0.0093) | Acc_1: (99.66%) (42225/42368)\n",
      "Epoch: 111 | Batch_idx: 340 |  Loss_1: (0.0094) | Acc_1: (99.66%) (43498/43648)\n",
      "Epoch: 111 | Batch_idx: 350 |  Loss_1: (0.0093) | Acc_1: (99.66%) (44775/44928)\n",
      "Epoch: 111 | Batch_idx: 360 |  Loss_1: (0.0092) | Acc_1: (99.66%) (46053/46208)\n",
      "Epoch: 111 | Batch_idx: 370 |  Loss_1: (0.0092) | Acc_1: (99.67%) (47329/47488)\n",
      "Epoch: 111 | Batch_idx: 380 |  Loss_1: (0.0092) | Acc_1: (99.67%) (48606/48768)\n",
      "Epoch: 111 | Batch_idx: 390 |  Loss_1: (0.0091) | Acc_1: (99.67%) (49835/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4554) | Acc: (92.27%) (9227/10000)\n",
      "Epoch: 112 | Batch_idx: 0 |  Loss_1: (0.0013) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 112 | Batch_idx: 10 |  Loss_1: (0.0030) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 112 | Batch_idx: 20 |  Loss_1: (0.0055) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 112 | Batch_idx: 30 |  Loss_1: (0.0082) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 112 | Batch_idx: 40 |  Loss_1: (0.0068) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 112 | Batch_idx: 50 |  Loss_1: (0.0064) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 112 | Batch_idx: 60 |  Loss_1: (0.0067) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 112 | Batch_idx: 70 |  Loss_1: (0.0071) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 112 | Batch_idx: 80 |  Loss_1: (0.0072) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 112 | Batch_idx: 90 |  Loss_1: (0.0071) | Acc_1: (99.71%) (11614/11648)\n",
      "Epoch: 112 | Batch_idx: 100 |  Loss_1: (0.0073) | Acc_1: (99.71%) (12890/12928)\n",
      "Epoch: 112 | Batch_idx: 110 |  Loss_1: (0.0073) | Acc_1: (99.70%) (14166/14208)\n",
      "Epoch: 112 | Batch_idx: 120 |  Loss_1: (0.0072) | Acc_1: (99.71%) (15443/15488)\n",
      "Epoch: 112 | Batch_idx: 130 |  Loss_1: (0.0070) | Acc_1: (99.73%) (16722/16768)\n",
      "Epoch: 112 | Batch_idx: 140 |  Loss_1: (0.0068) | Acc_1: (99.73%) (18000/18048)\n",
      "Epoch: 112 | Batch_idx: 150 |  Loss_1: (0.0067) | Acc_1: (99.75%) (19279/19328)\n",
      "Epoch: 112 | Batch_idx: 160 |  Loss_1: (0.0066) | Acc_1: (99.75%) (20557/20608)\n",
      "Epoch: 112 | Batch_idx: 170 |  Loss_1: (0.0064) | Acc_1: (99.76%) (21836/21888)\n",
      "Epoch: 112 | Batch_idx: 180 |  Loss_1: (0.0062) | Acc_1: (99.77%) (23115/23168)\n",
      "Epoch: 112 | Batch_idx: 190 |  Loss_1: (0.0061) | Acc_1: (99.78%) (24393/24448)\n",
      "Epoch: 112 | Batch_idx: 200 |  Loss_1: (0.0060) | Acc_1: (99.77%) (25670/25728)\n",
      "Epoch: 112 | Batch_idx: 210 |  Loss_1: (0.0058) | Acc_1: (99.79%) (26950/27008)\n",
      "Epoch: 112 | Batch_idx: 220 |  Loss_1: (0.0057) | Acc_1: (99.79%) (28229/28288)\n",
      "Epoch: 112 | Batch_idx: 230 |  Loss_1: (0.0056) | Acc_1: (99.80%) (29508/29568)\n",
      "Epoch: 112 | Batch_idx: 240 |  Loss_1: (0.0058) | Acc_1: (99.79%) (30784/30848)\n",
      "Epoch: 112 | Batch_idx: 250 |  Loss_1: (0.0059) | Acc_1: (99.79%) (32060/32128)\n",
      "Epoch: 112 | Batch_idx: 260 |  Loss_1: (0.0060) | Acc_1: (99.78%) (33335/33408)\n",
      "Epoch: 112 | Batch_idx: 270 |  Loss_1: (0.0062) | Acc_1: (99.78%) (34611/34688)\n",
      "Epoch: 112 | Batch_idx: 280 |  Loss_1: (0.0063) | Acc_1: (99.77%) (35885/35968)\n",
      "Epoch: 112 | Batch_idx: 290 |  Loss_1: (0.0062) | Acc_1: (99.77%) (37164/37248)\n",
      "Epoch: 112 | Batch_idx: 300 |  Loss_1: (0.0062) | Acc_1: (99.78%) (38442/38528)\n",
      "Epoch: 112 | Batch_idx: 310 |  Loss_1: (0.0063) | Acc_1: (99.78%) (39719/39808)\n",
      "Epoch: 112 | Batch_idx: 320 |  Loss_1: (0.0064) | Acc_1: (99.77%) (40993/41088)\n",
      "Epoch: 112 | Batch_idx: 330 |  Loss_1: (0.0064) | Acc_1: (99.77%) (42270/42368)\n",
      "Epoch: 112 | Batch_idx: 340 |  Loss_1: (0.0063) | Acc_1: (99.77%) (43549/43648)\n",
      "Epoch: 112 | Batch_idx: 350 |  Loss_1: (0.0063) | Acc_1: (99.77%) (44825/44928)\n",
      "Epoch: 112 | Batch_idx: 360 |  Loss_1: (0.0062) | Acc_1: (99.77%) (46103/46208)\n",
      "Epoch: 112 | Batch_idx: 370 |  Loss_1: (0.0062) | Acc_1: (99.77%) (47379/47488)\n",
      "Epoch: 112 | Batch_idx: 380 |  Loss_1: (0.0064) | Acc_1: (99.77%) (48654/48768)\n",
      "Epoch: 112 | Batch_idx: 390 |  Loss_1: (0.0064) | Acc_1: (99.76%) (49881/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4738) | Acc: (92.40%) (9240/10000)\n",
      "Epoch: 113 | Batch_idx: 0 |  Loss_1: (0.0004) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 113 | Batch_idx: 10 |  Loss_1: (0.0028) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 113 | Batch_idx: 20 |  Loss_1: (0.0045) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 113 | Batch_idx: 30 |  Loss_1: (0.0053) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 113 | Batch_idx: 40 |  Loss_1: (0.0062) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 113 | Batch_idx: 50 |  Loss_1: (0.0055) | Acc_1: (99.80%) (6515/6528)\n",
      "Epoch: 113 | Batch_idx: 60 |  Loss_1: (0.0062) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 113 | Batch_idx: 70 |  Loss_1: (0.0057) | Acc_1: (99.80%) (9070/9088)\n",
      "Epoch: 113 | Batch_idx: 80 |  Loss_1: (0.0058) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 113 | Batch_idx: 90 |  Loss_1: (0.0056) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 113 | Batch_idx: 100 |  Loss_1: (0.0058) | Acc_1: (99.80%) (12902/12928)\n",
      "Epoch: 113 | Batch_idx: 110 |  Loss_1: (0.0059) | Acc_1: (99.80%) (14179/14208)\n",
      "Epoch: 113 | Batch_idx: 120 |  Loss_1: (0.0059) | Acc_1: (99.80%) (15457/15488)\n",
      "Epoch: 113 | Batch_idx: 130 |  Loss_1: (0.0059) | Acc_1: (99.80%) (16735/16768)\n",
      "Epoch: 113 | Batch_idx: 140 |  Loss_1: (0.0060) | Acc_1: (99.80%) (18012/18048)\n",
      "Epoch: 113 | Batch_idx: 150 |  Loss_1: (0.0062) | Acc_1: (99.80%) (19290/19328)\n",
      "Epoch: 113 | Batch_idx: 160 |  Loss_1: (0.0063) | Acc_1: (99.79%) (20565/20608)\n",
      "Epoch: 113 | Batch_idx: 170 |  Loss_1: (0.0062) | Acc_1: (99.80%) (21844/21888)\n",
      "Epoch: 113 | Batch_idx: 180 |  Loss_1: (0.0063) | Acc_1: (99.79%) (23119/23168)\n",
      "Epoch: 113 | Batch_idx: 190 |  Loss_1: (0.0063) | Acc_1: (99.79%) (24396/24448)\n",
      "Epoch: 113 | Batch_idx: 200 |  Loss_1: (0.0066) | Acc_1: (99.79%) (25673/25728)\n",
      "Epoch: 113 | Batch_idx: 210 |  Loss_1: (0.0066) | Acc_1: (99.79%) (26951/27008)\n",
      "Epoch: 113 | Batch_idx: 220 |  Loss_1: (0.0066) | Acc_1: (99.79%) (28228/28288)\n",
      "Epoch: 113 | Batch_idx: 230 |  Loss_1: (0.0066) | Acc_1: (99.79%) (29505/29568)\n",
      "Epoch: 113 | Batch_idx: 240 |  Loss_1: (0.0066) | Acc_1: (99.79%) (30784/30848)\n",
      "Epoch: 113 | Batch_idx: 250 |  Loss_1: (0.0064) | Acc_1: (99.80%) (32063/32128)\n",
      "Epoch: 113 | Batch_idx: 260 |  Loss_1: (0.0066) | Acc_1: (99.79%) (33338/33408)\n",
      "Epoch: 113 | Batch_idx: 270 |  Loss_1: (0.0068) | Acc_1: (99.78%) (34611/34688)\n",
      "Epoch: 113 | Batch_idx: 280 |  Loss_1: (0.0069) | Acc_1: (99.78%) (35888/35968)\n",
      "Epoch: 113 | Batch_idx: 290 |  Loss_1: (0.0071) | Acc_1: (99.77%) (37163/37248)\n",
      "Epoch: 113 | Batch_idx: 300 |  Loss_1: (0.0072) | Acc_1: (99.77%) (38441/38528)\n",
      "Epoch: 113 | Batch_idx: 310 |  Loss_1: (0.0071) | Acc_1: (99.78%) (39721/39808)\n",
      "Epoch: 113 | Batch_idx: 320 |  Loss_1: (0.0071) | Acc_1: (99.78%) (40998/41088)\n",
      "Epoch: 113 | Batch_idx: 330 |  Loss_1: (0.0072) | Acc_1: (99.78%) (42275/42368)\n",
      "Epoch: 113 | Batch_idx: 340 |  Loss_1: (0.0071) | Acc_1: (99.78%) (43553/43648)\n",
      "Epoch: 113 | Batch_idx: 350 |  Loss_1: (0.0070) | Acc_1: (99.79%) (44833/44928)\n",
      "Epoch: 113 | Batch_idx: 360 |  Loss_1: (0.0069) | Acc_1: (99.79%) (46110/46208)\n",
      "Epoch: 113 | Batch_idx: 370 |  Loss_1: (0.0069) | Acc_1: (99.79%) (47387/47488)\n",
      "Epoch: 113 | Batch_idx: 380 |  Loss_1: (0.0068) | Acc_1: (99.79%) (48665/48768)\n",
      "Epoch: 113 | Batch_idx: 390 |  Loss_1: (0.0068) | Acc_1: (99.79%) (49895/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4871) | Acc: (92.22%) (9222/10000)\n",
      "Epoch: 114 | Batch_idx: 0 |  Loss_1: (0.0043) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 114 | Batch_idx: 10 |  Loss_1: (0.0031) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 114 | Batch_idx: 20 |  Loss_1: (0.0055) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 114 | Batch_idx: 30 |  Loss_1: (0.0066) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 114 | Batch_idx: 40 |  Loss_1: (0.0066) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 114 | Batch_idx: 50 |  Loss_1: (0.0065) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 114 | Batch_idx: 60 |  Loss_1: (0.0065) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 114 | Batch_idx: 70 |  Loss_1: (0.0063) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 114 | Batch_idx: 80 |  Loss_1: (0.0061) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 114 | Batch_idx: 90 |  Loss_1: (0.0070) | Acc_1: (99.78%) (11622/11648)\n",
      "Epoch: 114 | Batch_idx: 100 |  Loss_1: (0.0076) | Acc_1: (99.74%) (12894/12928)\n",
      "Epoch: 114 | Batch_idx: 110 |  Loss_1: (0.0078) | Acc_1: (99.73%) (14170/14208)\n",
      "Epoch: 114 | Batch_idx: 120 |  Loss_1: (0.0078) | Acc_1: (99.73%) (15446/15488)\n",
      "Epoch: 114 | Batch_idx: 130 |  Loss_1: (0.0076) | Acc_1: (99.74%) (16724/16768)\n",
      "Epoch: 114 | Batch_idx: 140 |  Loss_1: (0.0075) | Acc_1: (99.73%) (18000/18048)\n",
      "Epoch: 114 | Batch_idx: 150 |  Loss_1: (0.0075) | Acc_1: (99.74%) (19277/19328)\n",
      "Epoch: 114 | Batch_idx: 160 |  Loss_1: (0.0075) | Acc_1: (99.73%) (20553/20608)\n",
      "Epoch: 114 | Batch_idx: 170 |  Loss_1: (0.0074) | Acc_1: (99.74%) (21831/21888)\n",
      "Epoch: 114 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.75%) (23111/23168)\n",
      "Epoch: 114 | Batch_idx: 190 |  Loss_1: (0.0071) | Acc_1: (99.75%) (24388/24448)\n",
      "Epoch: 114 | Batch_idx: 200 |  Loss_1: (0.0072) | Acc_1: (99.75%) (25664/25728)\n",
      "Epoch: 114 | Batch_idx: 210 |  Loss_1: (0.0071) | Acc_1: (99.75%) (26940/27008)\n",
      "Epoch: 114 | Batch_idx: 220 |  Loss_1: (0.0071) | Acc_1: (99.75%) (28217/28288)\n",
      "Epoch: 114 | Batch_idx: 230 |  Loss_1: (0.0071) | Acc_1: (99.75%) (29494/29568)\n",
      "Epoch: 114 | Batch_idx: 240 |  Loss_1: (0.0072) | Acc_1: (99.75%) (30770/30848)\n",
      "Epoch: 114 | Batch_idx: 250 |  Loss_1: (0.0073) | Acc_1: (99.75%) (32047/32128)\n",
      "Epoch: 114 | Batch_idx: 260 |  Loss_1: (0.0074) | Acc_1: (99.74%) (33320/33408)\n",
      "Epoch: 114 | Batch_idx: 270 |  Loss_1: (0.0074) | Acc_1: (99.73%) (34596/34688)\n",
      "Epoch: 114 | Batch_idx: 280 |  Loss_1: (0.0073) | Acc_1: (99.74%) (35873/35968)\n",
      "Epoch: 114 | Batch_idx: 290 |  Loss_1: (0.0073) | Acc_1: (99.73%) (37149/37248)\n",
      "Epoch: 114 | Batch_idx: 300 |  Loss_1: (0.0073) | Acc_1: (99.74%) (38426/38528)\n",
      "Epoch: 114 | Batch_idx: 310 |  Loss_1: (0.0073) | Acc_1: (99.74%) (39703/39808)\n",
      "Epoch: 114 | Batch_idx: 320 |  Loss_1: (0.0073) | Acc_1: (99.74%) (40980/41088)\n",
      "Epoch: 114 | Batch_idx: 330 |  Loss_1: (0.0074) | Acc_1: (99.73%) (42254/42368)\n",
      "Epoch: 114 | Batch_idx: 340 |  Loss_1: (0.0075) | Acc_1: (99.73%) (43530/43648)\n",
      "Epoch: 114 | Batch_idx: 350 |  Loss_1: (0.0077) | Acc_1: (99.72%) (44804/44928)\n",
      "Epoch: 114 | Batch_idx: 360 |  Loss_1: (0.0078) | Acc_1: (99.72%) (46078/46208)\n",
      "Epoch: 114 | Batch_idx: 370 |  Loss_1: (0.0078) | Acc_1: (99.72%) (47355/47488)\n",
      "Epoch: 114 | Batch_idx: 380 |  Loss_1: (0.0078) | Acc_1: (99.71%) (48628/48768)\n",
      "Epoch: 114 | Batch_idx: 390 |  Loss_1: (0.0080) | Acc_1: (99.71%) (49856/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4783) | Acc: (92.24%) (9224/10000)\n",
      "Epoch: 115 | Batch_idx: 0 |  Loss_1: (0.0016) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 115 | Batch_idx: 10 |  Loss_1: (0.0118) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 115 | Batch_idx: 20 |  Loss_1: (0.0093) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 115 | Batch_idx: 30 |  Loss_1: (0.0082) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 115 | Batch_idx: 40 |  Loss_1: (0.0090) | Acc_1: (99.68%) (5231/5248)\n",
      "Epoch: 115 | Batch_idx: 50 |  Loss_1: (0.0091) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 115 | Batch_idx: 60 |  Loss_1: (0.0094) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 115 | Batch_idx: 70 |  Loss_1: (0.0086) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 115 | Batch_idx: 80 |  Loss_1: (0.0079) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 115 | Batch_idx: 90 |  Loss_1: (0.0075) | Acc_1: (99.75%) (11619/11648)\n",
      "Epoch: 115 | Batch_idx: 100 |  Loss_1: (0.0074) | Acc_1: (99.74%) (12895/12928)\n",
      "Epoch: 115 | Batch_idx: 110 |  Loss_1: (0.0078) | Acc_1: (99.73%) (14169/14208)\n",
      "Epoch: 115 | Batch_idx: 120 |  Loss_1: (0.0075) | Acc_1: (99.73%) (15446/15488)\n",
      "Epoch: 115 | Batch_idx: 130 |  Loss_1: (0.0076) | Acc_1: (99.73%) (16723/16768)\n",
      "Epoch: 115 | Batch_idx: 140 |  Loss_1: (0.0080) | Acc_1: (99.72%) (17997/18048)\n",
      "Epoch: 115 | Batch_idx: 150 |  Loss_1: (0.0081) | Acc_1: (99.72%) (19274/19328)\n",
      "Epoch: 115 | Batch_idx: 160 |  Loss_1: (0.0078) | Acc_1: (99.73%) (20553/20608)\n",
      "Epoch: 115 | Batch_idx: 170 |  Loss_1: (0.0077) | Acc_1: (99.73%) (21829/21888)\n",
      "Epoch: 115 | Batch_idx: 180 |  Loss_1: (0.0074) | Acc_1: (99.74%) (23108/23168)\n",
      "Epoch: 115 | Batch_idx: 190 |  Loss_1: (0.0079) | Acc_1: (99.72%) (24380/24448)\n",
      "Epoch: 115 | Batch_idx: 200 |  Loss_1: (0.0079) | Acc_1: (99.73%) (25658/25728)\n",
      "Epoch: 115 | Batch_idx: 210 |  Loss_1: (0.0080) | Acc_1: (99.73%) (26936/27008)\n",
      "Epoch: 115 | Batch_idx: 220 |  Loss_1: (0.0080) | Acc_1: (99.73%) (28213/28288)\n",
      "Epoch: 115 | Batch_idx: 230 |  Loss_1: (0.0078) | Acc_1: (99.74%) (29491/29568)\n",
      "Epoch: 115 | Batch_idx: 240 |  Loss_1: (0.0078) | Acc_1: (99.73%) (30766/30848)\n",
      "Epoch: 115 | Batch_idx: 250 |  Loss_1: (0.0079) | Acc_1: (99.73%) (32042/32128)\n",
      "Epoch: 115 | Batch_idx: 260 |  Loss_1: (0.0078) | Acc_1: (99.74%) (33320/33408)\n",
      "Epoch: 115 | Batch_idx: 270 |  Loss_1: (0.0076) | Acc_1: (99.74%) (34599/34688)\n",
      "Epoch: 115 | Batch_idx: 280 |  Loss_1: (0.0077) | Acc_1: (99.75%) (35877/35968)\n",
      "Epoch: 115 | Batch_idx: 290 |  Loss_1: (0.0076) | Acc_1: (99.75%) (37156/37248)\n",
      "Epoch: 115 | Batch_idx: 300 |  Loss_1: (0.0077) | Acc_1: (99.75%) (38433/38528)\n",
      "Epoch: 115 | Batch_idx: 310 |  Loss_1: (0.0077) | Acc_1: (99.75%) (39710/39808)\n",
      "Epoch: 115 | Batch_idx: 320 |  Loss_1: (0.0076) | Acc_1: (99.76%) (40989/41088)\n",
      "Epoch: 115 | Batch_idx: 330 |  Loss_1: (0.0074) | Acc_1: (99.77%) (42269/42368)\n",
      "Epoch: 115 | Batch_idx: 340 |  Loss_1: (0.0075) | Acc_1: (99.77%) (43547/43648)\n",
      "Epoch: 115 | Batch_idx: 350 |  Loss_1: (0.0074) | Acc_1: (99.77%) (44826/44928)\n",
      "Epoch: 115 | Batch_idx: 360 |  Loss_1: (0.0074) | Acc_1: (99.77%) (46104/46208)\n",
      "Epoch: 115 | Batch_idx: 370 |  Loss_1: (0.0073) | Acc_1: (99.78%) (47382/47488)\n",
      "Epoch: 115 | Batch_idx: 380 |  Loss_1: (0.0072) | Acc_1: (99.78%) (48661/48768)\n",
      "Epoch: 115 | Batch_idx: 390 |  Loss_1: (0.0072) | Acc_1: (99.78%) (49890/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4413) | Acc: (92.58%) (9258/10000)\n",
      "Epoch: 116 | Batch_idx: 0 |  Loss_1: (0.0013) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 116 | Batch_idx: 10 |  Loss_1: (0.0054) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 116 | Batch_idx: 20 |  Loss_1: (0.0039) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 116 | Batch_idx: 30 |  Loss_1: (0.0046) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 116 | Batch_idx: 40 |  Loss_1: (0.0044) | Acc_1: (99.89%) (5242/5248)\n",
      "Epoch: 116 | Batch_idx: 50 |  Loss_1: (0.0051) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 116 | Batch_idx: 60 |  Loss_1: (0.0045) | Acc_1: (99.88%) (7799/7808)\n",
      "Epoch: 116 | Batch_idx: 70 |  Loss_1: (0.0050) | Acc_1: (99.86%) (9075/9088)\n",
      "Epoch: 116 | Batch_idx: 80 |  Loss_1: (0.0052) | Acc_1: (99.85%) (10352/10368)\n",
      "Epoch: 116 | Batch_idx: 90 |  Loss_1: (0.0051) | Acc_1: (99.84%) (11629/11648)\n",
      "Epoch: 116 | Batch_idx: 100 |  Loss_1: (0.0054) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 116 | Batch_idx: 110 |  Loss_1: (0.0058) | Acc_1: (99.80%) (14179/14208)\n",
      "Epoch: 116 | Batch_idx: 120 |  Loss_1: (0.0059) | Acc_1: (99.80%) (15457/15488)\n",
      "Epoch: 116 | Batch_idx: 130 |  Loss_1: (0.0057) | Acc_1: (99.82%) (16737/16768)\n",
      "Epoch: 116 | Batch_idx: 140 |  Loss_1: (0.0059) | Acc_1: (99.80%) (18012/18048)\n",
      "Epoch: 116 | Batch_idx: 150 |  Loss_1: (0.0061) | Acc_1: (99.79%) (19287/19328)\n",
      "Epoch: 116 | Batch_idx: 160 |  Loss_1: (0.0060) | Acc_1: (99.80%) (20566/20608)\n",
      "Epoch: 116 | Batch_idx: 170 |  Loss_1: (0.0058) | Acc_1: (99.80%) (21845/21888)\n",
      "Epoch: 116 | Batch_idx: 180 |  Loss_1: (0.0057) | Acc_1: (99.81%) (23123/23168)\n",
      "Epoch: 116 | Batch_idx: 190 |  Loss_1: (0.0057) | Acc_1: (99.80%) (24400/24448)\n",
      "Epoch: 116 | Batch_idx: 200 |  Loss_1: (0.0057) | Acc_1: (99.81%) (25679/25728)\n",
      "Epoch: 116 | Batch_idx: 210 |  Loss_1: (0.0058) | Acc_1: (99.81%) (26956/27008)\n",
      "Epoch: 116 | Batch_idx: 220 |  Loss_1: (0.0058) | Acc_1: (99.81%) (28234/28288)\n",
      "Epoch: 116 | Batch_idx: 230 |  Loss_1: (0.0058) | Acc_1: (99.81%) (29511/29568)\n",
      "Epoch: 116 | Batch_idx: 240 |  Loss_1: (0.0057) | Acc_1: (99.81%) (30789/30848)\n",
      "Epoch: 116 | Batch_idx: 250 |  Loss_1: (0.0057) | Acc_1: (99.81%) (32066/32128)\n",
      "Epoch: 116 | Batch_idx: 260 |  Loss_1: (0.0056) | Acc_1: (99.81%) (33344/33408)\n",
      "Epoch: 116 | Batch_idx: 270 |  Loss_1: (0.0056) | Acc_1: (99.81%) (34623/34688)\n",
      "Epoch: 116 | Batch_idx: 280 |  Loss_1: (0.0055) | Acc_1: (99.81%) (35901/35968)\n",
      "Epoch: 116 | Batch_idx: 290 |  Loss_1: (0.0055) | Acc_1: (99.82%) (37180/37248)\n",
      "Epoch: 116 | Batch_idx: 300 |  Loss_1: (0.0054) | Acc_1: (99.82%) (38459/38528)\n",
      "Epoch: 116 | Batch_idx: 310 |  Loss_1: (0.0054) | Acc_1: (99.82%) (39738/39808)\n",
      "Epoch: 116 | Batch_idx: 320 |  Loss_1: (0.0053) | Acc_1: (99.82%) (41016/41088)\n",
      "Epoch: 116 | Batch_idx: 330 |  Loss_1: (0.0053) | Acc_1: (99.83%) (42295/42368)\n",
      "Epoch: 116 | Batch_idx: 340 |  Loss_1: (0.0053) | Acc_1: (99.83%) (43573/43648)\n",
      "Epoch: 116 | Batch_idx: 350 |  Loss_1: (0.0053) | Acc_1: (99.82%) (44849/44928)\n",
      "Epoch: 116 | Batch_idx: 360 |  Loss_1: (0.0054) | Acc_1: (99.81%) (46122/46208)\n",
      "Epoch: 116 | Batch_idx: 370 |  Loss_1: (0.0054) | Acc_1: (99.82%) (47401/47488)\n",
      "Epoch: 116 | Batch_idx: 380 |  Loss_1: (0.0054) | Acc_1: (99.82%) (48681/48768)\n",
      "Epoch: 116 | Batch_idx: 390 |  Loss_1: (0.0054) | Acc_1: (99.82%) (49912/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4709) | Acc: (92.45%) (9245/10000)\n",
      "Epoch: 117 | Batch_idx: 0 |  Loss_1: (0.0023) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 117 | Batch_idx: 10 |  Loss_1: (0.0035) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 117 | Batch_idx: 20 |  Loss_1: (0.0060) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 117 | Batch_idx: 30 |  Loss_1: (0.0060) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 117 | Batch_idx: 40 |  Loss_1: (0.0067) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 117 | Batch_idx: 50 |  Loss_1: (0.0061) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 117 | Batch_idx: 60 |  Loss_1: (0.0060) | Acc_1: (99.78%) (7791/7808)\n",
      "Epoch: 117 | Batch_idx: 70 |  Loss_1: (0.0057) | Acc_1: (99.80%) (9070/9088)\n",
      "Epoch: 117 | Batch_idx: 80 |  Loss_1: (0.0052) | Acc_1: (99.82%) (10349/10368)\n",
      "Epoch: 117 | Batch_idx: 90 |  Loss_1: (0.0058) | Acc_1: (99.79%) (11624/11648)\n",
      "Epoch: 117 | Batch_idx: 100 |  Loss_1: (0.0056) | Acc_1: (99.80%) (12902/12928)\n",
      "Epoch: 117 | Batch_idx: 110 |  Loss_1: (0.0054) | Acc_1: (99.81%) (14181/14208)\n",
      "Epoch: 117 | Batch_idx: 120 |  Loss_1: (0.0052) | Acc_1: (99.83%) (15461/15488)\n",
      "Epoch: 117 | Batch_idx: 130 |  Loss_1: (0.0056) | Acc_1: (99.82%) (16738/16768)\n",
      "Epoch: 117 | Batch_idx: 140 |  Loss_1: (0.0062) | Acc_1: (99.79%) (18011/18048)\n",
      "Epoch: 117 | Batch_idx: 150 |  Loss_1: (0.0063) | Acc_1: (99.80%) (19289/19328)\n",
      "Epoch: 117 | Batch_idx: 160 |  Loss_1: (0.0062) | Acc_1: (99.81%) (20568/20608)\n",
      "Epoch: 117 | Batch_idx: 170 |  Loss_1: (0.0061) | Acc_1: (99.80%) (21845/21888)\n",
      "Epoch: 117 | Batch_idx: 180 |  Loss_1: (0.0062) | Acc_1: (99.80%) (23121/23168)\n",
      "Epoch: 117 | Batch_idx: 190 |  Loss_1: (0.0064) | Acc_1: (99.80%) (24398/24448)\n",
      "Epoch: 117 | Batch_idx: 200 |  Loss_1: (0.0063) | Acc_1: (99.80%) (25676/25728)\n",
      "Epoch: 117 | Batch_idx: 210 |  Loss_1: (0.0065) | Acc_1: (99.79%) (26951/27008)\n",
      "Epoch: 117 | Batch_idx: 220 |  Loss_1: (0.0064) | Acc_1: (99.79%) (28230/28288)\n",
      "Epoch: 117 | Batch_idx: 230 |  Loss_1: (0.0063) | Acc_1: (99.80%) (29508/29568)\n",
      "Epoch: 117 | Batch_idx: 240 |  Loss_1: (0.0063) | Acc_1: (99.79%) (30784/30848)\n",
      "Epoch: 117 | Batch_idx: 250 |  Loss_1: (0.0063) | Acc_1: (99.79%) (32060/32128)\n",
      "Epoch: 117 | Batch_idx: 260 |  Loss_1: (0.0062) | Acc_1: (99.79%) (33339/33408)\n",
      "Epoch: 117 | Batch_idx: 270 |  Loss_1: (0.0061) | Acc_1: (99.80%) (34618/34688)\n",
      "Epoch: 117 | Batch_idx: 280 |  Loss_1: (0.0059) | Acc_1: (99.80%) (35897/35968)\n",
      "Epoch: 117 | Batch_idx: 290 |  Loss_1: (0.0058) | Acc_1: (99.81%) (37176/37248)\n",
      "Epoch: 117 | Batch_idx: 300 |  Loss_1: (0.0059) | Acc_1: (99.81%) (38453/38528)\n",
      "Epoch: 117 | Batch_idx: 310 |  Loss_1: (0.0058) | Acc_1: (99.81%) (39733/39808)\n",
      "Epoch: 117 | Batch_idx: 320 |  Loss_1: (0.0056) | Acc_1: (99.82%) (41012/41088)\n",
      "Epoch: 117 | Batch_idx: 330 |  Loss_1: (0.0057) | Acc_1: (99.81%) (42289/42368)\n",
      "Epoch: 117 | Batch_idx: 340 |  Loss_1: (0.0056) | Acc_1: (99.82%) (43568/43648)\n",
      "Epoch: 117 | Batch_idx: 350 |  Loss_1: (0.0055) | Acc_1: (99.82%) (44848/44928)\n",
      "Epoch: 117 | Batch_idx: 360 |  Loss_1: (0.0056) | Acc_1: (99.82%) (46126/46208)\n",
      "Epoch: 117 | Batch_idx: 370 |  Loss_1: (0.0057) | Acc_1: (99.82%) (47403/47488)\n",
      "Epoch: 117 | Batch_idx: 380 |  Loss_1: (0.0058) | Acc_1: (99.82%) (48681/48768)\n",
      "Epoch: 117 | Batch_idx: 390 |  Loss_1: (0.0057) | Acc_1: (99.82%) (49912/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4711) | Acc: (92.39%) (9239/10000)\n",
      "Epoch: 118 | Batch_idx: 0 |  Loss_1: (0.0026) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 118 | Batch_idx: 10 |  Loss_1: (0.0039) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 118 | Batch_idx: 20 |  Loss_1: (0.0036) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 118 | Batch_idx: 30 |  Loss_1: (0.0035) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 118 | Batch_idx: 40 |  Loss_1: (0.0044) | Acc_1: (99.90%) (5243/5248)\n",
      "Epoch: 118 | Batch_idx: 50 |  Loss_1: (0.0046) | Acc_1: (99.89%) (6521/6528)\n",
      "Epoch: 118 | Batch_idx: 60 |  Loss_1: (0.0045) | Acc_1: (99.88%) (7799/7808)\n",
      "Epoch: 118 | Batch_idx: 70 |  Loss_1: (0.0055) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 118 | Batch_idx: 80 |  Loss_1: (0.0062) | Acc_1: (99.83%) (10350/10368)\n",
      "Epoch: 118 | Batch_idx: 90 |  Loss_1: (0.0063) | Acc_1: (99.83%) (11628/11648)\n",
      "Epoch: 118 | Batch_idx: 100 |  Loss_1: (0.0062) | Acc_1: (99.82%) (12905/12928)\n",
      "Epoch: 118 | Batch_idx: 110 |  Loss_1: (0.0062) | Acc_1: (99.82%) (14182/14208)\n",
      "Epoch: 118 | Batch_idx: 120 |  Loss_1: (0.0059) | Acc_1: (99.83%) (15461/15488)\n",
      "Epoch: 118 | Batch_idx: 130 |  Loss_1: (0.0056) | Acc_1: (99.84%) (16741/16768)\n",
      "Epoch: 118 | Batch_idx: 140 |  Loss_1: (0.0055) | Acc_1: (99.83%) (18018/18048)\n",
      "Epoch: 118 | Batch_idx: 150 |  Loss_1: (0.0052) | Acc_1: (99.84%) (19298/19328)\n",
      "Epoch: 118 | Batch_idx: 160 |  Loss_1: (0.0051) | Acc_1: (99.84%) (20576/20608)\n",
      "Epoch: 118 | Batch_idx: 170 |  Loss_1: (0.0050) | Acc_1: (99.85%) (21855/21888)\n",
      "Epoch: 118 | Batch_idx: 180 |  Loss_1: (0.0049) | Acc_1: (99.85%) (23134/23168)\n",
      "Epoch: 118 | Batch_idx: 190 |  Loss_1: (0.0047) | Acc_1: (99.86%) (24414/24448)\n",
      "Epoch: 118 | Batch_idx: 200 |  Loss_1: (0.0049) | Acc_1: (99.85%) (25689/25728)\n",
      "Epoch: 118 | Batch_idx: 210 |  Loss_1: (0.0048) | Acc_1: (99.85%) (26968/27008)\n",
      "Epoch: 118 | Batch_idx: 220 |  Loss_1: (0.0052) | Acc_1: (99.83%) (28241/28288)\n",
      "Epoch: 118 | Batch_idx: 230 |  Loss_1: (0.0051) | Acc_1: (99.84%) (29520/29568)\n",
      "Epoch: 118 | Batch_idx: 240 |  Loss_1: (0.0051) | Acc_1: (99.84%) (30798/30848)\n",
      "Epoch: 118 | Batch_idx: 250 |  Loss_1: (0.0051) | Acc_1: (99.84%) (32076/32128)\n",
      "Epoch: 118 | Batch_idx: 260 |  Loss_1: (0.0051) | Acc_1: (99.83%) (33352/33408)\n",
      "Epoch: 118 | Batch_idx: 270 |  Loss_1: (0.0051) | Acc_1: (99.83%) (34629/34688)\n",
      "Epoch: 118 | Batch_idx: 280 |  Loss_1: (0.0053) | Acc_1: (99.82%) (35905/35968)\n",
      "Epoch: 118 | Batch_idx: 290 |  Loss_1: (0.0053) | Acc_1: (99.83%) (37183/37248)\n",
      "Epoch: 118 | Batch_idx: 300 |  Loss_1: (0.0054) | Acc_1: (99.82%) (38460/38528)\n",
      "Epoch: 118 | Batch_idx: 310 |  Loss_1: (0.0055) | Acc_1: (99.82%) (39735/39808)\n",
      "Epoch: 118 | Batch_idx: 320 |  Loss_1: (0.0053) | Acc_1: (99.82%) (41014/41088)\n",
      "Epoch: 118 | Batch_idx: 330 |  Loss_1: (0.0053) | Acc_1: (99.82%) (42292/42368)\n",
      "Epoch: 118 | Batch_idx: 340 |  Loss_1: (0.0052) | Acc_1: (99.83%) (43572/43648)\n",
      "Epoch: 118 | Batch_idx: 350 |  Loss_1: (0.0051) | Acc_1: (99.83%) (44851/44928)\n",
      "Epoch: 118 | Batch_idx: 360 |  Loss_1: (0.0050) | Acc_1: (99.83%) (46130/46208)\n",
      "Epoch: 118 | Batch_idx: 370 |  Loss_1: (0.0052) | Acc_1: (99.83%) (47407/47488)\n",
      "Epoch: 118 | Batch_idx: 380 |  Loss_1: (0.0051) | Acc_1: (99.83%) (48687/48768)\n",
      "Epoch: 118 | Batch_idx: 390 |  Loss_1: (0.0051) | Acc_1: (99.84%) (49918/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4561) | Acc: (92.34%) (9234/10000)\n",
      "Epoch: 119 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 119 | Batch_idx: 10 |  Loss_1: (0.0026) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 119 | Batch_idx: 20 |  Loss_1: (0.0031) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 119 | Batch_idx: 30 |  Loss_1: (0.0043) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 119 | Batch_idx: 40 |  Loss_1: (0.0051) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 119 | Batch_idx: 50 |  Loss_1: (0.0055) | Acc_1: (99.82%) (6516/6528)\n",
      "Epoch: 119 | Batch_idx: 60 |  Loss_1: (0.0062) | Acc_1: (99.80%) (7792/7808)\n",
      "Epoch: 119 | Batch_idx: 70 |  Loss_1: (0.0064) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 119 | Batch_idx: 80 |  Loss_1: (0.0072) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 119 | Batch_idx: 90 |  Loss_1: (0.0069) | Acc_1: (99.74%) (11618/11648)\n",
      "Epoch: 119 | Batch_idx: 100 |  Loss_1: (0.0068) | Acc_1: (99.74%) (12895/12928)\n",
      "Epoch: 119 | Batch_idx: 110 |  Loss_1: (0.0067) | Acc_1: (99.75%) (14172/14208)\n",
      "Epoch: 119 | Batch_idx: 120 |  Loss_1: (0.0064) | Acc_1: (99.75%) (15450/15488)\n",
      "Epoch: 119 | Batch_idx: 130 |  Loss_1: (0.0062) | Acc_1: (99.76%) (16728/16768)\n",
      "Epoch: 119 | Batch_idx: 140 |  Loss_1: (0.0062) | Acc_1: (99.77%) (18006/18048)\n",
      "Epoch: 119 | Batch_idx: 150 |  Loss_1: (0.0059) | Acc_1: (99.78%) (19286/19328)\n",
      "Epoch: 119 | Batch_idx: 160 |  Loss_1: (0.0057) | Acc_1: (99.79%) (20565/20608)\n",
      "Epoch: 119 | Batch_idx: 170 |  Loss_1: (0.0056) | Acc_1: (99.79%) (21843/21888)\n",
      "Epoch: 119 | Batch_idx: 180 |  Loss_1: (0.0055) | Acc_1: (99.80%) (23122/23168)\n",
      "Epoch: 119 | Batch_idx: 190 |  Loss_1: (0.0053) | Acc_1: (99.81%) (24402/24448)\n",
      "Epoch: 119 | Batch_idx: 200 |  Loss_1: (0.0051) | Acc_1: (99.82%) (25681/25728)\n",
      "Epoch: 119 | Batch_idx: 210 |  Loss_1: (0.0050) | Acc_1: (99.83%) (26961/27008)\n",
      "Epoch: 119 | Batch_idx: 220 |  Loss_1: (0.0050) | Acc_1: (99.82%) (28238/28288)\n",
      "Epoch: 119 | Batch_idx: 230 |  Loss_1: (0.0048) | Acc_1: (99.83%) (29518/29568)\n",
      "Epoch: 119 | Batch_idx: 240 |  Loss_1: (0.0047) | Acc_1: (99.83%) (30796/30848)\n",
      "Epoch: 119 | Batch_idx: 250 |  Loss_1: (0.0048) | Acc_1: (99.83%) (32072/32128)\n",
      "Epoch: 119 | Batch_idx: 260 |  Loss_1: (0.0047) | Acc_1: (99.83%) (33351/33408)\n",
      "Epoch: 119 | Batch_idx: 270 |  Loss_1: (0.0047) | Acc_1: (99.83%) (34628/34688)\n",
      "Epoch: 119 | Batch_idx: 280 |  Loss_1: (0.0046) | Acc_1: (99.83%) (35908/35968)\n",
      "Epoch: 119 | Batch_idx: 290 |  Loss_1: (0.0045) | Acc_1: (99.84%) (37188/37248)\n",
      "Epoch: 119 | Batch_idx: 300 |  Loss_1: (0.0044) | Acc_1: (99.84%) (38467/38528)\n",
      "Epoch: 119 | Batch_idx: 310 |  Loss_1: (0.0044) | Acc_1: (99.84%) (39745/39808)\n",
      "Epoch: 119 | Batch_idx: 320 |  Loss_1: (0.0044) | Acc_1: (99.84%) (41023/41088)\n",
      "Epoch: 119 | Batch_idx: 330 |  Loss_1: (0.0043) | Acc_1: (99.85%) (42303/42368)\n",
      "Epoch: 119 | Batch_idx: 340 |  Loss_1: (0.0043) | Acc_1: (99.85%) (43581/43648)\n",
      "Epoch: 119 | Batch_idx: 350 |  Loss_1: (0.0042) | Acc_1: (99.85%) (44861/44928)\n",
      "Epoch: 119 | Batch_idx: 360 |  Loss_1: (0.0042) | Acc_1: (99.85%) (46140/46208)\n",
      "Epoch: 119 | Batch_idx: 370 |  Loss_1: (0.0042) | Acc_1: (99.85%) (47417/47488)\n",
      "Epoch: 119 | Batch_idx: 380 |  Loss_1: (0.0042) | Acc_1: (99.85%) (48696/48768)\n",
      "Epoch: 119 | Batch_idx: 390 |  Loss_1: (0.0041) | Acc_1: (99.85%) (49927/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4736) | Acc: (92.63%) (9263/10000)\n",
      "Epoch: 120 | Batch_idx: 0 |  Loss_1: (0.0020) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 120 | Batch_idx: 10 |  Loss_1: (0.0072) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 120 | Batch_idx: 20 |  Loss_1: (0.0045) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 120 | Batch_idx: 30 |  Loss_1: (0.0036) | Acc_1: (99.90%) (3964/3968)\n",
      "Epoch: 120 | Batch_idx: 40 |  Loss_1: (0.0032) | Acc_1: (99.90%) (5243/5248)\n",
      "Epoch: 120 | Batch_idx: 50 |  Loss_1: (0.0029) | Acc_1: (99.92%) (6523/6528)\n",
      "Epoch: 120 | Batch_idx: 60 |  Loss_1: (0.0026) | Acc_1: (99.94%) (7803/7808)\n",
      "Epoch: 120 | Batch_idx: 70 |  Loss_1: (0.0033) | Acc_1: (99.91%) (9080/9088)\n",
      "Epoch: 120 | Batch_idx: 80 |  Loss_1: (0.0030) | Acc_1: (99.92%) (10360/10368)\n",
      "Epoch: 120 | Batch_idx: 90 |  Loss_1: (0.0038) | Acc_1: (99.90%) (11636/11648)\n",
      "Epoch: 120 | Batch_idx: 100 |  Loss_1: (0.0036) | Acc_1: (99.90%) (12915/12928)\n",
      "Epoch: 120 | Batch_idx: 110 |  Loss_1: (0.0039) | Acc_1: (99.89%) (14193/14208)\n",
      "Epoch: 120 | Batch_idx: 120 |  Loss_1: (0.0039) | Acc_1: (99.89%) (15471/15488)\n",
      "Epoch: 120 | Batch_idx: 130 |  Loss_1: (0.0041) | Acc_1: (99.89%) (16749/16768)\n",
      "Epoch: 120 | Batch_idx: 140 |  Loss_1: (0.0040) | Acc_1: (99.89%) (18028/18048)\n",
      "Epoch: 120 | Batch_idx: 150 |  Loss_1: (0.0040) | Acc_1: (99.89%) (19307/19328)\n",
      "Epoch: 120 | Batch_idx: 160 |  Loss_1: (0.0041) | Acc_1: (99.87%) (20582/20608)\n",
      "Epoch: 120 | Batch_idx: 170 |  Loss_1: (0.0042) | Acc_1: (99.87%) (21860/21888)\n",
      "Epoch: 120 | Batch_idx: 180 |  Loss_1: (0.0041) | Acc_1: (99.87%) (23139/23168)\n",
      "Epoch: 120 | Batch_idx: 190 |  Loss_1: (0.0042) | Acc_1: (99.87%) (24415/24448)\n",
      "Epoch: 120 | Batch_idx: 200 |  Loss_1: (0.0042) | Acc_1: (99.86%) (25693/25728)\n",
      "Epoch: 120 | Batch_idx: 210 |  Loss_1: (0.0043) | Acc_1: (99.86%) (26970/27008)\n",
      "Epoch: 120 | Batch_idx: 220 |  Loss_1: (0.0043) | Acc_1: (99.86%) (28248/28288)\n",
      "Epoch: 120 | Batch_idx: 230 |  Loss_1: (0.0042) | Acc_1: (99.86%) (29528/29568)\n",
      "Epoch: 120 | Batch_idx: 240 |  Loss_1: (0.0041) | Acc_1: (99.86%) (30806/30848)\n",
      "Epoch: 120 | Batch_idx: 250 |  Loss_1: (0.0040) | Acc_1: (99.87%) (32086/32128)\n",
      "Epoch: 120 | Batch_idx: 260 |  Loss_1: (0.0039) | Acc_1: (99.87%) (33365/33408)\n",
      "Epoch: 120 | Batch_idx: 270 |  Loss_1: (0.0039) | Acc_1: (99.87%) (34644/34688)\n",
      "Epoch: 120 | Batch_idx: 280 |  Loss_1: (0.0039) | Acc_1: (99.87%) (35921/35968)\n",
      "Epoch: 120 | Batch_idx: 290 |  Loss_1: (0.0040) | Acc_1: (99.86%) (37197/37248)\n",
      "Epoch: 120 | Batch_idx: 300 |  Loss_1: (0.0040) | Acc_1: (99.87%) (38476/38528)\n",
      "Epoch: 120 | Batch_idx: 310 |  Loss_1: (0.0042) | Acc_1: (99.86%) (39753/39808)\n",
      "Epoch: 120 | Batch_idx: 320 |  Loss_1: (0.0041) | Acc_1: (99.86%) (41031/41088)\n",
      "Epoch: 120 | Batch_idx: 330 |  Loss_1: (0.0041) | Acc_1: (99.86%) (42310/42368)\n",
      "Epoch: 120 | Batch_idx: 340 |  Loss_1: (0.0041) | Acc_1: (99.86%) (43587/43648)\n",
      "Epoch: 120 | Batch_idx: 350 |  Loss_1: (0.0040) | Acc_1: (99.86%) (44865/44928)\n",
      "Epoch: 120 | Batch_idx: 360 |  Loss_1: (0.0040) | Acc_1: (99.86%) (46143/46208)\n",
      "Epoch: 120 | Batch_idx: 370 |  Loss_1: (0.0040) | Acc_1: (99.86%) (47421/47488)\n",
      "Epoch: 120 | Batch_idx: 380 |  Loss_1: (0.0040) | Acc_1: (99.86%) (48700/48768)\n",
      "Epoch: 120 | Batch_idx: 390 |  Loss_1: (0.0041) | Acc_1: (99.86%) (49929/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4570) | Acc: (92.89%) (9289/10000)\n",
      "Epoch: 121 | Batch_idx: 0 |  Loss_1: (0.0007) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 121 | Batch_idx: 10 |  Loss_1: (0.0027) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 121 | Batch_idx: 20 |  Loss_1: (0.0040) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 121 | Batch_idx: 30 |  Loss_1: (0.0044) | Acc_1: (99.82%) (3961/3968)\n",
      "Epoch: 121 | Batch_idx: 40 |  Loss_1: (0.0039) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 121 | Batch_idx: 50 |  Loss_1: (0.0047) | Acc_1: (99.82%) (6516/6528)\n",
      "Epoch: 121 | Batch_idx: 60 |  Loss_1: (0.0044) | Acc_1: (99.83%) (7795/7808)\n",
      "Epoch: 121 | Batch_idx: 70 |  Loss_1: (0.0056) | Acc_1: (99.79%) (9069/9088)\n",
      "Epoch: 121 | Batch_idx: 80 |  Loss_1: (0.0052) | Acc_1: (99.81%) (10348/10368)\n",
      "Epoch: 121 | Batch_idx: 90 |  Loss_1: (0.0050) | Acc_1: (99.82%) (11627/11648)\n",
      "Epoch: 121 | Batch_idx: 100 |  Loss_1: (0.0051) | Acc_1: (99.81%) (12903/12928)\n",
      "Epoch: 121 | Batch_idx: 110 |  Loss_1: (0.0049) | Acc_1: (99.81%) (14181/14208)\n",
      "Epoch: 121 | Batch_idx: 120 |  Loss_1: (0.0047) | Acc_1: (99.83%) (15461/15488)\n",
      "Epoch: 121 | Batch_idx: 130 |  Loss_1: (0.0048) | Acc_1: (99.82%) (16738/16768)\n",
      "Epoch: 121 | Batch_idx: 140 |  Loss_1: (0.0049) | Acc_1: (99.82%) (18016/18048)\n",
      "Epoch: 121 | Batch_idx: 150 |  Loss_1: (0.0047) | Acc_1: (99.83%) (19296/19328)\n",
      "Epoch: 121 | Batch_idx: 160 |  Loss_1: (0.0046) | Acc_1: (99.84%) (20575/20608)\n",
      "Epoch: 121 | Batch_idx: 170 |  Loss_1: (0.0045) | Acc_1: (99.84%) (21854/21888)\n",
      "Epoch: 121 | Batch_idx: 180 |  Loss_1: (0.0046) | Acc_1: (99.84%) (23132/23168)\n",
      "Epoch: 121 | Batch_idx: 190 |  Loss_1: (0.0044) | Acc_1: (99.85%) (24412/24448)\n",
      "Epoch: 121 | Batch_idx: 200 |  Loss_1: (0.0044) | Acc_1: (99.85%) (25689/25728)\n",
      "Epoch: 121 | Batch_idx: 210 |  Loss_1: (0.0043) | Acc_1: (99.85%) (26968/27008)\n",
      "Epoch: 121 | Batch_idx: 220 |  Loss_1: (0.0043) | Acc_1: (99.86%) (28248/28288)\n",
      "Epoch: 121 | Batch_idx: 230 |  Loss_1: (0.0042) | Acc_1: (99.86%) (29527/29568)\n",
      "Epoch: 121 | Batch_idx: 240 |  Loss_1: (0.0043) | Acc_1: (99.85%) (30803/30848)\n",
      "Epoch: 121 | Batch_idx: 250 |  Loss_1: (0.0042) | Acc_1: (99.86%) (32083/32128)\n",
      "Epoch: 121 | Batch_idx: 260 |  Loss_1: (0.0042) | Acc_1: (99.86%) (33362/33408)\n",
      "Epoch: 121 | Batch_idx: 270 |  Loss_1: (0.0040) | Acc_1: (99.87%) (34642/34688)\n",
      "Epoch: 121 | Batch_idx: 280 |  Loss_1: (0.0040) | Acc_1: (99.87%) (35921/35968)\n",
      "Epoch: 121 | Batch_idx: 290 |  Loss_1: (0.0039) | Acc_1: (99.87%) (37200/37248)\n",
      "Epoch: 121 | Batch_idx: 300 |  Loss_1: (0.0039) | Acc_1: (99.87%) (38479/38528)\n",
      "Epoch: 121 | Batch_idx: 310 |  Loss_1: (0.0038) | Acc_1: (99.87%) (39758/39808)\n",
      "Epoch: 121 | Batch_idx: 320 |  Loss_1: (0.0038) | Acc_1: (99.87%) (41036/41088)\n",
      "Epoch: 121 | Batch_idx: 330 |  Loss_1: (0.0038) | Acc_1: (99.87%) (42315/42368)\n",
      "Epoch: 121 | Batch_idx: 340 |  Loss_1: (0.0037) | Acc_1: (99.88%) (43594/43648)\n",
      "Epoch: 121 | Batch_idx: 350 |  Loss_1: (0.0037) | Acc_1: (99.88%) (44873/44928)\n",
      "Epoch: 121 | Batch_idx: 360 |  Loss_1: (0.0037) | Acc_1: (99.88%) (46151/46208)\n",
      "Epoch: 121 | Batch_idx: 370 |  Loss_1: (0.0037) | Acc_1: (99.88%) (47429/47488)\n",
      "Epoch: 121 | Batch_idx: 380 |  Loss_1: (0.0037) | Acc_1: (99.88%) (48709/48768)\n",
      "Epoch: 121 | Batch_idx: 390 |  Loss_1: (0.0036) | Acc_1: (99.88%) (49941/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4663) | Acc: (92.73%) (9273/10000)\n",
      "Epoch: 122 | Batch_idx: 0 |  Loss_1: (0.0009) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 122 | Batch_idx: 10 |  Loss_1: (0.0024) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 122 | Batch_idx: 20 |  Loss_1: (0.0033) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 122 | Batch_idx: 30 |  Loss_1: (0.0026) | Acc_1: (99.90%) (3964/3968)\n",
      "Epoch: 122 | Batch_idx: 40 |  Loss_1: (0.0038) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 122 | Batch_idx: 50 |  Loss_1: (0.0040) | Acc_1: (99.85%) (6518/6528)\n",
      "Epoch: 122 | Batch_idx: 60 |  Loss_1: (0.0039) | Acc_1: (99.85%) (7796/7808)\n",
      "Epoch: 122 | Batch_idx: 70 |  Loss_1: (0.0037) | Acc_1: (99.86%) (9075/9088)\n",
      "Epoch: 122 | Batch_idx: 80 |  Loss_1: (0.0036) | Acc_1: (99.86%) (10354/10368)\n",
      "Epoch: 122 | Batch_idx: 90 |  Loss_1: (0.0035) | Acc_1: (99.87%) (11633/11648)\n",
      "Epoch: 122 | Batch_idx: 100 |  Loss_1: (0.0034) | Acc_1: (99.88%) (12912/12928)\n",
      "Epoch: 122 | Batch_idx: 110 |  Loss_1: (0.0035) | Acc_1: (99.87%) (14190/14208)\n",
      "Epoch: 122 | Batch_idx: 120 |  Loss_1: (0.0032) | Acc_1: (99.88%) (15470/15488)\n",
      "Epoch: 122 | Batch_idx: 130 |  Loss_1: (0.0031) | Acc_1: (99.89%) (16749/16768)\n",
      "Epoch: 122 | Batch_idx: 140 |  Loss_1: (0.0034) | Acc_1: (99.88%) (18027/18048)\n",
      "Epoch: 122 | Batch_idx: 150 |  Loss_1: (0.0033) | Acc_1: (99.89%) (19307/19328)\n",
      "Epoch: 122 | Batch_idx: 160 |  Loss_1: (0.0034) | Acc_1: (99.89%) (20585/20608)\n",
      "Epoch: 122 | Batch_idx: 170 |  Loss_1: (0.0034) | Acc_1: (99.89%) (21865/21888)\n",
      "Epoch: 122 | Batch_idx: 180 |  Loss_1: (0.0034) | Acc_1: (99.89%) (23143/23168)\n",
      "Epoch: 122 | Batch_idx: 190 |  Loss_1: (0.0035) | Acc_1: (99.89%) (24420/24448)\n",
      "Epoch: 122 | Batch_idx: 200 |  Loss_1: (0.0034) | Acc_1: (99.88%) (25698/25728)\n",
      "Epoch: 122 | Batch_idx: 210 |  Loss_1: (0.0037) | Acc_1: (99.87%) (26974/27008)\n",
      "Epoch: 122 | Batch_idx: 220 |  Loss_1: (0.0037) | Acc_1: (99.87%) (28251/28288)\n",
      "Epoch: 122 | Batch_idx: 230 |  Loss_1: (0.0036) | Acc_1: (99.87%) (29531/29568)\n",
      "Epoch: 122 | Batch_idx: 240 |  Loss_1: (0.0036) | Acc_1: (99.87%) (30809/30848)\n",
      "Epoch: 122 | Batch_idx: 250 |  Loss_1: (0.0035) | Acc_1: (99.88%) (32088/32128)\n",
      "Epoch: 122 | Batch_idx: 260 |  Loss_1: (0.0034) | Acc_1: (99.88%) (33368/33408)\n",
      "Epoch: 122 | Batch_idx: 270 |  Loss_1: (0.0033) | Acc_1: (99.88%) (34648/34688)\n",
      "Epoch: 122 | Batch_idx: 280 |  Loss_1: (0.0034) | Acc_1: (99.89%) (35927/35968)\n",
      "Epoch: 122 | Batch_idx: 290 |  Loss_1: (0.0033) | Acc_1: (99.89%) (37207/37248)\n",
      "Epoch: 122 | Batch_idx: 300 |  Loss_1: (0.0032) | Acc_1: (99.89%) (38486/38528)\n",
      "Epoch: 122 | Batch_idx: 310 |  Loss_1: (0.0033) | Acc_1: (99.89%) (39764/39808)\n",
      "Epoch: 122 | Batch_idx: 320 |  Loss_1: (0.0033) | Acc_1: (99.89%) (41044/41088)\n",
      "Epoch: 122 | Batch_idx: 330 |  Loss_1: (0.0033) | Acc_1: (99.89%) (42322/42368)\n",
      "Epoch: 122 | Batch_idx: 340 |  Loss_1: (0.0032) | Acc_1: (99.89%) (43601/43648)\n",
      "Epoch: 122 | Batch_idx: 350 |  Loss_1: (0.0032) | Acc_1: (99.89%) (44880/44928)\n",
      "Epoch: 122 | Batch_idx: 360 |  Loss_1: (0.0031) | Acc_1: (99.90%) (46160/46208)\n",
      "Epoch: 122 | Batch_idx: 370 |  Loss_1: (0.0031) | Acc_1: (99.89%) (47438/47488)\n",
      "Epoch: 122 | Batch_idx: 380 |  Loss_1: (0.0031) | Acc_1: (99.89%) (48715/48768)\n",
      "Epoch: 122 | Batch_idx: 390 |  Loss_1: (0.0031) | Acc_1: (99.89%) (49946/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4579) | Acc: (92.86%) (9286/10000)\n",
      "Epoch: 123 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 123 | Batch_idx: 10 |  Loss_1: (0.0025) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 123 | Batch_idx: 20 |  Loss_1: (0.0029) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 123 | Batch_idx: 30 |  Loss_1: (0.0043) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 123 | Batch_idx: 40 |  Loss_1: (0.0036) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 123 | Batch_idx: 50 |  Loss_1: (0.0032) | Acc_1: (99.85%) (6518/6528)\n",
      "Epoch: 123 | Batch_idx: 60 |  Loss_1: (0.0030) | Acc_1: (99.87%) (7798/7808)\n",
      "Epoch: 123 | Batch_idx: 70 |  Loss_1: (0.0027) | Acc_1: (99.89%) (9078/9088)\n",
      "Epoch: 123 | Batch_idx: 80 |  Loss_1: (0.0033) | Acc_1: (99.86%) (10354/10368)\n",
      "Epoch: 123 | Batch_idx: 90 |  Loss_1: (0.0030) | Acc_1: (99.88%) (11634/11648)\n",
      "Epoch: 123 | Batch_idx: 100 |  Loss_1: (0.0031) | Acc_1: (99.88%) (12912/12928)\n",
      "Epoch: 123 | Batch_idx: 110 |  Loss_1: (0.0034) | Acc_1: (99.87%) (14189/14208)\n",
      "Epoch: 123 | Batch_idx: 120 |  Loss_1: (0.0035) | Acc_1: (99.86%) (15466/15488)\n",
      "Epoch: 123 | Batch_idx: 130 |  Loss_1: (0.0034) | Acc_1: (99.86%) (16745/16768)\n",
      "Epoch: 123 | Batch_idx: 140 |  Loss_1: (0.0033) | Acc_1: (99.87%) (18025/18048)\n",
      "Epoch: 123 | Batch_idx: 150 |  Loss_1: (0.0032) | Acc_1: (99.88%) (19305/19328)\n",
      "Epoch: 123 | Batch_idx: 160 |  Loss_1: (0.0031) | Acc_1: (99.89%) (20585/20608)\n",
      "Epoch: 123 | Batch_idx: 170 |  Loss_1: (0.0031) | Acc_1: (99.89%) (21864/21888)\n",
      "Epoch: 123 | Batch_idx: 180 |  Loss_1: (0.0031) | Acc_1: (99.89%) (23142/23168)\n",
      "Epoch: 123 | Batch_idx: 190 |  Loss_1: (0.0031) | Acc_1: (99.89%) (24421/24448)\n",
      "Epoch: 123 | Batch_idx: 200 |  Loss_1: (0.0030) | Acc_1: (99.90%) (25701/25728)\n",
      "Epoch: 123 | Batch_idx: 210 |  Loss_1: (0.0030) | Acc_1: (99.90%) (26980/27008)\n",
      "Epoch: 123 | Batch_idx: 220 |  Loss_1: (0.0030) | Acc_1: (99.89%) (28258/28288)\n",
      "Epoch: 123 | Batch_idx: 230 |  Loss_1: (0.0033) | Acc_1: (99.88%) (29533/29568)\n",
      "Epoch: 123 | Batch_idx: 240 |  Loss_1: (0.0032) | Acc_1: (99.88%) (30812/30848)\n",
      "Epoch: 123 | Batch_idx: 250 |  Loss_1: (0.0035) | Acc_1: (99.87%) (32086/32128)\n",
      "Epoch: 123 | Batch_idx: 260 |  Loss_1: (0.0034) | Acc_1: (99.87%) (33366/33408)\n",
      "Epoch: 123 | Batch_idx: 270 |  Loss_1: (0.0035) | Acc_1: (99.87%) (34643/34688)\n",
      "Epoch: 123 | Batch_idx: 280 |  Loss_1: (0.0036) | Acc_1: (99.87%) (35921/35968)\n",
      "Epoch: 123 | Batch_idx: 290 |  Loss_1: (0.0035) | Acc_1: (99.87%) (37201/37248)\n",
      "Epoch: 123 | Batch_idx: 300 |  Loss_1: (0.0036) | Acc_1: (99.87%) (38477/38528)\n",
      "Epoch: 123 | Batch_idx: 310 |  Loss_1: (0.0036) | Acc_1: (99.87%) (39756/39808)\n",
      "Epoch: 123 | Batch_idx: 320 |  Loss_1: (0.0036) | Acc_1: (99.87%) (41035/41088)\n",
      "Epoch: 123 | Batch_idx: 330 |  Loss_1: (0.0036) | Acc_1: (99.87%) (42313/42368)\n",
      "Epoch: 123 | Batch_idx: 340 |  Loss_1: (0.0036) | Acc_1: (99.87%) (43590/43648)\n",
      "Epoch: 123 | Batch_idx: 350 |  Loss_1: (0.0037) | Acc_1: (99.86%) (44866/44928)\n",
      "Epoch: 123 | Batch_idx: 360 |  Loss_1: (0.0038) | Acc_1: (99.86%) (46142/46208)\n",
      "Epoch: 123 | Batch_idx: 370 |  Loss_1: (0.0037) | Acc_1: (99.86%) (47422/47488)\n",
      "Epoch: 123 | Batch_idx: 380 |  Loss_1: (0.0039) | Acc_1: (99.85%) (48697/48768)\n",
      "Epoch: 123 | Batch_idx: 390 |  Loss_1: (0.0039) | Acc_1: (99.85%) (49926/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4711) | Acc: (92.55%) (9255/10000)\n",
      "Epoch: 124 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 124 | Batch_idx: 10 |  Loss_1: (0.0073) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 124 | Batch_idx: 20 |  Loss_1: (0.0059) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 124 | Batch_idx: 30 |  Loss_1: (0.0063) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 124 | Batch_idx: 40 |  Loss_1: (0.0053) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 124 | Batch_idx: 50 |  Loss_1: (0.0050) | Acc_1: (99.79%) (6514/6528)\n",
      "Epoch: 124 | Batch_idx: 60 |  Loss_1: (0.0049) | Acc_1: (99.81%) (7793/7808)\n",
      "Epoch: 124 | Batch_idx: 70 |  Loss_1: (0.0044) | Acc_1: (99.82%) (9072/9088)\n",
      "Epoch: 124 | Batch_idx: 80 |  Loss_1: (0.0045) | Acc_1: (99.82%) (10349/10368)\n",
      "Epoch: 124 | Batch_idx: 90 |  Loss_1: (0.0050) | Acc_1: (99.82%) (11627/11648)\n",
      "Epoch: 124 | Batch_idx: 100 |  Loss_1: (0.0047) | Acc_1: (99.83%) (12906/12928)\n",
      "Epoch: 124 | Batch_idx: 110 |  Loss_1: (0.0050) | Acc_1: (99.82%) (14183/14208)\n",
      "Epoch: 124 | Batch_idx: 120 |  Loss_1: (0.0047) | Acc_1: (99.84%) (15463/15488)\n",
      "Epoch: 124 | Batch_idx: 130 |  Loss_1: (0.0049) | Acc_1: (99.82%) (16738/16768)\n",
      "Epoch: 124 | Batch_idx: 140 |  Loss_1: (0.0051) | Acc_1: (99.81%) (18014/18048)\n",
      "Epoch: 124 | Batch_idx: 150 |  Loss_1: (0.0052) | Acc_1: (99.81%) (19292/19328)\n",
      "Epoch: 124 | Batch_idx: 160 |  Loss_1: (0.0050) | Acc_1: (99.82%) (20571/20608)\n",
      "Epoch: 124 | Batch_idx: 170 |  Loss_1: (0.0053) | Acc_1: (99.82%) (21848/21888)\n",
      "Epoch: 124 | Batch_idx: 180 |  Loss_1: (0.0050) | Acc_1: (99.83%) (23128/23168)\n",
      "Epoch: 124 | Batch_idx: 190 |  Loss_1: (0.0052) | Acc_1: (99.81%) (24402/24448)\n",
      "Epoch: 124 | Batch_idx: 200 |  Loss_1: (0.0051) | Acc_1: (99.82%) (25682/25728)\n",
      "Epoch: 124 | Batch_idx: 210 |  Loss_1: (0.0051) | Acc_1: (99.81%) (26958/27008)\n",
      "Epoch: 124 | Batch_idx: 220 |  Loss_1: (0.0052) | Acc_1: (99.81%) (28234/28288)\n",
      "Epoch: 124 | Batch_idx: 230 |  Loss_1: (0.0051) | Acc_1: (99.81%) (29511/29568)\n",
      "Epoch: 124 | Batch_idx: 240 |  Loss_1: (0.0050) | Acc_1: (99.81%) (30790/30848)\n",
      "Epoch: 124 | Batch_idx: 250 |  Loss_1: (0.0050) | Acc_1: (99.81%) (32068/32128)\n",
      "Epoch: 124 | Batch_idx: 260 |  Loss_1: (0.0052) | Acc_1: (99.81%) (33344/33408)\n",
      "Epoch: 124 | Batch_idx: 270 |  Loss_1: (0.0051) | Acc_1: (99.81%) (34623/34688)\n",
      "Epoch: 124 | Batch_idx: 280 |  Loss_1: (0.0051) | Acc_1: (99.81%) (35900/35968)\n",
      "Epoch: 124 | Batch_idx: 290 |  Loss_1: (0.0050) | Acc_1: (99.81%) (37179/37248)\n",
      "Epoch: 124 | Batch_idx: 300 |  Loss_1: (0.0050) | Acc_1: (99.81%) (38456/38528)\n",
      "Epoch: 124 | Batch_idx: 310 |  Loss_1: (0.0051) | Acc_1: (99.81%) (39733/39808)\n",
      "Epoch: 124 | Batch_idx: 320 |  Loss_1: (0.0050) | Acc_1: (99.82%) (41012/41088)\n",
      "Epoch: 124 | Batch_idx: 330 |  Loss_1: (0.0049) | Acc_1: (99.82%) (42291/42368)\n",
      "Epoch: 124 | Batch_idx: 340 |  Loss_1: (0.0049) | Acc_1: (99.82%) (43568/43648)\n",
      "Epoch: 124 | Batch_idx: 350 |  Loss_1: (0.0048) | Acc_1: (99.82%) (44847/44928)\n",
      "Epoch: 124 | Batch_idx: 360 |  Loss_1: (0.0048) | Acc_1: (99.82%) (46126/46208)\n",
      "Epoch: 124 | Batch_idx: 370 |  Loss_1: (0.0047) | Acc_1: (99.83%) (47406/47488)\n",
      "Epoch: 124 | Batch_idx: 380 |  Loss_1: (0.0046) | Acc_1: (99.83%) (48686/48768)\n",
      "Epoch: 124 | Batch_idx: 390 |  Loss_1: (0.0045) | Acc_1: (99.84%) (49918/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4361) | Acc: (93.08%) (9308/10000)\n",
      "Epoch: 125 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 125 | Batch_idx: 10 |  Loss_1: (0.0050) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 125 | Batch_idx: 20 |  Loss_1: (0.0035) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 125 | Batch_idx: 30 |  Loss_1: (0.0044) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 125 | Batch_idx: 40 |  Loss_1: (0.0042) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 125 | Batch_idx: 50 |  Loss_1: (0.0042) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 125 | Batch_idx: 60 |  Loss_1: (0.0040) | Acc_1: (99.86%) (7797/7808)\n",
      "Epoch: 125 | Batch_idx: 70 |  Loss_1: (0.0039) | Acc_1: (99.87%) (9076/9088)\n",
      "Epoch: 125 | Batch_idx: 80 |  Loss_1: (0.0039) | Acc_1: (99.86%) (10354/10368)\n",
      "Epoch: 125 | Batch_idx: 90 |  Loss_1: (0.0036) | Acc_1: (99.88%) (11634/11648)\n",
      "Epoch: 125 | Batch_idx: 100 |  Loss_1: (0.0034) | Acc_1: (99.89%) (12914/12928)\n",
      "Epoch: 125 | Batch_idx: 110 |  Loss_1: (0.0034) | Acc_1: (99.89%) (14193/14208)\n",
      "Epoch: 125 | Batch_idx: 120 |  Loss_1: (0.0032) | Acc_1: (99.90%) (15473/15488)\n",
      "Epoch: 125 | Batch_idx: 130 |  Loss_1: (0.0032) | Acc_1: (99.90%) (16752/16768)\n",
      "Epoch: 125 | Batch_idx: 140 |  Loss_1: (0.0032) | Acc_1: (99.90%) (18030/18048)\n",
      "Epoch: 125 | Batch_idx: 150 |  Loss_1: (0.0034) | Acc_1: (99.89%) (19306/19328)\n",
      "Epoch: 125 | Batch_idx: 160 |  Loss_1: (0.0034) | Acc_1: (99.88%) (20584/20608)\n",
      "Epoch: 125 | Batch_idx: 170 |  Loss_1: (0.0035) | Acc_1: (99.87%) (21860/21888)\n",
      "Epoch: 125 | Batch_idx: 180 |  Loss_1: (0.0035) | Acc_1: (99.87%) (23138/23168)\n",
      "Epoch: 125 | Batch_idx: 190 |  Loss_1: (0.0033) | Acc_1: (99.88%) (24418/24448)\n",
      "Epoch: 125 | Batch_idx: 200 |  Loss_1: (0.0034) | Acc_1: (99.88%) (25697/25728)\n",
      "Epoch: 125 | Batch_idx: 210 |  Loss_1: (0.0033) | Acc_1: (99.88%) (26976/27008)\n",
      "Epoch: 125 | Batch_idx: 220 |  Loss_1: (0.0032) | Acc_1: (99.88%) (28255/28288)\n",
      "Epoch: 125 | Batch_idx: 230 |  Loss_1: (0.0033) | Acc_1: (99.89%) (29534/29568)\n",
      "Epoch: 125 | Batch_idx: 240 |  Loss_1: (0.0032) | Acc_1: (99.89%) (30814/30848)\n",
      "Epoch: 125 | Batch_idx: 250 |  Loss_1: (0.0033) | Acc_1: (99.89%) (32093/32128)\n",
      "Epoch: 125 | Batch_idx: 260 |  Loss_1: (0.0034) | Acc_1: (99.88%) (33369/33408)\n",
      "Epoch: 125 | Batch_idx: 270 |  Loss_1: (0.0034) | Acc_1: (99.88%) (34647/34688)\n",
      "Epoch: 125 | Batch_idx: 280 |  Loss_1: (0.0034) | Acc_1: (99.89%) (35927/35968)\n",
      "Epoch: 125 | Batch_idx: 290 |  Loss_1: (0.0033) | Acc_1: (99.89%) (37207/37248)\n",
      "Epoch: 125 | Batch_idx: 300 |  Loss_1: (0.0032) | Acc_1: (99.89%) (38487/38528)\n",
      "Epoch: 125 | Batch_idx: 310 |  Loss_1: (0.0032) | Acc_1: (99.89%) (39766/39808)\n",
      "Epoch: 125 | Batch_idx: 320 |  Loss_1: (0.0031) | Acc_1: (99.90%) (41046/41088)\n",
      "Epoch: 125 | Batch_idx: 330 |  Loss_1: (0.0032) | Acc_1: (99.89%) (42323/42368)\n",
      "Epoch: 125 | Batch_idx: 340 |  Loss_1: (0.0031) | Acc_1: (99.89%) (43602/43648)\n",
      "Epoch: 125 | Batch_idx: 350 |  Loss_1: (0.0031) | Acc_1: (99.90%) (44882/44928)\n",
      "Epoch: 125 | Batch_idx: 360 |  Loss_1: (0.0031) | Acc_1: (99.90%) (46160/46208)\n",
      "Epoch: 125 | Batch_idx: 370 |  Loss_1: (0.0031) | Acc_1: (99.90%) (47439/47488)\n",
      "Epoch: 125 | Batch_idx: 380 |  Loss_1: (0.0031) | Acc_1: (99.90%) (48718/48768)\n",
      "Epoch: 125 | Batch_idx: 390 |  Loss_1: (0.0031) | Acc_1: (99.89%) (49947/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4696) | Acc: (92.91%) (9291/10000)\n",
      "Epoch: 126 | Batch_idx: 0 |  Loss_1: (0.0020) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 126 | Batch_idx: 10 |  Loss_1: (0.0013) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 126 | Batch_idx: 20 |  Loss_1: (0.0014) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 126 | Batch_idx: 30 |  Loss_1: (0.0020) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 126 | Batch_idx: 40 |  Loss_1: (0.0019) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 126 | Batch_idx: 50 |  Loss_1: (0.0018) | Acc_1: (99.95%) (6525/6528)\n",
      "Epoch: 126 | Batch_idx: 60 |  Loss_1: (0.0017) | Acc_1: (99.96%) (7805/7808)\n",
      "Epoch: 126 | Batch_idx: 70 |  Loss_1: (0.0019) | Acc_1: (99.96%) (9084/9088)\n",
      "Epoch: 126 | Batch_idx: 80 |  Loss_1: (0.0020) | Acc_1: (99.94%) (10362/10368)\n",
      "Epoch: 126 | Batch_idx: 90 |  Loss_1: (0.0019) | Acc_1: (99.95%) (11642/11648)\n",
      "Epoch: 126 | Batch_idx: 100 |  Loss_1: (0.0022) | Acc_1: (99.94%) (12920/12928)\n",
      "Epoch: 126 | Batch_idx: 110 |  Loss_1: (0.0022) | Acc_1: (99.94%) (14200/14208)\n",
      "Epoch: 126 | Batch_idx: 120 |  Loss_1: (0.0022) | Acc_1: (99.94%) (15478/15488)\n",
      "Epoch: 126 | Batch_idx: 130 |  Loss_1: (0.0022) | Acc_1: (99.94%) (16758/16768)\n",
      "Epoch: 126 | Batch_idx: 140 |  Loss_1: (0.0021) | Acc_1: (99.94%) (18038/18048)\n",
      "Epoch: 126 | Batch_idx: 150 |  Loss_1: (0.0023) | Acc_1: (99.94%) (19316/19328)\n",
      "Epoch: 126 | Batch_idx: 160 |  Loss_1: (0.0022) | Acc_1: (99.94%) (20596/20608)\n",
      "Epoch: 126 | Batch_idx: 170 |  Loss_1: (0.0025) | Acc_1: (99.94%) (21874/21888)\n",
      "Epoch: 126 | Batch_idx: 180 |  Loss_1: (0.0025) | Acc_1: (99.93%) (23152/23168)\n",
      "Epoch: 126 | Batch_idx: 190 |  Loss_1: (0.0025) | Acc_1: (99.93%) (24431/24448)\n",
      "Epoch: 126 | Batch_idx: 200 |  Loss_1: (0.0026) | Acc_1: (99.93%) (25709/25728)\n",
      "Epoch: 126 | Batch_idx: 210 |  Loss_1: (0.0025) | Acc_1: (99.93%) (26989/27008)\n",
      "Epoch: 126 | Batch_idx: 220 |  Loss_1: (0.0027) | Acc_1: (99.93%) (28267/28288)\n",
      "Epoch: 126 | Batch_idx: 230 |  Loss_1: (0.0028) | Acc_1: (99.92%) (29545/29568)\n",
      "Epoch: 126 | Batch_idx: 240 |  Loss_1: (0.0027) | Acc_1: (99.93%) (30825/30848)\n",
      "Epoch: 126 | Batch_idx: 250 |  Loss_1: (0.0026) | Acc_1: (99.93%) (32105/32128)\n",
      "Epoch: 126 | Batch_idx: 260 |  Loss_1: (0.0026) | Acc_1: (99.93%) (33384/33408)\n",
      "Epoch: 126 | Batch_idx: 270 |  Loss_1: (0.0026) | Acc_1: (99.93%) (34663/34688)\n",
      "Epoch: 126 | Batch_idx: 280 |  Loss_1: (0.0027) | Acc_1: (99.93%) (35942/35968)\n",
      "Epoch: 126 | Batch_idx: 290 |  Loss_1: (0.0027) | Acc_1: (99.92%) (37220/37248)\n",
      "Epoch: 126 | Batch_idx: 300 |  Loss_1: (0.0026) | Acc_1: (99.92%) (38499/38528)\n",
      "Epoch: 126 | Batch_idx: 310 |  Loss_1: (0.0027) | Acc_1: (99.92%) (39778/39808)\n",
      "Epoch: 126 | Batch_idx: 320 |  Loss_1: (0.0028) | Acc_1: (99.92%) (41054/41088)\n",
      "Epoch: 126 | Batch_idx: 330 |  Loss_1: (0.0028) | Acc_1: (99.92%) (42334/42368)\n",
      "Epoch: 126 | Batch_idx: 340 |  Loss_1: (0.0029) | Acc_1: (99.92%) (43612/43648)\n",
      "Epoch: 126 | Batch_idx: 350 |  Loss_1: (0.0029) | Acc_1: (99.92%) (44891/44928)\n",
      "Epoch: 126 | Batch_idx: 360 |  Loss_1: (0.0029) | Acc_1: (99.91%) (46168/46208)\n",
      "Epoch: 126 | Batch_idx: 370 |  Loss_1: (0.0029) | Acc_1: (99.91%) (47447/47488)\n",
      "Epoch: 126 | Batch_idx: 380 |  Loss_1: (0.0028) | Acc_1: (99.91%) (48726/48768)\n",
      "Epoch: 126 | Batch_idx: 390 |  Loss_1: (0.0028) | Acc_1: (99.92%) (49958/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4751) | Acc: (92.79%) (9279/10000)\n",
      "Epoch: 127 | Batch_idx: 0 |  Loss_1: (0.0034) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 127 | Batch_idx: 10 |  Loss_1: (0.0010) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 127 | Batch_idx: 20 |  Loss_1: (0.0023) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 127 | Batch_idx: 30 |  Loss_1: (0.0028) | Acc_1: (99.92%) (3965/3968)\n",
      "Epoch: 127 | Batch_idx: 40 |  Loss_1: (0.0027) | Acc_1: (99.92%) (5244/5248)\n",
      "Epoch: 127 | Batch_idx: 50 |  Loss_1: (0.0025) | Acc_1: (99.94%) (6524/6528)\n",
      "Epoch: 127 | Batch_idx: 60 |  Loss_1: (0.0028) | Acc_1: (99.94%) (7803/7808)\n",
      "Epoch: 127 | Batch_idx: 70 |  Loss_1: (0.0028) | Acc_1: (99.93%) (9082/9088)\n",
      "Epoch: 127 | Batch_idx: 80 |  Loss_1: (0.0026) | Acc_1: (99.94%) (10362/10368)\n",
      "Epoch: 127 | Batch_idx: 90 |  Loss_1: (0.0026) | Acc_1: (99.94%) (11641/11648)\n",
      "Epoch: 127 | Batch_idx: 100 |  Loss_1: (0.0025) | Acc_1: (99.94%) (12920/12928)\n",
      "Epoch: 127 | Batch_idx: 110 |  Loss_1: (0.0030) | Acc_1: (99.92%) (14196/14208)\n",
      "Epoch: 127 | Batch_idx: 120 |  Loss_1: (0.0031) | Acc_1: (99.91%) (15474/15488)\n",
      "Epoch: 127 | Batch_idx: 130 |  Loss_1: (0.0032) | Acc_1: (99.90%) (16751/16768)\n",
      "Epoch: 127 | Batch_idx: 140 |  Loss_1: (0.0038) | Acc_1: (99.88%) (18026/18048)\n",
      "Epoch: 127 | Batch_idx: 150 |  Loss_1: (0.0036) | Acc_1: (99.89%) (19306/19328)\n",
      "Epoch: 127 | Batch_idx: 160 |  Loss_1: (0.0037) | Acc_1: (99.87%) (20582/20608)\n",
      "Epoch: 127 | Batch_idx: 170 |  Loss_1: (0.0036) | Acc_1: (99.88%) (21861/21888)\n",
      "Epoch: 127 | Batch_idx: 180 |  Loss_1: (0.0035) | Acc_1: (99.88%) (23140/23168)\n",
      "Epoch: 127 | Batch_idx: 190 |  Loss_1: (0.0034) | Acc_1: (99.89%) (24420/24448)\n",
      "Epoch: 127 | Batch_idx: 200 |  Loss_1: (0.0033) | Acc_1: (99.89%) (25699/25728)\n",
      "Epoch: 127 | Batch_idx: 210 |  Loss_1: (0.0033) | Acc_1: (99.89%) (26979/27008)\n",
      "Epoch: 127 | Batch_idx: 220 |  Loss_1: (0.0032) | Acc_1: (99.89%) (28258/28288)\n",
      "Epoch: 127 | Batch_idx: 230 |  Loss_1: (0.0033) | Acc_1: (99.89%) (29536/29568)\n",
      "Epoch: 127 | Batch_idx: 240 |  Loss_1: (0.0034) | Acc_1: (99.89%) (30813/30848)\n",
      "Epoch: 127 | Batch_idx: 250 |  Loss_1: (0.0033) | Acc_1: (99.89%) (32093/32128)\n",
      "Epoch: 127 | Batch_idx: 260 |  Loss_1: (0.0033) | Acc_1: (99.89%) (33372/33408)\n",
      "Epoch: 127 | Batch_idx: 270 |  Loss_1: (0.0033) | Acc_1: (99.90%) (34652/34688)\n",
      "Epoch: 127 | Batch_idx: 280 |  Loss_1: (0.0033) | Acc_1: (99.90%) (35931/35968)\n",
      "Epoch: 127 | Batch_idx: 290 |  Loss_1: (0.0034) | Acc_1: (99.89%) (37208/37248)\n",
      "Epoch: 127 | Batch_idx: 300 |  Loss_1: (0.0035) | Acc_1: (99.89%) (38484/38528)\n",
      "Epoch: 127 | Batch_idx: 310 |  Loss_1: (0.0034) | Acc_1: (99.89%) (39764/39808)\n",
      "Epoch: 127 | Batch_idx: 320 |  Loss_1: (0.0034) | Acc_1: (99.89%) (41043/41088)\n",
      "Epoch: 127 | Batch_idx: 330 |  Loss_1: (0.0035) | Acc_1: (99.89%) (42321/42368)\n",
      "Epoch: 127 | Batch_idx: 340 |  Loss_1: (0.0034) | Acc_1: (99.89%) (43601/43648)\n",
      "Epoch: 127 | Batch_idx: 350 |  Loss_1: (0.0034) | Acc_1: (99.89%) (44879/44928)\n",
      "Epoch: 127 | Batch_idx: 360 |  Loss_1: (0.0033) | Acc_1: (99.89%) (46159/46208)\n",
      "Epoch: 127 | Batch_idx: 370 |  Loss_1: (0.0033) | Acc_1: (99.90%) (47439/47488)\n",
      "Epoch: 127 | Batch_idx: 380 |  Loss_1: (0.0032) | Acc_1: (99.90%) (48718/48768)\n",
      "Epoch: 127 | Batch_idx: 390 |  Loss_1: (0.0032) | Acc_1: (99.90%) (49950/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4473) | Acc: (93.02%) (9302/10000)\n",
      "Epoch: 128 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 128 | Batch_idx: 10 |  Loss_1: (0.0010) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 128 | Batch_idx: 20 |  Loss_1: (0.0043) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 128 | Batch_idx: 30 |  Loss_1: (0.0043) | Acc_1: (99.90%) (3964/3968)\n",
      "Epoch: 128 | Batch_idx: 40 |  Loss_1: (0.0044) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 128 | Batch_idx: 50 |  Loss_1: (0.0038) | Acc_1: (99.89%) (6521/6528)\n",
      "Epoch: 128 | Batch_idx: 60 |  Loss_1: (0.0039) | Acc_1: (99.88%) (7799/7808)\n",
      "Epoch: 128 | Batch_idx: 70 |  Loss_1: (0.0034) | Acc_1: (99.90%) (9079/9088)\n",
      "Epoch: 128 | Batch_idx: 80 |  Loss_1: (0.0032) | Acc_1: (99.91%) (10359/10368)\n",
      "Epoch: 128 | Batch_idx: 90 |  Loss_1: (0.0030) | Acc_1: (99.92%) (11639/11648)\n",
      "Epoch: 128 | Batch_idx: 100 |  Loss_1: (0.0030) | Acc_1: (99.91%) (12916/12928)\n",
      "Epoch: 128 | Batch_idx: 110 |  Loss_1: (0.0032) | Acc_1: (99.90%) (14194/14208)\n",
      "Epoch: 128 | Batch_idx: 120 |  Loss_1: (0.0032) | Acc_1: (99.90%) (15473/15488)\n",
      "Epoch: 128 | Batch_idx: 130 |  Loss_1: (0.0038) | Acc_1: (99.88%) (16748/16768)\n",
      "Epoch: 128 | Batch_idx: 140 |  Loss_1: (0.0037) | Acc_1: (99.88%) (18027/18048)\n",
      "Epoch: 128 | Batch_idx: 150 |  Loss_1: (0.0035) | Acc_1: (99.89%) (19306/19328)\n",
      "Epoch: 128 | Batch_idx: 160 |  Loss_1: (0.0035) | Acc_1: (99.88%) (20584/20608)\n",
      "Epoch: 128 | Batch_idx: 170 |  Loss_1: (0.0033) | Acc_1: (99.89%) (21864/21888)\n",
      "Epoch: 128 | Batch_idx: 180 |  Loss_1: (0.0032) | Acc_1: (99.90%) (23144/23168)\n",
      "Epoch: 128 | Batch_idx: 190 |  Loss_1: (0.0034) | Acc_1: (99.89%) (24420/24448)\n",
      "Epoch: 128 | Batch_idx: 200 |  Loss_1: (0.0033) | Acc_1: (99.89%) (25699/25728)\n",
      "Epoch: 128 | Batch_idx: 210 |  Loss_1: (0.0033) | Acc_1: (99.89%) (26977/27008)\n",
      "Epoch: 128 | Batch_idx: 220 |  Loss_1: (0.0032) | Acc_1: (99.89%) (28257/28288)\n",
      "Epoch: 128 | Batch_idx: 230 |  Loss_1: (0.0032) | Acc_1: (99.89%) (29534/29568)\n",
      "Epoch: 128 | Batch_idx: 240 |  Loss_1: (0.0032) | Acc_1: (99.88%) (30812/30848)\n",
      "Epoch: 128 | Batch_idx: 250 |  Loss_1: (0.0032) | Acc_1: (99.88%) (32090/32128)\n",
      "Epoch: 128 | Batch_idx: 260 |  Loss_1: (0.0033) | Acc_1: (99.88%) (33368/33408)\n",
      "Epoch: 128 | Batch_idx: 270 |  Loss_1: (0.0033) | Acc_1: (99.88%) (34646/34688)\n",
      "Epoch: 128 | Batch_idx: 280 |  Loss_1: (0.0033) | Acc_1: (99.87%) (35923/35968)\n",
      "Epoch: 128 | Batch_idx: 290 |  Loss_1: (0.0033) | Acc_1: (99.88%) (37203/37248)\n",
      "Epoch: 128 | Batch_idx: 300 |  Loss_1: (0.0032) | Acc_1: (99.88%) (38481/38528)\n",
      "Epoch: 128 | Batch_idx: 310 |  Loss_1: (0.0032) | Acc_1: (99.88%) (39760/39808)\n",
      "Epoch: 128 | Batch_idx: 320 |  Loss_1: (0.0032) | Acc_1: (99.88%) (41040/41088)\n",
      "Epoch: 128 | Batch_idx: 330 |  Loss_1: (0.0031) | Acc_1: (99.89%) (42320/42368)\n",
      "Epoch: 128 | Batch_idx: 340 |  Loss_1: (0.0030) | Acc_1: (99.89%) (43600/43648)\n",
      "Epoch: 128 | Batch_idx: 350 |  Loss_1: (0.0031) | Acc_1: (99.89%) (44878/44928)\n",
      "Epoch: 128 | Batch_idx: 360 |  Loss_1: (0.0031) | Acc_1: (99.89%) (46157/46208)\n",
      "Epoch: 128 | Batch_idx: 370 |  Loss_1: (0.0030) | Acc_1: (99.89%) (47437/47488)\n",
      "Epoch: 128 | Batch_idx: 380 |  Loss_1: (0.0030) | Acc_1: (99.90%) (48717/48768)\n",
      "Epoch: 128 | Batch_idx: 390 |  Loss_1: (0.0030) | Acc_1: (99.90%) (49948/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4559) | Acc: (92.89%) (9289/10000)\n",
      "Epoch: 129 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 129 | Batch_idx: 10 |  Loss_1: (0.0036) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 129 | Batch_idx: 20 |  Loss_1: (0.0030) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 129 | Batch_idx: 30 |  Loss_1: (0.0023) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 129 | Batch_idx: 40 |  Loss_1: (0.0020) | Acc_1: (99.90%) (5243/5248)\n",
      "Epoch: 129 | Batch_idx: 50 |  Loss_1: (0.0017) | Acc_1: (99.92%) (6523/6528)\n",
      "Epoch: 129 | Batch_idx: 60 |  Loss_1: (0.0018) | Acc_1: (99.92%) (7802/7808)\n",
      "Epoch: 129 | Batch_idx: 70 |  Loss_1: (0.0017) | Acc_1: (99.93%) (9082/9088)\n",
      "Epoch: 129 | Batch_idx: 80 |  Loss_1: (0.0018) | Acc_1: (99.93%) (10361/10368)\n",
      "Epoch: 129 | Batch_idx: 90 |  Loss_1: (0.0026) | Acc_1: (99.91%) (11637/11648)\n",
      "Epoch: 129 | Batch_idx: 100 |  Loss_1: (0.0027) | Acc_1: (99.91%) (12916/12928)\n",
      "Epoch: 129 | Batch_idx: 110 |  Loss_1: (0.0026) | Acc_1: (99.92%) (14196/14208)\n",
      "Epoch: 129 | Batch_idx: 120 |  Loss_1: (0.0026) | Acc_1: (99.91%) (15474/15488)\n",
      "Epoch: 129 | Batch_idx: 130 |  Loss_1: (0.0026) | Acc_1: (99.91%) (16753/16768)\n",
      "Epoch: 129 | Batch_idx: 140 |  Loss_1: (0.0026) | Acc_1: (99.91%) (18032/18048)\n",
      "Epoch: 129 | Batch_idx: 150 |  Loss_1: (0.0027) | Acc_1: (99.91%) (19310/19328)\n",
      "Epoch: 129 | Batch_idx: 160 |  Loss_1: (0.0026) | Acc_1: (99.91%) (20590/20608)\n",
      "Epoch: 129 | Batch_idx: 170 |  Loss_1: (0.0025) | Acc_1: (99.92%) (21870/21888)\n",
      "Epoch: 129 | Batch_idx: 180 |  Loss_1: (0.0024) | Acc_1: (99.92%) (23150/23168)\n",
      "Epoch: 129 | Batch_idx: 190 |  Loss_1: (0.0024) | Acc_1: (99.92%) (24428/24448)\n",
      "Epoch: 129 | Batch_idx: 200 |  Loss_1: (0.0024) | Acc_1: (99.91%) (25705/25728)\n",
      "Epoch: 129 | Batch_idx: 210 |  Loss_1: (0.0024) | Acc_1: (99.91%) (26984/27008)\n",
      "Epoch: 129 | Batch_idx: 220 |  Loss_1: (0.0024) | Acc_1: (99.92%) (28264/28288)\n",
      "Epoch: 129 | Batch_idx: 230 |  Loss_1: (0.0023) | Acc_1: (99.92%) (29544/29568)\n",
      "Epoch: 129 | Batch_idx: 240 |  Loss_1: (0.0024) | Acc_1: (99.92%) (30822/30848)\n",
      "Epoch: 129 | Batch_idx: 250 |  Loss_1: (0.0024) | Acc_1: (99.91%) (32100/32128)\n",
      "Epoch: 129 | Batch_idx: 260 |  Loss_1: (0.0025) | Acc_1: (99.91%) (33377/33408)\n",
      "Epoch: 129 | Batch_idx: 270 |  Loss_1: (0.0026) | Acc_1: (99.90%) (34654/34688)\n",
      "Epoch: 129 | Batch_idx: 280 |  Loss_1: (0.0026) | Acc_1: (99.90%) (35932/35968)\n",
      "Epoch: 129 | Batch_idx: 290 |  Loss_1: (0.0027) | Acc_1: (99.90%) (37209/37248)\n",
      "Epoch: 129 | Batch_idx: 300 |  Loss_1: (0.0028) | Acc_1: (99.89%) (38487/38528)\n",
      "Epoch: 129 | Batch_idx: 310 |  Loss_1: (0.0029) | Acc_1: (99.89%) (39764/39808)\n",
      "Epoch: 129 | Batch_idx: 320 |  Loss_1: (0.0029) | Acc_1: (99.89%) (41044/41088)\n",
      "Epoch: 129 | Batch_idx: 330 |  Loss_1: (0.0029) | Acc_1: (99.89%) (42323/42368)\n",
      "Epoch: 129 | Batch_idx: 340 |  Loss_1: (0.0028) | Acc_1: (99.90%) (43603/43648)\n",
      "Epoch: 129 | Batch_idx: 350 |  Loss_1: (0.0029) | Acc_1: (99.90%) (44882/44928)\n",
      "Epoch: 129 | Batch_idx: 360 |  Loss_1: (0.0028) | Acc_1: (99.90%) (46161/46208)\n",
      "Epoch: 129 | Batch_idx: 370 |  Loss_1: (0.0029) | Acc_1: (99.90%) (47439/47488)\n",
      "Epoch: 129 | Batch_idx: 380 |  Loss_1: (0.0032) | Acc_1: (99.89%) (48713/48768)\n",
      "Epoch: 129 | Batch_idx: 390 |  Loss_1: (0.0031) | Acc_1: (99.89%) (49944/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4635) | Acc: (92.76%) (9276/10000)\n",
      "Epoch: 130 | Batch_idx: 0 |  Loss_1: (0.0093) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 130 | Batch_idx: 10 |  Loss_1: (0.0044) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 130 | Batch_idx: 20 |  Loss_1: (0.0037) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 130 | Batch_idx: 30 |  Loss_1: (0.0037) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 130 | Batch_idx: 40 |  Loss_1: (0.0031) | Acc_1: (99.90%) (5243/5248)\n",
      "Epoch: 130 | Batch_idx: 50 |  Loss_1: (0.0037) | Acc_1: (99.88%) (6520/6528)\n",
      "Epoch: 130 | Batch_idx: 60 |  Loss_1: (0.0036) | Acc_1: (99.88%) (7799/7808)\n",
      "Epoch: 130 | Batch_idx: 70 |  Loss_1: (0.0033) | Acc_1: (99.90%) (9079/9088)\n",
      "Epoch: 130 | Batch_idx: 80 |  Loss_1: (0.0031) | Acc_1: (99.90%) (10358/10368)\n",
      "Epoch: 130 | Batch_idx: 90 |  Loss_1: (0.0029) | Acc_1: (99.91%) (11638/11648)\n",
      "Epoch: 130 | Batch_idx: 100 |  Loss_1: (0.0028) | Acc_1: (99.91%) (12917/12928)\n",
      "Epoch: 130 | Batch_idx: 110 |  Loss_1: (0.0027) | Acc_1: (99.92%) (14196/14208)\n",
      "Epoch: 130 | Batch_idx: 120 |  Loss_1: (0.0028) | Acc_1: (99.92%) (15475/15488)\n",
      "Epoch: 130 | Batch_idx: 130 |  Loss_1: (0.0027) | Acc_1: (99.92%) (16754/16768)\n",
      "Epoch: 130 | Batch_idx: 140 |  Loss_1: (0.0027) | Acc_1: (99.92%) (18033/18048)\n",
      "Epoch: 130 | Batch_idx: 150 |  Loss_1: (0.0028) | Acc_1: (99.92%) (19312/19328)\n",
      "Epoch: 130 | Batch_idx: 160 |  Loss_1: (0.0027) | Acc_1: (99.92%) (20592/20608)\n",
      "Epoch: 130 | Batch_idx: 170 |  Loss_1: (0.0027) | Acc_1: (99.92%) (21870/21888)\n",
      "Epoch: 130 | Batch_idx: 180 |  Loss_1: (0.0028) | Acc_1: (99.91%) (23148/23168)\n",
      "Epoch: 130 | Batch_idx: 190 |  Loss_1: (0.0028) | Acc_1: (99.91%) (24426/24448)\n",
      "Epoch: 130 | Batch_idx: 200 |  Loss_1: (0.0029) | Acc_1: (99.90%) (25703/25728)\n",
      "Epoch: 130 | Batch_idx: 210 |  Loss_1: (0.0028) | Acc_1: (99.91%) (26983/27008)\n",
      "Epoch: 130 | Batch_idx: 220 |  Loss_1: (0.0028) | Acc_1: (99.91%) (28262/28288)\n",
      "Epoch: 130 | Batch_idx: 230 |  Loss_1: (0.0027) | Acc_1: (99.91%) (29541/29568)\n",
      "Epoch: 130 | Batch_idx: 240 |  Loss_1: (0.0026) | Acc_1: (99.91%) (30821/30848)\n",
      "Epoch: 130 | Batch_idx: 250 |  Loss_1: (0.0026) | Acc_1: (99.91%) (32099/32128)\n",
      "Epoch: 130 | Batch_idx: 260 |  Loss_1: (0.0026) | Acc_1: (99.91%) (33378/33408)\n",
      "Epoch: 130 | Batch_idx: 270 |  Loss_1: (0.0025) | Acc_1: (99.91%) (34658/34688)\n",
      "Epoch: 130 | Batch_idx: 280 |  Loss_1: (0.0025) | Acc_1: (99.92%) (35938/35968)\n",
      "Epoch: 130 | Batch_idx: 290 |  Loss_1: (0.0024) | Acc_1: (99.92%) (37218/37248)\n",
      "Epoch: 130 | Batch_idx: 300 |  Loss_1: (0.0024) | Acc_1: (99.92%) (38498/38528)\n",
      "Epoch: 130 | Batch_idx: 310 |  Loss_1: (0.0024) | Acc_1: (99.92%) (39777/39808)\n",
      "Epoch: 130 | Batch_idx: 320 |  Loss_1: (0.0023) | Acc_1: (99.92%) (41056/41088)\n",
      "Epoch: 130 | Batch_idx: 330 |  Loss_1: (0.0023) | Acc_1: (99.92%) (42336/42368)\n",
      "Epoch: 130 | Batch_idx: 340 |  Loss_1: (0.0023) | Acc_1: (99.92%) (43615/43648)\n",
      "Epoch: 130 | Batch_idx: 350 |  Loss_1: (0.0022) | Acc_1: (99.92%) (44894/44928)\n",
      "Epoch: 130 | Batch_idx: 360 |  Loss_1: (0.0022) | Acc_1: (99.92%) (46173/46208)\n",
      "Epoch: 130 | Batch_idx: 370 |  Loss_1: (0.0023) | Acc_1: (99.92%) (47452/47488)\n",
      "Epoch: 130 | Batch_idx: 380 |  Loss_1: (0.0022) | Acc_1: (99.93%) (48732/48768)\n",
      "Epoch: 130 | Batch_idx: 390 |  Loss_1: (0.0023) | Acc_1: (99.93%) (49963/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4771) | Acc: (92.70%) (9270/10000)\n",
      "Epoch: 131 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 131 | Batch_idx: 10 |  Loss_1: (0.0020) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 131 | Batch_idx: 20 |  Loss_1: (0.0015) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 131 | Batch_idx: 30 |  Loss_1: (0.0014) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 131 | Batch_idx: 40 |  Loss_1: (0.0014) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 131 | Batch_idx: 50 |  Loss_1: (0.0014) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 131 | Batch_idx: 60 |  Loss_1: (0.0012) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 131 | Batch_idx: 70 |  Loss_1: (0.0013) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 131 | Batch_idx: 80 |  Loss_1: (0.0014) | Acc_1: (99.97%) (10365/10368)\n",
      "Epoch: 131 | Batch_idx: 90 |  Loss_1: (0.0013) | Acc_1: (99.97%) (11645/11648)\n",
      "Epoch: 131 | Batch_idx: 100 |  Loss_1: (0.0015) | Acc_1: (99.96%) (12923/12928)\n",
      "Epoch: 131 | Batch_idx: 110 |  Loss_1: (0.0016) | Acc_1: (99.96%) (14202/14208)\n",
      "Epoch: 131 | Batch_idx: 120 |  Loss_1: (0.0016) | Acc_1: (99.95%) (15481/15488)\n",
      "Epoch: 131 | Batch_idx: 130 |  Loss_1: (0.0016) | Acc_1: (99.95%) (16760/16768)\n",
      "Epoch: 131 | Batch_idx: 140 |  Loss_1: (0.0017) | Acc_1: (99.94%) (18038/18048)\n",
      "Epoch: 131 | Batch_idx: 150 |  Loss_1: (0.0022) | Acc_1: (99.93%) (19315/19328)\n",
      "Epoch: 131 | Batch_idx: 160 |  Loss_1: (0.0023) | Acc_1: (99.93%) (20593/20608)\n",
      "Epoch: 131 | Batch_idx: 170 |  Loss_1: (0.0024) | Acc_1: (99.92%) (21870/21888)\n",
      "Epoch: 131 | Batch_idx: 180 |  Loss_1: (0.0024) | Acc_1: (99.92%) (23149/23168)\n",
      "Epoch: 131 | Batch_idx: 190 |  Loss_1: (0.0026) | Acc_1: (99.91%) (24427/24448)\n",
      "Epoch: 131 | Batch_idx: 200 |  Loss_1: (0.0025) | Acc_1: (99.92%) (25707/25728)\n",
      "Epoch: 131 | Batch_idx: 210 |  Loss_1: (0.0024) | Acc_1: (99.92%) (26987/27008)\n",
      "Epoch: 131 | Batch_idx: 220 |  Loss_1: (0.0024) | Acc_1: (99.92%) (28266/28288)\n",
      "Epoch: 131 | Batch_idx: 230 |  Loss_1: (0.0025) | Acc_1: (99.92%) (29545/29568)\n",
      "Epoch: 131 | Batch_idx: 240 |  Loss_1: (0.0025) | Acc_1: (99.92%) (30823/30848)\n",
      "Epoch: 131 | Batch_idx: 250 |  Loss_1: (0.0027) | Acc_1: (99.92%) (32101/32128)\n",
      "Epoch: 131 | Batch_idx: 260 |  Loss_1: (0.0026) | Acc_1: (99.92%) (33381/33408)\n",
      "Epoch: 131 | Batch_idx: 270 |  Loss_1: (0.0026) | Acc_1: (99.91%) (34658/34688)\n",
      "Epoch: 131 | Batch_idx: 280 |  Loss_1: (0.0025) | Acc_1: (99.92%) (35938/35968)\n",
      "Epoch: 131 | Batch_idx: 290 |  Loss_1: (0.0025) | Acc_1: (99.92%) (37217/37248)\n",
      "Epoch: 131 | Batch_idx: 300 |  Loss_1: (0.0024) | Acc_1: (99.92%) (38497/38528)\n",
      "Epoch: 131 | Batch_idx: 310 |  Loss_1: (0.0024) | Acc_1: (99.92%) (39777/39808)\n",
      "Epoch: 131 | Batch_idx: 320 |  Loss_1: (0.0024) | Acc_1: (99.92%) (41055/41088)\n",
      "Epoch: 131 | Batch_idx: 330 |  Loss_1: (0.0024) | Acc_1: (99.92%) (42333/42368)\n",
      "Epoch: 131 | Batch_idx: 340 |  Loss_1: (0.0025) | Acc_1: (99.92%) (43611/43648)\n",
      "Epoch: 131 | Batch_idx: 350 |  Loss_1: (0.0025) | Acc_1: (99.92%) (44890/44928)\n",
      "Epoch: 131 | Batch_idx: 360 |  Loss_1: (0.0024) | Acc_1: (99.92%) (46169/46208)\n",
      "Epoch: 131 | Batch_idx: 370 |  Loss_1: (0.0024) | Acc_1: (99.92%) (47449/47488)\n",
      "Epoch: 131 | Batch_idx: 380 |  Loss_1: (0.0025) | Acc_1: (99.92%) (48727/48768)\n",
      "Epoch: 131 | Batch_idx: 390 |  Loss_1: (0.0025) | Acc_1: (99.92%) (49958/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4627) | Acc: (92.90%) (9290/10000)\n",
      "Epoch: 132 | Batch_idx: 0 |  Loss_1: (0.0010) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 132 | Batch_idx: 10 |  Loss_1: (0.0008) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 132 | Batch_idx: 20 |  Loss_1: (0.0011) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 132 | Batch_idx: 30 |  Loss_1: (0.0010) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 132 | Batch_idx: 40 |  Loss_1: (0.0010) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 132 | Batch_idx: 50 |  Loss_1: (0.0011) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 132 | Batch_idx: 60 |  Loss_1: (0.0012) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 132 | Batch_idx: 70 |  Loss_1: (0.0011) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 132 | Batch_idx: 80 |  Loss_1: (0.0011) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 132 | Batch_idx: 90 |  Loss_1: (0.0012) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 132 | Batch_idx: 100 |  Loss_1: (0.0013) | Acc_1: (99.98%) (12925/12928)\n",
      "Epoch: 132 | Batch_idx: 110 |  Loss_1: (0.0014) | Acc_1: (99.97%) (14204/14208)\n",
      "Epoch: 132 | Batch_idx: 120 |  Loss_1: (0.0014) | Acc_1: (99.97%) (15484/15488)\n",
      "Epoch: 132 | Batch_idx: 130 |  Loss_1: (0.0014) | Acc_1: (99.98%) (16764/16768)\n",
      "Epoch: 132 | Batch_idx: 140 |  Loss_1: (0.0015) | Acc_1: (99.97%) (18042/18048)\n",
      "Epoch: 132 | Batch_idx: 150 |  Loss_1: (0.0017) | Acc_1: (99.96%) (19320/19328)\n",
      "Epoch: 132 | Batch_idx: 160 |  Loss_1: (0.0016) | Acc_1: (99.96%) (20600/20608)\n",
      "Epoch: 132 | Batch_idx: 170 |  Loss_1: (0.0015) | Acc_1: (99.96%) (21880/21888)\n",
      "Epoch: 132 | Batch_idx: 180 |  Loss_1: (0.0015) | Acc_1: (99.97%) (23160/23168)\n",
      "Epoch: 132 | Batch_idx: 190 |  Loss_1: (0.0015) | Acc_1: (99.96%) (24439/24448)\n",
      "Epoch: 132 | Batch_idx: 200 |  Loss_1: (0.0016) | Acc_1: (99.96%) (25718/25728)\n",
      "Epoch: 132 | Batch_idx: 210 |  Loss_1: (0.0016) | Acc_1: (99.96%) (26996/27008)\n",
      "Epoch: 132 | Batch_idx: 220 |  Loss_1: (0.0018) | Acc_1: (99.95%) (28273/28288)\n",
      "Epoch: 132 | Batch_idx: 230 |  Loss_1: (0.0017) | Acc_1: (99.95%) (29553/29568)\n",
      "Epoch: 132 | Batch_idx: 240 |  Loss_1: (0.0017) | Acc_1: (99.95%) (30832/30848)\n",
      "Epoch: 132 | Batch_idx: 250 |  Loss_1: (0.0018) | Acc_1: (99.95%) (32111/32128)\n",
      "Epoch: 132 | Batch_idx: 260 |  Loss_1: (0.0017) | Acc_1: (99.95%) (33390/33408)\n",
      "Epoch: 132 | Batch_idx: 270 |  Loss_1: (0.0017) | Acc_1: (99.95%) (34669/34688)\n",
      "Epoch: 132 | Batch_idx: 280 |  Loss_1: (0.0017) | Acc_1: (99.94%) (35948/35968)\n",
      "Epoch: 132 | Batch_idx: 290 |  Loss_1: (0.0017) | Acc_1: (99.94%) (37227/37248)\n",
      "Epoch: 132 | Batch_idx: 300 |  Loss_1: (0.0018) | Acc_1: (99.94%) (38505/38528)\n",
      "Epoch: 132 | Batch_idx: 310 |  Loss_1: (0.0017) | Acc_1: (99.94%) (39785/39808)\n",
      "Epoch: 132 | Batch_idx: 320 |  Loss_1: (0.0017) | Acc_1: (99.94%) (41065/41088)\n",
      "Epoch: 132 | Batch_idx: 330 |  Loss_1: (0.0017) | Acc_1: (99.94%) (42344/42368)\n",
      "Epoch: 132 | Batch_idx: 340 |  Loss_1: (0.0017) | Acc_1: (99.95%) (43624/43648)\n",
      "Epoch: 132 | Batch_idx: 350 |  Loss_1: (0.0017) | Acc_1: (99.95%) (44904/44928)\n",
      "Epoch: 132 | Batch_idx: 360 |  Loss_1: (0.0016) | Acc_1: (99.95%) (46184/46208)\n",
      "Epoch: 132 | Batch_idx: 370 |  Loss_1: (0.0016) | Acc_1: (99.95%) (47464/47488)\n",
      "Epoch: 132 | Batch_idx: 380 |  Loss_1: (0.0016) | Acc_1: (99.95%) (48744/48768)\n",
      "Epoch: 132 | Batch_idx: 390 |  Loss_1: (0.0016) | Acc_1: (99.95%) (49975/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4646) | Acc: (93.01%) (9301/10000)\n",
      "Epoch: 133 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 133 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 133 | Batch_idx: 20 |  Loss_1: (0.0008) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 133 | Batch_idx: 30 |  Loss_1: (0.0009) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 133 | Batch_idx: 40 |  Loss_1: (0.0008) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 133 | Batch_idx: 50 |  Loss_1: (0.0008) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 133 | Batch_idx: 60 |  Loss_1: (0.0008) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 133 | Batch_idx: 70 |  Loss_1: (0.0009) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 133 | Batch_idx: 80 |  Loss_1: (0.0014) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 133 | Batch_idx: 90 |  Loss_1: (0.0014) | Acc_1: (99.97%) (11645/11648)\n",
      "Epoch: 133 | Batch_idx: 100 |  Loss_1: (0.0015) | Acc_1: (99.97%) (12924/12928)\n",
      "Epoch: 133 | Batch_idx: 110 |  Loss_1: (0.0017) | Acc_1: (99.96%) (14203/14208)\n",
      "Epoch: 133 | Batch_idx: 120 |  Loss_1: (0.0017) | Acc_1: (99.97%) (15483/15488)\n",
      "Epoch: 133 | Batch_idx: 130 |  Loss_1: (0.0017) | Acc_1: (99.96%) (16762/16768)\n",
      "Epoch: 133 | Batch_idx: 140 |  Loss_1: (0.0017) | Acc_1: (99.96%) (18041/18048)\n",
      "Epoch: 133 | Batch_idx: 150 |  Loss_1: (0.0017) | Acc_1: (99.96%) (19320/19328)\n",
      "Epoch: 133 | Batch_idx: 160 |  Loss_1: (0.0018) | Acc_1: (99.95%) (20597/20608)\n",
      "Epoch: 133 | Batch_idx: 170 |  Loss_1: (0.0020) | Acc_1: (99.94%) (21875/21888)\n",
      "Epoch: 133 | Batch_idx: 180 |  Loss_1: (0.0020) | Acc_1: (99.94%) (23154/23168)\n",
      "Epoch: 133 | Batch_idx: 190 |  Loss_1: (0.0021) | Acc_1: (99.94%) (24433/24448)\n",
      "Epoch: 133 | Batch_idx: 200 |  Loss_1: (0.0021) | Acc_1: (99.94%) (25713/25728)\n",
      "Epoch: 133 | Batch_idx: 210 |  Loss_1: (0.0021) | Acc_1: (99.94%) (26993/27008)\n",
      "Epoch: 133 | Batch_idx: 220 |  Loss_1: (0.0020) | Acc_1: (99.94%) (28272/28288)\n",
      "Epoch: 133 | Batch_idx: 230 |  Loss_1: (0.0020) | Acc_1: (99.95%) (29552/29568)\n",
      "Epoch: 133 | Batch_idx: 240 |  Loss_1: (0.0019) | Acc_1: (99.95%) (30832/30848)\n",
      "Epoch: 133 | Batch_idx: 250 |  Loss_1: (0.0019) | Acc_1: (99.95%) (32112/32128)\n",
      "Epoch: 133 | Batch_idx: 260 |  Loss_1: (0.0019) | Acc_1: (99.95%) (33392/33408)\n",
      "Epoch: 133 | Batch_idx: 270 |  Loss_1: (0.0018) | Acc_1: (99.95%) (34672/34688)\n",
      "Epoch: 133 | Batch_idx: 280 |  Loss_1: (0.0018) | Acc_1: (99.96%) (35952/35968)\n",
      "Epoch: 133 | Batch_idx: 290 |  Loss_1: (0.0019) | Acc_1: (99.95%) (37230/37248)\n",
      "Epoch: 133 | Batch_idx: 300 |  Loss_1: (0.0020) | Acc_1: (99.95%) (38509/38528)\n",
      "Epoch: 133 | Batch_idx: 310 |  Loss_1: (0.0020) | Acc_1: (99.95%) (39788/39808)\n",
      "Epoch: 133 | Batch_idx: 320 |  Loss_1: (0.0020) | Acc_1: (99.95%) (41068/41088)\n",
      "Epoch: 133 | Batch_idx: 330 |  Loss_1: (0.0019) | Acc_1: (99.95%) (42348/42368)\n",
      "Epoch: 133 | Batch_idx: 340 |  Loss_1: (0.0019) | Acc_1: (99.95%) (43627/43648)\n",
      "Epoch: 133 | Batch_idx: 350 |  Loss_1: (0.0019) | Acc_1: (99.95%) (44907/44928)\n",
      "Epoch: 133 | Batch_idx: 360 |  Loss_1: (0.0019) | Acc_1: (99.95%) (46187/46208)\n",
      "Epoch: 133 | Batch_idx: 370 |  Loss_1: (0.0018) | Acc_1: (99.96%) (47467/47488)\n",
      "Epoch: 133 | Batch_idx: 380 |  Loss_1: (0.0018) | Acc_1: (99.96%) (48747/48768)\n",
      "Epoch: 133 | Batch_idx: 390 |  Loss_1: (0.0018) | Acc_1: (99.96%) (49979/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4836) | Acc: (93.06%) (9306/10000)\n",
      "Epoch: 134 | Batch_idx: 0 |  Loss_1: (0.0035) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 134 | Batch_idx: 10 |  Loss_1: (0.0011) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 134 | Batch_idx: 20 |  Loss_1: (0.0018) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 134 | Batch_idx: 30 |  Loss_1: (0.0014) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 134 | Batch_idx: 40 |  Loss_1: (0.0013) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 134 | Batch_idx: 50 |  Loss_1: (0.0013) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 134 | Batch_idx: 60 |  Loss_1: (0.0011) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 134 | Batch_idx: 70 |  Loss_1: (0.0011) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 134 | Batch_idx: 80 |  Loss_1: (0.0010) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 134 | Batch_idx: 90 |  Loss_1: (0.0009) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 134 | Batch_idx: 100 |  Loss_1: (0.0010) | Acc_1: (99.98%) (12925/12928)\n",
      "Epoch: 134 | Batch_idx: 110 |  Loss_1: (0.0011) | Acc_1: (99.97%) (14204/14208)\n",
      "Epoch: 134 | Batch_idx: 120 |  Loss_1: (0.0011) | Acc_1: (99.97%) (15483/15488)\n",
      "Epoch: 134 | Batch_idx: 130 |  Loss_1: (0.0011) | Acc_1: (99.96%) (16762/16768)\n",
      "Epoch: 134 | Batch_idx: 140 |  Loss_1: (0.0011) | Acc_1: (99.97%) (18042/18048)\n",
      "Epoch: 134 | Batch_idx: 150 |  Loss_1: (0.0010) | Acc_1: (99.97%) (19322/19328)\n",
      "Epoch: 134 | Batch_idx: 160 |  Loss_1: (0.0010) | Acc_1: (99.97%) (20602/20608)\n",
      "Epoch: 134 | Batch_idx: 170 |  Loss_1: (0.0010) | Acc_1: (99.97%) (21882/21888)\n",
      "Epoch: 134 | Batch_idx: 180 |  Loss_1: (0.0010) | Acc_1: (99.97%) (23162/23168)\n",
      "Epoch: 134 | Batch_idx: 190 |  Loss_1: (0.0010) | Acc_1: (99.97%) (24441/24448)\n",
      "Epoch: 134 | Batch_idx: 200 |  Loss_1: (0.0010) | Acc_1: (99.97%) (25721/25728)\n",
      "Epoch: 134 | Batch_idx: 210 |  Loss_1: (0.0009) | Acc_1: (99.97%) (27001/27008)\n",
      "Epoch: 134 | Batch_idx: 220 |  Loss_1: (0.0009) | Acc_1: (99.98%) (28281/28288)\n",
      "Epoch: 134 | Batch_idx: 230 |  Loss_1: (0.0009) | Acc_1: (99.98%) (29561/29568)\n",
      "Epoch: 134 | Batch_idx: 240 |  Loss_1: (0.0010) | Acc_1: (99.97%) (30840/30848)\n",
      "Epoch: 134 | Batch_idx: 250 |  Loss_1: (0.0010) | Acc_1: (99.97%) (32119/32128)\n",
      "Epoch: 134 | Batch_idx: 260 |  Loss_1: (0.0010) | Acc_1: (99.97%) (33399/33408)\n",
      "Epoch: 134 | Batch_idx: 270 |  Loss_1: (0.0010) | Acc_1: (99.97%) (34678/34688)\n",
      "Epoch: 134 | Batch_idx: 280 |  Loss_1: (0.0010) | Acc_1: (99.97%) (35958/35968)\n",
      "Epoch: 134 | Batch_idx: 290 |  Loss_1: (0.0010) | Acc_1: (99.97%) (37238/37248)\n",
      "Epoch: 134 | Batch_idx: 300 |  Loss_1: (0.0011) | Acc_1: (99.97%) (38517/38528)\n",
      "Epoch: 134 | Batch_idx: 310 |  Loss_1: (0.0011) | Acc_1: (99.97%) (39797/39808)\n",
      "Epoch: 134 | Batch_idx: 320 |  Loss_1: (0.0011) | Acc_1: (99.97%) (41077/41088)\n",
      "Epoch: 134 | Batch_idx: 330 |  Loss_1: (0.0011) | Acc_1: (99.97%) (42356/42368)\n",
      "Epoch: 134 | Batch_idx: 340 |  Loss_1: (0.0011) | Acc_1: (99.97%) (43636/43648)\n",
      "Epoch: 134 | Batch_idx: 350 |  Loss_1: (0.0011) | Acc_1: (99.97%) (44916/44928)\n",
      "Epoch: 134 | Batch_idx: 360 |  Loss_1: (0.0012) | Acc_1: (99.97%) (46194/46208)\n",
      "Epoch: 134 | Batch_idx: 370 |  Loss_1: (0.0013) | Acc_1: (99.97%) (47473/47488)\n",
      "Epoch: 134 | Batch_idx: 380 |  Loss_1: (0.0012) | Acc_1: (99.97%) (48753/48768)\n",
      "Epoch: 134 | Batch_idx: 390 |  Loss_1: (0.0013) | Acc_1: (99.97%) (49984/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4783) | Acc: (92.66%) (9266/10000)\n",
      "Epoch: 135 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 135 | Batch_idx: 10 |  Loss_1: (0.0021) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 135 | Batch_idx: 20 |  Loss_1: (0.0019) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 135 | Batch_idx: 30 |  Loss_1: (0.0018) | Acc_1: (99.92%) (3965/3968)\n",
      "Epoch: 135 | Batch_idx: 40 |  Loss_1: (0.0016) | Acc_1: (99.94%) (5245/5248)\n",
      "Epoch: 135 | Batch_idx: 50 |  Loss_1: (0.0014) | Acc_1: (99.95%) (6525/6528)\n",
      "Epoch: 135 | Batch_idx: 60 |  Loss_1: (0.0013) | Acc_1: (99.96%) (7805/7808)\n",
      "Epoch: 135 | Batch_idx: 70 |  Loss_1: (0.0015) | Acc_1: (99.94%) (9083/9088)\n",
      "Epoch: 135 | Batch_idx: 80 |  Loss_1: (0.0015) | Acc_1: (99.95%) (10363/10368)\n",
      "Epoch: 135 | Batch_idx: 90 |  Loss_1: (0.0014) | Acc_1: (99.96%) (11643/11648)\n",
      "Epoch: 135 | Batch_idx: 100 |  Loss_1: (0.0016) | Acc_1: (99.95%) (12922/12928)\n",
      "Epoch: 135 | Batch_idx: 110 |  Loss_1: (0.0015) | Acc_1: (99.96%) (14202/14208)\n",
      "Epoch: 135 | Batch_idx: 120 |  Loss_1: (0.0014) | Acc_1: (99.96%) (15482/15488)\n",
      "Epoch: 135 | Batch_idx: 130 |  Loss_1: (0.0013) | Acc_1: (99.96%) (16762/16768)\n",
      "Epoch: 135 | Batch_idx: 140 |  Loss_1: (0.0013) | Acc_1: (99.97%) (18042/18048)\n",
      "Epoch: 135 | Batch_idx: 150 |  Loss_1: (0.0013) | Acc_1: (99.97%) (19322/19328)\n",
      "Epoch: 135 | Batch_idx: 160 |  Loss_1: (0.0012) | Acc_1: (99.97%) (20602/20608)\n",
      "Epoch: 135 | Batch_idx: 170 |  Loss_1: (0.0012) | Acc_1: (99.97%) (21881/21888)\n",
      "Epoch: 135 | Batch_idx: 180 |  Loss_1: (0.0012) | Acc_1: (99.97%) (23161/23168)\n",
      "Epoch: 135 | Batch_idx: 190 |  Loss_1: (0.0012) | Acc_1: (99.97%) (24441/24448)\n",
      "Epoch: 135 | Batch_idx: 200 |  Loss_1: (0.0012) | Acc_1: (99.97%) (25721/25728)\n",
      "Epoch: 135 | Batch_idx: 210 |  Loss_1: (0.0011) | Acc_1: (99.97%) (27001/27008)\n",
      "Epoch: 135 | Batch_idx: 220 |  Loss_1: (0.0011) | Acc_1: (99.98%) (28281/28288)\n",
      "Epoch: 135 | Batch_idx: 230 |  Loss_1: (0.0011) | Acc_1: (99.98%) (29561/29568)\n",
      "Epoch: 135 | Batch_idx: 240 |  Loss_1: (0.0011) | Acc_1: (99.98%) (30841/30848)\n",
      "Epoch: 135 | Batch_idx: 250 |  Loss_1: (0.0011) | Acc_1: (99.98%) (32121/32128)\n",
      "Epoch: 135 | Batch_idx: 260 |  Loss_1: (0.0010) | Acc_1: (99.98%) (33401/33408)\n",
      "Epoch: 135 | Batch_idx: 270 |  Loss_1: (0.0011) | Acc_1: (99.98%) (34680/34688)\n",
      "Epoch: 135 | Batch_idx: 280 |  Loss_1: (0.0011) | Acc_1: (99.98%) (35960/35968)\n",
      "Epoch: 135 | Batch_idx: 290 |  Loss_1: (0.0011) | Acc_1: (99.98%) (37240/37248)\n",
      "Epoch: 135 | Batch_idx: 300 |  Loss_1: (0.0010) | Acc_1: (99.98%) (38520/38528)\n",
      "Epoch: 135 | Batch_idx: 310 |  Loss_1: (0.0011) | Acc_1: (99.98%) (39800/39808)\n",
      "Epoch: 135 | Batch_idx: 320 |  Loss_1: (0.0011) | Acc_1: (99.98%) (41080/41088)\n",
      "Epoch: 135 | Batch_idx: 330 |  Loss_1: (0.0012) | Acc_1: (99.98%) (42358/42368)\n",
      "Epoch: 135 | Batch_idx: 340 |  Loss_1: (0.0011) | Acc_1: (99.98%) (43638/43648)\n",
      "Epoch: 135 | Batch_idx: 350 |  Loss_1: (0.0012) | Acc_1: (99.97%) (44916/44928)\n",
      "Epoch: 135 | Batch_idx: 360 |  Loss_1: (0.0012) | Acc_1: (99.97%) (46196/46208)\n",
      "Epoch: 135 | Batch_idx: 370 |  Loss_1: (0.0012) | Acc_1: (99.97%) (47475/47488)\n",
      "Epoch: 135 | Batch_idx: 380 |  Loss_1: (0.0012) | Acc_1: (99.97%) (48754/48768)\n",
      "Epoch: 135 | Batch_idx: 390 |  Loss_1: (0.0012) | Acc_1: (99.97%) (49985/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4675) | Acc: (92.85%) (9285/10000)\n",
      "Epoch: 136 | Batch_idx: 0 |  Loss_1: (0.0070) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 136 | Batch_idx: 10 |  Loss_1: (0.0011) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 136 | Batch_idx: 20 |  Loss_1: (0.0039) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 136 | Batch_idx: 30 |  Loss_1: (0.0027) | Acc_1: (99.92%) (3965/3968)\n",
      "Epoch: 136 | Batch_idx: 40 |  Loss_1: (0.0038) | Acc_1: (99.90%) (5243/5248)\n",
      "Epoch: 136 | Batch_idx: 50 |  Loss_1: (0.0031) | Acc_1: (99.92%) (6523/6528)\n",
      "Epoch: 136 | Batch_idx: 60 |  Loss_1: (0.0026) | Acc_1: (99.94%) (7803/7808)\n",
      "Epoch: 136 | Batch_idx: 70 |  Loss_1: (0.0023) | Acc_1: (99.94%) (9083/9088)\n",
      "Epoch: 136 | Batch_idx: 80 |  Loss_1: (0.0022) | Acc_1: (99.95%) (10363/10368)\n",
      "Epoch: 136 | Batch_idx: 90 |  Loss_1: (0.0021) | Acc_1: (99.96%) (11643/11648)\n",
      "Epoch: 136 | Batch_idx: 100 |  Loss_1: (0.0022) | Acc_1: (99.95%) (12922/12928)\n",
      "Epoch: 136 | Batch_idx: 110 |  Loss_1: (0.0021) | Acc_1: (99.96%) (14202/14208)\n",
      "Epoch: 136 | Batch_idx: 120 |  Loss_1: (0.0021) | Acc_1: (99.95%) (15481/15488)\n",
      "Epoch: 136 | Batch_idx: 130 |  Loss_1: (0.0026) | Acc_1: (99.95%) (16759/16768)\n",
      "Epoch: 136 | Batch_idx: 140 |  Loss_1: (0.0024) | Acc_1: (99.95%) (18039/18048)\n",
      "Epoch: 136 | Batch_idx: 150 |  Loss_1: (0.0023) | Acc_1: (99.95%) (19319/19328)\n",
      "Epoch: 136 | Batch_idx: 160 |  Loss_1: (0.0023) | Acc_1: (99.95%) (20598/20608)\n",
      "Epoch: 136 | Batch_idx: 170 |  Loss_1: (0.0022) | Acc_1: (99.95%) (21878/21888)\n",
      "Epoch: 136 | Batch_idx: 180 |  Loss_1: (0.0021) | Acc_1: (99.96%) (23158/23168)\n",
      "Epoch: 136 | Batch_idx: 190 |  Loss_1: (0.0021) | Acc_1: (99.96%) (24437/24448)\n",
      "Epoch: 136 | Batch_idx: 200 |  Loss_1: (0.0023) | Acc_1: (99.95%) (25714/25728)\n",
      "Epoch: 136 | Batch_idx: 210 |  Loss_1: (0.0022) | Acc_1: (99.95%) (26994/27008)\n",
      "Epoch: 136 | Batch_idx: 220 |  Loss_1: (0.0022) | Acc_1: (99.95%) (28274/28288)\n",
      "Epoch: 136 | Batch_idx: 230 |  Loss_1: (0.0022) | Acc_1: (99.95%) (29553/29568)\n",
      "Epoch: 136 | Batch_idx: 240 |  Loss_1: (0.0022) | Acc_1: (99.95%) (30832/30848)\n",
      "Epoch: 136 | Batch_idx: 250 |  Loss_1: (0.0021) | Acc_1: (99.95%) (32112/32128)\n",
      "Epoch: 136 | Batch_idx: 260 |  Loss_1: (0.0021) | Acc_1: (99.95%) (33392/33408)\n",
      "Epoch: 136 | Batch_idx: 270 |  Loss_1: (0.0020) | Acc_1: (99.95%) (34672/34688)\n",
      "Epoch: 136 | Batch_idx: 280 |  Loss_1: (0.0020) | Acc_1: (99.95%) (35951/35968)\n",
      "Epoch: 136 | Batch_idx: 290 |  Loss_1: (0.0019) | Acc_1: (99.95%) (37231/37248)\n",
      "Epoch: 136 | Batch_idx: 300 |  Loss_1: (0.0020) | Acc_1: (99.95%) (38510/38528)\n",
      "Epoch: 136 | Batch_idx: 310 |  Loss_1: (0.0019) | Acc_1: (99.95%) (39790/39808)\n",
      "Epoch: 136 | Batch_idx: 320 |  Loss_1: (0.0019) | Acc_1: (99.95%) (41069/41088)\n",
      "Epoch: 136 | Batch_idx: 330 |  Loss_1: (0.0020) | Acc_1: (99.95%) (42345/42368)\n",
      "Epoch: 136 | Batch_idx: 340 |  Loss_1: (0.0021) | Acc_1: (99.94%) (43623/43648)\n",
      "Epoch: 136 | Batch_idx: 350 |  Loss_1: (0.0021) | Acc_1: (99.94%) (44903/44928)\n",
      "Epoch: 136 | Batch_idx: 360 |  Loss_1: (0.0020) | Acc_1: (99.95%) (46183/46208)\n",
      "Epoch: 136 | Batch_idx: 370 |  Loss_1: (0.0020) | Acc_1: (99.95%) (47462/47488)\n",
      "Epoch: 136 | Batch_idx: 380 |  Loss_1: (0.0020) | Acc_1: (99.95%) (48742/48768)\n",
      "Epoch: 136 | Batch_idx: 390 |  Loss_1: (0.0021) | Acc_1: (99.94%) (49972/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4722) | Acc: (93.03%) (9303/10000)\n",
      "Epoch: 137 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 137 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 137 | Batch_idx: 20 |  Loss_1: (0.0013) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 137 | Batch_idx: 30 |  Loss_1: (0.0011) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 137 | Batch_idx: 40 |  Loss_1: (0.0010) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 137 | Batch_idx: 50 |  Loss_1: (0.0010) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 137 | Batch_idx: 60 |  Loss_1: (0.0012) | Acc_1: (99.96%) (7805/7808)\n",
      "Epoch: 137 | Batch_idx: 70 |  Loss_1: (0.0012) | Acc_1: (99.96%) (9084/9088)\n",
      "Epoch: 137 | Batch_idx: 80 |  Loss_1: (0.0011) | Acc_1: (99.96%) (10364/10368)\n",
      "Epoch: 137 | Batch_idx: 90 |  Loss_1: (0.0010) | Acc_1: (99.97%) (11644/11648)\n",
      "Epoch: 137 | Batch_idx: 100 |  Loss_1: (0.0010) | Acc_1: (99.97%) (12924/12928)\n",
      "Epoch: 137 | Batch_idx: 110 |  Loss_1: (0.0010) | Acc_1: (99.97%) (14204/14208)\n",
      "Epoch: 137 | Batch_idx: 120 |  Loss_1: (0.0009) | Acc_1: (99.97%) (15484/15488)\n",
      "Epoch: 137 | Batch_idx: 130 |  Loss_1: (0.0012) | Acc_1: (99.96%) (16762/16768)\n",
      "Epoch: 137 | Batch_idx: 140 |  Loss_1: (0.0014) | Acc_1: (99.96%) (18041/18048)\n",
      "Epoch: 137 | Batch_idx: 150 |  Loss_1: (0.0014) | Acc_1: (99.96%) (19321/19328)\n",
      "Epoch: 137 | Batch_idx: 160 |  Loss_1: (0.0013) | Acc_1: (99.97%) (20601/20608)\n",
      "Epoch: 137 | Batch_idx: 170 |  Loss_1: (0.0016) | Acc_1: (99.96%) (21880/21888)\n",
      "Epoch: 137 | Batch_idx: 180 |  Loss_1: (0.0015) | Acc_1: (99.97%) (23160/23168)\n",
      "Epoch: 137 | Batch_idx: 190 |  Loss_1: (0.0016) | Acc_1: (99.96%) (24439/24448)\n",
      "Epoch: 137 | Batch_idx: 200 |  Loss_1: (0.0015) | Acc_1: (99.97%) (25719/25728)\n",
      "Epoch: 137 | Batch_idx: 210 |  Loss_1: (0.0015) | Acc_1: (99.97%) (26999/27008)\n",
      "Epoch: 137 | Batch_idx: 220 |  Loss_1: (0.0015) | Acc_1: (99.97%) (28279/28288)\n",
      "Epoch: 137 | Batch_idx: 230 |  Loss_1: (0.0015) | Acc_1: (99.97%) (29559/29568)\n",
      "Epoch: 137 | Batch_idx: 240 |  Loss_1: (0.0014) | Acc_1: (99.97%) (30839/30848)\n",
      "Epoch: 137 | Batch_idx: 250 |  Loss_1: (0.0014) | Acc_1: (99.97%) (32119/32128)\n",
      "Epoch: 137 | Batch_idx: 260 |  Loss_1: (0.0014) | Acc_1: (99.97%) (33399/33408)\n",
      "Epoch: 137 | Batch_idx: 270 |  Loss_1: (0.0016) | Acc_1: (99.97%) (34678/34688)\n",
      "Epoch: 137 | Batch_idx: 280 |  Loss_1: (0.0016) | Acc_1: (99.97%) (35957/35968)\n",
      "Epoch: 137 | Batch_idx: 290 |  Loss_1: (0.0016) | Acc_1: (99.97%) (37237/37248)\n",
      "Epoch: 137 | Batch_idx: 300 |  Loss_1: (0.0015) | Acc_1: (99.97%) (38517/38528)\n",
      "Epoch: 137 | Batch_idx: 310 |  Loss_1: (0.0015) | Acc_1: (99.97%) (39797/39808)\n",
      "Epoch: 137 | Batch_idx: 320 |  Loss_1: (0.0015) | Acc_1: (99.97%) (41076/41088)\n",
      "Epoch: 137 | Batch_idx: 330 |  Loss_1: (0.0016) | Acc_1: (99.97%) (42355/42368)\n",
      "Epoch: 137 | Batch_idx: 340 |  Loss_1: (0.0015) | Acc_1: (99.97%) (43635/43648)\n",
      "Epoch: 137 | Batch_idx: 350 |  Loss_1: (0.0015) | Acc_1: (99.97%) (44914/44928)\n",
      "Epoch: 137 | Batch_idx: 360 |  Loss_1: (0.0015) | Acc_1: (99.97%) (46192/46208)\n",
      "Epoch: 137 | Batch_idx: 370 |  Loss_1: (0.0016) | Acc_1: (99.96%) (47471/47488)\n",
      "Epoch: 137 | Batch_idx: 380 |  Loss_1: (0.0016) | Acc_1: (99.97%) (48751/48768)\n",
      "Epoch: 137 | Batch_idx: 390 |  Loss_1: (0.0016) | Acc_1: (99.97%) (49983/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4587) | Acc: (92.86%) (9286/10000)\n",
      "Epoch: 138 | Batch_idx: 0 |  Loss_1: (0.0115) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 138 | Batch_idx: 10 |  Loss_1: (0.0012) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 138 | Batch_idx: 20 |  Loss_1: (0.0018) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 138 | Batch_idx: 30 |  Loss_1: (0.0013) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 138 | Batch_idx: 40 |  Loss_1: (0.0020) | Acc_1: (99.92%) (5244/5248)\n",
      "Epoch: 138 | Batch_idx: 50 |  Loss_1: (0.0017) | Acc_1: (99.94%) (6524/6528)\n",
      "Epoch: 138 | Batch_idx: 60 |  Loss_1: (0.0016) | Acc_1: (99.95%) (7804/7808)\n",
      "Epoch: 138 | Batch_idx: 70 |  Loss_1: (0.0015) | Acc_1: (99.96%) (9084/9088)\n",
      "Epoch: 138 | Batch_idx: 80 |  Loss_1: (0.0015) | Acc_1: (99.95%) (10363/10368)\n",
      "Epoch: 138 | Batch_idx: 90 |  Loss_1: (0.0016) | Acc_1: (99.95%) (11642/11648)\n",
      "Epoch: 138 | Batch_idx: 100 |  Loss_1: (0.0016) | Acc_1: (99.95%) (12922/12928)\n",
      "Epoch: 138 | Batch_idx: 110 |  Loss_1: (0.0015) | Acc_1: (99.96%) (14202/14208)\n",
      "Epoch: 138 | Batch_idx: 120 |  Loss_1: (0.0018) | Acc_1: (99.95%) (15480/15488)\n",
      "Epoch: 138 | Batch_idx: 130 |  Loss_1: (0.0017) | Acc_1: (99.95%) (16759/16768)\n",
      "Epoch: 138 | Batch_idx: 140 |  Loss_1: (0.0017) | Acc_1: (99.94%) (18037/18048)\n",
      "Epoch: 138 | Batch_idx: 150 |  Loss_1: (0.0017) | Acc_1: (99.94%) (19317/19328)\n",
      "Epoch: 138 | Batch_idx: 160 |  Loss_1: (0.0019) | Acc_1: (99.93%) (20594/20608)\n",
      "Epoch: 138 | Batch_idx: 170 |  Loss_1: (0.0018) | Acc_1: (99.94%) (21874/21888)\n",
      "Epoch: 138 | Batch_idx: 180 |  Loss_1: (0.0018) | Acc_1: (99.94%) (23153/23168)\n",
      "Epoch: 138 | Batch_idx: 190 |  Loss_1: (0.0018) | Acc_1: (99.94%) (24433/24448)\n",
      "Epoch: 138 | Batch_idx: 200 |  Loss_1: (0.0017) | Acc_1: (99.94%) (25712/25728)\n",
      "Epoch: 138 | Batch_idx: 210 |  Loss_1: (0.0018) | Acc_1: (99.94%) (26991/27008)\n",
      "Epoch: 138 | Batch_idx: 220 |  Loss_1: (0.0017) | Acc_1: (99.94%) (28271/28288)\n",
      "Epoch: 138 | Batch_idx: 230 |  Loss_1: (0.0017) | Acc_1: (99.94%) (29551/29568)\n",
      "Epoch: 138 | Batch_idx: 240 |  Loss_1: (0.0016) | Acc_1: (99.94%) (30830/30848)\n",
      "Epoch: 138 | Batch_idx: 250 |  Loss_1: (0.0016) | Acc_1: (99.94%) (32110/32128)\n",
      "Epoch: 138 | Batch_idx: 260 |  Loss_1: (0.0016) | Acc_1: (99.95%) (33390/33408)\n",
      "Epoch: 138 | Batch_idx: 270 |  Loss_1: (0.0015) | Acc_1: (99.95%) (34670/34688)\n",
      "Epoch: 138 | Batch_idx: 280 |  Loss_1: (0.0015) | Acc_1: (99.95%) (35950/35968)\n",
      "Epoch: 138 | Batch_idx: 290 |  Loss_1: (0.0015) | Acc_1: (99.95%) (37230/37248)\n",
      "Epoch: 138 | Batch_idx: 300 |  Loss_1: (0.0014) | Acc_1: (99.95%) (38510/38528)\n",
      "Epoch: 138 | Batch_idx: 310 |  Loss_1: (0.0014) | Acc_1: (99.95%) (39790/39808)\n",
      "Epoch: 138 | Batch_idx: 320 |  Loss_1: (0.0014) | Acc_1: (99.95%) (41069/41088)\n",
      "Epoch: 138 | Batch_idx: 330 |  Loss_1: (0.0014) | Acc_1: (99.96%) (42349/42368)\n",
      "Epoch: 138 | Batch_idx: 340 |  Loss_1: (0.0014) | Acc_1: (99.96%) (43629/43648)\n",
      "Epoch: 138 | Batch_idx: 350 |  Loss_1: (0.0013) | Acc_1: (99.96%) (44909/44928)\n",
      "Epoch: 138 | Batch_idx: 360 |  Loss_1: (0.0013) | Acc_1: (99.96%) (46189/46208)\n",
      "Epoch: 138 | Batch_idx: 370 |  Loss_1: (0.0013) | Acc_1: (99.96%) (47468/47488)\n",
      "Epoch: 138 | Batch_idx: 380 |  Loss_1: (0.0013) | Acc_1: (99.96%) (48748/48768)\n",
      "Epoch: 138 | Batch_idx: 390 |  Loss_1: (0.0013) | Acc_1: (99.96%) (49980/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4460) | Acc: (92.96%) (9296/10000)\n",
      "Epoch: 139 | Batch_idx: 0 |  Loss_1: (0.0007) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 139 | Batch_idx: 10 |  Loss_1: (0.0031) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 139 | Batch_idx: 20 |  Loss_1: (0.0021) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 139 | Batch_idx: 30 |  Loss_1: (0.0015) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 139 | Batch_idx: 40 |  Loss_1: (0.0013) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 139 | Batch_idx: 50 |  Loss_1: (0.0011) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 139 | Batch_idx: 60 |  Loss_1: (0.0016) | Acc_1: (99.95%) (7804/7808)\n",
      "Epoch: 139 | Batch_idx: 70 |  Loss_1: (0.0014) | Acc_1: (99.96%) (9084/9088)\n",
      "Epoch: 139 | Batch_idx: 80 |  Loss_1: (0.0014) | Acc_1: (99.96%) (10364/10368)\n",
      "Epoch: 139 | Batch_idx: 90 |  Loss_1: (0.0013) | Acc_1: (99.97%) (11644/11648)\n",
      "Epoch: 139 | Batch_idx: 100 |  Loss_1: (0.0012) | Acc_1: (99.97%) (12924/12928)\n",
      "Epoch: 139 | Batch_idx: 110 |  Loss_1: (0.0011) | Acc_1: (99.97%) (14204/14208)\n",
      "Epoch: 139 | Batch_idx: 120 |  Loss_1: (0.0011) | Acc_1: (99.97%) (15484/15488)\n",
      "Epoch: 139 | Batch_idx: 130 |  Loss_1: (0.0010) | Acc_1: (99.98%) (16764/16768)\n",
      "Epoch: 139 | Batch_idx: 140 |  Loss_1: (0.0011) | Acc_1: (99.97%) (18043/18048)\n",
      "Epoch: 139 | Batch_idx: 150 |  Loss_1: (0.0011) | Acc_1: (99.97%) (19323/19328)\n",
      "Epoch: 139 | Batch_idx: 160 |  Loss_1: (0.0011) | Acc_1: (99.98%) (20603/20608)\n",
      "Epoch: 139 | Batch_idx: 170 |  Loss_1: (0.0011) | Acc_1: (99.97%) (21882/21888)\n",
      "Epoch: 139 | Batch_idx: 180 |  Loss_1: (0.0011) | Acc_1: (99.97%) (23162/23168)\n",
      "Epoch: 139 | Batch_idx: 190 |  Loss_1: (0.0011) | Acc_1: (99.97%) (24441/24448)\n",
      "Epoch: 139 | Batch_idx: 200 |  Loss_1: (0.0011) | Acc_1: (99.97%) (25721/25728)\n",
      "Epoch: 139 | Batch_idx: 210 |  Loss_1: (0.0011) | Acc_1: (99.97%) (27001/27008)\n",
      "Epoch: 139 | Batch_idx: 220 |  Loss_1: (0.0011) | Acc_1: (99.98%) (28281/28288)\n",
      "Epoch: 139 | Batch_idx: 230 |  Loss_1: (0.0012) | Acc_1: (99.97%) (29558/29568)\n",
      "Epoch: 139 | Batch_idx: 240 |  Loss_1: (0.0012) | Acc_1: (99.97%) (30838/30848)\n",
      "Epoch: 139 | Batch_idx: 250 |  Loss_1: (0.0012) | Acc_1: (99.97%) (32117/32128)\n",
      "Epoch: 139 | Batch_idx: 260 |  Loss_1: (0.0012) | Acc_1: (99.96%) (33396/33408)\n",
      "Epoch: 139 | Batch_idx: 270 |  Loss_1: (0.0012) | Acc_1: (99.97%) (34676/34688)\n",
      "Epoch: 139 | Batch_idx: 280 |  Loss_1: (0.0012) | Acc_1: (99.97%) (35956/35968)\n",
      "Epoch: 139 | Batch_idx: 290 |  Loss_1: (0.0011) | Acc_1: (99.97%) (37236/37248)\n",
      "Epoch: 139 | Batch_idx: 300 |  Loss_1: (0.0011) | Acc_1: (99.97%) (38516/38528)\n",
      "Epoch: 139 | Batch_idx: 310 |  Loss_1: (0.0011) | Acc_1: (99.97%) (39796/39808)\n",
      "Epoch: 139 | Batch_idx: 320 |  Loss_1: (0.0011) | Acc_1: (99.97%) (41076/41088)\n",
      "Epoch: 139 | Batch_idx: 330 |  Loss_1: (0.0012) | Acc_1: (99.97%) (42355/42368)\n",
      "Epoch: 139 | Batch_idx: 340 |  Loss_1: (0.0011) | Acc_1: (99.97%) (43635/43648)\n",
      "Epoch: 139 | Batch_idx: 350 |  Loss_1: (0.0011) | Acc_1: (99.97%) (44915/44928)\n",
      "Epoch: 139 | Batch_idx: 360 |  Loss_1: (0.0012) | Acc_1: (99.97%) (46194/46208)\n",
      "Epoch: 139 | Batch_idx: 370 |  Loss_1: (0.0012) | Acc_1: (99.97%) (47474/47488)\n",
      "Epoch: 139 | Batch_idx: 380 |  Loss_1: (0.0012) | Acc_1: (99.97%) (48752/48768)\n",
      "Epoch: 139 | Batch_idx: 390 |  Loss_1: (0.0012) | Acc_1: (99.97%) (49983/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4695) | Acc: (92.84%) (9284/10000)\n",
      "Epoch: 140 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 140 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 140 | Batch_idx: 20 |  Loss_1: (0.0006) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 140 | Batch_idx: 30 |  Loss_1: (0.0007) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 140 | Batch_idx: 40 |  Loss_1: (0.0007) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 140 | Batch_idx: 50 |  Loss_1: (0.0007) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 140 | Batch_idx: 60 |  Loss_1: (0.0008) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 140 | Batch_idx: 70 |  Loss_1: (0.0008) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 140 | Batch_idx: 80 |  Loss_1: (0.0007) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 140 | Batch_idx: 90 |  Loss_1: (0.0007) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 140 | Batch_idx: 100 |  Loss_1: (0.0008) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 140 | Batch_idx: 110 |  Loss_1: (0.0007) | Acc_1: (99.99%) (14206/14208)\n",
      "Epoch: 140 | Batch_idx: 120 |  Loss_1: (0.0008) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 140 | Batch_idx: 130 |  Loss_1: (0.0007) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 140 | Batch_idx: 140 |  Loss_1: (0.0007) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 140 | Batch_idx: 150 |  Loss_1: (0.0007) | Acc_1: (99.99%) (19326/19328)\n",
      "Epoch: 140 | Batch_idx: 160 |  Loss_1: (0.0007) | Acc_1: (99.99%) (20606/20608)\n",
      "Epoch: 140 | Batch_idx: 170 |  Loss_1: (0.0007) | Acc_1: (99.99%) (21886/21888)\n",
      "Epoch: 140 | Batch_idx: 180 |  Loss_1: (0.0007) | Acc_1: (99.99%) (23166/23168)\n",
      "Epoch: 140 | Batch_idx: 190 |  Loss_1: (0.0007) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 140 | Batch_idx: 200 |  Loss_1: (0.0007) | Acc_1: (99.99%) (25726/25728)\n",
      "Epoch: 140 | Batch_idx: 210 |  Loss_1: (0.0009) | Acc_1: (99.99%) (27005/27008)\n",
      "Epoch: 140 | Batch_idx: 220 |  Loss_1: (0.0009) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 140 | Batch_idx: 230 |  Loss_1: (0.0010) | Acc_1: (99.98%) (29563/29568)\n",
      "Epoch: 140 | Batch_idx: 240 |  Loss_1: (0.0010) | Acc_1: (99.98%) (30843/30848)\n",
      "Epoch: 140 | Batch_idx: 250 |  Loss_1: (0.0010) | Acc_1: (99.98%) (32123/32128)\n",
      "Epoch: 140 | Batch_idx: 260 |  Loss_1: (0.0010) | Acc_1: (99.98%) (33402/33408)\n",
      "Epoch: 140 | Batch_idx: 270 |  Loss_1: (0.0010) | Acc_1: (99.98%) (34682/34688)\n",
      "Epoch: 140 | Batch_idx: 280 |  Loss_1: (0.0010) | Acc_1: (99.98%) (35962/35968)\n",
      "Epoch: 140 | Batch_idx: 290 |  Loss_1: (0.0010) | Acc_1: (99.98%) (37241/37248)\n",
      "Epoch: 140 | Batch_idx: 300 |  Loss_1: (0.0009) | Acc_1: (99.98%) (38521/38528)\n",
      "Epoch: 140 | Batch_idx: 310 |  Loss_1: (0.0009) | Acc_1: (99.98%) (39801/39808)\n",
      "Epoch: 140 | Batch_idx: 320 |  Loss_1: (0.0009) | Acc_1: (99.98%) (41081/41088)\n",
      "Epoch: 140 | Batch_idx: 330 |  Loss_1: (0.0009) | Acc_1: (99.98%) (42361/42368)\n",
      "Epoch: 140 | Batch_idx: 340 |  Loss_1: (0.0009) | Acc_1: (99.98%) (43641/43648)\n",
      "Epoch: 140 | Batch_idx: 350 |  Loss_1: (0.0009) | Acc_1: (99.98%) (44921/44928)\n",
      "Epoch: 140 | Batch_idx: 360 |  Loss_1: (0.0009) | Acc_1: (99.98%) (46201/46208)\n",
      "Epoch: 140 | Batch_idx: 370 |  Loss_1: (0.0009) | Acc_1: (99.99%) (47481/47488)\n",
      "Epoch: 140 | Batch_idx: 380 |  Loss_1: (0.0008) | Acc_1: (99.99%) (48761/48768)\n",
      "Epoch: 140 | Batch_idx: 390 |  Loss_1: (0.0008) | Acc_1: (99.99%) (49993/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4605) | Acc: (93.10%) (9310/10000)\n",
      "Epoch: 141 | Batch_idx: 0 |  Loss_1: (0.0006) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 141 | Batch_idx: 10 |  Loss_1: (0.0004) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 141 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 141 | Batch_idx: 30 |  Loss_1: (0.0004) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 141 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 141 | Batch_idx: 50 |  Loss_1: (0.0004) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 141 | Batch_idx: 60 |  Loss_1: (0.0006) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 141 | Batch_idx: 70 |  Loss_1: (0.0005) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 141 | Batch_idx: 80 |  Loss_1: (0.0005) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 141 | Batch_idx: 90 |  Loss_1: (0.0005) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 141 | Batch_idx: 100 |  Loss_1: (0.0006) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 141 | Batch_idx: 110 |  Loss_1: (0.0010) | Acc_1: (99.98%) (14205/14208)\n",
      "Epoch: 141 | Batch_idx: 120 |  Loss_1: (0.0010) | Acc_1: (99.98%) (15485/15488)\n",
      "Epoch: 141 | Batch_idx: 130 |  Loss_1: (0.0011) | Acc_1: (99.98%) (16764/16768)\n",
      "Epoch: 141 | Batch_idx: 140 |  Loss_1: (0.0011) | Acc_1: (99.97%) (18043/18048)\n",
      "Epoch: 141 | Batch_idx: 150 |  Loss_1: (0.0011) | Acc_1: (99.97%) (19323/19328)\n",
      "Epoch: 141 | Batch_idx: 160 |  Loss_1: (0.0010) | Acc_1: (99.98%) (20603/20608)\n",
      "Epoch: 141 | Batch_idx: 170 |  Loss_1: (0.0010) | Acc_1: (99.98%) (21883/21888)\n",
      "Epoch: 141 | Batch_idx: 180 |  Loss_1: (0.0010) | Acc_1: (99.98%) (23163/23168)\n",
      "Epoch: 141 | Batch_idx: 190 |  Loss_1: (0.0010) | Acc_1: (99.98%) (24442/24448)\n",
      "Epoch: 141 | Batch_idx: 200 |  Loss_1: (0.0011) | Acc_1: (99.98%) (25722/25728)\n",
      "Epoch: 141 | Batch_idx: 210 |  Loss_1: (0.0010) | Acc_1: (99.98%) (27002/27008)\n",
      "Epoch: 141 | Batch_idx: 220 |  Loss_1: (0.0010) | Acc_1: (99.98%) (28282/28288)\n",
      "Epoch: 141 | Batch_idx: 230 |  Loss_1: (0.0010) | Acc_1: (99.98%) (29561/29568)\n",
      "Epoch: 141 | Batch_idx: 240 |  Loss_1: (0.0010) | Acc_1: (99.97%) (30840/30848)\n",
      "Epoch: 141 | Batch_idx: 250 |  Loss_1: (0.0010) | Acc_1: (99.98%) (32120/32128)\n",
      "Epoch: 141 | Batch_idx: 260 |  Loss_1: (0.0010) | Acc_1: (99.97%) (33399/33408)\n",
      "Epoch: 141 | Batch_idx: 270 |  Loss_1: (0.0010) | Acc_1: (99.97%) (34679/34688)\n",
      "Epoch: 141 | Batch_idx: 280 |  Loss_1: (0.0010) | Acc_1: (99.97%) (35959/35968)\n",
      "Epoch: 141 | Batch_idx: 290 |  Loss_1: (0.0009) | Acc_1: (99.98%) (37239/37248)\n",
      "Epoch: 141 | Batch_idx: 300 |  Loss_1: (0.0010) | Acc_1: (99.97%) (38518/38528)\n",
      "Epoch: 141 | Batch_idx: 310 |  Loss_1: (0.0010) | Acc_1: (99.97%) (39798/39808)\n",
      "Epoch: 141 | Batch_idx: 320 |  Loss_1: (0.0010) | Acc_1: (99.98%) (41078/41088)\n",
      "Epoch: 141 | Batch_idx: 330 |  Loss_1: (0.0009) | Acc_1: (99.98%) (42358/42368)\n",
      "Epoch: 141 | Batch_idx: 340 |  Loss_1: (0.0009) | Acc_1: (99.98%) (43638/43648)\n",
      "Epoch: 141 | Batch_idx: 350 |  Loss_1: (0.0010) | Acc_1: (99.97%) (44916/44928)\n",
      "Epoch: 141 | Batch_idx: 360 |  Loss_1: (0.0009) | Acc_1: (99.97%) (46196/46208)\n",
      "Epoch: 141 | Batch_idx: 370 |  Loss_1: (0.0009) | Acc_1: (99.97%) (47476/47488)\n",
      "Epoch: 141 | Batch_idx: 380 |  Loss_1: (0.0009) | Acc_1: (99.98%) (48756/48768)\n",
      "Epoch: 141 | Batch_idx: 390 |  Loss_1: (0.0010) | Acc_1: (99.97%) (49987/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4538) | Acc: (92.97%) (9297/10000)\n",
      "Epoch: 142 | Batch_idx: 0 |  Loss_1: (0.0006) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 142 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 142 | Batch_idx: 20 |  Loss_1: (0.0025) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 142 | Batch_idx: 30 |  Loss_1: (0.0018) | Acc_1: (99.92%) (3965/3968)\n",
      "Epoch: 142 | Batch_idx: 40 |  Loss_1: (0.0017) | Acc_1: (99.92%) (5244/5248)\n",
      "Epoch: 142 | Batch_idx: 50 |  Loss_1: (0.0014) | Acc_1: (99.94%) (6524/6528)\n",
      "Epoch: 142 | Batch_idx: 60 |  Loss_1: (0.0015) | Acc_1: (99.94%) (7803/7808)\n",
      "Epoch: 142 | Batch_idx: 70 |  Loss_1: (0.0014) | Acc_1: (99.94%) (9083/9088)\n",
      "Epoch: 142 | Batch_idx: 80 |  Loss_1: (0.0016) | Acc_1: (99.93%) (10361/10368)\n",
      "Epoch: 142 | Batch_idx: 90 |  Loss_1: (0.0016) | Acc_1: (99.93%) (11640/11648)\n",
      "Epoch: 142 | Batch_idx: 100 |  Loss_1: (0.0015) | Acc_1: (99.94%) (12920/12928)\n",
      "Epoch: 142 | Batch_idx: 110 |  Loss_1: (0.0014) | Acc_1: (99.94%) (14200/14208)\n",
      "Epoch: 142 | Batch_idx: 120 |  Loss_1: (0.0013) | Acc_1: (99.95%) (15480/15488)\n",
      "Epoch: 142 | Batch_idx: 130 |  Loss_1: (0.0014) | Acc_1: (99.95%) (16759/16768)\n",
      "Epoch: 142 | Batch_idx: 140 |  Loss_1: (0.0014) | Acc_1: (99.95%) (18039/18048)\n",
      "Epoch: 142 | Batch_idx: 150 |  Loss_1: (0.0013) | Acc_1: (99.95%) (19319/19328)\n",
      "Epoch: 142 | Batch_idx: 160 |  Loss_1: (0.0012) | Acc_1: (99.96%) (20599/20608)\n",
      "Epoch: 142 | Batch_idx: 170 |  Loss_1: (0.0012) | Acc_1: (99.96%) (21879/21888)\n",
      "Epoch: 142 | Batch_idx: 180 |  Loss_1: (0.0011) | Acc_1: (99.96%) (23159/23168)\n",
      "Epoch: 142 | Batch_idx: 190 |  Loss_1: (0.0011) | Acc_1: (99.96%) (24438/24448)\n",
      "Epoch: 142 | Batch_idx: 200 |  Loss_1: (0.0011) | Acc_1: (99.96%) (25718/25728)\n",
      "Epoch: 142 | Batch_idx: 210 |  Loss_1: (0.0011) | Acc_1: (99.96%) (26998/27008)\n",
      "Epoch: 142 | Batch_idx: 220 |  Loss_1: (0.0010) | Acc_1: (99.96%) (28278/28288)\n",
      "Epoch: 142 | Batch_idx: 230 |  Loss_1: (0.0011) | Acc_1: (99.96%) (29557/29568)\n",
      "Epoch: 142 | Batch_idx: 240 |  Loss_1: (0.0011) | Acc_1: (99.96%) (30837/30848)\n",
      "Epoch: 142 | Batch_idx: 250 |  Loss_1: (0.0010) | Acc_1: (99.97%) (32117/32128)\n",
      "Epoch: 142 | Batch_idx: 260 |  Loss_1: (0.0010) | Acc_1: (99.97%) (33397/33408)\n",
      "Epoch: 142 | Batch_idx: 270 |  Loss_1: (0.0010) | Acc_1: (99.97%) (34677/34688)\n",
      "Epoch: 142 | Batch_idx: 280 |  Loss_1: (0.0010) | Acc_1: (99.97%) (35957/35968)\n",
      "Epoch: 142 | Batch_idx: 290 |  Loss_1: (0.0010) | Acc_1: (99.97%) (37236/37248)\n",
      "Epoch: 142 | Batch_idx: 300 |  Loss_1: (0.0010) | Acc_1: (99.96%) (38514/38528)\n",
      "Epoch: 142 | Batch_idx: 310 |  Loss_1: (0.0011) | Acc_1: (99.96%) (39793/39808)\n",
      "Epoch: 142 | Batch_idx: 320 |  Loss_1: (0.0011) | Acc_1: (99.96%) (41073/41088)\n",
      "Epoch: 142 | Batch_idx: 330 |  Loss_1: (0.0010) | Acc_1: (99.96%) (42353/42368)\n",
      "Epoch: 142 | Batch_idx: 340 |  Loss_1: (0.0011) | Acc_1: (99.96%) (43631/43648)\n",
      "Epoch: 142 | Batch_idx: 350 |  Loss_1: (0.0012) | Acc_1: (99.96%) (44910/44928)\n",
      "Epoch: 142 | Batch_idx: 360 |  Loss_1: (0.0012) | Acc_1: (99.96%) (46190/46208)\n",
      "Epoch: 142 | Batch_idx: 370 |  Loss_1: (0.0012) | Acc_1: (99.96%) (47470/47488)\n",
      "Epoch: 142 | Batch_idx: 380 |  Loss_1: (0.0012) | Acc_1: (99.96%) (48750/48768)\n",
      "Epoch: 142 | Batch_idx: 390 |  Loss_1: (0.0012) | Acc_1: (99.96%) (49981/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4680) | Acc: (92.91%) (9291/10000)\n",
      "Epoch: 143 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 143 | Batch_idx: 10 |  Loss_1: (0.0009) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 143 | Batch_idx: 20 |  Loss_1: (0.0011) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 143 | Batch_idx: 30 |  Loss_1: (0.0008) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 143 | Batch_idx: 40 |  Loss_1: (0.0007) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 143 | Batch_idx: 50 |  Loss_1: (0.0008) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 143 | Batch_idx: 60 |  Loss_1: (0.0008) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 143 | Batch_idx: 70 |  Loss_1: (0.0007) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 143 | Batch_idx: 80 |  Loss_1: (0.0007) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 143 | Batch_idx: 90 |  Loss_1: (0.0008) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 143 | Batch_idx: 100 |  Loss_1: (0.0007) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 143 | Batch_idx: 110 |  Loss_1: (0.0007) | Acc_1: (99.98%) (14205/14208)\n",
      "Epoch: 143 | Batch_idx: 120 |  Loss_1: (0.0007) | Acc_1: (99.98%) (15485/15488)\n",
      "Epoch: 143 | Batch_idx: 130 |  Loss_1: (0.0007) | Acc_1: (99.98%) (16764/16768)\n",
      "Epoch: 143 | Batch_idx: 140 |  Loss_1: (0.0007) | Acc_1: (99.98%) (18044/18048)\n",
      "Epoch: 143 | Batch_idx: 150 |  Loss_1: (0.0007) | Acc_1: (99.98%) (19324/19328)\n",
      "Epoch: 143 | Batch_idx: 160 |  Loss_1: (0.0007) | Acc_1: (99.98%) (20604/20608)\n",
      "Epoch: 143 | Batch_idx: 170 |  Loss_1: (0.0006) | Acc_1: (99.98%) (21884/21888)\n",
      "Epoch: 143 | Batch_idx: 180 |  Loss_1: (0.0006) | Acc_1: (99.98%) (23164/23168)\n",
      "Epoch: 143 | Batch_idx: 190 |  Loss_1: (0.0006) | Acc_1: (99.98%) (24444/24448)\n",
      "Epoch: 143 | Batch_idx: 200 |  Loss_1: (0.0006) | Acc_1: (99.98%) (25724/25728)\n",
      "Epoch: 143 | Batch_idx: 210 |  Loss_1: (0.0006) | Acc_1: (99.99%) (27004/27008)\n",
      "Epoch: 143 | Batch_idx: 220 |  Loss_1: (0.0006) | Acc_1: (99.99%) (28284/28288)\n",
      "Epoch: 143 | Batch_idx: 230 |  Loss_1: (0.0006) | Acc_1: (99.98%) (29563/29568)\n",
      "Epoch: 143 | Batch_idx: 240 |  Loss_1: (0.0006) | Acc_1: (99.98%) (30842/30848)\n",
      "Epoch: 143 | Batch_idx: 250 |  Loss_1: (0.0006) | Acc_1: (99.98%) (32122/32128)\n",
      "Epoch: 143 | Batch_idx: 260 |  Loss_1: (0.0006) | Acc_1: (99.98%) (33402/33408)\n",
      "Epoch: 143 | Batch_idx: 270 |  Loss_1: (0.0006) | Acc_1: (99.98%) (34682/34688)\n",
      "Epoch: 143 | Batch_idx: 280 |  Loss_1: (0.0006) | Acc_1: (99.98%) (35962/35968)\n",
      "Epoch: 143 | Batch_idx: 290 |  Loss_1: (0.0006) | Acc_1: (99.98%) (37241/37248)\n",
      "Epoch: 143 | Batch_idx: 300 |  Loss_1: (0.0006) | Acc_1: (99.98%) (38521/38528)\n",
      "Epoch: 143 | Batch_idx: 310 |  Loss_1: (0.0006) | Acc_1: (99.98%) (39801/39808)\n",
      "Epoch: 143 | Batch_idx: 320 |  Loss_1: (0.0006) | Acc_1: (99.98%) (41081/41088)\n",
      "Epoch: 143 | Batch_idx: 330 |  Loss_1: (0.0005) | Acc_1: (99.98%) (42361/42368)\n",
      "Epoch: 143 | Batch_idx: 340 |  Loss_1: (0.0005) | Acc_1: (99.98%) (43641/43648)\n",
      "Epoch: 143 | Batch_idx: 350 |  Loss_1: (0.0006) | Acc_1: (99.98%) (44919/44928)\n",
      "Epoch: 143 | Batch_idx: 360 |  Loss_1: (0.0006) | Acc_1: (99.98%) (46199/46208)\n",
      "Epoch: 143 | Batch_idx: 370 |  Loss_1: (0.0006) | Acc_1: (99.98%) (47479/47488)\n",
      "Epoch: 143 | Batch_idx: 380 |  Loss_1: (0.0006) | Acc_1: (99.98%) (48759/48768)\n",
      "Epoch: 143 | Batch_idx: 390 |  Loss_1: (0.0006) | Acc_1: (99.98%) (49991/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4584) | Acc: (93.18%) (9318/10000)\n",
      "Epoch: 144 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 144 | Batch_idx: 10 |  Loss_1: (0.0008) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 144 | Batch_idx: 20 |  Loss_1: (0.0006) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 144 | Batch_idx: 30 |  Loss_1: (0.0005) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 144 | Batch_idx: 40 |  Loss_1: (0.0006) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 144 | Batch_idx: 50 |  Loss_1: (0.0006) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 144 | Batch_idx: 60 |  Loss_1: (0.0006) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 144 | Batch_idx: 70 |  Loss_1: (0.0007) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 144 | Batch_idx: 80 |  Loss_1: (0.0007) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 144 | Batch_idx: 90 |  Loss_1: (0.0008) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 144 | Batch_idx: 100 |  Loss_1: (0.0008) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 144 | Batch_idx: 110 |  Loss_1: (0.0007) | Acc_1: (99.99%) (14206/14208)\n",
      "Epoch: 144 | Batch_idx: 120 |  Loss_1: (0.0007) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 144 | Batch_idx: 130 |  Loss_1: (0.0007) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 144 | Batch_idx: 140 |  Loss_1: (0.0006) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 144 | Batch_idx: 150 |  Loss_1: (0.0007) | Acc_1: (99.98%) (19325/19328)\n",
      "Epoch: 144 | Batch_idx: 160 |  Loss_1: (0.0007) | Acc_1: (99.98%) (20604/20608)\n",
      "Epoch: 144 | Batch_idx: 170 |  Loss_1: (0.0009) | Acc_1: (99.98%) (21883/21888)\n",
      "Epoch: 144 | Batch_idx: 180 |  Loss_1: (0.0009) | Acc_1: (99.98%) (23163/23168)\n",
      "Epoch: 144 | Batch_idx: 190 |  Loss_1: (0.0008) | Acc_1: (99.98%) (24443/24448)\n",
      "Epoch: 144 | Batch_idx: 200 |  Loss_1: (0.0008) | Acc_1: (99.98%) (25723/25728)\n",
      "Epoch: 144 | Batch_idx: 210 |  Loss_1: (0.0008) | Acc_1: (99.98%) (27003/27008)\n",
      "Epoch: 144 | Batch_idx: 220 |  Loss_1: (0.0008) | Acc_1: (99.98%) (28283/28288)\n",
      "Epoch: 144 | Batch_idx: 230 |  Loss_1: (0.0008) | Acc_1: (99.98%) (29563/29568)\n",
      "Epoch: 144 | Batch_idx: 240 |  Loss_1: (0.0007) | Acc_1: (99.98%) (30843/30848)\n",
      "Epoch: 144 | Batch_idx: 250 |  Loss_1: (0.0007) | Acc_1: (99.98%) (32123/32128)\n",
      "Epoch: 144 | Batch_idx: 260 |  Loss_1: (0.0007) | Acc_1: (99.99%) (33403/33408)\n",
      "Epoch: 144 | Batch_idx: 270 |  Loss_1: (0.0007) | Acc_1: (99.99%) (34683/34688)\n",
      "Epoch: 144 | Batch_idx: 280 |  Loss_1: (0.0008) | Acc_1: (99.98%) (35962/35968)\n",
      "Epoch: 144 | Batch_idx: 290 |  Loss_1: (0.0007) | Acc_1: (99.98%) (37242/37248)\n",
      "Epoch: 144 | Batch_idx: 300 |  Loss_1: (0.0007) | Acc_1: (99.98%) (38522/38528)\n",
      "Epoch: 144 | Batch_idx: 310 |  Loss_1: (0.0007) | Acc_1: (99.98%) (39802/39808)\n",
      "Epoch: 144 | Batch_idx: 320 |  Loss_1: (0.0007) | Acc_1: (99.99%) (41082/41088)\n",
      "Epoch: 144 | Batch_idx: 330 |  Loss_1: (0.0007) | Acc_1: (99.99%) (42362/42368)\n",
      "Epoch: 144 | Batch_idx: 340 |  Loss_1: (0.0007) | Acc_1: (99.99%) (43642/43648)\n",
      "Epoch: 144 | Batch_idx: 350 |  Loss_1: (0.0007) | Acc_1: (99.99%) (44922/44928)\n",
      "Epoch: 144 | Batch_idx: 360 |  Loss_1: (0.0007) | Acc_1: (99.99%) (46202/46208)\n",
      "Epoch: 144 | Batch_idx: 370 |  Loss_1: (0.0007) | Acc_1: (99.99%) (47481/47488)\n",
      "Epoch: 144 | Batch_idx: 380 |  Loss_1: (0.0007) | Acc_1: (99.99%) (48761/48768)\n",
      "Epoch: 144 | Batch_idx: 390 |  Loss_1: (0.0007) | Acc_1: (99.99%) (49993/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4701) | Acc: (93.18%) (9318/10000)\n",
      "Epoch: 145 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 145 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 145 | Batch_idx: 20 |  Loss_1: (0.0021) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 145 | Batch_idx: 30 |  Loss_1: (0.0015) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 145 | Batch_idx: 40 |  Loss_1: (0.0014) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 145 | Batch_idx: 50 |  Loss_1: (0.0012) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 145 | Batch_idx: 60 |  Loss_1: (0.0010) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 145 | Batch_idx: 70 |  Loss_1: (0.0011) | Acc_1: (99.96%) (9084/9088)\n",
      "Epoch: 145 | Batch_idx: 80 |  Loss_1: (0.0010) | Acc_1: (99.96%) (10364/10368)\n",
      "Epoch: 145 | Batch_idx: 90 |  Loss_1: (0.0012) | Acc_1: (99.96%) (11643/11648)\n",
      "Epoch: 145 | Batch_idx: 100 |  Loss_1: (0.0011) | Acc_1: (99.96%) (12923/12928)\n",
      "Epoch: 145 | Batch_idx: 110 |  Loss_1: (0.0011) | Acc_1: (99.96%) (14203/14208)\n",
      "Epoch: 145 | Batch_idx: 120 |  Loss_1: (0.0010) | Acc_1: (99.97%) (15483/15488)\n",
      "Epoch: 145 | Batch_idx: 130 |  Loss_1: (0.0010) | Acc_1: (99.97%) (16763/16768)\n",
      "Epoch: 145 | Batch_idx: 140 |  Loss_1: (0.0009) | Acc_1: (99.97%) (18043/18048)\n",
      "Epoch: 145 | Batch_idx: 150 |  Loss_1: (0.0009) | Acc_1: (99.97%) (19323/19328)\n",
      "Epoch: 145 | Batch_idx: 160 |  Loss_1: (0.0009) | Acc_1: (99.98%) (20603/20608)\n",
      "Epoch: 145 | Batch_idx: 170 |  Loss_1: (0.0009) | Acc_1: (99.98%) (21883/21888)\n",
      "Epoch: 145 | Batch_idx: 180 |  Loss_1: (0.0008) | Acc_1: (99.98%) (23163/23168)\n",
      "Epoch: 145 | Batch_idx: 190 |  Loss_1: (0.0008) | Acc_1: (99.98%) (24443/24448)\n",
      "Epoch: 145 | Batch_idx: 200 |  Loss_1: (0.0008) | Acc_1: (99.98%) (25722/25728)\n",
      "Epoch: 145 | Batch_idx: 210 |  Loss_1: (0.0008) | Acc_1: (99.98%) (27002/27008)\n",
      "Epoch: 145 | Batch_idx: 220 |  Loss_1: (0.0010) | Acc_1: (99.98%) (28281/28288)\n",
      "Epoch: 145 | Batch_idx: 230 |  Loss_1: (0.0009) | Acc_1: (99.98%) (29561/29568)\n",
      "Epoch: 145 | Batch_idx: 240 |  Loss_1: (0.0009) | Acc_1: (99.98%) (30841/30848)\n",
      "Epoch: 145 | Batch_idx: 250 |  Loss_1: (0.0009) | Acc_1: (99.98%) (32121/32128)\n",
      "Epoch: 145 | Batch_idx: 260 |  Loss_1: (0.0009) | Acc_1: (99.98%) (33400/33408)\n",
      "Epoch: 145 | Batch_idx: 270 |  Loss_1: (0.0009) | Acc_1: (99.98%) (34680/34688)\n",
      "Epoch: 145 | Batch_idx: 280 |  Loss_1: (0.0009) | Acc_1: (99.98%) (35960/35968)\n",
      "Epoch: 145 | Batch_idx: 290 |  Loss_1: (0.0009) | Acc_1: (99.98%) (37239/37248)\n",
      "Epoch: 145 | Batch_idx: 300 |  Loss_1: (0.0009) | Acc_1: (99.97%) (38518/38528)\n",
      "Epoch: 145 | Batch_idx: 310 |  Loss_1: (0.0009) | Acc_1: (99.97%) (39798/39808)\n",
      "Epoch: 145 | Batch_idx: 320 |  Loss_1: (0.0009) | Acc_1: (99.97%) (41077/41088)\n",
      "Epoch: 145 | Batch_idx: 330 |  Loss_1: (0.0009) | Acc_1: (99.97%) (42356/42368)\n",
      "Epoch: 145 | Batch_idx: 340 |  Loss_1: (0.0009) | Acc_1: (99.97%) (43636/43648)\n",
      "Epoch: 145 | Batch_idx: 350 |  Loss_1: (0.0009) | Acc_1: (99.97%) (44916/44928)\n",
      "Epoch: 145 | Batch_idx: 360 |  Loss_1: (0.0009) | Acc_1: (99.97%) (46196/46208)\n",
      "Epoch: 145 | Batch_idx: 370 |  Loss_1: (0.0009) | Acc_1: (99.97%) (47476/47488)\n",
      "Epoch: 145 | Batch_idx: 380 |  Loss_1: (0.0009) | Acc_1: (99.98%) (48756/48768)\n",
      "Epoch: 145 | Batch_idx: 390 |  Loss_1: (0.0009) | Acc_1: (99.97%) (49987/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4735) | Acc: (93.13%) (9313/10000)\n",
      "Epoch: 146 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 146 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 146 | Batch_idx: 20 |  Loss_1: (0.0007) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 146 | Batch_idx: 30 |  Loss_1: (0.0006) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 146 | Batch_idx: 40 |  Loss_1: (0.0006) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 146 | Batch_idx: 50 |  Loss_1: (0.0006) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 146 | Batch_idx: 60 |  Loss_1: (0.0006) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 146 | Batch_idx: 70 |  Loss_1: (0.0006) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 146 | Batch_idx: 80 |  Loss_1: (0.0006) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 146 | Batch_idx: 90 |  Loss_1: (0.0006) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 146 | Batch_idx: 100 |  Loss_1: (0.0006) | Acc_1: (99.98%) (12925/12928)\n",
      "Epoch: 146 | Batch_idx: 110 |  Loss_1: (0.0008) | Acc_1: (99.97%) (14204/14208)\n",
      "Epoch: 146 | Batch_idx: 120 |  Loss_1: (0.0008) | Acc_1: (99.97%) (15484/15488)\n",
      "Epoch: 146 | Batch_idx: 130 |  Loss_1: (0.0008) | Acc_1: (99.98%) (16764/16768)\n",
      "Epoch: 146 | Batch_idx: 140 |  Loss_1: (0.0008) | Acc_1: (99.97%) (18043/18048)\n",
      "Epoch: 146 | Batch_idx: 150 |  Loss_1: (0.0008) | Acc_1: (99.97%) (19323/19328)\n",
      "Epoch: 146 | Batch_idx: 160 |  Loss_1: (0.0008) | Acc_1: (99.98%) (20603/20608)\n",
      "Epoch: 146 | Batch_idx: 170 |  Loss_1: (0.0008) | Acc_1: (99.97%) (21882/21888)\n",
      "Epoch: 146 | Batch_idx: 180 |  Loss_1: (0.0008) | Acc_1: (99.97%) (23161/23168)\n",
      "Epoch: 146 | Batch_idx: 190 |  Loss_1: (0.0010) | Acc_1: (99.96%) (24438/24448)\n",
      "Epoch: 146 | Batch_idx: 200 |  Loss_1: (0.0011) | Acc_1: (99.96%) (25717/25728)\n",
      "Epoch: 146 | Batch_idx: 210 |  Loss_1: (0.0011) | Acc_1: (99.96%) (26997/27008)\n",
      "Epoch: 146 | Batch_idx: 220 |  Loss_1: (0.0011) | Acc_1: (99.96%) (28277/28288)\n",
      "Epoch: 146 | Batch_idx: 230 |  Loss_1: (0.0011) | Acc_1: (99.96%) (29556/29568)\n",
      "Epoch: 146 | Batch_idx: 240 |  Loss_1: (0.0012) | Acc_1: (99.95%) (30834/30848)\n",
      "Epoch: 146 | Batch_idx: 250 |  Loss_1: (0.0012) | Acc_1: (99.95%) (32113/32128)\n",
      "Epoch: 146 | Batch_idx: 260 |  Loss_1: (0.0012) | Acc_1: (99.96%) (33393/33408)\n",
      "Epoch: 146 | Batch_idx: 270 |  Loss_1: (0.0011) | Acc_1: (99.96%) (34673/34688)\n",
      "Epoch: 146 | Batch_idx: 280 |  Loss_1: (0.0011) | Acc_1: (99.96%) (35953/35968)\n",
      "Epoch: 146 | Batch_idx: 290 |  Loss_1: (0.0011) | Acc_1: (99.96%) (37232/37248)\n",
      "Epoch: 146 | Batch_idx: 300 |  Loss_1: (0.0011) | Acc_1: (99.96%) (38512/38528)\n",
      "Epoch: 146 | Batch_idx: 310 |  Loss_1: (0.0011) | Acc_1: (99.96%) (39791/39808)\n",
      "Epoch: 146 | Batch_idx: 320 |  Loss_1: (0.0011) | Acc_1: (99.96%) (41071/41088)\n",
      "Epoch: 146 | Batch_idx: 330 |  Loss_1: (0.0011) | Acc_1: (99.96%) (42351/42368)\n",
      "Epoch: 146 | Batch_idx: 340 |  Loss_1: (0.0011) | Acc_1: (99.96%) (43630/43648)\n",
      "Epoch: 146 | Batch_idx: 350 |  Loss_1: (0.0011) | Acc_1: (99.96%) (44910/44928)\n",
      "Epoch: 146 | Batch_idx: 360 |  Loss_1: (0.0011) | Acc_1: (99.96%) (46190/46208)\n",
      "Epoch: 146 | Batch_idx: 370 |  Loss_1: (0.0011) | Acc_1: (99.96%) (47470/47488)\n",
      "Epoch: 146 | Batch_idx: 380 |  Loss_1: (0.0010) | Acc_1: (99.96%) (48750/48768)\n",
      "Epoch: 146 | Batch_idx: 390 |  Loss_1: (0.0010) | Acc_1: (99.96%) (49981/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4673) | Acc: (93.14%) (9314/10000)\n",
      "Epoch: 147 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 147 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 147 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 147 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 147 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 147 | Batch_idx: 50 |  Loss_1: (0.0004) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 147 | Batch_idx: 60 |  Loss_1: (0.0004) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 147 | Batch_idx: 70 |  Loss_1: (0.0004) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 147 | Batch_idx: 80 |  Loss_1: (0.0004) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 147 | Batch_idx: 90 |  Loss_1: (0.0004) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 147 | Batch_idx: 100 |  Loss_1: (0.0005) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 147 | Batch_idx: 110 |  Loss_1: (0.0005) | Acc_1: (99.99%) (14206/14208)\n",
      "Epoch: 147 | Batch_idx: 120 |  Loss_1: (0.0005) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 147 | Batch_idx: 130 |  Loss_1: (0.0004) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 147 | Batch_idx: 140 |  Loss_1: (0.0004) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 147 | Batch_idx: 150 |  Loss_1: (0.0004) | Acc_1: (99.99%) (19326/19328)\n",
      "Epoch: 147 | Batch_idx: 160 |  Loss_1: (0.0004) | Acc_1: (99.99%) (20606/20608)\n",
      "Epoch: 147 | Batch_idx: 170 |  Loss_1: (0.0004) | Acc_1: (99.99%) (21886/21888)\n",
      "Epoch: 147 | Batch_idx: 180 |  Loss_1: (0.0004) | Acc_1: (99.99%) (23166/23168)\n",
      "Epoch: 147 | Batch_idx: 190 |  Loss_1: (0.0004) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 147 | Batch_idx: 200 |  Loss_1: (0.0004) | Acc_1: (99.99%) (25726/25728)\n",
      "Epoch: 147 | Batch_idx: 210 |  Loss_1: (0.0005) | Acc_1: (99.99%) (27006/27008)\n",
      "Epoch: 147 | Batch_idx: 220 |  Loss_1: (0.0005) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 147 | Batch_idx: 230 |  Loss_1: (0.0005) | Acc_1: (99.99%) (29565/29568)\n",
      "Epoch: 147 | Batch_idx: 240 |  Loss_1: (0.0005) | Acc_1: (99.99%) (30845/30848)\n",
      "Epoch: 147 | Batch_idx: 250 |  Loss_1: (0.0005) | Acc_1: (99.99%) (32125/32128)\n",
      "Epoch: 147 | Batch_idx: 260 |  Loss_1: (0.0005) | Acc_1: (99.99%) (33405/33408)\n",
      "Epoch: 147 | Batch_idx: 270 |  Loss_1: (0.0005) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 147 | Batch_idx: 280 |  Loss_1: (0.0005) | Acc_1: (99.99%) (35965/35968)\n",
      "Epoch: 147 | Batch_idx: 290 |  Loss_1: (0.0005) | Acc_1: (99.99%) (37245/37248)\n",
      "Epoch: 147 | Batch_idx: 300 |  Loss_1: (0.0005) | Acc_1: (99.99%) (38525/38528)\n",
      "Epoch: 147 | Batch_idx: 310 |  Loss_1: (0.0005) | Acc_1: (99.99%) (39805/39808)\n",
      "Epoch: 147 | Batch_idx: 320 |  Loss_1: (0.0005) | Acc_1: (99.99%) (41085/41088)\n",
      "Epoch: 147 | Batch_idx: 330 |  Loss_1: (0.0005) | Acc_1: (99.99%) (42364/42368)\n",
      "Epoch: 147 | Batch_idx: 340 |  Loss_1: (0.0005) | Acc_1: (99.99%) (43644/43648)\n",
      "Epoch: 147 | Batch_idx: 350 |  Loss_1: (0.0005) | Acc_1: (99.99%) (44924/44928)\n",
      "Epoch: 147 | Batch_idx: 360 |  Loss_1: (0.0005) | Acc_1: (99.99%) (46204/46208)\n",
      "Epoch: 147 | Batch_idx: 370 |  Loss_1: (0.0005) | Acc_1: (99.99%) (47484/47488)\n",
      "Epoch: 147 | Batch_idx: 380 |  Loss_1: (0.0005) | Acc_1: (99.99%) (48764/48768)\n",
      "Epoch: 147 | Batch_idx: 390 |  Loss_1: (0.0005) | Acc_1: (99.99%) (49996/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4672) | Acc: (93.28%) (9328/10000)\n",
      "Epoch: 148 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 148 | Batch_idx: 10 |  Loss_1: (0.0004) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 148 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 148 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 148 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 148 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 148 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 148 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 148 | Batch_idx: 80 |  Loss_1: (0.0003) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 148 | Batch_idx: 90 |  Loss_1: (0.0003) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 148 | Batch_idx: 100 |  Loss_1: (0.0003) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 148 | Batch_idx: 110 |  Loss_1: (0.0003) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 148 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 148 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 148 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 148 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 148 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 148 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 148 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 148 | Batch_idx: 190 |  Loss_1: (0.0004) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 148 | Batch_idx: 200 |  Loss_1: (0.0004) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 148 | Batch_idx: 210 |  Loss_1: (0.0004) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 148 | Batch_idx: 220 |  Loss_1: (0.0004) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 148 | Batch_idx: 230 |  Loss_1: (0.0004) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 148 | Batch_idx: 240 |  Loss_1: (0.0004) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 148 | Batch_idx: 250 |  Loss_1: (0.0004) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 148 | Batch_idx: 260 |  Loss_1: (0.0004) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 148 | Batch_idx: 270 |  Loss_1: (0.0004) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 148 | Batch_idx: 280 |  Loss_1: (0.0004) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 148 | Batch_idx: 290 |  Loss_1: (0.0004) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 148 | Batch_idx: 300 |  Loss_1: (0.0004) | Acc_1: (99.99%) (38525/38528)\n",
      "Epoch: 148 | Batch_idx: 310 |  Loss_1: (0.0004) | Acc_1: (99.99%) (39805/39808)\n",
      "Epoch: 148 | Batch_idx: 320 |  Loss_1: (0.0004) | Acc_1: (99.99%) (41085/41088)\n",
      "Epoch: 148 | Batch_idx: 330 |  Loss_1: (0.0004) | Acc_1: (99.99%) (42365/42368)\n",
      "Epoch: 148 | Batch_idx: 340 |  Loss_1: (0.0005) | Acc_1: (99.99%) (43643/43648)\n",
      "Epoch: 148 | Batch_idx: 350 |  Loss_1: (0.0005) | Acc_1: (99.99%) (44923/44928)\n",
      "Epoch: 148 | Batch_idx: 360 |  Loss_1: (0.0005) | Acc_1: (99.99%) (46203/46208)\n",
      "Epoch: 148 | Batch_idx: 370 |  Loss_1: (0.0005) | Acc_1: (99.99%) (47483/47488)\n",
      "Epoch: 148 | Batch_idx: 380 |  Loss_1: (0.0005) | Acc_1: (99.99%) (48763/48768)\n",
      "Epoch: 148 | Batch_idx: 390 |  Loss_1: (0.0005) | Acc_1: (99.99%) (49995/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4576) | Acc: (93.28%) (9328/10000)\n",
      "Epoch: 149 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 149 | Batch_idx: 10 |  Loss_1: (0.0013) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 149 | Batch_idx: 20 |  Loss_1: (0.0007) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 149 | Batch_idx: 30 |  Loss_1: (0.0010) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 149 | Batch_idx: 40 |  Loss_1: (0.0009) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 149 | Batch_idx: 50 |  Loss_1: (0.0009) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 149 | Batch_idx: 60 |  Loss_1: (0.0008) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 149 | Batch_idx: 70 |  Loss_1: (0.0008) | Acc_1: (99.97%) (9085/9088)\n",
      "Epoch: 149 | Batch_idx: 80 |  Loss_1: (0.0008) | Acc_1: (99.97%) (10365/10368)\n",
      "Epoch: 149 | Batch_idx: 90 |  Loss_1: (0.0007) | Acc_1: (99.97%) (11645/11648)\n",
      "Epoch: 149 | Batch_idx: 100 |  Loss_1: (0.0007) | Acc_1: (99.98%) (12925/12928)\n",
      "Epoch: 149 | Batch_idx: 110 |  Loss_1: (0.0006) | Acc_1: (99.98%) (14205/14208)\n",
      "Epoch: 149 | Batch_idx: 120 |  Loss_1: (0.0006) | Acc_1: (99.98%) (15485/15488)\n",
      "Epoch: 149 | Batch_idx: 130 |  Loss_1: (0.0006) | Acc_1: (99.98%) (16765/16768)\n",
      "Epoch: 149 | Batch_idx: 140 |  Loss_1: (0.0006) | Acc_1: (99.98%) (18045/18048)\n",
      "Epoch: 149 | Batch_idx: 150 |  Loss_1: (0.0006) | Acc_1: (99.98%) (19325/19328)\n",
      "Epoch: 149 | Batch_idx: 160 |  Loss_1: (0.0006) | Acc_1: (99.99%) (20605/20608)\n",
      "Epoch: 149 | Batch_idx: 170 |  Loss_1: (0.0006) | Acc_1: (99.99%) (21885/21888)\n",
      "Epoch: 149 | Batch_idx: 180 |  Loss_1: (0.0006) | Acc_1: (99.99%) (23165/23168)\n",
      "Epoch: 149 | Batch_idx: 190 |  Loss_1: (0.0006) | Acc_1: (99.98%) (24444/24448)\n",
      "Epoch: 149 | Batch_idx: 200 |  Loss_1: (0.0006) | Acc_1: (99.98%) (25724/25728)\n",
      "Epoch: 149 | Batch_idx: 210 |  Loss_1: (0.0006) | Acc_1: (99.98%) (27003/27008)\n",
      "Epoch: 149 | Batch_idx: 220 |  Loss_1: (0.0006) | Acc_1: (99.98%) (28283/28288)\n",
      "Epoch: 149 | Batch_idx: 230 |  Loss_1: (0.0006) | Acc_1: (99.98%) (29563/29568)\n",
      "Epoch: 149 | Batch_idx: 240 |  Loss_1: (0.0006) | Acc_1: (99.98%) (30843/30848)\n",
      "Epoch: 149 | Batch_idx: 250 |  Loss_1: (0.0006) | Acc_1: (99.98%) (32123/32128)\n",
      "Epoch: 149 | Batch_idx: 260 |  Loss_1: (0.0006) | Acc_1: (99.99%) (33403/33408)\n",
      "Epoch: 149 | Batch_idx: 270 |  Loss_1: (0.0006) | Acc_1: (99.99%) (34683/34688)\n",
      "Epoch: 149 | Batch_idx: 280 |  Loss_1: (0.0006) | Acc_1: (99.99%) (35963/35968)\n",
      "Epoch: 149 | Batch_idx: 290 |  Loss_1: (0.0006) | Acc_1: (99.99%) (37243/37248)\n",
      "Epoch: 149 | Batch_idx: 300 |  Loss_1: (0.0006) | Acc_1: (99.98%) (38521/38528)\n",
      "Epoch: 149 | Batch_idx: 310 |  Loss_1: (0.0006) | Acc_1: (99.98%) (39801/39808)\n",
      "Epoch: 149 | Batch_idx: 320 |  Loss_1: (0.0006) | Acc_1: (99.98%) (41081/41088)\n",
      "Epoch: 149 | Batch_idx: 330 |  Loss_1: (0.0007) | Acc_1: (99.98%) (42359/42368)\n",
      "Epoch: 149 | Batch_idx: 340 |  Loss_1: (0.0007) | Acc_1: (99.98%) (43639/43648)\n",
      "Epoch: 149 | Batch_idx: 350 |  Loss_1: (0.0007) | Acc_1: (99.98%) (44919/44928)\n",
      "Epoch: 149 | Batch_idx: 360 |  Loss_1: (0.0007) | Acc_1: (99.98%) (46199/46208)\n",
      "Epoch: 149 | Batch_idx: 370 |  Loss_1: (0.0007) | Acc_1: (99.98%) (47479/47488)\n",
      "Epoch: 149 | Batch_idx: 380 |  Loss_1: (0.0006) | Acc_1: (99.98%) (48759/48768)\n",
      "Epoch: 149 | Batch_idx: 390 |  Loss_1: (0.0006) | Acc_1: (99.98%) (49991/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4708) | Acc: (93.07%) (9307/10000)\n",
      "Epoch: 150 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 150 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 150 | Batch_idx: 20 |  Loss_1: (0.0002) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 150 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 150 | Batch_idx: 40 |  Loss_1: (0.0004) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 150 | Batch_idx: 50 |  Loss_1: (0.0004) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 150 | Batch_idx: 60 |  Loss_1: (0.0004) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 150 | Batch_idx: 70 |  Loss_1: (0.0004) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 150 | Batch_idx: 80 |  Loss_1: (0.0004) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 150 | Batch_idx: 90 |  Loss_1: (0.0004) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 150 | Batch_idx: 100 |  Loss_1: (0.0004) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 150 | Batch_idx: 110 |  Loss_1: (0.0004) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 150 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 150 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 150 | Batch_idx: 140 |  Loss_1: (0.0003) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 150 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 150 | Batch_idx: 160 |  Loss_1: (0.0003) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 150 | Batch_idx: 170 |  Loss_1: (0.0004) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 150 | Batch_idx: 180 |  Loss_1: (0.0004) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 150 | Batch_idx: 190 |  Loss_1: (0.0004) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 150 | Batch_idx: 200 |  Loss_1: (0.0004) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 150 | Batch_idx: 210 |  Loss_1: (0.0004) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 150 | Batch_idx: 220 |  Loss_1: (0.0004) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 150 | Batch_idx: 230 |  Loss_1: (0.0004) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 150 | Batch_idx: 240 |  Loss_1: (0.0004) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 150 | Batch_idx: 250 |  Loss_1: (0.0004) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 150 | Batch_idx: 260 |  Loss_1: (0.0004) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 150 | Batch_idx: 270 |  Loss_1: (0.0004) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 150 | Batch_idx: 280 |  Loss_1: (0.0004) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 150 | Batch_idx: 290 |  Loss_1: (0.0004) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 150 | Batch_idx: 300 |  Loss_1: (0.0005) | Acc_1: (99.99%) (38524/38528)\n",
      "Epoch: 150 | Batch_idx: 310 |  Loss_1: (0.0005) | Acc_1: (99.99%) (39804/39808)\n",
      "Epoch: 150 | Batch_idx: 320 |  Loss_1: (0.0005) | Acc_1: (99.99%) (41084/41088)\n",
      "Epoch: 150 | Batch_idx: 330 |  Loss_1: (0.0005) | Acc_1: (99.99%) (42364/42368)\n",
      "Epoch: 150 | Batch_idx: 340 |  Loss_1: (0.0005) | Acc_1: (99.99%) (43644/43648)\n",
      "Epoch: 150 | Batch_idx: 350 |  Loss_1: (0.0005) | Acc_1: (99.99%) (44923/44928)\n",
      "Epoch: 150 | Batch_idx: 360 |  Loss_1: (0.0005) | Acc_1: (99.99%) (46203/46208)\n",
      "Epoch: 150 | Batch_idx: 370 |  Loss_1: (0.0005) | Acc_1: (99.99%) (47483/47488)\n",
      "Epoch: 150 | Batch_idx: 380 |  Loss_1: (0.0005) | Acc_1: (99.99%) (48763/48768)\n",
      "Epoch: 150 | Batch_idx: 390 |  Loss_1: (0.0005) | Acc_1: (99.99%) (49995/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4618) | Acc: (93.16%) (9316/10000)\n",
      "Epoch: 151 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 151 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 151 | Batch_idx: 20 |  Loss_1: (0.0002) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 151 | Batch_idx: 30 |  Loss_1: (0.0005) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 151 | Batch_idx: 40 |  Loss_1: (0.0004) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 151 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 151 | Batch_idx: 60 |  Loss_1: (0.0004) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 151 | Batch_idx: 70 |  Loss_1: (0.0006) | Acc_1: (99.97%) (9085/9088)\n",
      "Epoch: 151 | Batch_idx: 80 |  Loss_1: (0.0005) | Acc_1: (99.97%) (10365/10368)\n",
      "Epoch: 151 | Batch_idx: 90 |  Loss_1: (0.0005) | Acc_1: (99.97%) (11645/11648)\n",
      "Epoch: 151 | Batch_idx: 100 |  Loss_1: (0.0005) | Acc_1: (99.98%) (12925/12928)\n",
      "Epoch: 151 | Batch_idx: 110 |  Loss_1: (0.0005) | Acc_1: (99.98%) (14205/14208)\n",
      "Epoch: 151 | Batch_idx: 120 |  Loss_1: (0.0005) | Acc_1: (99.97%) (15484/15488)\n",
      "Epoch: 151 | Batch_idx: 130 |  Loss_1: (0.0005) | Acc_1: (99.98%) (16764/16768)\n",
      "Epoch: 151 | Batch_idx: 140 |  Loss_1: (0.0005) | Acc_1: (99.98%) (18044/18048)\n",
      "Epoch: 151 | Batch_idx: 150 |  Loss_1: (0.0005) | Acc_1: (99.97%) (19323/19328)\n",
      "Epoch: 151 | Batch_idx: 160 |  Loss_1: (0.0005) | Acc_1: (99.98%) (20603/20608)\n",
      "Epoch: 151 | Batch_idx: 170 |  Loss_1: (0.0006) | Acc_1: (99.98%) (21883/21888)\n",
      "Epoch: 151 | Batch_idx: 180 |  Loss_1: (0.0006) | Acc_1: (99.98%) (23163/23168)\n",
      "Epoch: 151 | Batch_idx: 190 |  Loss_1: (0.0005) | Acc_1: (99.98%) (24443/24448)\n",
      "Epoch: 151 | Batch_idx: 200 |  Loss_1: (0.0005) | Acc_1: (99.98%) (25723/25728)\n",
      "Epoch: 151 | Batch_idx: 210 |  Loss_1: (0.0005) | Acc_1: (99.98%) (27003/27008)\n",
      "Epoch: 151 | Batch_idx: 220 |  Loss_1: (0.0005) | Acc_1: (99.98%) (28282/28288)\n",
      "Epoch: 151 | Batch_idx: 230 |  Loss_1: (0.0005) | Acc_1: (99.98%) (29562/29568)\n",
      "Epoch: 151 | Batch_idx: 240 |  Loss_1: (0.0005) | Acc_1: (99.98%) (30842/30848)\n",
      "Epoch: 151 | Batch_idx: 250 |  Loss_1: (0.0005) | Acc_1: (99.98%) (32122/32128)\n",
      "Epoch: 151 | Batch_idx: 260 |  Loss_1: (0.0005) | Acc_1: (99.98%) (33402/33408)\n",
      "Epoch: 151 | Batch_idx: 270 |  Loss_1: (0.0005) | Acc_1: (99.98%) (34682/34688)\n",
      "Epoch: 151 | Batch_idx: 280 |  Loss_1: (0.0005) | Acc_1: (99.98%) (35962/35968)\n",
      "Epoch: 151 | Batch_idx: 290 |  Loss_1: (0.0005) | Acc_1: (99.98%) (37242/37248)\n",
      "Epoch: 151 | Batch_idx: 300 |  Loss_1: (0.0005) | Acc_1: (99.98%) (38522/38528)\n",
      "Epoch: 151 | Batch_idx: 310 |  Loss_1: (0.0005) | Acc_1: (99.98%) (39802/39808)\n",
      "Epoch: 151 | Batch_idx: 320 |  Loss_1: (0.0005) | Acc_1: (99.99%) (41082/41088)\n",
      "Epoch: 151 | Batch_idx: 330 |  Loss_1: (0.0005) | Acc_1: (99.99%) (42362/42368)\n",
      "Epoch: 151 | Batch_idx: 340 |  Loss_1: (0.0005) | Acc_1: (99.98%) (43640/43648)\n",
      "Epoch: 151 | Batch_idx: 350 |  Loss_1: (0.0005) | Acc_1: (99.98%) (44920/44928)\n",
      "Epoch: 151 | Batch_idx: 360 |  Loss_1: (0.0005) | Acc_1: (99.98%) (46200/46208)\n",
      "Epoch: 151 | Batch_idx: 370 |  Loss_1: (0.0005) | Acc_1: (99.98%) (47480/47488)\n",
      "Epoch: 151 | Batch_idx: 380 |  Loss_1: (0.0005) | Acc_1: (99.98%) (48760/48768)\n",
      "Epoch: 151 | Batch_idx: 390 |  Loss_1: (0.0005) | Acc_1: (99.98%) (49992/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4618) | Acc: (93.39%) (9339/10000)\n",
      "Epoch: 152 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 152 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 152 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 152 | Batch_idx: 30 |  Loss_1: (0.0002) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 152 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 152 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 152 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 152 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 152 | Batch_idx: 80 |  Loss_1: (0.0003) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 152 | Batch_idx: 90 |  Loss_1: (0.0004) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 152 | Batch_idx: 100 |  Loss_1: (0.0005) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 152 | Batch_idx: 110 |  Loss_1: (0.0005) | Acc_1: (99.99%) (14206/14208)\n",
      "Epoch: 152 | Batch_idx: 120 |  Loss_1: (0.0005) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 152 | Batch_idx: 130 |  Loss_1: (0.0005) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 152 | Batch_idx: 140 |  Loss_1: (0.0004) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 152 | Batch_idx: 150 |  Loss_1: (0.0004) | Acc_1: (99.99%) (19326/19328)\n",
      "Epoch: 152 | Batch_idx: 160 |  Loss_1: (0.0004) | Acc_1: (99.99%) (20606/20608)\n",
      "Epoch: 152 | Batch_idx: 170 |  Loss_1: (0.0004) | Acc_1: (99.99%) (21886/21888)\n",
      "Epoch: 152 | Batch_idx: 180 |  Loss_1: (0.0004) | Acc_1: (99.99%) (23166/23168)\n",
      "Epoch: 152 | Batch_idx: 190 |  Loss_1: (0.0004) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 152 | Batch_idx: 200 |  Loss_1: (0.0004) | Acc_1: (99.99%) (25726/25728)\n",
      "Epoch: 152 | Batch_idx: 210 |  Loss_1: (0.0004) | Acc_1: (99.99%) (27006/27008)\n",
      "Epoch: 152 | Batch_idx: 220 |  Loss_1: (0.0004) | Acc_1: (99.99%) (28286/28288)\n",
      "Epoch: 152 | Batch_idx: 230 |  Loss_1: (0.0004) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 152 | Batch_idx: 240 |  Loss_1: (0.0004) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 152 | Batch_idx: 250 |  Loss_1: (0.0004) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 152 | Batch_idx: 260 |  Loss_1: (0.0004) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 152 | Batch_idx: 270 |  Loss_1: (0.0004) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 152 | Batch_idx: 280 |  Loss_1: (0.0004) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 152 | Batch_idx: 290 |  Loss_1: (0.0004) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 152 | Batch_idx: 300 |  Loss_1: (0.0004) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 152 | Batch_idx: 310 |  Loss_1: (0.0004) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 152 | Batch_idx: 320 |  Loss_1: (0.0004) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 152 | Batch_idx: 330 |  Loss_1: (0.0004) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 152 | Batch_idx: 340 |  Loss_1: (0.0004) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 152 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 152 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 152 | Batch_idx: 370 |  Loss_1: (0.0004) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 152 | Batch_idx: 380 |  Loss_1: (0.0004) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 152 | Batch_idx: 390 |  Loss_1: (0.0004) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4573) | Acc: (93.19%) (9319/10000)\n",
      "Epoch: 153 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 153 | Batch_idx: 10 |  Loss_1: (0.0006) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 153 | Batch_idx: 20 |  Loss_1: (0.0006) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 153 | Batch_idx: 30 |  Loss_1: (0.0005) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 153 | Batch_idx: 40 |  Loss_1: (0.0010) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 153 | Batch_idx: 50 |  Loss_1: (0.0008) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 153 | Batch_idx: 60 |  Loss_1: (0.0007) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 153 | Batch_idx: 70 |  Loss_1: (0.0009) | Acc_1: (99.97%) (9085/9088)\n",
      "Epoch: 153 | Batch_idx: 80 |  Loss_1: (0.0009) | Acc_1: (99.97%) (10365/10368)\n",
      "Epoch: 153 | Batch_idx: 90 |  Loss_1: (0.0008) | Acc_1: (99.97%) (11645/11648)\n",
      "Epoch: 153 | Batch_idx: 100 |  Loss_1: (0.0008) | Acc_1: (99.98%) (12925/12928)\n",
      "Epoch: 153 | Batch_idx: 110 |  Loss_1: (0.0007) | Acc_1: (99.98%) (14205/14208)\n",
      "Epoch: 153 | Batch_idx: 120 |  Loss_1: (0.0007) | Acc_1: (99.98%) (15485/15488)\n",
      "Epoch: 153 | Batch_idx: 130 |  Loss_1: (0.0007) | Acc_1: (99.98%) (16765/16768)\n",
      "Epoch: 153 | Batch_idx: 140 |  Loss_1: (0.0006) | Acc_1: (99.98%) (18045/18048)\n",
      "Epoch: 153 | Batch_idx: 150 |  Loss_1: (0.0006) | Acc_1: (99.98%) (19325/19328)\n",
      "Epoch: 153 | Batch_idx: 160 |  Loss_1: (0.0006) | Acc_1: (99.99%) (20605/20608)\n",
      "Epoch: 153 | Batch_idx: 170 |  Loss_1: (0.0005) | Acc_1: (99.99%) (21885/21888)\n",
      "Epoch: 153 | Batch_idx: 180 |  Loss_1: (0.0005) | Acc_1: (99.99%) (23165/23168)\n",
      "Epoch: 153 | Batch_idx: 190 |  Loss_1: (0.0005) | Acc_1: (99.99%) (24445/24448)\n",
      "Epoch: 153 | Batch_idx: 200 |  Loss_1: (0.0005) | Acc_1: (99.99%) (25725/25728)\n",
      "Epoch: 153 | Batch_idx: 210 |  Loss_1: (0.0005) | Acc_1: (99.99%) (27005/27008)\n",
      "Epoch: 153 | Batch_idx: 220 |  Loss_1: (0.0005) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 153 | Batch_idx: 230 |  Loss_1: (0.0005) | Acc_1: (99.99%) (29565/29568)\n",
      "Epoch: 153 | Batch_idx: 240 |  Loss_1: (0.0005) | Acc_1: (99.99%) (30845/30848)\n",
      "Epoch: 153 | Batch_idx: 250 |  Loss_1: (0.0005) | Acc_1: (99.99%) (32125/32128)\n",
      "Epoch: 153 | Batch_idx: 260 |  Loss_1: (0.0004) | Acc_1: (99.99%) (33405/33408)\n",
      "Epoch: 153 | Batch_idx: 270 |  Loss_1: (0.0004) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 153 | Batch_idx: 280 |  Loss_1: (0.0004) | Acc_1: (99.99%) (35965/35968)\n",
      "Epoch: 153 | Batch_idx: 290 |  Loss_1: (0.0004) | Acc_1: (99.99%) (37245/37248)\n",
      "Epoch: 153 | Batch_idx: 300 |  Loss_1: (0.0004) | Acc_1: (99.99%) (38525/38528)\n",
      "Epoch: 153 | Batch_idx: 310 |  Loss_1: (0.0004) | Acc_1: (99.99%) (39805/39808)\n",
      "Epoch: 153 | Batch_idx: 320 |  Loss_1: (0.0004) | Acc_1: (99.99%) (41085/41088)\n",
      "Epoch: 153 | Batch_idx: 330 |  Loss_1: (0.0004) | Acc_1: (99.99%) (42365/42368)\n",
      "Epoch: 153 | Batch_idx: 340 |  Loss_1: (0.0004) | Acc_1: (99.99%) (43645/43648)\n",
      "Epoch: 153 | Batch_idx: 350 |  Loss_1: (0.0004) | Acc_1: (99.99%) (44925/44928)\n",
      "Epoch: 153 | Batch_idx: 360 |  Loss_1: (0.0004) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 153 | Batch_idx: 370 |  Loss_1: (0.0004) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 153 | Batch_idx: 380 |  Loss_1: (0.0004) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 153 | Batch_idx: 390 |  Loss_1: (0.0004) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4452) | Acc: (93.34%) (9334/10000)\n",
      "Epoch: 154 | Batch_idx: 0 |  Loss_1: (0.0010) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 154 | Batch_idx: 10 |  Loss_1: (0.0012) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 154 | Batch_idx: 20 |  Loss_1: (0.0008) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 154 | Batch_idx: 30 |  Loss_1: (0.0010) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 154 | Batch_idx: 40 |  Loss_1: (0.0008) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 154 | Batch_idx: 50 |  Loss_1: (0.0007) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 154 | Batch_idx: 60 |  Loss_1: (0.0006) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 154 | Batch_idx: 70 |  Loss_1: (0.0006) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 154 | Batch_idx: 80 |  Loss_1: (0.0005) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 154 | Batch_idx: 90 |  Loss_1: (0.0005) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 154 | Batch_idx: 100 |  Loss_1: (0.0005) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 154 | Batch_idx: 110 |  Loss_1: (0.0005) | Acc_1: (99.99%) (14206/14208)\n",
      "Epoch: 154 | Batch_idx: 120 |  Loss_1: (0.0004) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 154 | Batch_idx: 130 |  Loss_1: (0.0004) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 154 | Batch_idx: 140 |  Loss_1: (0.0004) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 154 | Batch_idx: 150 |  Loss_1: (0.0004) | Acc_1: (99.99%) (19326/19328)\n",
      "Epoch: 154 | Batch_idx: 160 |  Loss_1: (0.0004) | Acc_1: (99.99%) (20606/20608)\n",
      "Epoch: 154 | Batch_idx: 170 |  Loss_1: (0.0003) | Acc_1: (99.99%) (21886/21888)\n",
      "Epoch: 154 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (99.99%) (23166/23168)\n",
      "Epoch: 154 | Batch_idx: 190 |  Loss_1: (0.0003) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 154 | Batch_idx: 200 |  Loss_1: (0.0003) | Acc_1: (99.99%) (25726/25728)\n",
      "Epoch: 154 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (99.99%) (27006/27008)\n",
      "Epoch: 154 | Batch_idx: 220 |  Loss_1: (0.0003) | Acc_1: (99.99%) (28286/28288)\n",
      "Epoch: 154 | Batch_idx: 230 |  Loss_1: (0.0003) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 154 | Batch_idx: 240 |  Loss_1: (0.0003) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 154 | Batch_idx: 250 |  Loss_1: (0.0003) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 154 | Batch_idx: 260 |  Loss_1: (0.0003) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 154 | Batch_idx: 270 |  Loss_1: (0.0003) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 154 | Batch_idx: 280 |  Loss_1: (0.0004) | Acc_1: (99.99%) (35964/35968)\n",
      "Epoch: 154 | Batch_idx: 290 |  Loss_1: (0.0003) | Acc_1: (99.99%) (37244/37248)\n",
      "Epoch: 154 | Batch_idx: 300 |  Loss_1: (0.0004) | Acc_1: (99.99%) (38524/38528)\n",
      "Epoch: 154 | Batch_idx: 310 |  Loss_1: (0.0004) | Acc_1: (99.99%) (39804/39808)\n",
      "Epoch: 154 | Batch_idx: 320 |  Loss_1: (0.0004) | Acc_1: (99.99%) (41084/41088)\n",
      "Epoch: 154 | Batch_idx: 330 |  Loss_1: (0.0004) | Acc_1: (99.99%) (42364/42368)\n",
      "Epoch: 154 | Batch_idx: 340 |  Loss_1: (0.0004) | Acc_1: (99.99%) (43644/43648)\n",
      "Epoch: 154 | Batch_idx: 350 |  Loss_1: (0.0004) | Acc_1: (99.99%) (44924/44928)\n",
      "Epoch: 154 | Batch_idx: 360 |  Loss_1: (0.0004) | Acc_1: (99.99%) (46204/46208)\n",
      "Epoch: 154 | Batch_idx: 370 |  Loss_1: (0.0004) | Acc_1: (99.99%) (47484/47488)\n",
      "Epoch: 154 | Batch_idx: 380 |  Loss_1: (0.0004) | Acc_1: (99.99%) (48764/48768)\n",
      "Epoch: 154 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49996/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4517) | Acc: (93.27%) (9327/10000)\n",
      "Epoch: 155 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 155 | Batch_idx: 10 |  Loss_1: (0.0006) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 155 | Batch_idx: 20 |  Loss_1: (0.0005) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 155 | Batch_idx: 30 |  Loss_1: (0.0004) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 155 | Batch_idx: 40 |  Loss_1: (0.0005) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 155 | Batch_idx: 50 |  Loss_1: (0.0005) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 155 | Batch_idx: 60 |  Loss_1: (0.0005) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 155 | Batch_idx: 70 |  Loss_1: (0.0004) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 155 | Batch_idx: 80 |  Loss_1: (0.0005) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 155 | Batch_idx: 90 |  Loss_1: (0.0004) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 155 | Batch_idx: 100 |  Loss_1: (0.0004) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 155 | Batch_idx: 110 |  Loss_1: (0.0004) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 155 | Batch_idx: 120 |  Loss_1: (0.0004) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 155 | Batch_idx: 130 |  Loss_1: (0.0004) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 155 | Batch_idx: 140 |  Loss_1: (0.0004) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 155 | Batch_idx: 150 |  Loss_1: (0.0004) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 155 | Batch_idx: 160 |  Loss_1: (0.0004) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 155 | Batch_idx: 170 |  Loss_1: (0.0003) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 155 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 155 | Batch_idx: 190 |  Loss_1: (0.0003) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 155 | Batch_idx: 200 |  Loss_1: (0.0003) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 155 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 155 | Batch_idx: 220 |  Loss_1: (0.0003) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 155 | Batch_idx: 230 |  Loss_1: (0.0003) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 155 | Batch_idx: 240 |  Loss_1: (0.0003) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 155 | Batch_idx: 250 |  Loss_1: (0.0003) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 155 | Batch_idx: 260 |  Loss_1: (0.0003) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 155 | Batch_idx: 270 |  Loss_1: (0.0003) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 155 | Batch_idx: 280 |  Loss_1: (0.0003) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 155 | Batch_idx: 290 |  Loss_1: (0.0003) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 155 | Batch_idx: 300 |  Loss_1: (0.0003) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 155 | Batch_idx: 310 |  Loss_1: (0.0003) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 155 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 155 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 155 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 155 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 155 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 155 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 155 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 155 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4573) | Acc: (93.35%) (9335/10000)\n",
      "Epoch: 156 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 156 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 156 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 156 | Batch_idx: 30 |  Loss_1: (0.0009) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 156 | Batch_idx: 40 |  Loss_1: (0.0007) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 156 | Batch_idx: 50 |  Loss_1: (0.0007) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 156 | Batch_idx: 60 |  Loss_1: (0.0006) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 156 | Batch_idx: 70 |  Loss_1: (0.0005) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 156 | Batch_idx: 80 |  Loss_1: (0.0005) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 156 | Batch_idx: 90 |  Loss_1: (0.0005) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 156 | Batch_idx: 100 |  Loss_1: (0.0004) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 156 | Batch_idx: 110 |  Loss_1: (0.0005) | Acc_1: (99.98%) (14205/14208)\n",
      "Epoch: 156 | Batch_idx: 120 |  Loss_1: (0.0005) | Acc_1: (99.98%) (15485/15488)\n",
      "Epoch: 156 | Batch_idx: 130 |  Loss_1: (0.0005) | Acc_1: (99.98%) (16765/16768)\n",
      "Epoch: 156 | Batch_idx: 140 |  Loss_1: (0.0008) | Acc_1: (99.98%) (18044/18048)\n",
      "Epoch: 156 | Batch_idx: 150 |  Loss_1: (0.0008) | Acc_1: (99.98%) (19324/19328)\n",
      "Epoch: 156 | Batch_idx: 160 |  Loss_1: (0.0007) | Acc_1: (99.98%) (20604/20608)\n",
      "Epoch: 156 | Batch_idx: 170 |  Loss_1: (0.0007) | Acc_1: (99.98%) (21884/21888)\n",
      "Epoch: 156 | Batch_idx: 180 |  Loss_1: (0.0007) | Acc_1: (99.98%) (23164/23168)\n",
      "Epoch: 156 | Batch_idx: 190 |  Loss_1: (0.0006) | Acc_1: (99.98%) (24444/24448)\n",
      "Epoch: 156 | Batch_idx: 200 |  Loss_1: (0.0006) | Acc_1: (99.98%) (25724/25728)\n",
      "Epoch: 156 | Batch_idx: 210 |  Loss_1: (0.0007) | Acc_1: (99.98%) (27003/27008)\n",
      "Epoch: 156 | Batch_idx: 220 |  Loss_1: (0.0007) | Acc_1: (99.98%) (28283/28288)\n",
      "Epoch: 156 | Batch_idx: 230 |  Loss_1: (0.0006) | Acc_1: (99.98%) (29563/29568)\n",
      "Epoch: 156 | Batch_idx: 240 |  Loss_1: (0.0006) | Acc_1: (99.98%) (30843/30848)\n",
      "Epoch: 156 | Batch_idx: 250 |  Loss_1: (0.0006) | Acc_1: (99.98%) (32123/32128)\n",
      "Epoch: 156 | Batch_idx: 260 |  Loss_1: (0.0006) | Acc_1: (99.99%) (33403/33408)\n",
      "Epoch: 156 | Batch_idx: 270 |  Loss_1: (0.0006) | Acc_1: (99.99%) (34683/34688)\n",
      "Epoch: 156 | Batch_idx: 280 |  Loss_1: (0.0006) | Acc_1: (99.99%) (35963/35968)\n",
      "Epoch: 156 | Batch_idx: 290 |  Loss_1: (0.0005) | Acc_1: (99.99%) (37243/37248)\n",
      "Epoch: 156 | Batch_idx: 300 |  Loss_1: (0.0006) | Acc_1: (99.98%) (38522/38528)\n",
      "Epoch: 156 | Batch_idx: 310 |  Loss_1: (0.0005) | Acc_1: (99.98%) (39802/39808)\n",
      "Epoch: 156 | Batch_idx: 320 |  Loss_1: (0.0006) | Acc_1: (99.98%) (41081/41088)\n",
      "Epoch: 156 | Batch_idx: 330 |  Loss_1: (0.0006) | Acc_1: (99.98%) (42361/42368)\n",
      "Epoch: 156 | Batch_idx: 340 |  Loss_1: (0.0005) | Acc_1: (99.98%) (43641/43648)\n",
      "Epoch: 156 | Batch_idx: 350 |  Loss_1: (0.0005) | Acc_1: (99.98%) (44921/44928)\n",
      "Epoch: 156 | Batch_idx: 360 |  Loss_1: (0.0005) | Acc_1: (99.98%) (46201/46208)\n",
      "Epoch: 156 | Batch_idx: 370 |  Loss_1: (0.0005) | Acc_1: (99.99%) (47481/47488)\n",
      "Epoch: 156 | Batch_idx: 380 |  Loss_1: (0.0005) | Acc_1: (99.99%) (48761/48768)\n",
      "Epoch: 156 | Batch_idx: 390 |  Loss_1: (0.0005) | Acc_1: (99.99%) (49993/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4575) | Acc: (93.29%) (9329/10000)\n",
      "Epoch: 157 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 157 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 157 | Batch_idx: 20 |  Loss_1: (0.0007) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 157 | Batch_idx: 30 |  Loss_1: (0.0008) | Acc_1: (99.95%) (3966/3968)\n",
      "Epoch: 157 | Batch_idx: 40 |  Loss_1: (0.0006) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 157 | Batch_idx: 50 |  Loss_1: (0.0006) | Acc_1: (99.97%) (6526/6528)\n",
      "Epoch: 157 | Batch_idx: 60 |  Loss_1: (0.0005) | Acc_1: (99.97%) (7806/7808)\n",
      "Epoch: 157 | Batch_idx: 70 |  Loss_1: (0.0005) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 157 | Batch_idx: 80 |  Loss_1: (0.0004) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 157 | Batch_idx: 90 |  Loss_1: (0.0004) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 157 | Batch_idx: 100 |  Loss_1: (0.0004) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 157 | Batch_idx: 110 |  Loss_1: (0.0004) | Acc_1: (99.99%) (14206/14208)\n",
      "Epoch: 157 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 157 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 157 | Batch_idx: 140 |  Loss_1: (0.0004) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 157 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (99.99%) (19326/19328)\n",
      "Epoch: 157 | Batch_idx: 160 |  Loss_1: (0.0003) | Acc_1: (99.99%) (20606/20608)\n",
      "Epoch: 157 | Batch_idx: 170 |  Loss_1: (0.0003) | Acc_1: (99.99%) (21886/21888)\n",
      "Epoch: 157 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (99.99%) (23166/23168)\n",
      "Epoch: 157 | Batch_idx: 190 |  Loss_1: (0.0003) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 157 | Batch_idx: 200 |  Loss_1: (0.0003) | Acc_1: (99.99%) (25726/25728)\n",
      "Epoch: 157 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (99.99%) (27006/27008)\n",
      "Epoch: 157 | Batch_idx: 220 |  Loss_1: (0.0003) | Acc_1: (99.99%) (28286/28288)\n",
      "Epoch: 157 | Batch_idx: 230 |  Loss_1: (0.0003) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 157 | Batch_idx: 240 |  Loss_1: (0.0003) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 157 | Batch_idx: 250 |  Loss_1: (0.0003) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 157 | Batch_idx: 260 |  Loss_1: (0.0003) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 157 | Batch_idx: 270 |  Loss_1: (0.0003) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 157 | Batch_idx: 280 |  Loss_1: (0.0003) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 157 | Batch_idx: 290 |  Loss_1: (0.0003) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 157 | Batch_idx: 300 |  Loss_1: (0.0003) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 157 | Batch_idx: 310 |  Loss_1: (0.0003) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 157 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 157 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 157 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 157 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 157 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 157 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 157 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 157 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4587) | Acc: (93.34%) (9334/10000)\n",
      "Epoch: 158 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 158 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 158 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 158 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 158 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 158 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 158 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 158 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 158 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 158 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 158 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 158 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 158 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 158 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 158 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 158 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 158 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 158 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 158 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 158 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 158 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 158 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (99.99%) (27006/27008)\n",
      "Epoch: 158 | Batch_idx: 220 |  Loss_1: (0.0004) | Acc_1: (99.99%) (28286/28288)\n",
      "Epoch: 158 | Batch_idx: 230 |  Loss_1: (0.0004) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 158 | Batch_idx: 240 |  Loss_1: (0.0004) | Acc_1: (99.99%) (30845/30848)\n",
      "Epoch: 158 | Batch_idx: 250 |  Loss_1: (0.0004) | Acc_1: (99.99%) (32125/32128)\n",
      "Epoch: 158 | Batch_idx: 260 |  Loss_1: (0.0004) | Acc_1: (99.99%) (33405/33408)\n",
      "Epoch: 158 | Batch_idx: 270 |  Loss_1: (0.0004) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 158 | Batch_idx: 280 |  Loss_1: (0.0004) | Acc_1: (99.99%) (35965/35968)\n",
      "Epoch: 158 | Batch_idx: 290 |  Loss_1: (0.0004) | Acc_1: (99.99%) (37245/37248)\n",
      "Epoch: 158 | Batch_idx: 300 |  Loss_1: (0.0004) | Acc_1: (99.99%) (38525/38528)\n",
      "Epoch: 158 | Batch_idx: 310 |  Loss_1: (0.0004) | Acc_1: (99.99%) (39805/39808)\n",
      "Epoch: 158 | Batch_idx: 320 |  Loss_1: (0.0004) | Acc_1: (99.99%) (41085/41088)\n",
      "Epoch: 158 | Batch_idx: 330 |  Loss_1: (0.0004) | Acc_1: (99.99%) (42365/42368)\n",
      "Epoch: 158 | Batch_idx: 340 |  Loss_1: (0.0004) | Acc_1: (99.99%) (43645/43648)\n",
      "Epoch: 158 | Batch_idx: 350 |  Loss_1: (0.0004) | Acc_1: (99.99%) (44925/44928)\n",
      "Epoch: 158 | Batch_idx: 360 |  Loss_1: (0.0004) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 158 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 158 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 158 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4626) | Acc: (93.31%) (9331/10000)\n",
      "Epoch: 159 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 159 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 159 | Batch_idx: 20 |  Loss_1: (0.0005) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 159 | Batch_idx: 30 |  Loss_1: (0.0005) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 159 | Batch_idx: 40 |  Loss_1: (0.0004) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 159 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 159 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 159 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 159 | Batch_idx: 80 |  Loss_1: (0.0003) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 159 | Batch_idx: 90 |  Loss_1: (0.0003) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 159 | Batch_idx: 100 |  Loss_1: (0.0003) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 159 | Batch_idx: 110 |  Loss_1: (0.0003) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 159 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 159 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 159 | Batch_idx: 140 |  Loss_1: (0.0003) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 159 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 159 | Batch_idx: 160 |  Loss_1: (0.0003) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 159 | Batch_idx: 170 |  Loss_1: (0.0003) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 159 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 159 | Batch_idx: 190 |  Loss_1: (0.0003) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 159 | Batch_idx: 200 |  Loss_1: (0.0003) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 159 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 159 | Batch_idx: 220 |  Loss_1: (0.0003) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 159 | Batch_idx: 230 |  Loss_1: (0.0003) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 159 | Batch_idx: 240 |  Loss_1: (0.0003) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 159 | Batch_idx: 250 |  Loss_1: (0.0003) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 159 | Batch_idx: 260 |  Loss_1: (0.0003) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 159 | Batch_idx: 270 |  Loss_1: (0.0003) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 159 | Batch_idx: 280 |  Loss_1: (0.0003) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 159 | Batch_idx: 290 |  Loss_1: (0.0003) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 159 | Batch_idx: 300 |  Loss_1: (0.0003) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 159 | Batch_idx: 310 |  Loss_1: (0.0003) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 159 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 159 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 159 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 159 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 159 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 159 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 159 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 159 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4602) | Acc: (93.36%) (9336/10000)\n",
      "Epoch: 160 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 160 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 160 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 160 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 160 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 160 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 160 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 160 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 160 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 160 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 160 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 160 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 160 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 160 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 160 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 160 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 160 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 160 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 160 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 160 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 160 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 160 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 160 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 160 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 160 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 160 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 160 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 160 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 160 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 160 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 160 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 160 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 160 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 160 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 160 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 160 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 160 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 160 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 160 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 160 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4538) | Acc: (93.33%) (9333/10000)\n",
      "Epoch: 161 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 161 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 161 | Batch_idx: 20 |  Loss_1: (0.0002) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 161 | Batch_idx: 30 |  Loss_1: (0.0002) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 161 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 161 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 161 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 161 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 161 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 161 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 161 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 161 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 161 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 161 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 161 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 161 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 161 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 161 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 161 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 161 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 161 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 161 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 161 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 161 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 161 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 161 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 161 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 161 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 161 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 161 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 161 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 161 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 161 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 161 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 161 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 161 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 161 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 161 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 161 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 161 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4583) | Acc: (93.37%) (9337/10000)\n",
      "Epoch: 162 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 162 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 162 | Batch_idx: 20 |  Loss_1: (0.0004) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 162 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 162 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 162 | Batch_idx: 50 |  Loss_1: (0.0005) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 162 | Batch_idx: 60 |  Loss_1: (0.0005) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 162 | Batch_idx: 70 |  Loss_1: (0.0008) | Acc_1: (99.98%) (9086/9088)\n",
      "Epoch: 162 | Batch_idx: 80 |  Loss_1: (0.0007) | Acc_1: (99.98%) (10366/10368)\n",
      "Epoch: 162 | Batch_idx: 90 |  Loss_1: (0.0007) | Acc_1: (99.98%) (11646/11648)\n",
      "Epoch: 162 | Batch_idx: 100 |  Loss_1: (0.0006) | Acc_1: (99.98%) (12926/12928)\n",
      "Epoch: 162 | Batch_idx: 110 |  Loss_1: (0.0006) | Acc_1: (99.99%) (14206/14208)\n",
      "Epoch: 162 | Batch_idx: 120 |  Loss_1: (0.0006) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 162 | Batch_idx: 130 |  Loss_1: (0.0005) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 162 | Batch_idx: 140 |  Loss_1: (0.0005) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 162 | Batch_idx: 150 |  Loss_1: (0.0005) | Acc_1: (99.99%) (19326/19328)\n",
      "Epoch: 162 | Batch_idx: 160 |  Loss_1: (0.0006) | Acc_1: (99.99%) (20605/20608)\n",
      "Epoch: 162 | Batch_idx: 170 |  Loss_1: (0.0006) | Acc_1: (99.99%) (21885/21888)\n",
      "Epoch: 162 | Batch_idx: 180 |  Loss_1: (0.0006) | Acc_1: (99.99%) (23165/23168)\n",
      "Epoch: 162 | Batch_idx: 190 |  Loss_1: (0.0006) | Acc_1: (99.99%) (24445/24448)\n",
      "Epoch: 162 | Batch_idx: 200 |  Loss_1: (0.0006) | Acc_1: (99.99%) (25725/25728)\n",
      "Epoch: 162 | Batch_idx: 210 |  Loss_1: (0.0005) | Acc_1: (99.99%) (27005/27008)\n",
      "Epoch: 162 | Batch_idx: 220 |  Loss_1: (0.0005) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 162 | Batch_idx: 230 |  Loss_1: (0.0005) | Acc_1: (99.99%) (29565/29568)\n",
      "Epoch: 162 | Batch_idx: 240 |  Loss_1: (0.0005) | Acc_1: (99.99%) (30845/30848)\n",
      "Epoch: 162 | Batch_idx: 250 |  Loss_1: (0.0005) | Acc_1: (99.99%) (32125/32128)\n",
      "Epoch: 162 | Batch_idx: 260 |  Loss_1: (0.0005) | Acc_1: (99.99%) (33405/33408)\n",
      "Epoch: 162 | Batch_idx: 270 |  Loss_1: (0.0005) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 162 | Batch_idx: 280 |  Loss_1: (0.0005) | Acc_1: (99.99%) (35965/35968)\n",
      "Epoch: 162 | Batch_idx: 290 |  Loss_1: (0.0005) | Acc_1: (99.99%) (37245/37248)\n",
      "Epoch: 162 | Batch_idx: 300 |  Loss_1: (0.0005) | Acc_1: (99.99%) (38524/38528)\n",
      "Epoch: 162 | Batch_idx: 310 |  Loss_1: (0.0005) | Acc_1: (99.99%) (39804/39808)\n",
      "Epoch: 162 | Batch_idx: 320 |  Loss_1: (0.0005) | Acc_1: (99.99%) (41084/41088)\n",
      "Epoch: 162 | Batch_idx: 330 |  Loss_1: (0.0005) | Acc_1: (99.99%) (42364/42368)\n",
      "Epoch: 162 | Batch_idx: 340 |  Loss_1: (0.0005) | Acc_1: (99.99%) (43644/43648)\n",
      "Epoch: 162 | Batch_idx: 350 |  Loss_1: (0.0005) | Acc_1: (99.99%) (44924/44928)\n",
      "Epoch: 162 | Batch_idx: 360 |  Loss_1: (0.0005) | Acc_1: (99.99%) (46204/46208)\n",
      "Epoch: 162 | Batch_idx: 370 |  Loss_1: (0.0005) | Acc_1: (99.99%) (47484/47488)\n",
      "Epoch: 162 | Batch_idx: 380 |  Loss_1: (0.0005) | Acc_1: (99.99%) (48764/48768)\n",
      "Epoch: 162 | Batch_idx: 390 |  Loss_1: (0.0005) | Acc_1: (99.99%) (49996/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4532) | Acc: (93.23%) (9323/10000)\n",
      "Epoch: 163 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 163 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 163 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 163 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 163 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 163 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 163 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 163 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 163 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 163 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 163 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 163 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 163 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 163 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 163 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 163 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 163 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 163 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 163 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 163 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 163 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 163 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 163 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 163 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 163 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 163 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 163 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 163 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 163 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 163 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 163 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 163 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 163 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 163 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 163 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 163 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 163 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 163 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 163 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 163 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4505) | Acc: (93.32%) (9332/10000)\n",
      "Epoch: 164 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 164 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 164 | Batch_idx: 20 |  Loss_1: (0.0002) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 164 | Batch_idx: 30 |  Loss_1: (0.0002) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 164 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 164 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 164 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 164 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 164 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 164 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 164 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 164 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 164 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 164 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 164 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 164 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 164 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 164 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 164 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 164 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 164 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 164 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 164 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 164 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 164 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 164 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 164 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 164 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 164 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 164 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 164 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 164 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 164 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 164 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 164 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 164 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 164 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 164 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 164 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 164 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4470) | Acc: (93.47%) (9347/10000)\n",
      "Epoch: 165 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 165 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 165 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 165 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 165 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 165 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 165 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 165 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 165 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 165 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 165 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 165 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 165 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 165 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 165 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 165 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 165 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 165 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 165 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 165 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 165 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 165 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 165 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 165 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 165 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 165 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 165 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 165 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 165 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 165 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 165 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 165 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 165 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 165 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 165 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 165 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 165 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 165 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 165 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 165 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4475) | Acc: (93.40%) (9340/10000)\n",
      "Epoch: 166 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 166 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 166 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 166 | Batch_idx: 30 |  Loss_1: (0.0005) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 166 | Batch_idx: 40 |  Loss_1: (0.0004) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 166 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 166 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 166 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 166 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 166 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 166 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 166 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 166 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 166 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 166 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 166 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 166 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 166 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 166 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 166 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 166 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 166 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 166 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 166 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 166 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 166 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 166 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 166 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 166 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 166 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 166 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 166 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 166 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 166 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 166 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 166 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 166 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 166 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 166 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 166 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4471) | Acc: (93.41%) (9341/10000)\n",
      "Epoch: 167 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 167 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 167 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 167 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 167 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 167 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 167 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 167 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 167 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 167 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 167 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 167 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 167 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 167 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 167 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 167 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 167 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 167 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 167 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 167 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 167 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 167 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 167 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (99.99%) (28286/28288)\n",
      "Epoch: 167 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 167 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 167 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 167 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 167 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 167 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35965/35968)\n",
      "Epoch: 167 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37245/37248)\n",
      "Epoch: 167 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38525/38528)\n",
      "Epoch: 167 | Batch_idx: 310 |  Loss_1: (0.0003) | Acc_1: (99.99%) (39804/39808)\n",
      "Epoch: 167 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (99.99%) (41084/41088)\n",
      "Epoch: 167 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (99.99%) (42364/42368)\n",
      "Epoch: 167 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (99.99%) (43644/43648)\n",
      "Epoch: 167 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (99.99%) (44924/44928)\n",
      "Epoch: 167 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (99.99%) (46204/46208)\n",
      "Epoch: 167 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (99.99%) (47483/47488)\n",
      "Epoch: 167 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (99.99%) (48763/48768)\n",
      "Epoch: 167 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49995/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4537) | Acc: (93.37%) (9337/10000)\n",
      "Epoch: 168 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 168 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 168 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 168 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 168 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 168 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 168 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 168 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 168 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 168 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 168 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 168 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 168 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 168 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 168 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 168 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 168 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 168 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 168 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 168 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 168 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 168 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 168 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 168 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 168 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 168 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 168 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 168 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 168 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 168 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 168 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 168 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 168 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 168 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 168 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 168 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 168 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 168 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 168 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 168 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4524) | Acc: (93.55%) (9355/10000)\n",
      "Epoch: 169 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 169 | Batch_idx: 10 |  Loss_1: (0.0008) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 169 | Batch_idx: 20 |  Loss_1: (0.0005) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 169 | Batch_idx: 30 |  Loss_1: (0.0004) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 169 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 169 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 169 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 169 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 169 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 169 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 169 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 169 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 169 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 169 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 169 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 169 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 169 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 169 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 169 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 169 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 169 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 169 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 169 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 169 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 169 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 169 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 169 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 169 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 169 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 169 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 169 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 169 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 169 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 169 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 169 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 169 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 169 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 169 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 169 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 169 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4516) | Acc: (93.46%) (9346/10000)\n",
      "Epoch: 170 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 170 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 170 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 170 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 170 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 170 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 170 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 170 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 170 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 170 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 170 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 170 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 170 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 170 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 170 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 170 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 170 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 170 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 170 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 170 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 170 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 170 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 170 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 170 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 170 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 170 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 170 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 170 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 170 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 170 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 170 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 170 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 170 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 170 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 170 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (99.99%) (43645/43648)\n",
      "Epoch: 170 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (99.99%) (44925/44928)\n",
      "Epoch: 170 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 170 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 170 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 170 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4541) | Acc: (93.30%) (9330/10000)\n",
      "Epoch: 171 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 171 | Batch_idx: 10 |  Loss_1: (0.0004) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 171 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 171 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 171 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 171 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 171 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 171 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 171 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 171 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 171 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 171 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 171 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 171 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 171 | Batch_idx: 140 |  Loss_1: (0.0003) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 171 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 171 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 171 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 171 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 171 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 171 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 171 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 171 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 171 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 171 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 171 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 171 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 171 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 171 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 171 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 171 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 171 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 171 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 171 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 171 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43648/43648)\n",
      "Epoch: 171 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44928/44928)\n",
      "Epoch: 171 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46208/46208)\n",
      "Epoch: 171 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47488/47488)\n",
      "Epoch: 171 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48768/48768)\n",
      "Epoch: 171 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (50000/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4550) | Acc: (93.32%) (9332/10000)\n",
      "Epoch: 172 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 172 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 172 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 172 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 172 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 172 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 172 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 172 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 172 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 172 | Batch_idx: 90 |  Loss_1: (0.0003) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 172 | Batch_idx: 100 |  Loss_1: (0.0003) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 172 | Batch_idx: 110 |  Loss_1: (0.0003) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 172 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 172 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 172 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 172 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 172 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 172 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 172 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 172 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 172 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 172 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 172 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 172 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 172 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 172 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 172 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 172 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 172 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 172 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 172 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 172 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 172 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 172 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 172 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 172 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 172 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 172 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 172 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 172 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4532) | Acc: (93.32%) (9332/10000)\n",
      "Epoch: 173 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 173 | Batch_idx: 10 |  Loss_1: (0.0000) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 173 | Batch_idx: 20 |  Loss_1: (0.0000) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 173 | Batch_idx: 30 |  Loss_1: (0.0000) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 173 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 173 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 173 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 173 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 173 | Batch_idx: 80 |  Loss_1: (0.0003) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 173 | Batch_idx: 90 |  Loss_1: (0.0003) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 173 | Batch_idx: 100 |  Loss_1: (0.0003) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 173 | Batch_idx: 110 |  Loss_1: (0.0003) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 173 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 173 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 173 | Batch_idx: 140 |  Loss_1: (0.0003) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 173 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 173 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 173 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 173 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 173 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 173 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 173 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 173 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 173 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 173 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 173 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 173 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 173 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 173 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 173 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 173 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 173 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 173 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 173 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 173 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (99.99%) (43645/43648)\n",
      "Epoch: 173 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (99.99%) (44925/44928)\n",
      "Epoch: 173 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 173 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 173 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 173 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4460) | Acc: (93.45%) (9345/10000)\n",
      "Epoch: 174 | Batch_idx: 0 |  Loss_1: (0.0004) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 174 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 174 | Batch_idx: 20 |  Loss_1: (0.0002) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 174 | Batch_idx: 30 |  Loss_1: (0.0002) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 174 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 174 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 174 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 174 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 174 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 174 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 174 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 174 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 174 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 174 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 174 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 174 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 174 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 174 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 174 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 174 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 174 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 174 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 174 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 174 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 174 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 174 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 174 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 174 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 174 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 174 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 174 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 174 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 174 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 174 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 174 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 174 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 174 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 174 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 174 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 174 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4506) | Acc: (93.34%) (9334/10000)\n",
      "Epoch: 175 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 175 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 175 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 175 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 175 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 175 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 175 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 175 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 175 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 175 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 175 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 175 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 175 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 175 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 175 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 175 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 175 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 175 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 175 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 175 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 175 | Batch_idx: 200 |  Loss_1: (0.0003) | Acc_1: (99.99%) (25725/25728)\n",
      "Epoch: 175 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (99.99%) (27005/27008)\n",
      "Epoch: 175 | Batch_idx: 220 |  Loss_1: (0.0003) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 175 | Batch_idx: 230 |  Loss_1: (0.0003) | Acc_1: (99.99%) (29565/29568)\n",
      "Epoch: 175 | Batch_idx: 240 |  Loss_1: (0.0003) | Acc_1: (99.99%) (30844/30848)\n",
      "Epoch: 175 | Batch_idx: 250 |  Loss_1: (0.0003) | Acc_1: (99.99%) (32124/32128)\n",
      "Epoch: 175 | Batch_idx: 260 |  Loss_1: (0.0003) | Acc_1: (99.99%) (33404/33408)\n",
      "Epoch: 175 | Batch_idx: 270 |  Loss_1: (0.0003) | Acc_1: (99.99%) (34684/34688)\n",
      "Epoch: 175 | Batch_idx: 280 |  Loss_1: (0.0003) | Acc_1: (99.99%) (35964/35968)\n",
      "Epoch: 175 | Batch_idx: 290 |  Loss_1: (0.0003) | Acc_1: (99.99%) (37244/37248)\n",
      "Epoch: 175 | Batch_idx: 300 |  Loss_1: (0.0003) | Acc_1: (99.99%) (38524/38528)\n",
      "Epoch: 175 | Batch_idx: 310 |  Loss_1: (0.0003) | Acc_1: (99.99%) (39804/39808)\n",
      "Epoch: 175 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (99.99%) (41084/41088)\n",
      "Epoch: 175 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (99.99%) (42364/42368)\n",
      "Epoch: 175 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (99.99%) (43644/43648)\n",
      "Epoch: 175 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (99.99%) (44924/44928)\n",
      "Epoch: 175 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (99.99%) (46204/46208)\n",
      "Epoch: 175 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (99.99%) (47484/47488)\n",
      "Epoch: 175 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (99.99%) (48763/48768)\n",
      "Epoch: 175 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49995/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4503) | Acc: (93.36%) (9336/10000)\n",
      "Epoch: 176 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 176 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 176 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 176 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 176 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 176 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 176 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 176 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 176 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 176 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 176 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 176 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 176 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 176 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 176 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 176 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 176 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 176 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 176 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 176 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 176 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 176 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 176 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 176 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 176 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 176 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 176 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 176 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 176 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 176 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 176 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 176 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 176 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 176 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 176 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 176 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 176 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 176 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 176 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 176 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4535) | Acc: (93.44%) (9344/10000)\n",
      "Epoch: 177 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 177 | Batch_idx: 10 |  Loss_1: (0.0000) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 177 | Batch_idx: 20 |  Loss_1: (0.0000) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 177 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 177 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 177 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 177 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 177 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 177 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 177 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 177 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 177 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 177 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 177 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 177 | Batch_idx: 140 |  Loss_1: (0.0003) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 177 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 177 | Batch_idx: 160 |  Loss_1: (0.0003) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 177 | Batch_idx: 170 |  Loss_1: (0.0003) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 177 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 177 | Batch_idx: 190 |  Loss_1: (0.0003) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 177 | Batch_idx: 200 |  Loss_1: (0.0003) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 177 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 177 | Batch_idx: 220 |  Loss_1: (0.0003) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 177 | Batch_idx: 230 |  Loss_1: (0.0003) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 177 | Batch_idx: 240 |  Loss_1: (0.0003) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 177 | Batch_idx: 250 |  Loss_1: (0.0003) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 177 | Batch_idx: 260 |  Loss_1: (0.0003) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 177 | Batch_idx: 270 |  Loss_1: (0.0003) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 177 | Batch_idx: 280 |  Loss_1: (0.0003) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 177 | Batch_idx: 290 |  Loss_1: (0.0003) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 177 | Batch_idx: 300 |  Loss_1: (0.0003) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 177 | Batch_idx: 310 |  Loss_1: (0.0003) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 177 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 177 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 177 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 177 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 177 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 177 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 177 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 177 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4487) | Acc: (93.44%) (9344/10000)\n",
      "Epoch: 178 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 178 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 178 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 178 | Batch_idx: 30 |  Loss_1: (0.0002) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 178 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 178 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 178 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 178 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 178 | Batch_idx: 80 |  Loss_1: (0.0003) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 178 | Batch_idx: 90 |  Loss_1: (0.0003) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 178 | Batch_idx: 100 |  Loss_1: (0.0003) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 178 | Batch_idx: 110 |  Loss_1: (0.0003) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 178 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 178 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 178 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 178 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 178 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 178 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 178 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 178 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 178 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 178 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 178 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 178 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 178 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 178 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 178 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 178 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 178 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 178 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 178 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 178 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 178 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 178 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 178 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 178 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 178 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 178 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 178 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 178 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4523) | Acc: (93.44%) (9344/10000)\n",
      "Epoch: 179 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 179 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 179 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 179 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 179 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 179 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 179 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 179 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 179 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 179 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 179 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 179 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 179 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 179 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 179 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 179 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 179 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 179 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 179 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 179 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 179 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 179 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (99.99%) (27006/27008)\n",
      "Epoch: 179 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (99.99%) (28286/28288)\n",
      "Epoch: 179 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 179 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 179 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 179 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 179 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 179 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 179 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 179 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 179 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39805/39808)\n",
      "Epoch: 179 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (99.99%) (41085/41088)\n",
      "Epoch: 179 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (99.99%) (42365/42368)\n",
      "Epoch: 179 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (99.99%) (43645/43648)\n",
      "Epoch: 179 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (99.99%) (44925/44928)\n",
      "Epoch: 179 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 179 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 179 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 179 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4483) | Acc: (93.41%) (9341/10000)\n",
      "Epoch: 180 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 180 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 180 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 180 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 180 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 180 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 180 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 180 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 180 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 180 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 180 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 180 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 180 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 180 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 180 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 180 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 180 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 180 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 180 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 180 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 180 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (99.99%) (25726/25728)\n",
      "Epoch: 180 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (99.99%) (27005/27008)\n",
      "Epoch: 180 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 180 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29565/29568)\n",
      "Epoch: 180 | Batch_idx: 240 |  Loss_1: (0.0003) | Acc_1: (99.99%) (30844/30848)\n",
      "Epoch: 180 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32124/32128)\n",
      "Epoch: 180 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33404/33408)\n",
      "Epoch: 180 | Batch_idx: 270 |  Loss_1: (0.0003) | Acc_1: (99.99%) (34683/34688)\n",
      "Epoch: 180 | Batch_idx: 280 |  Loss_1: (0.0003) | Acc_1: (99.99%) (35963/35968)\n",
      "Epoch: 180 | Batch_idx: 290 |  Loss_1: (0.0003) | Acc_1: (99.99%) (37243/37248)\n",
      "Epoch: 180 | Batch_idx: 300 |  Loss_1: (0.0003) | Acc_1: (99.99%) (38523/38528)\n",
      "Epoch: 180 | Batch_idx: 310 |  Loss_1: (0.0003) | Acc_1: (99.99%) (39803/39808)\n",
      "Epoch: 180 | Batch_idx: 320 |  Loss_1: (0.0003) | Acc_1: (99.99%) (41083/41088)\n",
      "Epoch: 180 | Batch_idx: 330 |  Loss_1: (0.0003) | Acc_1: (99.99%) (42363/42368)\n",
      "Epoch: 180 | Batch_idx: 340 |  Loss_1: (0.0003) | Acc_1: (99.99%) (43643/43648)\n",
      "Epoch: 180 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (99.99%) (44923/44928)\n",
      "Epoch: 180 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (99.99%) (46203/46208)\n",
      "Epoch: 180 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (99.99%) (47483/47488)\n",
      "Epoch: 180 | Batch_idx: 380 |  Loss_1: (0.0003) | Acc_1: (99.99%) (48763/48768)\n",
      "Epoch: 180 | Batch_idx: 390 |  Loss_1: (0.0003) | Acc_1: (99.99%) (49995/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4500) | Acc: (93.38%) (9338/10000)\n",
      "Epoch: 181 | Batch_idx: 0 |  Loss_1: (0.0027) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 181 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 181 | Batch_idx: 20 |  Loss_1: (0.0005) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 181 | Batch_idx: 30 |  Loss_1: (0.0004) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 181 | Batch_idx: 40 |  Loss_1: (0.0004) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 181 | Batch_idx: 50 |  Loss_1: (0.0004) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 181 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 181 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 181 | Batch_idx: 80 |  Loss_1: (0.0003) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 181 | Batch_idx: 90 |  Loss_1: (0.0003) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 181 | Batch_idx: 100 |  Loss_1: (0.0003) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 181 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 181 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 181 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 181 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 181 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 181 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 181 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 181 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 181 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 181 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 181 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 181 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 181 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 181 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 181 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 181 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 181 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 181 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 181 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 181 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 181 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 181 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 181 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 181 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43648/43648)\n",
      "Epoch: 181 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44928/44928)\n",
      "Epoch: 181 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46208/46208)\n",
      "Epoch: 181 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47488/47488)\n",
      "Epoch: 181 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48768/48768)\n",
      "Epoch: 181 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (50000/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4453) | Acc: (93.54%) (9354/10000)\n",
      "Epoch: 182 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 182 | Batch_idx: 10 |  Loss_1: (0.0008) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 182 | Batch_idx: 20 |  Loss_1: (0.0004) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 182 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 182 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 182 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 182 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 182 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 182 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 182 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 182 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 182 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 182 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 182 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 182 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 182 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 182 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 182 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 182 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 182 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 182 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 182 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 182 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 182 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 182 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 182 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 182 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 182 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 182 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 182 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 182 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 182 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 182 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 182 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 182 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 182 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 182 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 182 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 182 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 182 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4504) | Acc: (93.30%) (9330/10000)\n",
      "Epoch: 183 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 183 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 183 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 183 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 183 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 183 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 183 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 183 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 183 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 183 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 183 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 183 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 183 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 183 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 183 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 183 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 183 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 183 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 183 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 183 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 183 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 183 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 183 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 183 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 183 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 183 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 183 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 183 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 183 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 183 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 183 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 183 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 183 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 183 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 183 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43648/43648)\n",
      "Epoch: 183 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44928/44928)\n",
      "Epoch: 183 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46208/46208)\n",
      "Epoch: 183 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47488/47488)\n",
      "Epoch: 183 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48768/48768)\n",
      "Epoch: 183 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (50000/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4510) | Acc: (93.36%) (9336/10000)\n",
      "Epoch: 184 | Batch_idx: 0 |  Loss_1: (0.0011) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 184 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 184 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 184 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 184 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 184 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 184 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 184 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 184 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 184 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 184 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 184 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 184 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 184 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 184 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 184 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 184 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 184 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 184 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 184 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 184 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 184 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 184 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 184 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 184 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 184 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 184 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 184 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 184 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 184 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 184 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 184 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 184 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 184 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 184 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 184 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 184 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 184 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 184 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 184 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4496) | Acc: (93.39%) (9339/10000)\n",
      "Epoch: 185 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 185 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 185 | Batch_idx: 20 |  Loss_1: (0.0005) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 185 | Batch_idx: 30 |  Loss_1: (0.0004) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 185 | Batch_idx: 40 |  Loss_1: (0.0004) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 185 | Batch_idx: 50 |  Loss_1: (0.0003) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 185 | Batch_idx: 60 |  Loss_1: (0.0003) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 185 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 185 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 185 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 185 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 185 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 185 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (99.99%) (15486/15488)\n",
      "Epoch: 185 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (99.99%) (16766/16768)\n",
      "Epoch: 185 | Batch_idx: 140 |  Loss_1: (0.0003) | Acc_1: (99.99%) (18046/18048)\n",
      "Epoch: 185 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (99.99%) (19326/19328)\n",
      "Epoch: 185 | Batch_idx: 160 |  Loss_1: (0.0003) | Acc_1: (99.99%) (20606/20608)\n",
      "Epoch: 185 | Batch_idx: 170 |  Loss_1: (0.0003) | Acc_1: (99.99%) (21886/21888)\n",
      "Epoch: 185 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (99.99%) (23166/23168)\n",
      "Epoch: 185 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (99.99%) (24446/24448)\n",
      "Epoch: 185 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (99.99%) (25726/25728)\n",
      "Epoch: 185 | Batch_idx: 210 |  Loss_1: (0.0003) | Acc_1: (99.99%) (27005/27008)\n",
      "Epoch: 185 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 185 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29565/29568)\n",
      "Epoch: 185 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (99.99%) (30845/30848)\n",
      "Epoch: 185 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32125/32128)\n",
      "Epoch: 185 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33405/33408)\n",
      "Epoch: 185 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 185 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35965/35968)\n",
      "Epoch: 185 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37245/37248)\n",
      "Epoch: 185 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38525/38528)\n",
      "Epoch: 185 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39805/39808)\n",
      "Epoch: 185 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (99.99%) (41085/41088)\n",
      "Epoch: 185 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (99.99%) (42365/42368)\n",
      "Epoch: 185 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (99.99%) (43645/43648)\n",
      "Epoch: 185 | Batch_idx: 350 |  Loss_1: (0.0003) | Acc_1: (99.99%) (44925/44928)\n",
      "Epoch: 185 | Batch_idx: 360 |  Loss_1: (0.0003) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 185 | Batch_idx: 370 |  Loss_1: (0.0003) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 185 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 185 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4473) | Acc: (93.34%) (9334/10000)\n",
      "Epoch: 186 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 186 | Batch_idx: 10 |  Loss_1: (0.0008) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 186 | Batch_idx: 20 |  Loss_1: (0.0004) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 186 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 186 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 186 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 186 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 186 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 186 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 186 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 186 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 186 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 186 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 186 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 186 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 186 | Batch_idx: 150 |  Loss_1: (0.0003) | Acc_1: (99.98%) (19325/19328)\n",
      "Epoch: 186 | Batch_idx: 160 |  Loss_1: (0.0003) | Acc_1: (99.99%) (20605/20608)\n",
      "Epoch: 186 | Batch_idx: 170 |  Loss_1: (0.0003) | Acc_1: (99.99%) (21885/21888)\n",
      "Epoch: 186 | Batch_idx: 180 |  Loss_1: (0.0003) | Acc_1: (99.99%) (23165/23168)\n",
      "Epoch: 186 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (99.99%) (24445/24448)\n",
      "Epoch: 186 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (99.99%) (25725/25728)\n",
      "Epoch: 186 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (99.99%) (27005/27008)\n",
      "Epoch: 186 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (99.99%) (28285/28288)\n",
      "Epoch: 186 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29565/29568)\n",
      "Epoch: 186 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (99.99%) (30845/30848)\n",
      "Epoch: 186 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32125/32128)\n",
      "Epoch: 186 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33405/33408)\n",
      "Epoch: 186 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34685/34688)\n",
      "Epoch: 186 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35965/35968)\n",
      "Epoch: 186 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37245/37248)\n",
      "Epoch: 186 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38525/38528)\n",
      "Epoch: 186 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39805/39808)\n",
      "Epoch: 186 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (99.99%) (41085/41088)\n",
      "Epoch: 186 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (99.99%) (42365/42368)\n",
      "Epoch: 186 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (99.99%) (43645/43648)\n",
      "Epoch: 186 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (99.99%) (44925/44928)\n",
      "Epoch: 186 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (99.99%) (46205/46208)\n",
      "Epoch: 186 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (99.99%) (47485/47488)\n",
      "Epoch: 186 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (99.99%) (48765/48768)\n",
      "Epoch: 186 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (99.99%) (49997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4425) | Acc: (93.38%) (9338/10000)\n",
      "Epoch: 187 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 187 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 187 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 187 | Batch_idx: 30 |  Loss_1: (0.0002) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 187 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 187 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 187 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 187 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 187 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 187 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 187 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 187 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 187 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 187 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 187 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 187 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 187 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 187 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 187 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 187 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 187 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 187 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 187 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 187 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 187 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 187 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 187 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 187 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 187 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 187 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 187 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 187 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 187 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 187 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 187 | Batch_idx: 340 |  Loss_1: (0.0001) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 187 | Batch_idx: 350 |  Loss_1: (0.0001) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 187 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 187 | Batch_idx: 370 |  Loss_1: (0.0001) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 187 | Batch_idx: 380 |  Loss_1: (0.0001) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 187 | Batch_idx: 390 |  Loss_1: (0.0001) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4442) | Acc: (93.33%) (9333/10000)\n",
      "Epoch: 188 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 188 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 188 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 188 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 188 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 188 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 188 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 188 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 188 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 188 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 188 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 188 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 188 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 188 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 188 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 188 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 188 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 188 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 188 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 188 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 188 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 188 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (99.99%) (27006/27008)\n",
      "Epoch: 188 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (99.99%) (28286/28288)\n",
      "Epoch: 188 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (99.99%) (29566/29568)\n",
      "Epoch: 188 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (99.99%) (30846/30848)\n",
      "Epoch: 188 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (99.99%) (32126/32128)\n",
      "Epoch: 188 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 188 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 188 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 188 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 188 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 188 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 188 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 188 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 188 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 188 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 188 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 188 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 188 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 188 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4474) | Acc: (93.48%) (9348/10000)\n",
      "Epoch: 189 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 189 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 189 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 189 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 189 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 189 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 189 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 189 | Batch_idx: 70 |  Loss_1: (0.0003) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 189 | Batch_idx: 80 |  Loss_1: (0.0003) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 189 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 189 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 189 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 189 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 189 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 189 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 189 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 189 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 189 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 189 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 189 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 189 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 189 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 189 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 189 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 189 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 189 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 189 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 189 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 189 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 189 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 189 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 189 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 189 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 189 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 189 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 189 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 189 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 189 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 189 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 189 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4451) | Acc: (93.26%) (9326/10000)\n",
      "Epoch: 190 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 190 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 190 | Batch_idx: 20 |  Loss_1: (0.0003) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 190 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 190 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 190 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 190 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 190 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 190 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 190 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 190 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 190 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 190 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 190 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 190 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 190 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 190 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 190 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 190 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 190 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 190 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 190 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 190 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 190 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 190 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 190 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 190 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 190 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 190 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 190 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 190 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 190 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 190 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 190 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 190 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 190 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 190 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 190 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 190 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 190 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4412) | Acc: (93.33%) (9333/10000)\n",
      "Epoch: 191 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 191 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 191 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 191 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 191 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 191 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 191 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 191 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 191 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 191 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 191 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 191 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 191 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 191 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 191 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 191 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 191 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 191 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 191 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 191 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 191 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 191 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 191 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 191 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 191 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 191 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 191 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 191 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 191 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 191 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 191 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 191 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 191 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 191 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 191 | Batch_idx: 340 |  Loss_1: (0.0001) | Acc_1: (100.00%) (43648/43648)\n",
      "Epoch: 191 | Batch_idx: 350 |  Loss_1: (0.0001) | Acc_1: (100.00%) (44928/44928)\n",
      "Epoch: 191 | Batch_idx: 360 |  Loss_1: (0.0001) | Acc_1: (100.00%) (46208/46208)\n",
      "Epoch: 191 | Batch_idx: 370 |  Loss_1: (0.0001) | Acc_1: (100.00%) (47488/47488)\n",
      "Epoch: 191 | Batch_idx: 380 |  Loss_1: (0.0001) | Acc_1: (100.00%) (48768/48768)\n",
      "Epoch: 191 | Batch_idx: 390 |  Loss_1: (0.0001) | Acc_1: (100.00%) (50000/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4399) | Acc: (93.33%) (9333/10000)\n",
      "Epoch: 192 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 192 | Batch_idx: 10 |  Loss_1: (0.0003) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 192 | Batch_idx: 20 |  Loss_1: (0.0002) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 192 | Batch_idx: 30 |  Loss_1: (0.0002) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 192 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 192 | Batch_idx: 50 |  Loss_1: (0.0005) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 192 | Batch_idx: 60 |  Loss_1: (0.0004) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 192 | Batch_idx: 70 |  Loss_1: (0.0004) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 192 | Batch_idx: 80 |  Loss_1: (0.0004) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 192 | Batch_idx: 90 |  Loss_1: (0.0003) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 192 | Batch_idx: 100 |  Loss_1: (0.0003) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 192 | Batch_idx: 110 |  Loss_1: (0.0003) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 192 | Batch_idx: 120 |  Loss_1: (0.0003) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 192 | Batch_idx: 130 |  Loss_1: (0.0003) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 192 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 192 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 192 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 192 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 192 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 192 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 192 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 192 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 192 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 192 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 192 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 192 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 192 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (99.99%) (33406/33408)\n",
      "Epoch: 192 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (99.99%) (34686/34688)\n",
      "Epoch: 192 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (99.99%) (35966/35968)\n",
      "Epoch: 192 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (99.99%) (37246/37248)\n",
      "Epoch: 192 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (99.99%) (38526/38528)\n",
      "Epoch: 192 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (99.99%) (39806/39808)\n",
      "Epoch: 192 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41086/41088)\n",
      "Epoch: 192 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42366/42368)\n",
      "Epoch: 192 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43646/43648)\n",
      "Epoch: 192 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44926/44928)\n",
      "Epoch: 192 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46206/46208)\n",
      "Epoch: 192 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47486/47488)\n",
      "Epoch: 192 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48766/48768)\n",
      "Epoch: 192 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4412) | Acc: (93.52%) (9352/10000)\n",
      "Epoch: 193 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 193 | Batch_idx: 10 |  Loss_1: (0.0000) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 193 | Batch_idx: 20 |  Loss_1: (0.0000) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 193 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 193 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 193 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 193 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 193 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 193 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 193 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 193 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 193 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 193 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 193 | Batch_idx: 130 |  Loss_1: (0.0002) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 193 | Batch_idx: 140 |  Loss_1: (0.0002) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 193 | Batch_idx: 150 |  Loss_1: (0.0002) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 193 | Batch_idx: 160 |  Loss_1: (0.0002) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 193 | Batch_idx: 170 |  Loss_1: (0.0002) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 193 | Batch_idx: 180 |  Loss_1: (0.0002) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 193 | Batch_idx: 190 |  Loss_1: (0.0002) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 193 | Batch_idx: 200 |  Loss_1: (0.0002) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 193 | Batch_idx: 210 |  Loss_1: (0.0002) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 193 | Batch_idx: 220 |  Loss_1: (0.0002) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 193 | Batch_idx: 230 |  Loss_1: (0.0002) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 193 | Batch_idx: 240 |  Loss_1: (0.0002) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 193 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 193 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 193 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 193 | Batch_idx: 280 |  Loss_1: (0.0002) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 193 | Batch_idx: 290 |  Loss_1: (0.0002) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 193 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 193 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 193 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 193 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 193 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 193 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 193 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 193 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 193 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 193 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4411) | Acc: (93.44%) (9344/10000)\n",
      "Epoch: 194 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 194 | Batch_idx: 10 |  Loss_1: (0.0000) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 194 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 194 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 194 | Batch_idx: 40 |  Loss_1: (0.0002) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 194 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 194 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 194 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 194 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 194 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 194 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 194 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 194 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 194 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 194 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 194 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 194 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 194 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 194 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 194 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 194 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 194 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 194 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 194 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 194 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 194 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 194 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 194 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 194 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 194 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 194 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 194 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 194 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 194 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 194 | Batch_idx: 340 |  Loss_1: (0.0001) | Acc_1: (100.00%) (43648/43648)\n",
      "Epoch: 194 | Batch_idx: 350 |  Loss_1: (0.0001) | Acc_1: (100.00%) (44928/44928)\n",
      "Epoch: 194 | Batch_idx: 360 |  Loss_1: (0.0001) | Acc_1: (100.00%) (46208/46208)\n",
      "Epoch: 194 | Batch_idx: 370 |  Loss_1: (0.0001) | Acc_1: (100.00%) (47488/47488)\n",
      "Epoch: 194 | Batch_idx: 380 |  Loss_1: (0.0001) | Acc_1: (100.00%) (48768/48768)\n",
      "Epoch: 194 | Batch_idx: 390 |  Loss_1: (0.0001) | Acc_1: (100.00%) (50000/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4429) | Acc: (93.38%) (9338/10000)\n",
      "Epoch: 195 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 195 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 195 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 195 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 195 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 195 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 195 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 195 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 195 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 195 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 195 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 195 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 195 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 195 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 195 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 195 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 195 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 195 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 195 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 195 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 195 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 195 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 195 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 195 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 195 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 195 | Batch_idx: 250 |  Loss_1: (0.0002) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 195 | Batch_idx: 260 |  Loss_1: (0.0002) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 195 | Batch_idx: 270 |  Loss_1: (0.0002) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 195 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 195 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 195 | Batch_idx: 300 |  Loss_1: (0.0002) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 195 | Batch_idx: 310 |  Loss_1: (0.0002) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 195 | Batch_idx: 320 |  Loss_1: (0.0002) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 195 | Batch_idx: 330 |  Loss_1: (0.0002) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 195 | Batch_idx: 340 |  Loss_1: (0.0002) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 195 | Batch_idx: 350 |  Loss_1: (0.0002) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 195 | Batch_idx: 360 |  Loss_1: (0.0002) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 195 | Batch_idx: 370 |  Loss_1: (0.0002) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 195 | Batch_idx: 380 |  Loss_1: (0.0002) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 195 | Batch_idx: 390 |  Loss_1: (0.0002) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4413) | Acc: (93.33%) (9333/10000)\n",
      "Epoch: 196 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 196 | Batch_idx: 10 |  Loss_1: (0.0000) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 196 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 196 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 196 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 196 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 196 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 196 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 196 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 196 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 196 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 196 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 196 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 196 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 196 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 196 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 196 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 196 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 196 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 196 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 196 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 196 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 196 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 196 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 196 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 196 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 196 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 196 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 196 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 196 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 196 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 196 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 196 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 196 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 196 | Batch_idx: 340 |  Loss_1: (0.0001) | Acc_1: (100.00%) (43648/43648)\n",
      "Epoch: 196 | Batch_idx: 350 |  Loss_1: (0.0001) | Acc_1: (100.00%) (44928/44928)\n",
      "Epoch: 196 | Batch_idx: 360 |  Loss_1: (0.0001) | Acc_1: (100.00%) (46208/46208)\n",
      "Epoch: 196 | Batch_idx: 370 |  Loss_1: (0.0001) | Acc_1: (100.00%) (47488/47488)\n",
      "Epoch: 196 | Batch_idx: 380 |  Loss_1: (0.0001) | Acc_1: (100.00%) (48768/48768)\n",
      "Epoch: 196 | Batch_idx: 390 |  Loss_1: (0.0001) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4397) | Acc: (93.44%) (9344/10000)\n",
      "Epoch: 197 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 197 | Batch_idx: 10 |  Loss_1: (0.0002) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 197 | Batch_idx: 20 |  Loss_1: (0.0002) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 197 | Batch_idx: 30 |  Loss_1: (0.0001) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 197 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 197 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 197 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 197 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 197 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 197 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 197 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 197 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 197 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 197 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 197 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 197 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 197 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 197 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 197 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 197 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 197 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 197 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 197 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 197 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 197 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 197 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 197 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 197 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 197 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 197 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 197 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 197 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 197 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 197 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 197 | Batch_idx: 340 |  Loss_1: (0.0001) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 197 | Batch_idx: 350 |  Loss_1: (0.0001) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 197 | Batch_idx: 360 |  Loss_1: (0.0001) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 197 | Batch_idx: 370 |  Loss_1: (0.0001) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 197 | Batch_idx: 380 |  Loss_1: (0.0001) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 197 | Batch_idx: 390 |  Loss_1: (0.0001) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4413) | Acc: (93.44%) (9344/10000)\n",
      "Epoch: 198 | Batch_idx: 0 |  Loss_1: (0.0005) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 198 | Batch_idx: 10 |  Loss_1: (0.0001) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 198 | Batch_idx: 20 |  Loss_1: (0.0001) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 198 | Batch_idx: 30 |  Loss_1: (0.0003) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 198 | Batch_idx: 40 |  Loss_1: (0.0003) | Acc_1: (99.98%) (5247/5248)\n",
      "Epoch: 198 | Batch_idx: 50 |  Loss_1: (0.0002) | Acc_1: (99.98%) (6527/6528)\n",
      "Epoch: 198 | Batch_idx: 60 |  Loss_1: (0.0002) | Acc_1: (99.99%) (7807/7808)\n",
      "Epoch: 198 | Batch_idx: 70 |  Loss_1: (0.0002) | Acc_1: (99.99%) (9087/9088)\n",
      "Epoch: 198 | Batch_idx: 80 |  Loss_1: (0.0002) | Acc_1: (99.99%) (10367/10368)\n",
      "Epoch: 198 | Batch_idx: 90 |  Loss_1: (0.0002) | Acc_1: (99.99%) (11647/11648)\n",
      "Epoch: 198 | Batch_idx: 100 |  Loss_1: (0.0002) | Acc_1: (99.99%) (12927/12928)\n",
      "Epoch: 198 | Batch_idx: 110 |  Loss_1: (0.0002) | Acc_1: (99.99%) (14207/14208)\n",
      "Epoch: 198 | Batch_idx: 120 |  Loss_1: (0.0002) | Acc_1: (99.99%) (15487/15488)\n",
      "Epoch: 198 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (99.99%) (16767/16768)\n",
      "Epoch: 198 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (99.99%) (18047/18048)\n",
      "Epoch: 198 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (99.99%) (19327/19328)\n",
      "Epoch: 198 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20607/20608)\n",
      "Epoch: 198 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21887/21888)\n",
      "Epoch: 198 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23167/23168)\n",
      "Epoch: 198 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24447/24448)\n",
      "Epoch: 198 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25727/25728)\n",
      "Epoch: 198 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27007/27008)\n",
      "Epoch: 198 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28287/28288)\n",
      "Epoch: 198 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29567/29568)\n",
      "Epoch: 198 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30847/30848)\n",
      "Epoch: 198 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32127/32128)\n",
      "Epoch: 198 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33407/33408)\n",
      "Epoch: 198 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34687/34688)\n",
      "Epoch: 198 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35967/35968)\n",
      "Epoch: 198 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37247/37248)\n",
      "Epoch: 198 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38527/38528)\n",
      "Epoch: 198 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39807/39808)\n",
      "Epoch: 198 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41087/41088)\n",
      "Epoch: 198 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42367/42368)\n",
      "Epoch: 198 | Batch_idx: 340 |  Loss_1: (0.0001) | Acc_1: (100.00%) (43647/43648)\n",
      "Epoch: 198 | Batch_idx: 350 |  Loss_1: (0.0001) | Acc_1: (100.00%) (44927/44928)\n",
      "Epoch: 198 | Batch_idx: 360 |  Loss_1: (0.0001) | Acc_1: (100.00%) (46207/46208)\n",
      "Epoch: 198 | Batch_idx: 370 |  Loss_1: (0.0001) | Acc_1: (100.00%) (47487/47488)\n",
      "Epoch: 198 | Batch_idx: 380 |  Loss_1: (0.0001) | Acc_1: (100.00%) (48767/48768)\n",
      "Epoch: 198 | Batch_idx: 390 |  Loss_1: (0.0001) | Acc_1: (100.00%) (49999/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4439) | Acc: (93.49%) (9349/10000)\n",
      "Epoch: 199 | Batch_idx: 0 |  Loss_1: (0.0000) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 199 | Batch_idx: 10 |  Loss_1: (0.0000) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 199 | Batch_idx: 20 |  Loss_1: (0.0000) | Acc_1: (100.00%) (2688/2688)\n",
      "Epoch: 199 | Batch_idx: 30 |  Loss_1: (0.0000) | Acc_1: (100.00%) (3968/3968)\n",
      "Epoch: 199 | Batch_idx: 40 |  Loss_1: (0.0001) | Acc_1: (100.00%) (5248/5248)\n",
      "Epoch: 199 | Batch_idx: 50 |  Loss_1: (0.0001) | Acc_1: (100.00%) (6528/6528)\n",
      "Epoch: 199 | Batch_idx: 60 |  Loss_1: (0.0001) | Acc_1: (100.00%) (7808/7808)\n",
      "Epoch: 199 | Batch_idx: 70 |  Loss_1: (0.0001) | Acc_1: (100.00%) (9088/9088)\n",
      "Epoch: 199 | Batch_idx: 80 |  Loss_1: (0.0001) | Acc_1: (100.00%) (10368/10368)\n",
      "Epoch: 199 | Batch_idx: 90 |  Loss_1: (0.0001) | Acc_1: (100.00%) (11648/11648)\n",
      "Epoch: 199 | Batch_idx: 100 |  Loss_1: (0.0001) | Acc_1: (100.00%) (12928/12928)\n",
      "Epoch: 199 | Batch_idx: 110 |  Loss_1: (0.0001) | Acc_1: (100.00%) (14208/14208)\n",
      "Epoch: 199 | Batch_idx: 120 |  Loss_1: (0.0001) | Acc_1: (100.00%) (15488/15488)\n",
      "Epoch: 199 | Batch_idx: 130 |  Loss_1: (0.0001) | Acc_1: (100.00%) (16768/16768)\n",
      "Epoch: 199 | Batch_idx: 140 |  Loss_1: (0.0001) | Acc_1: (100.00%) (18048/18048)\n",
      "Epoch: 199 | Batch_idx: 150 |  Loss_1: (0.0001) | Acc_1: (100.00%) (19328/19328)\n",
      "Epoch: 199 | Batch_idx: 160 |  Loss_1: (0.0001) | Acc_1: (100.00%) (20608/20608)\n",
      "Epoch: 199 | Batch_idx: 170 |  Loss_1: (0.0001) | Acc_1: (100.00%) (21888/21888)\n",
      "Epoch: 199 | Batch_idx: 180 |  Loss_1: (0.0001) | Acc_1: (100.00%) (23168/23168)\n",
      "Epoch: 199 | Batch_idx: 190 |  Loss_1: (0.0001) | Acc_1: (100.00%) (24448/24448)\n",
      "Epoch: 199 | Batch_idx: 200 |  Loss_1: (0.0001) | Acc_1: (100.00%) (25728/25728)\n",
      "Epoch: 199 | Batch_idx: 210 |  Loss_1: (0.0001) | Acc_1: (100.00%) (27008/27008)\n",
      "Epoch: 199 | Batch_idx: 220 |  Loss_1: (0.0001) | Acc_1: (100.00%) (28288/28288)\n",
      "Epoch: 199 | Batch_idx: 230 |  Loss_1: (0.0001) | Acc_1: (100.00%) (29568/29568)\n",
      "Epoch: 199 | Batch_idx: 240 |  Loss_1: (0.0001) | Acc_1: (100.00%) (30848/30848)\n",
      "Epoch: 199 | Batch_idx: 250 |  Loss_1: (0.0001) | Acc_1: (100.00%) (32128/32128)\n",
      "Epoch: 199 | Batch_idx: 260 |  Loss_1: (0.0001) | Acc_1: (100.00%) (33408/33408)\n",
      "Epoch: 199 | Batch_idx: 270 |  Loss_1: (0.0001) | Acc_1: (100.00%) (34688/34688)\n",
      "Epoch: 199 | Batch_idx: 280 |  Loss_1: (0.0001) | Acc_1: (100.00%) (35968/35968)\n",
      "Epoch: 199 | Batch_idx: 290 |  Loss_1: (0.0001) | Acc_1: (100.00%) (37248/37248)\n",
      "Epoch: 199 | Batch_idx: 300 |  Loss_1: (0.0001) | Acc_1: (100.00%) (38528/38528)\n",
      "Epoch: 199 | Batch_idx: 310 |  Loss_1: (0.0001) | Acc_1: (100.00%) (39808/39808)\n",
      "Epoch: 199 | Batch_idx: 320 |  Loss_1: (0.0001) | Acc_1: (100.00%) (41088/41088)\n",
      "Epoch: 199 | Batch_idx: 330 |  Loss_1: (0.0001) | Acc_1: (100.00%) (42368/42368)\n",
      "Epoch: 199 | Batch_idx: 340 |  Loss_1: (0.0001) | Acc_1: (100.00%) (43648/43648)\n",
      "Epoch: 199 | Batch_idx: 350 |  Loss_1: (0.0001) | Acc_1: (100.00%) (44928/44928)\n",
      "Epoch: 199 | Batch_idx: 360 |  Loss_1: (0.0001) | Acc_1: (100.00%) (46208/46208)\n",
      "Epoch: 199 | Batch_idx: 370 |  Loss_1: (0.0001) | Acc_1: (100.00%) (47488/47488)\n",
      "Epoch: 199 | Batch_idx: 380 |  Loss_1: (0.0001) | Acc_1: (100.00%) (48768/48768)\n",
      "Epoch: 199 | Batch_idx: 390 |  Loss_1: (0.0001) | Acc_1: (100.00%) (50000/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4364) | Acc: (93.44%) (9344/10000)\n",
      "3 hours 40 mins 11 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet_50_cosine.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 200):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet_50_cosine.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8766a17f2584f17ae9875767170f3464b2a051bfe2b6423fb227ac503acbc200"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hw2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
