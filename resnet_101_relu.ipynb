{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.006\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet_101_relu') #!#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformer_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ColorJitter(brightness=(0.2, 2), contrast=(0.3, 2), saturation=(0.2, 2), hue=(-0.3, 0.3)),\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "validation_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlock2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, drop_prob, block_size):\n",
    "        super(DropBlock2D, self).__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        assert x.dim() == 4, \\\n",
    "            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n",
    "\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        else:\n",
    "            # get gamma value\n",
    "            gamma = self._compute_gamma(x)\n",
    "\n",
    "            # sample mask\n",
    "            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n",
    "\n",
    "            # place mask on input device\n",
    "            mask = mask.to(x.device)\n",
    "\n",
    "            # compute block mask\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "\n",
    "            # apply block mask\n",
    "            out = x * block_mask[:, None, :, :]\n",
    "\n",
    "            # scale output\n",
    "            out = out * block_mask.numel() / block_mask.sum()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n",
    "                                  kernel_size=(self.block_size, self.block_size),\n",
    "                                  stride=(1, 1),\n",
    "                                  padding=self.block_size // 2)\n",
    "\n",
    "        if self.block_size % 2 == 0:\n",
    "            block_mask = block_mask[:, :, :-1, :-1]\n",
    "\n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "\n",
    "        return block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob / (self.block_size ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(nn.Module):\n",
    "    def __init__(self, dropblock, start_value, stop_value, nr_steps):\n",
    "        super(LinearScheduler, self).__init__()\n",
    "        self.dropblock = dropblock\n",
    "        self.i = 0\n",
    "        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=int(nr_steps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropblock(x)\n",
    "\n",
    "    def step(self):\n",
    "        if self.i < len(self.drop_values):\n",
    "            self.dropblock.drop_prob = self.drop_values[self.i]\n",
    "\n",
    "        self.i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, start_prob, stop_prob, block_size, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        self.start_prob = start_prob\n",
    "        self.stop_prob = stop_prob\n",
    "        self.block_size = block_size\n",
    "        self.dropblock = LinearScheduler(DropBlock2D(drop_prob=self.start_prob, block_size=self.block_size), start_value=self.start_prob, stop_value=self.start_prob, nr_steps=60000)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropblock.step()\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.dropblock(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.dropblock(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BottleNeck, [3, 4, 23, 3], 0.6, 0.8, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 병렬연산 사용 유무 및 GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 1 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
      "             ReLU-13          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-14          [-1, 256, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "           Conv2d-18           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
      "             ReLU-20           [-1, 64, 32, 32]               0\n",
      "           Conv2d-21          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-22          [-1, 256, 32, 32]             512\n",
      "             ReLU-23          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
      "             ReLU-27           [-1, 64, 32, 32]               0\n",
      "           Conv2d-28           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 32, 32]             128\n",
      "             ReLU-30           [-1, 64, 32, 32]               0\n",
      "           Conv2d-31          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
      "             ReLU-33          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "             ReLU-40          [-1, 128, 16, 16]               0\n",
      "           Conv2d-41          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-42          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-43          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-45          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-46          [-1, 512, 16, 16]               0\n",
      "           Conv2d-47          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-48          [-1, 128, 16, 16]             256\n",
      "             ReLU-49          [-1, 128, 16, 16]               0\n",
      "           Conv2d-50          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-51          [-1, 128, 16, 16]             256\n",
      "             ReLU-52          [-1, 128, 16, 16]               0\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-55          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-56          [-1, 512, 16, 16]               0\n",
      "           Conv2d-57          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-58          [-1, 128, 16, 16]             256\n",
      "             ReLU-59          [-1, 128, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-66          [-1, 512, 16, 16]               0\n",
      "           Conv2d-67          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-68          [-1, 128, 16, 16]             256\n",
      "             ReLU-69          [-1, 128, 16, 16]               0\n",
      "           Conv2d-70          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-71          [-1, 128, 16, 16]             256\n",
      "             ReLU-72          [-1, 128, 16, 16]               0\n",
      "           Conv2d-73          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-74          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-75          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-76          [-1, 512, 16, 16]               0\n",
      "      DropBlock2D-77          [-1, 512, 16, 16]               0\n",
      "  LinearScheduler-78          [-1, 512, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
      "             ReLU-81          [-1, 256, 16, 16]               0\n",
      "           Conv2d-82            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 8, 8]             512\n",
      "             ReLU-84            [-1, 256, 8, 8]               0\n",
      "           Conv2d-85           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-87           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-89           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-90           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-91            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 8, 8]             512\n",
      "             ReLU-93            [-1, 256, 8, 8]               0\n",
      "           Conv2d-94            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 8, 8]             512\n",
      "             ReLU-96            [-1, 256, 8, 8]               0\n",
      "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-99           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-100           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-101            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 8, 8]             512\n",
      "            ReLU-103            [-1, 256, 8, 8]               0\n",
      "          Conv2d-104            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 8, 8]             512\n",
      "            ReLU-106            [-1, 256, 8, 8]               0\n",
      "          Conv2d-107           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-109           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-110           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-111            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 8, 8]             512\n",
      "            ReLU-113            [-1, 256, 8, 8]               0\n",
      "          Conv2d-114            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 8, 8]             512\n",
      "            ReLU-116            [-1, 256, 8, 8]               0\n",
      "          Conv2d-117           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-119           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-120           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-121            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 8, 8]             512\n",
      "            ReLU-123            [-1, 256, 8, 8]               0\n",
      "          Conv2d-124            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 8, 8]             512\n",
      "            ReLU-126            [-1, 256, 8, 8]               0\n",
      "          Conv2d-127           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-129           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-130           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-131            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 8, 8]             512\n",
      "            ReLU-133            [-1, 256, 8, 8]               0\n",
      "          Conv2d-134            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 8, 8]             512\n",
      "            ReLU-136            [-1, 256, 8, 8]               0\n",
      "          Conv2d-137           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-139           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-140           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-141            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-142            [-1, 256, 8, 8]             512\n",
      "            ReLU-143            [-1, 256, 8, 8]               0\n",
      "          Conv2d-144            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-145            [-1, 256, 8, 8]             512\n",
      "            ReLU-146            [-1, 256, 8, 8]               0\n",
      "          Conv2d-147           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-148           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-149           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-150           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-151            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-152            [-1, 256, 8, 8]             512\n",
      "            ReLU-153            [-1, 256, 8, 8]               0\n",
      "          Conv2d-154            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-155            [-1, 256, 8, 8]             512\n",
      "            ReLU-156            [-1, 256, 8, 8]               0\n",
      "          Conv2d-157           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-158           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-159           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-160           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-161            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-162            [-1, 256, 8, 8]             512\n",
      "            ReLU-163            [-1, 256, 8, 8]               0\n",
      "          Conv2d-164            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-165            [-1, 256, 8, 8]             512\n",
      "            ReLU-166            [-1, 256, 8, 8]               0\n",
      "          Conv2d-167           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-168           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-169           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-170           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-171            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-172            [-1, 256, 8, 8]             512\n",
      "            ReLU-173            [-1, 256, 8, 8]               0\n",
      "          Conv2d-174            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-175            [-1, 256, 8, 8]             512\n",
      "            ReLU-176            [-1, 256, 8, 8]               0\n",
      "          Conv2d-177           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-178           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-179           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-180           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-181            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-182            [-1, 256, 8, 8]             512\n",
      "            ReLU-183            [-1, 256, 8, 8]               0\n",
      "          Conv2d-184            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-185            [-1, 256, 8, 8]             512\n",
      "            ReLU-186            [-1, 256, 8, 8]               0\n",
      "          Conv2d-187           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-188           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-189           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-190           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-191            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-192            [-1, 256, 8, 8]             512\n",
      "            ReLU-193            [-1, 256, 8, 8]               0\n",
      "          Conv2d-194            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-195            [-1, 256, 8, 8]             512\n",
      "            ReLU-196            [-1, 256, 8, 8]               0\n",
      "          Conv2d-197           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-198           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-199           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-200           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-201            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-202            [-1, 256, 8, 8]             512\n",
      "            ReLU-203            [-1, 256, 8, 8]               0\n",
      "          Conv2d-204            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-205            [-1, 256, 8, 8]             512\n",
      "            ReLU-206            [-1, 256, 8, 8]               0\n",
      "          Conv2d-207           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-208           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-209           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-210           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-211            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-212            [-1, 256, 8, 8]             512\n",
      "            ReLU-213            [-1, 256, 8, 8]               0\n",
      "          Conv2d-214            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-215            [-1, 256, 8, 8]             512\n",
      "            ReLU-216            [-1, 256, 8, 8]               0\n",
      "          Conv2d-217           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-218           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-219           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-220           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-221            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-222            [-1, 256, 8, 8]             512\n",
      "            ReLU-223            [-1, 256, 8, 8]               0\n",
      "          Conv2d-224            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-225            [-1, 256, 8, 8]             512\n",
      "            ReLU-226            [-1, 256, 8, 8]               0\n",
      "          Conv2d-227           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-228           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-229           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-230           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-231            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-232            [-1, 256, 8, 8]             512\n",
      "            ReLU-233            [-1, 256, 8, 8]               0\n",
      "          Conv2d-234            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-235            [-1, 256, 8, 8]             512\n",
      "            ReLU-236            [-1, 256, 8, 8]               0\n",
      "          Conv2d-237           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-238           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-239           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-240           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-241            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-242            [-1, 256, 8, 8]             512\n",
      "            ReLU-243            [-1, 256, 8, 8]               0\n",
      "          Conv2d-244            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-245            [-1, 256, 8, 8]             512\n",
      "            ReLU-246            [-1, 256, 8, 8]               0\n",
      "          Conv2d-247           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-248           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-249           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-250           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-251            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-252            [-1, 256, 8, 8]             512\n",
      "            ReLU-253            [-1, 256, 8, 8]               0\n",
      "          Conv2d-254            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-255            [-1, 256, 8, 8]             512\n",
      "            ReLU-256            [-1, 256, 8, 8]               0\n",
      "          Conv2d-257           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-258           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-259           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-260           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-261            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-262            [-1, 256, 8, 8]             512\n",
      "            ReLU-263            [-1, 256, 8, 8]               0\n",
      "          Conv2d-264            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-265            [-1, 256, 8, 8]             512\n",
      "            ReLU-266            [-1, 256, 8, 8]               0\n",
      "          Conv2d-267           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-268           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-269           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-270           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-271            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-272            [-1, 256, 8, 8]             512\n",
      "            ReLU-273            [-1, 256, 8, 8]               0\n",
      "          Conv2d-274            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-275            [-1, 256, 8, 8]             512\n",
      "            ReLU-276            [-1, 256, 8, 8]               0\n",
      "          Conv2d-277           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-278           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-279           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-280           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-281            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-282            [-1, 256, 8, 8]             512\n",
      "            ReLU-283            [-1, 256, 8, 8]               0\n",
      "          Conv2d-284            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-285            [-1, 256, 8, 8]             512\n",
      "            ReLU-286            [-1, 256, 8, 8]               0\n",
      "          Conv2d-287           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-288           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-289           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-290           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-291            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-292            [-1, 256, 8, 8]             512\n",
      "            ReLU-293            [-1, 256, 8, 8]               0\n",
      "          Conv2d-294            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-295            [-1, 256, 8, 8]             512\n",
      "            ReLU-296            [-1, 256, 8, 8]               0\n",
      "          Conv2d-297           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-298           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-299           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-300           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-301            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-302            [-1, 256, 8, 8]             512\n",
      "            ReLU-303            [-1, 256, 8, 8]               0\n",
      "          Conv2d-304            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-305            [-1, 256, 8, 8]             512\n",
      "            ReLU-306            [-1, 256, 8, 8]               0\n",
      "          Conv2d-307           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-308           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-309           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-310           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-311            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-312            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-313            [-1, 512, 8, 8]               0\n",
      "          Conv2d-314            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-316            [-1, 512, 4, 4]               0\n",
      "          Conv2d-317           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-319           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-321           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-322           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-323            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-325            [-1, 512, 4, 4]               0\n",
      "          Conv2d-326            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-328            [-1, 512, 4, 4]               0\n",
      "          Conv2d-329           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-331           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-332           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-333            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-335            [-1, 512, 4, 4]               0\n",
      "          Conv2d-336            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-338            [-1, 512, 4, 4]               0\n",
      "          Conv2d-339           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-341           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-342           [-1, 2048, 4, 4]               0\n",
      "     DropBlock2D-343           [-1, 2048, 4, 4]               0\n",
      " LinearScheduler-344           [-1, 2048, 4, 4]               0\n",
      "          Linear-345                   [-1, 10]          20,490\n",
      "          ResNet-346                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 42,512,970\n",
      "Trainable params: 42,512,970\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 137.31\n",
      "Params size (MB): 162.17\n",
      "Estimated Total Size (MB): 299.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Scheduler & Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(training_loader)\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(training_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(validation_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(validation_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheckPoint (model save and model load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (2.4877) | Acc_1: (8.59%) (11/128)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (3.4945) | Acc_1: (9.45%) (133/1408)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (3.1473) | Acc_1: (9.64%) (259/2688)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (2.9713) | Acc_1: (9.85%) (391/3968)\n",
      "Epoch: 0 | Batch_idx: 40 |  Loss_1: (2.8391) | Acc_1: (9.81%) (515/5248)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss_1: (2.7562) | Acc_1: (9.87%) (644/6528)\n",
      "Epoch: 0 | Batch_idx: 60 |  Loss_1: (2.6978) | Acc_1: (9.91%) (774/7808)\n",
      "Epoch: 0 | Batch_idx: 70 |  Loss_1: (2.6531) | Acc_1: (10.09%) (917/9088)\n",
      "Epoch: 0 | Batch_idx: 80 |  Loss_1: (2.6181) | Acc_1: (10.50%) (1089/10368)\n",
      "Epoch: 0 | Batch_idx: 90 |  Loss_1: (2.5885) | Acc_1: (10.60%) (1235/11648)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss_1: (2.5631) | Acc_1: (11.05%) (1428/12928)\n",
      "Epoch: 0 | Batch_idx: 110 |  Loss_1: (2.5437) | Acc_1: (11.18%) (1588/14208)\n",
      "Epoch: 0 | Batch_idx: 120 |  Loss_1: (2.5291) | Acc_1: (11.43%) (1770/15488)\n",
      "Epoch: 0 | Batch_idx: 130 |  Loss_1: (2.5147) | Acc_1: (11.54%) (1935/16768)\n",
      "Epoch: 0 | Batch_idx: 140 |  Loss_1: (2.5035) | Acc_1: (11.59%) (2092/18048)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss_1: (2.4906) | Acc_1: (11.68%) (2257/19328)\n",
      "Epoch: 0 | Batch_idx: 160 |  Loss_1: (2.4777) | Acc_1: (11.85%) (2442/20608)\n",
      "Epoch: 0 | Batch_idx: 170 |  Loss_1: (2.4672) | Acc_1: (12.01%) (2629/21888)\n",
      "Epoch: 0 | Batch_idx: 180 |  Loss_1: (2.4583) | Acc_1: (12.10%) (2804/23168)\n",
      "Epoch: 0 | Batch_idx: 190 |  Loss_1: (2.4488) | Acc_1: (12.26%) (2998/24448)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss_1: (2.4397) | Acc_1: (12.47%) (3209/25728)\n",
      "Epoch: 0 | Batch_idx: 210 |  Loss_1: (2.4304) | Acc_1: (12.73%) (3437/27008)\n",
      "Epoch: 0 | Batch_idx: 220 |  Loss_1: (2.4229) | Acc_1: (12.82%) (3626/28288)\n",
      "Epoch: 0 | Batch_idx: 230 |  Loss_1: (2.4154) | Acc_1: (12.98%) (3837/29568)\n",
      "Epoch: 0 | Batch_idx: 240 |  Loss_1: (2.4088) | Acc_1: (13.08%) (4034/30848)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss_1: (2.4021) | Acc_1: (13.31%) (4275/32128)\n",
      "Epoch: 0 | Batch_idx: 260 |  Loss_1: (2.3958) | Acc_1: (13.44%) (4490/33408)\n",
      "Epoch: 0 | Batch_idx: 270 |  Loss_1: (2.3881) | Acc_1: (13.65%) (4736/34688)\n",
      "Epoch: 0 | Batch_idx: 280 |  Loss_1: (2.3815) | Acc_1: (13.88%) (4991/35968)\n",
      "Epoch: 0 | Batch_idx: 290 |  Loss_1: (2.3752) | Acc_1: (14.07%) (5241/37248)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss_1: (2.3697) | Acc_1: (14.20%) (5470/38528)\n",
      "Epoch: 0 | Batch_idx: 310 |  Loss_1: (2.3649) | Acc_1: (14.32%) (5699/39808)\n",
      "Epoch: 0 | Batch_idx: 320 |  Loss_1: (2.3591) | Acc_1: (14.48%) (5949/41088)\n",
      "Epoch: 0 | Batch_idx: 330 |  Loss_1: (2.3541) | Acc_1: (14.58%) (6179/42368)\n",
      "Epoch: 0 | Batch_idx: 340 |  Loss_1: (2.3501) | Acc_1: (14.65%) (6396/43648)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss_1: (2.3448) | Acc_1: (14.79%) (6647/44928)\n",
      "Epoch: 0 | Batch_idx: 360 |  Loss_1: (2.3396) | Acc_1: (14.96%) (6915/46208)\n",
      "Epoch: 0 | Batch_idx: 370 |  Loss_1: (2.3354) | Acc_1: (15.12%) (7181/47488)\n",
      "Epoch: 0 | Batch_idx: 380 |  Loss_1: (2.3303) | Acc_1: (15.29%) (7456/48768)\n",
      "Epoch: 0 | Batch_idx: 390 |  Loss_1: (2.3258) | Acc_1: (15.43%) (7713/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.0600) | Acc: (20.42%) (2042/10000)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (2.0985) | Acc_1: (21.09%) (27/128)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (2.1273) | Acc_1: (21.52%) (303/1408)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (2.1316) | Acc_1: (21.99%) (591/2688)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (2.1320) | Acc_1: (21.62%) (858/3968)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss_1: (2.1304) | Acc_1: (21.89%) (1149/5248)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss_1: (2.1289) | Acc_1: (21.66%) (1414/6528)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss_1: (2.1241) | Acc_1: (21.89%) (1709/7808)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss_1: (2.1196) | Acc_1: (22.02%) (2001/9088)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss_1: (2.1210) | Acc_1: (22.14%) (2295/10368)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss_1: (2.1195) | Acc_1: (21.96%) (2558/11648)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss_1: (2.1180) | Acc_1: (22.07%) (2853/12928)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss_1: (2.1180) | Acc_1: (22.17%) (3150/14208)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss_1: (2.1150) | Acc_1: (22.17%) (3434/15488)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss_1: (2.1129) | Acc_1: (22.28%) (3736/16768)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss_1: (2.1113) | Acc_1: (22.22%) (4011/18048)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss_1: (2.1082) | Acc_1: (22.34%) (4317/19328)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss_1: (2.1023) | Acc_1: (22.52%) (4641/20608)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss_1: (2.0995) | Acc_1: (22.72%) (4973/21888)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss_1: (2.0967) | Acc_1: (22.82%) (5286/23168)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss_1: (2.0938) | Acc_1: (22.83%) (5582/24448)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss_1: (2.0901) | Acc_1: (22.85%) (5879/25728)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss_1: (2.0871) | Acc_1: (22.94%) (6195/27008)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss_1: (2.0859) | Acc_1: (22.99%) (6502/28288)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss_1: (2.0839) | Acc_1: (23.13%) (6840/29568)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss_1: (2.0811) | Acc_1: (23.17%) (7149/30848)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss_1: (2.0777) | Acc_1: (23.23%) (7463/32128)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss_1: (2.0748) | Acc_1: (23.33%) (7794/33408)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss_1: (2.0738) | Acc_1: (23.36%) (8102/34688)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss_1: (2.0713) | Acc_1: (23.43%) (8428/35968)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss_1: (2.0686) | Acc_1: (23.55%) (8771/37248)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss_1: (2.0659) | Acc_1: (23.66%) (9116/38528)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss_1: (2.0651) | Acc_1: (23.67%) (9424/39808)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss_1: (2.0615) | Acc_1: (23.81%) (9781/41088)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss_1: (2.0583) | Acc_1: (23.92%) (10134/42368)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss_1: (2.0566) | Acc_1: (23.97%) (10461/43648)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss_1: (2.0546) | Acc_1: (24.02%) (10792/44928)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss_1: (2.0532) | Acc_1: (24.07%) (11124/46208)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss_1: (2.0510) | Acc_1: (24.16%) (11474/47488)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss_1: (2.0495) | Acc_1: (24.20%) (11804/48768)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss_1: (2.0476) | Acc_1: (24.27%) (12133/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.0211) | Acc: (23.17%) (2317/10000)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.9553) | Acc_1: (33.59%) (43/128)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.9595) | Acc_1: (28.84%) (406/1408)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.9663) | Acc_1: (28.87%) (776/2688)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.9634) | Acc_1: (28.40%) (1127/3968)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss_1: (1.9710) | Acc_1: (28.11%) (1475/5248)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss_1: (1.9635) | Acc_1: (28.05%) (1831/6528)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss_1: (1.9611) | Acc_1: (27.93%) (2181/7808)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss_1: (1.9609) | Acc_1: (27.95%) (2540/9088)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss_1: (1.9572) | Acc_1: (28.08%) (2911/10368)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss_1: (1.9536) | Acc_1: (27.98%) (3259/11648)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss_1: (1.9536) | Acc_1: (27.95%) (3614/12928)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss_1: (1.9515) | Acc_1: (28.04%) (3984/14208)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss_1: (1.9472) | Acc_1: (28.17%) (4363/15488)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss_1: (1.9438) | Acc_1: (28.26%) (4738/16768)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss_1: (1.9422) | Acc_1: (28.40%) (5126/18048)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss_1: (1.9423) | Acc_1: (28.42%) (5493/19328)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss_1: (1.9390) | Acc_1: (28.54%) (5882/20608)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss_1: (1.9370) | Acc_1: (28.55%) (6249/21888)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss_1: (1.9360) | Acc_1: (28.59%) (6623/23168)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss_1: (1.9330) | Acc_1: (28.72%) (7022/24448)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss_1: (1.9299) | Acc_1: (28.80%) (7410/25728)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss_1: (1.9284) | Acc_1: (28.83%) (7786/27008)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss_1: (1.9270) | Acc_1: (28.91%) (8178/28288)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss_1: (1.9273) | Acc_1: (28.92%) (8550/29568)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss_1: (1.9241) | Acc_1: (29.07%) (8968/30848)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss_1: (1.9234) | Acc_1: (29.12%) (9356/32128)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss_1: (1.9225) | Acc_1: (29.24%) (9770/33408)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss_1: (1.9186) | Acc_1: (29.32%) (10170/34688)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss_1: (1.9158) | Acc_1: (29.47%) (10601/35968)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss_1: (1.9146) | Acc_1: (29.54%) (11004/37248)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss_1: (1.9116) | Acc_1: (29.64%) (11419/38528)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss_1: (1.9091) | Acc_1: (29.75%) (11841/39808)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss_1: (1.9066) | Acc_1: (29.83%) (12257/41088)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss_1: (1.9047) | Acc_1: (29.95%) (12688/42368)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss_1: (1.9046) | Acc_1: (29.95%) (13072/43648)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss_1: (1.9027) | Acc_1: (30.03%) (13492/44928)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss_1: (1.8996) | Acc_1: (30.13%) (13924/46208)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss_1: (1.8975) | Acc_1: (30.20%) (14340/47488)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss_1: (1.8925) | Acc_1: (30.42%) (14833/48768)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss_1: (1.8903) | Acc_1: (30.50%) (15248/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.0785) | Acc: (25.62%) (2562/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.7265) | Acc_1: (37.50%) (48/128)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.7965) | Acc_1: (33.52%) (472/1408)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.7774) | Acc_1: (33.30%) (895/2688)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.7931) | Acc_1: (33.19%) (1317/3968)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss_1: (1.8083) | Acc_1: (32.85%) (1724/5248)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss_1: (1.8105) | Acc_1: (33.04%) (2157/6528)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss_1: (1.8060) | Acc_1: (33.52%) (2617/7808)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss_1: (1.8026) | Acc_1: (33.65%) (3058/9088)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss_1: (1.7985) | Acc_1: (33.99%) (3524/10368)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss_1: (1.7966) | Acc_1: (34.08%) (3970/11648)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss_1: (1.7967) | Acc_1: (33.93%) (4386/12928)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss_1: (1.7944) | Acc_1: (33.92%) (4819/14208)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss_1: (1.7899) | Acc_1: (34.07%) (5277/15488)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss_1: (1.7888) | Acc_1: (34.21%) (5736/16768)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss_1: (1.7861) | Acc_1: (34.47%) (6221/18048)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss_1: (1.7819) | Acc_1: (34.70%) (6706/19328)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss_1: (1.7816) | Acc_1: (34.73%) (7157/20608)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss_1: (1.7789) | Acc_1: (34.95%) (7650/21888)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss_1: (1.7736) | Acc_1: (35.10%) (8132/23168)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss_1: (1.7713) | Acc_1: (35.22%) (8610/24448)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss_1: (1.7707) | Acc_1: (35.33%) (9089/25728)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss_1: (1.7678) | Acc_1: (35.52%) (9593/27008)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss_1: (1.7656) | Acc_1: (35.68%) (10092/28288)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss_1: (1.7631) | Acc_1: (35.73%) (10566/29568)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss_1: (1.7604) | Acc_1: (35.87%) (11065/30848)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss_1: (1.7604) | Acc_1: (35.86%) (11521/32128)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss_1: (1.7589) | Acc_1: (35.97%) (12018/33408)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss_1: (1.7571) | Acc_1: (36.10%) (12524/34688)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss_1: (1.7553) | Acc_1: (36.23%) (13031/35968)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss_1: (1.7525) | Acc_1: (36.33%) (13533/37248)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss_1: (1.7498) | Acc_1: (36.50%) (14062/38528)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss_1: (1.7463) | Acc_1: (36.59%) (14566/39808)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss_1: (1.7442) | Acc_1: (36.64%) (15053/41088)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss_1: (1.7423) | Acc_1: (36.77%) (15577/42368)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss_1: (1.7398) | Acc_1: (36.85%) (16085/43648)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss_1: (1.7375) | Acc_1: (36.92%) (16587/44928)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss_1: (1.7366) | Acc_1: (37.00%) (17098/46208)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss_1: (1.7352) | Acc_1: (37.03%) (17587/47488)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss_1: (1.7350) | Acc_1: (37.08%) (18085/48768)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss_1: (1.7324) | Acc_1: (37.25%) (18623/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (2.0565) | Acc: (26.01%) (2601/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.6577) | Acc_1: (45.31%) (58/128)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.6456) | Acc_1: (42.47%) (598/1408)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.6628) | Acc_1: (40.40%) (1086/2688)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.6674) | Acc_1: (40.10%) (1591/3968)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss_1: (1.6624) | Acc_1: (40.55%) (2128/5248)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss_1: (1.6514) | Acc_1: (40.73%) (2659/6528)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss_1: (1.6393) | Acc_1: (41.21%) (3218/7808)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss_1: (1.6298) | Acc_1: (41.31%) (3754/9088)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss_1: (1.6317) | Acc_1: (41.41%) (4293/10368)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss_1: (1.6338) | Acc_1: (41.32%) (4813/11648)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss_1: (1.6333) | Acc_1: (41.32%) (5342/12928)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss_1: (1.6275) | Acc_1: (41.41%) (5883/14208)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss_1: (1.6207) | Acc_1: (41.68%) (6455/15488)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss_1: (1.6231) | Acc_1: (41.64%) (6983/16768)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss_1: (1.6208) | Acc_1: (41.76%) (7537/18048)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss_1: (1.6173) | Acc_1: (41.60%) (8040/19328)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss_1: (1.6149) | Acc_1: (41.81%) (8616/20608)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss_1: (1.6113) | Acc_1: (42.04%) (9201/21888)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss_1: (1.6115) | Acc_1: (42.03%) (9738/23168)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss_1: (1.6123) | Acc_1: (42.07%) (10286/24448)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss_1: (1.6084) | Acc_1: (42.28%) (10879/25728)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss_1: (1.6087) | Acc_1: (42.27%) (11416/27008)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss_1: (1.6077) | Acc_1: (42.33%) (11975/28288)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss_1: (1.6045) | Acc_1: (42.46%) (12555/29568)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss_1: (1.6030) | Acc_1: (42.61%) (13144/30848)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss_1: (1.6030) | Acc_1: (42.59%) (13683/32128)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss_1: (1.6018) | Acc_1: (42.78%) (14291/33408)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss_1: (1.6016) | Acc_1: (42.77%) (14836/34688)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss_1: (1.6009) | Acc_1: (42.79%) (15389/35968)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss_1: (1.6003) | Acc_1: (42.80%) (15943/37248)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss_1: (1.5965) | Acc_1: (42.91%) (16532/38528)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss_1: (1.5933) | Acc_1: (43.02%) (17125/39808)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss_1: (1.5912) | Acc_1: (43.06%) (17694/41088)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss_1: (1.5886) | Acc_1: (43.15%) (18280/42368)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss_1: (1.5870) | Acc_1: (43.23%) (18867/43648)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss_1: (1.5854) | Acc_1: (43.28%) (19447/44928)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss_1: (1.5840) | Acc_1: (43.37%) (20039/46208)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss_1: (1.5818) | Acc_1: (43.50%) (20655/47488)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss_1: (1.5812) | Acc_1: (43.56%) (21245/48768)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss_1: (1.5794) | Acc_1: (43.64%) (21821/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.6891) | Acc: (40.02%) (4002/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (1.5852) | Acc_1: (40.62%) (52/128)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (1.4951) | Acc_1: (47.66%) (671/1408)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (1.5183) | Acc_1: (46.43%) (1248/2688)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (1.4993) | Acc_1: (47.08%) (1868/3968)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss_1: (1.4971) | Acc_1: (47.35%) (2485/5248)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss_1: (1.4882) | Acc_1: (47.79%) (3120/6528)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss_1: (1.4888) | Acc_1: (47.72%) (3726/7808)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss_1: (1.4860) | Acc_1: (47.74%) (4339/9088)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss_1: (1.4794) | Acc_1: (48.04%) (4981/10368)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss_1: (1.4901) | Acc_1: (47.53%) (5536/11648)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss_1: (1.4925) | Acc_1: (47.48%) (6138/12928)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss_1: (1.4936) | Acc_1: (47.45%) (6742/14208)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss_1: (1.4921) | Acc_1: (47.49%) (7356/15488)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss_1: (1.4905) | Acc_1: (47.51%) (7967/16768)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss_1: (1.4871) | Acc_1: (47.71%) (8611/18048)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss_1: (1.4898) | Acc_1: (47.64%) (9208/19328)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss_1: (1.4893) | Acc_1: (47.55%) (9800/20608)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss_1: (1.4880) | Acc_1: (47.63%) (10426/21888)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss_1: (1.4851) | Acc_1: (47.70%) (11050/23168)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss_1: (1.4827) | Acc_1: (47.72%) (11667/24448)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss_1: (1.4801) | Acc_1: (47.78%) (12293/25728)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss_1: (1.4789) | Acc_1: (47.82%) (12915/27008)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss_1: (1.4766) | Acc_1: (47.97%) (13571/28288)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss_1: (1.4763) | Acc_1: (48.04%) (14204/29568)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss_1: (1.4745) | Acc_1: (48.11%) (14840/30848)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss_1: (1.4718) | Acc_1: (48.19%) (15482/32128)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss_1: (1.4697) | Acc_1: (48.28%) (16130/33408)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss_1: (1.4670) | Acc_1: (48.35%) (16770/34688)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss_1: (1.4663) | Acc_1: (48.39%) (17404/35968)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss_1: (1.4635) | Acc_1: (48.47%) (18055/37248)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss_1: (1.4619) | Acc_1: (48.53%) (18696/38528)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss_1: (1.4609) | Acc_1: (48.61%) (19350/39808)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss_1: (1.4590) | Acc_1: (48.62%) (19976/41088)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss_1: (1.4574) | Acc_1: (48.69%) (20627/42368)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss_1: (1.4561) | Acc_1: (48.74%) (21274/43648)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss_1: (1.4556) | Acc_1: (48.76%) (21909/44928)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss_1: (1.4543) | Acc_1: (48.81%) (22555/46208)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss_1: (1.4541) | Acc_1: (48.83%) (23189/47488)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss_1: (1.4528) | Acc_1: (48.87%) (23834/48768)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss_1: (1.4514) | Acc_1: (48.90%) (24450/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3779) | Acc: (52.28%) (5228/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (1.3280) | Acc_1: (51.56%) (66/128)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (1.4495) | Acc_1: (47.30%) (666/1408)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (1.4286) | Acc_1: (49.22%) (1323/2688)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (1.4211) | Acc_1: (49.57%) (1967/3968)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss_1: (1.4190) | Acc_1: (49.92%) (2620/5248)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss_1: (1.4213) | Acc_1: (49.71%) (3245/6528)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss_1: (1.4101) | Acc_1: (50.22%) (3921/7808)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss_1: (1.4015) | Acc_1: (50.41%) (4581/9088)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss_1: (1.3979) | Acc_1: (50.39%) (5224/10368)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss_1: (1.3938) | Acc_1: (50.65%) (5900/11648)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss_1: (1.3876) | Acc_1: (50.77%) (6564/12928)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss_1: (1.3900) | Acc_1: (50.80%) (7217/14208)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss_1: (1.3887) | Acc_1: (50.96%) (7892/15488)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss_1: (1.3877) | Acc_1: (51.00%) (8551/16768)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss_1: (1.3846) | Acc_1: (51.05%) (9214/18048)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss_1: (1.3851) | Acc_1: (51.11%) (9879/19328)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss_1: (1.3841) | Acc_1: (51.20%) (10551/20608)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss_1: (1.3850) | Acc_1: (51.27%) (11221/21888)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss_1: (1.3845) | Acc_1: (51.30%) (11885/23168)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss_1: (1.3865) | Acc_1: (51.25%) (12530/24448)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss_1: (1.3846) | Acc_1: (51.32%) (13203/25728)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss_1: (1.3842) | Acc_1: (51.35%) (13868/27008)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss_1: (1.3817) | Acc_1: (51.40%) (14541/28288)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss_1: (1.3808) | Acc_1: (51.39%) (15196/29568)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss_1: (1.3805) | Acc_1: (51.37%) (15847/30848)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss_1: (1.3784) | Acc_1: (51.48%) (16539/32128)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss_1: (1.3788) | Acc_1: (51.42%) (17178/33408)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss_1: (1.3786) | Acc_1: (51.44%) (17844/34688)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss_1: (1.3784) | Acc_1: (51.42%) (18496/35968)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss_1: (1.3760) | Acc_1: (51.50%) (19181/37248)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss_1: (1.3726) | Acc_1: (51.64%) (19895/38528)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss_1: (1.3695) | Acc_1: (51.78%) (20614/39808)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss_1: (1.3685) | Acc_1: (51.84%) (21300/41088)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss_1: (1.3681) | Acc_1: (51.86%) (21973/42368)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss_1: (1.3672) | Acc_1: (51.89%) (22649/43648)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss_1: (1.3665) | Acc_1: (51.93%) (23330/44928)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss_1: (1.3651) | Acc_1: (52.01%) (24035/46208)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss_1: (1.3637) | Acc_1: (52.05%) (24716/47488)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss_1: (1.3622) | Acc_1: (52.10%) (25410/48768)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss_1: (1.3602) | Acc_1: (52.15%) (26075/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4051) | Acc: (51.77%) (5177/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (1.6145) | Acc_1: (45.31%) (58/128)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (1.3095) | Acc_1: (55.11%) (776/1408)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (1.3354) | Acc_1: (54.80%) (1473/2688)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (1.3259) | Acc_1: (54.54%) (2164/3968)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss_1: (1.3315) | Acc_1: (54.36%) (2853/5248)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss_1: (1.3248) | Acc_1: (54.38%) (3550/6528)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss_1: (1.3166) | Acc_1: (54.20%) (4232/7808)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss_1: (1.3128) | Acc_1: (54.43%) (4947/9088)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss_1: (1.3153) | Acc_1: (54.13%) (5612/10368)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss_1: (1.3115) | Acc_1: (54.29%) (6324/11648)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss_1: (1.3080) | Acc_1: (54.41%) (7034/12928)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss_1: (1.3031) | Acc_1: (54.65%) (7764/14208)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss_1: (1.3071) | Acc_1: (54.39%) (8424/15488)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss_1: (1.3065) | Acc_1: (54.41%) (9123/16768)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss_1: (1.3080) | Acc_1: (54.39%) (9816/18048)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss_1: (1.3054) | Acc_1: (54.44%) (10522/19328)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss_1: (1.3019) | Acc_1: (54.56%) (11244/20608)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss_1: (1.3026) | Acc_1: (54.54%) (11938/21888)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss_1: (1.3008) | Acc_1: (54.59%) (12648/23168)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss_1: (1.2969) | Acc_1: (54.74%) (13382/24448)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss_1: (1.2966) | Acc_1: (54.73%) (14082/25728)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss_1: (1.2986) | Acc_1: (54.66%) (14763/27008)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss_1: (1.2971) | Acc_1: (54.75%) (15488/28288)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss_1: (1.2966) | Acc_1: (54.69%) (16170/29568)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss_1: (1.2946) | Acc_1: (54.69%) (16870/30848)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss_1: (1.2930) | Acc_1: (54.72%) (17580/32128)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss_1: (1.2926) | Acc_1: (54.70%) (18273/33408)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss_1: (1.2925) | Acc_1: (54.70%) (18975/34688)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss_1: (1.2910) | Acc_1: (54.76%) (19696/35968)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss_1: (1.2876) | Acc_1: (54.89%) (20447/37248)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss_1: (1.2854) | Acc_1: (54.95%) (21172/38528)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss_1: (1.2852) | Acc_1: (55.00%) (21896/39808)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss_1: (1.2865) | Acc_1: (54.98%) (22590/41088)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss_1: (1.2867) | Acc_1: (55.03%) (23315/42368)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss_1: (1.2852) | Acc_1: (55.06%) (24032/43648)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss_1: (1.2824) | Acc_1: (55.12%) (24766/44928)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss_1: (1.2799) | Acc_1: (55.21%) (25512/46208)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss_1: (1.2785) | Acc_1: (55.23%) (26229/47488)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss_1: (1.2800) | Acc_1: (55.16%) (26901/48768)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss_1: (1.2801) | Acc_1: (55.15%) (27573/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.8030) | Acc: (56.45%) (5645/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (1.0766) | Acc_1: (58.59%) (75/128)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (1.2333) | Acc_1: (56.89%) (801/1408)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (1.2458) | Acc_1: (56.85%) (1528/2688)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (1.2334) | Acc_1: (57.18%) (2269/3968)\n",
      "Epoch: 8 | Batch_idx: 40 |  Loss_1: (1.2252) | Acc_1: (57.30%) (3007/5248)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss_1: (1.2170) | Acc_1: (57.71%) (3767/6528)\n",
      "Epoch: 8 | Batch_idx: 60 |  Loss_1: (1.2174) | Acc_1: (57.42%) (4483/7808)\n",
      "Epoch: 8 | Batch_idx: 70 |  Loss_1: (1.2290) | Acc_1: (56.91%) (5172/9088)\n",
      "Epoch: 8 | Batch_idx: 80 |  Loss_1: (1.2219) | Acc_1: (57.11%) (5921/10368)\n",
      "Epoch: 8 | Batch_idx: 90 |  Loss_1: (1.2253) | Acc_1: (56.88%) (6625/11648)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss_1: (1.2261) | Acc_1: (56.90%) (7356/12928)\n",
      "Epoch: 8 | Batch_idx: 110 |  Loss_1: (1.2222) | Acc_1: (56.94%) (8090/14208)\n",
      "Epoch: 8 | Batch_idx: 120 |  Loss_1: (1.2220) | Acc_1: (56.99%) (8827/15488)\n",
      "Epoch: 8 | Batch_idx: 130 |  Loss_1: (1.2209) | Acc_1: (57.07%) (9569/16768)\n",
      "Epoch: 8 | Batch_idx: 140 |  Loss_1: (1.2164) | Acc_1: (57.13%) (10311/18048)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss_1: (1.2117) | Acc_1: (57.23%) (11062/19328)\n",
      "Epoch: 8 | Batch_idx: 160 |  Loss_1: (1.2130) | Acc_1: (57.20%) (11787/20608)\n",
      "Epoch: 8 | Batch_idx: 170 |  Loss_1: (1.2172) | Acc_1: (57.02%) (12480/21888)\n",
      "Epoch: 8 | Batch_idx: 180 |  Loss_1: (1.2175) | Acc_1: (57.03%) (13212/23168)\n",
      "Epoch: 8 | Batch_idx: 190 |  Loss_1: (1.2163) | Acc_1: (57.00%) (13936/24448)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss_1: (1.2176) | Acc_1: (56.95%) (14652/25728)\n",
      "Epoch: 8 | Batch_idx: 210 |  Loss_1: (1.2168) | Acc_1: (56.92%) (15374/27008)\n",
      "Epoch: 8 | Batch_idx: 220 |  Loss_1: (1.2160) | Acc_1: (56.97%) (16115/28288)\n",
      "Epoch: 8 | Batch_idx: 230 |  Loss_1: (1.2134) | Acc_1: (57.05%) (16870/29568)\n",
      "Epoch: 8 | Batch_idx: 240 |  Loss_1: (1.2118) | Acc_1: (57.16%) (17634/30848)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss_1: (1.2116) | Acc_1: (57.26%) (18395/32128)\n",
      "Epoch: 8 | Batch_idx: 260 |  Loss_1: (1.2110) | Acc_1: (57.27%) (19132/33408)\n",
      "Epoch: 8 | Batch_idx: 270 |  Loss_1: (1.2092) | Acc_1: (57.32%) (19883/34688)\n",
      "Epoch: 8 | Batch_idx: 280 |  Loss_1: (1.2081) | Acc_1: (57.30%) (20608/35968)\n",
      "Epoch: 8 | Batch_idx: 290 |  Loss_1: (1.2080) | Acc_1: (57.37%) (21369/37248)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss_1: (1.2093) | Acc_1: (57.33%) (22089/38528)\n",
      "Epoch: 8 | Batch_idx: 310 |  Loss_1: (1.2081) | Acc_1: (57.39%) (22844/39808)\n",
      "Epoch: 8 | Batch_idx: 320 |  Loss_1: (1.2092) | Acc_1: (57.38%) (23575/41088)\n",
      "Epoch: 8 | Batch_idx: 330 |  Loss_1: (1.2097) | Acc_1: (57.39%) (24316/42368)\n",
      "Epoch: 8 | Batch_idx: 340 |  Loss_1: (1.2088) | Acc_1: (57.40%) (25053/43648)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss_1: (1.2066) | Acc_1: (57.47%) (25819/44928)\n",
      "Epoch: 8 | Batch_idx: 360 |  Loss_1: (1.2073) | Acc_1: (57.47%) (26556/46208)\n",
      "Epoch: 8 | Batch_idx: 370 |  Loss_1: (1.2064) | Acc_1: (57.50%) (27305/47488)\n",
      "Epoch: 8 | Batch_idx: 380 |  Loss_1: (1.2060) | Acc_1: (57.55%) (28066/48768)\n",
      "Epoch: 8 | Batch_idx: 390 |  Loss_1: (1.2037) | Acc_1: (57.63%) (28816/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0780) | Acc: (63.47%) (6347/10000)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (1.0868) | Acc_1: (64.84%) (83/128)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (1.1350) | Acc_1: (60.16%) (847/1408)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (1.1285) | Acc_1: (60.71%) (1632/2688)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (1.1274) | Acc_1: (60.99%) (2420/3968)\n",
      "Epoch: 9 | Batch_idx: 40 |  Loss_1: (1.1428) | Acc_1: (60.14%) (3156/5248)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss_1: (1.1412) | Acc_1: (60.31%) (3937/6528)\n",
      "Epoch: 9 | Batch_idx: 60 |  Loss_1: (1.1473) | Acc_1: (60.03%) (4687/7808)\n",
      "Epoch: 9 | Batch_idx: 70 |  Loss_1: (1.1458) | Acc_1: (59.97%) (5450/9088)\n",
      "Epoch: 9 | Batch_idx: 80 |  Loss_1: (1.1519) | Acc_1: (59.70%) (6190/10368)\n",
      "Epoch: 9 | Batch_idx: 90 |  Loss_1: (1.1502) | Acc_1: (59.80%) (6965/11648)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss_1: (1.1516) | Acc_1: (59.78%) (7729/12928)\n",
      "Epoch: 9 | Batch_idx: 110 |  Loss_1: (1.1527) | Acc_1: (59.81%) (8498/14208)\n",
      "Epoch: 9 | Batch_idx: 120 |  Loss_1: (1.1537) | Acc_1: (59.79%) (9260/15488)\n",
      "Epoch: 9 | Batch_idx: 130 |  Loss_1: (1.1532) | Acc_1: (59.85%) (10036/16768)\n",
      "Epoch: 9 | Batch_idx: 140 |  Loss_1: (1.1521) | Acc_1: (59.86%) (10804/18048)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss_1: (1.1493) | Acc_1: (59.94%) (11585/19328)\n",
      "Epoch: 9 | Batch_idx: 160 |  Loss_1: (1.1497) | Acc_1: (59.95%) (12355/20608)\n",
      "Epoch: 9 | Batch_idx: 170 |  Loss_1: (1.1499) | Acc_1: (59.95%) (13122/21888)\n",
      "Epoch: 9 | Batch_idx: 180 |  Loss_1: (1.1536) | Acc_1: (59.82%) (13859/23168)\n",
      "Epoch: 9 | Batch_idx: 190 |  Loss_1: (1.1540) | Acc_1: (59.80%) (14621/24448)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss_1: (1.1542) | Acc_1: (59.85%) (15399/25728)\n",
      "Epoch: 9 | Batch_idx: 210 |  Loss_1: (1.1529) | Acc_1: (59.86%) (16166/27008)\n",
      "Epoch: 9 | Batch_idx: 220 |  Loss_1: (1.1506) | Acc_1: (59.96%) (16962/28288)\n",
      "Epoch: 9 | Batch_idx: 230 |  Loss_1: (1.1500) | Acc_1: (59.98%) (17736/29568)\n",
      "Epoch: 9 | Batch_idx: 240 |  Loss_1: (1.1485) | Acc_1: (60.04%) (18522/30848)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss_1: (1.1478) | Acc_1: (60.02%) (19283/32128)\n",
      "Epoch: 9 | Batch_idx: 260 |  Loss_1: (1.1472) | Acc_1: (60.04%) (20059/33408)\n",
      "Epoch: 9 | Batch_idx: 270 |  Loss_1: (1.1462) | Acc_1: (60.04%) (20828/34688)\n",
      "Epoch: 9 | Batch_idx: 280 |  Loss_1: (1.1453) | Acc_1: (60.05%) (21598/35968)\n",
      "Epoch: 9 | Batch_idx: 290 |  Loss_1: (1.1430) | Acc_1: (60.14%) (22400/37248)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss_1: (1.1422) | Acc_1: (60.18%) (23185/38528)\n",
      "Epoch: 9 | Batch_idx: 310 |  Loss_1: (1.1403) | Acc_1: (60.25%) (23984/39808)\n",
      "Epoch: 9 | Batch_idx: 320 |  Loss_1: (1.1405) | Acc_1: (60.29%) (24770/41088)\n",
      "Epoch: 9 | Batch_idx: 330 |  Loss_1: (1.1386) | Acc_1: (60.32%) (25557/42368)\n",
      "Epoch: 9 | Batch_idx: 340 |  Loss_1: (1.1362) | Acc_1: (60.40%) (26363/43648)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss_1: (1.1353) | Acc_1: (60.38%) (27127/44928)\n",
      "Epoch: 9 | Batch_idx: 360 |  Loss_1: (1.1353) | Acc_1: (60.36%) (27890/46208)\n",
      "Epoch: 9 | Batch_idx: 370 |  Loss_1: (1.1331) | Acc_1: (60.46%) (28709/47488)\n",
      "Epoch: 9 | Batch_idx: 380 |  Loss_1: (1.1319) | Acc_1: (60.50%) (29505/48768)\n",
      "Epoch: 9 | Batch_idx: 390 |  Loss_1: (1.1322) | Acc_1: (60.49%) (30244/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9714) | Acc: (65.52%) (6552/10000)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (1.2096) | Acc_1: (53.91%) (69/128)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (1.1077) | Acc_1: (62.29%) (877/1408)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (1.1077) | Acc_1: (61.94%) (1665/2688)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (1.0904) | Acc_1: (62.30%) (2472/3968)\n",
      "Epoch: 10 | Batch_idx: 40 |  Loss_1: (1.0970) | Acc_1: (62.12%) (3260/5248)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss_1: (1.0916) | Acc_1: (62.32%) (4068/6528)\n",
      "Epoch: 10 | Batch_idx: 60 |  Loss_1: (1.0925) | Acc_1: (62.33%) (4867/7808)\n",
      "Epoch: 10 | Batch_idx: 70 |  Loss_1: (1.0980) | Acc_1: (61.94%) (5629/9088)\n",
      "Epoch: 10 | Batch_idx: 80 |  Loss_1: (1.0998) | Acc_1: (61.82%) (6410/10368)\n",
      "Epoch: 10 | Batch_idx: 90 |  Loss_1: (1.1006) | Acc_1: (61.76%) (7194/11648)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss_1: (1.0988) | Acc_1: (61.85%) (7996/12928)\n",
      "Epoch: 10 | Batch_idx: 110 |  Loss_1: (1.0989) | Acc_1: (61.85%) (8787/14208)\n",
      "Epoch: 10 | Batch_idx: 120 |  Loss_1: (1.0933) | Acc_1: (62.14%) (9624/15488)\n",
      "Epoch: 10 | Batch_idx: 130 |  Loss_1: (1.0954) | Acc_1: (61.96%) (10389/16768)\n",
      "Epoch: 10 | Batch_idx: 140 |  Loss_1: (1.0894) | Acc_1: (62.18%) (11223/18048)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss_1: (1.0856) | Acc_1: (62.30%) (12042/19328)\n",
      "Epoch: 10 | Batch_idx: 160 |  Loss_1: (1.0854) | Acc_1: (62.24%) (12827/20608)\n",
      "Epoch: 10 | Batch_idx: 170 |  Loss_1: (1.0878) | Acc_1: (62.15%) (13603/21888)\n",
      "Epoch: 10 | Batch_idx: 180 |  Loss_1: (1.0884) | Acc_1: (62.03%) (14372/23168)\n",
      "Epoch: 10 | Batch_idx: 190 |  Loss_1: (1.0890) | Acc_1: (62.01%) (15161/24448)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss_1: (1.0894) | Acc_1: (62.01%) (15953/25728)\n",
      "Epoch: 10 | Batch_idx: 210 |  Loss_1: (1.0911) | Acc_1: (61.96%) (16733/27008)\n",
      "Epoch: 10 | Batch_idx: 220 |  Loss_1: (1.0897) | Acc_1: (61.97%) (17531/28288)\n",
      "Epoch: 10 | Batch_idx: 230 |  Loss_1: (1.0882) | Acc_1: (62.05%) (18347/29568)\n",
      "Epoch: 10 | Batch_idx: 240 |  Loss_1: (1.0909) | Acc_1: (61.94%) (19107/30848)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss_1: (1.0905) | Acc_1: (61.96%) (19907/32128)\n",
      "Epoch: 10 | Batch_idx: 260 |  Loss_1: (1.0892) | Acc_1: (62.05%) (20730/33408)\n",
      "Epoch: 10 | Batch_idx: 270 |  Loss_1: (1.0875) | Acc_1: (62.08%) (21535/34688)\n",
      "Epoch: 10 | Batch_idx: 280 |  Loss_1: (1.0882) | Acc_1: (62.01%) (22304/35968)\n",
      "Epoch: 10 | Batch_idx: 290 |  Loss_1: (1.0887) | Acc_1: (62.05%) (23113/37248)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss_1: (1.0878) | Acc_1: (62.10%) (23924/38528)\n",
      "Epoch: 10 | Batch_idx: 310 |  Loss_1: (1.0885) | Acc_1: (62.10%) (24721/39808)\n",
      "Epoch: 10 | Batch_idx: 320 |  Loss_1: (1.0877) | Acc_1: (62.15%) (25535/41088)\n",
      "Epoch: 10 | Batch_idx: 330 |  Loss_1: (1.0856) | Acc_1: (62.20%) (26352/42368)\n",
      "Epoch: 10 | Batch_idx: 340 |  Loss_1: (1.0833) | Acc_1: (62.26%) (27175/43648)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss_1: (1.0833) | Acc_1: (62.27%) (27977/44928)\n",
      "Epoch: 10 | Batch_idx: 360 |  Loss_1: (1.0817) | Acc_1: (62.32%) (28796/46208)\n",
      "Epoch: 10 | Batch_idx: 370 |  Loss_1: (1.0816) | Acc_1: (62.30%) (29587/47488)\n",
      "Epoch: 10 | Batch_idx: 380 |  Loss_1: (1.0808) | Acc_1: (62.37%) (30417/48768)\n",
      "Epoch: 10 | Batch_idx: 390 |  Loss_1: (1.0807) | Acc_1: (62.38%) (31192/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9300) | Acc: (66.76%) (6676/10000)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (1.0412) | Acc_1: (64.84%) (83/128)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (1.0472) | Acc_1: (63.64%) (896/1408)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (1.0408) | Acc_1: (63.84%) (1716/2688)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (1.0623) | Acc_1: (63.23%) (2509/3968)\n",
      "Epoch: 11 | Batch_idx: 40 |  Loss_1: (1.0578) | Acc_1: (63.61%) (3338/5248)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss_1: (1.0458) | Acc_1: (63.66%) (4156/6528)\n",
      "Epoch: 11 | Batch_idx: 60 |  Loss_1: (1.0427) | Acc_1: (63.73%) (4976/7808)\n",
      "Epoch: 11 | Batch_idx: 70 |  Loss_1: (1.0471) | Acc_1: (63.68%) (5787/9088)\n",
      "Epoch: 11 | Batch_idx: 80 |  Loss_1: (1.0411) | Acc_1: (63.73%) (6608/10368)\n",
      "Epoch: 11 | Batch_idx: 90 |  Loss_1: (1.0451) | Acc_1: (63.69%) (7419/11648)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss_1: (1.0462) | Acc_1: (63.75%) (8241/12928)\n",
      "Epoch: 11 | Batch_idx: 110 |  Loss_1: (1.0407) | Acc_1: (63.91%) (9080/14208)\n",
      "Epoch: 11 | Batch_idx: 120 |  Loss_1: (1.0402) | Acc_1: (63.86%) (9891/15488)\n",
      "Epoch: 11 | Batch_idx: 130 |  Loss_1: (1.0368) | Acc_1: (63.96%) (10725/16768)\n",
      "Epoch: 11 | Batch_idx: 140 |  Loss_1: (1.0386) | Acc_1: (63.92%) (11537/18048)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss_1: (1.0402) | Acc_1: (63.84%) (12339/19328)\n",
      "Epoch: 11 | Batch_idx: 160 |  Loss_1: (1.0399) | Acc_1: (64.00%) (13189/20608)\n",
      "Epoch: 11 | Batch_idx: 170 |  Loss_1: (1.0367) | Acc_1: (64.08%) (14025/21888)\n",
      "Epoch: 11 | Batch_idx: 180 |  Loss_1: (1.0349) | Acc_1: (64.16%) (14865/23168)\n",
      "Epoch: 11 | Batch_idx: 190 |  Loss_1: (1.0339) | Acc_1: (64.21%) (15699/24448)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss_1: (1.0329) | Acc_1: (64.29%) (16541/25728)\n",
      "Epoch: 11 | Batch_idx: 210 |  Loss_1: (1.0314) | Acc_1: (64.33%) (17373/27008)\n",
      "Epoch: 11 | Batch_idx: 220 |  Loss_1: (1.0273) | Acc_1: (64.43%) (18225/28288)\n",
      "Epoch: 11 | Batch_idx: 230 |  Loss_1: (1.0286) | Acc_1: (64.44%) (19054/29568)\n",
      "Epoch: 11 | Batch_idx: 240 |  Loss_1: (1.0275) | Acc_1: (64.48%) (19890/30848)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss_1: (1.0279) | Acc_1: (64.44%) (20702/32128)\n",
      "Epoch: 11 | Batch_idx: 260 |  Loss_1: (1.0281) | Acc_1: (64.50%) (21548/33408)\n",
      "Epoch: 11 | Batch_idx: 270 |  Loss_1: (1.0264) | Acc_1: (64.55%) (22392/34688)\n",
      "Epoch: 11 | Batch_idx: 280 |  Loss_1: (1.0254) | Acc_1: (64.57%) (23226/35968)\n",
      "Epoch: 11 | Batch_idx: 290 |  Loss_1: (1.0260) | Acc_1: (64.51%) (24028/37248)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss_1: (1.0240) | Acc_1: (64.64%) (24903/38528)\n",
      "Epoch: 11 | Batch_idx: 310 |  Loss_1: (1.0242) | Acc_1: (64.61%) (25718/39808)\n",
      "Epoch: 11 | Batch_idx: 320 |  Loss_1: (1.0245) | Acc_1: (64.61%) (26545/41088)\n",
      "Epoch: 11 | Batch_idx: 330 |  Loss_1: (1.0231) | Acc_1: (64.64%) (27385/42368)\n",
      "Epoch: 11 | Batch_idx: 340 |  Loss_1: (1.0220) | Acc_1: (64.65%) (28217/43648)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss_1: (1.0224) | Acc_1: (64.62%) (29031/44928)\n",
      "Epoch: 11 | Batch_idx: 360 |  Loss_1: (1.0206) | Acc_1: (64.68%) (29889/46208)\n",
      "Epoch: 11 | Batch_idx: 370 |  Loss_1: (1.0202) | Acc_1: (64.70%) (30724/47488)\n",
      "Epoch: 11 | Batch_idx: 380 |  Loss_1: (1.0181) | Acc_1: (64.77%) (31586/48768)\n",
      "Epoch: 11 | Batch_idx: 390 |  Loss_1: (1.0173) | Acc_1: (64.82%) (32412/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.6023) | Acc: (66.44%) (6644/10000)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.9503) | Acc_1: (70.31%) (90/128)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.9845) | Acc_1: (65.55%) (923/1408)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.9980) | Acc_1: (65.25%) (1754/2688)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.9838) | Acc_1: (65.55%) (2601/3968)\n",
      "Epoch: 12 | Batch_idx: 40 |  Loss_1: (0.9852) | Acc_1: (66.04%) (3466/5248)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss_1: (0.9731) | Acc_1: (66.42%) (4336/6528)\n",
      "Epoch: 12 | Batch_idx: 60 |  Loss_1: (0.9773) | Acc_1: (66.29%) (5176/7808)\n",
      "Epoch: 12 | Batch_idx: 70 |  Loss_1: (0.9694) | Acc_1: (66.80%) (6071/9088)\n",
      "Epoch: 12 | Batch_idx: 80 |  Loss_1: (0.9709) | Acc_1: (66.72%) (6918/10368)\n",
      "Epoch: 12 | Batch_idx: 90 |  Loss_1: (0.9702) | Acc_1: (66.80%) (7781/11648)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss_1: (0.9675) | Acc_1: (66.83%) (8640/12928)\n",
      "Epoch: 12 | Batch_idx: 110 |  Loss_1: (0.9650) | Acc_1: (66.97%) (9515/14208)\n",
      "Epoch: 12 | Batch_idx: 120 |  Loss_1: (0.9674) | Acc_1: (66.84%) (10352/15488)\n",
      "Epoch: 12 | Batch_idx: 130 |  Loss_1: (0.9712) | Acc_1: (66.82%) (11204/16768)\n",
      "Epoch: 12 | Batch_idx: 140 |  Loss_1: (0.9692) | Acc_1: (66.93%) (12079/18048)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss_1: (0.9725) | Acc_1: (66.81%) (12913/19328)\n",
      "Epoch: 12 | Batch_idx: 160 |  Loss_1: (0.9678) | Acc_1: (66.92%) (13790/20608)\n",
      "Epoch: 12 | Batch_idx: 170 |  Loss_1: (0.9682) | Acc_1: (66.88%) (14639/21888)\n",
      "Epoch: 12 | Batch_idx: 180 |  Loss_1: (0.9693) | Acc_1: (66.79%) (15474/23168)\n",
      "Epoch: 12 | Batch_idx: 190 |  Loss_1: (0.9698) | Acc_1: (66.75%) (16319/24448)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss_1: (0.9709) | Acc_1: (66.69%) (17159/25728)\n",
      "Epoch: 12 | Batch_idx: 210 |  Loss_1: (0.9730) | Acc_1: (66.62%) (17993/27008)\n",
      "Epoch: 12 | Batch_idx: 220 |  Loss_1: (0.9730) | Acc_1: (66.60%) (18841/28288)\n",
      "Epoch: 12 | Batch_idx: 230 |  Loss_1: (0.9715) | Acc_1: (66.62%) (19697/29568)\n",
      "Epoch: 12 | Batch_idx: 240 |  Loss_1: (0.9723) | Acc_1: (66.62%) (20552/30848)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss_1: (0.9724) | Acc_1: (66.63%) (21407/32128)\n",
      "Epoch: 12 | Batch_idx: 260 |  Loss_1: (0.9700) | Acc_1: (66.66%) (22271/33408)\n",
      "Epoch: 12 | Batch_idx: 270 |  Loss_1: (0.9701) | Acc_1: (66.58%) (23095/34688)\n",
      "Epoch: 12 | Batch_idx: 280 |  Loss_1: (0.9702) | Acc_1: (66.56%) (23940/35968)\n",
      "Epoch: 12 | Batch_idx: 290 |  Loss_1: (0.9731) | Acc_1: (66.43%) (24743/37248)\n",
      "Epoch: 12 | Batch_idx: 300 |  Loss_1: (0.9749) | Acc_1: (66.41%) (25585/38528)\n",
      "Epoch: 12 | Batch_idx: 310 |  Loss_1: (0.9749) | Acc_1: (66.41%) (26435/39808)\n",
      "Epoch: 12 | Batch_idx: 320 |  Loss_1: (0.9741) | Acc_1: (66.50%) (27323/41088)\n",
      "Epoch: 12 | Batch_idx: 330 |  Loss_1: (0.9732) | Acc_1: (66.56%) (28199/42368)\n",
      "Epoch: 12 | Batch_idx: 340 |  Loss_1: (0.9731) | Acc_1: (66.54%) (29043/43648)\n",
      "Epoch: 12 | Batch_idx: 350 |  Loss_1: (0.9716) | Acc_1: (66.59%) (29918/44928)\n",
      "Epoch: 12 | Batch_idx: 360 |  Loss_1: (0.9704) | Acc_1: (66.64%) (30792/46208)\n",
      "Epoch: 12 | Batch_idx: 370 |  Loss_1: (0.9690) | Acc_1: (66.68%) (31667/47488)\n",
      "Epoch: 12 | Batch_idx: 380 |  Loss_1: (0.9679) | Acc_1: (66.71%) (32532/48768)\n",
      "Epoch: 12 | Batch_idx: 390 |  Loss_1: (0.9685) | Acc_1: (66.73%) (33363/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8639) | Acc: (70.00%) (7000/10000)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.9609) | Acc_1: (65.62%) (84/128)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (1.0047) | Acc_1: (65.20%) (918/1408)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.9592) | Acc_1: (67.30%) (1809/2688)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.9375) | Acc_1: (68.12%) (2703/3968)\n",
      "Epoch: 13 | Batch_idx: 40 |  Loss_1: (0.9297) | Acc_1: (68.35%) (3587/5248)\n",
      "Epoch: 13 | Batch_idx: 50 |  Loss_1: (0.9252) | Acc_1: (68.63%) (4480/6528)\n",
      "Epoch: 13 | Batch_idx: 60 |  Loss_1: (0.9297) | Acc_1: (68.34%) (5336/7808)\n",
      "Epoch: 13 | Batch_idx: 70 |  Loss_1: (0.9349) | Acc_1: (68.03%) (6183/9088)\n",
      "Epoch: 13 | Batch_idx: 80 |  Loss_1: (0.9314) | Acc_1: (67.87%) (7037/10368)\n",
      "Epoch: 13 | Batch_idx: 90 |  Loss_1: (0.9286) | Acc_1: (67.81%) (7898/11648)\n",
      "Epoch: 13 | Batch_idx: 100 |  Loss_1: (0.9236) | Acc_1: (67.89%) (8777/12928)\n",
      "Epoch: 13 | Batch_idx: 110 |  Loss_1: (0.9280) | Acc_1: (67.79%) (9632/14208)\n",
      "Epoch: 13 | Batch_idx: 120 |  Loss_1: (0.9269) | Acc_1: (67.86%) (10510/15488)\n",
      "Epoch: 13 | Batch_idx: 130 |  Loss_1: (0.9277) | Acc_1: (67.89%) (11384/16768)\n",
      "Epoch: 13 | Batch_idx: 140 |  Loss_1: (0.9241) | Acc_1: (67.97%) (12268/18048)\n",
      "Epoch: 13 | Batch_idx: 150 |  Loss_1: (0.9235) | Acc_1: (67.99%) (13141/19328)\n",
      "Epoch: 13 | Batch_idx: 160 |  Loss_1: (0.9247) | Acc_1: (67.87%) (13986/20608)\n",
      "Epoch: 13 | Batch_idx: 170 |  Loss_1: (0.9238) | Acc_1: (67.95%) (14872/21888)\n",
      "Epoch: 13 | Batch_idx: 180 |  Loss_1: (0.9252) | Acc_1: (67.94%) (15740/23168)\n",
      "Epoch: 13 | Batch_idx: 190 |  Loss_1: (0.9262) | Acc_1: (67.94%) (16609/24448)\n",
      "Epoch: 13 | Batch_idx: 200 |  Loss_1: (0.9266) | Acc_1: (67.93%) (17477/25728)\n",
      "Epoch: 13 | Batch_idx: 210 |  Loss_1: (0.9240) | Acc_1: (68.01%) (18369/27008)\n",
      "Epoch: 13 | Batch_idx: 220 |  Loss_1: (0.9240) | Acc_1: (67.97%) (19227/28288)\n",
      "Epoch: 13 | Batch_idx: 230 |  Loss_1: (0.9223) | Acc_1: (68.06%) (20125/29568)\n",
      "Epoch: 13 | Batch_idx: 240 |  Loss_1: (0.9218) | Acc_1: (68.09%) (21005/30848)\n",
      "Epoch: 13 | Batch_idx: 250 |  Loss_1: (0.9233) | Acc_1: (68.01%) (21851/32128)\n",
      "Epoch: 13 | Batch_idx: 260 |  Loss_1: (0.9244) | Acc_1: (67.98%) (22711/33408)\n",
      "Epoch: 13 | Batch_idx: 270 |  Loss_1: (0.9244) | Acc_1: (68.03%) (23599/34688)\n",
      "Epoch: 13 | Batch_idx: 280 |  Loss_1: (0.9251) | Acc_1: (68.05%) (24477/35968)\n",
      "Epoch: 13 | Batch_idx: 290 |  Loss_1: (0.9249) | Acc_1: (68.11%) (25370/37248)\n",
      "Epoch: 13 | Batch_idx: 300 |  Loss_1: (0.9253) | Acc_1: (68.10%) (26236/38528)\n",
      "Epoch: 13 | Batch_idx: 310 |  Loss_1: (0.9253) | Acc_1: (68.10%) (27109/39808)\n",
      "Epoch: 13 | Batch_idx: 320 |  Loss_1: (0.9241) | Acc_1: (68.14%) (27999/41088)\n",
      "Epoch: 13 | Batch_idx: 330 |  Loss_1: (0.9256) | Acc_1: (68.09%) (28847/42368)\n",
      "Epoch: 13 | Batch_idx: 340 |  Loss_1: (0.9254) | Acc_1: (68.09%) (29722/43648)\n",
      "Epoch: 13 | Batch_idx: 350 |  Loss_1: (0.9261) | Acc_1: (68.10%) (30597/44928)\n",
      "Epoch: 13 | Batch_idx: 360 |  Loss_1: (0.9242) | Acc_1: (68.18%) (31503/46208)\n",
      "Epoch: 13 | Batch_idx: 370 |  Loss_1: (0.9235) | Acc_1: (68.19%) (32381/47488)\n",
      "Epoch: 13 | Batch_idx: 380 |  Loss_1: (0.9225) | Acc_1: (68.19%) (33255/48768)\n",
      "Epoch: 13 | Batch_idx: 390 |  Loss_1: (0.9230) | Acc_1: (68.22%) (34108/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7001) | Acc: (76.46%) (7646/10000)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.8839) | Acc_1: (67.19%) (86/128)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.9303) | Acc_1: (66.55%) (937/1408)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.8968) | Acc_1: (68.42%) (1839/2688)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.8939) | Acc_1: (68.80%) (2730/3968)\n",
      "Epoch: 14 | Batch_idx: 40 |  Loss_1: (0.8964) | Acc_1: (68.75%) (3608/5248)\n",
      "Epoch: 14 | Batch_idx: 50 |  Loss_1: (0.9010) | Acc_1: (68.73%) (4487/6528)\n",
      "Epoch: 14 | Batch_idx: 60 |  Loss_1: (0.8907) | Acc_1: (69.08%) (5394/7808)\n",
      "Epoch: 14 | Batch_idx: 70 |  Loss_1: (0.8953) | Acc_1: (68.98%) (6269/9088)\n",
      "Epoch: 14 | Batch_idx: 80 |  Loss_1: (0.8887) | Acc_1: (69.24%) (7179/10368)\n",
      "Epoch: 14 | Batch_idx: 90 |  Loss_1: (0.8907) | Acc_1: (69.19%) (8059/11648)\n",
      "Epoch: 14 | Batch_idx: 100 |  Loss_1: (0.8909) | Acc_1: (69.10%) (8933/12928)\n",
      "Epoch: 14 | Batch_idx: 110 |  Loss_1: (0.8910) | Acc_1: (69.10%) (9818/14208)\n",
      "Epoch: 14 | Batch_idx: 120 |  Loss_1: (0.8919) | Acc_1: (69.14%) (10709/15488)\n",
      "Epoch: 14 | Batch_idx: 130 |  Loss_1: (0.8920) | Acc_1: (69.10%) (11587/16768)\n",
      "Epoch: 14 | Batch_idx: 140 |  Loss_1: (0.8898) | Acc_1: (69.27%) (12502/18048)\n",
      "Epoch: 14 | Batch_idx: 150 |  Loss_1: (0.8883) | Acc_1: (69.32%) (13399/19328)\n",
      "Epoch: 14 | Batch_idx: 160 |  Loss_1: (0.8884) | Acc_1: (69.35%) (14292/20608)\n",
      "Epoch: 14 | Batch_idx: 170 |  Loss_1: (0.8873) | Acc_1: (69.41%) (15192/21888)\n",
      "Epoch: 14 | Batch_idx: 180 |  Loss_1: (0.8879) | Acc_1: (69.38%) (16075/23168)\n",
      "Epoch: 14 | Batch_idx: 190 |  Loss_1: (0.8883) | Acc_1: (69.44%) (16976/24448)\n",
      "Epoch: 14 | Batch_idx: 200 |  Loss_1: (0.8848) | Acc_1: (69.61%) (17910/25728)\n",
      "Epoch: 14 | Batch_idx: 210 |  Loss_1: (0.8831) | Acc_1: (69.67%) (18817/27008)\n",
      "Epoch: 14 | Batch_idx: 220 |  Loss_1: (0.8804) | Acc_1: (69.79%) (19741/28288)\n",
      "Epoch: 14 | Batch_idx: 230 |  Loss_1: (0.8797) | Acc_1: (69.77%) (20629/29568)\n",
      "Epoch: 14 | Batch_idx: 240 |  Loss_1: (0.8789) | Acc_1: (69.76%) (21521/30848)\n",
      "Epoch: 14 | Batch_idx: 250 |  Loss_1: (0.8781) | Acc_1: (69.81%) (22428/32128)\n",
      "Epoch: 14 | Batch_idx: 260 |  Loss_1: (0.8786) | Acc_1: (69.81%) (23323/33408)\n",
      "Epoch: 14 | Batch_idx: 270 |  Loss_1: (0.8794) | Acc_1: (69.77%) (24203/34688)\n",
      "Epoch: 14 | Batch_idx: 280 |  Loss_1: (0.8800) | Acc_1: (69.79%) (25102/35968)\n",
      "Epoch: 14 | Batch_idx: 290 |  Loss_1: (0.8803) | Acc_1: (69.79%) (25996/37248)\n",
      "Epoch: 14 | Batch_idx: 300 |  Loss_1: (0.8801) | Acc_1: (69.81%) (26896/38528)\n",
      "Epoch: 14 | Batch_idx: 310 |  Loss_1: (0.8789) | Acc_1: (69.79%) (27781/39808)\n",
      "Epoch: 14 | Batch_idx: 320 |  Loss_1: (0.8800) | Acc_1: (69.75%) (28660/41088)\n",
      "Epoch: 14 | Batch_idx: 330 |  Loss_1: (0.8799) | Acc_1: (69.77%) (29560/42368)\n",
      "Epoch: 14 | Batch_idx: 340 |  Loss_1: (0.8790) | Acc_1: (69.81%) (30472/43648)\n",
      "Epoch: 14 | Batch_idx: 350 |  Loss_1: (0.8793) | Acc_1: (69.81%) (31366/44928)\n",
      "Epoch: 14 | Batch_idx: 360 |  Loss_1: (0.8810) | Acc_1: (69.75%) (32229/46208)\n",
      "Epoch: 14 | Batch_idx: 370 |  Loss_1: (0.8812) | Acc_1: (69.77%) (33134/47488)\n",
      "Epoch: 14 | Batch_idx: 380 |  Loss_1: (0.8817) | Acc_1: (69.74%) (34013/48768)\n",
      "Epoch: 14 | Batch_idx: 390 |  Loss_1: (0.8817) | Acc_1: (69.75%) (34876/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7274) | Acc: (75.12%) (7512/10000)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.7076) | Acc_1: (75.00%) (96/128)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.8797) | Acc_1: (68.82%) (969/1408)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.8508) | Acc_1: (70.24%) (1888/2688)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.8513) | Acc_1: (70.84%) (2811/3968)\n",
      "Epoch: 15 | Batch_idx: 40 |  Loss_1: (0.8568) | Acc_1: (70.58%) (3704/5248)\n",
      "Epoch: 15 | Batch_idx: 50 |  Loss_1: (0.8542) | Acc_1: (70.31%) (4590/6528)\n",
      "Epoch: 15 | Batch_idx: 60 |  Loss_1: (0.8476) | Acc_1: (70.52%) (5506/7808)\n",
      "Epoch: 15 | Batch_idx: 70 |  Loss_1: (0.8482) | Acc_1: (70.31%) (6390/9088)\n",
      "Epoch: 15 | Batch_idx: 80 |  Loss_1: (0.8502) | Acc_1: (70.37%) (7296/10368)\n",
      "Epoch: 15 | Batch_idx: 90 |  Loss_1: (0.8546) | Acc_1: (70.29%) (8187/11648)\n",
      "Epoch: 15 | Batch_idx: 100 |  Loss_1: (0.8516) | Acc_1: (70.37%) (9098/12928)\n",
      "Epoch: 15 | Batch_idx: 110 |  Loss_1: (0.8499) | Acc_1: (70.48%) (10014/14208)\n",
      "Epoch: 15 | Batch_idx: 120 |  Loss_1: (0.8493) | Acc_1: (70.51%) (10921/15488)\n",
      "Epoch: 15 | Batch_idx: 130 |  Loss_1: (0.8493) | Acc_1: (70.56%) (11832/16768)\n",
      "Epoch: 15 | Batch_idx: 140 |  Loss_1: (0.8493) | Acc_1: (70.51%) (12726/18048)\n",
      "Epoch: 15 | Batch_idx: 150 |  Loss_1: (0.8479) | Acc_1: (70.53%) (13632/19328)\n",
      "Epoch: 15 | Batch_idx: 160 |  Loss_1: (0.8494) | Acc_1: (70.46%) (14521/20608)\n",
      "Epoch: 15 | Batch_idx: 170 |  Loss_1: (0.8497) | Acc_1: (70.52%) (15435/21888)\n",
      "Epoch: 15 | Batch_idx: 180 |  Loss_1: (0.8474) | Acc_1: (70.54%) (16342/23168)\n",
      "Epoch: 15 | Batch_idx: 190 |  Loss_1: (0.8458) | Acc_1: (70.53%) (17243/24448)\n",
      "Epoch: 15 | Batch_idx: 200 |  Loss_1: (0.8452) | Acc_1: (70.55%) (18151/25728)\n",
      "Epoch: 15 | Batch_idx: 210 |  Loss_1: (0.8426) | Acc_1: (70.63%) (19075/27008)\n",
      "Epoch: 15 | Batch_idx: 220 |  Loss_1: (0.8423) | Acc_1: (70.64%) (19984/28288)\n",
      "Epoch: 15 | Batch_idx: 230 |  Loss_1: (0.8432) | Acc_1: (70.64%) (20888/29568)\n",
      "Epoch: 15 | Batch_idx: 240 |  Loss_1: (0.8413) | Acc_1: (70.67%) (21801/30848)\n",
      "Epoch: 15 | Batch_idx: 250 |  Loss_1: (0.8420) | Acc_1: (70.64%) (22696/32128)\n",
      "Epoch: 15 | Batch_idx: 260 |  Loss_1: (0.8406) | Acc_1: (70.72%) (23625/33408)\n",
      "Epoch: 15 | Batch_idx: 270 |  Loss_1: (0.8424) | Acc_1: (70.67%) (24513/34688)\n",
      "Epoch: 15 | Batch_idx: 280 |  Loss_1: (0.8441) | Acc_1: (70.58%) (25385/35968)\n",
      "Epoch: 15 | Batch_idx: 290 |  Loss_1: (0.8438) | Acc_1: (70.58%) (26289/37248)\n",
      "Epoch: 15 | Batch_idx: 300 |  Loss_1: (0.8437) | Acc_1: (70.64%) (27215/38528)\n",
      "Epoch: 15 | Batch_idx: 310 |  Loss_1: (0.8439) | Acc_1: (70.68%) (28137/39808)\n",
      "Epoch: 15 | Batch_idx: 320 |  Loss_1: (0.8446) | Acc_1: (70.65%) (29029/41088)\n",
      "Epoch: 15 | Batch_idx: 330 |  Loss_1: (0.8453) | Acc_1: (70.66%) (29937/42368)\n",
      "Epoch: 15 | Batch_idx: 340 |  Loss_1: (0.8458) | Acc_1: (70.68%) (30849/43648)\n",
      "Epoch: 15 | Batch_idx: 350 |  Loss_1: (0.8453) | Acc_1: (70.75%) (31785/44928)\n",
      "Epoch: 15 | Batch_idx: 360 |  Loss_1: (0.8438) | Acc_1: (70.82%) (32725/46208)\n",
      "Epoch: 15 | Batch_idx: 370 |  Loss_1: (0.8432) | Acc_1: (70.87%) (33655/47488)\n",
      "Epoch: 15 | Batch_idx: 380 |  Loss_1: (0.8425) | Acc_1: (70.88%) (34568/48768)\n",
      "Epoch: 15 | Batch_idx: 390 |  Loss_1: (0.8418) | Acc_1: (70.91%) (35457/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7146) | Acc: (75.66%) (7566/10000)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.7170) | Acc_1: (74.22%) (95/128)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.7988) | Acc_1: (72.59%) (1022/1408)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.8214) | Acc_1: (71.28%) (1916/2688)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.8369) | Acc_1: (70.69%) (2805/3968)\n",
      "Epoch: 16 | Batch_idx: 40 |  Loss_1: (0.8307) | Acc_1: (70.98%) (3725/5248)\n",
      "Epoch: 16 | Batch_idx: 50 |  Loss_1: (0.8326) | Acc_1: (71.09%) (4641/6528)\n",
      "Epoch: 16 | Batch_idx: 60 |  Loss_1: (0.8240) | Acc_1: (71.25%) (5563/7808)\n",
      "Epoch: 16 | Batch_idx: 70 |  Loss_1: (0.8250) | Acc_1: (71.25%) (6475/9088)\n",
      "Epoch: 16 | Batch_idx: 80 |  Loss_1: (0.8282) | Acc_1: (71.24%) (7386/10368)\n",
      "Epoch: 16 | Batch_idx: 90 |  Loss_1: (0.8251) | Acc_1: (71.37%) (8313/11648)\n",
      "Epoch: 16 | Batch_idx: 100 |  Loss_1: (0.8219) | Acc_1: (71.52%) (9246/12928)\n",
      "Epoch: 16 | Batch_idx: 110 |  Loss_1: (0.8244) | Acc_1: (71.45%) (10151/14208)\n",
      "Epoch: 16 | Batch_idx: 120 |  Loss_1: (0.8224) | Acc_1: (71.64%) (11095/15488)\n",
      "Epoch: 16 | Batch_idx: 130 |  Loss_1: (0.8234) | Acc_1: (71.62%) (12010/16768)\n",
      "Epoch: 16 | Batch_idx: 140 |  Loss_1: (0.8247) | Acc_1: (71.59%) (12921/18048)\n",
      "Epoch: 16 | Batch_idx: 150 |  Loss_1: (0.8253) | Acc_1: (71.63%) (13845/19328)\n",
      "Epoch: 16 | Batch_idx: 160 |  Loss_1: (0.8239) | Acc_1: (71.69%) (14773/20608)\n",
      "Epoch: 16 | Batch_idx: 170 |  Loss_1: (0.8236) | Acc_1: (71.72%) (15699/21888)\n",
      "Epoch: 16 | Batch_idx: 180 |  Loss_1: (0.8259) | Acc_1: (71.64%) (16598/23168)\n",
      "Epoch: 16 | Batch_idx: 190 |  Loss_1: (0.8258) | Acc_1: (71.62%) (17509/24448)\n",
      "Epoch: 16 | Batch_idx: 200 |  Loss_1: (0.8225) | Acc_1: (71.75%) (18459/25728)\n",
      "Epoch: 16 | Batch_idx: 210 |  Loss_1: (0.8216) | Acc_1: (71.79%) (19389/27008)\n",
      "Epoch: 16 | Batch_idx: 220 |  Loss_1: (0.8201) | Acc_1: (71.78%) (20306/28288)\n",
      "Epoch: 16 | Batch_idx: 230 |  Loss_1: (0.8167) | Acc_1: (71.91%) (21261/29568)\n",
      "Epoch: 16 | Batch_idx: 240 |  Loss_1: (0.8174) | Acc_1: (71.87%) (22170/30848)\n",
      "Epoch: 16 | Batch_idx: 250 |  Loss_1: (0.8166) | Acc_1: (71.92%) (23108/32128)\n",
      "Epoch: 16 | Batch_idx: 260 |  Loss_1: (0.8158) | Acc_1: (72.01%) (24057/33408)\n",
      "Epoch: 16 | Batch_idx: 270 |  Loss_1: (0.8164) | Acc_1: (72.00%) (24975/34688)\n",
      "Epoch: 16 | Batch_idx: 280 |  Loss_1: (0.8173) | Acc_1: (71.96%) (25884/35968)\n",
      "Epoch: 16 | Batch_idx: 290 |  Loss_1: (0.8157) | Acc_1: (72.01%) (26823/37248)\n",
      "Epoch: 16 | Batch_idx: 300 |  Loss_1: (0.8166) | Acc_1: (71.95%) (27721/38528)\n",
      "Epoch: 16 | Batch_idx: 310 |  Loss_1: (0.8149) | Acc_1: (72.04%) (28678/39808)\n",
      "Epoch: 16 | Batch_idx: 320 |  Loss_1: (0.8155) | Acc_1: (72.06%) (29608/41088)\n",
      "Epoch: 16 | Batch_idx: 330 |  Loss_1: (0.8140) | Acc_1: (72.12%) (30557/42368)\n",
      "Epoch: 16 | Batch_idx: 340 |  Loss_1: (0.8144) | Acc_1: (72.12%) (31478/43648)\n",
      "Epoch: 16 | Batch_idx: 350 |  Loss_1: (0.8134) | Acc_1: (72.15%) (32415/44928)\n",
      "Epoch: 16 | Batch_idx: 360 |  Loss_1: (0.8149) | Acc_1: (72.11%) (33321/46208)\n",
      "Epoch: 16 | Batch_idx: 370 |  Loss_1: (0.8135) | Acc_1: (72.13%) (34252/47488)\n",
      "Epoch: 16 | Batch_idx: 380 |  Loss_1: (0.8146) | Acc_1: (72.10%) (35163/48768)\n",
      "Epoch: 16 | Batch_idx: 390 |  Loss_1: (0.8137) | Acc_1: (72.16%) (36078/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6713) | Acc: (77.22%) (7722/10000)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.8054) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.8021) | Acc_1: (72.44%) (1020/1408)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.7941) | Acc_1: (72.62%) (1952/2688)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.7986) | Acc_1: (72.05%) (2859/3968)\n",
      "Epoch: 17 | Batch_idx: 40 |  Loss_1: (0.7962) | Acc_1: (72.08%) (3783/5248)\n",
      "Epoch: 17 | Batch_idx: 50 |  Loss_1: (0.7937) | Acc_1: (72.26%) (4717/6528)\n",
      "Epoch: 17 | Batch_idx: 60 |  Loss_1: (0.7921) | Acc_1: (72.41%) (5654/7808)\n",
      "Epoch: 17 | Batch_idx: 70 |  Loss_1: (0.7867) | Acc_1: (72.54%) (6592/9088)\n",
      "Epoch: 17 | Batch_idx: 80 |  Loss_1: (0.7895) | Acc_1: (72.42%) (7509/10368)\n",
      "Epoch: 17 | Batch_idx: 90 |  Loss_1: (0.7856) | Acc_1: (72.76%) (8475/11648)\n",
      "Epoch: 17 | Batch_idx: 100 |  Loss_1: (0.7890) | Acc_1: (72.61%) (9387/12928)\n",
      "Epoch: 17 | Batch_idx: 110 |  Loss_1: (0.7902) | Acc_1: (72.74%) (10335/14208)\n",
      "Epoch: 17 | Batch_idx: 120 |  Loss_1: (0.7918) | Acc_1: (72.66%) (11254/15488)\n",
      "Epoch: 17 | Batch_idx: 130 |  Loss_1: (0.7940) | Acc_1: (72.55%) (12165/16768)\n",
      "Epoch: 17 | Batch_idx: 140 |  Loss_1: (0.7960) | Acc_1: (72.55%) (13093/18048)\n",
      "Epoch: 17 | Batch_idx: 150 |  Loss_1: (0.7999) | Acc_1: (72.48%) (14008/19328)\n",
      "Epoch: 17 | Batch_idx: 160 |  Loss_1: (0.7965) | Acc_1: (72.59%) (14960/20608)\n",
      "Epoch: 17 | Batch_idx: 170 |  Loss_1: (0.7967) | Acc_1: (72.61%) (15893/21888)\n",
      "Epoch: 17 | Batch_idx: 180 |  Loss_1: (0.7953) | Acc_1: (72.73%) (16849/23168)\n",
      "Epoch: 17 | Batch_idx: 190 |  Loss_1: (0.7946) | Acc_1: (72.77%) (17790/24448)\n",
      "Epoch: 17 | Batch_idx: 200 |  Loss_1: (0.7954) | Acc_1: (72.74%) (18714/25728)\n",
      "Epoch: 17 | Batch_idx: 210 |  Loss_1: (0.7945) | Acc_1: (72.75%) (19648/27008)\n",
      "Epoch: 17 | Batch_idx: 220 |  Loss_1: (0.7952) | Acc_1: (72.75%) (20579/28288)\n",
      "Epoch: 17 | Batch_idx: 230 |  Loss_1: (0.7948) | Acc_1: (72.83%) (21533/29568)\n",
      "Epoch: 17 | Batch_idx: 240 |  Loss_1: (0.7932) | Acc_1: (72.86%) (22476/30848)\n",
      "Epoch: 17 | Batch_idx: 250 |  Loss_1: (0.7928) | Acc_1: (72.79%) (23386/32128)\n",
      "Epoch: 17 | Batch_idx: 260 |  Loss_1: (0.7921) | Acc_1: (72.79%) (24318/33408)\n",
      "Epoch: 17 | Batch_idx: 270 |  Loss_1: (0.7925) | Acc_1: (72.76%) (25239/34688)\n",
      "Epoch: 17 | Batch_idx: 280 |  Loss_1: (0.7921) | Acc_1: (72.74%) (26164/35968)\n",
      "Epoch: 17 | Batch_idx: 290 |  Loss_1: (0.7920) | Acc_1: (72.73%) (27092/37248)\n",
      "Epoch: 17 | Batch_idx: 300 |  Loss_1: (0.7924) | Acc_1: (72.72%) (28018/38528)\n",
      "Epoch: 17 | Batch_idx: 310 |  Loss_1: (0.7924) | Acc_1: (72.75%) (28961/39808)\n",
      "Epoch: 17 | Batch_idx: 320 |  Loss_1: (0.7921) | Acc_1: (72.73%) (29885/41088)\n",
      "Epoch: 17 | Batch_idx: 330 |  Loss_1: (0.7919) | Acc_1: (72.73%) (30816/42368)\n",
      "Epoch: 17 | Batch_idx: 340 |  Loss_1: (0.7924) | Acc_1: (72.71%) (31738/43648)\n",
      "Epoch: 17 | Batch_idx: 350 |  Loss_1: (0.7917) | Acc_1: (72.73%) (32678/44928)\n",
      "Epoch: 17 | Batch_idx: 360 |  Loss_1: (0.7926) | Acc_1: (72.73%) (33607/46208)\n",
      "Epoch: 17 | Batch_idx: 370 |  Loss_1: (0.7928) | Acc_1: (72.74%) (34541/47488)\n",
      "Epoch: 17 | Batch_idx: 380 |  Loss_1: (0.7926) | Acc_1: (72.76%) (35486/48768)\n",
      "Epoch: 17 | Batch_idx: 390 |  Loss_1: (0.7926) | Acc_1: (72.76%) (36382/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6237) | Acc: (79.04%) (7904/10000)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.8323) | Acc_1: (71.88%) (92/128)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.7672) | Acc_1: (74.86%) (1054/1408)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.7836) | Acc_1: (73.55%) (1977/2688)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.7620) | Acc_1: (73.84%) (2930/3968)\n",
      "Epoch: 18 | Batch_idx: 40 |  Loss_1: (0.7605) | Acc_1: (74.01%) (3884/5248)\n",
      "Epoch: 18 | Batch_idx: 50 |  Loss_1: (0.7632) | Acc_1: (74.03%) (4833/6528)\n",
      "Epoch: 18 | Batch_idx: 60 |  Loss_1: (0.7596) | Acc_1: (74.17%) (5791/7808)\n",
      "Epoch: 18 | Batch_idx: 70 |  Loss_1: (0.7641) | Acc_1: (74.10%) (6734/9088)\n",
      "Epoch: 18 | Batch_idx: 80 |  Loss_1: (0.7581) | Acc_1: (74.34%) (7708/10368)\n",
      "Epoch: 18 | Batch_idx: 90 |  Loss_1: (0.7550) | Acc_1: (74.36%) (8662/11648)\n",
      "Epoch: 18 | Batch_idx: 100 |  Loss_1: (0.7519) | Acc_1: (74.52%) (9634/12928)\n",
      "Epoch: 18 | Batch_idx: 110 |  Loss_1: (0.7476) | Acc_1: (74.61%) (10601/14208)\n",
      "Epoch: 18 | Batch_idx: 120 |  Loss_1: (0.7470) | Acc_1: (74.54%) (11545/15488)\n",
      "Epoch: 18 | Batch_idx: 130 |  Loss_1: (0.7454) | Acc_1: (74.64%) (12515/16768)\n",
      "Epoch: 18 | Batch_idx: 140 |  Loss_1: (0.7465) | Acc_1: (74.47%) (13440/18048)\n",
      "Epoch: 18 | Batch_idx: 150 |  Loss_1: (0.7446) | Acc_1: (74.48%) (14396/19328)\n",
      "Epoch: 18 | Batch_idx: 160 |  Loss_1: (0.7497) | Acc_1: (74.31%) (15314/20608)\n",
      "Epoch: 18 | Batch_idx: 170 |  Loss_1: (0.7511) | Acc_1: (74.23%) (16248/21888)\n",
      "Epoch: 18 | Batch_idx: 180 |  Loss_1: (0.7514) | Acc_1: (74.20%) (17190/23168)\n",
      "Epoch: 18 | Batch_idx: 190 |  Loss_1: (0.7504) | Acc_1: (74.19%) (18139/24448)\n",
      "Epoch: 18 | Batch_idx: 200 |  Loss_1: (0.7528) | Acc_1: (74.14%) (19074/25728)\n",
      "Epoch: 18 | Batch_idx: 210 |  Loss_1: (0.7513) | Acc_1: (74.18%) (20035/27008)\n",
      "Epoch: 18 | Batch_idx: 220 |  Loss_1: (0.7494) | Acc_1: (74.30%) (21017/28288)\n",
      "Epoch: 18 | Batch_idx: 230 |  Loss_1: (0.7491) | Acc_1: (74.25%) (21953/29568)\n",
      "Epoch: 18 | Batch_idx: 240 |  Loss_1: (0.7493) | Acc_1: (74.28%) (22915/30848)\n",
      "Epoch: 18 | Batch_idx: 250 |  Loss_1: (0.7499) | Acc_1: (74.24%) (23853/32128)\n",
      "Epoch: 18 | Batch_idx: 260 |  Loss_1: (0.7495) | Acc_1: (74.23%) (24798/33408)\n",
      "Epoch: 18 | Batch_idx: 270 |  Loss_1: (0.7490) | Acc_1: (74.24%) (25754/34688)\n",
      "Epoch: 18 | Batch_idx: 280 |  Loss_1: (0.7496) | Acc_1: (74.25%) (26705/35968)\n",
      "Epoch: 18 | Batch_idx: 290 |  Loss_1: (0.7489) | Acc_1: (74.27%) (27665/37248)\n",
      "Epoch: 18 | Batch_idx: 300 |  Loss_1: (0.7509) | Acc_1: (74.20%) (28588/38528)\n",
      "Epoch: 18 | Batch_idx: 310 |  Loss_1: (0.7512) | Acc_1: (74.23%) (29548/39808)\n",
      "Epoch: 18 | Batch_idx: 320 |  Loss_1: (0.7503) | Acc_1: (74.28%) (30521/41088)\n",
      "Epoch: 18 | Batch_idx: 330 |  Loss_1: (0.7496) | Acc_1: (74.32%) (31486/42368)\n",
      "Epoch: 18 | Batch_idx: 340 |  Loss_1: (0.7504) | Acc_1: (74.26%) (32415/43648)\n",
      "Epoch: 18 | Batch_idx: 350 |  Loss_1: (0.7506) | Acc_1: (74.25%) (33361/44928)\n",
      "Epoch: 18 | Batch_idx: 360 |  Loss_1: (0.7515) | Acc_1: (74.23%) (34300/46208)\n",
      "Epoch: 18 | Batch_idx: 370 |  Loss_1: (0.7513) | Acc_1: (74.23%) (35250/47488)\n",
      "Epoch: 18 | Batch_idx: 380 |  Loss_1: (0.7507) | Acc_1: (74.23%) (36201/48768)\n",
      "Epoch: 18 | Batch_idx: 390 |  Loss_1: (0.7506) | Acc_1: (74.25%) (37123/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5594) | Acc: (80.29%) (8029/10000)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.9914) | Acc_1: (69.53%) (89/128)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.7350) | Acc_1: (74.86%) (1054/1408)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.7214) | Acc_1: (74.96%) (2015/2688)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.7433) | Acc_1: (74.52%) (2957/3968)\n",
      "Epoch: 19 | Batch_idx: 40 |  Loss_1: (0.7365) | Acc_1: (74.56%) (3913/5248)\n",
      "Epoch: 19 | Batch_idx: 50 |  Loss_1: (0.7302) | Acc_1: (75.00%) (4896/6528)\n",
      "Epoch: 19 | Batch_idx: 60 |  Loss_1: (0.7267) | Acc_1: (75.18%) (5870/7808)\n",
      "Epoch: 19 | Batch_idx: 70 |  Loss_1: (0.7277) | Acc_1: (74.98%) (6814/9088)\n",
      "Epoch: 19 | Batch_idx: 80 |  Loss_1: (0.7255) | Acc_1: (75.21%) (7798/10368)\n",
      "Epoch: 19 | Batch_idx: 90 |  Loss_1: (0.7275) | Acc_1: (75.18%) (8757/11648)\n",
      "Epoch: 19 | Batch_idx: 100 |  Loss_1: (0.7290) | Acc_1: (75.17%) (9718/12928)\n",
      "Epoch: 19 | Batch_idx: 110 |  Loss_1: (0.7232) | Acc_1: (75.30%) (10699/14208)\n",
      "Epoch: 19 | Batch_idx: 120 |  Loss_1: (0.7231) | Acc_1: (75.32%) (11665/15488)\n",
      "Epoch: 19 | Batch_idx: 130 |  Loss_1: (0.7249) | Acc_1: (75.16%) (12603/16768)\n",
      "Epoch: 19 | Batch_idx: 140 |  Loss_1: (0.7237) | Acc_1: (75.17%) (13567/18048)\n",
      "Epoch: 19 | Batch_idx: 150 |  Loss_1: (0.7244) | Acc_1: (75.13%) (14521/19328)\n",
      "Epoch: 19 | Batch_idx: 160 |  Loss_1: (0.7232) | Acc_1: (75.17%) (15491/20608)\n",
      "Epoch: 19 | Batch_idx: 170 |  Loss_1: (0.7258) | Acc_1: (75.13%) (16444/21888)\n",
      "Epoch: 19 | Batch_idx: 180 |  Loss_1: (0.7294) | Acc_1: (75.02%) (17380/23168)\n",
      "Epoch: 19 | Batch_idx: 190 |  Loss_1: (0.7282) | Acc_1: (75.08%) (18356/24448)\n",
      "Epoch: 19 | Batch_idx: 200 |  Loss_1: (0.7283) | Acc_1: (75.04%) (19306/25728)\n",
      "Epoch: 19 | Batch_idx: 210 |  Loss_1: (0.7261) | Acc_1: (75.13%) (20291/27008)\n",
      "Epoch: 19 | Batch_idx: 220 |  Loss_1: (0.7266) | Acc_1: (75.11%) (21246/28288)\n",
      "Epoch: 19 | Batch_idx: 230 |  Loss_1: (0.7257) | Acc_1: (75.16%) (22224/29568)\n",
      "Epoch: 19 | Batch_idx: 240 |  Loss_1: (0.7276) | Acc_1: (75.12%) (23174/30848)\n",
      "Epoch: 19 | Batch_idx: 250 |  Loss_1: (0.7266) | Acc_1: (75.18%) (24154/32128)\n",
      "Epoch: 19 | Batch_idx: 260 |  Loss_1: (0.7275) | Acc_1: (75.17%) (25114/33408)\n",
      "Epoch: 19 | Batch_idx: 270 |  Loss_1: (0.7282) | Acc_1: (75.13%) (26060/34688)\n",
      "Epoch: 19 | Batch_idx: 280 |  Loss_1: (0.7285) | Acc_1: (75.11%) (27015/35968)\n",
      "Epoch: 19 | Batch_idx: 290 |  Loss_1: (0.7273) | Acc_1: (75.12%) (27979/37248)\n",
      "Epoch: 19 | Batch_idx: 300 |  Loss_1: (0.7266) | Acc_1: (75.12%) (28941/38528)\n",
      "Epoch: 19 | Batch_idx: 310 |  Loss_1: (0.7268) | Acc_1: (75.07%) (29882/39808)\n",
      "Epoch: 19 | Batch_idx: 320 |  Loss_1: (0.7263) | Acc_1: (75.05%) (30836/41088)\n",
      "Epoch: 19 | Batch_idx: 330 |  Loss_1: (0.7272) | Acc_1: (75.03%) (31789/42368)\n",
      "Epoch: 19 | Batch_idx: 340 |  Loss_1: (0.7274) | Acc_1: (75.01%) (32739/43648)\n",
      "Epoch: 19 | Batch_idx: 350 |  Loss_1: (0.7281) | Acc_1: (74.94%) (33668/44928)\n",
      "Epoch: 19 | Batch_idx: 360 |  Loss_1: (0.7275) | Acc_1: (74.97%) (34640/46208)\n",
      "Epoch: 19 | Batch_idx: 370 |  Loss_1: (0.7289) | Acc_1: (74.90%) (35570/47488)\n",
      "Epoch: 19 | Batch_idx: 380 |  Loss_1: (0.7287) | Acc_1: (74.92%) (36535/48768)\n",
      "Epoch: 19 | Batch_idx: 390 |  Loss_1: (0.7279) | Acc_1: (74.94%) (37472/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5946) | Acc: (79.84%) (7984/10000)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.8147) | Acc_1: (68.75%) (88/128)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.7119) | Acc_1: (74.79%) (1053/1408)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.6930) | Acc_1: (75.93%) (2041/2688)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.7027) | Acc_1: (75.91%) (3012/3968)\n",
      "Epoch: 20 | Batch_idx: 40 |  Loss_1: (0.7091) | Acc_1: (75.84%) (3980/5248)\n",
      "Epoch: 20 | Batch_idx: 50 |  Loss_1: (0.6965) | Acc_1: (76.32%) (4982/6528)\n",
      "Epoch: 20 | Batch_idx: 60 |  Loss_1: (0.7087) | Acc_1: (76.08%) (5940/7808)\n",
      "Epoch: 20 | Batch_idx: 70 |  Loss_1: (0.7072) | Acc_1: (76.13%) (6919/9088)\n",
      "Epoch: 20 | Batch_idx: 80 |  Loss_1: (0.7075) | Acc_1: (76.08%) (7888/10368)\n",
      "Epoch: 20 | Batch_idx: 90 |  Loss_1: (0.7014) | Acc_1: (76.27%) (8884/11648)\n",
      "Epoch: 20 | Batch_idx: 100 |  Loss_1: (0.6983) | Acc_1: (76.36%) (9872/12928)\n",
      "Epoch: 20 | Batch_idx: 110 |  Loss_1: (0.7014) | Acc_1: (76.25%) (10834/14208)\n",
      "Epoch: 20 | Batch_idx: 120 |  Loss_1: (0.7040) | Acc_1: (76.12%) (11790/15488)\n",
      "Epoch: 20 | Batch_idx: 130 |  Loss_1: (0.7069) | Acc_1: (76.10%) (12760/16768)\n",
      "Epoch: 20 | Batch_idx: 140 |  Loss_1: (0.7079) | Acc_1: (76.03%) (13722/18048)\n",
      "Epoch: 20 | Batch_idx: 150 |  Loss_1: (0.7078) | Acc_1: (76.08%) (14704/19328)\n",
      "Epoch: 20 | Batch_idx: 160 |  Loss_1: (0.7073) | Acc_1: (76.00%) (15662/20608)\n",
      "Epoch: 20 | Batch_idx: 170 |  Loss_1: (0.7077) | Acc_1: (75.98%) (16630/21888)\n",
      "Epoch: 20 | Batch_idx: 180 |  Loss_1: (0.7079) | Acc_1: (75.95%) (17597/23168)\n",
      "Epoch: 20 | Batch_idx: 190 |  Loss_1: (0.7091) | Acc_1: (75.95%) (18569/24448)\n",
      "Epoch: 20 | Batch_idx: 200 |  Loss_1: (0.7110) | Acc_1: (75.82%) (19506/25728)\n",
      "Epoch: 20 | Batch_idx: 210 |  Loss_1: (0.7103) | Acc_1: (75.85%) (20485/27008)\n",
      "Epoch: 20 | Batch_idx: 220 |  Loss_1: (0.7105) | Acc_1: (75.78%) (21436/28288)\n",
      "Epoch: 20 | Batch_idx: 230 |  Loss_1: (0.7092) | Acc_1: (75.83%) (22422/29568)\n",
      "Epoch: 20 | Batch_idx: 240 |  Loss_1: (0.7118) | Acc_1: (75.74%) (23365/30848)\n",
      "Epoch: 20 | Batch_idx: 250 |  Loss_1: (0.7120) | Acc_1: (75.71%) (24324/32128)\n",
      "Epoch: 20 | Batch_idx: 260 |  Loss_1: (0.7115) | Acc_1: (75.69%) (25288/33408)\n",
      "Epoch: 20 | Batch_idx: 270 |  Loss_1: (0.7135) | Acc_1: (75.63%) (26235/34688)\n",
      "Epoch: 20 | Batch_idx: 280 |  Loss_1: (0.7118) | Acc_1: (75.64%) (27205/35968)\n",
      "Epoch: 20 | Batch_idx: 290 |  Loss_1: (0.7136) | Acc_1: (75.53%) (28133/37248)\n",
      "Epoch: 20 | Batch_idx: 300 |  Loss_1: (0.7125) | Acc_1: (75.55%) (29106/38528)\n",
      "Epoch: 20 | Batch_idx: 310 |  Loss_1: (0.7109) | Acc_1: (75.59%) (30089/39808)\n",
      "Epoch: 20 | Batch_idx: 320 |  Loss_1: (0.7100) | Acc_1: (75.61%) (31068/41088)\n",
      "Epoch: 20 | Batch_idx: 330 |  Loss_1: (0.7096) | Acc_1: (75.62%) (32040/42368)\n",
      "Epoch: 20 | Batch_idx: 340 |  Loss_1: (0.7092) | Acc_1: (75.65%) (33019/43648)\n",
      "Epoch: 20 | Batch_idx: 350 |  Loss_1: (0.7097) | Acc_1: (75.64%) (33984/44928)\n",
      "Epoch: 20 | Batch_idx: 360 |  Loss_1: (0.7111) | Acc_1: (75.59%) (34927/46208)\n",
      "Epoch: 20 | Batch_idx: 370 |  Loss_1: (0.7122) | Acc_1: (75.55%) (35876/47488)\n",
      "Epoch: 20 | Batch_idx: 380 |  Loss_1: (0.7109) | Acc_1: (75.56%) (36847/48768)\n",
      "Epoch: 20 | Batch_idx: 390 |  Loss_1: (0.7104) | Acc_1: (75.57%) (37787/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5046) | Acc: (82.54%) (8254/10000)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.6503) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.7025) | Acc_1: (77.41%) (1090/1408)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.7062) | Acc_1: (76.86%) (2066/2688)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.6939) | Acc_1: (76.94%) (3053/3968)\n",
      "Epoch: 21 | Batch_idx: 40 |  Loss_1: (0.6866) | Acc_1: (77.06%) (4044/5248)\n",
      "Epoch: 21 | Batch_idx: 50 |  Loss_1: (0.6859) | Acc_1: (77.11%) (5034/6528)\n",
      "Epoch: 21 | Batch_idx: 60 |  Loss_1: (0.6869) | Acc_1: (76.95%) (6008/7808)\n",
      "Epoch: 21 | Batch_idx: 70 |  Loss_1: (0.6901) | Acc_1: (76.82%) (6981/9088)\n",
      "Epoch: 21 | Batch_idx: 80 |  Loss_1: (0.6832) | Acc_1: (77.03%) (7986/10368)\n",
      "Epoch: 21 | Batch_idx: 90 |  Loss_1: (0.6830) | Acc_1: (77.00%) (8969/11648)\n",
      "Epoch: 21 | Batch_idx: 100 |  Loss_1: (0.6775) | Acc_1: (77.10%) (9967/12928)\n",
      "Epoch: 21 | Batch_idx: 110 |  Loss_1: (0.6778) | Acc_1: (77.14%) (10960/14208)\n",
      "Epoch: 21 | Batch_idx: 120 |  Loss_1: (0.6786) | Acc_1: (77.16%) (11950/15488)\n",
      "Epoch: 21 | Batch_idx: 130 |  Loss_1: (0.6762) | Acc_1: (77.22%) (12949/16768)\n",
      "Epoch: 21 | Batch_idx: 140 |  Loss_1: (0.6768) | Acc_1: (77.09%) (13913/18048)\n",
      "Epoch: 21 | Batch_idx: 150 |  Loss_1: (0.6778) | Acc_1: (77.05%) (14892/19328)\n",
      "Epoch: 21 | Batch_idx: 160 |  Loss_1: (0.6770) | Acc_1: (77.07%) (15883/20608)\n",
      "Epoch: 21 | Batch_idx: 170 |  Loss_1: (0.6787) | Acc_1: (77.03%) (16861/21888)\n",
      "Epoch: 21 | Batch_idx: 180 |  Loss_1: (0.6795) | Acc_1: (77.04%) (17848/23168)\n",
      "Epoch: 21 | Batch_idx: 190 |  Loss_1: (0.6821) | Acc_1: (76.97%) (18818/24448)\n",
      "Epoch: 21 | Batch_idx: 200 |  Loss_1: (0.6834) | Acc_1: (76.90%) (19785/25728)\n",
      "Epoch: 21 | Batch_idx: 210 |  Loss_1: (0.6848) | Acc_1: (76.77%) (20735/27008)\n",
      "Epoch: 21 | Batch_idx: 220 |  Loss_1: (0.6856) | Acc_1: (76.75%) (21711/28288)\n",
      "Epoch: 21 | Batch_idx: 230 |  Loss_1: (0.6866) | Acc_1: (76.73%) (22687/29568)\n",
      "Epoch: 21 | Batch_idx: 240 |  Loss_1: (0.6868) | Acc_1: (76.76%) (23679/30848)\n",
      "Epoch: 21 | Batch_idx: 250 |  Loss_1: (0.6857) | Acc_1: (76.76%) (24661/32128)\n",
      "Epoch: 21 | Batch_idx: 260 |  Loss_1: (0.6866) | Acc_1: (76.76%) (25644/33408)\n",
      "Epoch: 21 | Batch_idx: 270 |  Loss_1: (0.6859) | Acc_1: (76.74%) (26620/34688)\n",
      "Epoch: 21 | Batch_idx: 280 |  Loss_1: (0.6861) | Acc_1: (76.73%) (27599/35968)\n",
      "Epoch: 21 | Batch_idx: 290 |  Loss_1: (0.6849) | Acc_1: (76.75%) (28588/37248)\n",
      "Epoch: 21 | Batch_idx: 300 |  Loss_1: (0.6831) | Acc_1: (76.77%) (29577/38528)\n",
      "Epoch: 21 | Batch_idx: 310 |  Loss_1: (0.6848) | Acc_1: (76.70%) (30533/39808)\n",
      "Epoch: 21 | Batch_idx: 320 |  Loss_1: (0.6849) | Acc_1: (76.67%) (31501/41088)\n",
      "Epoch: 21 | Batch_idx: 330 |  Loss_1: (0.6849) | Acc_1: (76.68%) (32487/42368)\n",
      "Epoch: 21 | Batch_idx: 340 |  Loss_1: (0.6847) | Acc_1: (76.65%) (33458/43648)\n",
      "Epoch: 21 | Batch_idx: 350 |  Loss_1: (0.6856) | Acc_1: (76.61%) (34421/44928)\n",
      "Epoch: 21 | Batch_idx: 360 |  Loss_1: (0.6864) | Acc_1: (76.57%) (35381/46208)\n",
      "Epoch: 21 | Batch_idx: 370 |  Loss_1: (0.6873) | Acc_1: (76.52%) (36338/47488)\n",
      "Epoch: 21 | Batch_idx: 380 |  Loss_1: (0.6867) | Acc_1: (76.54%) (37325/48768)\n",
      "Epoch: 21 | Batch_idx: 390 |  Loss_1: (0.6865) | Acc_1: (76.54%) (38272/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5401) | Acc: (81.33%) (8133/10000)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.7146) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.6474) | Acc_1: (78.05%) (1099/1408)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.6560) | Acc_1: (77.64%) (2087/2688)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.6627) | Acc_1: (77.29%) (3067/3968)\n",
      "Epoch: 22 | Batch_idx: 40 |  Loss_1: (0.6640) | Acc_1: (77.12%) (4047/5248)\n",
      "Epoch: 22 | Batch_idx: 50 |  Loss_1: (0.6736) | Acc_1: (76.87%) (5018/6528)\n",
      "Epoch: 22 | Batch_idx: 60 |  Loss_1: (0.6716) | Acc_1: (77.20%) (6028/7808)\n",
      "Epoch: 22 | Batch_idx: 70 |  Loss_1: (0.6646) | Acc_1: (77.40%) (7034/9088)\n",
      "Epoch: 22 | Batch_idx: 80 |  Loss_1: (0.6623) | Acc_1: (77.44%) (8029/10368)\n",
      "Epoch: 22 | Batch_idx: 90 |  Loss_1: (0.6646) | Acc_1: (77.37%) (9012/11648)\n",
      "Epoch: 22 | Batch_idx: 100 |  Loss_1: (0.6616) | Acc_1: (77.38%) (10004/12928)\n",
      "Epoch: 22 | Batch_idx: 110 |  Loss_1: (0.6638) | Acc_1: (77.32%) (10986/14208)\n",
      "Epoch: 22 | Batch_idx: 120 |  Loss_1: (0.6604) | Acc_1: (77.36%) (11982/15488)\n",
      "Epoch: 22 | Batch_idx: 130 |  Loss_1: (0.6624) | Acc_1: (77.29%) (12960/16768)\n",
      "Epoch: 22 | Batch_idx: 140 |  Loss_1: (0.6664) | Acc_1: (77.09%) (13914/18048)\n",
      "Epoch: 22 | Batch_idx: 150 |  Loss_1: (0.6651) | Acc_1: (77.07%) (14896/19328)\n",
      "Epoch: 22 | Batch_idx: 160 |  Loss_1: (0.6637) | Acc_1: (77.14%) (15896/20608)\n",
      "Epoch: 22 | Batch_idx: 170 |  Loss_1: (0.6649) | Acc_1: (77.06%) (16867/21888)\n",
      "Epoch: 22 | Batch_idx: 180 |  Loss_1: (0.6647) | Acc_1: (77.04%) (17849/23168)\n",
      "Epoch: 22 | Batch_idx: 190 |  Loss_1: (0.6617) | Acc_1: (77.16%) (18863/24448)\n",
      "Epoch: 22 | Batch_idx: 200 |  Loss_1: (0.6618) | Acc_1: (77.10%) (19836/25728)\n",
      "Epoch: 22 | Batch_idx: 210 |  Loss_1: (0.6612) | Acc_1: (77.10%) (20823/27008)\n",
      "Epoch: 22 | Batch_idx: 220 |  Loss_1: (0.6628) | Acc_1: (77.05%) (21797/28288)\n",
      "Epoch: 22 | Batch_idx: 230 |  Loss_1: (0.6623) | Acc_1: (77.04%) (22779/29568)\n",
      "Epoch: 22 | Batch_idx: 240 |  Loss_1: (0.6634) | Acc_1: (77.05%) (23767/30848)\n",
      "Epoch: 22 | Batch_idx: 250 |  Loss_1: (0.6605) | Acc_1: (77.18%) (24795/32128)\n",
      "Epoch: 22 | Batch_idx: 260 |  Loss_1: (0.6619) | Acc_1: (77.13%) (25769/33408)\n",
      "Epoch: 22 | Batch_idx: 270 |  Loss_1: (0.6608) | Acc_1: (77.19%) (26774/34688)\n",
      "Epoch: 22 | Batch_idx: 280 |  Loss_1: (0.6599) | Acc_1: (77.23%) (27778/35968)\n",
      "Epoch: 22 | Batch_idx: 290 |  Loss_1: (0.6592) | Acc_1: (77.26%) (28776/37248)\n",
      "Epoch: 22 | Batch_idx: 300 |  Loss_1: (0.6580) | Acc_1: (77.29%) (29777/38528)\n",
      "Epoch: 22 | Batch_idx: 310 |  Loss_1: (0.6588) | Acc_1: (77.24%) (30746/39808)\n",
      "Epoch: 22 | Batch_idx: 320 |  Loss_1: (0.6585) | Acc_1: (77.26%) (31744/41088)\n",
      "Epoch: 22 | Batch_idx: 330 |  Loss_1: (0.6583) | Acc_1: (77.25%) (32729/42368)\n",
      "Epoch: 22 | Batch_idx: 340 |  Loss_1: (0.6577) | Acc_1: (77.26%) (33723/43648)\n",
      "Epoch: 22 | Batch_idx: 350 |  Loss_1: (0.6570) | Acc_1: (77.29%) (34723/44928)\n",
      "Epoch: 22 | Batch_idx: 360 |  Loss_1: (0.6574) | Acc_1: (77.27%) (35707/46208)\n",
      "Epoch: 22 | Batch_idx: 370 |  Loss_1: (0.6586) | Acc_1: (77.26%) (36690/47488)\n",
      "Epoch: 22 | Batch_idx: 380 |  Loss_1: (0.6585) | Acc_1: (77.28%) (37686/48768)\n",
      "Epoch: 22 | Batch_idx: 390 |  Loss_1: (0.6597) | Acc_1: (77.25%) (38623/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5805) | Acc: (80.79%) (8079/10000)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.7719) | Acc_1: (69.53%) (89/128)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.6960) | Acc_1: (75.99%) (1070/1408)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.6515) | Acc_1: (77.16%) (2074/2688)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.6426) | Acc_1: (77.60%) (3079/3968)\n",
      "Epoch: 23 | Batch_idx: 40 |  Loss_1: (0.6389) | Acc_1: (77.69%) (4077/5248)\n",
      "Epoch: 23 | Batch_idx: 50 |  Loss_1: (0.6355) | Acc_1: (77.74%) (5075/6528)\n",
      "Epoch: 23 | Batch_idx: 60 |  Loss_1: (0.6359) | Acc_1: (77.65%) (6063/7808)\n",
      "Epoch: 23 | Batch_idx: 70 |  Loss_1: (0.6324) | Acc_1: (77.90%) (7080/9088)\n",
      "Epoch: 23 | Batch_idx: 80 |  Loss_1: (0.6283) | Acc_1: (78.03%) (8090/10368)\n",
      "Epoch: 23 | Batch_idx: 90 |  Loss_1: (0.6324) | Acc_1: (77.98%) (9083/11648)\n",
      "Epoch: 23 | Batch_idx: 100 |  Loss_1: (0.6326) | Acc_1: (77.92%) (10073/12928)\n",
      "Epoch: 23 | Batch_idx: 110 |  Loss_1: (0.6294) | Acc_1: (78.05%) (11090/14208)\n",
      "Epoch: 23 | Batch_idx: 120 |  Loss_1: (0.6319) | Acc_1: (77.97%) (12076/15488)\n",
      "Epoch: 23 | Batch_idx: 130 |  Loss_1: (0.6332) | Acc_1: (77.96%) (13072/16768)\n",
      "Epoch: 23 | Batch_idx: 140 |  Loss_1: (0.6345) | Acc_1: (77.89%) (14058/18048)\n",
      "Epoch: 23 | Batch_idx: 150 |  Loss_1: (0.6341) | Acc_1: (77.90%) (15057/19328)\n",
      "Epoch: 23 | Batch_idx: 160 |  Loss_1: (0.6352) | Acc_1: (77.87%) (16048/20608)\n",
      "Epoch: 23 | Batch_idx: 170 |  Loss_1: (0.6348) | Acc_1: (77.90%) (17050/21888)\n",
      "Epoch: 23 | Batch_idx: 180 |  Loss_1: (0.6351) | Acc_1: (77.89%) (18045/23168)\n",
      "Epoch: 23 | Batch_idx: 190 |  Loss_1: (0.6370) | Acc_1: (77.81%) (19022/24448)\n",
      "Epoch: 23 | Batch_idx: 200 |  Loss_1: (0.6373) | Acc_1: (77.79%) (20013/25728)\n",
      "Epoch: 23 | Batch_idx: 210 |  Loss_1: (0.6405) | Acc_1: (77.71%) (20988/27008)\n",
      "Epoch: 23 | Batch_idx: 220 |  Loss_1: (0.6412) | Acc_1: (77.74%) (21992/28288)\n",
      "Epoch: 23 | Batch_idx: 230 |  Loss_1: (0.6414) | Acc_1: (77.74%) (22987/29568)\n",
      "Epoch: 23 | Batch_idx: 240 |  Loss_1: (0.6416) | Acc_1: (77.78%) (23995/30848)\n",
      "Epoch: 23 | Batch_idx: 250 |  Loss_1: (0.6425) | Acc_1: (77.76%) (24982/32128)\n",
      "Epoch: 23 | Batch_idx: 260 |  Loss_1: (0.6432) | Acc_1: (77.74%) (25971/33408)\n",
      "Epoch: 23 | Batch_idx: 270 |  Loss_1: (0.6427) | Acc_1: (77.72%) (26961/34688)\n",
      "Epoch: 23 | Batch_idx: 280 |  Loss_1: (0.6411) | Acc_1: (77.79%) (27978/35968)\n",
      "Epoch: 23 | Batch_idx: 290 |  Loss_1: (0.6394) | Acc_1: (77.87%) (29005/37248)\n",
      "Epoch: 23 | Batch_idx: 300 |  Loss_1: (0.6393) | Acc_1: (77.85%) (29994/38528)\n",
      "Epoch: 23 | Batch_idx: 310 |  Loss_1: (0.6404) | Acc_1: (77.79%) (30968/39808)\n",
      "Epoch: 23 | Batch_idx: 320 |  Loss_1: (0.6404) | Acc_1: (77.81%) (31972/41088)\n",
      "Epoch: 23 | Batch_idx: 330 |  Loss_1: (0.6409) | Acc_1: (77.82%) (32969/42368)\n",
      "Epoch: 23 | Batch_idx: 340 |  Loss_1: (0.6388) | Acc_1: (77.90%) (34001/43648)\n",
      "Epoch: 23 | Batch_idx: 350 |  Loss_1: (0.6395) | Acc_1: (77.87%) (34984/44928)\n",
      "Epoch: 23 | Batch_idx: 360 |  Loss_1: (0.6402) | Acc_1: (77.84%) (35966/46208)\n",
      "Epoch: 23 | Batch_idx: 370 |  Loss_1: (0.6400) | Acc_1: (77.86%) (36976/47488)\n",
      "Epoch: 23 | Batch_idx: 380 |  Loss_1: (0.6395) | Acc_1: (77.86%) (37972/48768)\n",
      "Epoch: 23 | Batch_idx: 390 |  Loss_1: (0.6392) | Acc_1: (77.89%) (38945/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5122) | Acc: (82.71%) (8271/10000)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.6039) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.6362) | Acc_1: (77.63%) (1093/1408)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.6477) | Acc_1: (77.31%) (2078/2688)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.6385) | Acc_1: (77.67%) (3082/3968)\n",
      "Epoch: 24 | Batch_idx: 40 |  Loss_1: (0.6200) | Acc_1: (78.24%) (4106/5248)\n",
      "Epoch: 24 | Batch_idx: 50 |  Loss_1: (0.6195) | Acc_1: (78.40%) (5118/6528)\n",
      "Epoch: 24 | Batch_idx: 60 |  Loss_1: (0.6227) | Acc_1: (78.37%) (6119/7808)\n",
      "Epoch: 24 | Batch_idx: 70 |  Loss_1: (0.6252) | Acc_1: (78.33%) (7119/9088)\n",
      "Epoch: 24 | Batch_idx: 80 |  Loss_1: (0.6233) | Acc_1: (78.57%) (8146/10368)\n",
      "Epoch: 24 | Batch_idx: 90 |  Loss_1: (0.6257) | Acc_1: (78.55%) (9150/11648)\n",
      "Epoch: 24 | Batch_idx: 100 |  Loss_1: (0.6201) | Acc_1: (78.74%) (10180/12928)\n",
      "Epoch: 24 | Batch_idx: 110 |  Loss_1: (0.6194) | Acc_1: (78.70%) (11181/14208)\n",
      "Epoch: 24 | Batch_idx: 120 |  Loss_1: (0.6187) | Acc_1: (78.69%) (12187/15488)\n",
      "Epoch: 24 | Batch_idx: 130 |  Loss_1: (0.6222) | Acc_1: (78.54%) (13170/16768)\n",
      "Epoch: 24 | Batch_idx: 140 |  Loss_1: (0.6224) | Acc_1: (78.51%) (14170/18048)\n",
      "Epoch: 24 | Batch_idx: 150 |  Loss_1: (0.6215) | Acc_1: (78.50%) (15173/19328)\n",
      "Epoch: 24 | Batch_idx: 160 |  Loss_1: (0.6213) | Acc_1: (78.50%) (16177/20608)\n",
      "Epoch: 24 | Batch_idx: 170 |  Loss_1: (0.6209) | Acc_1: (78.57%) (17198/21888)\n",
      "Epoch: 24 | Batch_idx: 180 |  Loss_1: (0.6202) | Acc_1: (78.58%) (18205/23168)\n",
      "Epoch: 24 | Batch_idx: 190 |  Loss_1: (0.6211) | Acc_1: (78.60%) (19217/24448)\n",
      "Epoch: 24 | Batch_idx: 200 |  Loss_1: (0.6218) | Acc_1: (78.56%) (20211/25728)\n",
      "Epoch: 24 | Batch_idx: 210 |  Loss_1: (0.6236) | Acc_1: (78.48%) (21197/27008)\n",
      "Epoch: 24 | Batch_idx: 220 |  Loss_1: (0.6248) | Acc_1: (78.45%) (22192/28288)\n",
      "Epoch: 24 | Batch_idx: 230 |  Loss_1: (0.6240) | Acc_1: (78.47%) (23201/29568)\n",
      "Epoch: 24 | Batch_idx: 240 |  Loss_1: (0.6229) | Acc_1: (78.54%) (24227/30848)\n",
      "Epoch: 24 | Batch_idx: 250 |  Loss_1: (0.6223) | Acc_1: (78.56%) (25240/32128)\n",
      "Epoch: 24 | Batch_idx: 260 |  Loss_1: (0.6232) | Acc_1: (78.52%) (26233/33408)\n",
      "Epoch: 24 | Batch_idx: 270 |  Loss_1: (0.6226) | Acc_1: (78.51%) (27234/34688)\n",
      "Epoch: 24 | Batch_idx: 280 |  Loss_1: (0.6228) | Acc_1: (78.52%) (28241/35968)\n",
      "Epoch: 24 | Batch_idx: 290 |  Loss_1: (0.6225) | Acc_1: (78.53%) (29250/37248)\n",
      "Epoch: 24 | Batch_idx: 300 |  Loss_1: (0.6212) | Acc_1: (78.55%) (30263/38528)\n",
      "Epoch: 24 | Batch_idx: 310 |  Loss_1: (0.6210) | Acc_1: (78.55%) (31271/39808)\n",
      "Epoch: 24 | Batch_idx: 320 |  Loss_1: (0.6209) | Acc_1: (78.56%) (32278/41088)\n",
      "Epoch: 24 | Batch_idx: 330 |  Loss_1: (0.6201) | Acc_1: (78.62%) (33308/42368)\n",
      "Epoch: 24 | Batch_idx: 340 |  Loss_1: (0.6197) | Acc_1: (78.64%) (34324/43648)\n",
      "Epoch: 24 | Batch_idx: 350 |  Loss_1: (0.6222) | Acc_1: (78.57%) (35299/44928)\n",
      "Epoch: 24 | Batch_idx: 360 |  Loss_1: (0.6229) | Acc_1: (78.55%) (36296/46208)\n",
      "Epoch: 24 | Batch_idx: 370 |  Loss_1: (0.6241) | Acc_1: (78.53%) (37294/47488)\n",
      "Epoch: 24 | Batch_idx: 380 |  Loss_1: (0.6242) | Acc_1: (78.54%) (38302/48768)\n",
      "Epoch: 24 | Batch_idx: 390 |  Loss_1: (0.6231) | Acc_1: (78.57%) (39287/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5157) | Acc: (82.41%) (8241/10000)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.5311) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.6432) | Acc_1: (77.56%) (1092/1408)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.6352) | Acc_1: (78.65%) (2114/2688)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.6214) | Acc_1: (79.03%) (3136/3968)\n",
      "Epoch: 25 | Batch_idx: 40 |  Loss_1: (0.6217) | Acc_1: (78.64%) (4127/5248)\n",
      "Epoch: 25 | Batch_idx: 50 |  Loss_1: (0.6184) | Acc_1: (78.92%) (5152/6528)\n",
      "Epoch: 25 | Batch_idx: 60 |  Loss_1: (0.6078) | Acc_1: (79.30%) (6192/7808)\n",
      "Epoch: 25 | Batch_idx: 70 |  Loss_1: (0.6129) | Acc_1: (79.14%) (7192/9088)\n",
      "Epoch: 25 | Batch_idx: 80 |  Loss_1: (0.6160) | Acc_1: (78.98%) (8189/10368)\n",
      "Epoch: 25 | Batch_idx: 90 |  Loss_1: (0.6141) | Acc_1: (78.91%) (9191/11648)\n",
      "Epoch: 25 | Batch_idx: 100 |  Loss_1: (0.6100) | Acc_1: (78.99%) (10212/12928)\n",
      "Epoch: 25 | Batch_idx: 110 |  Loss_1: (0.6078) | Acc_1: (79.09%) (11237/14208)\n",
      "Epoch: 25 | Batch_idx: 120 |  Loss_1: (0.6071) | Acc_1: (79.07%) (12247/15488)\n",
      "Epoch: 25 | Batch_idx: 130 |  Loss_1: (0.6068) | Acc_1: (79.14%) (13271/16768)\n",
      "Epoch: 25 | Batch_idx: 140 |  Loss_1: (0.6080) | Acc_1: (79.07%) (14271/18048)\n",
      "Epoch: 25 | Batch_idx: 150 |  Loss_1: (0.6094) | Acc_1: (79.03%) (15275/19328)\n",
      "Epoch: 25 | Batch_idx: 160 |  Loss_1: (0.6093) | Acc_1: (78.92%) (16263/20608)\n",
      "Epoch: 25 | Batch_idx: 170 |  Loss_1: (0.6072) | Acc_1: (79.01%) (17293/21888)\n",
      "Epoch: 25 | Batch_idx: 180 |  Loss_1: (0.6082) | Acc_1: (78.96%) (18294/23168)\n",
      "Epoch: 25 | Batch_idx: 190 |  Loss_1: (0.6070) | Acc_1: (78.99%) (19312/24448)\n",
      "Epoch: 25 | Batch_idx: 200 |  Loss_1: (0.6077) | Acc_1: (78.96%) (20315/25728)\n",
      "Epoch: 25 | Batch_idx: 210 |  Loss_1: (0.6068) | Acc_1: (78.99%) (21334/27008)\n",
      "Epoch: 25 | Batch_idx: 220 |  Loss_1: (0.6052) | Acc_1: (79.07%) (22366/28288)\n",
      "Epoch: 25 | Batch_idx: 230 |  Loss_1: (0.6024) | Acc_1: (79.18%) (23412/29568)\n",
      "Epoch: 25 | Batch_idx: 240 |  Loss_1: (0.6035) | Acc_1: (79.13%) (24410/30848)\n",
      "Epoch: 25 | Batch_idx: 250 |  Loss_1: (0.6044) | Acc_1: (79.10%) (25412/32128)\n",
      "Epoch: 25 | Batch_idx: 260 |  Loss_1: (0.6034) | Acc_1: (79.14%) (26440/33408)\n",
      "Epoch: 25 | Batch_idx: 270 |  Loss_1: (0.6036) | Acc_1: (79.14%) (27453/34688)\n",
      "Epoch: 25 | Batch_idx: 280 |  Loss_1: (0.6046) | Acc_1: (79.12%) (28457/35968)\n",
      "Epoch: 25 | Batch_idx: 290 |  Loss_1: (0.6045) | Acc_1: (79.09%) (29461/37248)\n",
      "Epoch: 25 | Batch_idx: 300 |  Loss_1: (0.6044) | Acc_1: (79.10%) (30474/38528)\n",
      "Epoch: 25 | Batch_idx: 310 |  Loss_1: (0.6046) | Acc_1: (79.07%) (31477/39808)\n",
      "Epoch: 25 | Batch_idx: 320 |  Loss_1: (0.6046) | Acc_1: (79.06%) (32485/41088)\n",
      "Epoch: 25 | Batch_idx: 330 |  Loss_1: (0.6053) | Acc_1: (79.04%) (33486/42368)\n",
      "Epoch: 25 | Batch_idx: 340 |  Loss_1: (0.6064) | Acc_1: (79.00%) (34480/43648)\n",
      "Epoch: 25 | Batch_idx: 350 |  Loss_1: (0.6071) | Acc_1: (78.98%) (35483/44928)\n",
      "Epoch: 25 | Batch_idx: 360 |  Loss_1: (0.6058) | Acc_1: (79.04%) (36521/46208)\n",
      "Epoch: 25 | Batch_idx: 370 |  Loss_1: (0.6049) | Acc_1: (79.07%) (37551/47488)\n",
      "Epoch: 25 | Batch_idx: 380 |  Loss_1: (0.6059) | Acc_1: (79.06%) (38556/48768)\n",
      "Epoch: 25 | Batch_idx: 390 |  Loss_1: (0.6060) | Acc_1: (79.07%) (39533/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4426) | Acc: (85.15%) (8515/10000)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.5653) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.5760) | Acc_1: (79.83%) (1124/1408)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.5686) | Acc_1: (79.72%) (2143/2688)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.5690) | Acc_1: (80.09%) (3178/3968)\n",
      "Epoch: 26 | Batch_idx: 40 |  Loss_1: (0.5715) | Acc_1: (80.28%) (4213/5248)\n",
      "Epoch: 26 | Batch_idx: 50 |  Loss_1: (0.5773) | Acc_1: (80.36%) (5246/6528)\n",
      "Epoch: 26 | Batch_idx: 60 |  Loss_1: (0.5778) | Acc_1: (80.23%) (6264/7808)\n",
      "Epoch: 26 | Batch_idx: 70 |  Loss_1: (0.5822) | Acc_1: (80.04%) (7274/9088)\n",
      "Epoch: 26 | Batch_idx: 80 |  Loss_1: (0.5779) | Acc_1: (80.18%) (8313/10368)\n",
      "Epoch: 26 | Batch_idx: 90 |  Loss_1: (0.5793) | Acc_1: (80.02%) (9321/11648)\n",
      "Epoch: 26 | Batch_idx: 100 |  Loss_1: (0.5765) | Acc_1: (80.13%) (10359/12928)\n",
      "Epoch: 26 | Batch_idx: 110 |  Loss_1: (0.5790) | Acc_1: (80.05%) (11373/14208)\n",
      "Epoch: 26 | Batch_idx: 120 |  Loss_1: (0.5827) | Acc_1: (79.78%) (12357/15488)\n",
      "Epoch: 26 | Batch_idx: 130 |  Loss_1: (0.5856) | Acc_1: (79.69%) (13362/16768)\n",
      "Epoch: 26 | Batch_idx: 140 |  Loss_1: (0.5860) | Acc_1: (79.64%) (14374/18048)\n",
      "Epoch: 26 | Batch_idx: 150 |  Loss_1: (0.5832) | Acc_1: (79.74%) (15413/19328)\n",
      "Epoch: 26 | Batch_idx: 160 |  Loss_1: (0.5845) | Acc_1: (79.71%) (16427/20608)\n",
      "Epoch: 26 | Batch_idx: 170 |  Loss_1: (0.5834) | Acc_1: (79.83%) (17474/21888)\n",
      "Epoch: 26 | Batch_idx: 180 |  Loss_1: (0.5843) | Acc_1: (79.76%) (18478/23168)\n",
      "Epoch: 26 | Batch_idx: 190 |  Loss_1: (0.5827) | Acc_1: (79.84%) (19519/24448)\n",
      "Epoch: 26 | Batch_idx: 200 |  Loss_1: (0.5838) | Acc_1: (79.82%) (20537/25728)\n",
      "Epoch: 26 | Batch_idx: 210 |  Loss_1: (0.5847) | Acc_1: (79.81%) (21554/27008)\n",
      "Epoch: 26 | Batch_idx: 220 |  Loss_1: (0.5841) | Acc_1: (79.84%) (22584/28288)\n",
      "Epoch: 26 | Batch_idx: 230 |  Loss_1: (0.5847) | Acc_1: (79.85%) (23609/29568)\n",
      "Epoch: 26 | Batch_idx: 240 |  Loss_1: (0.5861) | Acc_1: (79.77%) (24607/30848)\n",
      "Epoch: 26 | Batch_idx: 250 |  Loss_1: (0.5865) | Acc_1: (79.76%) (25624/32128)\n",
      "Epoch: 26 | Batch_idx: 260 |  Loss_1: (0.5874) | Acc_1: (79.73%) (26635/33408)\n",
      "Epoch: 26 | Batch_idx: 270 |  Loss_1: (0.5896) | Acc_1: (79.63%) (27623/34688)\n",
      "Epoch: 26 | Batch_idx: 280 |  Loss_1: (0.5885) | Acc_1: (79.69%) (28662/35968)\n",
      "Epoch: 26 | Batch_idx: 290 |  Loss_1: (0.5896) | Acc_1: (79.65%) (29669/37248)\n",
      "Epoch: 26 | Batch_idx: 300 |  Loss_1: (0.5893) | Acc_1: (79.62%) (30677/38528)\n",
      "Epoch: 26 | Batch_idx: 310 |  Loss_1: (0.5896) | Acc_1: (79.60%) (31689/39808)\n",
      "Epoch: 26 | Batch_idx: 320 |  Loss_1: (0.5901) | Acc_1: (79.61%) (32709/41088)\n",
      "Epoch: 26 | Batch_idx: 330 |  Loss_1: (0.5909) | Acc_1: (79.58%) (33718/42368)\n",
      "Epoch: 26 | Batch_idx: 340 |  Loss_1: (0.5915) | Acc_1: (79.54%) (34716/43648)\n",
      "Epoch: 26 | Batch_idx: 350 |  Loss_1: (0.5902) | Acc_1: (79.58%) (35754/44928)\n",
      "Epoch: 26 | Batch_idx: 360 |  Loss_1: (0.5899) | Acc_1: (79.58%) (36772/46208)\n",
      "Epoch: 26 | Batch_idx: 370 |  Loss_1: (0.5908) | Acc_1: (79.53%) (37768/47488)\n",
      "Epoch: 26 | Batch_idx: 380 |  Loss_1: (0.5904) | Acc_1: (79.54%) (38790/48768)\n",
      "Epoch: 26 | Batch_idx: 390 |  Loss_1: (0.5908) | Acc_1: (79.55%) (39775/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5798) | Acc: (82.57%) (8257/10000)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.6086) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.5479) | Acc_1: (80.61%) (1135/1408)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.5557) | Acc_1: (80.21%) (2156/2688)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.5773) | Acc_1: (79.51%) (3155/3968)\n",
      "Epoch: 27 | Batch_idx: 40 |  Loss_1: (0.5792) | Acc_1: (79.69%) (4182/5248)\n",
      "Epoch: 27 | Batch_idx: 50 |  Loss_1: (0.5771) | Acc_1: (79.72%) (5204/6528)\n",
      "Epoch: 27 | Batch_idx: 60 |  Loss_1: (0.5730) | Acc_1: (80.07%) (6252/7808)\n",
      "Epoch: 27 | Batch_idx: 70 |  Loss_1: (0.5675) | Acc_1: (80.42%) (7309/9088)\n",
      "Epoch: 27 | Batch_idx: 80 |  Loss_1: (0.5741) | Acc_1: (80.27%) (8322/10368)\n",
      "Epoch: 27 | Batch_idx: 90 |  Loss_1: (0.5757) | Acc_1: (80.19%) (9341/11648)\n",
      "Epoch: 27 | Batch_idx: 100 |  Loss_1: (0.5816) | Acc_1: (80.04%) (10347/12928)\n",
      "Epoch: 27 | Batch_idx: 110 |  Loss_1: (0.5807) | Acc_1: (80.05%) (11374/14208)\n",
      "Epoch: 27 | Batch_idx: 120 |  Loss_1: (0.5800) | Acc_1: (80.05%) (12398/15488)\n",
      "Epoch: 27 | Batch_idx: 130 |  Loss_1: (0.5821) | Acc_1: (79.96%) (13407/16768)\n",
      "Epoch: 27 | Batch_idx: 140 |  Loss_1: (0.5810) | Acc_1: (79.96%) (14431/18048)\n",
      "Epoch: 27 | Batch_idx: 150 |  Loss_1: (0.5782) | Acc_1: (80.02%) (15466/19328)\n",
      "Epoch: 27 | Batch_idx: 160 |  Loss_1: (0.5759) | Acc_1: (80.13%) (16514/20608)\n",
      "Epoch: 27 | Batch_idx: 170 |  Loss_1: (0.5766) | Acc_1: (80.15%) (17544/21888)\n",
      "Epoch: 27 | Batch_idx: 180 |  Loss_1: (0.5746) | Acc_1: (80.26%) (18595/23168)\n",
      "Epoch: 27 | Batch_idx: 190 |  Loss_1: (0.5720) | Acc_1: (80.34%) (19641/24448)\n",
      "Epoch: 27 | Batch_idx: 200 |  Loss_1: (0.5744) | Acc_1: (80.25%) (20647/25728)\n",
      "Epoch: 27 | Batch_idx: 210 |  Loss_1: (0.5738) | Acc_1: (80.32%) (21694/27008)\n",
      "Epoch: 27 | Batch_idx: 220 |  Loss_1: (0.5719) | Acc_1: (80.39%) (22742/28288)\n",
      "Epoch: 27 | Batch_idx: 230 |  Loss_1: (0.5715) | Acc_1: (80.43%) (23782/29568)\n",
      "Epoch: 27 | Batch_idx: 240 |  Loss_1: (0.5711) | Acc_1: (80.44%) (24813/30848)\n",
      "Epoch: 27 | Batch_idx: 250 |  Loss_1: (0.5730) | Acc_1: (80.40%) (25830/32128)\n",
      "Epoch: 27 | Batch_idx: 260 |  Loss_1: (0.5745) | Acc_1: (80.35%) (26844/33408)\n",
      "Epoch: 27 | Batch_idx: 270 |  Loss_1: (0.5750) | Acc_1: (80.36%) (27876/34688)\n",
      "Epoch: 27 | Batch_idx: 280 |  Loss_1: (0.5749) | Acc_1: (80.36%) (28903/35968)\n",
      "Epoch: 27 | Batch_idx: 290 |  Loss_1: (0.5729) | Acc_1: (80.42%) (29954/37248)\n",
      "Epoch: 27 | Batch_idx: 300 |  Loss_1: (0.5736) | Acc_1: (80.34%) (30955/38528)\n",
      "Epoch: 27 | Batch_idx: 310 |  Loss_1: (0.5742) | Acc_1: (80.32%) (31972/39808)\n",
      "Epoch: 27 | Batch_idx: 320 |  Loss_1: (0.5756) | Acc_1: (80.27%) (32983/41088)\n",
      "Epoch: 27 | Batch_idx: 330 |  Loss_1: (0.5740) | Acc_1: (80.33%) (34036/42368)\n",
      "Epoch: 27 | Batch_idx: 340 |  Loss_1: (0.5739) | Acc_1: (80.34%) (35067/43648)\n",
      "Epoch: 27 | Batch_idx: 350 |  Loss_1: (0.5737) | Acc_1: (80.33%) (36092/44928)\n",
      "Epoch: 27 | Batch_idx: 360 |  Loss_1: (0.5719) | Acc_1: (80.35%) (37129/46208)\n",
      "Epoch: 27 | Batch_idx: 370 |  Loss_1: (0.5711) | Acc_1: (80.41%) (38185/47488)\n",
      "Epoch: 27 | Batch_idx: 380 |  Loss_1: (0.5708) | Acc_1: (80.43%) (39222/48768)\n",
      "Epoch: 27 | Batch_idx: 390 |  Loss_1: (0.5709) | Acc_1: (80.41%) (40205/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4345) | Acc: (84.54%) (8454/10000)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.5135) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.5393) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.5419) | Acc_1: (81.62%) (2194/2688)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.5408) | Acc_1: (81.25%) (3224/3968)\n",
      "Epoch: 28 | Batch_idx: 40 |  Loss_1: (0.5550) | Acc_1: (80.79%) (4240/5248)\n",
      "Epoch: 28 | Batch_idx: 50 |  Loss_1: (0.5488) | Acc_1: (80.96%) (5285/6528)\n",
      "Epoch: 28 | Batch_idx: 60 |  Loss_1: (0.5555) | Acc_1: (80.65%) (6297/7808)\n",
      "Epoch: 28 | Batch_idx: 70 |  Loss_1: (0.5551) | Acc_1: (80.58%) (7323/9088)\n",
      "Epoch: 28 | Batch_idx: 80 |  Loss_1: (0.5582) | Acc_1: (80.57%) (8353/10368)\n",
      "Epoch: 28 | Batch_idx: 90 |  Loss_1: (0.5581) | Acc_1: (80.50%) (9377/11648)\n",
      "Epoch: 28 | Batch_idx: 100 |  Loss_1: (0.5594) | Acc_1: (80.56%) (10415/12928)\n",
      "Epoch: 28 | Batch_idx: 110 |  Loss_1: (0.5576) | Acc_1: (80.63%) (11456/14208)\n",
      "Epoch: 28 | Batch_idx: 120 |  Loss_1: (0.5600) | Acc_1: (80.53%) (12473/15488)\n",
      "Epoch: 28 | Batch_idx: 130 |  Loss_1: (0.5559) | Acc_1: (80.66%) (13525/16768)\n",
      "Epoch: 28 | Batch_idx: 140 |  Loss_1: (0.5557) | Acc_1: (80.67%) (14560/18048)\n",
      "Epoch: 28 | Batch_idx: 150 |  Loss_1: (0.5534) | Acc_1: (80.74%) (15606/19328)\n",
      "Epoch: 28 | Batch_idx: 160 |  Loss_1: (0.5540) | Acc_1: (80.81%) (16654/20608)\n",
      "Epoch: 28 | Batch_idx: 170 |  Loss_1: (0.5587) | Acc_1: (80.67%) (17658/21888)\n",
      "Epoch: 28 | Batch_idx: 180 |  Loss_1: (0.5599) | Acc_1: (80.62%) (18679/23168)\n",
      "Epoch: 28 | Batch_idx: 190 |  Loss_1: (0.5594) | Acc_1: (80.65%) (19717/24448)\n",
      "Epoch: 28 | Batch_idx: 200 |  Loss_1: (0.5594) | Acc_1: (80.66%) (20751/25728)\n",
      "Epoch: 28 | Batch_idx: 210 |  Loss_1: (0.5599) | Acc_1: (80.65%) (21781/27008)\n",
      "Epoch: 28 | Batch_idx: 220 |  Loss_1: (0.5604) | Acc_1: (80.59%) (22797/28288)\n",
      "Epoch: 28 | Batch_idx: 230 |  Loss_1: (0.5609) | Acc_1: (80.50%) (23802/29568)\n",
      "Epoch: 28 | Batch_idx: 240 |  Loss_1: (0.5609) | Acc_1: (80.47%) (24822/30848)\n",
      "Epoch: 28 | Batch_idx: 250 |  Loss_1: (0.5592) | Acc_1: (80.51%) (25865/32128)\n",
      "Epoch: 28 | Batch_idx: 260 |  Loss_1: (0.5596) | Acc_1: (80.51%) (26896/33408)\n",
      "Epoch: 28 | Batch_idx: 270 |  Loss_1: (0.5583) | Acc_1: (80.56%) (27946/34688)\n",
      "Epoch: 28 | Batch_idx: 280 |  Loss_1: (0.5593) | Acc_1: (80.54%) (28969/35968)\n",
      "Epoch: 28 | Batch_idx: 290 |  Loss_1: (0.5582) | Acc_1: (80.59%) (30019/37248)\n",
      "Epoch: 28 | Batch_idx: 300 |  Loss_1: (0.5574) | Acc_1: (80.62%) (31062/38528)\n",
      "Epoch: 28 | Batch_idx: 310 |  Loss_1: (0.5568) | Acc_1: (80.65%) (32104/39808)\n",
      "Epoch: 28 | Batch_idx: 320 |  Loss_1: (0.5573) | Acc_1: (80.62%) (33127/41088)\n",
      "Epoch: 28 | Batch_idx: 330 |  Loss_1: (0.5568) | Acc_1: (80.63%) (34161/42368)\n",
      "Epoch: 28 | Batch_idx: 340 |  Loss_1: (0.5553) | Acc_1: (80.72%) (35231/43648)\n",
      "Epoch: 28 | Batch_idx: 350 |  Loss_1: (0.5571) | Acc_1: (80.69%) (36252/44928)\n",
      "Epoch: 28 | Batch_idx: 360 |  Loss_1: (0.5573) | Acc_1: (80.65%) (37268/46208)\n",
      "Epoch: 28 | Batch_idx: 370 |  Loss_1: (0.5581) | Acc_1: (80.62%) (38287/47488)\n",
      "Epoch: 28 | Batch_idx: 380 |  Loss_1: (0.5592) | Acc_1: (80.59%) (39303/48768)\n",
      "Epoch: 28 | Batch_idx: 390 |  Loss_1: (0.5583) | Acc_1: (80.62%) (40308/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4199) | Acc: (85.80%) (8580/10000)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.5244) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.5677) | Acc_1: (80.61%) (1135/1408)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.5752) | Acc_1: (80.28%) (2158/2688)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.5497) | Acc_1: (81.05%) (3216/3968)\n",
      "Epoch: 29 | Batch_idx: 40 |  Loss_1: (0.5368) | Acc_1: (81.59%) (4282/5248)\n",
      "Epoch: 29 | Batch_idx: 50 |  Loss_1: (0.5421) | Acc_1: (81.60%) (5327/6528)\n",
      "Epoch: 29 | Batch_idx: 60 |  Loss_1: (0.5491) | Acc_1: (81.15%) (6336/7808)\n",
      "Epoch: 29 | Batch_idx: 70 |  Loss_1: (0.5463) | Acc_1: (81.40%) (7398/9088)\n",
      "Epoch: 29 | Batch_idx: 80 |  Loss_1: (0.5483) | Acc_1: (81.36%) (8435/10368)\n",
      "Epoch: 29 | Batch_idx: 90 |  Loss_1: (0.5483) | Acc_1: (81.36%) (9477/11648)\n",
      "Epoch: 29 | Batch_idx: 100 |  Loss_1: (0.5500) | Acc_1: (81.10%) (10484/12928)\n",
      "Epoch: 29 | Batch_idx: 110 |  Loss_1: (0.5489) | Acc_1: (81.15%) (11530/14208)\n",
      "Epoch: 29 | Batch_idx: 120 |  Loss_1: (0.5492) | Acc_1: (81.15%) (12568/15488)\n",
      "Epoch: 29 | Batch_idx: 130 |  Loss_1: (0.5426) | Acc_1: (81.37%) (13644/16768)\n",
      "Epoch: 29 | Batch_idx: 140 |  Loss_1: (0.5425) | Acc_1: (81.38%) (14688/18048)\n",
      "Epoch: 29 | Batch_idx: 150 |  Loss_1: (0.5417) | Acc_1: (81.48%) (15748/19328)\n",
      "Epoch: 29 | Batch_idx: 160 |  Loss_1: (0.5424) | Acc_1: (81.42%) (16779/20608)\n",
      "Epoch: 29 | Batch_idx: 170 |  Loss_1: (0.5408) | Acc_1: (81.48%) (17834/21888)\n",
      "Epoch: 29 | Batch_idx: 180 |  Loss_1: (0.5403) | Acc_1: (81.49%) (18879/23168)\n",
      "Epoch: 29 | Batch_idx: 190 |  Loss_1: (0.5386) | Acc_1: (81.59%) (19946/24448)\n",
      "Epoch: 29 | Batch_idx: 200 |  Loss_1: (0.5396) | Acc_1: (81.53%) (20975/25728)\n",
      "Epoch: 29 | Batch_idx: 210 |  Loss_1: (0.5395) | Acc_1: (81.48%) (22006/27008)\n",
      "Epoch: 29 | Batch_idx: 220 |  Loss_1: (0.5415) | Acc_1: (81.43%) (23036/28288)\n",
      "Epoch: 29 | Batch_idx: 230 |  Loss_1: (0.5432) | Acc_1: (81.41%) (24072/29568)\n",
      "Epoch: 29 | Batch_idx: 240 |  Loss_1: (0.5441) | Acc_1: (81.35%) (25094/30848)\n",
      "Epoch: 29 | Batch_idx: 250 |  Loss_1: (0.5449) | Acc_1: (81.31%) (26122/32128)\n",
      "Epoch: 29 | Batch_idx: 260 |  Loss_1: (0.5449) | Acc_1: (81.31%) (27163/33408)\n",
      "Epoch: 29 | Batch_idx: 270 |  Loss_1: (0.5457) | Acc_1: (81.27%) (28191/34688)\n",
      "Epoch: 29 | Batch_idx: 280 |  Loss_1: (0.5449) | Acc_1: (81.30%) (29243/35968)\n",
      "Epoch: 29 | Batch_idx: 290 |  Loss_1: (0.5445) | Acc_1: (81.33%) (30293/37248)\n",
      "Epoch: 29 | Batch_idx: 300 |  Loss_1: (0.5433) | Acc_1: (81.36%) (31348/38528)\n",
      "Epoch: 29 | Batch_idx: 310 |  Loss_1: (0.5422) | Acc_1: (81.42%) (32410/39808)\n",
      "Epoch: 29 | Batch_idx: 320 |  Loss_1: (0.5416) | Acc_1: (81.44%) (33462/41088)\n",
      "Epoch: 29 | Batch_idx: 330 |  Loss_1: (0.5408) | Acc_1: (81.45%) (34507/42368)\n",
      "Epoch: 29 | Batch_idx: 340 |  Loss_1: (0.5413) | Acc_1: (81.39%) (35525/43648)\n",
      "Epoch: 29 | Batch_idx: 350 |  Loss_1: (0.5424) | Acc_1: (81.37%) (36559/44928)\n",
      "Epoch: 29 | Batch_idx: 360 |  Loss_1: (0.5423) | Acc_1: (81.36%) (37595/46208)\n",
      "Epoch: 29 | Batch_idx: 370 |  Loss_1: (0.5423) | Acc_1: (81.33%) (38621/47488)\n",
      "Epoch: 29 | Batch_idx: 380 |  Loss_1: (0.5420) | Acc_1: (81.33%) (39663/48768)\n",
      "Epoch: 29 | Batch_idx: 390 |  Loss_1: (0.5417) | Acc_1: (81.32%) (40660/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4136) | Acc: (85.64%) (8564/10000)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.4251) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.5169) | Acc_1: (82.03%) (1155/1408)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.5063) | Acc_1: (82.66%) (2222/2688)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.5031) | Acc_1: (82.69%) (3281/3968)\n",
      "Epoch: 30 | Batch_idx: 40 |  Loss_1: (0.5236) | Acc_1: (82.03%) (4305/5248)\n",
      "Epoch: 30 | Batch_idx: 50 |  Loss_1: (0.5294) | Acc_1: (81.72%) (5335/6528)\n",
      "Epoch: 30 | Batch_idx: 60 |  Loss_1: (0.5297) | Acc_1: (81.54%) (6367/7808)\n",
      "Epoch: 30 | Batch_idx: 70 |  Loss_1: (0.5283) | Acc_1: (81.53%) (7409/9088)\n",
      "Epoch: 30 | Batch_idx: 80 |  Loss_1: (0.5310) | Acc_1: (81.47%) (8447/10368)\n",
      "Epoch: 30 | Batch_idx: 90 |  Loss_1: (0.5345) | Acc_1: (81.26%) (9465/11648)\n",
      "Epoch: 30 | Batch_idx: 100 |  Loss_1: (0.5359) | Acc_1: (81.21%) (10499/12928)\n",
      "Epoch: 30 | Batch_idx: 110 |  Loss_1: (0.5337) | Acc_1: (81.36%) (11559/14208)\n",
      "Epoch: 30 | Batch_idx: 120 |  Loss_1: (0.5343) | Acc_1: (81.35%) (12600/15488)\n",
      "Epoch: 30 | Batch_idx: 130 |  Loss_1: (0.5320) | Acc_1: (81.42%) (13653/16768)\n",
      "Epoch: 30 | Batch_idx: 140 |  Loss_1: (0.5298) | Acc_1: (81.51%) (14711/18048)\n",
      "Epoch: 30 | Batch_idx: 150 |  Loss_1: (0.5301) | Acc_1: (81.53%) (15759/19328)\n",
      "Epoch: 30 | Batch_idx: 160 |  Loss_1: (0.5290) | Acc_1: (81.52%) (16800/20608)\n",
      "Epoch: 30 | Batch_idx: 170 |  Loss_1: (0.5310) | Acc_1: (81.43%) (17824/21888)\n",
      "Epoch: 30 | Batch_idx: 180 |  Loss_1: (0.5285) | Acc_1: (81.54%) (18892/23168)\n",
      "Epoch: 30 | Batch_idx: 190 |  Loss_1: (0.5279) | Acc_1: (81.56%) (19941/24448)\n",
      "Epoch: 30 | Batch_idx: 200 |  Loss_1: (0.5274) | Acc_1: (81.62%) (21000/25728)\n",
      "Epoch: 30 | Batch_idx: 210 |  Loss_1: (0.5277) | Acc_1: (81.61%) (22041/27008)\n",
      "Epoch: 30 | Batch_idx: 220 |  Loss_1: (0.5283) | Acc_1: (81.61%) (23086/28288)\n",
      "Epoch: 30 | Batch_idx: 230 |  Loss_1: (0.5278) | Acc_1: (81.63%) (24135/29568)\n",
      "Epoch: 30 | Batch_idx: 240 |  Loss_1: (0.5279) | Acc_1: (81.59%) (25168/30848)\n",
      "Epoch: 30 | Batch_idx: 250 |  Loss_1: (0.5268) | Acc_1: (81.65%) (26233/32128)\n",
      "Epoch: 30 | Batch_idx: 260 |  Loss_1: (0.5276) | Acc_1: (81.60%) (27262/33408)\n",
      "Epoch: 30 | Batch_idx: 270 |  Loss_1: (0.5284) | Acc_1: (81.59%) (28301/34688)\n",
      "Epoch: 30 | Batch_idx: 280 |  Loss_1: (0.5294) | Acc_1: (81.56%) (29335/35968)\n",
      "Epoch: 30 | Batch_idx: 290 |  Loss_1: (0.5280) | Acc_1: (81.57%) (30383/37248)\n",
      "Epoch: 30 | Batch_idx: 300 |  Loss_1: (0.5276) | Acc_1: (81.57%) (31426/38528)\n",
      "Epoch: 30 | Batch_idx: 310 |  Loss_1: (0.5286) | Acc_1: (81.50%) (32443/39808)\n",
      "Epoch: 30 | Batch_idx: 320 |  Loss_1: (0.5292) | Acc_1: (81.48%) (33480/41088)\n",
      "Epoch: 30 | Batch_idx: 330 |  Loss_1: (0.5286) | Acc_1: (81.50%) (34532/42368)\n",
      "Epoch: 30 | Batch_idx: 340 |  Loss_1: (0.5275) | Acc_1: (81.57%) (35603/43648)\n",
      "Epoch: 30 | Batch_idx: 350 |  Loss_1: (0.5269) | Acc_1: (81.61%) (36667/44928)\n",
      "Epoch: 30 | Batch_idx: 360 |  Loss_1: (0.5259) | Acc_1: (81.66%) (37734/46208)\n",
      "Epoch: 30 | Batch_idx: 370 |  Loss_1: (0.5263) | Acc_1: (81.67%) (38784/47488)\n",
      "Epoch: 30 | Batch_idx: 380 |  Loss_1: (0.5270) | Acc_1: (81.65%) (39818/48768)\n",
      "Epoch: 30 | Batch_idx: 390 |  Loss_1: (0.5272) | Acc_1: (81.66%) (40831/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4160) | Acc: (85.27%) (8527/10000)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.5935) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.5497) | Acc_1: (80.75%) (1137/1408)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.5222) | Acc_1: (81.73%) (2197/2688)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.5228) | Acc_1: (81.70%) (3242/3968)\n",
      "Epoch: 31 | Batch_idx: 40 |  Loss_1: (0.5126) | Acc_1: (81.97%) (4302/5248)\n",
      "Epoch: 31 | Batch_idx: 50 |  Loss_1: (0.5115) | Acc_1: (82.20%) (5366/6528)\n",
      "Epoch: 31 | Batch_idx: 60 |  Loss_1: (0.5102) | Acc_1: (82.27%) (6424/7808)\n",
      "Epoch: 31 | Batch_idx: 70 |  Loss_1: (0.5112) | Acc_1: (82.23%) (7473/9088)\n",
      "Epoch: 31 | Batch_idx: 80 |  Loss_1: (0.5103) | Acc_1: (82.32%) (8535/10368)\n",
      "Epoch: 31 | Batch_idx: 90 |  Loss_1: (0.5083) | Acc_1: (82.40%) (9598/11648)\n",
      "Epoch: 31 | Batch_idx: 100 |  Loss_1: (0.5139) | Acc_1: (82.19%) (10625/12928)\n",
      "Epoch: 31 | Batch_idx: 110 |  Loss_1: (0.5175) | Acc_1: (82.00%) (11650/14208)\n",
      "Epoch: 31 | Batch_idx: 120 |  Loss_1: (0.5200) | Acc_1: (81.95%) (12693/15488)\n",
      "Epoch: 31 | Batch_idx: 130 |  Loss_1: (0.5190) | Acc_1: (82.04%) (13756/16768)\n",
      "Epoch: 31 | Batch_idx: 140 |  Loss_1: (0.5204) | Acc_1: (82.05%) (14809/18048)\n",
      "Epoch: 31 | Batch_idx: 150 |  Loss_1: (0.5182) | Acc_1: (82.11%) (15871/19328)\n",
      "Epoch: 31 | Batch_idx: 160 |  Loss_1: (0.5166) | Acc_1: (82.13%) (16925/20608)\n",
      "Epoch: 31 | Batch_idx: 170 |  Loss_1: (0.5174) | Acc_1: (82.05%) (17960/21888)\n",
      "Epoch: 31 | Batch_idx: 180 |  Loss_1: (0.5195) | Acc_1: (81.98%) (18992/23168)\n",
      "Epoch: 31 | Batch_idx: 190 |  Loss_1: (0.5194) | Acc_1: (81.94%) (20032/24448)\n",
      "Epoch: 31 | Batch_idx: 200 |  Loss_1: (0.5183) | Acc_1: (81.98%) (21093/25728)\n",
      "Epoch: 31 | Batch_idx: 210 |  Loss_1: (0.5154) | Acc_1: (82.07%) (22165/27008)\n",
      "Epoch: 31 | Batch_idx: 220 |  Loss_1: (0.5161) | Acc_1: (82.05%) (23209/28288)\n",
      "Epoch: 31 | Batch_idx: 230 |  Loss_1: (0.5136) | Acc_1: (82.10%) (24276/29568)\n",
      "Epoch: 31 | Batch_idx: 240 |  Loss_1: (0.5140) | Acc_1: (82.06%) (25313/30848)\n",
      "Epoch: 31 | Batch_idx: 250 |  Loss_1: (0.5137) | Acc_1: (82.03%) (26355/32128)\n",
      "Epoch: 31 | Batch_idx: 260 |  Loss_1: (0.5114) | Acc_1: (82.11%) (27432/33408)\n",
      "Epoch: 31 | Batch_idx: 270 |  Loss_1: (0.5119) | Acc_1: (82.11%) (28482/34688)\n",
      "Epoch: 31 | Batch_idx: 280 |  Loss_1: (0.5116) | Acc_1: (82.08%) (29524/35968)\n",
      "Epoch: 31 | Batch_idx: 290 |  Loss_1: (0.5129) | Acc_1: (82.07%) (30569/37248)\n",
      "Epoch: 31 | Batch_idx: 300 |  Loss_1: (0.5125) | Acc_1: (82.08%) (31624/38528)\n",
      "Epoch: 31 | Batch_idx: 310 |  Loss_1: (0.5131) | Acc_1: (82.07%) (32672/39808)\n",
      "Epoch: 31 | Batch_idx: 320 |  Loss_1: (0.5135) | Acc_1: (82.06%) (33715/41088)\n",
      "Epoch: 31 | Batch_idx: 330 |  Loss_1: (0.5145) | Acc_1: (82.01%) (34747/42368)\n",
      "Epoch: 31 | Batch_idx: 340 |  Loss_1: (0.5153) | Acc_1: (81.99%) (35788/43648)\n",
      "Epoch: 31 | Batch_idx: 350 |  Loss_1: (0.5163) | Acc_1: (81.94%) (36815/44928)\n",
      "Epoch: 31 | Batch_idx: 360 |  Loss_1: (0.5171) | Acc_1: (81.93%) (37857/46208)\n",
      "Epoch: 31 | Batch_idx: 370 |  Loss_1: (0.5171) | Acc_1: (81.89%) (38890/47488)\n",
      "Epoch: 31 | Batch_idx: 380 |  Loss_1: (0.5177) | Acc_1: (81.85%) (39917/48768)\n",
      "Epoch: 31 | Batch_idx: 390 |  Loss_1: (0.5171) | Acc_1: (81.87%) (40937/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4108) | Acc: (85.96%) (8596/10000)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.4792) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.4575) | Acc_1: (83.52%) (1176/1408)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.4773) | Acc_1: (83.11%) (2234/2688)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.4687) | Acc_1: (83.72%) (3322/3968)\n",
      "Epoch: 32 | Batch_idx: 40 |  Loss_1: (0.4726) | Acc_1: (83.52%) (4383/5248)\n",
      "Epoch: 32 | Batch_idx: 50 |  Loss_1: (0.4767) | Acc_1: (83.30%) (5438/6528)\n",
      "Epoch: 32 | Batch_idx: 60 |  Loss_1: (0.4828) | Acc_1: (83.20%) (6496/7808)\n",
      "Epoch: 32 | Batch_idx: 70 |  Loss_1: (0.4859) | Acc_1: (83.02%) (7545/9088)\n",
      "Epoch: 32 | Batch_idx: 80 |  Loss_1: (0.4922) | Acc_1: (82.83%) (8588/10368)\n",
      "Epoch: 32 | Batch_idx: 90 |  Loss_1: (0.4944) | Acc_1: (82.69%) (9632/11648)\n",
      "Epoch: 32 | Batch_idx: 100 |  Loss_1: (0.4960) | Acc_1: (82.55%) (10672/12928)\n",
      "Epoch: 32 | Batch_idx: 110 |  Loss_1: (0.4961) | Acc_1: (82.65%) (11743/14208)\n",
      "Epoch: 32 | Batch_idx: 120 |  Loss_1: (0.4958) | Acc_1: (82.60%) (12793/15488)\n",
      "Epoch: 32 | Batch_idx: 130 |  Loss_1: (0.4974) | Acc_1: (82.59%) (13848/16768)\n",
      "Epoch: 32 | Batch_idx: 140 |  Loss_1: (0.4961) | Acc_1: (82.65%) (14916/18048)\n",
      "Epoch: 32 | Batch_idx: 150 |  Loss_1: (0.4972) | Acc_1: (82.69%) (15982/19328)\n",
      "Epoch: 32 | Batch_idx: 160 |  Loss_1: (0.4982) | Acc_1: (82.66%) (17034/20608)\n",
      "Epoch: 32 | Batch_idx: 170 |  Loss_1: (0.4973) | Acc_1: (82.72%) (18105/21888)\n",
      "Epoch: 32 | Batch_idx: 180 |  Loss_1: (0.4988) | Acc_1: (82.65%) (19149/23168)\n",
      "Epoch: 32 | Batch_idx: 190 |  Loss_1: (0.4992) | Acc_1: (82.61%) (20197/24448)\n",
      "Epoch: 32 | Batch_idx: 200 |  Loss_1: (0.4977) | Acc_1: (82.61%) (21254/25728)\n",
      "Epoch: 32 | Batch_idx: 210 |  Loss_1: (0.4967) | Acc_1: (82.61%) (22311/27008)\n",
      "Epoch: 32 | Batch_idx: 220 |  Loss_1: (0.4974) | Acc_1: (82.65%) (23381/28288)\n",
      "Epoch: 32 | Batch_idx: 230 |  Loss_1: (0.4970) | Acc_1: (82.66%) (24440/29568)\n",
      "Epoch: 32 | Batch_idx: 240 |  Loss_1: (0.4978) | Acc_1: (82.63%) (25491/30848)\n",
      "Epoch: 32 | Batch_idx: 250 |  Loss_1: (0.4988) | Acc_1: (82.62%) (26544/32128)\n",
      "Epoch: 32 | Batch_idx: 260 |  Loss_1: (0.5000) | Acc_1: (82.61%) (27598/33408)\n",
      "Epoch: 32 | Batch_idx: 270 |  Loss_1: (0.4987) | Acc_1: (82.63%) (28661/34688)\n",
      "Epoch: 32 | Batch_idx: 280 |  Loss_1: (0.4982) | Acc_1: (82.65%) (29726/35968)\n",
      "Epoch: 32 | Batch_idx: 290 |  Loss_1: (0.4991) | Acc_1: (82.61%) (30769/37248)\n",
      "Epoch: 32 | Batch_idx: 300 |  Loss_1: (0.4983) | Acc_1: (82.62%) (31832/38528)\n",
      "Epoch: 32 | Batch_idx: 310 |  Loss_1: (0.4978) | Acc_1: (82.65%) (32900/39808)\n",
      "Epoch: 32 | Batch_idx: 320 |  Loss_1: (0.4996) | Acc_1: (82.56%) (33922/41088)\n",
      "Epoch: 32 | Batch_idx: 330 |  Loss_1: (0.4993) | Acc_1: (82.56%) (34980/42368)\n",
      "Epoch: 32 | Batch_idx: 340 |  Loss_1: (0.4983) | Acc_1: (82.61%) (36057/43648)\n",
      "Epoch: 32 | Batch_idx: 350 |  Loss_1: (0.4992) | Acc_1: (82.56%) (37093/44928)\n",
      "Epoch: 32 | Batch_idx: 360 |  Loss_1: (0.4995) | Acc_1: (82.57%) (38153/46208)\n",
      "Epoch: 32 | Batch_idx: 370 |  Loss_1: (0.4992) | Acc_1: (82.60%) (39223/47488)\n",
      "Epoch: 32 | Batch_idx: 380 |  Loss_1: (0.4988) | Acc_1: (82.61%) (40286/48768)\n",
      "Epoch: 32 | Batch_idx: 390 |  Loss_1: (0.4989) | Acc_1: (82.62%) (41308/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4037) | Acc: (86.12%) (8612/10000)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.6142) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.5219) | Acc_1: (81.82%) (1152/1408)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.4934) | Acc_1: (82.48%) (2217/2688)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.4790) | Acc_1: (83.06%) (3296/3968)\n",
      "Epoch: 33 | Batch_idx: 40 |  Loss_1: (0.4900) | Acc_1: (82.70%) (4340/5248)\n",
      "Epoch: 33 | Batch_idx: 50 |  Loss_1: (0.4960) | Acc_1: (82.54%) (5388/6528)\n",
      "Epoch: 33 | Batch_idx: 60 |  Loss_1: (0.4900) | Acc_1: (82.80%) (6465/7808)\n",
      "Epoch: 33 | Batch_idx: 70 |  Loss_1: (0.4884) | Acc_1: (82.87%) (7531/9088)\n",
      "Epoch: 33 | Batch_idx: 80 |  Loss_1: (0.4895) | Acc_1: (82.76%) (8581/10368)\n",
      "Epoch: 33 | Batch_idx: 90 |  Loss_1: (0.4890) | Acc_1: (82.96%) (9663/11648)\n",
      "Epoch: 33 | Batch_idx: 100 |  Loss_1: (0.4872) | Acc_1: (83.15%) (10749/12928)\n",
      "Epoch: 33 | Batch_idx: 110 |  Loss_1: (0.4805) | Acc_1: (83.38%) (11846/14208)\n",
      "Epoch: 33 | Batch_idx: 120 |  Loss_1: (0.4812) | Acc_1: (83.38%) (12914/15488)\n",
      "Epoch: 33 | Batch_idx: 130 |  Loss_1: (0.4829) | Acc_1: (83.24%) (13958/16768)\n",
      "Epoch: 33 | Batch_idx: 140 |  Loss_1: (0.4810) | Acc_1: (83.27%) (15028/18048)\n",
      "Epoch: 33 | Batch_idx: 150 |  Loss_1: (0.4803) | Acc_1: (83.24%) (16089/19328)\n",
      "Epoch: 33 | Batch_idx: 160 |  Loss_1: (0.4817) | Acc_1: (83.18%) (17141/20608)\n",
      "Epoch: 33 | Batch_idx: 170 |  Loss_1: (0.4827) | Acc_1: (83.07%) (18182/21888)\n",
      "Epoch: 33 | Batch_idx: 180 |  Loss_1: (0.4847) | Acc_1: (82.96%) (19220/23168)\n",
      "Epoch: 33 | Batch_idx: 190 |  Loss_1: (0.4851) | Acc_1: (82.99%) (20290/24448)\n",
      "Epoch: 33 | Batch_idx: 200 |  Loss_1: (0.4820) | Acc_1: (83.09%) (21377/25728)\n",
      "Epoch: 33 | Batch_idx: 210 |  Loss_1: (0.4838) | Acc_1: (83.00%) (22417/27008)\n",
      "Epoch: 33 | Batch_idx: 220 |  Loss_1: (0.4842) | Acc_1: (82.99%) (23475/28288)\n",
      "Epoch: 33 | Batch_idx: 230 |  Loss_1: (0.4854) | Acc_1: (82.93%) (24520/29568)\n",
      "Epoch: 33 | Batch_idx: 240 |  Loss_1: (0.4852) | Acc_1: (83.01%) (25606/30848)\n",
      "Epoch: 33 | Batch_idx: 250 |  Loss_1: (0.4849) | Acc_1: (83.02%) (26672/32128)\n",
      "Epoch: 33 | Batch_idx: 260 |  Loss_1: (0.4839) | Acc_1: (83.05%) (27744/33408)\n",
      "Epoch: 33 | Batch_idx: 270 |  Loss_1: (0.4840) | Acc_1: (83.05%) (28807/34688)\n",
      "Epoch: 33 | Batch_idx: 280 |  Loss_1: (0.4837) | Acc_1: (83.06%) (29874/35968)\n",
      "Epoch: 33 | Batch_idx: 290 |  Loss_1: (0.4842) | Acc_1: (83.04%) (30930/37248)\n",
      "Epoch: 33 | Batch_idx: 300 |  Loss_1: (0.4837) | Acc_1: (83.09%) (32014/38528)\n",
      "Epoch: 33 | Batch_idx: 310 |  Loss_1: (0.4845) | Acc_1: (83.06%) (33066/39808)\n",
      "Epoch: 33 | Batch_idx: 320 |  Loss_1: (0.4835) | Acc_1: (83.10%) (34146/41088)\n",
      "Epoch: 33 | Batch_idx: 330 |  Loss_1: (0.4822) | Acc_1: (83.12%) (35217/42368)\n",
      "Epoch: 33 | Batch_idx: 340 |  Loss_1: (0.4807) | Acc_1: (83.17%) (36300/43648)\n",
      "Epoch: 33 | Batch_idx: 350 |  Loss_1: (0.4808) | Acc_1: (83.14%) (37352/44928)\n",
      "Epoch: 33 | Batch_idx: 360 |  Loss_1: (0.4817) | Acc_1: (83.11%) (38403/46208)\n",
      "Epoch: 33 | Batch_idx: 370 |  Loss_1: (0.4831) | Acc_1: (83.05%) (39438/47488)\n",
      "Epoch: 33 | Batch_idx: 380 |  Loss_1: (0.4835) | Acc_1: (83.02%) (40488/48768)\n",
      "Epoch: 33 | Batch_idx: 390 |  Loss_1: (0.4831) | Acc_1: (83.02%) (41508/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3673) | Acc: (87.58%) (8758/10000)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.5359) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.5009) | Acc_1: (81.96%) (1154/1408)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.4881) | Acc_1: (82.96%) (2230/2688)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.4825) | Acc_1: (83.22%) (3302/3968)\n",
      "Epoch: 34 | Batch_idx: 40 |  Loss_1: (0.4787) | Acc_1: (83.33%) (4373/5248)\n",
      "Epoch: 34 | Batch_idx: 50 |  Loss_1: (0.4837) | Acc_1: (83.09%) (5424/6528)\n",
      "Epoch: 34 | Batch_idx: 60 |  Loss_1: (0.4823) | Acc_1: (83.11%) (6489/7808)\n",
      "Epoch: 34 | Batch_idx: 70 |  Loss_1: (0.4824) | Acc_1: (83.03%) (7546/9088)\n",
      "Epoch: 34 | Batch_idx: 80 |  Loss_1: (0.4794) | Acc_1: (83.17%) (8623/10368)\n",
      "Epoch: 34 | Batch_idx: 90 |  Loss_1: (0.4765) | Acc_1: (83.25%) (9697/11648)\n",
      "Epoch: 34 | Batch_idx: 100 |  Loss_1: (0.4730) | Acc_1: (83.40%) (10782/12928)\n",
      "Epoch: 34 | Batch_idx: 110 |  Loss_1: (0.4713) | Acc_1: (83.52%) (11867/14208)\n",
      "Epoch: 34 | Batch_idx: 120 |  Loss_1: (0.4724) | Acc_1: (83.55%) (12940/15488)\n",
      "Epoch: 34 | Batch_idx: 130 |  Loss_1: (0.4727) | Acc_1: (83.50%) (14002/16768)\n",
      "Epoch: 34 | Batch_idx: 140 |  Loss_1: (0.4719) | Acc_1: (83.65%) (15097/18048)\n",
      "Epoch: 34 | Batch_idx: 150 |  Loss_1: (0.4721) | Acc_1: (83.64%) (16166/19328)\n",
      "Epoch: 34 | Batch_idx: 160 |  Loss_1: (0.4709) | Acc_1: (83.66%) (17241/20608)\n",
      "Epoch: 34 | Batch_idx: 170 |  Loss_1: (0.4727) | Acc_1: (83.65%) (18310/21888)\n",
      "Epoch: 34 | Batch_idx: 180 |  Loss_1: (0.4771) | Acc_1: (83.47%) (19338/23168)\n",
      "Epoch: 34 | Batch_idx: 190 |  Loss_1: (0.4805) | Acc_1: (83.37%) (20382/24448)\n",
      "Epoch: 34 | Batch_idx: 200 |  Loss_1: (0.4807) | Acc_1: (83.38%) (21451/25728)\n",
      "Epoch: 34 | Batch_idx: 210 |  Loss_1: (0.4798) | Acc_1: (83.37%) (22517/27008)\n",
      "Epoch: 34 | Batch_idx: 220 |  Loss_1: (0.4783) | Acc_1: (83.46%) (23610/28288)\n",
      "Epoch: 34 | Batch_idx: 230 |  Loss_1: (0.4774) | Acc_1: (83.51%) (24692/29568)\n",
      "Epoch: 34 | Batch_idx: 240 |  Loss_1: (0.4768) | Acc_1: (83.51%) (25762/30848)\n",
      "Epoch: 34 | Batch_idx: 250 |  Loss_1: (0.4757) | Acc_1: (83.54%) (26841/32128)\n",
      "Epoch: 34 | Batch_idx: 260 |  Loss_1: (0.4770) | Acc_1: (83.50%) (27896/33408)\n",
      "Epoch: 34 | Batch_idx: 270 |  Loss_1: (0.4777) | Acc_1: (83.48%) (28959/34688)\n",
      "Epoch: 34 | Batch_idx: 280 |  Loss_1: (0.4782) | Acc_1: (83.44%) (30013/35968)\n",
      "Epoch: 34 | Batch_idx: 290 |  Loss_1: (0.4777) | Acc_1: (83.45%) (31085/37248)\n",
      "Epoch: 34 | Batch_idx: 300 |  Loss_1: (0.4771) | Acc_1: (83.49%) (32166/38528)\n",
      "Epoch: 34 | Batch_idx: 310 |  Loss_1: (0.4769) | Acc_1: (83.52%) (33247/39808)\n",
      "Epoch: 34 | Batch_idx: 320 |  Loss_1: (0.4764) | Acc_1: (83.52%) (34318/41088)\n",
      "Epoch: 34 | Batch_idx: 330 |  Loss_1: (0.4765) | Acc_1: (83.51%) (35380/42368)\n",
      "Epoch: 34 | Batch_idx: 340 |  Loss_1: (0.4753) | Acc_1: (83.55%) (36467/43648)\n",
      "Epoch: 34 | Batch_idx: 350 |  Loss_1: (0.4767) | Acc_1: (83.48%) (37508/44928)\n",
      "Epoch: 34 | Batch_idx: 360 |  Loss_1: (0.4764) | Acc_1: (83.50%) (38582/46208)\n",
      "Epoch: 34 | Batch_idx: 370 |  Loss_1: (0.4759) | Acc_1: (83.52%) (39661/47488)\n",
      "Epoch: 34 | Batch_idx: 380 |  Loss_1: (0.4767) | Acc_1: (83.48%) (40710/48768)\n",
      "Epoch: 34 | Batch_idx: 390 |  Loss_1: (0.4767) | Acc_1: (83.47%) (41735/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3602) | Acc: (87.13%) (8713/10000)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.6438) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.4579) | Acc_1: (84.16%) (1185/1408)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.4713) | Acc_1: (83.22%) (2237/2688)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.4617) | Acc_1: (83.62%) (3318/3968)\n",
      "Epoch: 35 | Batch_idx: 40 |  Loss_1: (0.4705) | Acc_1: (83.65%) (4390/5248)\n",
      "Epoch: 35 | Batch_idx: 50 |  Loss_1: (0.4709) | Acc_1: (83.70%) (5464/6528)\n",
      "Epoch: 35 | Batch_idx: 60 |  Loss_1: (0.4761) | Acc_1: (83.53%) (6522/7808)\n",
      "Epoch: 35 | Batch_idx: 70 |  Loss_1: (0.4730) | Acc_1: (83.66%) (7603/9088)\n",
      "Epoch: 35 | Batch_idx: 80 |  Loss_1: (0.4680) | Acc_1: (83.79%) (8687/10368)\n",
      "Epoch: 35 | Batch_idx: 90 |  Loss_1: (0.4671) | Acc_1: (83.78%) (9759/11648)\n",
      "Epoch: 35 | Batch_idx: 100 |  Loss_1: (0.4693) | Acc_1: (83.84%) (10839/12928)\n",
      "Epoch: 35 | Batch_idx: 110 |  Loss_1: (0.4675) | Acc_1: (83.88%) (11918/14208)\n",
      "Epoch: 35 | Batch_idx: 120 |  Loss_1: (0.4701) | Acc_1: (83.75%) (12971/15488)\n",
      "Epoch: 35 | Batch_idx: 130 |  Loss_1: (0.4708) | Acc_1: (83.74%) (14041/16768)\n",
      "Epoch: 35 | Batch_idx: 140 |  Loss_1: (0.4718) | Acc_1: (83.67%) (15101/18048)\n",
      "Epoch: 35 | Batch_idx: 150 |  Loss_1: (0.4728) | Acc_1: (83.64%) (16165/19328)\n",
      "Epoch: 35 | Batch_idx: 160 |  Loss_1: (0.4762) | Acc_1: (83.44%) (17195/20608)\n",
      "Epoch: 35 | Batch_idx: 170 |  Loss_1: (0.4758) | Acc_1: (83.47%) (18270/21888)\n",
      "Epoch: 35 | Batch_idx: 180 |  Loss_1: (0.4734) | Acc_1: (83.58%) (19363/23168)\n",
      "Epoch: 35 | Batch_idx: 190 |  Loss_1: (0.4726) | Acc_1: (83.61%) (20442/24448)\n",
      "Epoch: 35 | Batch_idx: 200 |  Loss_1: (0.4709) | Acc_1: (83.65%) (21521/25728)\n",
      "Epoch: 35 | Batch_idx: 210 |  Loss_1: (0.4719) | Acc_1: (83.61%) (22581/27008)\n",
      "Epoch: 35 | Batch_idx: 220 |  Loss_1: (0.4694) | Acc_1: (83.75%) (23691/28288)\n",
      "Epoch: 35 | Batch_idx: 230 |  Loss_1: (0.4701) | Acc_1: (83.75%) (24762/29568)\n",
      "Epoch: 35 | Batch_idx: 240 |  Loss_1: (0.4712) | Acc_1: (83.71%) (25822/30848)\n",
      "Epoch: 35 | Batch_idx: 250 |  Loss_1: (0.4707) | Acc_1: (83.69%) (26889/32128)\n",
      "Epoch: 35 | Batch_idx: 260 |  Loss_1: (0.4697) | Acc_1: (83.72%) (27970/33408)\n",
      "Epoch: 35 | Batch_idx: 270 |  Loss_1: (0.4700) | Acc_1: (83.70%) (29033/34688)\n",
      "Epoch: 35 | Batch_idx: 280 |  Loss_1: (0.4700) | Acc_1: (83.70%) (30107/35968)\n",
      "Epoch: 35 | Batch_idx: 290 |  Loss_1: (0.4701) | Acc_1: (83.69%) (31173/37248)\n",
      "Epoch: 35 | Batch_idx: 300 |  Loss_1: (0.4698) | Acc_1: (83.69%) (32244/38528)\n",
      "Epoch: 35 | Batch_idx: 310 |  Loss_1: (0.4703) | Acc_1: (83.65%) (33298/39808)\n",
      "Epoch: 35 | Batch_idx: 320 |  Loss_1: (0.4702) | Acc_1: (83.65%) (34371/41088)\n",
      "Epoch: 35 | Batch_idx: 330 |  Loss_1: (0.4705) | Acc_1: (83.66%) (35445/42368)\n",
      "Epoch: 35 | Batch_idx: 340 |  Loss_1: (0.4692) | Acc_1: (83.75%) (36556/43648)\n",
      "Epoch: 35 | Batch_idx: 350 |  Loss_1: (0.4690) | Acc_1: (83.71%) (37611/44928)\n",
      "Epoch: 35 | Batch_idx: 360 |  Loss_1: (0.4685) | Acc_1: (83.75%) (38699/46208)\n",
      "Epoch: 35 | Batch_idx: 370 |  Loss_1: (0.4691) | Acc_1: (83.74%) (39766/47488)\n",
      "Epoch: 35 | Batch_idx: 380 |  Loss_1: (0.4683) | Acc_1: (83.77%) (40855/48768)\n",
      "Epoch: 35 | Batch_idx: 390 |  Loss_1: (0.4683) | Acc_1: (83.75%) (41877/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4133) | Acc: (85.69%) (8569/10000)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.3170) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.4729) | Acc_1: (83.59%) (1177/1408)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.4570) | Acc_1: (84.34%) (2267/2688)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.4757) | Acc_1: (83.52%) (3314/3968)\n",
      "Epoch: 36 | Batch_idx: 40 |  Loss_1: (0.4600) | Acc_1: (84.07%) (4412/5248)\n",
      "Epoch: 36 | Batch_idx: 50 |  Loss_1: (0.4561) | Acc_1: (84.21%) (5497/6528)\n",
      "Epoch: 36 | Batch_idx: 60 |  Loss_1: (0.4574) | Acc_1: (84.23%) (6577/7808)\n",
      "Epoch: 36 | Batch_idx: 70 |  Loss_1: (0.4619) | Acc_1: (84.06%) (7639/9088)\n",
      "Epoch: 36 | Batch_idx: 80 |  Loss_1: (0.4653) | Acc_1: (83.82%) (8690/10368)\n",
      "Epoch: 36 | Batch_idx: 90 |  Loss_1: (0.4616) | Acc_1: (83.81%) (9762/11648)\n",
      "Epoch: 36 | Batch_idx: 100 |  Loss_1: (0.4609) | Acc_1: (83.85%) (10840/12928)\n",
      "Epoch: 36 | Batch_idx: 110 |  Loss_1: (0.4602) | Acc_1: (83.83%) (11910/14208)\n",
      "Epoch: 36 | Batch_idx: 120 |  Loss_1: (0.4590) | Acc_1: (83.85%) (12986/15488)\n",
      "Epoch: 36 | Batch_idx: 130 |  Loss_1: (0.4553) | Acc_1: (84.01%) (14087/16768)\n",
      "Epoch: 36 | Batch_idx: 140 |  Loss_1: (0.4542) | Acc_1: (84.06%) (15171/18048)\n",
      "Epoch: 36 | Batch_idx: 150 |  Loss_1: (0.4528) | Acc_1: (84.14%) (16263/19328)\n",
      "Epoch: 36 | Batch_idx: 160 |  Loss_1: (0.4493) | Acc_1: (84.29%) (17370/20608)\n",
      "Epoch: 36 | Batch_idx: 170 |  Loss_1: (0.4464) | Acc_1: (84.38%) (18468/21888)\n",
      "Epoch: 36 | Batch_idx: 180 |  Loss_1: (0.4461) | Acc_1: (84.44%) (19562/23168)\n",
      "Epoch: 36 | Batch_idx: 190 |  Loss_1: (0.4454) | Acc_1: (84.46%) (20650/24448)\n",
      "Epoch: 36 | Batch_idx: 200 |  Loss_1: (0.4478) | Acc_1: (84.38%) (21709/25728)\n",
      "Epoch: 36 | Batch_idx: 210 |  Loss_1: (0.4462) | Acc_1: (84.40%) (22795/27008)\n",
      "Epoch: 36 | Batch_idx: 220 |  Loss_1: (0.4453) | Acc_1: (84.46%) (23893/28288)\n",
      "Epoch: 36 | Batch_idx: 230 |  Loss_1: (0.4473) | Acc_1: (84.42%) (24961/29568)\n",
      "Epoch: 36 | Batch_idx: 240 |  Loss_1: (0.4473) | Acc_1: (84.43%) (26044/30848)\n",
      "Epoch: 36 | Batch_idx: 250 |  Loss_1: (0.4485) | Acc_1: (84.37%) (27106/32128)\n",
      "Epoch: 36 | Batch_idx: 260 |  Loss_1: (0.4491) | Acc_1: (84.36%) (28184/33408)\n",
      "Epoch: 36 | Batch_idx: 270 |  Loss_1: (0.4505) | Acc_1: (84.31%) (29247/34688)\n",
      "Epoch: 36 | Batch_idx: 280 |  Loss_1: (0.4512) | Acc_1: (84.28%) (30315/35968)\n",
      "Epoch: 36 | Batch_idx: 290 |  Loss_1: (0.4520) | Acc_1: (84.24%) (31378/37248)\n",
      "Epoch: 36 | Batch_idx: 300 |  Loss_1: (0.4533) | Acc_1: (84.18%) (32434/38528)\n",
      "Epoch: 36 | Batch_idx: 310 |  Loss_1: (0.4533) | Acc_1: (84.15%) (33499/39808)\n",
      "Epoch: 36 | Batch_idx: 320 |  Loss_1: (0.4536) | Acc_1: (84.16%) (34578/41088)\n",
      "Epoch: 36 | Batch_idx: 330 |  Loss_1: (0.4529) | Acc_1: (84.18%) (35664/42368)\n",
      "Epoch: 36 | Batch_idx: 340 |  Loss_1: (0.4521) | Acc_1: (84.19%) (36746/43648)\n",
      "Epoch: 36 | Batch_idx: 350 |  Loss_1: (0.4522) | Acc_1: (84.22%) (37837/44928)\n",
      "Epoch: 36 | Batch_idx: 360 |  Loss_1: (0.4528) | Acc_1: (84.18%) (38900/46208)\n",
      "Epoch: 36 | Batch_idx: 370 |  Loss_1: (0.4531) | Acc_1: (84.19%) (39982/47488)\n",
      "Epoch: 36 | Batch_idx: 380 |  Loss_1: (0.4544) | Acc_1: (84.17%) (41046/48768)\n",
      "Epoch: 36 | Batch_idx: 390 |  Loss_1: (0.4536) | Acc_1: (84.20%) (42099/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3585) | Acc: (87.58%) (8758/10000)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.5048) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.4359) | Acc_1: (84.73%) (1193/1408)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.4164) | Acc_1: (85.31%) (2293/2688)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.4244) | Acc_1: (85.18%) (3380/3968)\n",
      "Epoch: 37 | Batch_idx: 40 |  Loss_1: (0.4232) | Acc_1: (85.14%) (4468/5248)\n",
      "Epoch: 37 | Batch_idx: 50 |  Loss_1: (0.4256) | Acc_1: (85.05%) (5552/6528)\n",
      "Epoch: 37 | Batch_idx: 60 |  Loss_1: (0.4265) | Acc_1: (85.07%) (6642/7808)\n",
      "Epoch: 37 | Batch_idx: 70 |  Loss_1: (0.4273) | Acc_1: (85.00%) (7725/9088)\n",
      "Epoch: 37 | Batch_idx: 80 |  Loss_1: (0.4254) | Acc_1: (85.05%) (8818/10368)\n",
      "Epoch: 37 | Batch_idx: 90 |  Loss_1: (0.4277) | Acc_1: (84.98%) (9899/11648)\n",
      "Epoch: 37 | Batch_idx: 100 |  Loss_1: (0.4280) | Acc_1: (85.00%) (10989/12928)\n",
      "Epoch: 37 | Batch_idx: 110 |  Loss_1: (0.4296) | Acc_1: (84.96%) (12071/14208)\n",
      "Epoch: 37 | Batch_idx: 120 |  Loss_1: (0.4326) | Acc_1: (84.85%) (13141/15488)\n",
      "Epoch: 37 | Batch_idx: 130 |  Loss_1: (0.4360) | Acc_1: (84.73%) (14207/16768)\n",
      "Epoch: 37 | Batch_idx: 140 |  Loss_1: (0.4368) | Acc_1: (84.66%) (15280/18048)\n",
      "Epoch: 37 | Batch_idx: 150 |  Loss_1: (0.4379) | Acc_1: (84.65%) (16362/19328)\n",
      "Epoch: 37 | Batch_idx: 160 |  Loss_1: (0.4394) | Acc_1: (84.66%) (17446/20608)\n",
      "Epoch: 37 | Batch_idx: 170 |  Loss_1: (0.4382) | Acc_1: (84.74%) (18548/21888)\n",
      "Epoch: 37 | Batch_idx: 180 |  Loss_1: (0.4378) | Acc_1: (84.77%) (19640/23168)\n",
      "Epoch: 37 | Batch_idx: 190 |  Loss_1: (0.4388) | Acc_1: (84.71%) (20711/24448)\n",
      "Epoch: 37 | Batch_idx: 200 |  Loss_1: (0.4381) | Acc_1: (84.77%) (21809/25728)\n",
      "Epoch: 37 | Batch_idx: 210 |  Loss_1: (0.4366) | Acc_1: (84.83%) (22912/27008)\n",
      "Epoch: 37 | Batch_idx: 220 |  Loss_1: (0.4379) | Acc_1: (84.79%) (23984/28288)\n",
      "Epoch: 37 | Batch_idx: 230 |  Loss_1: (0.4396) | Acc_1: (84.76%) (25061/29568)\n",
      "Epoch: 37 | Batch_idx: 240 |  Loss_1: (0.4399) | Acc_1: (84.75%) (26143/30848)\n",
      "Epoch: 37 | Batch_idx: 250 |  Loss_1: (0.4397) | Acc_1: (84.74%) (27225/32128)\n",
      "Epoch: 37 | Batch_idx: 260 |  Loss_1: (0.4399) | Acc_1: (84.73%) (28307/33408)\n",
      "Epoch: 37 | Batch_idx: 270 |  Loss_1: (0.4404) | Acc_1: (84.71%) (29385/34688)\n",
      "Epoch: 37 | Batch_idx: 280 |  Loss_1: (0.4394) | Acc_1: (84.75%) (30483/35968)\n",
      "Epoch: 37 | Batch_idx: 290 |  Loss_1: (0.4389) | Acc_1: (84.75%) (31569/37248)\n",
      "Epoch: 37 | Batch_idx: 300 |  Loss_1: (0.4386) | Acc_1: (84.77%) (32661/38528)\n",
      "Epoch: 37 | Batch_idx: 310 |  Loss_1: (0.4389) | Acc_1: (84.77%) (33747/39808)\n",
      "Epoch: 37 | Batch_idx: 320 |  Loss_1: (0.4390) | Acc_1: (84.77%) (34832/41088)\n",
      "Epoch: 37 | Batch_idx: 330 |  Loss_1: (0.4388) | Acc_1: (84.76%) (35910/42368)\n",
      "Epoch: 37 | Batch_idx: 340 |  Loss_1: (0.4394) | Acc_1: (84.72%) (36979/43648)\n",
      "Epoch: 37 | Batch_idx: 350 |  Loss_1: (0.4388) | Acc_1: (84.75%) (38075/44928)\n",
      "Epoch: 37 | Batch_idx: 360 |  Loss_1: (0.4393) | Acc_1: (84.72%) (39148/46208)\n",
      "Epoch: 37 | Batch_idx: 370 |  Loss_1: (0.4387) | Acc_1: (84.75%) (40245/47488)\n",
      "Epoch: 37 | Batch_idx: 380 |  Loss_1: (0.4391) | Acc_1: (84.72%) (41316/48768)\n",
      "Epoch: 37 | Batch_idx: 390 |  Loss_1: (0.4394) | Acc_1: (84.71%) (42357/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3388) | Acc: (88.51%) (8851/10000)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.4788) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.4273) | Acc_1: (84.80%) (1194/1408)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.4248) | Acc_1: (84.82%) (2280/2688)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.4268) | Acc_1: (84.98%) (3372/3968)\n",
      "Epoch: 38 | Batch_idx: 40 |  Loss_1: (0.4169) | Acc_1: (85.12%) (4467/5248)\n",
      "Epoch: 38 | Batch_idx: 50 |  Loss_1: (0.4188) | Acc_1: (85.20%) (5562/6528)\n",
      "Epoch: 38 | Batch_idx: 60 |  Loss_1: (0.4183) | Acc_1: (85.13%) (6647/7808)\n",
      "Epoch: 38 | Batch_idx: 70 |  Loss_1: (0.4215) | Acc_1: (84.99%) (7724/9088)\n",
      "Epoch: 38 | Batch_idx: 80 |  Loss_1: (0.4193) | Acc_1: (85.05%) (8818/10368)\n",
      "Epoch: 38 | Batch_idx: 90 |  Loss_1: (0.4237) | Acc_1: (84.86%) (9884/11648)\n",
      "Epoch: 38 | Batch_idx: 100 |  Loss_1: (0.4243) | Acc_1: (84.82%) (10965/12928)\n",
      "Epoch: 38 | Batch_idx: 110 |  Loss_1: (0.4248) | Acc_1: (84.87%) (12058/14208)\n",
      "Epoch: 38 | Batch_idx: 120 |  Loss_1: (0.4256) | Acc_1: (84.93%) (13154/15488)\n",
      "Epoch: 38 | Batch_idx: 130 |  Loss_1: (0.4268) | Acc_1: (84.86%) (14230/16768)\n",
      "Epoch: 38 | Batch_idx: 140 |  Loss_1: (0.4280) | Acc_1: (84.86%) (15316/18048)\n",
      "Epoch: 38 | Batch_idx: 150 |  Loss_1: (0.4298) | Acc_1: (84.80%) (16390/19328)\n",
      "Epoch: 38 | Batch_idx: 160 |  Loss_1: (0.4278) | Acc_1: (84.91%) (17498/20608)\n",
      "Epoch: 38 | Batch_idx: 170 |  Loss_1: (0.4253) | Acc_1: (84.97%) (18598/21888)\n",
      "Epoch: 38 | Batch_idx: 180 |  Loss_1: (0.4250) | Acc_1: (84.98%) (19689/23168)\n",
      "Epoch: 38 | Batch_idx: 190 |  Loss_1: (0.4223) | Acc_1: (85.05%) (20793/24448)\n",
      "Epoch: 38 | Batch_idx: 200 |  Loss_1: (0.4229) | Acc_1: (85.04%) (21879/25728)\n",
      "Epoch: 38 | Batch_idx: 210 |  Loss_1: (0.4239) | Acc_1: (85.01%) (22960/27008)\n",
      "Epoch: 38 | Batch_idx: 220 |  Loss_1: (0.4238) | Acc_1: (84.95%) (24030/28288)\n",
      "Epoch: 38 | Batch_idx: 230 |  Loss_1: (0.4235) | Acc_1: (84.98%) (25128/29568)\n",
      "Epoch: 38 | Batch_idx: 240 |  Loss_1: (0.4235) | Acc_1: (85.00%) (26220/30848)\n",
      "Epoch: 38 | Batch_idx: 250 |  Loss_1: (0.4234) | Acc_1: (85.01%) (27313/32128)\n",
      "Epoch: 38 | Batch_idx: 260 |  Loss_1: (0.4232) | Acc_1: (85.01%) (28400/33408)\n",
      "Epoch: 38 | Batch_idx: 270 |  Loss_1: (0.4246) | Acc_1: (84.97%) (29476/34688)\n",
      "Epoch: 38 | Batch_idx: 280 |  Loss_1: (0.4249) | Acc_1: (84.98%) (30564/35968)\n",
      "Epoch: 38 | Batch_idx: 290 |  Loss_1: (0.4249) | Acc_1: (84.96%) (31647/37248)\n",
      "Epoch: 38 | Batch_idx: 300 |  Loss_1: (0.4246) | Acc_1: (84.99%) (32746/38528)\n",
      "Epoch: 38 | Batch_idx: 310 |  Loss_1: (0.4249) | Acc_1: (84.99%) (33831/39808)\n",
      "Epoch: 38 | Batch_idx: 320 |  Loss_1: (0.4255) | Acc_1: (84.98%) (34915/41088)\n",
      "Epoch: 38 | Batch_idx: 330 |  Loss_1: (0.4253) | Acc_1: (84.98%) (36004/42368)\n",
      "Epoch: 38 | Batch_idx: 340 |  Loss_1: (0.4262) | Acc_1: (84.95%) (37081/43648)\n",
      "Epoch: 38 | Batch_idx: 350 |  Loss_1: (0.4262) | Acc_1: (84.90%) (38145/44928)\n",
      "Epoch: 38 | Batch_idx: 360 |  Loss_1: (0.4256) | Acc_1: (84.93%) (39246/46208)\n",
      "Epoch: 38 | Batch_idx: 370 |  Loss_1: (0.4251) | Acc_1: (84.99%) (40360/47488)\n",
      "Epoch: 38 | Batch_idx: 380 |  Loss_1: (0.4260) | Acc_1: (84.97%) (41437/48768)\n",
      "Epoch: 38 | Batch_idx: 390 |  Loss_1: (0.4257) | Acc_1: (84.98%) (42489/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3432) | Acc: (88.43%) (8843/10000)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.4205) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.3769) | Acc_1: (86.86%) (1223/1408)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.3849) | Acc_1: (86.76%) (2332/2688)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.3884) | Acc_1: (86.72%) (3441/3968)\n",
      "Epoch: 39 | Batch_idx: 40 |  Loss_1: (0.3921) | Acc_1: (86.51%) (4540/5248)\n",
      "Epoch: 39 | Batch_idx: 50 |  Loss_1: (0.4004) | Acc_1: (86.18%) (5626/6528)\n",
      "Epoch: 39 | Batch_idx: 60 |  Loss_1: (0.4004) | Acc_1: (86.08%) (6721/7808)\n",
      "Epoch: 39 | Batch_idx: 70 |  Loss_1: (0.4035) | Acc_1: (86.01%) (7817/9088)\n",
      "Epoch: 39 | Batch_idx: 80 |  Loss_1: (0.4016) | Acc_1: (86.13%) (8930/10368)\n",
      "Epoch: 39 | Batch_idx: 90 |  Loss_1: (0.4041) | Acc_1: (86.02%) (10020/11648)\n",
      "Epoch: 39 | Batch_idx: 100 |  Loss_1: (0.4030) | Acc_1: (86.01%) (11119/12928)\n",
      "Epoch: 39 | Batch_idx: 110 |  Loss_1: (0.4004) | Acc_1: (86.10%) (12233/14208)\n",
      "Epoch: 39 | Batch_idx: 120 |  Loss_1: (0.4018) | Acc_1: (86.08%) (13332/15488)\n",
      "Epoch: 39 | Batch_idx: 130 |  Loss_1: (0.4031) | Acc_1: (86.02%) (14423/16768)\n",
      "Epoch: 39 | Batch_idx: 140 |  Loss_1: (0.4049) | Acc_1: (85.98%) (15518/18048)\n",
      "Epoch: 39 | Batch_idx: 150 |  Loss_1: (0.4064) | Acc_1: (85.97%) (16616/19328)\n",
      "Epoch: 39 | Batch_idx: 160 |  Loss_1: (0.4088) | Acc_1: (85.87%) (17697/20608)\n",
      "Epoch: 39 | Batch_idx: 170 |  Loss_1: (0.4083) | Acc_1: (85.88%) (18798/21888)\n",
      "Epoch: 39 | Batch_idx: 180 |  Loss_1: (0.4080) | Acc_1: (85.87%) (19895/23168)\n",
      "Epoch: 39 | Batch_idx: 190 |  Loss_1: (0.4085) | Acc_1: (85.83%) (20984/24448)\n",
      "Epoch: 39 | Batch_idx: 200 |  Loss_1: (0.4087) | Acc_1: (85.83%) (22083/25728)\n",
      "Epoch: 39 | Batch_idx: 210 |  Loss_1: (0.4075) | Acc_1: (85.87%) (23193/27008)\n",
      "Epoch: 39 | Batch_idx: 220 |  Loss_1: (0.4085) | Acc_1: (85.83%) (24280/28288)\n",
      "Epoch: 39 | Batch_idx: 230 |  Loss_1: (0.4095) | Acc_1: (85.80%) (25370/29568)\n",
      "Epoch: 39 | Batch_idx: 240 |  Loss_1: (0.4104) | Acc_1: (85.75%) (26453/30848)\n",
      "Epoch: 39 | Batch_idx: 250 |  Loss_1: (0.4088) | Acc_1: (85.78%) (27559/32128)\n",
      "Epoch: 39 | Batch_idx: 260 |  Loss_1: (0.4088) | Acc_1: (85.81%) (28667/33408)\n",
      "Epoch: 39 | Batch_idx: 270 |  Loss_1: (0.4094) | Acc_1: (85.76%) (29748/34688)\n",
      "Epoch: 39 | Batch_idx: 280 |  Loss_1: (0.4096) | Acc_1: (85.77%) (30849/35968)\n",
      "Epoch: 39 | Batch_idx: 290 |  Loss_1: (0.4117) | Acc_1: (85.72%) (31930/37248)\n",
      "Epoch: 39 | Batch_idx: 300 |  Loss_1: (0.4114) | Acc_1: (85.74%) (33032/38528)\n",
      "Epoch: 39 | Batch_idx: 310 |  Loss_1: (0.4121) | Acc_1: (85.72%) (34122/39808)\n",
      "Epoch: 39 | Batch_idx: 320 |  Loss_1: (0.4118) | Acc_1: (85.72%) (35220/41088)\n",
      "Epoch: 39 | Batch_idx: 330 |  Loss_1: (0.4114) | Acc_1: (85.73%) (36322/42368)\n",
      "Epoch: 39 | Batch_idx: 340 |  Loss_1: (0.4116) | Acc_1: (85.72%) (37413/43648)\n",
      "Epoch: 39 | Batch_idx: 350 |  Loss_1: (0.4116) | Acc_1: (85.70%) (38503/44928)\n",
      "Epoch: 39 | Batch_idx: 360 |  Loss_1: (0.4115) | Acc_1: (85.70%) (39598/46208)\n",
      "Epoch: 39 | Batch_idx: 370 |  Loss_1: (0.4120) | Acc_1: (85.67%) (40682/47488)\n",
      "Epoch: 39 | Batch_idx: 380 |  Loss_1: (0.4123) | Acc_1: (85.64%) (41767/48768)\n",
      "Epoch: 39 | Batch_idx: 390 |  Loss_1: (0.4118) | Acc_1: (85.66%) (42828/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3691) | Acc: (87.13%) (8713/10000)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.2813) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.3923) | Acc_1: (86.22%) (1214/1408)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.3923) | Acc_1: (85.97%) (2311/2688)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.4032) | Acc_1: (85.76%) (3403/3968)\n",
      "Epoch: 40 | Batch_idx: 40 |  Loss_1: (0.4055) | Acc_1: (86.05%) (4516/5248)\n",
      "Epoch: 40 | Batch_idx: 50 |  Loss_1: (0.4047) | Acc_1: (85.98%) (5613/6528)\n",
      "Epoch: 40 | Batch_idx: 60 |  Loss_1: (0.3999) | Acc_1: (86.04%) (6718/7808)\n",
      "Epoch: 40 | Batch_idx: 70 |  Loss_1: (0.4042) | Acc_1: (85.94%) (7810/9088)\n",
      "Epoch: 40 | Batch_idx: 80 |  Loss_1: (0.4080) | Acc_1: (85.79%) (8895/10368)\n",
      "Epoch: 40 | Batch_idx: 90 |  Loss_1: (0.4078) | Acc_1: (85.77%) (9991/11648)\n",
      "Epoch: 40 | Batch_idx: 100 |  Loss_1: (0.4116) | Acc_1: (85.72%) (11082/12928)\n",
      "Epoch: 40 | Batch_idx: 110 |  Loss_1: (0.4152) | Acc_1: (85.51%) (12149/14208)\n",
      "Epoch: 40 | Batch_idx: 120 |  Loss_1: (0.4140) | Acc_1: (85.61%) (13260/15488)\n",
      "Epoch: 40 | Batch_idx: 130 |  Loss_1: (0.4143) | Acc_1: (85.57%) (14349/16768)\n",
      "Epoch: 40 | Batch_idx: 140 |  Loss_1: (0.4148) | Acc_1: (85.57%) (15444/18048)\n",
      "Epoch: 40 | Batch_idx: 150 |  Loss_1: (0.4110) | Acc_1: (85.68%) (16561/19328)\n",
      "Epoch: 40 | Batch_idx: 160 |  Loss_1: (0.4103) | Acc_1: (85.72%) (17665/20608)\n",
      "Epoch: 40 | Batch_idx: 170 |  Loss_1: (0.4073) | Acc_1: (85.77%) (18774/21888)\n",
      "Epoch: 40 | Batch_idx: 180 |  Loss_1: (0.4062) | Acc_1: (85.81%) (19880/23168)\n",
      "Epoch: 40 | Batch_idx: 190 |  Loss_1: (0.4070) | Acc_1: (85.77%) (20969/24448)\n",
      "Epoch: 40 | Batch_idx: 200 |  Loss_1: (0.4071) | Acc_1: (85.78%) (22070/25728)\n",
      "Epoch: 40 | Batch_idx: 210 |  Loss_1: (0.4084) | Acc_1: (85.81%) (23175/27008)\n",
      "Epoch: 40 | Batch_idx: 220 |  Loss_1: (0.4075) | Acc_1: (85.86%) (24289/28288)\n",
      "Epoch: 40 | Batch_idx: 230 |  Loss_1: (0.4070) | Acc_1: (85.86%) (25387/29568)\n",
      "Epoch: 40 | Batch_idx: 240 |  Loss_1: (0.4065) | Acc_1: (85.88%) (26492/30848)\n",
      "Epoch: 40 | Batch_idx: 250 |  Loss_1: (0.4075) | Acc_1: (85.85%) (27582/32128)\n",
      "Epoch: 40 | Batch_idx: 260 |  Loss_1: (0.4067) | Acc_1: (85.84%) (28679/33408)\n",
      "Epoch: 40 | Batch_idx: 270 |  Loss_1: (0.4050) | Acc_1: (85.94%) (29811/34688)\n",
      "Epoch: 40 | Batch_idx: 280 |  Loss_1: (0.4047) | Acc_1: (85.96%) (30918/35968)\n",
      "Epoch: 40 | Batch_idx: 290 |  Loss_1: (0.4044) | Acc_1: (85.99%) (32031/37248)\n",
      "Epoch: 40 | Batch_idx: 300 |  Loss_1: (0.4048) | Acc_1: (85.99%) (33129/38528)\n",
      "Epoch: 40 | Batch_idx: 310 |  Loss_1: (0.4065) | Acc_1: (85.93%) (34209/39808)\n",
      "Epoch: 40 | Batch_idx: 320 |  Loss_1: (0.4073) | Acc_1: (85.89%) (35291/41088)\n",
      "Epoch: 40 | Batch_idx: 330 |  Loss_1: (0.4078) | Acc_1: (85.87%) (36380/42368)\n",
      "Epoch: 40 | Batch_idx: 340 |  Loss_1: (0.4073) | Acc_1: (85.86%) (37474/43648)\n",
      "Epoch: 40 | Batch_idx: 350 |  Loss_1: (0.4088) | Acc_1: (85.80%) (38549/44928)\n",
      "Epoch: 40 | Batch_idx: 360 |  Loss_1: (0.4093) | Acc_1: (85.80%) (39647/46208)\n",
      "Epoch: 40 | Batch_idx: 370 |  Loss_1: (0.4090) | Acc_1: (85.78%) (40735/47488)\n",
      "Epoch: 40 | Batch_idx: 380 |  Loss_1: (0.4090) | Acc_1: (85.79%) (41837/48768)\n",
      "Epoch: 40 | Batch_idx: 390 |  Loss_1: (0.4090) | Acc_1: (85.77%) (42885/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3383) | Acc: (88.68%) (8868/10000)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.5000) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.3833) | Acc_1: (86.72%) (1221/1408)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.3983) | Acc_1: (85.97%) (2311/2688)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.4066) | Acc_1: (85.89%) (3408/3968)\n",
      "Epoch: 41 | Batch_idx: 40 |  Loss_1: (0.4069) | Acc_1: (85.71%) (4498/5248)\n",
      "Epoch: 41 | Batch_idx: 50 |  Loss_1: (0.4000) | Acc_1: (85.89%) (5607/6528)\n",
      "Epoch: 41 | Batch_idx: 60 |  Loss_1: (0.3938) | Acc_1: (85.95%) (6711/7808)\n",
      "Epoch: 41 | Batch_idx: 70 |  Loss_1: (0.3871) | Acc_1: (86.28%) (7841/9088)\n",
      "Epoch: 41 | Batch_idx: 80 |  Loss_1: (0.3877) | Acc_1: (86.20%) (8937/10368)\n",
      "Epoch: 41 | Batch_idx: 90 |  Loss_1: (0.3833) | Acc_1: (86.40%) (10064/11648)\n",
      "Epoch: 41 | Batch_idx: 100 |  Loss_1: (0.3865) | Acc_1: (86.34%) (11162/12928)\n",
      "Epoch: 41 | Batch_idx: 110 |  Loss_1: (0.3852) | Acc_1: (86.37%) (12271/14208)\n",
      "Epoch: 41 | Batch_idx: 120 |  Loss_1: (0.3895) | Acc_1: (86.25%) (13359/15488)\n",
      "Epoch: 41 | Batch_idx: 130 |  Loss_1: (0.3898) | Acc_1: (86.31%) (14472/16768)\n",
      "Epoch: 41 | Batch_idx: 140 |  Loss_1: (0.3890) | Acc_1: (86.42%) (15597/18048)\n",
      "Epoch: 41 | Batch_idx: 150 |  Loss_1: (0.3917) | Acc_1: (86.33%) (16686/19328)\n",
      "Epoch: 41 | Batch_idx: 160 |  Loss_1: (0.3912) | Acc_1: (86.32%) (17788/20608)\n",
      "Epoch: 41 | Batch_idx: 170 |  Loss_1: (0.3921) | Acc_1: (86.29%) (18887/21888)\n",
      "Epoch: 41 | Batch_idx: 180 |  Loss_1: (0.3932) | Acc_1: (86.24%) (19979/23168)\n",
      "Epoch: 41 | Batch_idx: 190 |  Loss_1: (0.3924) | Acc_1: (86.22%) (21079/24448)\n",
      "Epoch: 41 | Batch_idx: 200 |  Loss_1: (0.3931) | Acc_1: (86.19%) (22174/25728)\n",
      "Epoch: 41 | Batch_idx: 210 |  Loss_1: (0.3931) | Acc_1: (86.19%) (23278/27008)\n",
      "Epoch: 41 | Batch_idx: 220 |  Loss_1: (0.3956) | Acc_1: (86.13%) (24364/28288)\n",
      "Epoch: 41 | Batch_idx: 230 |  Loss_1: (0.3964) | Acc_1: (86.07%) (25450/29568)\n",
      "Epoch: 41 | Batch_idx: 240 |  Loss_1: (0.3954) | Acc_1: (86.10%) (26559/30848)\n",
      "Epoch: 41 | Batch_idx: 250 |  Loss_1: (0.3935) | Acc_1: (86.15%) (27677/32128)\n",
      "Epoch: 41 | Batch_idx: 260 |  Loss_1: (0.3934) | Acc_1: (86.15%) (28781/33408)\n",
      "Epoch: 41 | Batch_idx: 270 |  Loss_1: (0.3925) | Acc_1: (86.17%) (29892/34688)\n",
      "Epoch: 41 | Batch_idx: 280 |  Loss_1: (0.3923) | Acc_1: (86.20%) (31005/35968)\n",
      "Epoch: 41 | Batch_idx: 290 |  Loss_1: (0.3929) | Acc_1: (86.19%) (32103/37248)\n",
      "Epoch: 41 | Batch_idx: 300 |  Loss_1: (0.3926) | Acc_1: (86.19%) (33209/38528)\n",
      "Epoch: 41 | Batch_idx: 310 |  Loss_1: (0.3921) | Acc_1: (86.20%) (34313/39808)\n",
      "Epoch: 41 | Batch_idx: 320 |  Loss_1: (0.3921) | Acc_1: (86.19%) (35413/41088)\n",
      "Epoch: 41 | Batch_idx: 330 |  Loss_1: (0.3919) | Acc_1: (86.17%) (36508/42368)\n",
      "Epoch: 41 | Batch_idx: 340 |  Loss_1: (0.3919) | Acc_1: (86.16%) (37606/43648)\n",
      "Epoch: 41 | Batch_idx: 350 |  Loss_1: (0.3926) | Acc_1: (86.16%) (38712/44928)\n",
      "Epoch: 41 | Batch_idx: 360 |  Loss_1: (0.3911) | Acc_1: (86.23%) (39843/46208)\n",
      "Epoch: 41 | Batch_idx: 370 |  Loss_1: (0.3915) | Acc_1: (86.22%) (40945/47488)\n",
      "Epoch: 41 | Batch_idx: 380 |  Loss_1: (0.3926) | Acc_1: (86.19%) (42034/48768)\n",
      "Epoch: 41 | Batch_idx: 390 |  Loss_1: (0.3934) | Acc_1: (86.16%) (43080/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3588) | Acc: (87.64%) (8764/10000)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.4291) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.4267) | Acc_1: (84.02%) (1183/1408)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.4180) | Acc_1: (84.86%) (2281/2688)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.4067) | Acc_1: (85.31%) (3385/3968)\n",
      "Epoch: 42 | Batch_idx: 40 |  Loss_1: (0.4100) | Acc_1: (85.14%) (4468/5248)\n",
      "Epoch: 42 | Batch_idx: 50 |  Loss_1: (0.4074) | Acc_1: (85.26%) (5566/6528)\n",
      "Epoch: 42 | Batch_idx: 60 |  Loss_1: (0.3990) | Acc_1: (85.63%) (6686/7808)\n",
      "Epoch: 42 | Batch_idx: 70 |  Loss_1: (0.3940) | Acc_1: (85.90%) (7807/9088)\n",
      "Epoch: 42 | Batch_idx: 80 |  Loss_1: (0.3949) | Acc_1: (85.85%) (8901/10368)\n",
      "Epoch: 42 | Batch_idx: 90 |  Loss_1: (0.3947) | Acc_1: (86.01%) (10018/11648)\n",
      "Epoch: 42 | Batch_idx: 100 |  Loss_1: (0.3952) | Acc_1: (86.03%) (11122/12928)\n",
      "Epoch: 42 | Batch_idx: 110 |  Loss_1: (0.3955) | Acc_1: (86.07%) (12229/14208)\n",
      "Epoch: 42 | Batch_idx: 120 |  Loss_1: (0.3978) | Acc_1: (85.96%) (13314/15488)\n",
      "Epoch: 42 | Batch_idx: 130 |  Loss_1: (0.4021) | Acc_1: (85.81%) (14389/16768)\n",
      "Epoch: 42 | Batch_idx: 140 |  Loss_1: (0.3991) | Acc_1: (85.91%) (15505/18048)\n",
      "Epoch: 42 | Batch_idx: 150 |  Loss_1: (0.3962) | Acc_1: (85.97%) (16616/19328)\n",
      "Epoch: 42 | Batch_idx: 160 |  Loss_1: (0.3942) | Acc_1: (86.02%) (17726/20608)\n",
      "Epoch: 42 | Batch_idx: 170 |  Loss_1: (0.3906) | Acc_1: (86.12%) (18850/21888)\n",
      "Epoch: 42 | Batch_idx: 180 |  Loss_1: (0.3899) | Acc_1: (86.15%) (19959/23168)\n",
      "Epoch: 42 | Batch_idx: 190 |  Loss_1: (0.3900) | Acc_1: (86.16%) (21064/24448)\n",
      "Epoch: 42 | Batch_idx: 200 |  Loss_1: (0.3896) | Acc_1: (86.16%) (22168/25728)\n",
      "Epoch: 42 | Batch_idx: 210 |  Loss_1: (0.3896) | Acc_1: (86.20%) (23281/27008)\n",
      "Epoch: 42 | Batch_idx: 220 |  Loss_1: (0.3900) | Acc_1: (86.22%) (24390/28288)\n",
      "Epoch: 42 | Batch_idx: 230 |  Loss_1: (0.3898) | Acc_1: (86.24%) (25498/29568)\n",
      "Epoch: 42 | Batch_idx: 240 |  Loss_1: (0.3912) | Acc_1: (86.16%) (26580/30848)\n",
      "Epoch: 42 | Batch_idx: 250 |  Loss_1: (0.3898) | Acc_1: (86.21%) (27697/32128)\n",
      "Epoch: 42 | Batch_idx: 260 |  Loss_1: (0.3887) | Acc_1: (86.25%) (28813/33408)\n",
      "Epoch: 42 | Batch_idx: 270 |  Loss_1: (0.3891) | Acc_1: (86.26%) (29923/34688)\n",
      "Epoch: 42 | Batch_idx: 280 |  Loss_1: (0.3892) | Acc_1: (86.29%) (31038/35968)\n",
      "Epoch: 42 | Batch_idx: 290 |  Loss_1: (0.3895) | Acc_1: (86.30%) (32145/37248)\n",
      "Epoch: 42 | Batch_idx: 300 |  Loss_1: (0.3892) | Acc_1: (86.33%) (33261/38528)\n",
      "Epoch: 42 | Batch_idx: 310 |  Loss_1: (0.3880) | Acc_1: (86.40%) (34395/39808)\n",
      "Epoch: 42 | Batch_idx: 320 |  Loss_1: (0.3882) | Acc_1: (86.39%) (35494/41088)\n",
      "Epoch: 42 | Batch_idx: 330 |  Loss_1: (0.3884) | Acc_1: (86.39%) (36601/42368)\n",
      "Epoch: 42 | Batch_idx: 340 |  Loss_1: (0.3900) | Acc_1: (86.35%) (37692/43648)\n",
      "Epoch: 42 | Batch_idx: 350 |  Loss_1: (0.3899) | Acc_1: (86.37%) (38805/44928)\n",
      "Epoch: 42 | Batch_idx: 360 |  Loss_1: (0.3905) | Acc_1: (86.36%) (39907/46208)\n",
      "Epoch: 42 | Batch_idx: 370 |  Loss_1: (0.3906) | Acc_1: (86.36%) (41010/47488)\n",
      "Epoch: 42 | Batch_idx: 380 |  Loss_1: (0.3905) | Acc_1: (86.36%) (42115/48768)\n",
      "Epoch: 42 | Batch_idx: 390 |  Loss_1: (0.3901) | Acc_1: (86.36%) (43179/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3320) | Acc: (88.43%) (8843/10000)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.4373) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.3792) | Acc_1: (86.72%) (1221/1408)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.3818) | Acc_1: (86.24%) (2318/2688)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.3819) | Acc_1: (86.27%) (3423/3968)\n",
      "Epoch: 43 | Batch_idx: 40 |  Loss_1: (0.3931) | Acc_1: (86.19%) (4523/5248)\n",
      "Epoch: 43 | Batch_idx: 50 |  Loss_1: (0.3896) | Acc_1: (86.17%) (5625/6528)\n",
      "Epoch: 43 | Batch_idx: 60 |  Loss_1: (0.3866) | Acc_1: (86.12%) (6724/7808)\n",
      "Epoch: 43 | Batch_idx: 70 |  Loss_1: (0.3880) | Acc_1: (86.10%) (7825/9088)\n",
      "Epoch: 43 | Batch_idx: 80 |  Loss_1: (0.3888) | Acc_1: (86.06%) (8923/10368)\n",
      "Epoch: 43 | Batch_idx: 90 |  Loss_1: (0.3858) | Acc_1: (86.13%) (10033/11648)\n",
      "Epoch: 43 | Batch_idx: 100 |  Loss_1: (0.3841) | Acc_1: (86.28%) (11154/12928)\n",
      "Epoch: 43 | Batch_idx: 110 |  Loss_1: (0.3831) | Acc_1: (86.22%) (12250/14208)\n",
      "Epoch: 43 | Batch_idx: 120 |  Loss_1: (0.3862) | Acc_1: (86.16%) (13344/15488)\n",
      "Epoch: 43 | Batch_idx: 130 |  Loss_1: (0.3873) | Acc_1: (86.16%) (14448/16768)\n",
      "Epoch: 43 | Batch_idx: 140 |  Loss_1: (0.3884) | Acc_1: (86.18%) (15553/18048)\n",
      "Epoch: 43 | Batch_idx: 150 |  Loss_1: (0.3872) | Acc_1: (86.22%) (16665/19328)\n",
      "Epoch: 43 | Batch_idx: 160 |  Loss_1: (0.3860) | Acc_1: (86.29%) (17782/20608)\n",
      "Epoch: 43 | Batch_idx: 170 |  Loss_1: (0.3842) | Acc_1: (86.32%) (18894/21888)\n",
      "Epoch: 43 | Batch_idx: 180 |  Loss_1: (0.3854) | Acc_1: (86.33%) (20001/23168)\n",
      "Epoch: 43 | Batch_idx: 190 |  Loss_1: (0.3845) | Acc_1: (86.35%) (21111/24448)\n",
      "Epoch: 43 | Batch_idx: 200 |  Loss_1: (0.3845) | Acc_1: (86.35%) (22217/25728)\n",
      "Epoch: 43 | Batch_idx: 210 |  Loss_1: (0.3857) | Acc_1: (86.32%) (23312/27008)\n",
      "Epoch: 43 | Batch_idx: 220 |  Loss_1: (0.3863) | Acc_1: (86.27%) (24403/28288)\n",
      "Epoch: 43 | Batch_idx: 230 |  Loss_1: (0.3896) | Acc_1: (86.17%) (25478/29568)\n",
      "Epoch: 43 | Batch_idx: 240 |  Loss_1: (0.3893) | Acc_1: (86.16%) (26579/30848)\n",
      "Epoch: 43 | Batch_idx: 250 |  Loss_1: (0.3896) | Acc_1: (86.13%) (27673/32128)\n",
      "Epoch: 43 | Batch_idx: 260 |  Loss_1: (0.3906) | Acc_1: (86.08%) (28756/33408)\n",
      "Epoch: 43 | Batch_idx: 270 |  Loss_1: (0.3924) | Acc_1: (86.04%) (29845/34688)\n",
      "Epoch: 43 | Batch_idx: 280 |  Loss_1: (0.3904) | Acc_1: (86.09%) (30966/35968)\n",
      "Epoch: 43 | Batch_idx: 290 |  Loss_1: (0.3909) | Acc_1: (86.05%) (32053/37248)\n",
      "Epoch: 43 | Batch_idx: 300 |  Loss_1: (0.3897) | Acc_1: (86.12%) (33180/38528)\n",
      "Epoch: 43 | Batch_idx: 310 |  Loss_1: (0.3904) | Acc_1: (86.10%) (34274/39808)\n",
      "Epoch: 43 | Batch_idx: 320 |  Loss_1: (0.3901) | Acc_1: (86.12%) (35386/41088)\n",
      "Epoch: 43 | Batch_idx: 330 |  Loss_1: (0.3904) | Acc_1: (86.11%) (36482/42368)\n",
      "Epoch: 43 | Batch_idx: 340 |  Loss_1: (0.3913) | Acc_1: (86.07%) (37568/43648)\n",
      "Epoch: 43 | Batch_idx: 350 |  Loss_1: (0.3909) | Acc_1: (86.08%) (38675/44928)\n",
      "Epoch: 43 | Batch_idx: 360 |  Loss_1: (0.3918) | Acc_1: (86.08%) (39775/46208)\n",
      "Epoch: 43 | Batch_idx: 370 |  Loss_1: (0.3914) | Acc_1: (86.09%) (40882/47488)\n",
      "Epoch: 43 | Batch_idx: 380 |  Loss_1: (0.3916) | Acc_1: (86.07%) (41975/48768)\n",
      "Epoch: 43 | Batch_idx: 390 |  Loss_1: (0.3912) | Acc_1: (86.06%) (43032/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3296) | Acc: (88.66%) (8866/10000)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.3746) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.4160) | Acc_1: (84.66%) (1192/1408)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.3913) | Acc_1: (85.45%) (2297/2688)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.3799) | Acc_1: (86.11%) (3417/3968)\n",
      "Epoch: 44 | Batch_idx: 40 |  Loss_1: (0.3840) | Acc_1: (85.99%) (4513/5248)\n",
      "Epoch: 44 | Batch_idx: 50 |  Loss_1: (0.3824) | Acc_1: (86.29%) (5633/6528)\n",
      "Epoch: 44 | Batch_idx: 60 |  Loss_1: (0.3792) | Acc_1: (86.51%) (6755/7808)\n",
      "Epoch: 44 | Batch_idx: 70 |  Loss_1: (0.3734) | Acc_1: (86.75%) (7884/9088)\n",
      "Epoch: 44 | Batch_idx: 80 |  Loss_1: (0.3775) | Acc_1: (86.56%) (8975/10368)\n",
      "Epoch: 44 | Batch_idx: 90 |  Loss_1: (0.3776) | Acc_1: (86.56%) (10083/11648)\n",
      "Epoch: 44 | Batch_idx: 100 |  Loss_1: (0.3747) | Acc_1: (86.74%) (11214/12928)\n",
      "Epoch: 44 | Batch_idx: 110 |  Loss_1: (0.3722) | Acc_1: (86.81%) (12334/14208)\n",
      "Epoch: 44 | Batch_idx: 120 |  Loss_1: (0.3731) | Acc_1: (86.84%) (13450/15488)\n",
      "Epoch: 44 | Batch_idx: 130 |  Loss_1: (0.3767) | Acc_1: (86.67%) (14532/16768)\n",
      "Epoch: 44 | Batch_idx: 140 |  Loss_1: (0.3763) | Acc_1: (86.69%) (15646/18048)\n",
      "Epoch: 44 | Batch_idx: 150 |  Loss_1: (0.3754) | Acc_1: (86.68%) (16754/19328)\n",
      "Epoch: 44 | Batch_idx: 160 |  Loss_1: (0.3762) | Acc_1: (86.70%) (17868/20608)\n",
      "Epoch: 44 | Batch_idx: 170 |  Loss_1: (0.3747) | Acc_1: (86.72%) (18982/21888)\n",
      "Epoch: 44 | Batch_idx: 180 |  Loss_1: (0.3761) | Acc_1: (86.65%) (20074/23168)\n",
      "Epoch: 44 | Batch_idx: 190 |  Loss_1: (0.3769) | Acc_1: (86.63%) (21180/24448)\n",
      "Epoch: 44 | Batch_idx: 200 |  Loss_1: (0.3767) | Acc_1: (86.68%) (22301/25728)\n",
      "Epoch: 44 | Batch_idx: 210 |  Loss_1: (0.3767) | Acc_1: (86.66%) (23404/27008)\n",
      "Epoch: 44 | Batch_idx: 220 |  Loss_1: (0.3790) | Acc_1: (86.60%) (24497/28288)\n",
      "Epoch: 44 | Batch_idx: 230 |  Loss_1: (0.3787) | Acc_1: (86.60%) (25607/29568)\n",
      "Epoch: 44 | Batch_idx: 240 |  Loss_1: (0.3790) | Acc_1: (86.57%) (26704/30848)\n",
      "Epoch: 44 | Batch_idx: 250 |  Loss_1: (0.3790) | Acc_1: (86.55%) (27808/32128)\n",
      "Epoch: 44 | Batch_idx: 260 |  Loss_1: (0.3787) | Acc_1: (86.58%) (28925/33408)\n",
      "Epoch: 44 | Batch_idx: 270 |  Loss_1: (0.3790) | Acc_1: (86.56%) (30026/34688)\n",
      "Epoch: 44 | Batch_idx: 280 |  Loss_1: (0.3785) | Acc_1: (86.57%) (31138/35968)\n",
      "Epoch: 44 | Batch_idx: 290 |  Loss_1: (0.3782) | Acc_1: (86.59%) (32254/37248)\n",
      "Epoch: 44 | Batch_idx: 300 |  Loss_1: (0.3785) | Acc_1: (86.59%) (33362/38528)\n",
      "Epoch: 44 | Batch_idx: 310 |  Loss_1: (0.3780) | Acc_1: (86.62%) (34480/39808)\n",
      "Epoch: 44 | Batch_idx: 320 |  Loss_1: (0.3783) | Acc_1: (86.61%) (35587/41088)\n",
      "Epoch: 44 | Batch_idx: 330 |  Loss_1: (0.3779) | Acc_1: (86.63%) (36703/42368)\n",
      "Epoch: 44 | Batch_idx: 340 |  Loss_1: (0.3776) | Acc_1: (86.64%) (37816/43648)\n",
      "Epoch: 44 | Batch_idx: 350 |  Loss_1: (0.3776) | Acc_1: (86.62%) (38917/44928)\n",
      "Epoch: 44 | Batch_idx: 360 |  Loss_1: (0.3778) | Acc_1: (86.60%) (40014/46208)\n",
      "Epoch: 44 | Batch_idx: 370 |  Loss_1: (0.3779) | Acc_1: (86.58%) (41117/47488)\n",
      "Epoch: 44 | Batch_idx: 380 |  Loss_1: (0.3786) | Acc_1: (86.59%) (42227/48768)\n",
      "Epoch: 44 | Batch_idx: 390 |  Loss_1: (0.3787) | Acc_1: (86.57%) (43285/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3445) | Acc: (88.42%) (8842/10000)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.3313) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.4074) | Acc_1: (85.16%) (1199/1408)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.3897) | Acc_1: (86.09%) (2314/2688)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.3839) | Acc_1: (86.37%) (3427/3968)\n",
      "Epoch: 45 | Batch_idx: 40 |  Loss_1: (0.3702) | Acc_1: (86.93%) (4562/5248)\n",
      "Epoch: 45 | Batch_idx: 50 |  Loss_1: (0.3668) | Acc_1: (86.99%) (5679/6528)\n",
      "Epoch: 45 | Batch_idx: 60 |  Loss_1: (0.3672) | Acc_1: (87.04%) (6796/7808)\n",
      "Epoch: 45 | Batch_idx: 70 |  Loss_1: (0.3693) | Acc_1: (87.05%) (7911/9088)\n",
      "Epoch: 45 | Batch_idx: 80 |  Loss_1: (0.3686) | Acc_1: (87.06%) (9026/10368)\n",
      "Epoch: 45 | Batch_idx: 90 |  Loss_1: (0.3698) | Acc_1: (86.89%) (10121/11648)\n",
      "Epoch: 45 | Batch_idx: 100 |  Loss_1: (0.3691) | Acc_1: (86.95%) (11241/12928)\n",
      "Epoch: 45 | Batch_idx: 110 |  Loss_1: (0.3666) | Acc_1: (86.99%) (12360/14208)\n",
      "Epoch: 45 | Batch_idx: 120 |  Loss_1: (0.3680) | Acc_1: (86.90%) (13459/15488)\n",
      "Epoch: 45 | Batch_idx: 130 |  Loss_1: (0.3691) | Acc_1: (86.93%) (14577/16768)\n",
      "Epoch: 45 | Batch_idx: 140 |  Loss_1: (0.3675) | Acc_1: (86.91%) (15685/18048)\n",
      "Epoch: 45 | Batch_idx: 150 |  Loss_1: (0.3668) | Acc_1: (86.93%) (16802/19328)\n",
      "Epoch: 45 | Batch_idx: 160 |  Loss_1: (0.3664) | Acc_1: (86.91%) (17911/20608)\n",
      "Epoch: 45 | Batch_idx: 170 |  Loss_1: (0.3672) | Acc_1: (86.92%) (19025/21888)\n",
      "Epoch: 45 | Batch_idx: 180 |  Loss_1: (0.3660) | Acc_1: (86.96%) (20148/23168)\n",
      "Epoch: 45 | Batch_idx: 190 |  Loss_1: (0.3663) | Acc_1: (86.94%) (21254/24448)\n",
      "Epoch: 45 | Batch_idx: 200 |  Loss_1: (0.3673) | Acc_1: (86.90%) (22358/25728)\n",
      "Epoch: 45 | Batch_idx: 210 |  Loss_1: (0.3664) | Acc_1: (86.93%) (23477/27008)\n",
      "Epoch: 45 | Batch_idx: 220 |  Loss_1: (0.3661) | Acc_1: (86.96%) (24600/28288)\n",
      "Epoch: 45 | Batch_idx: 230 |  Loss_1: (0.3658) | Acc_1: (86.93%) (25704/29568)\n",
      "Epoch: 45 | Batch_idx: 240 |  Loss_1: (0.3671) | Acc_1: (86.90%) (26808/30848)\n",
      "Epoch: 45 | Batch_idx: 250 |  Loss_1: (0.3664) | Acc_1: (86.96%) (27940/32128)\n",
      "Epoch: 45 | Batch_idx: 260 |  Loss_1: (0.3663) | Acc_1: (87.00%) (29065/33408)\n",
      "Epoch: 45 | Batch_idx: 270 |  Loss_1: (0.3664) | Acc_1: (87.03%) (30188/34688)\n",
      "Epoch: 45 | Batch_idx: 280 |  Loss_1: (0.3676) | Acc_1: (86.99%) (31290/35968)\n",
      "Epoch: 45 | Batch_idx: 290 |  Loss_1: (0.3672) | Acc_1: (87.02%) (32415/37248)\n",
      "Epoch: 45 | Batch_idx: 300 |  Loss_1: (0.3674) | Acc_1: (87.03%) (33529/38528)\n",
      "Epoch: 45 | Batch_idx: 310 |  Loss_1: (0.3691) | Acc_1: (86.94%) (34609/39808)\n",
      "Epoch: 45 | Batch_idx: 320 |  Loss_1: (0.3692) | Acc_1: (86.95%) (35727/41088)\n",
      "Epoch: 45 | Batch_idx: 330 |  Loss_1: (0.3697) | Acc_1: (86.92%) (36828/42368)\n",
      "Epoch: 45 | Batch_idx: 340 |  Loss_1: (0.3698) | Acc_1: (86.94%) (37948/43648)\n",
      "Epoch: 45 | Batch_idx: 350 |  Loss_1: (0.3696) | Acc_1: (86.97%) (39072/44928)\n",
      "Epoch: 45 | Batch_idx: 360 |  Loss_1: (0.3691) | Acc_1: (86.97%) (40188/46208)\n",
      "Epoch: 45 | Batch_idx: 370 |  Loss_1: (0.3697) | Acc_1: (86.95%) (41293/47488)\n",
      "Epoch: 45 | Batch_idx: 380 |  Loss_1: (0.3696) | Acc_1: (86.98%) (42416/48768)\n",
      "Epoch: 45 | Batch_idx: 390 |  Loss_1: (0.3696) | Acc_1: (86.97%) (43485/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3306) | Acc: (89.10%) (8910/10000)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.4428) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.3792) | Acc_1: (86.22%) (1214/1408)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.3654) | Acc_1: (87.09%) (2341/2688)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.3574) | Acc_1: (87.32%) (3465/3968)\n",
      "Epoch: 46 | Batch_idx: 40 |  Loss_1: (0.3656) | Acc_1: (87.29%) (4581/5248)\n",
      "Epoch: 46 | Batch_idx: 50 |  Loss_1: (0.3623) | Acc_1: (87.30%) (5699/6528)\n",
      "Epoch: 46 | Batch_idx: 60 |  Loss_1: (0.3597) | Acc_1: (87.37%) (6822/7808)\n",
      "Epoch: 46 | Batch_idx: 70 |  Loss_1: (0.3572) | Acc_1: (87.43%) (7946/9088)\n",
      "Epoch: 46 | Batch_idx: 80 |  Loss_1: (0.3544) | Acc_1: (87.45%) (9067/10368)\n",
      "Epoch: 46 | Batch_idx: 90 |  Loss_1: (0.3627) | Acc_1: (87.13%) (10149/11648)\n",
      "Epoch: 46 | Batch_idx: 100 |  Loss_1: (0.3594) | Acc_1: (87.22%) (11276/12928)\n",
      "Epoch: 46 | Batch_idx: 110 |  Loss_1: (0.3608) | Acc_1: (87.07%) (12371/14208)\n",
      "Epoch: 46 | Batch_idx: 120 |  Loss_1: (0.3610) | Acc_1: (87.01%) (13476/15488)\n",
      "Epoch: 46 | Batch_idx: 130 |  Loss_1: (0.3623) | Acc_1: (86.93%) (14577/16768)\n",
      "Epoch: 46 | Batch_idx: 140 |  Loss_1: (0.3635) | Acc_1: (86.86%) (15677/18048)\n",
      "Epoch: 46 | Batch_idx: 150 |  Loss_1: (0.3622) | Acc_1: (86.95%) (16806/19328)\n",
      "Epoch: 46 | Batch_idx: 160 |  Loss_1: (0.3605) | Acc_1: (87.02%) (17934/20608)\n",
      "Epoch: 46 | Batch_idx: 170 |  Loss_1: (0.3613) | Acc_1: (87.03%) (19049/21888)\n",
      "Epoch: 46 | Batch_idx: 180 |  Loss_1: (0.3614) | Acc_1: (87.02%) (20160/23168)\n",
      "Epoch: 46 | Batch_idx: 190 |  Loss_1: (0.3618) | Acc_1: (87.01%) (21272/24448)\n",
      "Epoch: 46 | Batch_idx: 200 |  Loss_1: (0.3613) | Acc_1: (87.05%) (22395/25728)\n",
      "Epoch: 46 | Batch_idx: 210 |  Loss_1: (0.3617) | Acc_1: (87.03%) (23506/27008)\n",
      "Epoch: 46 | Batch_idx: 220 |  Loss_1: (0.3605) | Acc_1: (87.08%) (24633/28288)\n",
      "Epoch: 46 | Batch_idx: 230 |  Loss_1: (0.3612) | Acc_1: (87.04%) (25736/29568)\n",
      "Epoch: 46 | Batch_idx: 240 |  Loss_1: (0.3623) | Acc_1: (87.03%) (26848/30848)\n",
      "Epoch: 46 | Batch_idx: 250 |  Loss_1: (0.3616) | Acc_1: (87.08%) (27977/32128)\n",
      "Epoch: 46 | Batch_idx: 260 |  Loss_1: (0.3613) | Acc_1: (87.08%) (29092/33408)\n",
      "Epoch: 46 | Batch_idx: 270 |  Loss_1: (0.3606) | Acc_1: (87.12%) (30219/34688)\n",
      "Epoch: 46 | Batch_idx: 280 |  Loss_1: (0.3607) | Acc_1: (87.10%) (31329/35968)\n",
      "Epoch: 46 | Batch_idx: 290 |  Loss_1: (0.3616) | Acc_1: (87.08%) (32436/37248)\n",
      "Epoch: 46 | Batch_idx: 300 |  Loss_1: (0.3623) | Acc_1: (87.07%) (33548/38528)\n",
      "Epoch: 46 | Batch_idx: 310 |  Loss_1: (0.3635) | Acc_1: (87.05%) (34651/39808)\n",
      "Epoch: 46 | Batch_idx: 320 |  Loss_1: (0.3633) | Acc_1: (87.04%) (35762/41088)\n",
      "Epoch: 46 | Batch_idx: 330 |  Loss_1: (0.3636) | Acc_1: (87.03%) (36873/42368)\n",
      "Epoch: 46 | Batch_idx: 340 |  Loss_1: (0.3627) | Acc_1: (87.06%) (38001/43648)\n",
      "Epoch: 46 | Batch_idx: 350 |  Loss_1: (0.3641) | Acc_1: (87.00%) (39087/44928)\n",
      "Epoch: 46 | Batch_idx: 360 |  Loss_1: (0.3646) | Acc_1: (87.00%) (40202/46208)\n",
      "Epoch: 46 | Batch_idx: 370 |  Loss_1: (0.3639) | Acc_1: (87.03%) (41327/47488)\n",
      "Epoch: 46 | Batch_idx: 380 |  Loss_1: (0.3633) | Acc_1: (87.06%) (42456/48768)\n",
      "Epoch: 46 | Batch_idx: 390 |  Loss_1: (0.3646) | Acc_1: (87.03%) (43517/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3447) | Acc: (88.33%) (8833/10000)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.3393) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.3399) | Acc_1: (87.86%) (1237/1408)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.3301) | Acc_1: (88.36%) (2375/2688)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.3333) | Acc_1: (88.43%) (3509/3968)\n",
      "Epoch: 47 | Batch_idx: 40 |  Loss_1: (0.3354) | Acc_1: (88.15%) (4626/5248)\n",
      "Epoch: 47 | Batch_idx: 50 |  Loss_1: (0.3461) | Acc_1: (87.84%) (5734/6528)\n",
      "Epoch: 47 | Batch_idx: 60 |  Loss_1: (0.3468) | Acc_1: (87.77%) (6853/7808)\n",
      "Epoch: 47 | Batch_idx: 70 |  Loss_1: (0.3548) | Acc_1: (87.47%) (7949/9088)\n",
      "Epoch: 47 | Batch_idx: 80 |  Loss_1: (0.3556) | Acc_1: (87.47%) (9069/10368)\n",
      "Epoch: 47 | Batch_idx: 90 |  Loss_1: (0.3574) | Acc_1: (87.35%) (10175/11648)\n",
      "Epoch: 47 | Batch_idx: 100 |  Loss_1: (0.3563) | Acc_1: (87.35%) (11292/12928)\n",
      "Epoch: 47 | Batch_idx: 110 |  Loss_1: (0.3535) | Acc_1: (87.35%) (12410/14208)\n",
      "Epoch: 47 | Batch_idx: 120 |  Loss_1: (0.3485) | Acc_1: (87.47%) (13547/15488)\n",
      "Epoch: 47 | Batch_idx: 130 |  Loss_1: (0.3482) | Acc_1: (87.52%) (14675/16768)\n",
      "Epoch: 47 | Batch_idx: 140 |  Loss_1: (0.3496) | Acc_1: (87.45%) (15783/18048)\n",
      "Epoch: 47 | Batch_idx: 150 |  Loss_1: (0.3476) | Acc_1: (87.56%) (16924/19328)\n",
      "Epoch: 47 | Batch_idx: 160 |  Loss_1: (0.3498) | Acc_1: (87.49%) (18029/20608)\n",
      "Epoch: 47 | Batch_idx: 170 |  Loss_1: (0.3498) | Acc_1: (87.51%) (19155/21888)\n",
      "Epoch: 47 | Batch_idx: 180 |  Loss_1: (0.3478) | Acc_1: (87.59%) (20292/23168)\n",
      "Epoch: 47 | Batch_idx: 190 |  Loss_1: (0.3463) | Acc_1: (87.65%) (21428/24448)\n",
      "Epoch: 47 | Batch_idx: 200 |  Loss_1: (0.3485) | Acc_1: (87.60%) (22537/25728)\n",
      "Epoch: 47 | Batch_idx: 210 |  Loss_1: (0.3503) | Acc_1: (87.57%) (23652/27008)\n",
      "Epoch: 47 | Batch_idx: 220 |  Loss_1: (0.3501) | Acc_1: (87.60%) (24781/28288)\n",
      "Epoch: 47 | Batch_idx: 230 |  Loss_1: (0.3508) | Acc_1: (87.59%) (25900/29568)\n",
      "Epoch: 47 | Batch_idx: 240 |  Loss_1: (0.3506) | Acc_1: (87.62%) (27028/30848)\n",
      "Epoch: 47 | Batch_idx: 250 |  Loss_1: (0.3494) | Acc_1: (87.68%) (28170/32128)\n",
      "Epoch: 47 | Batch_idx: 260 |  Loss_1: (0.3501) | Acc_1: (87.63%) (29274/33408)\n",
      "Epoch: 47 | Batch_idx: 270 |  Loss_1: (0.3508) | Acc_1: (87.62%) (30394/34688)\n",
      "Epoch: 47 | Batch_idx: 280 |  Loss_1: (0.3529) | Acc_1: (87.56%) (31494/35968)\n",
      "Epoch: 47 | Batch_idx: 290 |  Loss_1: (0.3537) | Acc_1: (87.53%) (32604/37248)\n",
      "Epoch: 47 | Batch_idx: 300 |  Loss_1: (0.3516) | Acc_1: (87.63%) (33763/38528)\n",
      "Epoch: 47 | Batch_idx: 310 |  Loss_1: (0.3521) | Acc_1: (87.60%) (34873/39808)\n",
      "Epoch: 47 | Batch_idx: 320 |  Loss_1: (0.3521) | Acc_1: (87.61%) (35996/41088)\n",
      "Epoch: 47 | Batch_idx: 330 |  Loss_1: (0.3531) | Acc_1: (87.58%) (37104/42368)\n",
      "Epoch: 47 | Batch_idx: 340 |  Loss_1: (0.3532) | Acc_1: (87.60%) (38235/43648)\n",
      "Epoch: 47 | Batch_idx: 350 |  Loss_1: (0.3545) | Acc_1: (87.54%) (39331/44928)\n",
      "Epoch: 47 | Batch_idx: 360 |  Loss_1: (0.3531) | Acc_1: (87.60%) (40477/46208)\n",
      "Epoch: 47 | Batch_idx: 370 |  Loss_1: (0.3526) | Acc_1: (87.61%) (41602/47488)\n",
      "Epoch: 47 | Batch_idx: 380 |  Loss_1: (0.3525) | Acc_1: (87.59%) (42717/48768)\n",
      "Epoch: 47 | Batch_idx: 390 |  Loss_1: (0.3537) | Acc_1: (87.54%) (43769/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3478) | Acc: (88.14%) (8814/10000)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.3675) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.3354) | Acc_1: (87.86%) (1237/1408)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.3421) | Acc_1: (87.39%) (2349/2688)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.3380) | Acc_1: (87.53%) (3473/3968)\n",
      "Epoch: 48 | Batch_idx: 40 |  Loss_1: (0.3467) | Acc_1: (87.23%) (4578/5248)\n",
      "Epoch: 48 | Batch_idx: 50 |  Loss_1: (0.3473) | Acc_1: (87.27%) (5697/6528)\n",
      "Epoch: 48 | Batch_idx: 60 |  Loss_1: (0.3465) | Acc_1: (87.33%) (6819/7808)\n",
      "Epoch: 48 | Batch_idx: 70 |  Loss_1: (0.3447) | Acc_1: (87.44%) (7947/9088)\n",
      "Epoch: 48 | Batch_idx: 80 |  Loss_1: (0.3440) | Acc_1: (87.49%) (9071/10368)\n",
      "Epoch: 48 | Batch_idx: 90 |  Loss_1: (0.3449) | Acc_1: (87.44%) (10185/11648)\n",
      "Epoch: 48 | Batch_idx: 100 |  Loss_1: (0.3466) | Acc_1: (87.45%) (11305/12928)\n",
      "Epoch: 48 | Batch_idx: 110 |  Loss_1: (0.3480) | Acc_1: (87.46%) (12426/14208)\n",
      "Epoch: 48 | Batch_idx: 120 |  Loss_1: (0.3481) | Acc_1: (87.52%) (13555/15488)\n",
      "Epoch: 48 | Batch_idx: 130 |  Loss_1: (0.3480) | Acc_1: (87.55%) (14680/16768)\n",
      "Epoch: 48 | Batch_idx: 140 |  Loss_1: (0.3504) | Acc_1: (87.42%) (15778/18048)\n",
      "Epoch: 48 | Batch_idx: 150 |  Loss_1: (0.3502) | Acc_1: (87.40%) (16892/19328)\n",
      "Epoch: 48 | Batch_idx: 160 |  Loss_1: (0.3491) | Acc_1: (87.42%) (18015/20608)\n",
      "Epoch: 48 | Batch_idx: 170 |  Loss_1: (0.3508) | Acc_1: (87.37%) (19123/21888)\n",
      "Epoch: 48 | Batch_idx: 180 |  Loss_1: (0.3494) | Acc_1: (87.44%) (20257/23168)\n",
      "Epoch: 48 | Batch_idx: 190 |  Loss_1: (0.3477) | Acc_1: (87.50%) (21392/24448)\n",
      "Epoch: 48 | Batch_idx: 200 |  Loss_1: (0.3494) | Acc_1: (87.44%) (22496/25728)\n",
      "Epoch: 48 | Batch_idx: 210 |  Loss_1: (0.3508) | Acc_1: (87.40%) (23604/27008)\n",
      "Epoch: 48 | Batch_idx: 220 |  Loss_1: (0.3498) | Acc_1: (87.46%) (24740/28288)\n",
      "Epoch: 48 | Batch_idx: 230 |  Loss_1: (0.3506) | Acc_1: (87.45%) (25856/29568)\n",
      "Epoch: 48 | Batch_idx: 240 |  Loss_1: (0.3494) | Acc_1: (87.43%) (26971/30848)\n",
      "Epoch: 48 | Batch_idx: 250 |  Loss_1: (0.3494) | Acc_1: (87.42%) (28085/32128)\n",
      "Epoch: 48 | Batch_idx: 260 |  Loss_1: (0.3483) | Acc_1: (87.46%) (29220/33408)\n",
      "Epoch: 48 | Batch_idx: 270 |  Loss_1: (0.3479) | Acc_1: (87.49%) (30348/34688)\n",
      "Epoch: 48 | Batch_idx: 280 |  Loss_1: (0.3482) | Acc_1: (87.48%) (31466/35968)\n",
      "Epoch: 48 | Batch_idx: 290 |  Loss_1: (0.3483) | Acc_1: (87.49%) (32587/37248)\n",
      "Epoch: 48 | Batch_idx: 300 |  Loss_1: (0.3489) | Acc_1: (87.50%) (33713/38528)\n",
      "Epoch: 48 | Batch_idx: 310 |  Loss_1: (0.3501) | Acc_1: (87.43%) (34803/39808)\n",
      "Epoch: 48 | Batch_idx: 320 |  Loss_1: (0.3503) | Acc_1: (87.43%) (35925/41088)\n",
      "Epoch: 48 | Batch_idx: 330 |  Loss_1: (0.3499) | Acc_1: (87.44%) (37045/42368)\n",
      "Epoch: 48 | Batch_idx: 340 |  Loss_1: (0.3494) | Acc_1: (87.49%) (38187/43648)\n",
      "Epoch: 48 | Batch_idx: 350 |  Loss_1: (0.3504) | Acc_1: (87.48%) (39302/44928)\n",
      "Epoch: 48 | Batch_idx: 360 |  Loss_1: (0.3511) | Acc_1: (87.45%) (40408/46208)\n",
      "Epoch: 48 | Batch_idx: 370 |  Loss_1: (0.3512) | Acc_1: (87.43%) (41517/47488)\n",
      "Epoch: 48 | Batch_idx: 380 |  Loss_1: (0.3511) | Acc_1: (87.42%) (42632/48768)\n",
      "Epoch: 48 | Batch_idx: 390 |  Loss_1: (0.3518) | Acc_1: (87.37%) (43685/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3334) | Acc: (88.32%) (8832/10000)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.3644) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.3599) | Acc_1: (87.78%) (1236/1408)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.3564) | Acc_1: (87.24%) (2345/2688)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.3481) | Acc_1: (87.27%) (3463/3968)\n",
      "Epoch: 49 | Batch_idx: 40 |  Loss_1: (0.3475) | Acc_1: (87.54%) (4594/5248)\n",
      "Epoch: 49 | Batch_idx: 50 |  Loss_1: (0.3470) | Acc_1: (87.62%) (5720/6528)\n",
      "Epoch: 49 | Batch_idx: 60 |  Loss_1: (0.3458) | Acc_1: (87.87%) (6861/7808)\n",
      "Epoch: 49 | Batch_idx: 70 |  Loss_1: (0.3447) | Acc_1: (87.90%) (7988/9088)\n",
      "Epoch: 49 | Batch_idx: 80 |  Loss_1: (0.3456) | Acc_1: (87.86%) (9109/10368)\n",
      "Epoch: 49 | Batch_idx: 90 |  Loss_1: (0.3432) | Acc_1: (87.87%) (10235/11648)\n",
      "Epoch: 49 | Batch_idx: 100 |  Loss_1: (0.3460) | Acc_1: (87.79%) (11350/12928)\n",
      "Epoch: 49 | Batch_idx: 110 |  Loss_1: (0.3476) | Acc_1: (87.72%) (12463/14208)\n",
      "Epoch: 49 | Batch_idx: 120 |  Loss_1: (0.3474) | Acc_1: (87.78%) (13595/15488)\n",
      "Epoch: 49 | Batch_idx: 130 |  Loss_1: (0.3471) | Acc_1: (87.83%) (14727/16768)\n",
      "Epoch: 49 | Batch_idx: 140 |  Loss_1: (0.3486) | Acc_1: (87.76%) (15839/18048)\n",
      "Epoch: 49 | Batch_idx: 150 |  Loss_1: (0.3486) | Acc_1: (87.76%) (16963/19328)\n",
      "Epoch: 49 | Batch_idx: 160 |  Loss_1: (0.3490) | Acc_1: (87.72%) (18078/20608)\n",
      "Epoch: 49 | Batch_idx: 170 |  Loss_1: (0.3484) | Acc_1: (87.76%) (19209/21888)\n",
      "Epoch: 49 | Batch_idx: 180 |  Loss_1: (0.3507) | Acc_1: (87.67%) (20312/23168)\n",
      "Epoch: 49 | Batch_idx: 190 |  Loss_1: (0.3494) | Acc_1: (87.70%) (21440/24448)\n",
      "Epoch: 49 | Batch_idx: 200 |  Loss_1: (0.3486) | Acc_1: (87.73%) (22571/25728)\n",
      "Epoch: 49 | Batch_idx: 210 |  Loss_1: (0.3490) | Acc_1: (87.71%) (23688/27008)\n",
      "Epoch: 49 | Batch_idx: 220 |  Loss_1: (0.3485) | Acc_1: (87.72%) (24814/28288)\n",
      "Epoch: 49 | Batch_idx: 230 |  Loss_1: (0.3481) | Acc_1: (87.75%) (25945/29568)\n",
      "Epoch: 49 | Batch_idx: 240 |  Loss_1: (0.3476) | Acc_1: (87.77%) (27076/30848)\n",
      "Epoch: 49 | Batch_idx: 250 |  Loss_1: (0.3475) | Acc_1: (87.76%) (28196/32128)\n",
      "Epoch: 49 | Batch_idx: 260 |  Loss_1: (0.3470) | Acc_1: (87.77%) (29322/33408)\n",
      "Epoch: 49 | Batch_idx: 270 |  Loss_1: (0.3477) | Acc_1: (87.76%) (30441/34688)\n",
      "Epoch: 49 | Batch_idx: 280 |  Loss_1: (0.3483) | Acc_1: (87.71%) (31548/35968)\n",
      "Epoch: 49 | Batch_idx: 290 |  Loss_1: (0.3490) | Acc_1: (87.70%) (32667/37248)\n",
      "Epoch: 49 | Batch_idx: 300 |  Loss_1: (0.3485) | Acc_1: (87.71%) (33791/38528)\n",
      "Epoch: 49 | Batch_idx: 310 |  Loss_1: (0.3487) | Acc_1: (87.72%) (34919/39808)\n",
      "Epoch: 49 | Batch_idx: 320 |  Loss_1: (0.3486) | Acc_1: (87.73%) (36047/41088)\n",
      "Epoch: 49 | Batch_idx: 330 |  Loss_1: (0.3475) | Acc_1: (87.76%) (37184/42368)\n",
      "Epoch: 49 | Batch_idx: 340 |  Loss_1: (0.3467) | Acc_1: (87.80%) (38325/43648)\n",
      "Epoch: 49 | Batch_idx: 350 |  Loss_1: (0.3465) | Acc_1: (87.80%) (39448/44928)\n",
      "Epoch: 49 | Batch_idx: 360 |  Loss_1: (0.3458) | Acc_1: (87.82%) (40580/46208)\n",
      "Epoch: 49 | Batch_idx: 370 |  Loss_1: (0.3461) | Acc_1: (87.80%) (41696/47488)\n",
      "Epoch: 49 | Batch_idx: 380 |  Loss_1: (0.3474) | Acc_1: (87.77%) (42805/48768)\n",
      "Epoch: 49 | Batch_idx: 390 |  Loss_1: (0.3494) | Acc_1: (87.71%) (43856/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3361) | Acc: (88.50%) (8850/10000)\n",
      "Epoch: 50 | Batch_idx: 0 |  Loss_1: (0.2884) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 50 | Batch_idx: 10 |  Loss_1: (0.4178) | Acc_1: (84.66%) (1192/1408)\n",
      "Epoch: 50 | Batch_idx: 20 |  Loss_1: (0.4727) | Acc_1: (82.55%) (2219/2688)\n",
      "Epoch: 50 | Batch_idx: 30 |  Loss_1: (0.4952) | Acc_1: (82.21%) (3262/3968)\n",
      "Epoch: 50 | Batch_idx: 40 |  Loss_1: (0.5194) | Acc_1: (81.55%) (4280/5248)\n",
      "Epoch: 50 | Batch_idx: 50 |  Loss_1: (0.5440) | Acc_1: (80.71%) (5269/6528)\n",
      "Epoch: 50 | Batch_idx: 60 |  Loss_1: (0.5585) | Acc_1: (80.07%) (6252/7808)\n",
      "Epoch: 50 | Batch_idx: 70 |  Loss_1: (0.5611) | Acc_1: (79.97%) (7268/9088)\n",
      "Epoch: 50 | Batch_idx: 80 |  Loss_1: (0.5620) | Acc_1: (79.83%) (8277/10368)\n",
      "Epoch: 50 | Batch_idx: 90 |  Loss_1: (0.5674) | Acc_1: (79.65%) (9278/11648)\n",
      "Epoch: 50 | Batch_idx: 100 |  Loss_1: (0.5713) | Acc_1: (79.50%) (10278/12928)\n",
      "Epoch: 50 | Batch_idx: 110 |  Loss_1: (0.5732) | Acc_1: (79.35%) (11274/14208)\n",
      "Epoch: 50 | Batch_idx: 120 |  Loss_1: (0.5744) | Acc_1: (79.34%) (12288/15488)\n",
      "Epoch: 50 | Batch_idx: 130 |  Loss_1: (0.5763) | Acc_1: (79.29%) (13296/16768)\n",
      "Epoch: 50 | Batch_idx: 140 |  Loss_1: (0.5784) | Acc_1: (79.34%) (14320/18048)\n",
      "Epoch: 50 | Batch_idx: 150 |  Loss_1: (0.5791) | Acc_1: (79.39%) (15345/19328)\n",
      "Epoch: 50 | Batch_idx: 160 |  Loss_1: (0.5809) | Acc_1: (79.39%) (16361/20608)\n",
      "Epoch: 50 | Batch_idx: 170 |  Loss_1: (0.5780) | Acc_1: (79.53%) (17408/21888)\n",
      "Epoch: 50 | Batch_idx: 180 |  Loss_1: (0.5772) | Acc_1: (79.60%) (18442/23168)\n",
      "Epoch: 50 | Batch_idx: 190 |  Loss_1: (0.5752) | Acc_1: (79.68%) (19481/24448)\n",
      "Epoch: 50 | Batch_idx: 200 |  Loss_1: (0.5730) | Acc_1: (79.79%) (20529/25728)\n",
      "Epoch: 50 | Batch_idx: 210 |  Loss_1: (0.5726) | Acc_1: (79.75%) (21538/27008)\n",
      "Epoch: 50 | Batch_idx: 220 |  Loss_1: (0.5719) | Acc_1: (79.85%) (22589/28288)\n",
      "Epoch: 50 | Batch_idx: 230 |  Loss_1: (0.5708) | Acc_1: (79.90%) (23626/29568)\n",
      "Epoch: 50 | Batch_idx: 240 |  Loss_1: (0.5715) | Acc_1: (79.86%) (24636/30848)\n",
      "Epoch: 50 | Batch_idx: 250 |  Loss_1: (0.5718) | Acc_1: (79.84%) (25651/32128)\n",
      "Epoch: 50 | Batch_idx: 260 |  Loss_1: (0.5733) | Acc_1: (79.82%) (26667/33408)\n",
      "Epoch: 50 | Batch_idx: 270 |  Loss_1: (0.5720) | Acc_1: (79.86%) (27702/34688)\n",
      "Epoch: 50 | Batch_idx: 280 |  Loss_1: (0.5719) | Acc_1: (79.85%) (28719/35968)\n",
      "Epoch: 50 | Batch_idx: 290 |  Loss_1: (0.5742) | Acc_1: (79.79%) (29720/37248)\n",
      "Epoch: 50 | Batch_idx: 300 |  Loss_1: (0.5722) | Acc_1: (79.86%) (30767/38528)\n",
      "Epoch: 50 | Batch_idx: 310 |  Loss_1: (0.5721) | Acc_1: (79.87%) (31793/39808)\n",
      "Epoch: 50 | Batch_idx: 320 |  Loss_1: (0.5707) | Acc_1: (79.93%) (32843/41088)\n",
      "Epoch: 50 | Batch_idx: 330 |  Loss_1: (0.5705) | Acc_1: (79.94%) (33868/42368)\n",
      "Epoch: 50 | Batch_idx: 340 |  Loss_1: (0.5708) | Acc_1: (79.94%) (34894/43648)\n",
      "Epoch: 50 | Batch_idx: 350 |  Loss_1: (0.5705) | Acc_1: (79.99%) (35936/44928)\n",
      "Epoch: 50 | Batch_idx: 360 |  Loss_1: (0.5698) | Acc_1: (80.03%) (36979/46208)\n",
      "Epoch: 50 | Batch_idx: 370 |  Loss_1: (0.5693) | Acc_1: (80.06%) (38018/47488)\n",
      "Epoch: 50 | Batch_idx: 380 |  Loss_1: (0.5705) | Acc_1: (80.05%) (39041/48768)\n",
      "Epoch: 50 | Batch_idx: 390 |  Loss_1: (0.5705) | Acc_1: (80.08%) (40039/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4770) | Acc: (83.82%) (8382/10000)\n",
      "Epoch: 51 | Batch_idx: 0 |  Loss_1: (0.4876) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 51 | Batch_idx: 10 |  Loss_1: (0.5669) | Acc_1: (80.11%) (1128/1408)\n",
      "Epoch: 51 | Batch_idx: 20 |  Loss_1: (0.5534) | Acc_1: (80.65%) (2168/2688)\n",
      "Epoch: 51 | Batch_idx: 30 |  Loss_1: (0.5525) | Acc_1: (80.65%) (3200/3968)\n",
      "Epoch: 51 | Batch_idx: 40 |  Loss_1: (0.5547) | Acc_1: (80.68%) (4234/5248)\n",
      "Epoch: 51 | Batch_idx: 50 |  Loss_1: (0.5604) | Acc_1: (80.53%) (5257/6528)\n",
      "Epoch: 51 | Batch_idx: 60 |  Loss_1: (0.5618) | Acc_1: (80.74%) (6304/7808)\n",
      "Epoch: 51 | Batch_idx: 70 |  Loss_1: (0.5587) | Acc_1: (80.82%) (7345/9088)\n",
      "Epoch: 51 | Batch_idx: 80 |  Loss_1: (0.5579) | Acc_1: (80.94%) (8392/10368)\n",
      "Epoch: 51 | Batch_idx: 90 |  Loss_1: (0.5589) | Acc_1: (80.92%) (9425/11648)\n",
      "Epoch: 51 | Batch_idx: 100 |  Loss_1: (0.5612) | Acc_1: (80.91%) (10460/12928)\n",
      "Epoch: 51 | Batch_idx: 110 |  Loss_1: (0.5646) | Acc_1: (80.81%) (11482/14208)\n",
      "Epoch: 51 | Batch_idx: 120 |  Loss_1: (0.5626) | Acc_1: (80.83%) (12519/15488)\n",
      "Epoch: 51 | Batch_idx: 130 |  Loss_1: (0.5640) | Acc_1: (80.71%) (13533/16768)\n",
      "Epoch: 51 | Batch_idx: 140 |  Loss_1: (0.5651) | Acc_1: (80.63%) (14553/18048)\n",
      "Epoch: 51 | Batch_idx: 150 |  Loss_1: (0.5627) | Acc_1: (80.62%) (15583/19328)\n",
      "Epoch: 51 | Batch_idx: 160 |  Loss_1: (0.5625) | Acc_1: (80.63%) (16617/20608)\n",
      "Epoch: 51 | Batch_idx: 170 |  Loss_1: (0.5625) | Acc_1: (80.64%) (17650/21888)\n",
      "Epoch: 51 | Batch_idx: 180 |  Loss_1: (0.5610) | Acc_1: (80.68%) (18691/23168)\n",
      "Epoch: 51 | Batch_idx: 190 |  Loss_1: (0.5599) | Acc_1: (80.72%) (19735/24448)\n",
      "Epoch: 51 | Batch_idx: 200 |  Loss_1: (0.5600) | Acc_1: (80.64%) (20748/25728)\n",
      "Epoch: 51 | Batch_idx: 210 |  Loss_1: (0.5604) | Acc_1: (80.64%) (21778/27008)\n",
      "Epoch: 51 | Batch_idx: 220 |  Loss_1: (0.5611) | Acc_1: (80.60%) (22800/28288)\n",
      "Epoch: 51 | Batch_idx: 230 |  Loss_1: (0.5627) | Acc_1: (80.51%) (23804/29568)\n",
      "Epoch: 51 | Batch_idx: 240 |  Loss_1: (0.5619) | Acc_1: (80.58%) (24858/30848)\n",
      "Epoch: 51 | Batch_idx: 250 |  Loss_1: (0.5605) | Acc_1: (80.60%) (25895/32128)\n",
      "Epoch: 51 | Batch_idx: 260 |  Loss_1: (0.5608) | Acc_1: (80.56%) (26914/33408)\n",
      "Epoch: 51 | Batch_idx: 270 |  Loss_1: (0.5609) | Acc_1: (80.53%) (27935/34688)\n",
      "Epoch: 51 | Batch_idx: 280 |  Loss_1: (0.5573) | Acc_1: (80.65%) (29007/35968)\n",
      "Epoch: 51 | Batch_idx: 290 |  Loss_1: (0.5569) | Acc_1: (80.69%) (30057/37248)\n",
      "Epoch: 51 | Batch_idx: 300 |  Loss_1: (0.5567) | Acc_1: (80.71%) (31097/38528)\n",
      "Epoch: 51 | Batch_idx: 310 |  Loss_1: (0.5587) | Acc_1: (80.64%) (32102/39808)\n",
      "Epoch: 51 | Batch_idx: 320 |  Loss_1: (0.5586) | Acc_1: (80.67%) (33147/41088)\n",
      "Epoch: 51 | Batch_idx: 330 |  Loss_1: (0.5594) | Acc_1: (80.66%) (34175/42368)\n",
      "Epoch: 51 | Batch_idx: 340 |  Loss_1: (0.5585) | Acc_1: (80.67%) (35213/43648)\n",
      "Epoch: 51 | Batch_idx: 350 |  Loss_1: (0.5578) | Acc_1: (80.69%) (36251/44928)\n",
      "Epoch: 51 | Batch_idx: 360 |  Loss_1: (0.5581) | Acc_1: (80.67%) (37275/46208)\n",
      "Epoch: 51 | Batch_idx: 370 |  Loss_1: (0.5599) | Acc_1: (80.63%) (38290/47488)\n",
      "Epoch: 51 | Batch_idx: 380 |  Loss_1: (0.5606) | Acc_1: (80.60%) (39305/48768)\n",
      "Epoch: 51 | Batch_idx: 390 |  Loss_1: (0.5617) | Acc_1: (80.56%) (40280/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4291) | Acc: (85.27%) (8527/10000)\n",
      "Epoch: 52 | Batch_idx: 0 |  Loss_1: (0.6590) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 52 | Batch_idx: 10 |  Loss_1: (0.5021) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 52 | Batch_idx: 20 |  Loss_1: (0.4956) | Acc_1: (82.51%) (2218/2688)\n",
      "Epoch: 52 | Batch_idx: 30 |  Loss_1: (0.5060) | Acc_1: (82.08%) (3257/3968)\n",
      "Epoch: 52 | Batch_idx: 40 |  Loss_1: (0.5163) | Acc_1: (81.61%) (4283/5248)\n",
      "Epoch: 52 | Batch_idx: 50 |  Loss_1: (0.5246) | Acc_1: (81.30%) (5307/6528)\n",
      "Epoch: 52 | Batch_idx: 60 |  Loss_1: (0.5185) | Acc_1: (81.58%) (6370/7808)\n",
      "Epoch: 52 | Batch_idx: 70 |  Loss_1: (0.5201) | Acc_1: (81.51%) (7408/9088)\n",
      "Epoch: 52 | Batch_idx: 80 |  Loss_1: (0.5227) | Acc_1: (81.62%) (8462/10368)\n",
      "Epoch: 52 | Batch_idx: 90 |  Loss_1: (0.5245) | Acc_1: (81.62%) (9507/11648)\n",
      "Epoch: 52 | Batch_idx: 100 |  Loss_1: (0.5295) | Acc_1: (81.45%) (10530/12928)\n",
      "Epoch: 52 | Batch_idx: 110 |  Loss_1: (0.5287) | Acc_1: (81.48%) (11576/14208)\n",
      "Epoch: 52 | Batch_idx: 120 |  Loss_1: (0.5318) | Acc_1: (81.41%) (12609/15488)\n",
      "Epoch: 52 | Batch_idx: 130 |  Loss_1: (0.5296) | Acc_1: (81.50%) (13666/16768)\n",
      "Epoch: 52 | Batch_idx: 140 |  Loss_1: (0.5295) | Acc_1: (81.52%) (14712/18048)\n",
      "Epoch: 52 | Batch_idx: 150 |  Loss_1: (0.5308) | Acc_1: (81.53%) (15758/19328)\n",
      "Epoch: 52 | Batch_idx: 160 |  Loss_1: (0.5319) | Acc_1: (81.51%) (16798/20608)\n",
      "Epoch: 52 | Batch_idx: 170 |  Loss_1: (0.5333) | Acc_1: (81.47%) (17833/21888)\n",
      "Epoch: 52 | Batch_idx: 180 |  Loss_1: (0.5310) | Acc_1: (81.57%) (18897/23168)\n",
      "Epoch: 52 | Batch_idx: 190 |  Loss_1: (0.5306) | Acc_1: (81.59%) (19946/24448)\n",
      "Epoch: 52 | Batch_idx: 200 |  Loss_1: (0.5310) | Acc_1: (81.58%) (20989/25728)\n",
      "Epoch: 52 | Batch_idx: 210 |  Loss_1: (0.5291) | Acc_1: (81.67%) (22057/27008)\n",
      "Epoch: 52 | Batch_idx: 220 |  Loss_1: (0.5311) | Acc_1: (81.60%) (23082/28288)\n",
      "Epoch: 52 | Batch_idx: 230 |  Loss_1: (0.5304) | Acc_1: (81.65%) (24142/29568)\n",
      "Epoch: 52 | Batch_idx: 240 |  Loss_1: (0.5304) | Acc_1: (81.66%) (25191/30848)\n",
      "Epoch: 52 | Batch_idx: 250 |  Loss_1: (0.5283) | Acc_1: (81.76%) (26268/32128)\n",
      "Epoch: 52 | Batch_idx: 260 |  Loss_1: (0.5287) | Acc_1: (81.78%) (27321/33408)\n",
      "Epoch: 52 | Batch_idx: 270 |  Loss_1: (0.5275) | Acc_1: (81.81%) (28378/34688)\n",
      "Epoch: 52 | Batch_idx: 280 |  Loss_1: (0.5256) | Acc_1: (81.87%) (29447/35968)\n",
      "Epoch: 52 | Batch_idx: 290 |  Loss_1: (0.5275) | Acc_1: (81.78%) (30461/37248)\n",
      "Epoch: 52 | Batch_idx: 300 |  Loss_1: (0.5295) | Acc_1: (81.71%) (31480/38528)\n",
      "Epoch: 52 | Batch_idx: 310 |  Loss_1: (0.5298) | Acc_1: (81.70%) (32524/39808)\n",
      "Epoch: 52 | Batch_idx: 320 |  Loss_1: (0.5303) | Acc_1: (81.66%) (33554/41088)\n",
      "Epoch: 52 | Batch_idx: 330 |  Loss_1: (0.5308) | Acc_1: (81.68%) (34605/42368)\n",
      "Epoch: 52 | Batch_idx: 340 |  Loss_1: (0.5313) | Acc_1: (81.65%) (35640/43648)\n",
      "Epoch: 52 | Batch_idx: 350 |  Loss_1: (0.5329) | Acc_1: (81.61%) (36666/44928)\n",
      "Epoch: 52 | Batch_idx: 360 |  Loss_1: (0.5345) | Acc_1: (81.55%) (37684/46208)\n",
      "Epoch: 52 | Batch_idx: 370 |  Loss_1: (0.5346) | Acc_1: (81.56%) (38729/47488)\n",
      "Epoch: 52 | Batch_idx: 380 |  Loss_1: (0.5351) | Acc_1: (81.54%) (39765/48768)\n",
      "Epoch: 52 | Batch_idx: 390 |  Loss_1: (0.5357) | Acc_1: (81.52%) (40758/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4230) | Acc: (85.55%) (8555/10000)\n",
      "Epoch: 53 | Batch_idx: 0 |  Loss_1: (0.6343) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 53 | Batch_idx: 10 |  Loss_1: (0.5553) | Acc_1: (81.25%) (1144/1408)\n",
      "Epoch: 53 | Batch_idx: 20 |  Loss_1: (0.5345) | Acc_1: (81.55%) (2192/2688)\n",
      "Epoch: 53 | Batch_idx: 30 |  Loss_1: (0.5230) | Acc_1: (81.75%) (3244/3968)\n",
      "Epoch: 53 | Batch_idx: 40 |  Loss_1: (0.5085) | Acc_1: (82.22%) (4315/5248)\n",
      "Epoch: 53 | Batch_idx: 50 |  Loss_1: (0.5141) | Acc_1: (81.91%) (5347/6528)\n",
      "Epoch: 53 | Batch_idx: 60 |  Loss_1: (0.5116) | Acc_1: (82.06%) (6407/7808)\n",
      "Epoch: 53 | Batch_idx: 70 |  Loss_1: (0.5145) | Acc_1: (81.95%) (7448/9088)\n",
      "Epoch: 53 | Batch_idx: 80 |  Loss_1: (0.5217) | Acc_1: (81.54%) (8454/10368)\n",
      "Epoch: 53 | Batch_idx: 90 |  Loss_1: (0.5264) | Acc_1: (81.26%) (9465/11648)\n",
      "Epoch: 53 | Batch_idx: 100 |  Loss_1: (0.5277) | Acc_1: (81.25%) (10504/12928)\n",
      "Epoch: 53 | Batch_idx: 110 |  Loss_1: (0.5236) | Acc_1: (81.36%) (11559/14208)\n",
      "Epoch: 53 | Batch_idx: 120 |  Loss_1: (0.5217) | Acc_1: (81.51%) (12624/15488)\n",
      "Epoch: 53 | Batch_idx: 130 |  Loss_1: (0.5244) | Acc_1: (81.52%) (13670/16768)\n",
      "Epoch: 53 | Batch_idx: 140 |  Loss_1: (0.5242) | Acc_1: (81.62%) (14731/18048)\n",
      "Epoch: 53 | Batch_idx: 150 |  Loss_1: (0.5269) | Acc_1: (81.59%) (15769/19328)\n",
      "Epoch: 53 | Batch_idx: 160 |  Loss_1: (0.5264) | Acc_1: (81.63%) (16823/20608)\n",
      "Epoch: 53 | Batch_idx: 170 |  Loss_1: (0.5263) | Acc_1: (81.61%) (17863/21888)\n",
      "Epoch: 53 | Batch_idx: 180 |  Loss_1: (0.5282) | Acc_1: (81.48%) (18877/23168)\n",
      "Epoch: 53 | Batch_idx: 190 |  Loss_1: (0.5290) | Acc_1: (81.45%) (19913/24448)\n",
      "Epoch: 53 | Batch_idx: 200 |  Loss_1: (0.5293) | Acc_1: (81.45%) (20955/25728)\n",
      "Epoch: 53 | Batch_idx: 210 |  Loss_1: (0.5292) | Acc_1: (81.49%) (22008/27008)\n",
      "Epoch: 53 | Batch_idx: 220 |  Loss_1: (0.5288) | Acc_1: (81.52%) (23059/28288)\n",
      "Epoch: 53 | Batch_idx: 230 |  Loss_1: (0.5293) | Acc_1: (81.54%) (24110/29568)\n",
      "Epoch: 53 | Batch_idx: 240 |  Loss_1: (0.5294) | Acc_1: (81.52%) (25146/30848)\n",
      "Epoch: 53 | Batch_idx: 250 |  Loss_1: (0.5306) | Acc_1: (81.51%) (26187/32128)\n",
      "Epoch: 53 | Batch_idx: 260 |  Loss_1: (0.5308) | Acc_1: (81.47%) (27218/33408)\n",
      "Epoch: 53 | Batch_idx: 270 |  Loss_1: (0.5305) | Acc_1: (81.46%) (28258/34688)\n",
      "Epoch: 53 | Batch_idx: 280 |  Loss_1: (0.5305) | Acc_1: (81.49%) (29309/35968)\n",
      "Epoch: 53 | Batch_idx: 290 |  Loss_1: (0.5299) | Acc_1: (81.51%) (30361/37248)\n",
      "Epoch: 53 | Batch_idx: 300 |  Loss_1: (0.5298) | Acc_1: (81.54%) (31414/38528)\n",
      "Epoch: 53 | Batch_idx: 310 |  Loss_1: (0.5309) | Acc_1: (81.54%) (32459/39808)\n",
      "Epoch: 53 | Batch_idx: 320 |  Loss_1: (0.5312) | Acc_1: (81.53%) (33501/41088)\n",
      "Epoch: 53 | Batch_idx: 330 |  Loss_1: (0.5300) | Acc_1: (81.55%) (34553/42368)\n",
      "Epoch: 53 | Batch_idx: 340 |  Loss_1: (0.5294) | Acc_1: (81.60%) (35616/43648)\n",
      "Epoch: 53 | Batch_idx: 350 |  Loss_1: (0.5294) | Acc_1: (81.60%) (36662/44928)\n",
      "Epoch: 53 | Batch_idx: 360 |  Loss_1: (0.5285) | Acc_1: (81.64%) (37725/46208)\n",
      "Epoch: 53 | Batch_idx: 370 |  Loss_1: (0.5282) | Acc_1: (81.64%) (38770/47488)\n",
      "Epoch: 53 | Batch_idx: 380 |  Loss_1: (0.5270) | Acc_1: (81.66%) (39826/48768)\n",
      "Epoch: 53 | Batch_idx: 390 |  Loss_1: (0.5279) | Acc_1: (81.64%) (40822/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4959) | Acc: (84.07%) (8407/10000)\n",
      "Epoch: 54 | Batch_idx: 0 |  Loss_1: (0.3828) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 54 | Batch_idx: 10 |  Loss_1: (0.4907) | Acc_1: (82.81%) (1166/1408)\n",
      "Epoch: 54 | Batch_idx: 20 |  Loss_1: (0.5152) | Acc_1: (82.25%) (2211/2688)\n",
      "Epoch: 54 | Batch_idx: 30 |  Loss_1: (0.5097) | Acc_1: (82.28%) (3265/3968)\n",
      "Epoch: 54 | Batch_idx: 40 |  Loss_1: (0.5096) | Acc_1: (82.43%) (4326/5248)\n",
      "Epoch: 54 | Batch_idx: 50 |  Loss_1: (0.5008) | Acc_1: (82.80%) (5405/6528)\n",
      "Epoch: 54 | Batch_idx: 60 |  Loss_1: (0.5014) | Acc_1: (82.83%) (6467/7808)\n",
      "Epoch: 54 | Batch_idx: 70 |  Loss_1: (0.5057) | Acc_1: (82.59%) (7506/9088)\n",
      "Epoch: 54 | Batch_idx: 80 |  Loss_1: (0.5044) | Acc_1: (82.64%) (8568/10368)\n",
      "Epoch: 54 | Batch_idx: 90 |  Loss_1: (0.5040) | Acc_1: (82.62%) (9624/11648)\n",
      "Epoch: 54 | Batch_idx: 100 |  Loss_1: (0.5061) | Acc_1: (82.65%) (10685/12928)\n",
      "Epoch: 54 | Batch_idx: 110 |  Loss_1: (0.5073) | Acc_1: (82.58%) (11733/14208)\n",
      "Epoch: 54 | Batch_idx: 120 |  Loss_1: (0.5098) | Acc_1: (82.43%) (12767/15488)\n",
      "Epoch: 54 | Batch_idx: 130 |  Loss_1: (0.5106) | Acc_1: (82.44%) (13823/16768)\n",
      "Epoch: 54 | Batch_idx: 140 |  Loss_1: (0.5103) | Acc_1: (82.50%) (14889/18048)\n",
      "Epoch: 54 | Batch_idx: 150 |  Loss_1: (0.5092) | Acc_1: (82.53%) (15952/19328)\n",
      "Epoch: 54 | Batch_idx: 160 |  Loss_1: (0.5073) | Acc_1: (82.57%) (17015/20608)\n",
      "Epoch: 54 | Batch_idx: 170 |  Loss_1: (0.5068) | Acc_1: (82.52%) (18062/21888)\n",
      "Epoch: 54 | Batch_idx: 180 |  Loss_1: (0.5077) | Acc_1: (82.50%) (19113/23168)\n",
      "Epoch: 54 | Batch_idx: 190 |  Loss_1: (0.5081) | Acc_1: (82.47%) (20162/24448)\n",
      "Epoch: 54 | Batch_idx: 200 |  Loss_1: (0.5083) | Acc_1: (82.45%) (21212/25728)\n",
      "Epoch: 54 | Batch_idx: 210 |  Loss_1: (0.5086) | Acc_1: (82.46%) (22272/27008)\n",
      "Epoch: 54 | Batch_idx: 220 |  Loss_1: (0.5091) | Acc_1: (82.43%) (23319/28288)\n",
      "Epoch: 54 | Batch_idx: 230 |  Loss_1: (0.5112) | Acc_1: (82.38%) (24359/29568)\n",
      "Epoch: 54 | Batch_idx: 240 |  Loss_1: (0.5110) | Acc_1: (82.40%) (25420/30848)\n",
      "Epoch: 54 | Batch_idx: 250 |  Loss_1: (0.5119) | Acc_1: (82.40%) (26473/32128)\n",
      "Epoch: 54 | Batch_idx: 260 |  Loss_1: (0.5095) | Acc_1: (82.47%) (27553/33408)\n",
      "Epoch: 54 | Batch_idx: 270 |  Loss_1: (0.5091) | Acc_1: (82.52%) (28625/34688)\n",
      "Epoch: 54 | Batch_idx: 280 |  Loss_1: (0.5103) | Acc_1: (82.49%) (29669/35968)\n",
      "Epoch: 54 | Batch_idx: 290 |  Loss_1: (0.5118) | Acc_1: (82.43%) (30704/37248)\n",
      "Epoch: 54 | Batch_idx: 300 |  Loss_1: (0.5124) | Acc_1: (82.37%) (31736/38528)\n",
      "Epoch: 54 | Batch_idx: 310 |  Loss_1: (0.5123) | Acc_1: (82.35%) (32782/39808)\n",
      "Epoch: 54 | Batch_idx: 320 |  Loss_1: (0.5121) | Acc_1: (82.35%) (33834/41088)\n",
      "Epoch: 54 | Batch_idx: 330 |  Loss_1: (0.5124) | Acc_1: (82.35%) (34889/42368)\n",
      "Epoch: 54 | Batch_idx: 340 |  Loss_1: (0.5115) | Acc_1: (82.36%) (35950/43648)\n",
      "Epoch: 54 | Batch_idx: 350 |  Loss_1: (0.5128) | Acc_1: (82.33%) (36987/44928)\n",
      "Epoch: 54 | Batch_idx: 360 |  Loss_1: (0.5130) | Acc_1: (82.33%) (38045/46208)\n",
      "Epoch: 54 | Batch_idx: 370 |  Loss_1: (0.5140) | Acc_1: (82.27%) (39070/47488)\n",
      "Epoch: 54 | Batch_idx: 380 |  Loss_1: (0.5142) | Acc_1: (82.25%) (40111/48768)\n",
      "Epoch: 54 | Batch_idx: 390 |  Loss_1: (0.5146) | Acc_1: (82.23%) (41114/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4041) | Acc: (86.21%) (8621/10000)\n",
      "Epoch: 55 | Batch_idx: 0 |  Loss_1: (0.3606) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 55 | Batch_idx: 10 |  Loss_1: (0.4725) | Acc_1: (83.66%) (1178/1408)\n",
      "Epoch: 55 | Batch_idx: 20 |  Loss_1: (0.4778) | Acc_1: (83.30%) (2239/2688)\n",
      "Epoch: 55 | Batch_idx: 30 |  Loss_1: (0.4874) | Acc_1: (82.86%) (3288/3968)\n",
      "Epoch: 55 | Batch_idx: 40 |  Loss_1: (0.4933) | Acc_1: (82.62%) (4336/5248)\n",
      "Epoch: 55 | Batch_idx: 50 |  Loss_1: (0.4789) | Acc_1: (83.21%) (5432/6528)\n",
      "Epoch: 55 | Batch_idx: 60 |  Loss_1: (0.4855) | Acc_1: (83.07%) (6486/7808)\n",
      "Epoch: 55 | Batch_idx: 70 |  Loss_1: (0.4826) | Acc_1: (83.01%) (7544/9088)\n",
      "Epoch: 55 | Batch_idx: 80 |  Loss_1: (0.4829) | Acc_1: (83.05%) (8611/10368)\n",
      "Epoch: 55 | Batch_idx: 90 |  Loss_1: (0.4794) | Acc_1: (83.21%) (9692/11648)\n",
      "Epoch: 55 | Batch_idx: 100 |  Loss_1: (0.4817) | Acc_1: (83.23%) (10760/12928)\n",
      "Epoch: 55 | Batch_idx: 110 |  Loss_1: (0.4807) | Acc_1: (83.27%) (11831/14208)\n",
      "Epoch: 55 | Batch_idx: 120 |  Loss_1: (0.4810) | Acc_1: (83.29%) (12900/15488)\n",
      "Epoch: 55 | Batch_idx: 130 |  Loss_1: (0.4828) | Acc_1: (83.28%) (13964/16768)\n",
      "Epoch: 55 | Batch_idx: 140 |  Loss_1: (0.4840) | Acc_1: (83.18%) (15012/18048)\n",
      "Epoch: 55 | Batch_idx: 150 |  Loss_1: (0.4852) | Acc_1: (83.15%) (16071/19328)\n",
      "Epoch: 55 | Batch_idx: 160 |  Loss_1: (0.4892) | Acc_1: (83.04%) (17112/20608)\n",
      "Epoch: 55 | Batch_idx: 170 |  Loss_1: (0.4911) | Acc_1: (83.01%) (18170/21888)\n",
      "Epoch: 55 | Batch_idx: 180 |  Loss_1: (0.4907) | Acc_1: (83.01%) (19231/23168)\n",
      "Epoch: 55 | Batch_idx: 190 |  Loss_1: (0.4923) | Acc_1: (83.00%) (20292/24448)\n",
      "Epoch: 55 | Batch_idx: 200 |  Loss_1: (0.4925) | Acc_1: (82.99%) (21351/25728)\n",
      "Epoch: 55 | Batch_idx: 210 |  Loss_1: (0.4932) | Acc_1: (82.96%) (22406/27008)\n",
      "Epoch: 55 | Batch_idx: 220 |  Loss_1: (0.4952) | Acc_1: (82.90%) (23450/28288)\n",
      "Epoch: 55 | Batch_idx: 230 |  Loss_1: (0.4955) | Acc_1: (82.87%) (24504/29568)\n",
      "Epoch: 55 | Batch_idx: 240 |  Loss_1: (0.4961) | Acc_1: (82.84%) (25555/30848)\n",
      "Epoch: 55 | Batch_idx: 250 |  Loss_1: (0.4962) | Acc_1: (82.85%) (26617/32128)\n",
      "Epoch: 55 | Batch_idx: 260 |  Loss_1: (0.4978) | Acc_1: (82.82%) (27669/33408)\n",
      "Epoch: 55 | Batch_idx: 270 |  Loss_1: (0.4979) | Acc_1: (82.85%) (28739/34688)\n",
      "Epoch: 55 | Batch_idx: 280 |  Loss_1: (0.4977) | Acc_1: (82.85%) (29801/35968)\n",
      "Epoch: 55 | Batch_idx: 290 |  Loss_1: (0.4987) | Acc_1: (82.80%) (30841/37248)\n",
      "Epoch: 55 | Batch_idx: 300 |  Loss_1: (0.4987) | Acc_1: (82.79%) (31897/38528)\n",
      "Epoch: 55 | Batch_idx: 310 |  Loss_1: (0.4986) | Acc_1: (82.81%) (32964/39808)\n",
      "Epoch: 55 | Batch_idx: 320 |  Loss_1: (0.4980) | Acc_1: (82.82%) (34031/41088)\n",
      "Epoch: 55 | Batch_idx: 330 |  Loss_1: (0.4983) | Acc_1: (82.79%) (35077/42368)\n",
      "Epoch: 55 | Batch_idx: 340 |  Loss_1: (0.4978) | Acc_1: (82.79%) (36137/43648)\n",
      "Epoch: 55 | Batch_idx: 350 |  Loss_1: (0.4987) | Acc_1: (82.76%) (37182/44928)\n",
      "Epoch: 55 | Batch_idx: 360 |  Loss_1: (0.4996) | Acc_1: (82.71%) (38218/46208)\n",
      "Epoch: 55 | Batch_idx: 370 |  Loss_1: (0.4997) | Acc_1: (82.69%) (39267/47488)\n",
      "Epoch: 55 | Batch_idx: 380 |  Loss_1: (0.4988) | Acc_1: (82.72%) (40341/48768)\n",
      "Epoch: 55 | Batch_idx: 390 |  Loss_1: (0.4993) | Acc_1: (82.68%) (41340/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4036) | Acc: (86.12%) (8612/10000)\n",
      "Epoch: 56 | Batch_idx: 0 |  Loss_1: (0.4595) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 56 | Batch_idx: 10 |  Loss_1: (0.4906) | Acc_1: (82.60%) (1163/1408)\n",
      "Epoch: 56 | Batch_idx: 20 |  Loss_1: (0.5020) | Acc_1: (82.44%) (2216/2688)\n",
      "Epoch: 56 | Batch_idx: 30 |  Loss_1: (0.5006) | Acc_1: (82.64%) (3279/3968)\n",
      "Epoch: 56 | Batch_idx: 40 |  Loss_1: (0.4996) | Acc_1: (82.62%) (4336/5248)\n",
      "Epoch: 56 | Batch_idx: 50 |  Loss_1: (0.4974) | Acc_1: (82.64%) (5395/6528)\n",
      "Epoch: 56 | Batch_idx: 60 |  Loss_1: (0.5012) | Acc_1: (82.58%) (6448/7808)\n",
      "Epoch: 56 | Batch_idx: 70 |  Loss_1: (0.5056) | Acc_1: (82.48%) (7496/9088)\n",
      "Epoch: 56 | Batch_idx: 80 |  Loss_1: (0.4964) | Acc_1: (82.75%) (8580/10368)\n",
      "Epoch: 56 | Batch_idx: 90 |  Loss_1: (0.4910) | Acc_1: (82.99%) (9667/11648)\n",
      "Epoch: 56 | Batch_idx: 100 |  Loss_1: (0.4910) | Acc_1: (83.13%) (10747/12928)\n",
      "Epoch: 56 | Batch_idx: 110 |  Loss_1: (0.4897) | Acc_1: (83.20%) (11821/14208)\n",
      "Epoch: 56 | Batch_idx: 120 |  Loss_1: (0.4933) | Acc_1: (83.11%) (12872/15488)\n",
      "Epoch: 56 | Batch_idx: 130 |  Loss_1: (0.4920) | Acc_1: (83.08%) (13931/16768)\n",
      "Epoch: 56 | Batch_idx: 140 |  Loss_1: (0.4939) | Acc_1: (83.06%) (14991/18048)\n",
      "Epoch: 56 | Batch_idx: 150 |  Loss_1: (0.4928) | Acc_1: (83.03%) (16049/19328)\n",
      "Epoch: 56 | Batch_idx: 160 |  Loss_1: (0.4919) | Acc_1: (83.10%) (17125/20608)\n",
      "Epoch: 56 | Batch_idx: 170 |  Loss_1: (0.4916) | Acc_1: (83.12%) (18194/21888)\n",
      "Epoch: 56 | Batch_idx: 180 |  Loss_1: (0.4912) | Acc_1: (83.09%) (19251/23168)\n",
      "Epoch: 56 | Batch_idx: 190 |  Loss_1: (0.4925) | Acc_1: (83.03%) (20300/24448)\n",
      "Epoch: 56 | Batch_idx: 200 |  Loss_1: (0.4941) | Acc_1: (82.98%) (21350/25728)\n",
      "Epoch: 56 | Batch_idx: 210 |  Loss_1: (0.4947) | Acc_1: (82.91%) (22393/27008)\n",
      "Epoch: 56 | Batch_idx: 220 |  Loss_1: (0.4975) | Acc_1: (82.84%) (23435/28288)\n",
      "Epoch: 56 | Batch_idx: 230 |  Loss_1: (0.4981) | Acc_1: (82.79%) (24479/29568)\n",
      "Epoch: 56 | Batch_idx: 240 |  Loss_1: (0.4981) | Acc_1: (82.75%) (25526/30848)\n",
      "Epoch: 56 | Batch_idx: 250 |  Loss_1: (0.4983) | Acc_1: (82.71%) (26573/32128)\n",
      "Epoch: 56 | Batch_idx: 260 |  Loss_1: (0.4983) | Acc_1: (82.69%) (27626/33408)\n",
      "Epoch: 56 | Batch_idx: 270 |  Loss_1: (0.4990) | Acc_1: (82.65%) (28671/34688)\n",
      "Epoch: 56 | Batch_idx: 280 |  Loss_1: (0.4986) | Acc_1: (82.70%) (29745/35968)\n",
      "Epoch: 56 | Batch_idx: 290 |  Loss_1: (0.4975) | Acc_1: (82.76%) (30828/37248)\n",
      "Epoch: 56 | Batch_idx: 300 |  Loss_1: (0.4983) | Acc_1: (82.73%) (31874/38528)\n",
      "Epoch: 56 | Batch_idx: 310 |  Loss_1: (0.4980) | Acc_1: (82.72%) (32931/39808)\n",
      "Epoch: 56 | Batch_idx: 320 |  Loss_1: (0.4963) | Acc_1: (82.79%) (34018/41088)\n",
      "Epoch: 56 | Batch_idx: 330 |  Loss_1: (0.4970) | Acc_1: (82.77%) (35068/42368)\n",
      "Epoch: 56 | Batch_idx: 340 |  Loss_1: (0.4977) | Acc_1: (82.74%) (36114/43648)\n",
      "Epoch: 56 | Batch_idx: 350 |  Loss_1: (0.4980) | Acc_1: (82.74%) (37174/44928)\n",
      "Epoch: 56 | Batch_idx: 360 |  Loss_1: (0.4980) | Acc_1: (82.75%) (38236/46208)\n",
      "Epoch: 56 | Batch_idx: 370 |  Loss_1: (0.4979) | Acc_1: (82.76%) (39300/47488)\n",
      "Epoch: 56 | Batch_idx: 380 |  Loss_1: (0.4971) | Acc_1: (82.83%) (40395/48768)\n",
      "Epoch: 56 | Batch_idx: 390 |  Loss_1: (0.4966) | Acc_1: (82.84%) (41420/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4009) | Acc: (86.24%) (8624/10000)\n",
      "Epoch: 57 | Batch_idx: 0 |  Loss_1: (0.4609) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 57 | Batch_idx: 10 |  Loss_1: (0.4599) | Acc_1: (83.95%) (1182/1408)\n",
      "Epoch: 57 | Batch_idx: 20 |  Loss_1: (0.4756) | Acc_1: (83.37%) (2241/2688)\n",
      "Epoch: 57 | Batch_idx: 30 |  Loss_1: (0.4802) | Acc_1: (83.22%) (3302/3968)\n",
      "Epoch: 57 | Batch_idx: 40 |  Loss_1: (0.4811) | Acc_1: (83.40%) (4377/5248)\n",
      "Epoch: 57 | Batch_idx: 50 |  Loss_1: (0.4805) | Acc_1: (83.53%) (5453/6528)\n",
      "Epoch: 57 | Batch_idx: 60 |  Loss_1: (0.4752) | Acc_1: (83.84%) (6546/7808)\n",
      "Epoch: 57 | Batch_idx: 70 |  Loss_1: (0.4733) | Acc_1: (83.82%) (7618/9088)\n",
      "Epoch: 57 | Batch_idx: 80 |  Loss_1: (0.4744) | Acc_1: (83.75%) (8683/10368)\n",
      "Epoch: 57 | Batch_idx: 90 |  Loss_1: (0.4756) | Acc_1: (83.59%) (9736/11648)\n",
      "Epoch: 57 | Batch_idx: 100 |  Loss_1: (0.4764) | Acc_1: (83.58%) (10805/12928)\n",
      "Epoch: 57 | Batch_idx: 110 |  Loss_1: (0.4781) | Acc_1: (83.57%) (11873/14208)\n",
      "Epoch: 57 | Batch_idx: 120 |  Loss_1: (0.4770) | Acc_1: (83.57%) (12943/15488)\n",
      "Epoch: 57 | Batch_idx: 130 |  Loss_1: (0.4764) | Acc_1: (83.60%) (14018/16768)\n",
      "Epoch: 57 | Batch_idx: 140 |  Loss_1: (0.4759) | Acc_1: (83.61%) (15090/18048)\n",
      "Epoch: 57 | Batch_idx: 150 |  Loss_1: (0.4765) | Acc_1: (83.66%) (16170/19328)\n",
      "Epoch: 57 | Batch_idx: 160 |  Loss_1: (0.4787) | Acc_1: (83.65%) (17239/20608)\n",
      "Epoch: 57 | Batch_idx: 170 |  Loss_1: (0.4796) | Acc_1: (83.65%) (18310/21888)\n",
      "Epoch: 57 | Batch_idx: 180 |  Loss_1: (0.4797) | Acc_1: (83.66%) (19382/23168)\n",
      "Epoch: 57 | Batch_idx: 190 |  Loss_1: (0.4799) | Acc_1: (83.66%) (20452/24448)\n",
      "Epoch: 57 | Batch_idx: 200 |  Loss_1: (0.4812) | Acc_1: (83.54%) (21493/25728)\n",
      "Epoch: 57 | Batch_idx: 210 |  Loss_1: (0.4804) | Acc_1: (83.51%) (22554/27008)\n",
      "Epoch: 57 | Batch_idx: 220 |  Loss_1: (0.4805) | Acc_1: (83.48%) (23615/28288)\n",
      "Epoch: 57 | Batch_idx: 230 |  Loss_1: (0.4803) | Acc_1: (83.48%) (24683/29568)\n",
      "Epoch: 57 | Batch_idx: 240 |  Loss_1: (0.4796) | Acc_1: (83.48%) (25752/30848)\n",
      "Epoch: 57 | Batch_idx: 250 |  Loss_1: (0.4811) | Acc_1: (83.46%) (26813/32128)\n",
      "Epoch: 57 | Batch_idx: 260 |  Loss_1: (0.4800) | Acc_1: (83.48%) (27889/33408)\n",
      "Epoch: 57 | Batch_idx: 270 |  Loss_1: (0.4810) | Acc_1: (83.45%) (28947/34688)\n",
      "Epoch: 57 | Batch_idx: 280 |  Loss_1: (0.4830) | Acc_1: (83.37%) (29985/35968)\n",
      "Epoch: 57 | Batch_idx: 290 |  Loss_1: (0.4829) | Acc_1: (83.34%) (31042/37248)\n",
      "Epoch: 57 | Batch_idx: 300 |  Loss_1: (0.4820) | Acc_1: (83.33%) (32106/38528)\n",
      "Epoch: 57 | Batch_idx: 310 |  Loss_1: (0.4822) | Acc_1: (83.35%) (33178/39808)\n",
      "Epoch: 57 | Batch_idx: 320 |  Loss_1: (0.4835) | Acc_1: (83.30%) (34225/41088)\n",
      "Epoch: 57 | Batch_idx: 330 |  Loss_1: (0.4843) | Acc_1: (83.30%) (35291/42368)\n",
      "Epoch: 57 | Batch_idx: 340 |  Loss_1: (0.4845) | Acc_1: (83.29%) (36353/43648)\n",
      "Epoch: 57 | Batch_idx: 350 |  Loss_1: (0.4838) | Acc_1: (83.30%) (37427/44928)\n",
      "Epoch: 57 | Batch_idx: 360 |  Loss_1: (0.4831) | Acc_1: (83.32%) (38501/46208)\n",
      "Epoch: 57 | Batch_idx: 370 |  Loss_1: (0.4833) | Acc_1: (83.33%) (39571/47488)\n",
      "Epoch: 57 | Batch_idx: 380 |  Loss_1: (0.4830) | Acc_1: (83.37%) (40660/48768)\n",
      "Epoch: 57 | Batch_idx: 390 |  Loss_1: (0.4830) | Acc_1: (83.37%) (41687/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7630) | Acc: (85.35%) (8535/10000)\n",
      "Epoch: 58 | Batch_idx: 0 |  Loss_1: (0.4744) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 58 | Batch_idx: 10 |  Loss_1: (0.4430) | Acc_1: (85.44%) (1203/1408)\n",
      "Epoch: 58 | Batch_idx: 20 |  Loss_1: (0.4502) | Acc_1: (84.78%) (2279/2688)\n",
      "Epoch: 58 | Batch_idx: 30 |  Loss_1: (0.4611) | Acc_1: (84.38%) (3348/3968)\n",
      "Epoch: 58 | Batch_idx: 40 |  Loss_1: (0.4551) | Acc_1: (84.66%) (4443/5248)\n",
      "Epoch: 58 | Batch_idx: 50 |  Loss_1: (0.4509) | Acc_1: (84.59%) (5522/6528)\n",
      "Epoch: 58 | Batch_idx: 60 |  Loss_1: (0.4548) | Acc_1: (84.40%) (6590/7808)\n",
      "Epoch: 58 | Batch_idx: 70 |  Loss_1: (0.4617) | Acc_1: (84.13%) (7646/9088)\n",
      "Epoch: 58 | Batch_idx: 80 |  Loss_1: (0.4626) | Acc_1: (84.01%) (8710/10368)\n",
      "Epoch: 58 | Batch_idx: 90 |  Loss_1: (0.4643) | Acc_1: (84.04%) (9789/11648)\n",
      "Epoch: 58 | Batch_idx: 100 |  Loss_1: (0.4645) | Acc_1: (83.97%) (10855/12928)\n",
      "Epoch: 58 | Batch_idx: 110 |  Loss_1: (0.4623) | Acc_1: (83.96%) (11929/14208)\n",
      "Epoch: 58 | Batch_idx: 120 |  Loss_1: (0.4600) | Acc_1: (84.01%) (13011/15488)\n",
      "Epoch: 58 | Batch_idx: 130 |  Loss_1: (0.4625) | Acc_1: (83.99%) (14084/16768)\n",
      "Epoch: 58 | Batch_idx: 140 |  Loss_1: (0.4636) | Acc_1: (83.92%) (15145/18048)\n",
      "Epoch: 58 | Batch_idx: 150 |  Loss_1: (0.4649) | Acc_1: (83.91%) (16219/19328)\n",
      "Epoch: 58 | Batch_idx: 160 |  Loss_1: (0.4660) | Acc_1: (83.91%) (17292/20608)\n",
      "Epoch: 58 | Batch_idx: 170 |  Loss_1: (0.4677) | Acc_1: (83.85%) (18352/21888)\n",
      "Epoch: 58 | Batch_idx: 180 |  Loss_1: (0.4645) | Acc_1: (83.93%) (19446/23168)\n",
      "Epoch: 58 | Batch_idx: 190 |  Loss_1: (0.4651) | Acc_1: (83.91%) (20514/24448)\n",
      "Epoch: 58 | Batch_idx: 200 |  Loss_1: (0.4686) | Acc_1: (83.79%) (21557/25728)\n",
      "Epoch: 58 | Batch_idx: 210 |  Loss_1: (0.4681) | Acc_1: (83.80%) (22632/27008)\n",
      "Epoch: 58 | Batch_idx: 220 |  Loss_1: (0.4690) | Acc_1: (83.77%) (23696/28288)\n",
      "Epoch: 58 | Batch_idx: 230 |  Loss_1: (0.4676) | Acc_1: (83.77%) (24770/29568)\n",
      "Epoch: 58 | Batch_idx: 240 |  Loss_1: (0.4660) | Acc_1: (83.83%) (25860/30848)\n",
      "Epoch: 58 | Batch_idx: 250 |  Loss_1: (0.4663) | Acc_1: (83.79%) (26919/32128)\n",
      "Epoch: 58 | Batch_idx: 260 |  Loss_1: (0.4663) | Acc_1: (83.79%) (27994/33408)\n",
      "Epoch: 58 | Batch_idx: 270 |  Loss_1: (0.4666) | Acc_1: (83.80%) (29070/34688)\n",
      "Epoch: 58 | Batch_idx: 280 |  Loss_1: (0.4669) | Acc_1: (83.79%) (30138/35968)\n",
      "Epoch: 58 | Batch_idx: 290 |  Loss_1: (0.4666) | Acc_1: (83.81%) (31216/37248)\n",
      "Epoch: 58 | Batch_idx: 300 |  Loss_1: (0.4672) | Acc_1: (83.79%) (32284/38528)\n",
      "Epoch: 58 | Batch_idx: 310 |  Loss_1: (0.4673) | Acc_1: (83.79%) (33356/39808)\n",
      "Epoch: 58 | Batch_idx: 320 |  Loss_1: (0.4659) | Acc_1: (83.84%) (34447/41088)\n",
      "Epoch: 58 | Batch_idx: 330 |  Loss_1: (0.4675) | Acc_1: (83.77%) (35491/42368)\n",
      "Epoch: 58 | Batch_idx: 340 |  Loss_1: (0.4675) | Acc_1: (83.75%) (36554/43648)\n",
      "Epoch: 58 | Batch_idx: 350 |  Loss_1: (0.4668) | Acc_1: (83.78%) (37642/44928)\n",
      "Epoch: 58 | Batch_idx: 360 |  Loss_1: (0.4667) | Acc_1: (83.76%) (38705/46208)\n",
      "Epoch: 58 | Batch_idx: 370 |  Loss_1: (0.4682) | Acc_1: (83.73%) (39760/47488)\n",
      "Epoch: 58 | Batch_idx: 380 |  Loss_1: (0.4679) | Acc_1: (83.75%) (40842/48768)\n",
      "Epoch: 58 | Batch_idx: 390 |  Loss_1: (0.4687) | Acc_1: (83.70%) (41849/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4256) | Acc: (86.59%) (8659/10000)\n",
      "Epoch: 59 | Batch_idx: 0 |  Loss_1: (0.5231) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 59 | Batch_idx: 10 |  Loss_1: (0.4621) | Acc_1: (84.23%) (1186/1408)\n",
      "Epoch: 59 | Batch_idx: 20 |  Loss_1: (0.4543) | Acc_1: (84.19%) (2263/2688)\n",
      "Epoch: 59 | Batch_idx: 30 |  Loss_1: (0.4557) | Acc_1: (83.85%) (3327/3968)\n",
      "Epoch: 59 | Batch_idx: 40 |  Loss_1: (0.4513) | Acc_1: (84.11%) (4414/5248)\n",
      "Epoch: 59 | Batch_idx: 50 |  Loss_1: (0.4463) | Acc_1: (84.28%) (5502/6528)\n",
      "Epoch: 59 | Batch_idx: 60 |  Loss_1: (0.4420) | Acc_1: (84.38%) (6588/7808)\n",
      "Epoch: 59 | Batch_idx: 70 |  Loss_1: (0.4437) | Acc_1: (84.34%) (7665/9088)\n",
      "Epoch: 59 | Batch_idx: 80 |  Loss_1: (0.4475) | Acc_1: (84.38%) (8749/10368)\n",
      "Epoch: 59 | Batch_idx: 90 |  Loss_1: (0.4477) | Acc_1: (84.40%) (9831/11648)\n",
      "Epoch: 59 | Batch_idx: 100 |  Loss_1: (0.4478) | Acc_1: (84.38%) (10909/12928)\n",
      "Epoch: 59 | Batch_idx: 110 |  Loss_1: (0.4502) | Acc_1: (84.18%) (11961/14208)\n",
      "Epoch: 59 | Batch_idx: 120 |  Loss_1: (0.4523) | Acc_1: (84.14%) (13032/15488)\n",
      "Epoch: 59 | Batch_idx: 130 |  Loss_1: (0.4582) | Acc_1: (83.93%) (14074/16768)\n",
      "Epoch: 59 | Batch_idx: 140 |  Loss_1: (0.4588) | Acc_1: (83.85%) (15133/18048)\n",
      "Epoch: 59 | Batch_idx: 150 |  Loss_1: (0.4570) | Acc_1: (83.89%) (16215/19328)\n",
      "Epoch: 59 | Batch_idx: 160 |  Loss_1: (0.4570) | Acc_1: (83.89%) (17289/20608)\n",
      "Epoch: 59 | Batch_idx: 170 |  Loss_1: (0.4551) | Acc_1: (83.99%) (18383/21888)\n",
      "Epoch: 59 | Batch_idx: 180 |  Loss_1: (0.4535) | Acc_1: (84.05%) (19472/23168)\n",
      "Epoch: 59 | Batch_idx: 190 |  Loss_1: (0.4562) | Acc_1: (83.96%) (20527/24448)\n",
      "Epoch: 59 | Batch_idx: 200 |  Loss_1: (0.4571) | Acc_1: (83.99%) (21608/25728)\n",
      "Epoch: 59 | Batch_idx: 210 |  Loss_1: (0.4563) | Acc_1: (84.06%) (22702/27008)\n",
      "Epoch: 59 | Batch_idx: 220 |  Loss_1: (0.4565) | Acc_1: (84.01%) (23766/28288)\n",
      "Epoch: 59 | Batch_idx: 230 |  Loss_1: (0.4576) | Acc_1: (84.03%) (24845/29568)\n",
      "Epoch: 59 | Batch_idx: 240 |  Loss_1: (0.4550) | Acc_1: (84.09%) (25940/30848)\n",
      "Epoch: 59 | Batch_idx: 250 |  Loss_1: (0.4541) | Acc_1: (84.14%) (27034/32128)\n",
      "Epoch: 59 | Batch_idx: 260 |  Loss_1: (0.4543) | Acc_1: (84.11%) (28099/33408)\n",
      "Epoch: 59 | Batch_idx: 270 |  Loss_1: (0.4549) | Acc_1: (84.11%) (29176/34688)\n",
      "Epoch: 59 | Batch_idx: 280 |  Loss_1: (0.4546) | Acc_1: (84.10%) (30250/35968)\n",
      "Epoch: 59 | Batch_idx: 290 |  Loss_1: (0.4555) | Acc_1: (84.07%) (31313/37248)\n",
      "Epoch: 59 | Batch_idx: 300 |  Loss_1: (0.4566) | Acc_1: (84.04%) (32380/38528)\n",
      "Epoch: 59 | Batch_idx: 310 |  Loss_1: (0.4573) | Acc_1: (84.02%) (33447/39808)\n",
      "Epoch: 59 | Batch_idx: 320 |  Loss_1: (0.4570) | Acc_1: (84.04%) (34531/41088)\n",
      "Epoch: 59 | Batch_idx: 330 |  Loss_1: (0.4570) | Acc_1: (84.05%) (35610/42368)\n",
      "Epoch: 59 | Batch_idx: 340 |  Loss_1: (0.4576) | Acc_1: (84.01%) (36668/43648)\n",
      "Epoch: 59 | Batch_idx: 350 |  Loss_1: (0.4561) | Acc_1: (84.07%) (37773/44928)\n",
      "Epoch: 59 | Batch_idx: 360 |  Loss_1: (0.4555) | Acc_1: (84.11%) (38866/46208)\n",
      "Epoch: 59 | Batch_idx: 370 |  Loss_1: (0.4551) | Acc_1: (84.15%) (39960/47488)\n",
      "Epoch: 59 | Batch_idx: 380 |  Loss_1: (0.4555) | Acc_1: (84.15%) (41039/48768)\n",
      "Epoch: 59 | Batch_idx: 390 |  Loss_1: (0.4558) | Acc_1: (84.13%) (42065/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3835) | Acc: (86.83%) (8683/10000)\n",
      "Epoch: 60 | Batch_idx: 0 |  Loss_1: (0.4852) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 60 | Batch_idx: 10 |  Loss_1: (0.4673) | Acc_1: (83.95%) (1182/1408)\n",
      "Epoch: 60 | Batch_idx: 20 |  Loss_1: (0.4440) | Acc_1: (85.12%) (2288/2688)\n",
      "Epoch: 60 | Batch_idx: 30 |  Loss_1: (0.4442) | Acc_1: (84.88%) (3368/3968)\n",
      "Epoch: 60 | Batch_idx: 40 |  Loss_1: (0.4401) | Acc_1: (85.00%) (4461/5248)\n",
      "Epoch: 60 | Batch_idx: 50 |  Loss_1: (0.4398) | Acc_1: (84.85%) (5539/6528)\n",
      "Epoch: 60 | Batch_idx: 60 |  Loss_1: (0.4362) | Acc_1: (84.86%) (6626/7808)\n",
      "Epoch: 60 | Batch_idx: 70 |  Loss_1: (0.4364) | Acc_1: (84.91%) (7717/9088)\n",
      "Epoch: 60 | Batch_idx: 80 |  Loss_1: (0.4460) | Acc_1: (84.62%) (8773/10368)\n",
      "Epoch: 60 | Batch_idx: 90 |  Loss_1: (0.4459) | Acc_1: (84.57%) (9851/11648)\n",
      "Epoch: 60 | Batch_idx: 100 |  Loss_1: (0.4422) | Acc_1: (84.67%) (10946/12928)\n",
      "Epoch: 60 | Batch_idx: 110 |  Loss_1: (0.4415) | Acc_1: (84.66%) (12029/14208)\n",
      "Epoch: 60 | Batch_idx: 120 |  Loss_1: (0.4437) | Acc_1: (84.52%) (13091/15488)\n",
      "Epoch: 60 | Batch_idx: 130 |  Loss_1: (0.4420) | Acc_1: (84.63%) (14191/16768)\n",
      "Epoch: 60 | Batch_idx: 140 |  Loss_1: (0.4443) | Acc_1: (84.60%) (15268/18048)\n",
      "Epoch: 60 | Batch_idx: 150 |  Loss_1: (0.4445) | Acc_1: (84.56%) (16343/19328)\n",
      "Epoch: 60 | Batch_idx: 160 |  Loss_1: (0.4472) | Acc_1: (84.46%) (17406/20608)\n",
      "Epoch: 60 | Batch_idx: 170 |  Loss_1: (0.4460) | Acc_1: (84.48%) (18492/21888)\n",
      "Epoch: 60 | Batch_idx: 180 |  Loss_1: (0.4479) | Acc_1: (84.44%) (19564/23168)\n",
      "Epoch: 60 | Batch_idx: 190 |  Loss_1: (0.4472) | Acc_1: (84.43%) (20641/24448)\n",
      "Epoch: 60 | Batch_idx: 200 |  Loss_1: (0.4451) | Acc_1: (84.51%) (21744/25728)\n",
      "Epoch: 60 | Batch_idx: 210 |  Loss_1: (0.4451) | Acc_1: (84.55%) (22835/27008)\n",
      "Epoch: 60 | Batch_idx: 220 |  Loss_1: (0.4477) | Acc_1: (84.42%) (23882/28288)\n",
      "Epoch: 60 | Batch_idx: 230 |  Loss_1: (0.4480) | Acc_1: (84.40%) (24956/29568)\n",
      "Epoch: 60 | Batch_idx: 240 |  Loss_1: (0.4500) | Acc_1: (84.33%) (26014/30848)\n",
      "Epoch: 60 | Batch_idx: 250 |  Loss_1: (0.4487) | Acc_1: (84.36%) (27104/32128)\n",
      "Epoch: 60 | Batch_idx: 260 |  Loss_1: (0.4489) | Acc_1: (84.35%) (28181/33408)\n",
      "Epoch: 60 | Batch_idx: 270 |  Loss_1: (0.4481) | Acc_1: (84.39%) (29272/34688)\n",
      "Epoch: 60 | Batch_idx: 280 |  Loss_1: (0.4484) | Acc_1: (84.35%) (30338/35968)\n",
      "Epoch: 60 | Batch_idx: 290 |  Loss_1: (0.4461) | Acc_1: (84.46%) (31459/37248)\n",
      "Epoch: 60 | Batch_idx: 300 |  Loss_1: (0.4448) | Acc_1: (84.51%) (32559/38528)\n",
      "Epoch: 60 | Batch_idx: 310 |  Loss_1: (0.4453) | Acc_1: (84.53%) (33648/39808)\n",
      "Epoch: 60 | Batch_idx: 320 |  Loss_1: (0.4455) | Acc_1: (84.51%) (34724/41088)\n",
      "Epoch: 60 | Batch_idx: 330 |  Loss_1: (0.4453) | Acc_1: (84.53%) (35814/42368)\n",
      "Epoch: 60 | Batch_idx: 340 |  Loss_1: (0.4461) | Acc_1: (84.50%) (36884/43648)\n",
      "Epoch: 60 | Batch_idx: 350 |  Loss_1: (0.4482) | Acc_1: (84.43%) (37931/44928)\n",
      "Epoch: 60 | Batch_idx: 360 |  Loss_1: (0.4475) | Acc_1: (84.45%) (39021/46208)\n",
      "Epoch: 60 | Batch_idx: 370 |  Loss_1: (0.4475) | Acc_1: (84.45%) (40102/47488)\n",
      "Epoch: 60 | Batch_idx: 380 |  Loss_1: (0.4474) | Acc_1: (84.45%) (41187/48768)\n",
      "Epoch: 60 | Batch_idx: 390 |  Loss_1: (0.4468) | Acc_1: (84.48%) (42242/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3869) | Acc: (87.13%) (8713/10000)\n",
      "Epoch: 61 | Batch_idx: 0 |  Loss_1: (0.5192) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 61 | Batch_idx: 10 |  Loss_1: (0.4773) | Acc_1: (84.59%) (1191/1408)\n",
      "Epoch: 61 | Batch_idx: 20 |  Loss_1: (0.4811) | Acc_1: (84.34%) (2267/2688)\n",
      "Epoch: 61 | Batch_idx: 30 |  Loss_1: (0.4717) | Acc_1: (84.80%) (3365/3968)\n",
      "Epoch: 61 | Batch_idx: 40 |  Loss_1: (0.4658) | Acc_1: (84.78%) (4449/5248)\n",
      "Epoch: 61 | Batch_idx: 50 |  Loss_1: (0.4580) | Acc_1: (84.85%) (5539/6528)\n",
      "Epoch: 61 | Batch_idx: 60 |  Loss_1: (0.4459) | Acc_1: (85.28%) (6659/7808)\n",
      "Epoch: 61 | Batch_idx: 70 |  Loss_1: (0.4468) | Acc_1: (85.17%) (7740/9088)\n",
      "Epoch: 61 | Batch_idx: 80 |  Loss_1: (0.4429) | Acc_1: (85.19%) (8832/10368)\n",
      "Epoch: 61 | Batch_idx: 90 |  Loss_1: (0.4447) | Acc_1: (85.10%) (9913/11648)\n",
      "Epoch: 61 | Batch_idx: 100 |  Loss_1: (0.4452) | Acc_1: (85.04%) (10994/12928)\n",
      "Epoch: 61 | Batch_idx: 110 |  Loss_1: (0.4430) | Acc_1: (85.06%) (12086/14208)\n",
      "Epoch: 61 | Batch_idx: 120 |  Loss_1: (0.4445) | Acc_1: (84.96%) (13159/15488)\n",
      "Epoch: 61 | Batch_idx: 130 |  Loss_1: (0.4447) | Acc_1: (85.01%) (14255/16768)\n",
      "Epoch: 61 | Batch_idx: 140 |  Loss_1: (0.4438) | Acc_1: (84.93%) (15329/18048)\n",
      "Epoch: 61 | Batch_idx: 150 |  Loss_1: (0.4441) | Acc_1: (84.89%) (16408/19328)\n",
      "Epoch: 61 | Batch_idx: 160 |  Loss_1: (0.4444) | Acc_1: (84.83%) (17482/20608)\n",
      "Epoch: 61 | Batch_idx: 170 |  Loss_1: (0.4464) | Acc_1: (84.81%) (18563/21888)\n",
      "Epoch: 61 | Batch_idx: 180 |  Loss_1: (0.4472) | Acc_1: (84.79%) (19644/23168)\n",
      "Epoch: 61 | Batch_idx: 190 |  Loss_1: (0.4450) | Acc_1: (84.82%) (20736/24448)\n",
      "Epoch: 61 | Batch_idx: 200 |  Loss_1: (0.4461) | Acc_1: (84.79%) (21814/25728)\n",
      "Epoch: 61 | Batch_idx: 210 |  Loss_1: (0.4459) | Acc_1: (84.76%) (22892/27008)\n",
      "Epoch: 61 | Batch_idx: 220 |  Loss_1: (0.4469) | Acc_1: (84.74%) (23970/28288)\n",
      "Epoch: 61 | Batch_idx: 230 |  Loss_1: (0.4462) | Acc_1: (84.78%) (25067/29568)\n",
      "Epoch: 61 | Batch_idx: 240 |  Loss_1: (0.4461) | Acc_1: (84.75%) (26144/30848)\n",
      "Epoch: 61 | Batch_idx: 250 |  Loss_1: (0.4446) | Acc_1: (84.78%) (27238/32128)\n",
      "Epoch: 61 | Batch_idx: 260 |  Loss_1: (0.4459) | Acc_1: (84.74%) (28309/33408)\n",
      "Epoch: 61 | Batch_idx: 270 |  Loss_1: (0.4454) | Acc_1: (84.76%) (29400/34688)\n",
      "Epoch: 61 | Batch_idx: 280 |  Loss_1: (0.4456) | Acc_1: (84.74%) (30479/35968)\n",
      "Epoch: 61 | Batch_idx: 290 |  Loss_1: (0.4451) | Acc_1: (84.73%) (31559/37248)\n",
      "Epoch: 61 | Batch_idx: 300 |  Loss_1: (0.4443) | Acc_1: (84.74%) (32649/38528)\n",
      "Epoch: 61 | Batch_idx: 310 |  Loss_1: (0.4436) | Acc_1: (84.75%) (33738/39808)\n",
      "Epoch: 61 | Batch_idx: 320 |  Loss_1: (0.4449) | Acc_1: (84.66%) (34785/41088)\n",
      "Epoch: 61 | Batch_idx: 330 |  Loss_1: (0.4447) | Acc_1: (84.64%) (35861/42368)\n",
      "Epoch: 61 | Batch_idx: 340 |  Loss_1: (0.4452) | Acc_1: (84.67%) (36957/43648)\n",
      "Epoch: 61 | Batch_idx: 350 |  Loss_1: (0.4448) | Acc_1: (84.66%) (38036/44928)\n",
      "Epoch: 61 | Batch_idx: 360 |  Loss_1: (0.4454) | Acc_1: (84.60%) (39092/46208)\n",
      "Epoch: 61 | Batch_idx: 370 |  Loss_1: (0.4457) | Acc_1: (84.60%) (40173/47488)\n",
      "Epoch: 61 | Batch_idx: 380 |  Loss_1: (0.4463) | Acc_1: (84.60%) (41258/48768)\n",
      "Epoch: 61 | Batch_idx: 390 |  Loss_1: (0.4458) | Acc_1: (84.60%) (42301/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3552) | Acc: (87.94%) (8794/10000)\n",
      "Epoch: 62 | Batch_idx: 0 |  Loss_1: (0.4193) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 62 | Batch_idx: 10 |  Loss_1: (0.3982) | Acc_1: (85.44%) (1203/1408)\n",
      "Epoch: 62 | Batch_idx: 20 |  Loss_1: (0.4010) | Acc_1: (85.97%) (2311/2688)\n",
      "Epoch: 62 | Batch_idx: 30 |  Loss_1: (0.4143) | Acc_1: (85.43%) (3390/3968)\n",
      "Epoch: 62 | Batch_idx: 40 |  Loss_1: (0.4094) | Acc_1: (85.56%) (4490/5248)\n",
      "Epoch: 62 | Batch_idx: 50 |  Loss_1: (0.4092) | Acc_1: (85.60%) (5588/6528)\n",
      "Epoch: 62 | Batch_idx: 60 |  Loss_1: (0.4140) | Acc_1: (85.43%) (6670/7808)\n",
      "Epoch: 62 | Batch_idx: 70 |  Loss_1: (0.4146) | Acc_1: (85.44%) (7765/9088)\n",
      "Epoch: 62 | Batch_idx: 80 |  Loss_1: (0.4201) | Acc_1: (85.23%) (8837/10368)\n",
      "Epoch: 62 | Batch_idx: 90 |  Loss_1: (0.4192) | Acc_1: (85.35%) (9941/11648)\n",
      "Epoch: 62 | Batch_idx: 100 |  Loss_1: (0.4253) | Acc_1: (85.05%) (10995/12928)\n",
      "Epoch: 62 | Batch_idx: 110 |  Loss_1: (0.4245) | Acc_1: (85.04%) (12082/14208)\n",
      "Epoch: 62 | Batch_idx: 120 |  Loss_1: (0.4270) | Acc_1: (85.03%) (13170/15488)\n",
      "Epoch: 62 | Batch_idx: 130 |  Loss_1: (0.4288) | Acc_1: (84.98%) (14250/16768)\n",
      "Epoch: 62 | Batch_idx: 140 |  Loss_1: (0.4260) | Acc_1: (85.17%) (15371/18048)\n",
      "Epoch: 62 | Batch_idx: 150 |  Loss_1: (0.4275) | Acc_1: (85.09%) (16447/19328)\n",
      "Epoch: 62 | Batch_idx: 160 |  Loss_1: (0.4301) | Acc_1: (84.99%) (17514/20608)\n",
      "Epoch: 62 | Batch_idx: 170 |  Loss_1: (0.4298) | Acc_1: (85.02%) (18609/21888)\n",
      "Epoch: 62 | Batch_idx: 180 |  Loss_1: (0.4284) | Acc_1: (85.04%) (19701/23168)\n",
      "Epoch: 62 | Batch_idx: 190 |  Loss_1: (0.4299) | Acc_1: (85.03%) (20789/24448)\n",
      "Epoch: 62 | Batch_idx: 200 |  Loss_1: (0.4284) | Acc_1: (85.07%) (21888/25728)\n",
      "Epoch: 62 | Batch_idx: 210 |  Loss_1: (0.4282) | Acc_1: (85.10%) (22984/27008)\n",
      "Epoch: 62 | Batch_idx: 220 |  Loss_1: (0.4288) | Acc_1: (85.06%) (24061/28288)\n",
      "Epoch: 62 | Batch_idx: 230 |  Loss_1: (0.4283) | Acc_1: (85.07%) (25154/29568)\n",
      "Epoch: 62 | Batch_idx: 240 |  Loss_1: (0.4292) | Acc_1: (85.07%) (26242/30848)\n",
      "Epoch: 62 | Batch_idx: 250 |  Loss_1: (0.4304) | Acc_1: (85.01%) (27312/32128)\n",
      "Epoch: 62 | Batch_idx: 260 |  Loss_1: (0.4303) | Acc_1: (85.04%) (28411/33408)\n",
      "Epoch: 62 | Batch_idx: 270 |  Loss_1: (0.4314) | Acc_1: (84.97%) (29476/34688)\n",
      "Epoch: 62 | Batch_idx: 280 |  Loss_1: (0.4298) | Acc_1: (85.04%) (30587/35968)\n",
      "Epoch: 62 | Batch_idx: 290 |  Loss_1: (0.4311) | Acc_1: (84.96%) (31647/37248)\n",
      "Epoch: 62 | Batch_idx: 300 |  Loss_1: (0.4302) | Acc_1: (84.99%) (32746/38528)\n",
      "Epoch: 62 | Batch_idx: 310 |  Loss_1: (0.4300) | Acc_1: (84.97%) (33823/39808)\n",
      "Epoch: 62 | Batch_idx: 320 |  Loss_1: (0.4293) | Acc_1: (84.98%) (34916/41088)\n",
      "Epoch: 62 | Batch_idx: 330 |  Loss_1: (0.4301) | Acc_1: (84.94%) (35986/42368)\n",
      "Epoch: 62 | Batch_idx: 340 |  Loss_1: (0.4313) | Acc_1: (84.90%) (37058/43648)\n",
      "Epoch: 62 | Batch_idx: 350 |  Loss_1: (0.4303) | Acc_1: (84.94%) (38164/44928)\n",
      "Epoch: 62 | Batch_idx: 360 |  Loss_1: (0.4306) | Acc_1: (84.98%) (39267/46208)\n",
      "Epoch: 62 | Batch_idx: 370 |  Loss_1: (0.4307) | Acc_1: (84.99%) (40359/47488)\n",
      "Epoch: 62 | Batch_idx: 380 |  Loss_1: (0.4310) | Acc_1: (84.98%) (41445/48768)\n",
      "Epoch: 62 | Batch_idx: 390 |  Loss_1: (0.4313) | Acc_1: (84.97%) (42486/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3677) | Acc: (87.63%) (8763/10000)\n",
      "Epoch: 63 | Batch_idx: 0 |  Loss_1: (0.4486) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 63 | Batch_idx: 10 |  Loss_1: (0.4284) | Acc_1: (84.66%) (1192/1408)\n",
      "Epoch: 63 | Batch_idx: 20 |  Loss_1: (0.4195) | Acc_1: (85.12%) (2288/2688)\n",
      "Epoch: 63 | Batch_idx: 30 |  Loss_1: (0.4109) | Acc_1: (85.51%) (3393/3968)\n",
      "Epoch: 63 | Batch_idx: 40 |  Loss_1: (0.4144) | Acc_1: (85.33%) (4478/5248)\n",
      "Epoch: 63 | Batch_idx: 50 |  Loss_1: (0.4137) | Acc_1: (85.52%) (5583/6528)\n",
      "Epoch: 63 | Batch_idx: 60 |  Loss_1: (0.4141) | Acc_1: (85.59%) (6683/7808)\n",
      "Epoch: 63 | Batch_idx: 70 |  Loss_1: (0.4116) | Acc_1: (85.73%) (7791/9088)\n",
      "Epoch: 63 | Batch_idx: 80 |  Loss_1: (0.4144) | Acc_1: (85.59%) (8874/10368)\n",
      "Epoch: 63 | Batch_idx: 90 |  Loss_1: (0.4131) | Acc_1: (85.57%) (9967/11648)\n",
      "Epoch: 63 | Batch_idx: 100 |  Loss_1: (0.4123) | Acc_1: (85.51%) (11055/12928)\n",
      "Epoch: 63 | Batch_idx: 110 |  Loss_1: (0.4143) | Acc_1: (85.42%) (12137/14208)\n",
      "Epoch: 63 | Batch_idx: 120 |  Loss_1: (0.4171) | Acc_1: (85.43%) (13231/15488)\n",
      "Epoch: 63 | Batch_idx: 130 |  Loss_1: (0.4153) | Acc_1: (85.46%) (14330/16768)\n",
      "Epoch: 63 | Batch_idx: 140 |  Loss_1: (0.4164) | Acc_1: (85.41%) (15415/18048)\n",
      "Epoch: 63 | Batch_idx: 150 |  Loss_1: (0.4183) | Acc_1: (85.32%) (16490/19328)\n",
      "Epoch: 63 | Batch_idx: 160 |  Loss_1: (0.4195) | Acc_1: (85.25%) (17569/20608)\n",
      "Epoch: 63 | Batch_idx: 170 |  Loss_1: (0.4179) | Acc_1: (85.28%) (18666/21888)\n",
      "Epoch: 63 | Batch_idx: 180 |  Loss_1: (0.4164) | Acc_1: (85.37%) (19778/23168)\n",
      "Epoch: 63 | Batch_idx: 190 |  Loss_1: (0.4163) | Acc_1: (85.27%) (20847/24448)\n",
      "Epoch: 63 | Batch_idx: 200 |  Loss_1: (0.4159) | Acc_1: (85.29%) (21943/25728)\n",
      "Epoch: 63 | Batch_idx: 210 |  Loss_1: (0.4152) | Acc_1: (85.30%) (23038/27008)\n",
      "Epoch: 63 | Batch_idx: 220 |  Loss_1: (0.4170) | Acc_1: (85.24%) (24114/28288)\n",
      "Epoch: 63 | Batch_idx: 230 |  Loss_1: (0.4159) | Acc_1: (85.32%) (25226/29568)\n",
      "Epoch: 63 | Batch_idx: 240 |  Loss_1: (0.4165) | Acc_1: (85.32%) (26318/30848)\n",
      "Epoch: 63 | Batch_idx: 250 |  Loss_1: (0.4180) | Acc_1: (85.29%) (27403/32128)\n",
      "Epoch: 63 | Batch_idx: 260 |  Loss_1: (0.4196) | Acc_1: (85.29%) (28493/33408)\n",
      "Epoch: 63 | Batch_idx: 270 |  Loss_1: (0.4181) | Acc_1: (85.36%) (29608/34688)\n",
      "Epoch: 63 | Batch_idx: 280 |  Loss_1: (0.4193) | Acc_1: (85.33%) (30693/35968)\n",
      "Epoch: 63 | Batch_idx: 290 |  Loss_1: (0.4211) | Acc_1: (85.26%) (31759/37248)\n",
      "Epoch: 63 | Batch_idx: 300 |  Loss_1: (0.4209) | Acc_1: (85.26%) (32849/38528)\n",
      "Epoch: 63 | Batch_idx: 310 |  Loss_1: (0.4218) | Acc_1: (85.24%) (33931/39808)\n",
      "Epoch: 63 | Batch_idx: 320 |  Loss_1: (0.4218) | Acc_1: (85.26%) (35032/41088)\n",
      "Epoch: 63 | Batch_idx: 330 |  Loss_1: (0.4221) | Acc_1: (85.26%) (36125/42368)\n",
      "Epoch: 63 | Batch_idx: 340 |  Loss_1: (0.4226) | Acc_1: (85.24%) (37204/43648)\n",
      "Epoch: 63 | Batch_idx: 350 |  Loss_1: (0.4226) | Acc_1: (85.25%) (38300/44928)\n",
      "Epoch: 63 | Batch_idx: 360 |  Loss_1: (0.4224) | Acc_1: (85.28%) (39406/46208)\n",
      "Epoch: 63 | Batch_idx: 370 |  Loss_1: (0.4216) | Acc_1: (85.31%) (40510/47488)\n",
      "Epoch: 63 | Batch_idx: 380 |  Loss_1: (0.4204) | Acc_1: (85.35%) (41623/48768)\n",
      "Epoch: 63 | Batch_idx: 390 |  Loss_1: (0.4204) | Acc_1: (85.32%) (42658/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3893) | Acc: (86.95%) (8695/10000)\n",
      "Epoch: 64 | Batch_idx: 0 |  Loss_1: (0.4407) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 64 | Batch_idx: 10 |  Loss_1: (0.4143) | Acc_1: (85.80%) (1208/1408)\n",
      "Epoch: 64 | Batch_idx: 20 |  Loss_1: (0.4021) | Acc_1: (86.01%) (2312/2688)\n",
      "Epoch: 64 | Batch_idx: 30 |  Loss_1: (0.4111) | Acc_1: (85.71%) (3401/3968)\n",
      "Epoch: 64 | Batch_idx: 40 |  Loss_1: (0.4153) | Acc_1: (85.65%) (4495/5248)\n",
      "Epoch: 64 | Batch_idx: 50 |  Loss_1: (0.4086) | Acc_1: (85.88%) (5606/6528)\n",
      "Epoch: 64 | Batch_idx: 60 |  Loss_1: (0.4116) | Acc_1: (85.77%) (6697/7808)\n",
      "Epoch: 64 | Batch_idx: 70 |  Loss_1: (0.4145) | Acc_1: (85.66%) (7785/9088)\n",
      "Epoch: 64 | Batch_idx: 80 |  Loss_1: (0.4153) | Acc_1: (85.46%) (8860/10368)\n",
      "Epoch: 64 | Batch_idx: 90 |  Loss_1: (0.4132) | Acc_1: (85.53%) (9962/11648)\n",
      "Epoch: 64 | Batch_idx: 100 |  Loss_1: (0.4078) | Acc_1: (85.73%) (11083/12928)\n",
      "Epoch: 64 | Batch_idx: 110 |  Loss_1: (0.4050) | Acc_1: (85.82%) (12193/14208)\n",
      "Epoch: 64 | Batch_idx: 120 |  Loss_1: (0.4050) | Acc_1: (85.80%) (13289/15488)\n",
      "Epoch: 64 | Batch_idx: 130 |  Loss_1: (0.4063) | Acc_1: (85.81%) (14388/16768)\n",
      "Epoch: 64 | Batch_idx: 140 |  Loss_1: (0.4079) | Acc_1: (85.74%) (15475/18048)\n",
      "Epoch: 64 | Batch_idx: 150 |  Loss_1: (0.4073) | Acc_1: (85.80%) (16584/19328)\n",
      "Epoch: 64 | Batch_idx: 160 |  Loss_1: (0.4077) | Acc_1: (85.77%) (17676/20608)\n",
      "Epoch: 64 | Batch_idx: 170 |  Loss_1: (0.4078) | Acc_1: (85.74%) (18767/21888)\n",
      "Epoch: 64 | Batch_idx: 180 |  Loss_1: (0.4078) | Acc_1: (85.76%) (19869/23168)\n",
      "Epoch: 64 | Batch_idx: 190 |  Loss_1: (0.4090) | Acc_1: (85.68%) (20948/24448)\n",
      "Epoch: 64 | Batch_idx: 200 |  Loss_1: (0.4094) | Acc_1: (85.72%) (22054/25728)\n",
      "Epoch: 64 | Batch_idx: 210 |  Loss_1: (0.4105) | Acc_1: (85.61%) (23122/27008)\n",
      "Epoch: 64 | Batch_idx: 220 |  Loss_1: (0.4106) | Acc_1: (85.63%) (24224/28288)\n",
      "Epoch: 64 | Batch_idx: 230 |  Loss_1: (0.4091) | Acc_1: (85.70%) (25340/29568)\n",
      "Epoch: 64 | Batch_idx: 240 |  Loss_1: (0.4094) | Acc_1: (85.69%) (26435/30848)\n",
      "Epoch: 64 | Batch_idx: 250 |  Loss_1: (0.4085) | Acc_1: (85.73%) (27543/32128)\n",
      "Epoch: 64 | Batch_idx: 260 |  Loss_1: (0.4089) | Acc_1: (85.70%) (28629/33408)\n",
      "Epoch: 64 | Batch_idx: 270 |  Loss_1: (0.4087) | Acc_1: (85.70%) (29727/34688)\n",
      "Epoch: 64 | Batch_idx: 280 |  Loss_1: (0.4086) | Acc_1: (85.70%) (30825/35968)\n",
      "Epoch: 64 | Batch_idx: 290 |  Loss_1: (0.4093) | Acc_1: (85.69%) (31919/37248)\n",
      "Epoch: 64 | Batch_idx: 300 |  Loss_1: (0.4110) | Acc_1: (85.65%) (33001/38528)\n",
      "Epoch: 64 | Batch_idx: 310 |  Loss_1: (0.4098) | Acc_1: (85.69%) (34110/39808)\n",
      "Epoch: 64 | Batch_idx: 320 |  Loss_1: (0.4108) | Acc_1: (85.66%) (35198/41088)\n",
      "Epoch: 64 | Batch_idx: 330 |  Loss_1: (0.4114) | Acc_1: (85.65%) (36289/42368)\n",
      "Epoch: 64 | Batch_idx: 340 |  Loss_1: (0.4126) | Acc_1: (85.61%) (37365/43648)\n",
      "Epoch: 64 | Batch_idx: 350 |  Loss_1: (0.4129) | Acc_1: (85.58%) (38450/44928)\n",
      "Epoch: 64 | Batch_idx: 360 |  Loss_1: (0.4129) | Acc_1: (85.58%) (39547/46208)\n",
      "Epoch: 64 | Batch_idx: 370 |  Loss_1: (0.4119) | Acc_1: (85.60%) (40650/47488)\n",
      "Epoch: 64 | Batch_idx: 380 |  Loss_1: (0.4114) | Acc_1: (85.60%) (41744/48768)\n",
      "Epoch: 64 | Batch_idx: 390 |  Loss_1: (0.4107) | Acc_1: (85.62%) (42809/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3497) | Acc: (87.98%) (8798/10000)\n",
      "Epoch: 65 | Batch_idx: 0 |  Loss_1: (0.5397) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 65 | Batch_idx: 10 |  Loss_1: (0.4137) | Acc_1: (85.87%) (1209/1408)\n",
      "Epoch: 65 | Batch_idx: 20 |  Loss_1: (0.4400) | Acc_1: (84.97%) (2284/2688)\n",
      "Epoch: 65 | Batch_idx: 30 |  Loss_1: (0.4179) | Acc_1: (85.86%) (3407/3968)\n",
      "Epoch: 65 | Batch_idx: 40 |  Loss_1: (0.4163) | Acc_1: (85.65%) (4495/5248)\n",
      "Epoch: 65 | Batch_idx: 50 |  Loss_1: (0.4153) | Acc_1: (85.74%) (5597/6528)\n",
      "Epoch: 65 | Batch_idx: 60 |  Loss_1: (0.4179) | Acc_1: (85.72%) (6693/7808)\n",
      "Epoch: 65 | Batch_idx: 70 |  Loss_1: (0.4128) | Acc_1: (85.86%) (7803/9088)\n",
      "Epoch: 65 | Batch_idx: 80 |  Loss_1: (0.4132) | Acc_1: (85.97%) (8913/10368)\n",
      "Epoch: 65 | Batch_idx: 90 |  Loss_1: (0.4096) | Acc_1: (86.05%) (10023/11648)\n",
      "Epoch: 65 | Batch_idx: 100 |  Loss_1: (0.4116) | Acc_1: (85.98%) (11116/12928)\n",
      "Epoch: 65 | Batch_idx: 110 |  Loss_1: (0.4116) | Acc_1: (86.06%) (12227/14208)\n",
      "Epoch: 65 | Batch_idx: 120 |  Loss_1: (0.4118) | Acc_1: (86.08%) (13332/15488)\n",
      "Epoch: 65 | Batch_idx: 130 |  Loss_1: (0.4131) | Acc_1: (85.97%) (14416/16768)\n",
      "Epoch: 65 | Batch_idx: 140 |  Loss_1: (0.4121) | Acc_1: (85.87%) (15497/18048)\n",
      "Epoch: 65 | Batch_idx: 150 |  Loss_1: (0.4123) | Acc_1: (85.79%) (16582/19328)\n",
      "Epoch: 65 | Batch_idx: 160 |  Loss_1: (0.4116) | Acc_1: (85.80%) (17681/20608)\n",
      "Epoch: 65 | Batch_idx: 170 |  Loss_1: (0.4112) | Acc_1: (85.74%) (18766/21888)\n",
      "Epoch: 65 | Batch_idx: 180 |  Loss_1: (0.4113) | Acc_1: (85.73%) (19863/23168)\n",
      "Epoch: 65 | Batch_idx: 190 |  Loss_1: (0.4120) | Acc_1: (85.70%) (20953/24448)\n",
      "Epoch: 65 | Batch_idx: 200 |  Loss_1: (0.4110) | Acc_1: (85.74%) (22060/25728)\n",
      "Epoch: 65 | Batch_idx: 210 |  Loss_1: (0.4113) | Acc_1: (85.76%) (23162/27008)\n",
      "Epoch: 65 | Batch_idx: 220 |  Loss_1: (0.4108) | Acc_1: (85.76%) (24260/28288)\n",
      "Epoch: 65 | Batch_idx: 230 |  Loss_1: (0.4108) | Acc_1: (85.77%) (25359/29568)\n",
      "Epoch: 65 | Batch_idx: 240 |  Loss_1: (0.4108) | Acc_1: (85.76%) (26455/30848)\n",
      "Epoch: 65 | Batch_idx: 250 |  Loss_1: (0.4125) | Acc_1: (85.74%) (27545/32128)\n",
      "Epoch: 65 | Batch_idx: 260 |  Loss_1: (0.4114) | Acc_1: (85.77%) (28655/33408)\n",
      "Epoch: 65 | Batch_idx: 270 |  Loss_1: (0.4114) | Acc_1: (85.76%) (29747/34688)\n",
      "Epoch: 65 | Batch_idx: 280 |  Loss_1: (0.4120) | Acc_1: (85.73%) (30834/35968)\n",
      "Epoch: 65 | Batch_idx: 290 |  Loss_1: (0.4123) | Acc_1: (85.71%) (31925/37248)\n",
      "Epoch: 65 | Batch_idx: 300 |  Loss_1: (0.4115) | Acc_1: (85.73%) (33030/38528)\n",
      "Epoch: 65 | Batch_idx: 310 |  Loss_1: (0.4106) | Acc_1: (85.78%) (34149/39808)\n",
      "Epoch: 65 | Batch_idx: 320 |  Loss_1: (0.4102) | Acc_1: (85.77%) (35243/41088)\n",
      "Epoch: 65 | Batch_idx: 330 |  Loss_1: (0.4096) | Acc_1: (85.79%) (36347/42368)\n",
      "Epoch: 65 | Batch_idx: 340 |  Loss_1: (0.4100) | Acc_1: (85.78%) (37440/43648)\n",
      "Epoch: 65 | Batch_idx: 350 |  Loss_1: (0.4095) | Acc_1: (85.76%) (38530/44928)\n",
      "Epoch: 65 | Batch_idx: 360 |  Loss_1: (0.4093) | Acc_1: (85.76%) (39629/46208)\n",
      "Epoch: 65 | Batch_idx: 370 |  Loss_1: (0.4088) | Acc_1: (85.79%) (40740/47488)\n",
      "Epoch: 65 | Batch_idx: 380 |  Loss_1: (0.4096) | Acc_1: (85.74%) (41813/48768)\n",
      "Epoch: 65 | Batch_idx: 390 |  Loss_1: (0.4097) | Acc_1: (85.72%) (42859/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3839) | Acc: (87.21%) (8721/10000)\n",
      "Epoch: 66 | Batch_idx: 0 |  Loss_1: (0.3179) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 66 | Batch_idx: 10 |  Loss_1: (0.4035) | Acc_1: (85.51%) (1204/1408)\n",
      "Epoch: 66 | Batch_idx: 20 |  Loss_1: (0.3930) | Acc_1: (86.12%) (2315/2688)\n",
      "Epoch: 66 | Batch_idx: 30 |  Loss_1: (0.3884) | Acc_1: (86.44%) (3430/3968)\n",
      "Epoch: 66 | Batch_idx: 40 |  Loss_1: (0.3924) | Acc_1: (86.28%) (4528/5248)\n",
      "Epoch: 66 | Batch_idx: 50 |  Loss_1: (0.3923) | Acc_1: (86.37%) (5638/6528)\n",
      "Epoch: 66 | Batch_idx: 60 |  Loss_1: (0.3953) | Acc_1: (86.18%) (6729/7808)\n",
      "Epoch: 66 | Batch_idx: 70 |  Loss_1: (0.4032) | Acc_1: (85.94%) (7810/9088)\n",
      "Epoch: 66 | Batch_idx: 80 |  Loss_1: (0.4079) | Acc_1: (85.70%) (8885/10368)\n",
      "Epoch: 66 | Batch_idx: 90 |  Loss_1: (0.4023) | Acc_1: (85.99%) (10016/11648)\n",
      "Epoch: 66 | Batch_idx: 100 |  Loss_1: (0.4005) | Acc_1: (85.98%) (11115/12928)\n",
      "Epoch: 66 | Batch_idx: 110 |  Loss_1: (0.3974) | Acc_1: (86.04%) (12225/14208)\n",
      "Epoch: 66 | Batch_idx: 120 |  Loss_1: (0.4011) | Acc_1: (85.91%) (13306/15488)\n",
      "Epoch: 66 | Batch_idx: 130 |  Loss_1: (0.3995) | Acc_1: (85.98%) (14417/16768)\n",
      "Epoch: 66 | Batch_idx: 140 |  Loss_1: (0.3958) | Acc_1: (86.10%) (15540/18048)\n",
      "Epoch: 66 | Batch_idx: 150 |  Loss_1: (0.3958) | Acc_1: (86.13%) (16648/19328)\n",
      "Epoch: 66 | Batch_idx: 160 |  Loss_1: (0.3966) | Acc_1: (86.09%) (17741/20608)\n",
      "Epoch: 66 | Batch_idx: 170 |  Loss_1: (0.3968) | Acc_1: (86.06%) (18837/21888)\n",
      "Epoch: 66 | Batch_idx: 180 |  Loss_1: (0.3973) | Acc_1: (86.08%) (19942/23168)\n",
      "Epoch: 66 | Batch_idx: 190 |  Loss_1: (0.3979) | Acc_1: (86.08%) (21046/24448)\n",
      "Epoch: 66 | Batch_idx: 200 |  Loss_1: (0.3985) | Acc_1: (86.07%) (22145/25728)\n",
      "Epoch: 66 | Batch_idx: 210 |  Loss_1: (0.3989) | Acc_1: (86.04%) (23238/27008)\n",
      "Epoch: 66 | Batch_idx: 220 |  Loss_1: (0.4001) | Acc_1: (85.99%) (24326/28288)\n",
      "Epoch: 66 | Batch_idx: 230 |  Loss_1: (0.4006) | Acc_1: (86.00%) (25428/29568)\n",
      "Epoch: 66 | Batch_idx: 240 |  Loss_1: (0.4009) | Acc_1: (85.99%) (26527/30848)\n",
      "Epoch: 66 | Batch_idx: 250 |  Loss_1: (0.3999) | Acc_1: (85.97%) (27619/32128)\n",
      "Epoch: 66 | Batch_idx: 260 |  Loss_1: (0.3988) | Acc_1: (86.01%) (28735/33408)\n",
      "Epoch: 66 | Batch_idx: 270 |  Loss_1: (0.3988) | Acc_1: (86.02%) (29837/34688)\n",
      "Epoch: 66 | Batch_idx: 280 |  Loss_1: (0.3989) | Acc_1: (86.02%) (30940/35968)\n",
      "Epoch: 66 | Batch_idx: 290 |  Loss_1: (0.3990) | Acc_1: (86.03%) (32043/37248)\n",
      "Epoch: 66 | Batch_idx: 300 |  Loss_1: (0.3997) | Acc_1: (86.01%) (33137/38528)\n",
      "Epoch: 66 | Batch_idx: 310 |  Loss_1: (0.3996) | Acc_1: (86.00%) (34235/39808)\n",
      "Epoch: 66 | Batch_idx: 320 |  Loss_1: (0.3990) | Acc_1: (86.03%) (35350/41088)\n",
      "Epoch: 66 | Batch_idx: 330 |  Loss_1: (0.3993) | Acc_1: (86.00%) (36437/42368)\n",
      "Epoch: 66 | Batch_idx: 340 |  Loss_1: (0.3987) | Acc_1: (86.00%) (37538/43648)\n",
      "Epoch: 66 | Batch_idx: 350 |  Loss_1: (0.3984) | Acc_1: (86.02%) (38645/44928)\n",
      "Epoch: 66 | Batch_idx: 360 |  Loss_1: (0.3999) | Acc_1: (85.97%) (39723/46208)\n",
      "Epoch: 66 | Batch_idx: 370 |  Loss_1: (0.4001) | Acc_1: (85.96%) (40822/47488)\n",
      "Epoch: 66 | Batch_idx: 380 |  Loss_1: (0.3993) | Acc_1: (85.98%) (41933/48768)\n",
      "Epoch: 66 | Batch_idx: 390 |  Loss_1: (0.4006) | Acc_1: (85.96%) (42979/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3495) | Acc: (88.44%) (8844/10000)\n",
      "Epoch: 67 | Batch_idx: 0 |  Loss_1: (0.3587) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 67 | Batch_idx: 10 |  Loss_1: (0.3890) | Acc_1: (86.29%) (1215/1408)\n",
      "Epoch: 67 | Batch_idx: 20 |  Loss_1: (0.3805) | Acc_1: (86.46%) (2324/2688)\n",
      "Epoch: 67 | Batch_idx: 30 |  Loss_1: (0.3951) | Acc_1: (85.76%) (3403/3968)\n",
      "Epoch: 67 | Batch_idx: 40 |  Loss_1: (0.3987) | Acc_1: (85.79%) (4502/5248)\n",
      "Epoch: 67 | Batch_idx: 50 |  Loss_1: (0.3943) | Acc_1: (85.97%) (5612/6528)\n",
      "Epoch: 67 | Batch_idx: 60 |  Loss_1: (0.3903) | Acc_1: (86.19%) (6730/7808)\n",
      "Epoch: 67 | Batch_idx: 70 |  Loss_1: (0.3862) | Acc_1: (86.15%) (7829/9088)\n",
      "Epoch: 67 | Batch_idx: 80 |  Loss_1: (0.3878) | Acc_1: (86.25%) (8942/10368)\n",
      "Epoch: 67 | Batch_idx: 90 |  Loss_1: (0.3930) | Acc_1: (86.09%) (10028/11648)\n",
      "Epoch: 67 | Batch_idx: 100 |  Loss_1: (0.3908) | Acc_1: (86.15%) (11137/12928)\n",
      "Epoch: 67 | Batch_idx: 110 |  Loss_1: (0.3934) | Acc_1: (86.04%) (12225/14208)\n",
      "Epoch: 67 | Batch_idx: 120 |  Loss_1: (0.3939) | Acc_1: (86.05%) (13328/15488)\n",
      "Epoch: 67 | Batch_idx: 130 |  Loss_1: (0.3956) | Acc_1: (86.06%) (14430/16768)\n",
      "Epoch: 67 | Batch_idx: 140 |  Loss_1: (0.3953) | Acc_1: (86.08%) (15535/18048)\n",
      "Epoch: 67 | Batch_idx: 150 |  Loss_1: (0.3962) | Acc_1: (86.02%) (16625/19328)\n",
      "Epoch: 67 | Batch_idx: 160 |  Loss_1: (0.3967) | Acc_1: (86.02%) (17728/20608)\n",
      "Epoch: 67 | Batch_idx: 170 |  Loss_1: (0.3972) | Acc_1: (86.05%) (18834/21888)\n",
      "Epoch: 67 | Batch_idx: 180 |  Loss_1: (0.3951) | Acc_1: (86.10%) (19948/23168)\n",
      "Epoch: 67 | Batch_idx: 190 |  Loss_1: (0.3929) | Acc_1: (86.24%) (21084/24448)\n",
      "Epoch: 67 | Batch_idx: 200 |  Loss_1: (0.3906) | Acc_1: (86.32%) (22209/25728)\n",
      "Epoch: 67 | Batch_idx: 210 |  Loss_1: (0.3910) | Acc_1: (86.30%) (23307/27008)\n",
      "Epoch: 67 | Batch_idx: 220 |  Loss_1: (0.3906) | Acc_1: (86.31%) (24415/28288)\n",
      "Epoch: 67 | Batch_idx: 230 |  Loss_1: (0.3903) | Acc_1: (86.31%) (25519/29568)\n",
      "Epoch: 67 | Batch_idx: 240 |  Loss_1: (0.3904) | Acc_1: (86.33%) (26632/30848)\n",
      "Epoch: 67 | Batch_idx: 250 |  Loss_1: (0.3897) | Acc_1: (86.38%) (27752/32128)\n",
      "Epoch: 67 | Batch_idx: 260 |  Loss_1: (0.3904) | Acc_1: (86.35%) (28847/33408)\n",
      "Epoch: 67 | Batch_idx: 270 |  Loss_1: (0.3910) | Acc_1: (86.33%) (29947/34688)\n",
      "Epoch: 67 | Batch_idx: 280 |  Loss_1: (0.3932) | Acc_1: (86.25%) (31023/35968)\n",
      "Epoch: 67 | Batch_idx: 290 |  Loss_1: (0.3933) | Acc_1: (86.24%) (32121/37248)\n",
      "Epoch: 67 | Batch_idx: 300 |  Loss_1: (0.3917) | Acc_1: (86.28%) (33242/38528)\n",
      "Epoch: 67 | Batch_idx: 310 |  Loss_1: (0.3924) | Acc_1: (86.26%) (34338/39808)\n",
      "Epoch: 67 | Batch_idx: 320 |  Loss_1: (0.3930) | Acc_1: (86.28%) (35451/41088)\n",
      "Epoch: 67 | Batch_idx: 330 |  Loss_1: (0.3929) | Acc_1: (86.27%) (36549/42368)\n",
      "Epoch: 67 | Batch_idx: 340 |  Loss_1: (0.3927) | Acc_1: (86.29%) (37664/43648)\n",
      "Epoch: 67 | Batch_idx: 350 |  Loss_1: (0.3951) | Acc_1: (86.20%) (38730/44928)\n",
      "Epoch: 67 | Batch_idx: 360 |  Loss_1: (0.3956) | Acc_1: (86.16%) (39814/46208)\n",
      "Epoch: 67 | Batch_idx: 370 |  Loss_1: (0.3956) | Acc_1: (86.17%) (40919/47488)\n",
      "Epoch: 67 | Batch_idx: 380 |  Loss_1: (0.3963) | Acc_1: (86.12%) (41997/48768)\n",
      "Epoch: 67 | Batch_idx: 390 |  Loss_1: (0.3966) | Acc_1: (86.12%) (43061/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3484) | Acc: (87.84%) (8784/10000)\n",
      "Epoch: 68 | Batch_idx: 0 |  Loss_1: (0.4244) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 68 | Batch_idx: 10 |  Loss_1: (0.3736) | Acc_1: (86.65%) (1220/1408)\n",
      "Epoch: 68 | Batch_idx: 20 |  Loss_1: (0.3767) | Acc_1: (87.24%) (2345/2688)\n",
      "Epoch: 68 | Batch_idx: 30 |  Loss_1: (0.3906) | Acc_1: (86.72%) (3441/3968)\n",
      "Epoch: 68 | Batch_idx: 40 |  Loss_1: (0.3811) | Acc_1: (86.87%) (4559/5248)\n",
      "Epoch: 68 | Batch_idx: 50 |  Loss_1: (0.3746) | Acc_1: (87.12%) (5687/6528)\n",
      "Epoch: 68 | Batch_idx: 60 |  Loss_1: (0.3791) | Acc_1: (87.08%) (6799/7808)\n",
      "Epoch: 68 | Batch_idx: 70 |  Loss_1: (0.3784) | Acc_1: (87.04%) (7910/9088)\n",
      "Epoch: 68 | Batch_idx: 80 |  Loss_1: (0.3809) | Acc_1: (86.82%) (9002/10368)\n",
      "Epoch: 68 | Batch_idx: 90 |  Loss_1: (0.3758) | Acc_1: (87.00%) (10134/11648)\n",
      "Epoch: 68 | Batch_idx: 100 |  Loss_1: (0.3766) | Acc_1: (86.91%) (11236/12928)\n",
      "Epoch: 68 | Batch_idx: 110 |  Loss_1: (0.3779) | Acc_1: (86.82%) (12335/14208)\n",
      "Epoch: 68 | Batch_idx: 120 |  Loss_1: (0.3773) | Acc_1: (86.83%) (13449/15488)\n",
      "Epoch: 68 | Batch_idx: 130 |  Loss_1: (0.3765) | Acc_1: (86.87%) (14567/16768)\n",
      "Epoch: 68 | Batch_idx: 140 |  Loss_1: (0.3808) | Acc_1: (86.70%) (15648/18048)\n",
      "Epoch: 68 | Batch_idx: 150 |  Loss_1: (0.3843) | Acc_1: (86.62%) (16742/19328)\n",
      "Epoch: 68 | Batch_idx: 160 |  Loss_1: (0.3855) | Acc_1: (86.56%) (17838/20608)\n",
      "Epoch: 68 | Batch_idx: 170 |  Loss_1: (0.3865) | Acc_1: (86.52%) (18938/21888)\n",
      "Epoch: 68 | Batch_idx: 180 |  Loss_1: (0.3884) | Acc_1: (86.44%) (20026/23168)\n",
      "Epoch: 68 | Batch_idx: 190 |  Loss_1: (0.3911) | Acc_1: (86.37%) (21116/24448)\n",
      "Epoch: 68 | Batch_idx: 200 |  Loss_1: (0.3891) | Acc_1: (86.42%) (22235/25728)\n",
      "Epoch: 68 | Batch_idx: 210 |  Loss_1: (0.3899) | Acc_1: (86.41%) (23338/27008)\n",
      "Epoch: 68 | Batch_idx: 220 |  Loss_1: (0.3911) | Acc_1: (86.33%) (24422/28288)\n",
      "Epoch: 68 | Batch_idx: 230 |  Loss_1: (0.3912) | Acc_1: (86.31%) (25519/29568)\n",
      "Epoch: 68 | Batch_idx: 240 |  Loss_1: (0.3908) | Acc_1: (86.31%) (26625/30848)\n",
      "Epoch: 68 | Batch_idx: 250 |  Loss_1: (0.3893) | Acc_1: (86.33%) (27736/32128)\n",
      "Epoch: 68 | Batch_idx: 260 |  Loss_1: (0.3876) | Acc_1: (86.39%) (28860/33408)\n",
      "Epoch: 68 | Batch_idx: 270 |  Loss_1: (0.3887) | Acc_1: (86.37%) (29959/34688)\n",
      "Epoch: 68 | Batch_idx: 280 |  Loss_1: (0.3883) | Acc_1: (86.37%) (31064/35968)\n",
      "Epoch: 68 | Batch_idx: 290 |  Loss_1: (0.3892) | Acc_1: (86.32%) (32152/37248)\n",
      "Epoch: 68 | Batch_idx: 300 |  Loss_1: (0.3886) | Acc_1: (86.35%) (33269/38528)\n",
      "Epoch: 68 | Batch_idx: 310 |  Loss_1: (0.3887) | Acc_1: (86.35%) (34374/39808)\n",
      "Epoch: 68 | Batch_idx: 320 |  Loss_1: (0.3889) | Acc_1: (86.33%) (35471/41088)\n",
      "Epoch: 68 | Batch_idx: 330 |  Loss_1: (0.3884) | Acc_1: (86.35%) (36584/42368)\n",
      "Epoch: 68 | Batch_idx: 340 |  Loss_1: (0.3872) | Acc_1: (86.39%) (37709/43648)\n",
      "Epoch: 68 | Batch_idx: 350 |  Loss_1: (0.3859) | Acc_1: (86.43%) (38833/44928)\n",
      "Epoch: 68 | Batch_idx: 360 |  Loss_1: (0.3860) | Acc_1: (86.45%) (39946/46208)\n",
      "Epoch: 68 | Batch_idx: 370 |  Loss_1: (0.3866) | Acc_1: (86.44%) (41051/47488)\n",
      "Epoch: 68 | Batch_idx: 380 |  Loss_1: (0.3871) | Acc_1: (86.42%) (42147/48768)\n",
      "Epoch: 68 | Batch_idx: 390 |  Loss_1: (0.3870) | Acc_1: (86.45%) (43223/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3293) | Acc: (88.63%) (8863/10000)\n",
      "Epoch: 69 | Batch_idx: 0 |  Loss_1: (0.3247) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 69 | Batch_idx: 10 |  Loss_1: (0.3535) | Acc_1: (87.64%) (1234/1408)\n",
      "Epoch: 69 | Batch_idx: 20 |  Loss_1: (0.3486) | Acc_1: (88.06%) (2367/2688)\n",
      "Epoch: 69 | Batch_idx: 30 |  Loss_1: (0.3490) | Acc_1: (87.60%) (3476/3968)\n",
      "Epoch: 69 | Batch_idx: 40 |  Loss_1: (0.3526) | Acc_1: (87.54%) (4594/5248)\n",
      "Epoch: 69 | Batch_idx: 50 |  Loss_1: (0.3562) | Acc_1: (87.45%) (5709/6528)\n",
      "Epoch: 69 | Batch_idx: 60 |  Loss_1: (0.3577) | Acc_1: (87.40%) (6824/7808)\n",
      "Epoch: 69 | Batch_idx: 70 |  Loss_1: (0.3610) | Acc_1: (87.37%) (7940/9088)\n",
      "Epoch: 69 | Batch_idx: 80 |  Loss_1: (0.3623) | Acc_1: (87.29%) (9050/10368)\n",
      "Epoch: 69 | Batch_idx: 90 |  Loss_1: (0.3630) | Acc_1: (87.22%) (10159/11648)\n",
      "Epoch: 69 | Batch_idx: 100 |  Loss_1: (0.3646) | Acc_1: (87.23%) (11277/12928)\n",
      "Epoch: 69 | Batch_idx: 110 |  Loss_1: (0.3643) | Acc_1: (87.21%) (12391/14208)\n",
      "Epoch: 69 | Batch_idx: 120 |  Loss_1: (0.3695) | Acc_1: (87.03%) (13479/15488)\n",
      "Epoch: 69 | Batch_idx: 130 |  Loss_1: (0.3716) | Acc_1: (86.95%) (14580/16768)\n",
      "Epoch: 69 | Batch_idx: 140 |  Loss_1: (0.3718) | Acc_1: (86.98%) (15698/18048)\n",
      "Epoch: 69 | Batch_idx: 150 |  Loss_1: (0.3743) | Acc_1: (86.96%) (16808/19328)\n",
      "Epoch: 69 | Batch_idx: 160 |  Loss_1: (0.3743) | Acc_1: (86.98%) (17924/20608)\n",
      "Epoch: 69 | Batch_idx: 170 |  Loss_1: (0.3749) | Acc_1: (86.94%) (19030/21888)\n",
      "Epoch: 69 | Batch_idx: 180 |  Loss_1: (0.3733) | Acc_1: (87.01%) (20158/23168)\n",
      "Epoch: 69 | Batch_idx: 190 |  Loss_1: (0.3743) | Acc_1: (86.97%) (21263/24448)\n",
      "Epoch: 69 | Batch_idx: 200 |  Loss_1: (0.3771) | Acc_1: (86.85%) (22345/25728)\n",
      "Epoch: 69 | Batch_idx: 210 |  Loss_1: (0.3792) | Acc_1: (86.81%) (23446/27008)\n",
      "Epoch: 69 | Batch_idx: 220 |  Loss_1: (0.3789) | Acc_1: (86.83%) (24563/28288)\n",
      "Epoch: 69 | Batch_idx: 230 |  Loss_1: (0.3792) | Acc_1: (86.81%) (25668/29568)\n",
      "Epoch: 69 | Batch_idx: 240 |  Loss_1: (0.3785) | Acc_1: (86.85%) (26791/30848)\n",
      "Epoch: 69 | Batch_idx: 250 |  Loss_1: (0.3781) | Acc_1: (86.87%) (27908/32128)\n",
      "Epoch: 69 | Batch_idx: 260 |  Loss_1: (0.3787) | Acc_1: (86.83%) (29007/33408)\n",
      "Epoch: 69 | Batch_idx: 270 |  Loss_1: (0.3788) | Acc_1: (86.81%) (30113/34688)\n",
      "Epoch: 69 | Batch_idx: 280 |  Loss_1: (0.3796) | Acc_1: (86.80%) (31220/35968)\n",
      "Epoch: 69 | Batch_idx: 290 |  Loss_1: (0.3792) | Acc_1: (86.83%) (32341/37248)\n",
      "Epoch: 69 | Batch_idx: 300 |  Loss_1: (0.3787) | Acc_1: (86.84%) (33458/38528)\n",
      "Epoch: 69 | Batch_idx: 310 |  Loss_1: (0.3796) | Acc_1: (86.83%) (34566/39808)\n",
      "Epoch: 69 | Batch_idx: 320 |  Loss_1: (0.3808) | Acc_1: (86.78%) (35657/41088)\n",
      "Epoch: 69 | Batch_idx: 330 |  Loss_1: (0.3802) | Acc_1: (86.82%) (36782/42368)\n",
      "Epoch: 69 | Batch_idx: 340 |  Loss_1: (0.3796) | Acc_1: (86.84%) (37906/43648)\n",
      "Epoch: 69 | Batch_idx: 350 |  Loss_1: (0.3788) | Acc_1: (86.87%) (39028/44928)\n",
      "Epoch: 69 | Batch_idx: 360 |  Loss_1: (0.3795) | Acc_1: (86.85%) (40131/46208)\n",
      "Epoch: 69 | Batch_idx: 370 |  Loss_1: (0.3808) | Acc_1: (86.82%) (41230/47488)\n",
      "Epoch: 69 | Batch_idx: 380 |  Loss_1: (0.3799) | Acc_1: (86.85%) (42356/48768)\n",
      "Epoch: 69 | Batch_idx: 390 |  Loss_1: (0.3804) | Acc_1: (86.83%) (43415/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3321) | Acc: (88.95%) (8895/10000)\n",
      "Epoch: 70 | Batch_idx: 0 |  Loss_1: (0.3444) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 70 | Batch_idx: 10 |  Loss_1: (0.3418) | Acc_1: (87.50%) (1232/1408)\n",
      "Epoch: 70 | Batch_idx: 20 |  Loss_1: (0.3473) | Acc_1: (87.54%) (2353/2688)\n",
      "Epoch: 70 | Batch_idx: 30 |  Loss_1: (0.3588) | Acc_1: (87.05%) (3454/3968)\n",
      "Epoch: 70 | Batch_idx: 40 |  Loss_1: (0.3561) | Acc_1: (86.99%) (4565/5248)\n",
      "Epoch: 70 | Batch_idx: 50 |  Loss_1: (0.3567) | Acc_1: (87.12%) (5687/6528)\n",
      "Epoch: 70 | Batch_idx: 60 |  Loss_1: (0.3603) | Acc_1: (87.10%) (6801/7808)\n",
      "Epoch: 70 | Batch_idx: 70 |  Loss_1: (0.3629) | Acc_1: (86.95%) (7902/9088)\n",
      "Epoch: 70 | Batch_idx: 80 |  Loss_1: (0.3630) | Acc_1: (87.00%) (9020/10368)\n",
      "Epoch: 70 | Batch_idx: 90 |  Loss_1: (0.3571) | Acc_1: (87.22%) (10159/11648)\n",
      "Epoch: 70 | Batch_idx: 100 |  Loss_1: (0.3580) | Acc_1: (87.28%) (11283/12928)\n",
      "Epoch: 70 | Batch_idx: 110 |  Loss_1: (0.3603) | Acc_1: (87.19%) (12388/14208)\n",
      "Epoch: 70 | Batch_idx: 120 |  Loss_1: (0.3587) | Acc_1: (87.24%) (13512/15488)\n",
      "Epoch: 70 | Batch_idx: 130 |  Loss_1: (0.3601) | Acc_1: (87.30%) (14638/16768)\n",
      "Epoch: 70 | Batch_idx: 140 |  Loss_1: (0.3601) | Acc_1: (87.31%) (15757/18048)\n",
      "Epoch: 70 | Batch_idx: 150 |  Loss_1: (0.3624) | Acc_1: (87.21%) (16855/19328)\n",
      "Epoch: 70 | Batch_idx: 160 |  Loss_1: (0.3635) | Acc_1: (87.23%) (17977/20608)\n",
      "Epoch: 70 | Batch_idx: 170 |  Loss_1: (0.3643) | Acc_1: (87.20%) (19087/21888)\n",
      "Epoch: 70 | Batch_idx: 180 |  Loss_1: (0.3663) | Acc_1: (87.08%) (20175/23168)\n",
      "Epoch: 70 | Batch_idx: 190 |  Loss_1: (0.3632) | Acc_1: (87.21%) (21320/24448)\n",
      "Epoch: 70 | Batch_idx: 200 |  Loss_1: (0.3635) | Acc_1: (87.21%) (22438/25728)\n",
      "Epoch: 70 | Batch_idx: 210 |  Loss_1: (0.3637) | Acc_1: (87.21%) (23555/27008)\n",
      "Epoch: 70 | Batch_idx: 220 |  Loss_1: (0.3657) | Acc_1: (87.13%) (24646/28288)\n",
      "Epoch: 70 | Batch_idx: 230 |  Loss_1: (0.3654) | Acc_1: (87.13%) (25763/29568)\n",
      "Epoch: 70 | Batch_idx: 240 |  Loss_1: (0.3660) | Acc_1: (87.11%) (26872/30848)\n",
      "Epoch: 70 | Batch_idx: 250 |  Loss_1: (0.3669) | Acc_1: (87.08%) (27977/32128)\n",
      "Epoch: 70 | Batch_idx: 260 |  Loss_1: (0.3674) | Acc_1: (87.04%) (29077/33408)\n",
      "Epoch: 70 | Batch_idx: 270 |  Loss_1: (0.3692) | Acc_1: (86.96%) (30166/34688)\n",
      "Epoch: 70 | Batch_idx: 280 |  Loss_1: (0.3673) | Acc_1: (87.04%) (31308/35968)\n",
      "Epoch: 70 | Batch_idx: 290 |  Loss_1: (0.3676) | Acc_1: (87.02%) (32415/37248)\n",
      "Epoch: 70 | Batch_idx: 300 |  Loss_1: (0.3689) | Acc_1: (87.00%) (33520/38528)\n",
      "Epoch: 70 | Batch_idx: 310 |  Loss_1: (0.3685) | Acc_1: (87.04%) (34648/39808)\n",
      "Epoch: 70 | Batch_idx: 320 |  Loss_1: (0.3702) | Acc_1: (87.00%) (35745/41088)\n",
      "Epoch: 70 | Batch_idx: 330 |  Loss_1: (0.3704) | Acc_1: (87.00%) (36861/42368)\n",
      "Epoch: 70 | Batch_idx: 340 |  Loss_1: (0.3703) | Acc_1: (87.01%) (37977/43648)\n",
      "Epoch: 70 | Batch_idx: 350 |  Loss_1: (0.3704) | Acc_1: (86.99%) (39084/44928)\n",
      "Epoch: 70 | Batch_idx: 360 |  Loss_1: (0.3691) | Acc_1: (87.06%) (40230/46208)\n",
      "Epoch: 70 | Batch_idx: 370 |  Loss_1: (0.3686) | Acc_1: (87.08%) (41354/47488)\n",
      "Epoch: 70 | Batch_idx: 380 |  Loss_1: (0.3682) | Acc_1: (87.12%) (42485/48768)\n",
      "Epoch: 70 | Batch_idx: 390 |  Loss_1: (0.3679) | Acc_1: (87.13%) (43566/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3282) | Acc: (88.78%) (8878/10000)\n",
      "Epoch: 71 | Batch_idx: 0 |  Loss_1: (0.3361) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 71 | Batch_idx: 10 |  Loss_1: (0.3906) | Acc_1: (86.79%) (1222/1408)\n",
      "Epoch: 71 | Batch_idx: 20 |  Loss_1: (0.3618) | Acc_1: (87.95%) (2364/2688)\n",
      "Epoch: 71 | Batch_idx: 30 |  Loss_1: (0.3704) | Acc_1: (87.78%) (3483/3968)\n",
      "Epoch: 71 | Batch_idx: 40 |  Loss_1: (0.3774) | Acc_1: (87.54%) (4594/5248)\n",
      "Epoch: 71 | Batch_idx: 50 |  Loss_1: (0.3761) | Acc_1: (87.53%) (5714/6528)\n",
      "Epoch: 71 | Batch_idx: 60 |  Loss_1: (0.3754) | Acc_1: (87.44%) (6827/7808)\n",
      "Epoch: 71 | Batch_idx: 70 |  Loss_1: (0.3713) | Acc_1: (87.53%) (7955/9088)\n",
      "Epoch: 71 | Batch_idx: 80 |  Loss_1: (0.3716) | Acc_1: (87.45%) (9067/10368)\n",
      "Epoch: 71 | Batch_idx: 90 |  Loss_1: (0.3672) | Acc_1: (87.60%) (10204/11648)\n",
      "Epoch: 71 | Batch_idx: 100 |  Loss_1: (0.3645) | Acc_1: (87.74%) (11343/12928)\n",
      "Epoch: 71 | Batch_idx: 110 |  Loss_1: (0.3654) | Acc_1: (87.58%) (12444/14208)\n",
      "Epoch: 71 | Batch_idx: 120 |  Loss_1: (0.3692) | Acc_1: (87.42%) (13540/15488)\n",
      "Epoch: 71 | Batch_idx: 130 |  Loss_1: (0.3680) | Acc_1: (87.51%) (14674/16768)\n",
      "Epoch: 71 | Batch_idx: 140 |  Loss_1: (0.3659) | Acc_1: (87.56%) (15803/18048)\n",
      "Epoch: 71 | Batch_idx: 150 |  Loss_1: (0.3648) | Acc_1: (87.57%) (16925/19328)\n",
      "Epoch: 71 | Batch_idx: 160 |  Loss_1: (0.3652) | Acc_1: (87.57%) (18046/20608)\n",
      "Epoch: 71 | Batch_idx: 170 |  Loss_1: (0.3668) | Acc_1: (87.51%) (19154/21888)\n",
      "Epoch: 71 | Batch_idx: 180 |  Loss_1: (0.3667) | Acc_1: (87.51%) (20274/23168)\n",
      "Epoch: 71 | Batch_idx: 190 |  Loss_1: (0.3684) | Acc_1: (87.45%) (21379/24448)\n",
      "Epoch: 71 | Batch_idx: 200 |  Loss_1: (0.3687) | Acc_1: (87.46%) (22502/25728)\n",
      "Epoch: 71 | Batch_idx: 210 |  Loss_1: (0.3681) | Acc_1: (87.45%) (23618/27008)\n",
      "Epoch: 71 | Batch_idx: 220 |  Loss_1: (0.3685) | Acc_1: (87.40%) (24725/28288)\n",
      "Epoch: 71 | Batch_idx: 230 |  Loss_1: (0.3676) | Acc_1: (87.38%) (25836/29568)\n",
      "Epoch: 71 | Batch_idx: 240 |  Loss_1: (0.3678) | Acc_1: (87.38%) (26956/30848)\n",
      "Epoch: 71 | Batch_idx: 250 |  Loss_1: (0.3670) | Acc_1: (87.40%) (28080/32128)\n",
      "Epoch: 71 | Batch_idx: 260 |  Loss_1: (0.3671) | Acc_1: (87.36%) (29185/33408)\n",
      "Epoch: 71 | Batch_idx: 270 |  Loss_1: (0.3674) | Acc_1: (87.33%) (30293/34688)\n",
      "Epoch: 71 | Batch_idx: 280 |  Loss_1: (0.3666) | Acc_1: (87.35%) (31417/35968)\n",
      "Epoch: 71 | Batch_idx: 290 |  Loss_1: (0.3660) | Acc_1: (87.34%) (32531/37248)\n",
      "Epoch: 71 | Batch_idx: 300 |  Loss_1: (0.3657) | Acc_1: (87.33%) (33646/38528)\n",
      "Epoch: 71 | Batch_idx: 310 |  Loss_1: (0.3670) | Acc_1: (87.29%) (34748/39808)\n",
      "Epoch: 71 | Batch_idx: 320 |  Loss_1: (0.3666) | Acc_1: (87.32%) (35879/41088)\n",
      "Epoch: 71 | Batch_idx: 330 |  Loss_1: (0.3677) | Acc_1: (87.29%) (36983/42368)\n",
      "Epoch: 71 | Batch_idx: 340 |  Loss_1: (0.3688) | Acc_1: (87.23%) (38072/43648)\n",
      "Epoch: 71 | Batch_idx: 350 |  Loss_1: (0.3676) | Acc_1: (87.26%) (39204/44928)\n",
      "Epoch: 71 | Batch_idx: 360 |  Loss_1: (0.3671) | Acc_1: (87.25%) (40317/46208)\n",
      "Epoch: 71 | Batch_idx: 370 |  Loss_1: (0.3677) | Acc_1: (87.22%) (41418/47488)\n",
      "Epoch: 71 | Batch_idx: 380 |  Loss_1: (0.3670) | Acc_1: (87.22%) (42534/48768)\n",
      "Epoch: 71 | Batch_idx: 390 |  Loss_1: (0.3681) | Acc_1: (87.17%) (43585/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3728) | Acc: (87.15%) (8715/10000)\n",
      "Epoch: 72 | Batch_idx: 0 |  Loss_1: (0.4274) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 72 | Batch_idx: 10 |  Loss_1: (0.4034) | Acc_1: (84.80%) (1194/1408)\n",
      "Epoch: 72 | Batch_idx: 20 |  Loss_1: (0.3816) | Acc_1: (86.16%) (2316/2688)\n",
      "Epoch: 72 | Batch_idx: 30 |  Loss_1: (0.3846) | Acc_1: (86.27%) (3423/3968)\n",
      "Epoch: 72 | Batch_idx: 40 |  Loss_1: (0.3702) | Acc_1: (86.89%) (4560/5248)\n",
      "Epoch: 72 | Batch_idx: 50 |  Loss_1: (0.3557) | Acc_1: (87.33%) (5701/6528)\n",
      "Epoch: 72 | Batch_idx: 60 |  Loss_1: (0.3531) | Acc_1: (87.44%) (6827/7808)\n",
      "Epoch: 72 | Batch_idx: 70 |  Loss_1: (0.3550) | Acc_1: (87.50%) (7952/9088)\n",
      "Epoch: 72 | Batch_idx: 80 |  Loss_1: (0.3552) | Acc_1: (87.42%) (9064/10368)\n",
      "Epoch: 72 | Batch_idx: 90 |  Loss_1: (0.3534) | Acc_1: (87.52%) (10194/11648)\n",
      "Epoch: 72 | Batch_idx: 100 |  Loss_1: (0.3545) | Acc_1: (87.50%) (11312/12928)\n",
      "Epoch: 72 | Batch_idx: 110 |  Loss_1: (0.3524) | Acc_1: (87.52%) (12435/14208)\n",
      "Epoch: 72 | Batch_idx: 120 |  Loss_1: (0.3527) | Acc_1: (87.55%) (13559/15488)\n",
      "Epoch: 72 | Batch_idx: 130 |  Loss_1: (0.3502) | Acc_1: (87.64%) (14696/16768)\n",
      "Epoch: 72 | Batch_idx: 140 |  Loss_1: (0.3517) | Acc_1: (87.60%) (15810/18048)\n",
      "Epoch: 72 | Batch_idx: 150 |  Loss_1: (0.3543) | Acc_1: (87.53%) (16918/19328)\n",
      "Epoch: 72 | Batch_idx: 160 |  Loss_1: (0.3562) | Acc_1: (87.41%) (18014/20608)\n",
      "Epoch: 72 | Batch_idx: 170 |  Loss_1: (0.3554) | Acc_1: (87.44%) (19139/21888)\n",
      "Epoch: 72 | Batch_idx: 180 |  Loss_1: (0.3563) | Acc_1: (87.45%) (20260/23168)\n",
      "Epoch: 72 | Batch_idx: 190 |  Loss_1: (0.3566) | Acc_1: (87.47%) (21385/24448)\n",
      "Epoch: 72 | Batch_idx: 200 |  Loss_1: (0.3569) | Acc_1: (87.45%) (22500/25728)\n",
      "Epoch: 72 | Batch_idx: 210 |  Loss_1: (0.3573) | Acc_1: (87.45%) (23619/27008)\n",
      "Epoch: 72 | Batch_idx: 220 |  Loss_1: (0.3565) | Acc_1: (87.52%) (24759/28288)\n",
      "Epoch: 72 | Batch_idx: 230 |  Loss_1: (0.3569) | Acc_1: (87.53%) (25880/29568)\n",
      "Epoch: 72 | Batch_idx: 240 |  Loss_1: (0.3575) | Acc_1: (87.51%) (26994/30848)\n",
      "Epoch: 72 | Batch_idx: 250 |  Loss_1: (0.3571) | Acc_1: (87.51%) (28115/32128)\n",
      "Epoch: 72 | Batch_idx: 260 |  Loss_1: (0.3572) | Acc_1: (87.46%) (29219/33408)\n",
      "Epoch: 72 | Batch_idx: 270 |  Loss_1: (0.3575) | Acc_1: (87.49%) (30348/34688)\n",
      "Epoch: 72 | Batch_idx: 280 |  Loss_1: (0.3587) | Acc_1: (87.45%) (31455/35968)\n",
      "Epoch: 72 | Batch_idx: 290 |  Loss_1: (0.3588) | Acc_1: (87.47%) (32581/37248)\n",
      "Epoch: 72 | Batch_idx: 300 |  Loss_1: (0.3601) | Acc_1: (87.43%) (33685/38528)\n",
      "Epoch: 72 | Batch_idx: 310 |  Loss_1: (0.3606) | Acc_1: (87.40%) (34792/39808)\n",
      "Epoch: 72 | Batch_idx: 320 |  Loss_1: (0.3607) | Acc_1: (87.42%) (35921/41088)\n",
      "Epoch: 72 | Batch_idx: 330 |  Loss_1: (0.3614) | Acc_1: (87.42%) (37038/42368)\n",
      "Epoch: 72 | Batch_idx: 340 |  Loss_1: (0.3611) | Acc_1: (87.41%) (38154/43648)\n",
      "Epoch: 72 | Batch_idx: 350 |  Loss_1: (0.3620) | Acc_1: (87.39%) (39262/44928)\n",
      "Epoch: 72 | Batch_idx: 360 |  Loss_1: (0.3615) | Acc_1: (87.40%) (40385/46208)\n",
      "Epoch: 72 | Batch_idx: 370 |  Loss_1: (0.3631) | Acc_1: (87.34%) (41477/47488)\n",
      "Epoch: 72 | Batch_idx: 380 |  Loss_1: (0.3628) | Acc_1: (87.35%) (42600/48768)\n",
      "Epoch: 72 | Batch_idx: 390 |  Loss_1: (0.3634) | Acc_1: (87.36%) (43681/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3383) | Acc: (88.64%) (8864/10000)\n",
      "Epoch: 73 | Batch_idx: 0 |  Loss_1: (0.4851) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 73 | Batch_idx: 10 |  Loss_1: (0.3211) | Acc_1: (89.20%) (1256/1408)\n",
      "Epoch: 73 | Batch_idx: 20 |  Loss_1: (0.3354) | Acc_1: (88.13%) (2369/2688)\n",
      "Epoch: 73 | Batch_idx: 30 |  Loss_1: (0.3269) | Acc_1: (88.61%) (3516/3968)\n",
      "Epoch: 73 | Batch_idx: 40 |  Loss_1: (0.3229) | Acc_1: (88.80%) (4660/5248)\n",
      "Epoch: 73 | Batch_idx: 50 |  Loss_1: (0.3275) | Acc_1: (88.59%) (5783/6528)\n",
      "Epoch: 73 | Batch_idx: 60 |  Loss_1: (0.3309) | Acc_1: (88.63%) (6920/7808)\n",
      "Epoch: 73 | Batch_idx: 70 |  Loss_1: (0.3320) | Acc_1: (88.52%) (8045/9088)\n",
      "Epoch: 73 | Batch_idx: 80 |  Loss_1: (0.3337) | Acc_1: (88.41%) (9166/10368)\n",
      "Epoch: 73 | Batch_idx: 90 |  Loss_1: (0.3354) | Acc_1: (88.34%) (10290/11648)\n",
      "Epoch: 73 | Batch_idx: 100 |  Loss_1: (0.3388) | Acc_1: (88.20%) (11403/12928)\n",
      "Epoch: 73 | Batch_idx: 110 |  Loss_1: (0.3378) | Acc_1: (88.23%) (12536/14208)\n",
      "Epoch: 73 | Batch_idx: 120 |  Loss_1: (0.3386) | Acc_1: (88.19%) (13659/15488)\n",
      "Epoch: 73 | Batch_idx: 130 |  Loss_1: (0.3384) | Acc_1: (88.23%) (14795/16768)\n",
      "Epoch: 73 | Batch_idx: 140 |  Loss_1: (0.3368) | Acc_1: (88.29%) (15935/18048)\n",
      "Epoch: 73 | Batch_idx: 150 |  Loss_1: (0.3397) | Acc_1: (88.16%) (17040/19328)\n",
      "Epoch: 73 | Batch_idx: 160 |  Loss_1: (0.3387) | Acc_1: (88.17%) (18170/20608)\n",
      "Epoch: 73 | Batch_idx: 170 |  Loss_1: (0.3386) | Acc_1: (88.14%) (19293/21888)\n",
      "Epoch: 73 | Batch_idx: 180 |  Loss_1: (0.3414) | Acc_1: (88.08%) (20406/23168)\n",
      "Epoch: 73 | Batch_idx: 190 |  Loss_1: (0.3431) | Acc_1: (88.03%) (21521/24448)\n",
      "Epoch: 73 | Batch_idx: 200 |  Loss_1: (0.3447) | Acc_1: (87.96%) (22631/25728)\n",
      "Epoch: 73 | Batch_idx: 210 |  Loss_1: (0.3473) | Acc_1: (87.89%) (23736/27008)\n",
      "Epoch: 73 | Batch_idx: 220 |  Loss_1: (0.3466) | Acc_1: (87.90%) (24865/28288)\n",
      "Epoch: 73 | Batch_idx: 230 |  Loss_1: (0.3462) | Acc_1: (87.93%) (25998/29568)\n",
      "Epoch: 73 | Batch_idx: 240 |  Loss_1: (0.3464) | Acc_1: (87.94%) (27127/30848)\n",
      "Epoch: 73 | Batch_idx: 250 |  Loss_1: (0.3458) | Acc_1: (87.95%) (28256/32128)\n",
      "Epoch: 73 | Batch_idx: 260 |  Loss_1: (0.3470) | Acc_1: (87.88%) (29360/33408)\n",
      "Epoch: 73 | Batch_idx: 270 |  Loss_1: (0.3473) | Acc_1: (87.87%) (30480/34688)\n",
      "Epoch: 73 | Batch_idx: 280 |  Loss_1: (0.3483) | Acc_1: (87.84%) (31594/35968)\n",
      "Epoch: 73 | Batch_idx: 290 |  Loss_1: (0.3480) | Acc_1: (87.88%) (32732/37248)\n",
      "Epoch: 73 | Batch_idx: 300 |  Loss_1: (0.3480) | Acc_1: (87.89%) (33864/38528)\n",
      "Epoch: 73 | Batch_idx: 310 |  Loss_1: (0.3490) | Acc_1: (87.86%) (34975/39808)\n",
      "Epoch: 73 | Batch_idx: 320 |  Loss_1: (0.3497) | Acc_1: (87.84%) (36090/41088)\n",
      "Epoch: 73 | Batch_idx: 330 |  Loss_1: (0.3506) | Acc_1: (87.81%) (37202/42368)\n",
      "Epoch: 73 | Batch_idx: 340 |  Loss_1: (0.3506) | Acc_1: (87.80%) (38324/43648)\n",
      "Epoch: 73 | Batch_idx: 350 |  Loss_1: (0.3520) | Acc_1: (87.75%) (39425/44928)\n",
      "Epoch: 73 | Batch_idx: 360 |  Loss_1: (0.3524) | Acc_1: (87.75%) (40547/46208)\n",
      "Epoch: 73 | Batch_idx: 370 |  Loss_1: (0.3536) | Acc_1: (87.71%) (41650/47488)\n",
      "Epoch: 73 | Batch_idx: 380 |  Loss_1: (0.3539) | Acc_1: (87.68%) (42762/48768)\n",
      "Epoch: 73 | Batch_idx: 390 |  Loss_1: (0.3534) | Acc_1: (87.69%) (43846/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3000) | Acc: (89.71%) (8971/10000)\n",
      "Epoch: 74 | Batch_idx: 0 |  Loss_1: (0.4478) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 74 | Batch_idx: 10 |  Loss_1: (0.3667) | Acc_1: (86.65%) (1220/1408)\n",
      "Epoch: 74 | Batch_idx: 20 |  Loss_1: (0.3381) | Acc_1: (88.13%) (2369/2688)\n",
      "Epoch: 74 | Batch_idx: 30 |  Loss_1: (0.3362) | Acc_1: (88.05%) (3494/3968)\n",
      "Epoch: 74 | Batch_idx: 40 |  Loss_1: (0.3383) | Acc_1: (87.67%) (4601/5248)\n",
      "Epoch: 74 | Batch_idx: 50 |  Loss_1: (0.3354) | Acc_1: (87.75%) (5728/6528)\n",
      "Epoch: 74 | Batch_idx: 60 |  Loss_1: (0.3394) | Acc_1: (87.70%) (6848/7808)\n",
      "Epoch: 74 | Batch_idx: 70 |  Loss_1: (0.3400) | Acc_1: (87.73%) (7973/9088)\n",
      "Epoch: 74 | Batch_idx: 80 |  Loss_1: (0.3391) | Acc_1: (87.81%) (9104/10368)\n",
      "Epoch: 74 | Batch_idx: 90 |  Loss_1: (0.3435) | Acc_1: (87.86%) (10234/11648)\n",
      "Epoch: 74 | Batch_idx: 100 |  Loss_1: (0.3428) | Acc_1: (87.86%) (11359/12928)\n",
      "Epoch: 74 | Batch_idx: 110 |  Loss_1: (0.3451) | Acc_1: (87.78%) (12472/14208)\n",
      "Epoch: 74 | Batch_idx: 120 |  Loss_1: (0.3459) | Acc_1: (87.71%) (13585/15488)\n",
      "Epoch: 74 | Batch_idx: 130 |  Loss_1: (0.3455) | Acc_1: (87.75%) (14714/16768)\n",
      "Epoch: 74 | Batch_idx: 140 |  Loss_1: (0.3467) | Acc_1: (87.74%) (15835/18048)\n",
      "Epoch: 74 | Batch_idx: 150 |  Loss_1: (0.3486) | Acc_1: (87.68%) (16946/19328)\n",
      "Epoch: 74 | Batch_idx: 160 |  Loss_1: (0.3485) | Acc_1: (87.72%) (18078/20608)\n",
      "Epoch: 74 | Batch_idx: 170 |  Loss_1: (0.3485) | Acc_1: (87.76%) (19208/21888)\n",
      "Epoch: 74 | Batch_idx: 180 |  Loss_1: (0.3491) | Acc_1: (87.75%) (20330/23168)\n",
      "Epoch: 74 | Batch_idx: 190 |  Loss_1: (0.3483) | Acc_1: (87.77%) (21458/24448)\n",
      "Epoch: 74 | Batch_idx: 200 |  Loss_1: (0.3487) | Acc_1: (87.75%) (22577/25728)\n",
      "Epoch: 74 | Batch_idx: 210 |  Loss_1: (0.3472) | Acc_1: (87.79%) (23709/27008)\n",
      "Epoch: 74 | Batch_idx: 220 |  Loss_1: (0.3468) | Acc_1: (87.80%) (24837/28288)\n",
      "Epoch: 74 | Batch_idx: 230 |  Loss_1: (0.3469) | Acc_1: (87.79%) (25958/29568)\n",
      "Epoch: 74 | Batch_idx: 240 |  Loss_1: (0.3471) | Acc_1: (87.79%) (27080/30848)\n",
      "Epoch: 74 | Batch_idx: 250 |  Loss_1: (0.3458) | Acc_1: (87.85%) (28223/32128)\n",
      "Epoch: 74 | Batch_idx: 260 |  Loss_1: (0.3456) | Acc_1: (87.88%) (29358/33408)\n",
      "Epoch: 74 | Batch_idx: 270 |  Loss_1: (0.3457) | Acc_1: (87.88%) (30483/34688)\n",
      "Epoch: 74 | Batch_idx: 280 |  Loss_1: (0.3459) | Acc_1: (87.88%) (31610/35968)\n",
      "Epoch: 74 | Batch_idx: 290 |  Loss_1: (0.3460) | Acc_1: (87.87%) (32728/37248)\n",
      "Epoch: 74 | Batch_idx: 300 |  Loss_1: (0.3478) | Acc_1: (87.79%) (33823/38528)\n",
      "Epoch: 74 | Batch_idx: 310 |  Loss_1: (0.3489) | Acc_1: (87.76%) (34935/39808)\n",
      "Epoch: 74 | Batch_idx: 320 |  Loss_1: (0.3502) | Acc_1: (87.72%) (36042/41088)\n",
      "Epoch: 74 | Batch_idx: 330 |  Loss_1: (0.3507) | Acc_1: (87.70%) (37155/42368)\n",
      "Epoch: 74 | Batch_idx: 340 |  Loss_1: (0.3493) | Acc_1: (87.71%) (38284/43648)\n",
      "Epoch: 74 | Batch_idx: 350 |  Loss_1: (0.3499) | Acc_1: (87.71%) (39405/44928)\n",
      "Epoch: 74 | Batch_idx: 360 |  Loss_1: (0.3499) | Acc_1: (87.73%) (40537/46208)\n",
      "Epoch: 74 | Batch_idx: 370 |  Loss_1: (0.3499) | Acc_1: (87.73%) (41660/47488)\n",
      "Epoch: 74 | Batch_idx: 380 |  Loss_1: (0.3500) | Acc_1: (87.71%) (42776/48768)\n",
      "Epoch: 74 | Batch_idx: 390 |  Loss_1: (0.3499) | Acc_1: (87.70%) (43851/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3253) | Acc: (89.01%) (8901/10000)\n",
      "Epoch: 75 | Batch_idx: 0 |  Loss_1: (0.4089) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 75 | Batch_idx: 10 |  Loss_1: (0.3438) | Acc_1: (87.07%) (1226/1408)\n",
      "Epoch: 75 | Batch_idx: 20 |  Loss_1: (0.3355) | Acc_1: (87.43%) (2350/2688)\n",
      "Epoch: 75 | Batch_idx: 30 |  Loss_1: (0.3230) | Acc_1: (88.10%) (3496/3968)\n",
      "Epoch: 75 | Batch_idx: 40 |  Loss_1: (0.3224) | Acc_1: (88.24%) (4631/5248)\n",
      "Epoch: 75 | Batch_idx: 50 |  Loss_1: (0.3279) | Acc_1: (88.13%) (5753/6528)\n",
      "Epoch: 75 | Batch_idx: 60 |  Loss_1: (0.3333) | Acc_1: (88.06%) (6876/7808)\n",
      "Epoch: 75 | Batch_idx: 70 |  Loss_1: (0.3376) | Acc_1: (87.89%) (7987/9088)\n",
      "Epoch: 75 | Batch_idx: 80 |  Loss_1: (0.3358) | Acc_1: (87.94%) (9118/10368)\n",
      "Epoch: 75 | Batch_idx: 90 |  Loss_1: (0.3330) | Acc_1: (88.06%) (10257/11648)\n",
      "Epoch: 75 | Batch_idx: 100 |  Loss_1: (0.3355) | Acc_1: (88.00%) (11376/12928)\n",
      "Epoch: 75 | Batch_idx: 110 |  Loss_1: (0.3354) | Acc_1: (88.01%) (12504/14208)\n",
      "Epoch: 75 | Batch_idx: 120 |  Loss_1: (0.3361) | Acc_1: (87.96%) (13624/15488)\n",
      "Epoch: 75 | Batch_idx: 130 |  Loss_1: (0.3342) | Acc_1: (87.98%) (14752/16768)\n",
      "Epoch: 75 | Batch_idx: 140 |  Loss_1: (0.3358) | Acc_1: (87.89%) (15863/18048)\n",
      "Epoch: 75 | Batch_idx: 150 |  Loss_1: (0.3376) | Acc_1: (87.85%) (16979/19328)\n",
      "Epoch: 75 | Batch_idx: 160 |  Loss_1: (0.3376) | Acc_1: (87.80%) (18093/20608)\n",
      "Epoch: 75 | Batch_idx: 170 |  Loss_1: (0.3366) | Acc_1: (87.84%) (19227/21888)\n",
      "Epoch: 75 | Batch_idx: 180 |  Loss_1: (0.3382) | Acc_1: (87.85%) (20352/23168)\n",
      "Epoch: 75 | Batch_idx: 190 |  Loss_1: (0.3384) | Acc_1: (87.86%) (21480/24448)\n",
      "Epoch: 75 | Batch_idx: 200 |  Loss_1: (0.3385) | Acc_1: (87.87%) (22607/25728)\n",
      "Epoch: 75 | Batch_idx: 210 |  Loss_1: (0.3386) | Acc_1: (87.87%) (23731/27008)\n",
      "Epoch: 75 | Batch_idx: 220 |  Loss_1: (0.3386) | Acc_1: (87.83%) (24846/28288)\n",
      "Epoch: 75 | Batch_idx: 230 |  Loss_1: (0.3388) | Acc_1: (87.85%) (25975/29568)\n",
      "Epoch: 75 | Batch_idx: 240 |  Loss_1: (0.3386) | Acc_1: (87.81%) (27088/30848)\n",
      "Epoch: 75 | Batch_idx: 250 |  Loss_1: (0.3379) | Acc_1: (87.85%) (28225/32128)\n",
      "Epoch: 75 | Batch_idx: 260 |  Loss_1: (0.3377) | Acc_1: (87.86%) (29351/33408)\n",
      "Epoch: 75 | Batch_idx: 270 |  Loss_1: (0.3380) | Acc_1: (87.87%) (30479/34688)\n",
      "Epoch: 75 | Batch_idx: 280 |  Loss_1: (0.3383) | Acc_1: (87.88%) (31609/35968)\n",
      "Epoch: 75 | Batch_idx: 290 |  Loss_1: (0.3389) | Acc_1: (87.85%) (32724/37248)\n",
      "Epoch: 75 | Batch_idx: 300 |  Loss_1: (0.3402) | Acc_1: (87.82%) (33835/38528)\n",
      "Epoch: 75 | Batch_idx: 310 |  Loss_1: (0.3405) | Acc_1: (87.80%) (34953/39808)\n",
      "Epoch: 75 | Batch_idx: 320 |  Loss_1: (0.3414) | Acc_1: (87.77%) (36064/41088)\n",
      "Epoch: 75 | Batch_idx: 330 |  Loss_1: (0.3423) | Acc_1: (87.74%) (37175/42368)\n",
      "Epoch: 75 | Batch_idx: 340 |  Loss_1: (0.3425) | Acc_1: (87.72%) (38290/43648)\n",
      "Epoch: 75 | Batch_idx: 350 |  Loss_1: (0.3427) | Acc_1: (87.72%) (39411/44928)\n",
      "Epoch: 75 | Batch_idx: 360 |  Loss_1: (0.3429) | Acc_1: (87.73%) (40536/46208)\n",
      "Epoch: 75 | Batch_idx: 370 |  Loss_1: (0.3428) | Acc_1: (87.76%) (41677/47488)\n",
      "Epoch: 75 | Batch_idx: 380 |  Loss_1: (0.3425) | Acc_1: (87.75%) (42795/48768)\n",
      "Epoch: 75 | Batch_idx: 390 |  Loss_1: (0.3415) | Acc_1: (87.80%) (43901/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3210) | Acc: (89.20%) (8920/10000)\n",
      "Epoch: 76 | Batch_idx: 0 |  Loss_1: (0.2148) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 76 | Batch_idx: 10 |  Loss_1: (0.2767) | Acc_1: (89.49%) (1260/1408)\n",
      "Epoch: 76 | Batch_idx: 20 |  Loss_1: (0.3120) | Acc_1: (88.84%) (2388/2688)\n",
      "Epoch: 76 | Batch_idx: 30 |  Loss_1: (0.3171) | Acc_1: (88.86%) (3526/3968)\n",
      "Epoch: 76 | Batch_idx: 40 |  Loss_1: (0.3238) | Acc_1: (88.59%) (4649/5248)\n",
      "Epoch: 76 | Batch_idx: 50 |  Loss_1: (0.3191) | Acc_1: (88.74%) (5793/6528)\n",
      "Epoch: 76 | Batch_idx: 60 |  Loss_1: (0.3199) | Acc_1: (88.72%) (6927/7808)\n",
      "Epoch: 76 | Batch_idx: 70 |  Loss_1: (0.3157) | Acc_1: (88.85%) (8075/9088)\n",
      "Epoch: 76 | Batch_idx: 80 |  Loss_1: (0.3213) | Acc_1: (88.72%) (9198/10368)\n",
      "Epoch: 76 | Batch_idx: 90 |  Loss_1: (0.3213) | Acc_1: (88.68%) (10329/11648)\n",
      "Epoch: 76 | Batch_idx: 100 |  Loss_1: (0.3215) | Acc_1: (88.62%) (11457/12928)\n",
      "Epoch: 76 | Batch_idx: 110 |  Loss_1: (0.3244) | Acc_1: (88.49%) (12573/14208)\n",
      "Epoch: 76 | Batch_idx: 120 |  Loss_1: (0.3267) | Acc_1: (88.40%) (13692/15488)\n",
      "Epoch: 76 | Batch_idx: 130 |  Loss_1: (0.3273) | Acc_1: (88.43%) (14828/16768)\n",
      "Epoch: 76 | Batch_idx: 140 |  Loss_1: (0.3292) | Acc_1: (88.36%) (15948/18048)\n",
      "Epoch: 76 | Batch_idx: 150 |  Loss_1: (0.3291) | Acc_1: (88.43%) (17091/19328)\n",
      "Epoch: 76 | Batch_idx: 160 |  Loss_1: (0.3294) | Acc_1: (88.46%) (18229/20608)\n",
      "Epoch: 76 | Batch_idx: 170 |  Loss_1: (0.3298) | Acc_1: (88.40%) (19350/21888)\n",
      "Epoch: 76 | Batch_idx: 180 |  Loss_1: (0.3299) | Acc_1: (88.38%) (20475/23168)\n",
      "Epoch: 76 | Batch_idx: 190 |  Loss_1: (0.3314) | Acc_1: (88.29%) (21586/24448)\n",
      "Epoch: 76 | Batch_idx: 200 |  Loss_1: (0.3320) | Acc_1: (88.29%) (22714/25728)\n",
      "Epoch: 76 | Batch_idx: 210 |  Loss_1: (0.3346) | Acc_1: (88.18%) (23817/27008)\n",
      "Epoch: 76 | Batch_idx: 220 |  Loss_1: (0.3344) | Acc_1: (88.16%) (24940/28288)\n",
      "Epoch: 76 | Batch_idx: 230 |  Loss_1: (0.3355) | Acc_1: (88.13%) (26058/29568)\n",
      "Epoch: 76 | Batch_idx: 240 |  Loss_1: (0.3348) | Acc_1: (88.17%) (27198/30848)\n",
      "Epoch: 76 | Batch_idx: 250 |  Loss_1: (0.3351) | Acc_1: (88.13%) (28316/32128)\n",
      "Epoch: 76 | Batch_idx: 260 |  Loss_1: (0.3355) | Acc_1: (88.13%) (29444/33408)\n",
      "Epoch: 76 | Batch_idx: 270 |  Loss_1: (0.3350) | Acc_1: (88.15%) (30577/34688)\n",
      "Epoch: 76 | Batch_idx: 280 |  Loss_1: (0.3339) | Acc_1: (88.15%) (31707/35968)\n",
      "Epoch: 76 | Batch_idx: 290 |  Loss_1: (0.3329) | Acc_1: (88.21%) (32858/37248)\n",
      "Epoch: 76 | Batch_idx: 300 |  Loss_1: (0.3343) | Acc_1: (88.18%) (33975/38528)\n",
      "Epoch: 76 | Batch_idx: 310 |  Loss_1: (0.3356) | Acc_1: (88.14%) (35087/39808)\n",
      "Epoch: 76 | Batch_idx: 320 |  Loss_1: (0.3357) | Acc_1: (88.13%) (36211/41088)\n",
      "Epoch: 76 | Batch_idx: 330 |  Loss_1: (0.3359) | Acc_1: (88.12%) (37333/42368)\n",
      "Epoch: 76 | Batch_idx: 340 |  Loss_1: (0.3360) | Acc_1: (88.11%) (38459/43648)\n",
      "Epoch: 76 | Batch_idx: 350 |  Loss_1: (0.3366) | Acc_1: (88.11%) (39585/44928)\n",
      "Epoch: 76 | Batch_idx: 360 |  Loss_1: (0.3364) | Acc_1: (88.12%) (40717/46208)\n",
      "Epoch: 76 | Batch_idx: 370 |  Loss_1: (0.3373) | Acc_1: (88.08%) (41828/47488)\n",
      "Epoch: 76 | Batch_idx: 380 |  Loss_1: (0.3373) | Acc_1: (88.08%) (42953/48768)\n",
      "Epoch: 76 | Batch_idx: 390 |  Loss_1: (0.3384) | Acc_1: (88.03%) (44017/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3532) | Acc: (89.00%) (8900/10000)\n",
      "Epoch: 77 | Batch_idx: 0 |  Loss_1: (0.4001) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 77 | Batch_idx: 10 |  Loss_1: (0.3857) | Acc_1: (87.93%) (1238/1408)\n",
      "Epoch: 77 | Batch_idx: 20 |  Loss_1: (0.3765) | Acc_1: (87.76%) (2359/2688)\n",
      "Epoch: 77 | Batch_idx: 30 |  Loss_1: (0.3788) | Acc_1: (87.17%) (3459/3968)\n",
      "Epoch: 77 | Batch_idx: 40 |  Loss_1: (0.3678) | Acc_1: (87.39%) (4586/5248)\n",
      "Epoch: 77 | Batch_idx: 50 |  Loss_1: (0.3581) | Acc_1: (87.71%) (5726/6528)\n",
      "Epoch: 77 | Batch_idx: 60 |  Loss_1: (0.3573) | Acc_1: (87.81%) (6856/7808)\n",
      "Epoch: 77 | Batch_idx: 70 |  Loss_1: (0.3520) | Acc_1: (87.81%) (7980/9088)\n",
      "Epoch: 77 | Batch_idx: 80 |  Loss_1: (0.3513) | Acc_1: (87.80%) (9103/10368)\n",
      "Epoch: 77 | Batch_idx: 90 |  Loss_1: (0.3470) | Acc_1: (88.02%) (10253/11648)\n",
      "Epoch: 77 | Batch_idx: 100 |  Loss_1: (0.3437) | Acc_1: (88.19%) (11401/12928)\n",
      "Epoch: 77 | Batch_idx: 110 |  Loss_1: (0.3456) | Acc_1: (88.17%) (12527/14208)\n",
      "Epoch: 77 | Batch_idx: 120 |  Loss_1: (0.3460) | Acc_1: (88.18%) (13658/15488)\n",
      "Epoch: 77 | Batch_idx: 130 |  Loss_1: (0.3462) | Acc_1: (88.13%) (14777/16768)\n",
      "Epoch: 77 | Batch_idx: 140 |  Loss_1: (0.3455) | Acc_1: (88.03%) (15887/18048)\n",
      "Epoch: 77 | Batch_idx: 150 |  Loss_1: (0.3441) | Acc_1: (88.04%) (17016/19328)\n",
      "Epoch: 77 | Batch_idx: 160 |  Loss_1: (0.3431) | Acc_1: (88.04%) (18144/20608)\n",
      "Epoch: 77 | Batch_idx: 170 |  Loss_1: (0.3402) | Acc_1: (88.14%) (19293/21888)\n",
      "Epoch: 77 | Batch_idx: 180 |  Loss_1: (0.3388) | Acc_1: (88.20%) (20434/23168)\n",
      "Epoch: 77 | Batch_idx: 190 |  Loss_1: (0.3397) | Acc_1: (88.17%) (21555/24448)\n",
      "Epoch: 77 | Batch_idx: 200 |  Loss_1: (0.3380) | Acc_1: (88.22%) (22696/25728)\n",
      "Epoch: 77 | Batch_idx: 210 |  Loss_1: (0.3377) | Acc_1: (88.25%) (23834/27008)\n",
      "Epoch: 77 | Batch_idx: 220 |  Loss_1: (0.3379) | Acc_1: (88.27%) (24970/28288)\n",
      "Epoch: 77 | Batch_idx: 230 |  Loss_1: (0.3409) | Acc_1: (88.17%) (26069/29568)\n",
      "Epoch: 77 | Batch_idx: 240 |  Loss_1: (0.3418) | Acc_1: (88.11%) (27181/30848)\n",
      "Epoch: 77 | Batch_idx: 250 |  Loss_1: (0.3413) | Acc_1: (88.12%) (28310/32128)\n",
      "Epoch: 77 | Batch_idx: 260 |  Loss_1: (0.3429) | Acc_1: (88.06%) (29419/33408)\n",
      "Epoch: 77 | Batch_idx: 270 |  Loss_1: (0.3417) | Acc_1: (88.07%) (30551/34688)\n",
      "Epoch: 77 | Batch_idx: 280 |  Loss_1: (0.3415) | Acc_1: (88.08%) (31681/35968)\n",
      "Epoch: 77 | Batch_idx: 290 |  Loss_1: (0.3424) | Acc_1: (88.06%) (32799/37248)\n",
      "Epoch: 77 | Batch_idx: 300 |  Loss_1: (0.3410) | Acc_1: (88.11%) (33948/38528)\n",
      "Epoch: 77 | Batch_idx: 310 |  Loss_1: (0.3409) | Acc_1: (88.09%) (35066/39808)\n",
      "Epoch: 77 | Batch_idx: 320 |  Loss_1: (0.3403) | Acc_1: (88.09%) (36195/41088)\n",
      "Epoch: 77 | Batch_idx: 330 |  Loss_1: (0.3408) | Acc_1: (88.09%) (37320/42368)\n",
      "Epoch: 77 | Batch_idx: 340 |  Loss_1: (0.3406) | Acc_1: (88.06%) (38437/43648)\n",
      "Epoch: 77 | Batch_idx: 350 |  Loss_1: (0.3392) | Acc_1: (88.12%) (39591/44928)\n",
      "Epoch: 77 | Batch_idx: 360 |  Loss_1: (0.3395) | Acc_1: (88.09%) (40704/46208)\n",
      "Epoch: 77 | Batch_idx: 370 |  Loss_1: (0.3392) | Acc_1: (88.11%) (41840/47488)\n",
      "Epoch: 77 | Batch_idx: 380 |  Loss_1: (0.3387) | Acc_1: (88.14%) (42985/48768)\n",
      "Epoch: 77 | Batch_idx: 390 |  Loss_1: (0.3382) | Acc_1: (88.15%) (44075/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3335) | Acc: (88.79%) (8879/10000)\n",
      "Epoch: 78 | Batch_idx: 0 |  Loss_1: (0.4045) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 78 | Batch_idx: 10 |  Loss_1: (0.3064) | Acc_1: (89.28%) (1257/1408)\n",
      "Epoch: 78 | Batch_idx: 20 |  Loss_1: (0.3131) | Acc_1: (89.10%) (2395/2688)\n",
      "Epoch: 78 | Batch_idx: 30 |  Loss_1: (0.3110) | Acc_1: (89.26%) (3542/3968)\n",
      "Epoch: 78 | Batch_idx: 40 |  Loss_1: (0.3126) | Acc_1: (89.14%) (4678/5248)\n",
      "Epoch: 78 | Batch_idx: 50 |  Loss_1: (0.3166) | Acc_1: (88.91%) (5804/6528)\n",
      "Epoch: 78 | Batch_idx: 60 |  Loss_1: (0.3188) | Acc_1: (88.87%) (6939/7808)\n",
      "Epoch: 78 | Batch_idx: 70 |  Loss_1: (0.3189) | Acc_1: (88.78%) (8068/9088)\n",
      "Epoch: 78 | Batch_idx: 80 |  Loss_1: (0.3172) | Acc_1: (88.75%) (9202/10368)\n",
      "Epoch: 78 | Batch_idx: 90 |  Loss_1: (0.3201) | Acc_1: (88.70%) (10332/11648)\n",
      "Epoch: 78 | Batch_idx: 100 |  Loss_1: (0.3250) | Acc_1: (88.54%) (11447/12928)\n",
      "Epoch: 78 | Batch_idx: 110 |  Loss_1: (0.3281) | Acc_1: (88.50%) (12574/14208)\n",
      "Epoch: 78 | Batch_idx: 120 |  Loss_1: (0.3274) | Acc_1: (88.57%) (13717/15488)\n",
      "Epoch: 78 | Batch_idx: 130 |  Loss_1: (0.3252) | Acc_1: (88.61%) (14858/16768)\n",
      "Epoch: 78 | Batch_idx: 140 |  Loss_1: (0.3275) | Acc_1: (88.52%) (15976/18048)\n",
      "Epoch: 78 | Batch_idx: 150 |  Loss_1: (0.3288) | Acc_1: (88.45%) (17096/19328)\n",
      "Epoch: 78 | Batch_idx: 160 |  Loss_1: (0.3293) | Acc_1: (88.42%) (18221/20608)\n",
      "Epoch: 78 | Batch_idx: 170 |  Loss_1: (0.3299) | Acc_1: (88.43%) (19356/21888)\n",
      "Epoch: 78 | Batch_idx: 180 |  Loss_1: (0.3300) | Acc_1: (88.44%) (20490/23168)\n",
      "Epoch: 78 | Batch_idx: 190 |  Loss_1: (0.3312) | Acc_1: (88.42%) (21617/24448)\n",
      "Epoch: 78 | Batch_idx: 200 |  Loss_1: (0.3301) | Acc_1: (88.45%) (22757/25728)\n",
      "Epoch: 78 | Batch_idx: 210 |  Loss_1: (0.3302) | Acc_1: (88.41%) (23877/27008)\n",
      "Epoch: 78 | Batch_idx: 220 |  Loss_1: (0.3306) | Acc_1: (88.40%) (25006/28288)\n",
      "Epoch: 78 | Batch_idx: 230 |  Loss_1: (0.3298) | Acc_1: (88.40%) (26139/29568)\n",
      "Epoch: 78 | Batch_idx: 240 |  Loss_1: (0.3296) | Acc_1: (88.43%) (27278/30848)\n",
      "Epoch: 78 | Batch_idx: 250 |  Loss_1: (0.3295) | Acc_1: (88.41%) (28403/32128)\n",
      "Epoch: 78 | Batch_idx: 260 |  Loss_1: (0.3281) | Acc_1: (88.45%) (29551/33408)\n",
      "Epoch: 78 | Batch_idx: 270 |  Loss_1: (0.3289) | Acc_1: (88.44%) (30679/34688)\n",
      "Epoch: 78 | Batch_idx: 280 |  Loss_1: (0.3282) | Acc_1: (88.48%) (31823/35968)\n",
      "Epoch: 78 | Batch_idx: 290 |  Loss_1: (0.3286) | Acc_1: (88.46%) (32948/37248)\n",
      "Epoch: 78 | Batch_idx: 300 |  Loss_1: (0.3298) | Acc_1: (88.40%) (34057/38528)\n",
      "Epoch: 78 | Batch_idx: 310 |  Loss_1: (0.3306) | Acc_1: (88.38%) (35181/39808)\n",
      "Epoch: 78 | Batch_idx: 320 |  Loss_1: (0.3307) | Acc_1: (88.37%) (36310/41088)\n",
      "Epoch: 78 | Batch_idx: 330 |  Loss_1: (0.3314) | Acc_1: (88.34%) (37430/42368)\n",
      "Epoch: 78 | Batch_idx: 340 |  Loss_1: (0.3303) | Acc_1: (88.41%) (38591/43648)\n",
      "Epoch: 78 | Batch_idx: 350 |  Loss_1: (0.3303) | Acc_1: (88.42%) (39725/44928)\n",
      "Epoch: 78 | Batch_idx: 360 |  Loss_1: (0.3308) | Acc_1: (88.41%) (40853/46208)\n",
      "Epoch: 78 | Batch_idx: 370 |  Loss_1: (0.3308) | Acc_1: (88.42%) (41987/47488)\n",
      "Epoch: 78 | Batch_idx: 380 |  Loss_1: (0.3311) | Acc_1: (88.38%) (43100/48768)\n",
      "Epoch: 78 | Batch_idx: 390 |  Loss_1: (0.3312) | Acc_1: (88.37%) (44187/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3093) | Acc: (89.74%) (8974/10000)\n",
      "Epoch: 79 | Batch_idx: 0 |  Loss_1: (0.1750) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 79 | Batch_idx: 10 |  Loss_1: (0.3287) | Acc_1: (89.42%) (1259/1408)\n",
      "Epoch: 79 | Batch_idx: 20 |  Loss_1: (0.3245) | Acc_1: (88.99%) (2392/2688)\n",
      "Epoch: 79 | Batch_idx: 30 |  Loss_1: (0.3135) | Acc_1: (89.31%) (3544/3968)\n",
      "Epoch: 79 | Batch_idx: 40 |  Loss_1: (0.3107) | Acc_1: (89.25%) (4684/5248)\n",
      "Epoch: 79 | Batch_idx: 50 |  Loss_1: (0.3049) | Acc_1: (89.37%) (5834/6528)\n",
      "Epoch: 79 | Batch_idx: 60 |  Loss_1: (0.3085) | Acc_1: (89.28%) (6971/7808)\n",
      "Epoch: 79 | Batch_idx: 70 |  Loss_1: (0.3100) | Acc_1: (89.03%) (8091/9088)\n",
      "Epoch: 79 | Batch_idx: 80 |  Loss_1: (0.3138) | Acc_1: (88.84%) (9211/10368)\n",
      "Epoch: 79 | Batch_idx: 90 |  Loss_1: (0.3122) | Acc_1: (88.95%) (10361/11648)\n",
      "Epoch: 79 | Batch_idx: 100 |  Loss_1: (0.3098) | Acc_1: (89.01%) (11507/12928)\n",
      "Epoch: 79 | Batch_idx: 110 |  Loss_1: (0.3171) | Acc_1: (88.75%) (12610/14208)\n",
      "Epoch: 79 | Batch_idx: 120 |  Loss_1: (0.3152) | Acc_1: (88.78%) (13751/15488)\n",
      "Epoch: 79 | Batch_idx: 130 |  Loss_1: (0.3155) | Acc_1: (88.79%) (14889/16768)\n",
      "Epoch: 79 | Batch_idx: 140 |  Loss_1: (0.3119) | Acc_1: (88.97%) (16058/18048)\n",
      "Epoch: 79 | Batch_idx: 150 |  Loss_1: (0.3112) | Acc_1: (88.97%) (17197/19328)\n",
      "Epoch: 79 | Batch_idx: 160 |  Loss_1: (0.3109) | Acc_1: (88.99%) (18339/20608)\n",
      "Epoch: 79 | Batch_idx: 170 |  Loss_1: (0.3097) | Acc_1: (89.07%) (19495/21888)\n",
      "Epoch: 79 | Batch_idx: 180 |  Loss_1: (0.3102) | Acc_1: (89.07%) (20636/23168)\n",
      "Epoch: 79 | Batch_idx: 190 |  Loss_1: (0.3088) | Acc_1: (89.11%) (21786/24448)\n",
      "Epoch: 79 | Batch_idx: 200 |  Loss_1: (0.3096) | Acc_1: (89.06%) (22914/25728)\n",
      "Epoch: 79 | Batch_idx: 210 |  Loss_1: (0.3124) | Acc_1: (88.96%) (24026/27008)\n",
      "Epoch: 79 | Batch_idx: 220 |  Loss_1: (0.3145) | Acc_1: (88.92%) (25154/28288)\n",
      "Epoch: 79 | Batch_idx: 230 |  Loss_1: (0.3156) | Acc_1: (88.89%) (26284/29568)\n",
      "Epoch: 79 | Batch_idx: 240 |  Loss_1: (0.3165) | Acc_1: (88.88%) (27418/30848)\n",
      "Epoch: 79 | Batch_idx: 250 |  Loss_1: (0.3168) | Acc_1: (88.88%) (28556/32128)\n",
      "Epoch: 79 | Batch_idx: 260 |  Loss_1: (0.3162) | Acc_1: (88.90%) (29700/33408)\n",
      "Epoch: 79 | Batch_idx: 270 |  Loss_1: (0.3157) | Acc_1: (88.92%) (30845/34688)\n",
      "Epoch: 79 | Batch_idx: 280 |  Loss_1: (0.3162) | Acc_1: (88.93%) (31988/35968)\n",
      "Epoch: 79 | Batch_idx: 290 |  Loss_1: (0.3154) | Acc_1: (88.96%) (33135/37248)\n",
      "Epoch: 79 | Batch_idx: 300 |  Loss_1: (0.3156) | Acc_1: (88.96%) (34274/38528)\n",
      "Epoch: 79 | Batch_idx: 310 |  Loss_1: (0.3159) | Acc_1: (88.94%) (35405/39808)\n",
      "Epoch: 79 | Batch_idx: 320 |  Loss_1: (0.3167) | Acc_1: (88.92%) (36536/41088)\n",
      "Epoch: 79 | Batch_idx: 330 |  Loss_1: (0.3168) | Acc_1: (88.91%) (37670/42368)\n",
      "Epoch: 79 | Batch_idx: 340 |  Loss_1: (0.3173) | Acc_1: (88.89%) (38798/43648)\n",
      "Epoch: 79 | Batch_idx: 350 |  Loss_1: (0.3186) | Acc_1: (88.83%) (39910/44928)\n",
      "Epoch: 79 | Batch_idx: 360 |  Loss_1: (0.3178) | Acc_1: (88.87%) (41063/46208)\n",
      "Epoch: 79 | Batch_idx: 370 |  Loss_1: (0.3179) | Acc_1: (88.88%) (42207/47488)\n",
      "Epoch: 79 | Batch_idx: 380 |  Loss_1: (0.3190) | Acc_1: (88.83%) (43323/48768)\n",
      "Epoch: 79 | Batch_idx: 390 |  Loss_1: (0.3201) | Acc_1: (88.79%) (44393/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3208) | Acc: (89.45%) (8945/10000)\n",
      "Epoch: 80 | Batch_idx: 0 |  Loss_1: (0.2874) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 80 | Batch_idx: 10 |  Loss_1: (0.2994) | Acc_1: (89.56%) (1261/1408)\n",
      "Epoch: 80 | Batch_idx: 20 |  Loss_1: (0.3031) | Acc_1: (89.62%) (2409/2688)\n",
      "Epoch: 80 | Batch_idx: 30 |  Loss_1: (0.3088) | Acc_1: (89.31%) (3544/3968)\n",
      "Epoch: 80 | Batch_idx: 40 |  Loss_1: (0.3037) | Acc_1: (89.33%) (4688/5248)\n",
      "Epoch: 80 | Batch_idx: 50 |  Loss_1: (0.2998) | Acc_1: (89.43%) (5838/6528)\n",
      "Epoch: 80 | Batch_idx: 60 |  Loss_1: (0.3023) | Acc_1: (89.43%) (6983/7808)\n",
      "Epoch: 80 | Batch_idx: 70 |  Loss_1: (0.3034) | Acc_1: (89.50%) (8134/9088)\n",
      "Epoch: 80 | Batch_idx: 80 |  Loss_1: (0.3044) | Acc_1: (89.45%) (9274/10368)\n",
      "Epoch: 80 | Batch_idx: 90 |  Loss_1: (0.3027) | Acc_1: (89.47%) (10422/11648)\n",
      "Epoch: 80 | Batch_idx: 100 |  Loss_1: (0.3008) | Acc_1: (89.51%) (11572/12928)\n",
      "Epoch: 80 | Batch_idx: 110 |  Loss_1: (0.3041) | Acc_1: (89.41%) (12704/14208)\n",
      "Epoch: 80 | Batch_idx: 120 |  Loss_1: (0.3053) | Acc_1: (89.37%) (13842/15488)\n",
      "Epoch: 80 | Batch_idx: 130 |  Loss_1: (0.3033) | Acc_1: (89.43%) (14995/16768)\n",
      "Epoch: 80 | Batch_idx: 140 |  Loss_1: (0.3041) | Acc_1: (89.39%) (16134/18048)\n",
      "Epoch: 80 | Batch_idx: 150 |  Loss_1: (0.3053) | Acc_1: (89.34%) (17268/19328)\n",
      "Epoch: 80 | Batch_idx: 160 |  Loss_1: (0.3085) | Acc_1: (89.16%) (18374/20608)\n",
      "Epoch: 80 | Batch_idx: 170 |  Loss_1: (0.3087) | Acc_1: (89.17%) (19518/21888)\n",
      "Epoch: 80 | Batch_idx: 180 |  Loss_1: (0.3081) | Acc_1: (89.23%) (20673/23168)\n",
      "Epoch: 80 | Batch_idx: 190 |  Loss_1: (0.3085) | Acc_1: (89.19%) (21804/24448)\n",
      "Epoch: 80 | Batch_idx: 200 |  Loss_1: (0.3072) | Acc_1: (89.20%) (22949/25728)\n",
      "Epoch: 80 | Batch_idx: 210 |  Loss_1: (0.3081) | Acc_1: (89.16%) (24079/27008)\n",
      "Epoch: 80 | Batch_idx: 220 |  Loss_1: (0.3083) | Acc_1: (89.13%) (25213/28288)\n",
      "Epoch: 80 | Batch_idx: 230 |  Loss_1: (0.3093) | Acc_1: (89.10%) (26344/29568)\n",
      "Epoch: 80 | Batch_idx: 240 |  Loss_1: (0.3095) | Acc_1: (89.09%) (27482/30848)\n",
      "Epoch: 80 | Batch_idx: 250 |  Loss_1: (0.3086) | Acc_1: (89.09%) (28623/32128)\n",
      "Epoch: 80 | Batch_idx: 260 |  Loss_1: (0.3077) | Acc_1: (89.09%) (29764/33408)\n",
      "Epoch: 80 | Batch_idx: 270 |  Loss_1: (0.3085) | Acc_1: (89.06%) (30892/34688)\n",
      "Epoch: 80 | Batch_idx: 280 |  Loss_1: (0.3099) | Acc_1: (89.02%) (32018/35968)\n",
      "Epoch: 80 | Batch_idx: 290 |  Loss_1: (0.3113) | Acc_1: (88.95%) (33132/37248)\n",
      "Epoch: 80 | Batch_idx: 300 |  Loss_1: (0.3119) | Acc_1: (88.96%) (34275/38528)\n",
      "Epoch: 80 | Batch_idx: 310 |  Loss_1: (0.3108) | Acc_1: (89.00%) (35428/39808)\n",
      "Epoch: 80 | Batch_idx: 320 |  Loss_1: (0.3120) | Acc_1: (88.94%) (36544/41088)\n",
      "Epoch: 80 | Batch_idx: 330 |  Loss_1: (0.3124) | Acc_1: (88.95%) (37687/42368)\n",
      "Epoch: 80 | Batch_idx: 340 |  Loss_1: (0.3129) | Acc_1: (88.94%) (38819/43648)\n",
      "Epoch: 80 | Batch_idx: 350 |  Loss_1: (0.3127) | Acc_1: (88.94%) (39960/44928)\n",
      "Epoch: 80 | Batch_idx: 360 |  Loss_1: (0.3125) | Acc_1: (88.97%) (41109/46208)\n",
      "Epoch: 80 | Batch_idx: 370 |  Loss_1: (0.3138) | Acc_1: (88.94%) (42237/47488)\n",
      "Epoch: 80 | Batch_idx: 380 |  Loss_1: (0.3138) | Acc_1: (88.94%) (43375/48768)\n",
      "Epoch: 80 | Batch_idx: 390 |  Loss_1: (0.3133) | Acc_1: (88.97%) (44483/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2919) | Acc: (90.05%) (9005/10000)\n",
      "Epoch: 81 | Batch_idx: 0 |  Loss_1: (0.3631) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 81 | Batch_idx: 10 |  Loss_1: (0.3204) | Acc_1: (88.99%) (1253/1408)\n",
      "Epoch: 81 | Batch_idx: 20 |  Loss_1: (0.3036) | Acc_1: (89.77%) (2413/2688)\n",
      "Epoch: 81 | Batch_idx: 30 |  Loss_1: (0.3009) | Acc_1: (89.69%) (3559/3968)\n",
      "Epoch: 81 | Batch_idx: 40 |  Loss_1: (0.3037) | Acc_1: (89.58%) (4701/5248)\n",
      "Epoch: 81 | Batch_idx: 50 |  Loss_1: (0.3074) | Acc_1: (89.43%) (5838/6528)\n",
      "Epoch: 81 | Batch_idx: 60 |  Loss_1: (0.3057) | Acc_1: (89.41%) (6981/7808)\n",
      "Epoch: 81 | Batch_idx: 70 |  Loss_1: (0.3070) | Acc_1: (89.33%) (8118/9088)\n",
      "Epoch: 81 | Batch_idx: 80 |  Loss_1: (0.3100) | Acc_1: (89.24%) (9252/10368)\n",
      "Epoch: 81 | Batch_idx: 90 |  Loss_1: (0.3099) | Acc_1: (89.27%) (10398/11648)\n",
      "Epoch: 81 | Batch_idx: 100 |  Loss_1: (0.3163) | Acc_1: (89.02%) (11509/12928)\n",
      "Epoch: 81 | Batch_idx: 110 |  Loss_1: (0.3156) | Acc_1: (89.08%) (12656/14208)\n",
      "Epoch: 81 | Batch_idx: 120 |  Loss_1: (0.3170) | Acc_1: (89.01%) (13786/15488)\n",
      "Epoch: 81 | Batch_idx: 130 |  Loss_1: (0.3161) | Acc_1: (89.02%) (14927/16768)\n",
      "Epoch: 81 | Batch_idx: 140 |  Loss_1: (0.3142) | Acc_1: (89.07%) (16076/18048)\n",
      "Epoch: 81 | Batch_idx: 150 |  Loss_1: (0.3156) | Acc_1: (89.03%) (17208/19328)\n",
      "Epoch: 81 | Batch_idx: 160 |  Loss_1: (0.3169) | Acc_1: (88.97%) (18334/20608)\n",
      "Epoch: 81 | Batch_idx: 170 |  Loss_1: (0.3173) | Acc_1: (88.95%) (19469/21888)\n",
      "Epoch: 81 | Batch_idx: 180 |  Loss_1: (0.3152) | Acc_1: (88.98%) (20615/23168)\n",
      "Epoch: 81 | Batch_idx: 190 |  Loss_1: (0.3171) | Acc_1: (88.96%) (21750/24448)\n",
      "Epoch: 81 | Batch_idx: 200 |  Loss_1: (0.3192) | Acc_1: (88.86%) (22863/25728)\n",
      "Epoch: 81 | Batch_idx: 210 |  Loss_1: (0.3188) | Acc_1: (88.89%) (24008/27008)\n",
      "Epoch: 81 | Batch_idx: 220 |  Loss_1: (0.3182) | Acc_1: (88.92%) (25154/28288)\n",
      "Epoch: 81 | Batch_idx: 230 |  Loss_1: (0.3174) | Acc_1: (88.96%) (26304/29568)\n",
      "Epoch: 81 | Batch_idx: 240 |  Loss_1: (0.3171) | Acc_1: (88.94%) (27437/30848)\n",
      "Epoch: 81 | Batch_idx: 250 |  Loss_1: (0.3156) | Acc_1: (89.00%) (28593/32128)\n",
      "Epoch: 81 | Batch_idx: 260 |  Loss_1: (0.3148) | Acc_1: (89.02%) (29739/33408)\n",
      "Epoch: 81 | Batch_idx: 270 |  Loss_1: (0.3157) | Acc_1: (88.98%) (30867/34688)\n",
      "Epoch: 81 | Batch_idx: 280 |  Loss_1: (0.3170) | Acc_1: (88.96%) (31996/35968)\n",
      "Epoch: 81 | Batch_idx: 290 |  Loss_1: (0.3166) | Acc_1: (88.98%) (33144/37248)\n",
      "Epoch: 81 | Batch_idx: 300 |  Loss_1: (0.3166) | Acc_1: (88.97%) (34278/38528)\n",
      "Epoch: 81 | Batch_idx: 310 |  Loss_1: (0.3161) | Acc_1: (88.98%) (35420/39808)\n",
      "Epoch: 81 | Batch_idx: 320 |  Loss_1: (0.3156) | Acc_1: (88.97%) (36557/41088)\n",
      "Epoch: 81 | Batch_idx: 330 |  Loss_1: (0.3160) | Acc_1: (88.96%) (37689/42368)\n",
      "Epoch: 81 | Batch_idx: 340 |  Loss_1: (0.3159) | Acc_1: (88.93%) (38818/43648)\n",
      "Epoch: 81 | Batch_idx: 350 |  Loss_1: (0.3166) | Acc_1: (88.88%) (39930/44928)\n",
      "Epoch: 81 | Batch_idx: 360 |  Loss_1: (0.3173) | Acc_1: (88.86%) (41061/46208)\n",
      "Epoch: 81 | Batch_idx: 370 |  Loss_1: (0.3166) | Acc_1: (88.88%) (42209/47488)\n",
      "Epoch: 81 | Batch_idx: 380 |  Loss_1: (0.3175) | Acc_1: (88.86%) (43334/48768)\n",
      "Epoch: 81 | Batch_idx: 390 |  Loss_1: (0.3165) | Acc_1: (88.89%) (44446/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3086) | Acc: (89.57%) (8957/10000)\n",
      "Epoch: 82 | Batch_idx: 0 |  Loss_1: (0.2316) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 82 | Batch_idx: 10 |  Loss_1: (0.3367) | Acc_1: (88.85%) (1251/1408)\n",
      "Epoch: 82 | Batch_idx: 20 |  Loss_1: (0.3187) | Acc_1: (88.95%) (2391/2688)\n",
      "Epoch: 82 | Batch_idx: 30 |  Loss_1: (0.2964) | Acc_1: (89.62%) (3556/3968)\n",
      "Epoch: 82 | Batch_idx: 40 |  Loss_1: (0.2951) | Acc_1: (89.65%) (4705/5248)\n",
      "Epoch: 82 | Batch_idx: 50 |  Loss_1: (0.3000) | Acc_1: (89.57%) (5847/6528)\n",
      "Epoch: 82 | Batch_idx: 60 |  Loss_1: (0.3021) | Acc_1: (89.37%) (6978/7808)\n",
      "Epoch: 82 | Batch_idx: 70 |  Loss_1: (0.3003) | Acc_1: (89.37%) (8122/9088)\n",
      "Epoch: 82 | Batch_idx: 80 |  Loss_1: (0.2966) | Acc_1: (89.49%) (9278/10368)\n",
      "Epoch: 82 | Batch_idx: 90 |  Loss_1: (0.2987) | Acc_1: (89.34%) (10406/11648)\n",
      "Epoch: 82 | Batch_idx: 100 |  Loss_1: (0.2982) | Acc_1: (89.36%) (11552/12928)\n",
      "Epoch: 82 | Batch_idx: 110 |  Loss_1: (0.2984) | Acc_1: (89.37%) (12698/14208)\n",
      "Epoch: 82 | Batch_idx: 120 |  Loss_1: (0.2975) | Acc_1: (89.42%) (13850/15488)\n",
      "Epoch: 82 | Batch_idx: 130 |  Loss_1: (0.2985) | Acc_1: (89.40%) (14990/16768)\n",
      "Epoch: 82 | Batch_idx: 140 |  Loss_1: (0.2959) | Acc_1: (89.51%) (16154/18048)\n",
      "Epoch: 82 | Batch_idx: 150 |  Loss_1: (0.2962) | Acc_1: (89.49%) (17296/19328)\n",
      "Epoch: 82 | Batch_idx: 160 |  Loss_1: (0.3010) | Acc_1: (89.33%) (18410/20608)\n",
      "Epoch: 82 | Batch_idx: 170 |  Loss_1: (0.2991) | Acc_1: (89.38%) (19564/21888)\n",
      "Epoch: 82 | Batch_idx: 180 |  Loss_1: (0.2985) | Acc_1: (89.41%) (20714/23168)\n",
      "Epoch: 82 | Batch_idx: 190 |  Loss_1: (0.2962) | Acc_1: (89.50%) (21881/24448)\n",
      "Epoch: 82 | Batch_idx: 200 |  Loss_1: (0.2969) | Acc_1: (89.47%) (23018/25728)\n",
      "Epoch: 82 | Batch_idx: 210 |  Loss_1: (0.2982) | Acc_1: (89.40%) (24146/27008)\n",
      "Epoch: 82 | Batch_idx: 220 |  Loss_1: (0.3000) | Acc_1: (89.36%) (25279/28288)\n",
      "Epoch: 82 | Batch_idx: 230 |  Loss_1: (0.3010) | Acc_1: (89.34%) (26416/29568)\n",
      "Epoch: 82 | Batch_idx: 240 |  Loss_1: (0.3006) | Acc_1: (89.36%) (27566/30848)\n",
      "Epoch: 82 | Batch_idx: 250 |  Loss_1: (0.3004) | Acc_1: (89.37%) (28714/32128)\n",
      "Epoch: 82 | Batch_idx: 260 |  Loss_1: (0.3003) | Acc_1: (89.36%) (29855/33408)\n",
      "Epoch: 82 | Batch_idx: 270 |  Loss_1: (0.2995) | Acc_1: (89.42%) (31017/34688)\n",
      "Epoch: 82 | Batch_idx: 280 |  Loss_1: (0.2993) | Acc_1: (89.40%) (32157/35968)\n",
      "Epoch: 82 | Batch_idx: 290 |  Loss_1: (0.2994) | Acc_1: (89.42%) (33307/37248)\n",
      "Epoch: 82 | Batch_idx: 300 |  Loss_1: (0.2983) | Acc_1: (89.46%) (34467/38528)\n",
      "Epoch: 82 | Batch_idx: 310 |  Loss_1: (0.3001) | Acc_1: (89.39%) (35584/39808)\n",
      "Epoch: 82 | Batch_idx: 320 |  Loss_1: (0.3005) | Acc_1: (89.39%) (36727/41088)\n",
      "Epoch: 82 | Batch_idx: 330 |  Loss_1: (0.3003) | Acc_1: (89.39%) (37874/42368)\n",
      "Epoch: 82 | Batch_idx: 340 |  Loss_1: (0.3003) | Acc_1: (89.40%) (39020/43648)\n",
      "Epoch: 82 | Batch_idx: 350 |  Loss_1: (0.3004) | Acc_1: (89.41%) (40168/44928)\n",
      "Epoch: 82 | Batch_idx: 360 |  Loss_1: (0.3002) | Acc_1: (89.40%) (41310/46208)\n",
      "Epoch: 82 | Batch_idx: 370 |  Loss_1: (0.3006) | Acc_1: (89.41%) (42457/47488)\n",
      "Epoch: 82 | Batch_idx: 380 |  Loss_1: (0.3011) | Acc_1: (89.38%) (43589/48768)\n",
      "Epoch: 82 | Batch_idx: 390 |  Loss_1: (0.3012) | Acc_1: (89.38%) (44689/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3154) | Acc: (90.08%) (9008/10000)\n",
      "Epoch: 83 | Batch_idx: 0 |  Loss_1: (0.2241) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 83 | Batch_idx: 10 |  Loss_1: (0.2912) | Acc_1: (89.49%) (1260/1408)\n",
      "Epoch: 83 | Batch_idx: 20 |  Loss_1: (0.2724) | Acc_1: (90.07%) (2421/2688)\n",
      "Epoch: 83 | Batch_idx: 30 |  Loss_1: (0.2770) | Acc_1: (89.94%) (3569/3968)\n",
      "Epoch: 83 | Batch_idx: 40 |  Loss_1: (0.2836) | Acc_1: (89.65%) (4705/5248)\n",
      "Epoch: 83 | Batch_idx: 50 |  Loss_1: (0.2831) | Acc_1: (89.80%) (5862/6528)\n",
      "Epoch: 83 | Batch_idx: 60 |  Loss_1: (0.2811) | Acc_1: (89.82%) (7013/7808)\n",
      "Epoch: 83 | Batch_idx: 70 |  Loss_1: (0.2869) | Acc_1: (89.78%) (8159/9088)\n",
      "Epoch: 83 | Batch_idx: 80 |  Loss_1: (0.2847) | Acc_1: (89.82%) (9313/10368)\n",
      "Epoch: 83 | Batch_idx: 90 |  Loss_1: (0.2851) | Acc_1: (89.77%) (10456/11648)\n",
      "Epoch: 83 | Batch_idx: 100 |  Loss_1: (0.2849) | Acc_1: (89.79%) (11608/12928)\n",
      "Epoch: 83 | Batch_idx: 110 |  Loss_1: (0.2895) | Acc_1: (89.62%) (12733/14208)\n",
      "Epoch: 83 | Batch_idx: 120 |  Loss_1: (0.2910) | Acc_1: (89.66%) (13887/15488)\n",
      "Epoch: 83 | Batch_idx: 130 |  Loss_1: (0.2930) | Acc_1: (89.69%) (15039/16768)\n",
      "Epoch: 83 | Batch_idx: 140 |  Loss_1: (0.2957) | Acc_1: (89.58%) (16168/18048)\n",
      "Epoch: 83 | Batch_idx: 150 |  Loss_1: (0.2968) | Acc_1: (89.53%) (17305/19328)\n",
      "Epoch: 83 | Batch_idx: 160 |  Loss_1: (0.2967) | Acc_1: (89.55%) (18454/20608)\n",
      "Epoch: 83 | Batch_idx: 170 |  Loss_1: (0.2954) | Acc_1: (89.58%) (19608/21888)\n",
      "Epoch: 83 | Batch_idx: 180 |  Loss_1: (0.2951) | Acc_1: (89.64%) (20767/23168)\n",
      "Epoch: 83 | Batch_idx: 190 |  Loss_1: (0.2955) | Acc_1: (89.60%) (21906/24448)\n",
      "Epoch: 83 | Batch_idx: 200 |  Loss_1: (0.2967) | Acc_1: (89.54%) (23037/25728)\n",
      "Epoch: 83 | Batch_idx: 210 |  Loss_1: (0.2973) | Acc_1: (89.54%) (24183/27008)\n",
      "Epoch: 83 | Batch_idx: 220 |  Loss_1: (0.2969) | Acc_1: (89.51%) (25321/28288)\n",
      "Epoch: 83 | Batch_idx: 230 |  Loss_1: (0.2958) | Acc_1: (89.54%) (26476/29568)\n",
      "Epoch: 83 | Batch_idx: 240 |  Loss_1: (0.2978) | Acc_1: (89.51%) (27612/30848)\n",
      "Epoch: 83 | Batch_idx: 250 |  Loss_1: (0.2985) | Acc_1: (89.47%) (28746/32128)\n",
      "Epoch: 83 | Batch_idx: 260 |  Loss_1: (0.2994) | Acc_1: (89.42%) (29872/33408)\n",
      "Epoch: 83 | Batch_idx: 270 |  Loss_1: (0.2986) | Acc_1: (89.45%) (31027/34688)\n",
      "Epoch: 83 | Batch_idx: 280 |  Loss_1: (0.2987) | Acc_1: (89.45%) (32174/35968)\n",
      "Epoch: 83 | Batch_idx: 290 |  Loss_1: (0.2989) | Acc_1: (89.46%) (33321/37248)\n",
      "Epoch: 83 | Batch_idx: 300 |  Loss_1: (0.2984) | Acc_1: (89.46%) (34467/38528)\n",
      "Epoch: 83 | Batch_idx: 310 |  Loss_1: (0.2987) | Acc_1: (89.44%) (35604/39808)\n",
      "Epoch: 83 | Batch_idx: 320 |  Loss_1: (0.3002) | Acc_1: (89.38%) (36724/41088)\n",
      "Epoch: 83 | Batch_idx: 330 |  Loss_1: (0.2996) | Acc_1: (89.40%) (37879/42368)\n",
      "Epoch: 83 | Batch_idx: 340 |  Loss_1: (0.2998) | Acc_1: (89.40%) (39021/43648)\n",
      "Epoch: 83 | Batch_idx: 350 |  Loss_1: (0.3004) | Acc_1: (89.37%) (40152/44928)\n",
      "Epoch: 83 | Batch_idx: 360 |  Loss_1: (0.2995) | Acc_1: (89.40%) (41312/46208)\n",
      "Epoch: 83 | Batch_idx: 370 |  Loss_1: (0.2999) | Acc_1: (89.38%) (42447/47488)\n",
      "Epoch: 83 | Batch_idx: 380 |  Loss_1: (0.2999) | Acc_1: (89.39%) (43593/48768)\n",
      "Epoch: 83 | Batch_idx: 390 |  Loss_1: (0.3005) | Acc_1: (89.35%) (44674/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3263) | Acc: (89.13%) (8913/10000)\n",
      "Epoch: 84 | Batch_idx: 0 |  Loss_1: (0.3218) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 84 | Batch_idx: 10 |  Loss_1: (0.2807) | Acc_1: (89.63%) (1262/1408)\n",
      "Epoch: 84 | Batch_idx: 20 |  Loss_1: (0.2804) | Acc_1: (89.96%) (2418/2688)\n",
      "Epoch: 84 | Batch_idx: 30 |  Loss_1: (0.2875) | Acc_1: (89.64%) (3557/3968)\n",
      "Epoch: 84 | Batch_idx: 40 |  Loss_1: (0.2891) | Acc_1: (89.75%) (4710/5248)\n",
      "Epoch: 84 | Batch_idx: 50 |  Loss_1: (0.2868) | Acc_1: (89.78%) (5861/6528)\n",
      "Epoch: 84 | Batch_idx: 60 |  Loss_1: (0.2824) | Acc_1: (89.86%) (7016/7808)\n",
      "Epoch: 84 | Batch_idx: 70 |  Loss_1: (0.2880) | Acc_1: (89.71%) (8153/9088)\n",
      "Epoch: 84 | Batch_idx: 80 |  Loss_1: (0.2845) | Acc_1: (89.91%) (9322/10368)\n",
      "Epoch: 84 | Batch_idx: 90 |  Loss_1: (0.2880) | Acc_1: (89.73%) (10452/11648)\n",
      "Epoch: 84 | Batch_idx: 100 |  Loss_1: (0.2900) | Acc_1: (89.69%) (11595/12928)\n",
      "Epoch: 84 | Batch_idx: 110 |  Loss_1: (0.2901) | Acc_1: (89.68%) (12742/14208)\n",
      "Epoch: 84 | Batch_idx: 120 |  Loss_1: (0.2944) | Acc_1: (89.59%) (13876/15488)\n",
      "Epoch: 84 | Batch_idx: 130 |  Loss_1: (0.2951) | Acc_1: (89.54%) (15014/16768)\n",
      "Epoch: 84 | Batch_idx: 140 |  Loss_1: (0.2947) | Acc_1: (89.48%) (16150/18048)\n",
      "Epoch: 84 | Batch_idx: 150 |  Loss_1: (0.2921) | Acc_1: (89.55%) (17308/19328)\n",
      "Epoch: 84 | Batch_idx: 160 |  Loss_1: (0.2917) | Acc_1: (89.55%) (18455/20608)\n",
      "Epoch: 84 | Batch_idx: 170 |  Loss_1: (0.2929) | Acc_1: (89.54%) (19599/21888)\n",
      "Epoch: 84 | Batch_idx: 180 |  Loss_1: (0.2931) | Acc_1: (89.55%) (20747/23168)\n",
      "Epoch: 84 | Batch_idx: 190 |  Loss_1: (0.2927) | Acc_1: (89.54%) (21891/24448)\n",
      "Epoch: 84 | Batch_idx: 200 |  Loss_1: (0.2933) | Acc_1: (89.56%) (23041/25728)\n",
      "Epoch: 84 | Batch_idx: 210 |  Loss_1: (0.2938) | Acc_1: (89.53%) (24180/27008)\n",
      "Epoch: 84 | Batch_idx: 220 |  Loss_1: (0.2940) | Acc_1: (89.52%) (25323/28288)\n",
      "Epoch: 84 | Batch_idx: 230 |  Loss_1: (0.2945) | Acc_1: (89.51%) (26466/29568)\n",
      "Epoch: 84 | Batch_idx: 240 |  Loss_1: (0.2962) | Acc_1: (89.46%) (27597/30848)\n",
      "Epoch: 84 | Batch_idx: 250 |  Loss_1: (0.2967) | Acc_1: (89.44%) (28735/32128)\n",
      "Epoch: 84 | Batch_idx: 260 |  Loss_1: (0.2967) | Acc_1: (89.42%) (29874/33408)\n",
      "Epoch: 84 | Batch_idx: 270 |  Loss_1: (0.2972) | Acc_1: (89.41%) (31013/34688)\n",
      "Epoch: 84 | Batch_idx: 280 |  Loss_1: (0.2965) | Acc_1: (89.44%) (32169/35968)\n",
      "Epoch: 84 | Batch_idx: 290 |  Loss_1: (0.2954) | Acc_1: (89.49%) (33332/37248)\n",
      "Epoch: 84 | Batch_idx: 300 |  Loss_1: (0.2949) | Acc_1: (89.49%) (34478/38528)\n",
      "Epoch: 84 | Batch_idx: 310 |  Loss_1: (0.2952) | Acc_1: (89.50%) (35628/39808)\n",
      "Epoch: 84 | Batch_idx: 320 |  Loss_1: (0.2951) | Acc_1: (89.51%) (36779/41088)\n",
      "Epoch: 84 | Batch_idx: 330 |  Loss_1: (0.2960) | Acc_1: (89.47%) (37908/42368)\n",
      "Epoch: 84 | Batch_idx: 340 |  Loss_1: (0.2964) | Acc_1: (89.47%) (39053/43648)\n",
      "Epoch: 84 | Batch_idx: 350 |  Loss_1: (0.2967) | Acc_1: (89.47%) (40199/44928)\n",
      "Epoch: 84 | Batch_idx: 360 |  Loss_1: (0.2966) | Acc_1: (89.47%) (41341/46208)\n",
      "Epoch: 84 | Batch_idx: 370 |  Loss_1: (0.2962) | Acc_1: (89.49%) (42495/47488)\n",
      "Epoch: 84 | Batch_idx: 380 |  Loss_1: (0.2966) | Acc_1: (89.47%) (43633/48768)\n",
      "Epoch: 84 | Batch_idx: 390 |  Loss_1: (0.2980) | Acc_1: (89.42%) (44711/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3163) | Acc: (89.58%) (8958/10000)\n",
      "Epoch: 85 | Batch_idx: 0 |  Loss_1: (0.3438) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 85 | Batch_idx: 10 |  Loss_1: (0.3223) | Acc_1: (87.86%) (1237/1408)\n",
      "Epoch: 85 | Batch_idx: 20 |  Loss_1: (0.3112) | Acc_1: (88.65%) (2383/2688)\n",
      "Epoch: 85 | Batch_idx: 30 |  Loss_1: (0.3007) | Acc_1: (89.31%) (3544/3968)\n",
      "Epoch: 85 | Batch_idx: 40 |  Loss_1: (0.2957) | Acc_1: (89.35%) (4689/5248)\n",
      "Epoch: 85 | Batch_idx: 50 |  Loss_1: (0.2908) | Acc_1: (89.60%) (5849/6528)\n",
      "Epoch: 85 | Batch_idx: 60 |  Loss_1: (0.2900) | Acc_1: (89.63%) (6998/7808)\n",
      "Epoch: 85 | Batch_idx: 70 |  Loss_1: (0.2926) | Acc_1: (89.49%) (8133/9088)\n",
      "Epoch: 85 | Batch_idx: 80 |  Loss_1: (0.2891) | Acc_1: (89.66%) (9296/10368)\n",
      "Epoch: 85 | Batch_idx: 90 |  Loss_1: (0.2896) | Acc_1: (89.64%) (10441/11648)\n",
      "Epoch: 85 | Batch_idx: 100 |  Loss_1: (0.2896) | Acc_1: (89.61%) (11585/12928)\n",
      "Epoch: 85 | Batch_idx: 110 |  Loss_1: (0.2878) | Acc_1: (89.65%) (12738/14208)\n",
      "Epoch: 85 | Batch_idx: 120 |  Loss_1: (0.2875) | Acc_1: (89.62%) (13880/15488)\n",
      "Epoch: 85 | Batch_idx: 130 |  Loss_1: (0.2869) | Acc_1: (89.62%) (15028/16768)\n",
      "Epoch: 85 | Batch_idx: 140 |  Loss_1: (0.2874) | Acc_1: (89.64%) (16178/18048)\n",
      "Epoch: 85 | Batch_idx: 150 |  Loss_1: (0.2887) | Acc_1: (89.60%) (17317/19328)\n",
      "Epoch: 85 | Batch_idx: 160 |  Loss_1: (0.2883) | Acc_1: (89.57%) (18458/20608)\n",
      "Epoch: 85 | Batch_idx: 170 |  Loss_1: (0.2869) | Acc_1: (89.61%) (19614/21888)\n",
      "Epoch: 85 | Batch_idx: 180 |  Loss_1: (0.2888) | Acc_1: (89.55%) (20746/23168)\n",
      "Epoch: 85 | Batch_idx: 190 |  Loss_1: (0.2890) | Acc_1: (89.59%) (21904/24448)\n",
      "Epoch: 85 | Batch_idx: 200 |  Loss_1: (0.2886) | Acc_1: (89.63%) (23061/25728)\n",
      "Epoch: 85 | Batch_idx: 210 |  Loss_1: (0.2897) | Acc_1: (89.60%) (24198/27008)\n",
      "Epoch: 85 | Batch_idx: 220 |  Loss_1: (0.2886) | Acc_1: (89.59%) (25342/28288)\n",
      "Epoch: 85 | Batch_idx: 230 |  Loss_1: (0.2887) | Acc_1: (89.60%) (26493/29568)\n",
      "Epoch: 85 | Batch_idx: 240 |  Loss_1: (0.2891) | Acc_1: (89.60%) (27639/30848)\n",
      "Epoch: 85 | Batch_idx: 250 |  Loss_1: (0.2891) | Acc_1: (89.60%) (28787/32128)\n",
      "Epoch: 85 | Batch_idx: 260 |  Loss_1: (0.2896) | Acc_1: (89.59%) (29930/33408)\n",
      "Epoch: 85 | Batch_idx: 270 |  Loss_1: (0.2886) | Acc_1: (89.63%) (31091/34688)\n",
      "Epoch: 85 | Batch_idx: 280 |  Loss_1: (0.2885) | Acc_1: (89.64%) (32241/35968)\n",
      "Epoch: 85 | Batch_idx: 290 |  Loss_1: (0.2877) | Acc_1: (89.68%) (33404/37248)\n",
      "Epoch: 85 | Batch_idx: 300 |  Loss_1: (0.2881) | Acc_1: (89.68%) (34551/38528)\n",
      "Epoch: 85 | Batch_idx: 310 |  Loss_1: (0.2884) | Acc_1: (89.68%) (35700/39808)\n",
      "Epoch: 85 | Batch_idx: 320 |  Loss_1: (0.2873) | Acc_1: (89.71%) (36858/41088)\n",
      "Epoch: 85 | Batch_idx: 330 |  Loss_1: (0.2874) | Acc_1: (89.68%) (37996/42368)\n",
      "Epoch: 85 | Batch_idx: 340 |  Loss_1: (0.2885) | Acc_1: (89.65%) (39131/43648)\n",
      "Epoch: 85 | Batch_idx: 350 |  Loss_1: (0.2892) | Acc_1: (89.62%) (40263/44928)\n",
      "Epoch: 85 | Batch_idx: 360 |  Loss_1: (0.2894) | Acc_1: (89.61%) (41407/46208)\n",
      "Epoch: 85 | Batch_idx: 370 |  Loss_1: (0.2898) | Acc_1: (89.60%) (42547/47488)\n",
      "Epoch: 85 | Batch_idx: 380 |  Loss_1: (0.2907) | Acc_1: (89.56%) (43679/48768)\n",
      "Epoch: 85 | Batch_idx: 390 |  Loss_1: (0.2912) | Acc_1: (89.55%) (44777/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3249) | Acc: (89.71%) (8971/10000)\n",
      "Epoch: 86 | Batch_idx: 0 |  Loss_1: (0.2352) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 86 | Batch_idx: 10 |  Loss_1: (0.2654) | Acc_1: (89.91%) (1266/1408)\n",
      "Epoch: 86 | Batch_idx: 20 |  Loss_1: (0.2733) | Acc_1: (89.96%) (2418/2688)\n",
      "Epoch: 86 | Batch_idx: 30 |  Loss_1: (0.2763) | Acc_1: (90.15%) (3577/3968)\n",
      "Epoch: 86 | Batch_idx: 40 |  Loss_1: (0.2657) | Acc_1: (90.68%) (4759/5248)\n",
      "Epoch: 86 | Batch_idx: 50 |  Loss_1: (0.2635) | Acc_1: (90.66%) (5918/6528)\n",
      "Epoch: 86 | Batch_idx: 60 |  Loss_1: (0.2654) | Acc_1: (90.59%) (7073/7808)\n",
      "Epoch: 86 | Batch_idx: 70 |  Loss_1: (0.2669) | Acc_1: (90.47%) (8222/9088)\n",
      "Epoch: 86 | Batch_idx: 80 |  Loss_1: (0.2682) | Acc_1: (90.43%) (9376/10368)\n",
      "Epoch: 86 | Batch_idx: 90 |  Loss_1: (0.2710) | Acc_1: (90.33%) (10522/11648)\n",
      "Epoch: 86 | Batch_idx: 100 |  Loss_1: (0.2726) | Acc_1: (90.24%) (11666/12928)\n",
      "Epoch: 86 | Batch_idx: 110 |  Loss_1: (0.2730) | Acc_1: (90.22%) (12819/14208)\n",
      "Epoch: 86 | Batch_idx: 120 |  Loss_1: (0.2749) | Acc_1: (90.14%) (13961/15488)\n",
      "Epoch: 86 | Batch_idx: 130 |  Loss_1: (0.2748) | Acc_1: (90.14%) (15114/16768)\n",
      "Epoch: 86 | Batch_idx: 140 |  Loss_1: (0.2763) | Acc_1: (90.05%) (16253/18048)\n",
      "Epoch: 86 | Batch_idx: 150 |  Loss_1: (0.2772) | Acc_1: (90.01%) (17397/19328)\n",
      "Epoch: 86 | Batch_idx: 160 |  Loss_1: (0.2789) | Acc_1: (89.91%) (18528/20608)\n",
      "Epoch: 86 | Batch_idx: 170 |  Loss_1: (0.2790) | Acc_1: (89.91%) (19680/21888)\n",
      "Epoch: 86 | Batch_idx: 180 |  Loss_1: (0.2790) | Acc_1: (89.91%) (20830/23168)\n",
      "Epoch: 86 | Batch_idx: 190 |  Loss_1: (0.2798) | Acc_1: (89.88%) (21974/24448)\n",
      "Epoch: 86 | Batch_idx: 200 |  Loss_1: (0.2803) | Acc_1: (89.89%) (23128/25728)\n",
      "Epoch: 86 | Batch_idx: 210 |  Loss_1: (0.2787) | Acc_1: (89.97%) (24299/27008)\n",
      "Epoch: 86 | Batch_idx: 220 |  Loss_1: (0.2808) | Acc_1: (89.93%) (25438/28288)\n",
      "Epoch: 86 | Batch_idx: 230 |  Loss_1: (0.2802) | Acc_1: (89.95%) (26595/29568)\n",
      "Epoch: 86 | Batch_idx: 240 |  Loss_1: (0.2796) | Acc_1: (89.98%) (27756/30848)\n",
      "Epoch: 86 | Batch_idx: 250 |  Loss_1: (0.2795) | Acc_1: (89.94%) (28895/32128)\n",
      "Epoch: 86 | Batch_idx: 260 |  Loss_1: (0.2787) | Acc_1: (89.98%) (30060/33408)\n",
      "Epoch: 86 | Batch_idx: 270 |  Loss_1: (0.2783) | Acc_1: (89.99%) (31216/34688)\n",
      "Epoch: 86 | Batch_idx: 280 |  Loss_1: (0.2792) | Acc_1: (89.97%) (32360/35968)\n",
      "Epoch: 86 | Batch_idx: 290 |  Loss_1: (0.2794) | Acc_1: (89.98%) (33515/37248)\n",
      "Epoch: 86 | Batch_idx: 300 |  Loss_1: (0.2798) | Acc_1: (89.97%) (34665/38528)\n",
      "Epoch: 86 | Batch_idx: 310 |  Loss_1: (0.2791) | Acc_1: (89.99%) (35822/39808)\n",
      "Epoch: 86 | Batch_idx: 320 |  Loss_1: (0.2788) | Acc_1: (90.00%) (36978/41088)\n",
      "Epoch: 86 | Batch_idx: 330 |  Loss_1: (0.2804) | Acc_1: (89.96%) (38116/42368)\n",
      "Epoch: 86 | Batch_idx: 340 |  Loss_1: (0.2806) | Acc_1: (89.95%) (39263/43648)\n",
      "Epoch: 86 | Batch_idx: 350 |  Loss_1: (0.2814) | Acc_1: (89.94%) (40407/44928)\n",
      "Epoch: 86 | Batch_idx: 360 |  Loss_1: (0.2818) | Acc_1: (89.93%) (41555/46208)\n",
      "Epoch: 86 | Batch_idx: 370 |  Loss_1: (0.2824) | Acc_1: (89.89%) (42689/47488)\n",
      "Epoch: 86 | Batch_idx: 380 |  Loss_1: (0.2829) | Acc_1: (89.86%) (43821/48768)\n",
      "Epoch: 86 | Batch_idx: 390 |  Loss_1: (0.2819) | Acc_1: (89.90%) (44951/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3203) | Acc: (90.01%) (9001/10000)\n",
      "Epoch: 87 | Batch_idx: 0 |  Loss_1: (0.1526) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 87 | Batch_idx: 10 |  Loss_1: (0.2542) | Acc_1: (90.27%) (1271/1408)\n",
      "Epoch: 87 | Batch_idx: 20 |  Loss_1: (0.2718) | Acc_1: (89.92%) (2417/2688)\n",
      "Epoch: 87 | Batch_idx: 30 |  Loss_1: (0.2602) | Acc_1: (90.47%) (3590/3968)\n",
      "Epoch: 87 | Batch_idx: 40 |  Loss_1: (0.2633) | Acc_1: (90.36%) (4742/5248)\n",
      "Epoch: 87 | Batch_idx: 50 |  Loss_1: (0.2646) | Acc_1: (90.41%) (5902/6528)\n",
      "Epoch: 87 | Batch_idx: 60 |  Loss_1: (0.2694) | Acc_1: (90.39%) (7058/7808)\n",
      "Epoch: 87 | Batch_idx: 70 |  Loss_1: (0.2689) | Acc_1: (90.49%) (8224/9088)\n",
      "Epoch: 87 | Batch_idx: 80 |  Loss_1: (0.2732) | Acc_1: (90.34%) (9366/10368)\n",
      "Epoch: 87 | Batch_idx: 90 |  Loss_1: (0.2725) | Acc_1: (90.39%) (10529/11648)\n",
      "Epoch: 87 | Batch_idx: 100 |  Loss_1: (0.2718) | Acc_1: (90.39%) (11686/12928)\n",
      "Epoch: 87 | Batch_idx: 110 |  Loss_1: (0.2702) | Acc_1: (90.45%) (12851/14208)\n",
      "Epoch: 87 | Batch_idx: 120 |  Loss_1: (0.2738) | Acc_1: (90.32%) (13989/15488)\n",
      "Epoch: 87 | Batch_idx: 130 |  Loss_1: (0.2763) | Acc_1: (90.21%) (15127/16768)\n",
      "Epoch: 87 | Batch_idx: 140 |  Loss_1: (0.2788) | Acc_1: (90.13%) (16266/18048)\n",
      "Epoch: 87 | Batch_idx: 150 |  Loss_1: (0.2811) | Acc_1: (90.02%) (17399/19328)\n",
      "Epoch: 87 | Batch_idx: 160 |  Loss_1: (0.2792) | Acc_1: (90.06%) (18560/20608)\n",
      "Epoch: 87 | Batch_idx: 170 |  Loss_1: (0.2778) | Acc_1: (90.14%) (19729/21888)\n",
      "Epoch: 87 | Batch_idx: 180 |  Loss_1: (0.2778) | Acc_1: (90.14%) (20884/23168)\n",
      "Epoch: 87 | Batch_idx: 190 |  Loss_1: (0.2769) | Acc_1: (90.21%) (22054/24448)\n",
      "Epoch: 87 | Batch_idx: 200 |  Loss_1: (0.2768) | Acc_1: (90.21%) (23209/25728)\n",
      "Epoch: 87 | Batch_idx: 210 |  Loss_1: (0.2769) | Acc_1: (90.20%) (24362/27008)\n",
      "Epoch: 87 | Batch_idx: 220 |  Loss_1: (0.2778) | Acc_1: (90.21%) (25518/28288)\n",
      "Epoch: 87 | Batch_idx: 230 |  Loss_1: (0.2777) | Acc_1: (90.18%) (26665/29568)\n",
      "Epoch: 87 | Batch_idx: 240 |  Loss_1: (0.2770) | Acc_1: (90.20%) (27825/30848)\n",
      "Epoch: 87 | Batch_idx: 250 |  Loss_1: (0.2772) | Acc_1: (90.21%) (28983/32128)\n",
      "Epoch: 87 | Batch_idx: 260 |  Loss_1: (0.2768) | Acc_1: (90.22%) (30142/33408)\n",
      "Epoch: 87 | Batch_idx: 270 |  Loss_1: (0.2771) | Acc_1: (90.22%) (31297/34688)\n",
      "Epoch: 87 | Batch_idx: 280 |  Loss_1: (0.2770) | Acc_1: (90.23%) (32454/35968)\n",
      "Epoch: 87 | Batch_idx: 290 |  Loss_1: (0.2771) | Acc_1: (90.22%) (33606/37248)\n",
      "Epoch: 87 | Batch_idx: 300 |  Loss_1: (0.2774) | Acc_1: (90.24%) (34769/38528)\n",
      "Epoch: 87 | Batch_idx: 310 |  Loss_1: (0.2786) | Acc_1: (90.23%) (35917/39808)\n",
      "Epoch: 87 | Batch_idx: 320 |  Loss_1: (0.2792) | Acc_1: (90.20%) (37061/41088)\n",
      "Epoch: 87 | Batch_idx: 330 |  Loss_1: (0.2789) | Acc_1: (90.20%) (38218/42368)\n",
      "Epoch: 87 | Batch_idx: 340 |  Loss_1: (0.2790) | Acc_1: (90.20%) (39369/43648)\n",
      "Epoch: 87 | Batch_idx: 350 |  Loss_1: (0.2790) | Acc_1: (90.21%) (40529/44928)\n",
      "Epoch: 87 | Batch_idx: 360 |  Loss_1: (0.2800) | Acc_1: (90.19%) (41675/46208)\n",
      "Epoch: 87 | Batch_idx: 370 |  Loss_1: (0.2797) | Acc_1: (90.20%) (42835/47488)\n",
      "Epoch: 87 | Batch_idx: 380 |  Loss_1: (0.2799) | Acc_1: (90.20%) (43990/48768)\n",
      "Epoch: 87 | Batch_idx: 390 |  Loss_1: (0.2801) | Acc_1: (90.20%) (45100/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3062) | Acc: (90.13%) (9013/10000)\n",
      "Epoch: 88 | Batch_idx: 0 |  Loss_1: (0.2639) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 88 | Batch_idx: 10 |  Loss_1: (0.2661) | Acc_1: (90.62%) (1276/1408)\n",
      "Epoch: 88 | Batch_idx: 20 |  Loss_1: (0.2615) | Acc_1: (90.62%) (2436/2688)\n",
      "Epoch: 88 | Batch_idx: 30 |  Loss_1: (0.2707) | Acc_1: (90.37%) (3586/3968)\n",
      "Epoch: 88 | Batch_idx: 40 |  Loss_1: (0.2706) | Acc_1: (90.43%) (4746/5248)\n",
      "Epoch: 88 | Batch_idx: 50 |  Loss_1: (0.2765) | Acc_1: (90.15%) (5885/6528)\n",
      "Epoch: 88 | Batch_idx: 60 |  Loss_1: (0.2737) | Acc_1: (90.32%) (7052/7808)\n",
      "Epoch: 88 | Batch_idx: 70 |  Loss_1: (0.2766) | Acc_1: (90.23%) (8200/9088)\n",
      "Epoch: 88 | Batch_idx: 80 |  Loss_1: (0.2767) | Acc_1: (90.22%) (9354/10368)\n",
      "Epoch: 88 | Batch_idx: 90 |  Loss_1: (0.2750) | Acc_1: (90.32%) (10521/11648)\n",
      "Epoch: 88 | Batch_idx: 100 |  Loss_1: (0.2761) | Acc_1: (90.24%) (11666/12928)\n",
      "Epoch: 88 | Batch_idx: 110 |  Loss_1: (0.2763) | Acc_1: (90.27%) (12826/14208)\n",
      "Epoch: 88 | Batch_idx: 120 |  Loss_1: (0.2767) | Acc_1: (90.23%) (13975/15488)\n",
      "Epoch: 88 | Batch_idx: 130 |  Loss_1: (0.2750) | Acc_1: (90.29%) (15139/16768)\n",
      "Epoch: 88 | Batch_idx: 140 |  Loss_1: (0.2742) | Acc_1: (90.33%) (16302/18048)\n",
      "Epoch: 88 | Batch_idx: 150 |  Loss_1: (0.2744) | Acc_1: (90.34%) (17460/19328)\n",
      "Epoch: 88 | Batch_idx: 160 |  Loss_1: (0.2751) | Acc_1: (90.29%) (18606/20608)\n",
      "Epoch: 88 | Batch_idx: 170 |  Loss_1: (0.2760) | Acc_1: (90.22%) (19747/21888)\n",
      "Epoch: 88 | Batch_idx: 180 |  Loss_1: (0.2750) | Acc_1: (90.29%) (20919/23168)\n",
      "Epoch: 88 | Batch_idx: 190 |  Loss_1: (0.2757) | Acc_1: (90.24%) (22062/24448)\n",
      "Epoch: 88 | Batch_idx: 200 |  Loss_1: (0.2760) | Acc_1: (90.23%) (23215/25728)\n",
      "Epoch: 88 | Batch_idx: 210 |  Loss_1: (0.2767) | Acc_1: (90.17%) (24354/27008)\n",
      "Epoch: 88 | Batch_idx: 220 |  Loss_1: (0.2782) | Acc_1: (90.12%) (25494/28288)\n",
      "Epoch: 88 | Batch_idx: 230 |  Loss_1: (0.2774) | Acc_1: (90.13%) (26650/29568)\n",
      "Epoch: 88 | Batch_idx: 240 |  Loss_1: (0.2772) | Acc_1: (90.14%) (27805/30848)\n",
      "Epoch: 88 | Batch_idx: 250 |  Loss_1: (0.2766) | Acc_1: (90.16%) (28968/32128)\n",
      "Epoch: 88 | Batch_idx: 260 |  Loss_1: (0.2747) | Acc_1: (90.25%) (30151/33408)\n",
      "Epoch: 88 | Batch_idx: 270 |  Loss_1: (0.2756) | Acc_1: (90.22%) (31297/34688)\n",
      "Epoch: 88 | Batch_idx: 280 |  Loss_1: (0.2760) | Acc_1: (90.23%) (32454/35968)\n",
      "Epoch: 88 | Batch_idx: 290 |  Loss_1: (0.2767) | Acc_1: (90.21%) (33601/37248)\n",
      "Epoch: 88 | Batch_idx: 300 |  Loss_1: (0.2771) | Acc_1: (90.17%) (34742/38528)\n",
      "Epoch: 88 | Batch_idx: 310 |  Loss_1: (0.2775) | Acc_1: (90.15%) (35887/39808)\n",
      "Epoch: 88 | Batch_idx: 320 |  Loss_1: (0.2765) | Acc_1: (90.16%) (37046/41088)\n",
      "Epoch: 88 | Batch_idx: 330 |  Loss_1: (0.2757) | Acc_1: (90.17%) (38203/42368)\n",
      "Epoch: 88 | Batch_idx: 340 |  Loss_1: (0.2758) | Acc_1: (90.15%) (39347/43648)\n",
      "Epoch: 88 | Batch_idx: 350 |  Loss_1: (0.2763) | Acc_1: (90.12%) (40490/44928)\n",
      "Epoch: 88 | Batch_idx: 360 |  Loss_1: (0.2764) | Acc_1: (90.12%) (41642/46208)\n",
      "Epoch: 88 | Batch_idx: 370 |  Loss_1: (0.2774) | Acc_1: (90.09%) (42782/47488)\n",
      "Epoch: 88 | Batch_idx: 380 |  Loss_1: (0.2774) | Acc_1: (90.08%) (43929/48768)\n",
      "Epoch: 88 | Batch_idx: 390 |  Loss_1: (0.2774) | Acc_1: (90.07%) (45036/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2982) | Acc: (90.42%) (9042/10000)\n",
      "Epoch: 89 | Batch_idx: 0 |  Loss_1: (0.2088) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 89 | Batch_idx: 10 |  Loss_1: (0.2506) | Acc_1: (90.06%) (1268/1408)\n",
      "Epoch: 89 | Batch_idx: 20 |  Loss_1: (0.2546) | Acc_1: (90.44%) (2431/2688)\n",
      "Epoch: 89 | Batch_idx: 30 |  Loss_1: (0.2432) | Acc_1: (91.00%) (3611/3968)\n",
      "Epoch: 89 | Batch_idx: 40 |  Loss_1: (0.2445) | Acc_1: (91.06%) (4779/5248)\n",
      "Epoch: 89 | Batch_idx: 50 |  Loss_1: (0.2452) | Acc_1: (91.13%) (5949/6528)\n",
      "Epoch: 89 | Batch_idx: 60 |  Loss_1: (0.2503) | Acc_1: (91.05%) (7109/7808)\n",
      "Epoch: 89 | Batch_idx: 70 |  Loss_1: (0.2529) | Acc_1: (90.98%) (8268/9088)\n",
      "Epoch: 89 | Batch_idx: 80 |  Loss_1: (0.2486) | Acc_1: (91.19%) (9455/10368)\n",
      "Epoch: 89 | Batch_idx: 90 |  Loss_1: (0.2495) | Acc_1: (91.23%) (10627/11648)\n",
      "Epoch: 89 | Batch_idx: 100 |  Loss_1: (0.2496) | Acc_1: (91.21%) (11792/12928)\n",
      "Epoch: 89 | Batch_idx: 110 |  Loss_1: (0.2580) | Acc_1: (90.95%) (12922/14208)\n",
      "Epoch: 89 | Batch_idx: 120 |  Loss_1: (0.2581) | Acc_1: (90.93%) (14084/15488)\n",
      "Epoch: 89 | Batch_idx: 130 |  Loss_1: (0.2598) | Acc_1: (90.86%) (15236/16768)\n",
      "Epoch: 89 | Batch_idx: 140 |  Loss_1: (0.2633) | Acc_1: (90.76%) (16381/18048)\n",
      "Epoch: 89 | Batch_idx: 150 |  Loss_1: (0.2634) | Acc_1: (90.75%) (17541/19328)\n",
      "Epoch: 89 | Batch_idx: 160 |  Loss_1: (0.2627) | Acc_1: (90.72%) (18696/20608)\n",
      "Epoch: 89 | Batch_idx: 170 |  Loss_1: (0.2620) | Acc_1: (90.71%) (19854/21888)\n",
      "Epoch: 89 | Batch_idx: 180 |  Loss_1: (0.2606) | Acc_1: (90.75%) (21024/23168)\n",
      "Epoch: 89 | Batch_idx: 190 |  Loss_1: (0.2618) | Acc_1: (90.71%) (22177/24448)\n",
      "Epoch: 89 | Batch_idx: 200 |  Loss_1: (0.2615) | Acc_1: (90.69%) (23333/25728)\n",
      "Epoch: 89 | Batch_idx: 210 |  Loss_1: (0.2626) | Acc_1: (90.68%) (24492/27008)\n",
      "Epoch: 89 | Batch_idx: 220 |  Loss_1: (0.2639) | Acc_1: (90.67%) (25649/28288)\n",
      "Epoch: 89 | Batch_idx: 230 |  Loss_1: (0.2657) | Acc_1: (90.64%) (26799/29568)\n",
      "Epoch: 89 | Batch_idx: 240 |  Loss_1: (0.2665) | Acc_1: (90.60%) (27947/30848)\n",
      "Epoch: 89 | Batch_idx: 250 |  Loss_1: (0.2674) | Acc_1: (90.58%) (29101/32128)\n",
      "Epoch: 89 | Batch_idx: 260 |  Loss_1: (0.2665) | Acc_1: (90.61%) (30271/33408)\n",
      "Epoch: 89 | Batch_idx: 270 |  Loss_1: (0.2677) | Acc_1: (90.55%) (31410/34688)\n",
      "Epoch: 89 | Batch_idx: 280 |  Loss_1: (0.2688) | Acc_1: (90.53%) (32563/35968)\n",
      "Epoch: 89 | Batch_idx: 290 |  Loss_1: (0.2699) | Acc_1: (90.51%) (33715/37248)\n",
      "Epoch: 89 | Batch_idx: 300 |  Loss_1: (0.2695) | Acc_1: (90.50%) (34868/38528)\n",
      "Epoch: 89 | Batch_idx: 310 |  Loss_1: (0.2706) | Acc_1: (90.45%) (36006/39808)\n",
      "Epoch: 89 | Batch_idx: 320 |  Loss_1: (0.2723) | Acc_1: (90.40%) (37145/41088)\n",
      "Epoch: 89 | Batch_idx: 330 |  Loss_1: (0.2735) | Acc_1: (90.39%) (38295/42368)\n",
      "Epoch: 89 | Batch_idx: 340 |  Loss_1: (0.2738) | Acc_1: (90.37%) (39443/43648)\n",
      "Epoch: 89 | Batch_idx: 350 |  Loss_1: (0.2746) | Acc_1: (90.34%) (40588/44928)\n",
      "Epoch: 89 | Batch_idx: 360 |  Loss_1: (0.2755) | Acc_1: (90.30%) (41724/46208)\n",
      "Epoch: 89 | Batch_idx: 370 |  Loss_1: (0.2759) | Acc_1: (90.29%) (42878/47488)\n",
      "Epoch: 89 | Batch_idx: 380 |  Loss_1: (0.2753) | Acc_1: (90.33%) (44051/48768)\n",
      "Epoch: 89 | Batch_idx: 390 |  Loss_1: (0.2754) | Acc_1: (90.31%) (45157/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3015) | Acc: (90.46%) (9046/10000)\n",
      "Epoch: 90 | Batch_idx: 0 |  Loss_1: (0.1939) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 90 | Batch_idx: 10 |  Loss_1: (0.2676) | Acc_1: (90.62%) (1276/1408)\n",
      "Epoch: 90 | Batch_idx: 20 |  Loss_1: (0.2662) | Acc_1: (90.10%) (2422/2688)\n",
      "Epoch: 90 | Batch_idx: 30 |  Loss_1: (0.2847) | Acc_1: (89.42%) (3548/3968)\n",
      "Epoch: 90 | Batch_idx: 40 |  Loss_1: (0.2879) | Acc_1: (89.50%) (4697/5248)\n",
      "Epoch: 90 | Batch_idx: 50 |  Loss_1: (0.2918) | Acc_1: (89.37%) (5834/6528)\n",
      "Epoch: 90 | Batch_idx: 60 |  Loss_1: (0.2871) | Acc_1: (89.55%) (6992/7808)\n",
      "Epoch: 90 | Batch_idx: 70 |  Loss_1: (0.2871) | Acc_1: (89.54%) (8137/9088)\n",
      "Epoch: 90 | Batch_idx: 80 |  Loss_1: (0.2854) | Acc_1: (89.65%) (9295/10368)\n",
      "Epoch: 90 | Batch_idx: 90 |  Loss_1: (0.2839) | Acc_1: (89.69%) (10447/11648)\n",
      "Epoch: 90 | Batch_idx: 100 |  Loss_1: (0.2876) | Acc_1: (89.67%) (11593/12928)\n",
      "Epoch: 90 | Batch_idx: 110 |  Loss_1: (0.2876) | Acc_1: (89.72%) (12748/14208)\n",
      "Epoch: 90 | Batch_idx: 120 |  Loss_1: (0.2870) | Acc_1: (89.71%) (13895/15488)\n",
      "Epoch: 90 | Batch_idx: 130 |  Loss_1: (0.2852) | Acc_1: (89.84%) (15064/16768)\n",
      "Epoch: 90 | Batch_idx: 140 |  Loss_1: (0.2850) | Acc_1: (89.85%) (16217/18048)\n",
      "Epoch: 90 | Batch_idx: 150 |  Loss_1: (0.2826) | Acc_1: (89.89%) (17374/19328)\n",
      "Epoch: 90 | Batch_idx: 160 |  Loss_1: (0.2823) | Acc_1: (89.91%) (18528/20608)\n",
      "Epoch: 90 | Batch_idx: 170 |  Loss_1: (0.2824) | Acc_1: (89.89%) (19676/21888)\n",
      "Epoch: 90 | Batch_idx: 180 |  Loss_1: (0.2827) | Acc_1: (89.87%) (20822/23168)\n",
      "Epoch: 90 | Batch_idx: 190 |  Loss_1: (0.2828) | Acc_1: (89.91%) (21980/24448)\n",
      "Epoch: 90 | Batch_idx: 200 |  Loss_1: (0.2822) | Acc_1: (89.91%) (23133/25728)\n",
      "Epoch: 90 | Batch_idx: 210 |  Loss_1: (0.2815) | Acc_1: (89.95%) (24294/27008)\n",
      "Epoch: 90 | Batch_idx: 220 |  Loss_1: (0.2804) | Acc_1: (89.98%) (25454/28288)\n",
      "Epoch: 90 | Batch_idx: 230 |  Loss_1: (0.2802) | Acc_1: (90.02%) (26618/29568)\n",
      "Epoch: 90 | Batch_idx: 240 |  Loss_1: (0.2787) | Acc_1: (90.09%) (27790/30848)\n",
      "Epoch: 90 | Batch_idx: 250 |  Loss_1: (0.2779) | Acc_1: (90.11%) (28951/32128)\n",
      "Epoch: 90 | Batch_idx: 260 |  Loss_1: (0.2794) | Acc_1: (90.06%) (30088/33408)\n",
      "Epoch: 90 | Batch_idx: 270 |  Loss_1: (0.2786) | Acc_1: (90.06%) (31240/34688)\n",
      "Epoch: 90 | Batch_idx: 280 |  Loss_1: (0.2785) | Acc_1: (90.08%) (32400/35968)\n",
      "Epoch: 90 | Batch_idx: 290 |  Loss_1: (0.2788) | Acc_1: (90.06%) (33545/37248)\n",
      "Epoch: 90 | Batch_idx: 300 |  Loss_1: (0.2783) | Acc_1: (90.09%) (34711/38528)\n",
      "Epoch: 90 | Batch_idx: 310 |  Loss_1: (0.2791) | Acc_1: (90.06%) (35852/39808)\n",
      "Epoch: 90 | Batch_idx: 320 |  Loss_1: (0.2791) | Acc_1: (90.05%) (37000/41088)\n",
      "Epoch: 90 | Batch_idx: 330 |  Loss_1: (0.2794) | Acc_1: (90.05%) (38154/42368)\n",
      "Epoch: 90 | Batch_idx: 340 |  Loss_1: (0.2793) | Acc_1: (90.04%) (39299/43648)\n",
      "Epoch: 90 | Batch_idx: 350 |  Loss_1: (0.2793) | Acc_1: (90.06%) (40461/44928)\n",
      "Epoch: 90 | Batch_idx: 360 |  Loss_1: (0.2801) | Acc_1: (90.04%) (41607/46208)\n",
      "Epoch: 90 | Batch_idx: 370 |  Loss_1: (0.2800) | Acc_1: (90.06%) (42770/47488)\n",
      "Epoch: 90 | Batch_idx: 380 |  Loss_1: (0.2794) | Acc_1: (90.08%) (43929/48768)\n",
      "Epoch: 90 | Batch_idx: 390 |  Loss_1: (0.2798) | Acc_1: (90.06%) (45029/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3097) | Acc: (90.35%) (9035/10000)\n",
      "Epoch: 91 | Batch_idx: 0 |  Loss_1: (0.3728) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 91 | Batch_idx: 10 |  Loss_1: (0.2835) | Acc_1: (90.55%) (1275/1408)\n",
      "Epoch: 91 | Batch_idx: 20 |  Loss_1: (0.2598) | Acc_1: (91.18%) (2451/2688)\n",
      "Epoch: 91 | Batch_idx: 30 |  Loss_1: (0.2630) | Acc_1: (90.83%) (3604/3968)\n",
      "Epoch: 91 | Batch_idx: 40 |  Loss_1: (0.2626) | Acc_1: (90.89%) (4770/5248)\n",
      "Epoch: 91 | Batch_idx: 50 |  Loss_1: (0.2636) | Acc_1: (90.78%) (5926/6528)\n",
      "Epoch: 91 | Batch_idx: 60 |  Loss_1: (0.2622) | Acc_1: (90.83%) (7092/7808)\n",
      "Epoch: 91 | Batch_idx: 70 |  Loss_1: (0.2586) | Acc_1: (90.89%) (8260/9088)\n",
      "Epoch: 91 | Batch_idx: 80 |  Loss_1: (0.2587) | Acc_1: (90.86%) (9420/10368)\n",
      "Epoch: 91 | Batch_idx: 90 |  Loss_1: (0.2569) | Acc_1: (90.84%) (10581/11648)\n",
      "Epoch: 91 | Batch_idx: 100 |  Loss_1: (0.2560) | Acc_1: (90.86%) (11747/12928)\n",
      "Epoch: 91 | Batch_idx: 110 |  Loss_1: (0.2597) | Acc_1: (90.75%) (12894/14208)\n",
      "Epoch: 91 | Batch_idx: 120 |  Loss_1: (0.2620) | Acc_1: (90.65%) (14040/15488)\n",
      "Epoch: 91 | Batch_idx: 130 |  Loss_1: (0.2609) | Acc_1: (90.71%) (15211/16768)\n",
      "Epoch: 91 | Batch_idx: 140 |  Loss_1: (0.2598) | Acc_1: (90.75%) (16378/18048)\n",
      "Epoch: 91 | Batch_idx: 150 |  Loss_1: (0.2575) | Acc_1: (90.82%) (17554/19328)\n",
      "Epoch: 91 | Batch_idx: 160 |  Loss_1: (0.2583) | Acc_1: (90.79%) (18709/20608)\n",
      "Epoch: 91 | Batch_idx: 170 |  Loss_1: (0.2579) | Acc_1: (90.83%) (19880/21888)\n",
      "Epoch: 91 | Batch_idx: 180 |  Loss_1: (0.2573) | Acc_1: (90.84%) (21046/23168)\n",
      "Epoch: 91 | Batch_idx: 190 |  Loss_1: (0.2597) | Acc_1: (90.79%) (22197/24448)\n",
      "Epoch: 91 | Batch_idx: 200 |  Loss_1: (0.2611) | Acc_1: (90.74%) (23345/25728)\n",
      "Epoch: 91 | Batch_idx: 210 |  Loss_1: (0.2614) | Acc_1: (90.72%) (24502/27008)\n",
      "Epoch: 91 | Batch_idx: 220 |  Loss_1: (0.2605) | Acc_1: (90.79%) (25684/28288)\n",
      "Epoch: 91 | Batch_idx: 230 |  Loss_1: (0.2611) | Acc_1: (90.77%) (26838/29568)\n",
      "Epoch: 91 | Batch_idx: 240 |  Loss_1: (0.2601) | Acc_1: (90.79%) (28007/30848)\n",
      "Epoch: 91 | Batch_idx: 250 |  Loss_1: (0.2596) | Acc_1: (90.82%) (29179/32128)\n",
      "Epoch: 91 | Batch_idx: 260 |  Loss_1: (0.2584) | Acc_1: (90.87%) (30358/33408)\n",
      "Epoch: 91 | Batch_idx: 270 |  Loss_1: (0.2590) | Acc_1: (90.85%) (31514/34688)\n",
      "Epoch: 91 | Batch_idx: 280 |  Loss_1: (0.2589) | Acc_1: (90.88%) (32686/35968)\n",
      "Epoch: 91 | Batch_idx: 290 |  Loss_1: (0.2602) | Acc_1: (90.80%) (33822/37248)\n",
      "Epoch: 91 | Batch_idx: 300 |  Loss_1: (0.2614) | Acc_1: (90.76%) (34969/38528)\n",
      "Epoch: 91 | Batch_idx: 310 |  Loss_1: (0.2604) | Acc_1: (90.79%) (36140/39808)\n",
      "Epoch: 91 | Batch_idx: 320 |  Loss_1: (0.2609) | Acc_1: (90.78%) (37298/41088)\n",
      "Epoch: 91 | Batch_idx: 330 |  Loss_1: (0.2614) | Acc_1: (90.76%) (38455/42368)\n",
      "Epoch: 91 | Batch_idx: 340 |  Loss_1: (0.2616) | Acc_1: (90.76%) (39614/43648)\n",
      "Epoch: 91 | Batch_idx: 350 |  Loss_1: (0.2618) | Acc_1: (90.76%) (40776/44928)\n",
      "Epoch: 91 | Batch_idx: 360 |  Loss_1: (0.2623) | Acc_1: (90.75%) (41932/46208)\n",
      "Epoch: 91 | Batch_idx: 370 |  Loss_1: (0.2620) | Acc_1: (90.74%) (43092/47488)\n",
      "Epoch: 91 | Batch_idx: 380 |  Loss_1: (0.2615) | Acc_1: (90.76%) (44264/48768)\n",
      "Epoch: 91 | Batch_idx: 390 |  Loss_1: (0.2615) | Acc_1: (90.78%) (45390/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3117) | Acc: (90.17%) (9017/10000)\n",
      "Epoch: 92 | Batch_idx: 0 |  Loss_1: (0.2639) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 92 | Batch_idx: 10 |  Loss_1: (0.2489) | Acc_1: (90.98%) (1281/1408)\n",
      "Epoch: 92 | Batch_idx: 20 |  Loss_1: (0.2357) | Acc_1: (91.41%) (2457/2688)\n",
      "Epoch: 92 | Batch_idx: 30 |  Loss_1: (0.2323) | Acc_1: (91.48%) (3630/3968)\n",
      "Epoch: 92 | Batch_idx: 40 |  Loss_1: (0.2329) | Acc_1: (91.44%) (4799/5248)\n",
      "Epoch: 92 | Batch_idx: 50 |  Loss_1: (0.2445) | Acc_1: (91.16%) (5951/6528)\n",
      "Epoch: 92 | Batch_idx: 60 |  Loss_1: (0.2500) | Acc_1: (91.03%) (7108/7808)\n",
      "Epoch: 92 | Batch_idx: 70 |  Loss_1: (0.2554) | Acc_1: (90.85%) (8256/9088)\n",
      "Epoch: 92 | Batch_idx: 80 |  Loss_1: (0.2573) | Acc_1: (90.75%) (9409/10368)\n",
      "Epoch: 92 | Batch_idx: 90 |  Loss_1: (0.2607) | Acc_1: (90.55%) (10547/11648)\n",
      "Epoch: 92 | Batch_idx: 100 |  Loss_1: (0.2574) | Acc_1: (90.70%) (11726/12928)\n",
      "Epoch: 92 | Batch_idx: 110 |  Loss_1: (0.2556) | Acc_1: (90.79%) (12900/14208)\n",
      "Epoch: 92 | Batch_idx: 120 |  Loss_1: (0.2561) | Acc_1: (90.81%) (14065/15488)\n",
      "Epoch: 92 | Batch_idx: 130 |  Loss_1: (0.2588) | Acc_1: (90.71%) (15211/16768)\n",
      "Epoch: 92 | Batch_idx: 140 |  Loss_1: (0.2572) | Acc_1: (90.78%) (16384/18048)\n",
      "Epoch: 92 | Batch_idx: 150 |  Loss_1: (0.2550) | Acc_1: (90.86%) (17562/19328)\n",
      "Epoch: 92 | Batch_idx: 160 |  Loss_1: (0.2589) | Acc_1: (90.74%) (18699/20608)\n",
      "Epoch: 92 | Batch_idx: 170 |  Loss_1: (0.2583) | Acc_1: (90.74%) (19861/21888)\n",
      "Epoch: 92 | Batch_idx: 180 |  Loss_1: (0.2595) | Acc_1: (90.71%) (21016/23168)\n",
      "Epoch: 92 | Batch_idx: 190 |  Loss_1: (0.2613) | Acc_1: (90.68%) (22170/24448)\n",
      "Epoch: 92 | Batch_idx: 200 |  Loss_1: (0.2616) | Acc_1: (90.69%) (23332/25728)\n",
      "Epoch: 92 | Batch_idx: 210 |  Loss_1: (0.2635) | Acc_1: (90.59%) (24466/27008)\n",
      "Epoch: 92 | Batch_idx: 220 |  Loss_1: (0.2632) | Acc_1: (90.58%) (25624/28288)\n",
      "Epoch: 92 | Batch_idx: 230 |  Loss_1: (0.2620) | Acc_1: (90.64%) (26800/29568)\n",
      "Epoch: 92 | Batch_idx: 240 |  Loss_1: (0.2621) | Acc_1: (90.63%) (27958/30848)\n",
      "Epoch: 92 | Batch_idx: 250 |  Loss_1: (0.2628) | Acc_1: (90.63%) (29117/32128)\n",
      "Epoch: 92 | Batch_idx: 260 |  Loss_1: (0.2641) | Acc_1: (90.59%) (30264/33408)\n",
      "Epoch: 92 | Batch_idx: 270 |  Loss_1: (0.2643) | Acc_1: (90.59%) (31424/34688)\n",
      "Epoch: 92 | Batch_idx: 280 |  Loss_1: (0.2639) | Acc_1: (90.62%) (32595/35968)\n",
      "Epoch: 92 | Batch_idx: 290 |  Loss_1: (0.2653) | Acc_1: (90.58%) (33738/37248)\n",
      "Epoch: 92 | Batch_idx: 300 |  Loss_1: (0.2645) | Acc_1: (90.61%) (34912/38528)\n",
      "Epoch: 92 | Batch_idx: 310 |  Loss_1: (0.2648) | Acc_1: (90.62%) (36076/39808)\n",
      "Epoch: 92 | Batch_idx: 320 |  Loss_1: (0.2639) | Acc_1: (90.66%) (37250/41088)\n",
      "Epoch: 92 | Batch_idx: 330 |  Loss_1: (0.2630) | Acc_1: (90.69%) (38425/42368)\n",
      "Epoch: 92 | Batch_idx: 340 |  Loss_1: (0.2633) | Acc_1: (90.66%) (39573/43648)\n",
      "Epoch: 92 | Batch_idx: 350 |  Loss_1: (0.2635) | Acc_1: (90.66%) (40730/44928)\n",
      "Epoch: 92 | Batch_idx: 360 |  Loss_1: (0.2636) | Acc_1: (90.65%) (41888/46208)\n",
      "Epoch: 92 | Batch_idx: 370 |  Loss_1: (0.2635) | Acc_1: (90.65%) (43049/47488)\n",
      "Epoch: 92 | Batch_idx: 380 |  Loss_1: (0.2628) | Acc_1: (90.68%) (44221/48768)\n",
      "Epoch: 92 | Batch_idx: 390 |  Loss_1: (0.2631) | Acc_1: (90.69%) (45343/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2858) | Acc: (90.51%) (9051/10000)\n",
      "Epoch: 93 | Batch_idx: 0 |  Loss_1: (0.2141) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 93 | Batch_idx: 10 |  Loss_1: (0.2422) | Acc_1: (90.70%) (1277/1408)\n",
      "Epoch: 93 | Batch_idx: 20 |  Loss_1: (0.2592) | Acc_1: (90.25%) (2426/2688)\n",
      "Epoch: 93 | Batch_idx: 30 |  Loss_1: (0.2550) | Acc_1: (90.55%) (3593/3968)\n",
      "Epoch: 93 | Batch_idx: 40 |  Loss_1: (0.2552) | Acc_1: (90.66%) (4758/5248)\n",
      "Epoch: 93 | Batch_idx: 50 |  Loss_1: (0.2579) | Acc_1: (90.61%) (5915/6528)\n",
      "Epoch: 93 | Batch_idx: 60 |  Loss_1: (0.2609) | Acc_1: (90.62%) (7076/7808)\n",
      "Epoch: 93 | Batch_idx: 70 |  Loss_1: (0.2596) | Acc_1: (90.65%) (8238/9088)\n",
      "Epoch: 93 | Batch_idx: 80 |  Loss_1: (0.2659) | Acc_1: (90.36%) (9369/10368)\n",
      "Epoch: 93 | Batch_idx: 90 |  Loss_1: (0.2692) | Acc_1: (90.26%) (10514/11648)\n",
      "Epoch: 93 | Batch_idx: 100 |  Loss_1: (0.2660) | Acc_1: (90.42%) (11689/12928)\n",
      "Epoch: 93 | Batch_idx: 110 |  Loss_1: (0.2624) | Acc_1: (90.53%) (12863/14208)\n",
      "Epoch: 93 | Batch_idx: 120 |  Loss_1: (0.2623) | Acc_1: (90.58%) (14029/15488)\n",
      "Epoch: 93 | Batch_idx: 130 |  Loss_1: (0.2602) | Acc_1: (90.68%) (15205/16768)\n",
      "Epoch: 93 | Batch_idx: 140 |  Loss_1: (0.2590) | Acc_1: (90.70%) (16370/18048)\n",
      "Epoch: 93 | Batch_idx: 150 |  Loss_1: (0.2584) | Acc_1: (90.70%) (17531/19328)\n",
      "Epoch: 93 | Batch_idx: 160 |  Loss_1: (0.2598) | Acc_1: (90.71%) (18694/20608)\n",
      "Epoch: 93 | Batch_idx: 170 |  Loss_1: (0.2607) | Acc_1: (90.69%) (19851/21888)\n",
      "Epoch: 93 | Batch_idx: 180 |  Loss_1: (0.2601) | Acc_1: (90.70%) (21013/23168)\n",
      "Epoch: 93 | Batch_idx: 190 |  Loss_1: (0.2612) | Acc_1: (90.67%) (22166/24448)\n",
      "Epoch: 93 | Batch_idx: 200 |  Loss_1: (0.2600) | Acc_1: (90.72%) (23340/25728)\n",
      "Epoch: 93 | Batch_idx: 210 |  Loss_1: (0.2605) | Acc_1: (90.72%) (24502/27008)\n",
      "Epoch: 93 | Batch_idx: 220 |  Loss_1: (0.2603) | Acc_1: (90.74%) (25668/28288)\n",
      "Epoch: 93 | Batch_idx: 230 |  Loss_1: (0.2593) | Acc_1: (90.80%) (26848/29568)\n",
      "Epoch: 93 | Batch_idx: 240 |  Loss_1: (0.2586) | Acc_1: (90.84%) (28022/30848)\n",
      "Epoch: 93 | Batch_idx: 250 |  Loss_1: (0.2589) | Acc_1: (90.84%) (29185/32128)\n",
      "Epoch: 93 | Batch_idx: 260 |  Loss_1: (0.2584) | Acc_1: (90.86%) (30354/33408)\n",
      "Epoch: 93 | Batch_idx: 270 |  Loss_1: (0.2585) | Acc_1: (90.86%) (31518/34688)\n",
      "Epoch: 93 | Batch_idx: 280 |  Loss_1: (0.2594) | Acc_1: (90.82%) (32667/35968)\n",
      "Epoch: 93 | Batch_idx: 290 |  Loss_1: (0.2589) | Acc_1: (90.83%) (33831/37248)\n",
      "Epoch: 93 | Batch_idx: 300 |  Loss_1: (0.2584) | Acc_1: (90.85%) (35002/38528)\n",
      "Epoch: 93 | Batch_idx: 310 |  Loss_1: (0.2589) | Acc_1: (90.84%) (36162/39808)\n",
      "Epoch: 93 | Batch_idx: 320 |  Loss_1: (0.2589) | Acc_1: (90.85%) (37328/41088)\n",
      "Epoch: 93 | Batch_idx: 330 |  Loss_1: (0.2598) | Acc_1: (90.83%) (38482/42368)\n",
      "Epoch: 93 | Batch_idx: 340 |  Loss_1: (0.2599) | Acc_1: (90.81%) (39637/43648)\n",
      "Epoch: 93 | Batch_idx: 350 |  Loss_1: (0.2600) | Acc_1: (90.80%) (40796/44928)\n",
      "Epoch: 93 | Batch_idx: 360 |  Loss_1: (0.2597) | Acc_1: (90.80%) (41957/46208)\n",
      "Epoch: 93 | Batch_idx: 370 |  Loss_1: (0.2605) | Acc_1: (90.78%) (43108/47488)\n",
      "Epoch: 93 | Batch_idx: 380 |  Loss_1: (0.2610) | Acc_1: (90.75%) (44255/48768)\n",
      "Epoch: 93 | Batch_idx: 390 |  Loss_1: (0.2604) | Acc_1: (90.75%) (45375/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3006) | Acc: (90.68%) (9068/10000)\n",
      "Epoch: 94 | Batch_idx: 0 |  Loss_1: (0.3062) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 94 | Batch_idx: 10 |  Loss_1: (0.2536) | Acc_1: (90.70%) (1277/1408)\n",
      "Epoch: 94 | Batch_idx: 20 |  Loss_1: (0.2507) | Acc_1: (91.00%) (2446/2688)\n",
      "Epoch: 94 | Batch_idx: 30 |  Loss_1: (0.2511) | Acc_1: (90.93%) (3608/3968)\n",
      "Epoch: 94 | Batch_idx: 40 |  Loss_1: (0.2478) | Acc_1: (91.12%) (4782/5248)\n",
      "Epoch: 94 | Batch_idx: 50 |  Loss_1: (0.2462) | Acc_1: (91.33%) (5962/6528)\n",
      "Epoch: 94 | Batch_idx: 60 |  Loss_1: (0.2441) | Acc_1: (91.39%) (7136/7808)\n",
      "Epoch: 94 | Batch_idx: 70 |  Loss_1: (0.2450) | Acc_1: (91.26%) (8294/9088)\n",
      "Epoch: 94 | Batch_idx: 80 |  Loss_1: (0.2450) | Acc_1: (91.22%) (9458/10368)\n",
      "Epoch: 94 | Batch_idx: 90 |  Loss_1: (0.2467) | Acc_1: (91.22%) (10625/11648)\n",
      "Epoch: 94 | Batch_idx: 100 |  Loss_1: (0.2469) | Acc_1: (91.25%) (11797/12928)\n",
      "Epoch: 94 | Batch_idx: 110 |  Loss_1: (0.2486) | Acc_1: (91.27%) (12967/14208)\n",
      "Epoch: 94 | Batch_idx: 120 |  Loss_1: (0.2488) | Acc_1: (91.26%) (14135/15488)\n",
      "Epoch: 94 | Batch_idx: 130 |  Loss_1: (0.2497) | Acc_1: (91.23%) (15297/16768)\n",
      "Epoch: 94 | Batch_idx: 140 |  Loss_1: (0.2499) | Acc_1: (91.21%) (16462/18048)\n",
      "Epoch: 94 | Batch_idx: 150 |  Loss_1: (0.2507) | Acc_1: (91.21%) (17630/19328)\n",
      "Epoch: 94 | Batch_idx: 160 |  Loss_1: (0.2533) | Acc_1: (91.10%) (18773/20608)\n",
      "Epoch: 94 | Batch_idx: 170 |  Loss_1: (0.2540) | Acc_1: (91.10%) (19941/21888)\n",
      "Epoch: 94 | Batch_idx: 180 |  Loss_1: (0.2535) | Acc_1: (91.08%) (21102/23168)\n",
      "Epoch: 94 | Batch_idx: 190 |  Loss_1: (0.2524) | Acc_1: (91.10%) (22271/24448)\n",
      "Epoch: 94 | Batch_idx: 200 |  Loss_1: (0.2522) | Acc_1: (91.13%) (23445/25728)\n",
      "Epoch: 94 | Batch_idx: 210 |  Loss_1: (0.2525) | Acc_1: (91.10%) (24605/27008)\n",
      "Epoch: 94 | Batch_idx: 220 |  Loss_1: (0.2516) | Acc_1: (91.13%) (25780/28288)\n",
      "Epoch: 94 | Batch_idx: 230 |  Loss_1: (0.2502) | Acc_1: (91.19%) (26963/29568)\n",
      "Epoch: 94 | Batch_idx: 240 |  Loss_1: (0.2500) | Acc_1: (91.19%) (28130/30848)\n",
      "Epoch: 94 | Batch_idx: 250 |  Loss_1: (0.2525) | Acc_1: (91.09%) (29267/32128)\n",
      "Epoch: 94 | Batch_idx: 260 |  Loss_1: (0.2526) | Acc_1: (91.08%) (30427/33408)\n",
      "Epoch: 94 | Batch_idx: 270 |  Loss_1: (0.2529) | Acc_1: (91.08%) (31594/34688)\n",
      "Epoch: 94 | Batch_idx: 280 |  Loss_1: (0.2535) | Acc_1: (91.06%) (32754/35968)\n",
      "Epoch: 94 | Batch_idx: 290 |  Loss_1: (0.2547) | Acc_1: (91.01%) (33899/37248)\n",
      "Epoch: 94 | Batch_idx: 300 |  Loss_1: (0.2552) | Acc_1: (90.99%) (35058/38528)\n",
      "Epoch: 94 | Batch_idx: 310 |  Loss_1: (0.2565) | Acc_1: (90.93%) (36198/39808)\n",
      "Epoch: 94 | Batch_idx: 320 |  Loss_1: (0.2566) | Acc_1: (90.92%) (37356/41088)\n",
      "Epoch: 94 | Batch_idx: 330 |  Loss_1: (0.2562) | Acc_1: (90.95%) (38532/42368)\n",
      "Epoch: 94 | Batch_idx: 340 |  Loss_1: (0.2565) | Acc_1: (90.96%) (39701/43648)\n",
      "Epoch: 94 | Batch_idx: 350 |  Loss_1: (0.2580) | Acc_1: (90.91%) (40846/44928)\n",
      "Epoch: 94 | Batch_idx: 360 |  Loss_1: (0.2589) | Acc_1: (90.88%) (41993/46208)\n",
      "Epoch: 94 | Batch_idx: 370 |  Loss_1: (0.2595) | Acc_1: (90.85%) (43145/47488)\n",
      "Epoch: 94 | Batch_idx: 380 |  Loss_1: (0.2608) | Acc_1: (90.80%) (44283/48768)\n",
      "Epoch: 94 | Batch_idx: 390 |  Loss_1: (0.2608) | Acc_1: (90.80%) (45398/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3335) | Acc: (89.05%) (8905/10000)\n",
      "Epoch: 95 | Batch_idx: 0 |  Loss_1: (0.2994) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 95 | Batch_idx: 10 |  Loss_1: (0.2607) | Acc_1: (91.26%) (1285/1408)\n",
      "Epoch: 95 | Batch_idx: 20 |  Loss_1: (0.2737) | Acc_1: (90.51%) (2433/2688)\n",
      "Epoch: 95 | Batch_idx: 30 |  Loss_1: (0.2608) | Acc_1: (91.03%) (3612/3968)\n",
      "Epoch: 95 | Batch_idx: 40 |  Loss_1: (0.2585) | Acc_1: (91.08%) (4780/5248)\n",
      "Epoch: 95 | Batch_idx: 50 |  Loss_1: (0.2595) | Acc_1: (91.07%) (5945/6528)\n",
      "Epoch: 95 | Batch_idx: 60 |  Loss_1: (0.2570) | Acc_1: (91.16%) (7118/7808)\n",
      "Epoch: 95 | Batch_idx: 70 |  Loss_1: (0.2559) | Acc_1: (91.10%) (8279/9088)\n",
      "Epoch: 95 | Batch_idx: 80 |  Loss_1: (0.2532) | Acc_1: (91.14%) (9449/10368)\n",
      "Epoch: 95 | Batch_idx: 90 |  Loss_1: (0.2526) | Acc_1: (91.09%) (10610/11648)\n",
      "Epoch: 95 | Batch_idx: 100 |  Loss_1: (0.2500) | Acc_1: (91.14%) (11783/12928)\n",
      "Epoch: 95 | Batch_idx: 110 |  Loss_1: (0.2491) | Acc_1: (91.18%) (12955/14208)\n",
      "Epoch: 95 | Batch_idx: 120 |  Loss_1: (0.2475) | Acc_1: (91.17%) (14121/15488)\n",
      "Epoch: 95 | Batch_idx: 130 |  Loss_1: (0.2482) | Acc_1: (91.11%) (15278/16768)\n",
      "Epoch: 95 | Batch_idx: 140 |  Loss_1: (0.2456) | Acc_1: (91.20%) (16459/18048)\n",
      "Epoch: 95 | Batch_idx: 150 |  Loss_1: (0.2451) | Acc_1: (91.20%) (17627/19328)\n",
      "Epoch: 95 | Batch_idx: 160 |  Loss_1: (0.2469) | Acc_1: (91.22%) (18799/20608)\n",
      "Epoch: 95 | Batch_idx: 170 |  Loss_1: (0.2480) | Acc_1: (91.19%) (19959/21888)\n",
      "Epoch: 95 | Batch_idx: 180 |  Loss_1: (0.2496) | Acc_1: (91.14%) (21116/23168)\n",
      "Epoch: 95 | Batch_idx: 190 |  Loss_1: (0.2487) | Acc_1: (91.15%) (22285/24448)\n",
      "Epoch: 95 | Batch_idx: 200 |  Loss_1: (0.2470) | Acc_1: (91.18%) (23459/25728)\n",
      "Epoch: 95 | Batch_idx: 210 |  Loss_1: (0.2485) | Acc_1: (91.12%) (24611/27008)\n",
      "Epoch: 95 | Batch_idx: 220 |  Loss_1: (0.2483) | Acc_1: (91.16%) (25786/28288)\n",
      "Epoch: 95 | Batch_idx: 230 |  Loss_1: (0.2490) | Acc_1: (91.12%) (26943/29568)\n",
      "Epoch: 95 | Batch_idx: 240 |  Loss_1: (0.2479) | Acc_1: (91.16%) (28120/30848)\n",
      "Epoch: 95 | Batch_idx: 250 |  Loss_1: (0.2485) | Acc_1: (91.13%) (29278/32128)\n",
      "Epoch: 95 | Batch_idx: 260 |  Loss_1: (0.2486) | Acc_1: (91.11%) (30439/33408)\n",
      "Epoch: 95 | Batch_idx: 270 |  Loss_1: (0.2469) | Acc_1: (91.20%) (31634/34688)\n",
      "Epoch: 95 | Batch_idx: 280 |  Loss_1: (0.2460) | Acc_1: (91.23%) (32814/35968)\n",
      "Epoch: 95 | Batch_idx: 290 |  Loss_1: (0.2469) | Acc_1: (91.19%) (33967/37248)\n",
      "Epoch: 95 | Batch_idx: 300 |  Loss_1: (0.2470) | Acc_1: (91.18%) (35130/38528)\n",
      "Epoch: 95 | Batch_idx: 310 |  Loss_1: (0.2469) | Acc_1: (91.20%) (36303/39808)\n",
      "Epoch: 95 | Batch_idx: 320 |  Loss_1: (0.2466) | Acc_1: (91.23%) (37483/41088)\n",
      "Epoch: 95 | Batch_idx: 330 |  Loss_1: (0.2467) | Acc_1: (91.23%) (38652/42368)\n",
      "Epoch: 95 | Batch_idx: 340 |  Loss_1: (0.2475) | Acc_1: (91.22%) (39817/43648)\n",
      "Epoch: 95 | Batch_idx: 350 |  Loss_1: (0.2473) | Acc_1: (91.22%) (40985/44928)\n",
      "Epoch: 95 | Batch_idx: 360 |  Loss_1: (0.2478) | Acc_1: (91.20%) (42140/46208)\n",
      "Epoch: 95 | Batch_idx: 370 |  Loss_1: (0.2479) | Acc_1: (91.21%) (43316/47488)\n",
      "Epoch: 95 | Batch_idx: 380 |  Loss_1: (0.2481) | Acc_1: (91.19%) (44472/48768)\n",
      "Epoch: 95 | Batch_idx: 390 |  Loss_1: (0.2480) | Acc_1: (91.19%) (45594/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3215) | Acc: (89.92%) (8992/10000)\n",
      "Epoch: 96 | Batch_idx: 0 |  Loss_1: (0.2366) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 96 | Batch_idx: 10 |  Loss_1: (0.2470) | Acc_1: (90.91%) (1280/1408)\n",
      "Epoch: 96 | Batch_idx: 20 |  Loss_1: (0.2301) | Acc_1: (91.93%) (2471/2688)\n",
      "Epoch: 96 | Batch_idx: 30 |  Loss_1: (0.2349) | Acc_1: (91.96%) (3649/3968)\n",
      "Epoch: 96 | Batch_idx: 40 |  Loss_1: (0.2425) | Acc_1: (91.56%) (4805/5248)\n",
      "Epoch: 96 | Batch_idx: 50 |  Loss_1: (0.2447) | Acc_1: (91.44%) (5969/6528)\n",
      "Epoch: 96 | Batch_idx: 60 |  Loss_1: (0.2416) | Acc_1: (91.48%) (7143/7808)\n",
      "Epoch: 96 | Batch_idx: 70 |  Loss_1: (0.2460) | Acc_1: (91.31%) (8298/9088)\n",
      "Epoch: 96 | Batch_idx: 80 |  Loss_1: (0.2485) | Acc_1: (91.23%) (9459/10368)\n",
      "Epoch: 96 | Batch_idx: 90 |  Loss_1: (0.2488) | Acc_1: (91.12%) (10614/11648)\n",
      "Epoch: 96 | Batch_idx: 100 |  Loss_1: (0.2491) | Acc_1: (91.10%) (11777/12928)\n",
      "Epoch: 96 | Batch_idx: 110 |  Loss_1: (0.2496) | Acc_1: (91.09%) (12942/14208)\n",
      "Epoch: 96 | Batch_idx: 120 |  Loss_1: (0.2515) | Acc_1: (90.97%) (14089/15488)\n",
      "Epoch: 96 | Batch_idx: 130 |  Loss_1: (0.2507) | Acc_1: (91.00%) (15259/16768)\n",
      "Epoch: 96 | Batch_idx: 140 |  Loss_1: (0.2502) | Acc_1: (91.01%) (16425/18048)\n",
      "Epoch: 96 | Batch_idx: 150 |  Loss_1: (0.2507) | Acc_1: (91.02%) (17592/19328)\n",
      "Epoch: 96 | Batch_idx: 160 |  Loss_1: (0.2518) | Acc_1: (90.93%) (18738/20608)\n",
      "Epoch: 96 | Batch_idx: 170 |  Loss_1: (0.2520) | Acc_1: (90.93%) (19902/21888)\n",
      "Epoch: 96 | Batch_idx: 180 |  Loss_1: (0.2510) | Acc_1: (90.97%) (21075/23168)\n",
      "Epoch: 96 | Batch_idx: 190 |  Loss_1: (0.2502) | Acc_1: (91.03%) (22256/24448)\n",
      "Epoch: 96 | Batch_idx: 200 |  Loss_1: (0.2486) | Acc_1: (91.09%) (23436/25728)\n",
      "Epoch: 96 | Batch_idx: 210 |  Loss_1: (0.2491) | Acc_1: (91.04%) (24589/27008)\n",
      "Epoch: 96 | Batch_idx: 220 |  Loss_1: (0.2487) | Acc_1: (91.05%) (25756/28288)\n",
      "Epoch: 96 | Batch_idx: 230 |  Loss_1: (0.2475) | Acc_1: (91.11%) (26938/29568)\n",
      "Epoch: 96 | Batch_idx: 240 |  Loss_1: (0.2471) | Acc_1: (91.10%) (28104/30848)\n",
      "Epoch: 96 | Batch_idx: 250 |  Loss_1: (0.2481) | Acc_1: (91.11%) (29272/32128)\n",
      "Epoch: 96 | Batch_idx: 260 |  Loss_1: (0.2482) | Acc_1: (91.12%) (30440/33408)\n",
      "Epoch: 96 | Batch_idx: 270 |  Loss_1: (0.2479) | Acc_1: (91.13%) (31610/34688)\n",
      "Epoch: 96 | Batch_idx: 280 |  Loss_1: (0.2479) | Acc_1: (91.13%) (32777/35968)\n",
      "Epoch: 96 | Batch_idx: 290 |  Loss_1: (0.2495) | Acc_1: (91.07%) (33923/37248)\n",
      "Epoch: 96 | Batch_idx: 300 |  Loss_1: (0.2491) | Acc_1: (91.09%) (35094/38528)\n",
      "Epoch: 96 | Batch_idx: 310 |  Loss_1: (0.2497) | Acc_1: (91.09%) (36260/39808)\n",
      "Epoch: 96 | Batch_idx: 320 |  Loss_1: (0.2498) | Acc_1: (91.10%) (37431/41088)\n",
      "Epoch: 96 | Batch_idx: 330 |  Loss_1: (0.2497) | Acc_1: (91.11%) (38601/42368)\n",
      "Epoch: 96 | Batch_idx: 340 |  Loss_1: (0.2504) | Acc_1: (91.05%) (39743/43648)\n",
      "Epoch: 96 | Batch_idx: 350 |  Loss_1: (0.2517) | Acc_1: (90.98%) (40876/44928)\n",
      "Epoch: 96 | Batch_idx: 360 |  Loss_1: (0.2517) | Acc_1: (90.99%) (42045/46208)\n",
      "Epoch: 96 | Batch_idx: 370 |  Loss_1: (0.2514) | Acc_1: (91.01%) (43221/47488)\n",
      "Epoch: 96 | Batch_idx: 380 |  Loss_1: (0.2520) | Acc_1: (91.00%) (44381/48768)\n",
      "Epoch: 96 | Batch_idx: 390 |  Loss_1: (0.2528) | Acc_1: (90.99%) (45496/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3081) | Acc: (90.39%) (9039/10000)\n",
      "Epoch: 97 | Batch_idx: 0 |  Loss_1: (0.1041) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 97 | Batch_idx: 10 |  Loss_1: (0.2337) | Acc_1: (91.62%) (1290/1408)\n",
      "Epoch: 97 | Batch_idx: 20 |  Loss_1: (0.2452) | Acc_1: (91.07%) (2448/2688)\n",
      "Epoch: 97 | Batch_idx: 30 |  Loss_1: (0.2429) | Acc_1: (91.46%) (3629/3968)\n",
      "Epoch: 97 | Batch_idx: 40 |  Loss_1: (0.2412) | Acc_1: (91.39%) (4796/5248)\n",
      "Epoch: 97 | Batch_idx: 50 |  Loss_1: (0.2412) | Acc_1: (91.42%) (5968/6528)\n",
      "Epoch: 97 | Batch_idx: 60 |  Loss_1: (0.2376) | Acc_1: (91.61%) (7153/7808)\n",
      "Epoch: 97 | Batch_idx: 70 |  Loss_1: (0.2347) | Acc_1: (91.66%) (8330/9088)\n",
      "Epoch: 97 | Batch_idx: 80 |  Loss_1: (0.2357) | Acc_1: (91.62%) (9499/10368)\n",
      "Epoch: 97 | Batch_idx: 90 |  Loss_1: (0.2359) | Acc_1: (91.62%) (10672/11648)\n",
      "Epoch: 97 | Batch_idx: 100 |  Loss_1: (0.2336) | Acc_1: (91.71%) (11856/12928)\n",
      "Epoch: 97 | Batch_idx: 110 |  Loss_1: (0.2332) | Acc_1: (91.69%) (13028/14208)\n",
      "Epoch: 97 | Batch_idx: 120 |  Loss_1: (0.2359) | Acc_1: (91.60%) (14187/15488)\n",
      "Epoch: 97 | Batch_idx: 130 |  Loss_1: (0.2345) | Acc_1: (91.69%) (15375/16768)\n",
      "Epoch: 97 | Batch_idx: 140 |  Loss_1: (0.2357) | Acc_1: (91.62%) (16535/18048)\n",
      "Epoch: 97 | Batch_idx: 150 |  Loss_1: (0.2376) | Acc_1: (91.55%) (17694/19328)\n",
      "Epoch: 97 | Batch_idx: 160 |  Loss_1: (0.2384) | Acc_1: (91.53%) (18863/20608)\n",
      "Epoch: 97 | Batch_idx: 170 |  Loss_1: (0.2385) | Acc_1: (91.54%) (20037/21888)\n",
      "Epoch: 97 | Batch_idx: 180 |  Loss_1: (0.2418) | Acc_1: (91.44%) (21185/23168)\n",
      "Epoch: 97 | Batch_idx: 190 |  Loss_1: (0.2429) | Acc_1: (91.39%) (22344/24448)\n",
      "Epoch: 97 | Batch_idx: 200 |  Loss_1: (0.2437) | Acc_1: (91.39%) (23513/25728)\n",
      "Epoch: 97 | Batch_idx: 210 |  Loss_1: (0.2441) | Acc_1: (91.38%) (24680/27008)\n",
      "Epoch: 97 | Batch_idx: 220 |  Loss_1: (0.2451) | Acc_1: (91.34%) (25838/28288)\n",
      "Epoch: 97 | Batch_idx: 230 |  Loss_1: (0.2444) | Acc_1: (91.34%) (27008/29568)\n",
      "Epoch: 97 | Batch_idx: 240 |  Loss_1: (0.2432) | Acc_1: (91.40%) (28195/30848)\n",
      "Epoch: 97 | Batch_idx: 250 |  Loss_1: (0.2426) | Acc_1: (91.39%) (29363/32128)\n",
      "Epoch: 97 | Batch_idx: 260 |  Loss_1: (0.2435) | Acc_1: (91.38%) (30528/33408)\n",
      "Epoch: 97 | Batch_idx: 270 |  Loss_1: (0.2443) | Acc_1: (91.36%) (31691/34688)\n",
      "Epoch: 97 | Batch_idx: 280 |  Loss_1: (0.2448) | Acc_1: (91.32%) (32845/35968)\n",
      "Epoch: 97 | Batch_idx: 290 |  Loss_1: (0.2453) | Acc_1: (91.29%) (34005/37248)\n",
      "Epoch: 97 | Batch_idx: 300 |  Loss_1: (0.2457) | Acc_1: (91.28%) (35169/38528)\n",
      "Epoch: 97 | Batch_idx: 310 |  Loss_1: (0.2463) | Acc_1: (91.26%) (36328/39808)\n",
      "Epoch: 97 | Batch_idx: 320 |  Loss_1: (0.2466) | Acc_1: (91.24%) (37490/41088)\n",
      "Epoch: 97 | Batch_idx: 330 |  Loss_1: (0.2465) | Acc_1: (91.25%) (38661/42368)\n",
      "Epoch: 97 | Batch_idx: 340 |  Loss_1: (0.2466) | Acc_1: (91.23%) (39818/43648)\n",
      "Epoch: 97 | Batch_idx: 350 |  Loss_1: (0.2462) | Acc_1: (91.25%) (40996/44928)\n",
      "Epoch: 97 | Batch_idx: 360 |  Loss_1: (0.2460) | Acc_1: (91.27%) (42175/46208)\n",
      "Epoch: 97 | Batch_idx: 370 |  Loss_1: (0.2466) | Acc_1: (91.25%) (43332/47488)\n",
      "Epoch: 97 | Batch_idx: 380 |  Loss_1: (0.2471) | Acc_1: (91.22%) (44488/48768)\n",
      "Epoch: 97 | Batch_idx: 390 |  Loss_1: (0.2481) | Acc_1: (91.19%) (45593/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3140) | Acc: (90.06%) (9006/10000)\n",
      "Epoch: 98 | Batch_idx: 0 |  Loss_1: (0.1011) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 98 | Batch_idx: 10 |  Loss_1: (0.2298) | Acc_1: (91.76%) (1292/1408)\n",
      "Epoch: 98 | Batch_idx: 20 |  Loss_1: (0.2518) | Acc_1: (91.11%) (2449/2688)\n",
      "Epoch: 98 | Batch_idx: 30 |  Loss_1: (0.2522) | Acc_1: (91.28%) (3622/3968)\n",
      "Epoch: 98 | Batch_idx: 40 |  Loss_1: (0.2571) | Acc_1: (90.89%) (4770/5248)\n",
      "Epoch: 98 | Batch_idx: 50 |  Loss_1: (0.2511) | Acc_1: (91.12%) (5948/6528)\n",
      "Epoch: 98 | Batch_idx: 60 |  Loss_1: (0.2557) | Acc_1: (90.88%) (7096/7808)\n",
      "Epoch: 98 | Batch_idx: 70 |  Loss_1: (0.2511) | Acc_1: (91.04%) (8274/9088)\n",
      "Epoch: 98 | Batch_idx: 80 |  Loss_1: (0.2464) | Acc_1: (91.20%) (9456/10368)\n",
      "Epoch: 98 | Batch_idx: 90 |  Loss_1: (0.2429) | Acc_1: (91.32%) (10637/11648)\n",
      "Epoch: 98 | Batch_idx: 100 |  Loss_1: (0.2430) | Acc_1: (91.37%) (11812/12928)\n",
      "Epoch: 98 | Batch_idx: 110 |  Loss_1: (0.2422) | Acc_1: (91.40%) (12986/14208)\n",
      "Epoch: 98 | Batch_idx: 120 |  Loss_1: (0.2460) | Acc_1: (91.29%) (14139/15488)\n",
      "Epoch: 98 | Batch_idx: 130 |  Loss_1: (0.2429) | Acc_1: (91.41%) (15327/16768)\n",
      "Epoch: 98 | Batch_idx: 140 |  Loss_1: (0.2474) | Acc_1: (91.27%) (16473/18048)\n",
      "Epoch: 98 | Batch_idx: 150 |  Loss_1: (0.2480) | Acc_1: (91.23%) (17632/19328)\n",
      "Epoch: 98 | Batch_idx: 160 |  Loss_1: (0.2489) | Acc_1: (91.18%) (18790/20608)\n",
      "Epoch: 98 | Batch_idx: 170 |  Loss_1: (0.2478) | Acc_1: (91.23%) (19969/21888)\n",
      "Epoch: 98 | Batch_idx: 180 |  Loss_1: (0.2492) | Acc_1: (91.20%) (21129/23168)\n",
      "Epoch: 98 | Batch_idx: 190 |  Loss_1: (0.2494) | Acc_1: (91.15%) (22285/24448)\n",
      "Epoch: 98 | Batch_idx: 200 |  Loss_1: (0.2495) | Acc_1: (91.13%) (23445/25728)\n",
      "Epoch: 98 | Batch_idx: 210 |  Loss_1: (0.2489) | Acc_1: (91.15%) (24618/27008)\n",
      "Epoch: 98 | Batch_idx: 220 |  Loss_1: (0.2495) | Acc_1: (91.14%) (25782/28288)\n",
      "Epoch: 98 | Batch_idx: 230 |  Loss_1: (0.2487) | Acc_1: (91.20%) (26967/29568)\n",
      "Epoch: 98 | Batch_idx: 240 |  Loss_1: (0.2493) | Acc_1: (91.19%) (28131/30848)\n",
      "Epoch: 98 | Batch_idx: 250 |  Loss_1: (0.2491) | Acc_1: (91.17%) (29290/32128)\n",
      "Epoch: 98 | Batch_idx: 260 |  Loss_1: (0.2484) | Acc_1: (91.18%) (30462/33408)\n",
      "Epoch: 98 | Batch_idx: 270 |  Loss_1: (0.2479) | Acc_1: (91.19%) (31633/34688)\n",
      "Epoch: 98 | Batch_idx: 280 |  Loss_1: (0.2469) | Acc_1: (91.24%) (32818/35968)\n",
      "Epoch: 98 | Batch_idx: 290 |  Loss_1: (0.2465) | Acc_1: (91.25%) (33989/37248)\n",
      "Epoch: 98 | Batch_idx: 300 |  Loss_1: (0.2461) | Acc_1: (91.26%) (35159/38528)\n",
      "Epoch: 98 | Batch_idx: 310 |  Loss_1: (0.2464) | Acc_1: (91.26%) (36327/39808)\n",
      "Epoch: 98 | Batch_idx: 320 |  Loss_1: (0.2471) | Acc_1: (91.22%) (37481/41088)\n",
      "Epoch: 98 | Batch_idx: 330 |  Loss_1: (0.2468) | Acc_1: (91.23%) (38653/42368)\n",
      "Epoch: 98 | Batch_idx: 340 |  Loss_1: (0.2458) | Acc_1: (91.24%) (39825/43648)\n",
      "Epoch: 98 | Batch_idx: 350 |  Loss_1: (0.2460) | Acc_1: (91.22%) (40985/44928)\n",
      "Epoch: 98 | Batch_idx: 360 |  Loss_1: (0.2452) | Acc_1: (91.25%) (42167/46208)\n",
      "Epoch: 98 | Batch_idx: 370 |  Loss_1: (0.2458) | Acc_1: (91.23%) (43321/47488)\n",
      "Epoch: 98 | Batch_idx: 380 |  Loss_1: (0.2452) | Acc_1: (91.24%) (44498/48768)\n",
      "Epoch: 98 | Batch_idx: 390 |  Loss_1: (0.2449) | Acc_1: (91.24%) (45618/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3032) | Acc: (90.72%) (9072/10000)\n",
      "Epoch: 99 | Batch_idx: 0 |  Loss_1: (0.2199) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 99 | Batch_idx: 10 |  Loss_1: (0.2239) | Acc_1: (91.69%) (1291/1408)\n",
      "Epoch: 99 | Batch_idx: 20 |  Loss_1: (0.2171) | Acc_1: (92.08%) (2475/2688)\n",
      "Epoch: 99 | Batch_idx: 30 |  Loss_1: (0.2123) | Acc_1: (92.29%) (3662/3968)\n",
      "Epoch: 99 | Batch_idx: 40 |  Loss_1: (0.2169) | Acc_1: (92.24%) (4841/5248)\n",
      "Epoch: 99 | Batch_idx: 50 |  Loss_1: (0.2208) | Acc_1: (92.05%) (6009/6528)\n",
      "Epoch: 99 | Batch_idx: 60 |  Loss_1: (0.2205) | Acc_1: (92.16%) (7196/7808)\n",
      "Epoch: 99 | Batch_idx: 70 |  Loss_1: (0.2244) | Acc_1: (92.00%) (8361/9088)\n",
      "Epoch: 99 | Batch_idx: 80 |  Loss_1: (0.2240) | Acc_1: (92.03%) (9542/10368)\n",
      "Epoch: 99 | Batch_idx: 90 |  Loss_1: (0.2289) | Acc_1: (91.96%) (10712/11648)\n",
      "Epoch: 99 | Batch_idx: 100 |  Loss_1: (0.2283) | Acc_1: (91.91%) (11882/12928)\n",
      "Epoch: 99 | Batch_idx: 110 |  Loss_1: (0.2274) | Acc_1: (91.91%) (13059/14208)\n",
      "Epoch: 99 | Batch_idx: 120 |  Loss_1: (0.2274) | Acc_1: (91.90%) (14234/15488)\n",
      "Epoch: 99 | Batch_idx: 130 |  Loss_1: (0.2276) | Acc_1: (91.85%) (15402/16768)\n",
      "Epoch: 99 | Batch_idx: 140 |  Loss_1: (0.2296) | Acc_1: (91.77%) (16562/18048)\n",
      "Epoch: 99 | Batch_idx: 150 |  Loss_1: (0.2309) | Acc_1: (91.77%) (17738/19328)\n",
      "Epoch: 99 | Batch_idx: 160 |  Loss_1: (0.2328) | Acc_1: (91.67%) (18891/20608)\n",
      "Epoch: 99 | Batch_idx: 170 |  Loss_1: (0.2329) | Acc_1: (91.65%) (20061/21888)\n",
      "Epoch: 99 | Batch_idx: 180 |  Loss_1: (0.2333) | Acc_1: (91.60%) (21223/23168)\n",
      "Epoch: 99 | Batch_idx: 190 |  Loss_1: (0.2352) | Acc_1: (91.52%) (22376/24448)\n",
      "Epoch: 99 | Batch_idx: 200 |  Loss_1: (0.2360) | Acc_1: (91.49%) (23539/25728)\n",
      "Epoch: 99 | Batch_idx: 210 |  Loss_1: (0.2376) | Acc_1: (91.45%) (24699/27008)\n",
      "Epoch: 99 | Batch_idx: 220 |  Loss_1: (0.2371) | Acc_1: (91.47%) (25876/28288)\n",
      "Epoch: 99 | Batch_idx: 230 |  Loss_1: (0.2374) | Acc_1: (91.47%) (27047/29568)\n",
      "Epoch: 99 | Batch_idx: 240 |  Loss_1: (0.2380) | Acc_1: (91.46%) (28215/30848)\n",
      "Epoch: 99 | Batch_idx: 250 |  Loss_1: (0.2384) | Acc_1: (91.47%) (29387/32128)\n",
      "Epoch: 99 | Batch_idx: 260 |  Loss_1: (0.2384) | Acc_1: (91.45%) (30553/33408)\n",
      "Epoch: 99 | Batch_idx: 270 |  Loss_1: (0.2389) | Acc_1: (91.44%) (31718/34688)\n",
      "Epoch: 99 | Batch_idx: 280 |  Loss_1: (0.2382) | Acc_1: (91.46%) (32897/35968)\n",
      "Epoch: 99 | Batch_idx: 290 |  Loss_1: (0.2383) | Acc_1: (91.48%) (34074/37248)\n",
      "Epoch: 99 | Batch_idx: 300 |  Loss_1: (0.2384) | Acc_1: (91.47%) (35241/38528)\n",
      "Epoch: 99 | Batch_idx: 310 |  Loss_1: (0.2399) | Acc_1: (91.41%) (36390/39808)\n",
      "Epoch: 99 | Batch_idx: 320 |  Loss_1: (0.2391) | Acc_1: (91.44%) (37570/41088)\n",
      "Epoch: 99 | Batch_idx: 330 |  Loss_1: (0.2396) | Acc_1: (91.42%) (38731/42368)\n",
      "Epoch: 99 | Batch_idx: 340 |  Loss_1: (0.2407) | Acc_1: (91.37%) (39880/43648)\n",
      "Epoch: 99 | Batch_idx: 350 |  Loss_1: (0.2410) | Acc_1: (91.36%) (41046/44928)\n",
      "Epoch: 99 | Batch_idx: 360 |  Loss_1: (0.2418) | Acc_1: (91.32%) (42197/46208)\n",
      "Epoch: 99 | Batch_idx: 370 |  Loss_1: (0.2421) | Acc_1: (91.30%) (43357/47488)\n",
      "Epoch: 99 | Batch_idx: 380 |  Loss_1: (0.2418) | Acc_1: (91.32%) (44533/48768)\n",
      "Epoch: 99 | Batch_idx: 390 |  Loss_1: (0.2418) | Acc_1: (91.30%) (45652/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3115) | Acc: (90.66%) (9066/10000)\n",
      "Epoch: 100 | Batch_idx: 0 |  Loss_1: (0.2289) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 100 | Batch_idx: 10 |  Loss_1: (0.2392) | Acc_1: (91.26%) (1285/1408)\n",
      "Epoch: 100 | Batch_idx: 20 |  Loss_1: (0.2397) | Acc_1: (91.52%) (2460/2688)\n",
      "Epoch: 100 | Batch_idx: 30 |  Loss_1: (0.2397) | Acc_1: (91.28%) (3622/3968)\n",
      "Epoch: 100 | Batch_idx: 40 |  Loss_1: (0.2469) | Acc_1: (91.10%) (4781/5248)\n",
      "Epoch: 100 | Batch_idx: 50 |  Loss_1: (0.2464) | Acc_1: (91.12%) (5948/6528)\n",
      "Epoch: 100 | Batch_idx: 60 |  Loss_1: (0.2430) | Acc_1: (91.38%) (7135/7808)\n",
      "Epoch: 100 | Batch_idx: 70 |  Loss_1: (0.2422) | Acc_1: (91.52%) (8317/9088)\n",
      "Epoch: 100 | Batch_idx: 80 |  Loss_1: (0.2356) | Acc_1: (91.71%) (9509/10368)\n",
      "Epoch: 100 | Batch_idx: 90 |  Loss_1: (0.2355) | Acc_1: (91.68%) (10679/11648)\n",
      "Epoch: 100 | Batch_idx: 100 |  Loss_1: (0.2355) | Acc_1: (91.68%) (11853/12928)\n",
      "Epoch: 100 | Batch_idx: 110 |  Loss_1: (0.2355) | Acc_1: (91.70%) (13029/14208)\n",
      "Epoch: 100 | Batch_idx: 120 |  Loss_1: (0.2354) | Acc_1: (91.68%) (14199/15488)\n",
      "Epoch: 100 | Batch_idx: 130 |  Loss_1: (0.2352) | Acc_1: (91.69%) (15374/16768)\n",
      "Epoch: 100 | Batch_idx: 140 |  Loss_1: (0.2339) | Acc_1: (91.72%) (16553/18048)\n",
      "Epoch: 100 | Batch_idx: 150 |  Loss_1: (0.2354) | Acc_1: (91.66%) (17717/19328)\n",
      "Epoch: 100 | Batch_idx: 160 |  Loss_1: (0.2348) | Acc_1: (91.67%) (18892/20608)\n",
      "Epoch: 100 | Batch_idx: 170 |  Loss_1: (0.2360) | Acc_1: (91.63%) (20057/21888)\n",
      "Epoch: 100 | Batch_idx: 180 |  Loss_1: (0.2354) | Acc_1: (91.63%) (21229/23168)\n",
      "Epoch: 100 | Batch_idx: 190 |  Loss_1: (0.2378) | Acc_1: (91.55%) (22383/24448)\n",
      "Epoch: 100 | Batch_idx: 200 |  Loss_1: (0.2381) | Acc_1: (91.59%) (23565/25728)\n",
      "Epoch: 100 | Batch_idx: 210 |  Loss_1: (0.2377) | Acc_1: (91.59%) (24736/27008)\n",
      "Epoch: 100 | Batch_idx: 220 |  Loss_1: (0.2363) | Acc_1: (91.65%) (25926/28288)\n",
      "Epoch: 100 | Batch_idx: 230 |  Loss_1: (0.2370) | Acc_1: (91.63%) (27092/29568)\n",
      "Epoch: 100 | Batch_idx: 240 |  Loss_1: (0.2357) | Acc_1: (91.70%) (28287/30848)\n",
      "Epoch: 100 | Batch_idx: 250 |  Loss_1: (0.2354) | Acc_1: (91.68%) (29456/32128)\n",
      "Epoch: 100 | Batch_idx: 260 |  Loss_1: (0.2356) | Acc_1: (91.70%) (30636/33408)\n",
      "Epoch: 100 | Batch_idx: 270 |  Loss_1: (0.2363) | Acc_1: (91.69%) (31804/34688)\n",
      "Epoch: 100 | Batch_idx: 280 |  Loss_1: (0.2361) | Acc_1: (91.68%) (32975/35968)\n",
      "Epoch: 100 | Batch_idx: 290 |  Loss_1: (0.2358) | Acc_1: (91.67%) (34147/37248)\n",
      "Epoch: 100 | Batch_idx: 300 |  Loss_1: (0.2353) | Acc_1: (91.67%) (35319/38528)\n",
      "Epoch: 100 | Batch_idx: 310 |  Loss_1: (0.2355) | Acc_1: (91.65%) (36486/39808)\n",
      "Epoch: 100 | Batch_idx: 320 |  Loss_1: (0.2358) | Acc_1: (91.63%) (37650/41088)\n",
      "Epoch: 100 | Batch_idx: 330 |  Loss_1: (0.2363) | Acc_1: (91.61%) (38812/42368)\n",
      "Epoch: 100 | Batch_idx: 340 |  Loss_1: (0.2371) | Acc_1: (91.59%) (39979/43648)\n",
      "Epoch: 100 | Batch_idx: 350 |  Loss_1: (0.2368) | Acc_1: (91.58%) (41146/44928)\n",
      "Epoch: 100 | Batch_idx: 360 |  Loss_1: (0.2366) | Acc_1: (91.61%) (42330/46208)\n",
      "Epoch: 100 | Batch_idx: 370 |  Loss_1: (0.2359) | Acc_1: (91.63%) (43512/47488)\n",
      "Epoch: 100 | Batch_idx: 380 |  Loss_1: (0.2362) | Acc_1: (91.61%) (44676/48768)\n",
      "Epoch: 100 | Batch_idx: 390 |  Loss_1: (0.2375) | Acc_1: (91.56%) (45780/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2969) | Acc: (90.94%) (9094/10000)\n",
      "Epoch: 101 | Batch_idx: 0 |  Loss_1: (0.2089) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 101 | Batch_idx: 10 |  Loss_1: (0.2182) | Acc_1: (92.19%) (1298/1408)\n",
      "Epoch: 101 | Batch_idx: 20 |  Loss_1: (0.2106) | Acc_1: (92.52%) (2487/2688)\n",
      "Epoch: 101 | Batch_idx: 30 |  Loss_1: (0.2187) | Acc_1: (92.14%) (3656/3968)\n",
      "Epoch: 101 | Batch_idx: 40 |  Loss_1: (0.2273) | Acc_1: (91.73%) (4814/5248)\n",
      "Epoch: 101 | Batch_idx: 50 |  Loss_1: (0.2289) | Acc_1: (91.68%) (5985/6528)\n",
      "Epoch: 101 | Batch_idx: 60 |  Loss_1: (0.2267) | Acc_1: (91.75%) (7164/7808)\n",
      "Epoch: 101 | Batch_idx: 70 |  Loss_1: (0.2257) | Acc_1: (91.79%) (8342/9088)\n",
      "Epoch: 101 | Batch_idx: 80 |  Loss_1: (0.2285) | Acc_1: (91.62%) (9499/10368)\n",
      "Epoch: 101 | Batch_idx: 90 |  Loss_1: (0.2262) | Acc_1: (91.68%) (10679/11648)\n",
      "Epoch: 101 | Batch_idx: 100 |  Loss_1: (0.2286) | Acc_1: (91.68%) (11852/12928)\n",
      "Epoch: 101 | Batch_idx: 110 |  Loss_1: (0.2276) | Acc_1: (91.65%) (13021/14208)\n",
      "Epoch: 101 | Batch_idx: 120 |  Loss_1: (0.2267) | Acc_1: (91.72%) (14205/15488)\n",
      "Epoch: 101 | Batch_idx: 130 |  Loss_1: (0.2302) | Acc_1: (91.60%) (15360/16768)\n",
      "Epoch: 101 | Batch_idx: 140 |  Loss_1: (0.2341) | Acc_1: (91.46%) (16506/18048)\n",
      "Epoch: 101 | Batch_idx: 150 |  Loss_1: (0.2347) | Acc_1: (91.43%) (17672/19328)\n",
      "Epoch: 101 | Batch_idx: 160 |  Loss_1: (0.2326) | Acc_1: (91.54%) (18865/20608)\n",
      "Epoch: 101 | Batch_idx: 170 |  Loss_1: (0.2344) | Acc_1: (91.46%) (20018/21888)\n",
      "Epoch: 101 | Batch_idx: 180 |  Loss_1: (0.2343) | Acc_1: (91.51%) (21201/23168)\n",
      "Epoch: 101 | Batch_idx: 190 |  Loss_1: (0.2331) | Acc_1: (91.57%) (22386/24448)\n",
      "Epoch: 101 | Batch_idx: 200 |  Loss_1: (0.2329) | Acc_1: (91.57%) (23559/25728)\n",
      "Epoch: 101 | Batch_idx: 210 |  Loss_1: (0.2311) | Acc_1: (91.67%) (24757/27008)\n",
      "Epoch: 101 | Batch_idx: 220 |  Loss_1: (0.2317) | Acc_1: (91.69%) (25937/28288)\n",
      "Epoch: 101 | Batch_idx: 230 |  Loss_1: (0.2318) | Acc_1: (91.69%) (27110/29568)\n",
      "Epoch: 101 | Batch_idx: 240 |  Loss_1: (0.2312) | Acc_1: (91.70%) (28287/30848)\n",
      "Epoch: 101 | Batch_idx: 250 |  Loss_1: (0.2305) | Acc_1: (91.72%) (29469/32128)\n",
      "Epoch: 101 | Batch_idx: 260 |  Loss_1: (0.2307) | Acc_1: (91.73%) (30645/33408)\n",
      "Epoch: 101 | Batch_idx: 270 |  Loss_1: (0.2305) | Acc_1: (91.74%) (31824/34688)\n",
      "Epoch: 101 | Batch_idx: 280 |  Loss_1: (0.2303) | Acc_1: (91.75%) (33001/35968)\n",
      "Epoch: 101 | Batch_idx: 290 |  Loss_1: (0.2307) | Acc_1: (91.76%) (34180/37248)\n",
      "Epoch: 101 | Batch_idx: 300 |  Loss_1: (0.2306) | Acc_1: (91.73%) (35343/38528)\n",
      "Epoch: 101 | Batch_idx: 310 |  Loss_1: (0.2313) | Acc_1: (91.70%) (36505/39808)\n",
      "Epoch: 101 | Batch_idx: 320 |  Loss_1: (0.2315) | Acc_1: (91.70%) (37678/41088)\n",
      "Epoch: 101 | Batch_idx: 330 |  Loss_1: (0.2310) | Acc_1: (91.73%) (38864/42368)\n",
      "Epoch: 101 | Batch_idx: 340 |  Loss_1: (0.2310) | Acc_1: (91.72%) (40034/43648)\n",
      "Epoch: 101 | Batch_idx: 350 |  Loss_1: (0.2307) | Acc_1: (91.73%) (41212/44928)\n",
      "Epoch: 101 | Batch_idx: 360 |  Loss_1: (0.2312) | Acc_1: (91.70%) (42371/46208)\n",
      "Epoch: 101 | Batch_idx: 370 |  Loss_1: (0.2319) | Acc_1: (91.68%) (43539/47488)\n",
      "Epoch: 101 | Batch_idx: 380 |  Loss_1: (0.2326) | Acc_1: (91.67%) (44707/48768)\n",
      "Epoch: 101 | Batch_idx: 390 |  Loss_1: (0.2332) | Acc_1: (91.66%) (45832/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3160) | Acc: (90.56%) (9056/10000)\n",
      "Epoch: 102 | Batch_idx: 0 |  Loss_1: (0.2536) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 102 | Batch_idx: 10 |  Loss_1: (0.2453) | Acc_1: (91.12%) (1283/1408)\n",
      "Epoch: 102 | Batch_idx: 20 |  Loss_1: (0.2329) | Acc_1: (91.41%) (2457/2688)\n",
      "Epoch: 102 | Batch_idx: 30 |  Loss_1: (0.2259) | Acc_1: (91.58%) (3634/3968)\n",
      "Epoch: 102 | Batch_idx: 40 |  Loss_1: (0.2289) | Acc_1: (91.54%) (4804/5248)\n",
      "Epoch: 102 | Batch_idx: 50 |  Loss_1: (0.2297) | Acc_1: (91.62%) (5981/6528)\n",
      "Epoch: 102 | Batch_idx: 60 |  Loss_1: (0.2317) | Acc_1: (91.73%) (7162/7808)\n",
      "Epoch: 102 | Batch_idx: 70 |  Loss_1: (0.2279) | Acc_1: (91.86%) (8348/9088)\n",
      "Epoch: 102 | Batch_idx: 80 |  Loss_1: (0.2281) | Acc_1: (91.92%) (9530/10368)\n",
      "Epoch: 102 | Batch_idx: 90 |  Loss_1: (0.2300) | Acc_1: (91.84%) (10697/11648)\n",
      "Epoch: 102 | Batch_idx: 100 |  Loss_1: (0.2288) | Acc_1: (91.89%) (11880/12928)\n",
      "Epoch: 102 | Batch_idx: 110 |  Loss_1: (0.2276) | Acc_1: (91.90%) (13057/14208)\n",
      "Epoch: 102 | Batch_idx: 120 |  Loss_1: (0.2259) | Acc_1: (91.97%) (14245/15488)\n",
      "Epoch: 102 | Batch_idx: 130 |  Loss_1: (0.2292) | Acc_1: (91.88%) (15407/16768)\n",
      "Epoch: 102 | Batch_idx: 140 |  Loss_1: (0.2299) | Acc_1: (91.84%) (16576/18048)\n",
      "Epoch: 102 | Batch_idx: 150 |  Loss_1: (0.2293) | Acc_1: (91.88%) (17758/19328)\n",
      "Epoch: 102 | Batch_idx: 160 |  Loss_1: (0.2293) | Acc_1: (91.91%) (18941/20608)\n",
      "Epoch: 102 | Batch_idx: 170 |  Loss_1: (0.2296) | Acc_1: (91.94%) (20123/21888)\n",
      "Epoch: 102 | Batch_idx: 180 |  Loss_1: (0.2276) | Acc_1: (91.99%) (21313/23168)\n",
      "Epoch: 102 | Batch_idx: 190 |  Loss_1: (0.2299) | Acc_1: (91.96%) (22482/24448)\n",
      "Epoch: 102 | Batch_idx: 200 |  Loss_1: (0.2299) | Acc_1: (91.98%) (23665/25728)\n",
      "Epoch: 102 | Batch_idx: 210 |  Loss_1: (0.2286) | Acc_1: (92.03%) (24856/27008)\n",
      "Epoch: 102 | Batch_idx: 220 |  Loss_1: (0.2292) | Acc_1: (91.98%) (26020/28288)\n",
      "Epoch: 102 | Batch_idx: 230 |  Loss_1: (0.2282) | Acc_1: (92.00%) (27204/29568)\n",
      "Epoch: 102 | Batch_idx: 240 |  Loss_1: (0.2273) | Acc_1: (92.03%) (28389/30848)\n",
      "Epoch: 102 | Batch_idx: 250 |  Loss_1: (0.2273) | Acc_1: (92.02%) (29564/32128)\n",
      "Epoch: 102 | Batch_idx: 260 |  Loss_1: (0.2288) | Acc_1: (91.98%) (30729/33408)\n",
      "Epoch: 102 | Batch_idx: 270 |  Loss_1: (0.2284) | Acc_1: (92.01%) (31917/34688)\n",
      "Epoch: 102 | Batch_idx: 280 |  Loss_1: (0.2285) | Acc_1: (92.02%) (33096/35968)\n",
      "Epoch: 102 | Batch_idx: 290 |  Loss_1: (0.2285) | Acc_1: (92.01%) (34273/37248)\n",
      "Epoch: 102 | Batch_idx: 300 |  Loss_1: (0.2284) | Acc_1: (92.02%) (35453/38528)\n",
      "Epoch: 102 | Batch_idx: 310 |  Loss_1: (0.2297) | Acc_1: (91.97%) (36611/39808)\n",
      "Epoch: 102 | Batch_idx: 320 |  Loss_1: (0.2296) | Acc_1: (91.94%) (37778/41088)\n",
      "Epoch: 102 | Batch_idx: 330 |  Loss_1: (0.2293) | Acc_1: (91.93%) (38950/42368)\n",
      "Epoch: 102 | Batch_idx: 340 |  Loss_1: (0.2297) | Acc_1: (91.90%) (40113/43648)\n",
      "Epoch: 102 | Batch_idx: 350 |  Loss_1: (0.2288) | Acc_1: (91.93%) (41302/44928)\n",
      "Epoch: 102 | Batch_idx: 360 |  Loss_1: (0.2291) | Acc_1: (91.90%) (42467/46208)\n",
      "Epoch: 102 | Batch_idx: 370 |  Loss_1: (0.2296) | Acc_1: (91.87%) (43629/47488)\n",
      "Epoch: 102 | Batch_idx: 380 |  Loss_1: (0.2299) | Acc_1: (91.86%) (44798/48768)\n",
      "Epoch: 102 | Batch_idx: 390 |  Loss_1: (0.2303) | Acc_1: (91.85%) (45927/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3006) | Acc: (90.63%) (9063/10000)\n",
      "Epoch: 103 | Batch_idx: 0 |  Loss_1: (0.1562) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 103 | Batch_idx: 10 |  Loss_1: (0.2459) | Acc_1: (91.26%) (1285/1408)\n",
      "Epoch: 103 | Batch_idx: 20 |  Loss_1: (0.2234) | Acc_1: (92.00%) (2473/2688)\n",
      "Epoch: 103 | Batch_idx: 30 |  Loss_1: (0.2213) | Acc_1: (92.04%) (3652/3968)\n",
      "Epoch: 103 | Batch_idx: 40 |  Loss_1: (0.2189) | Acc_1: (92.02%) (4829/5248)\n",
      "Epoch: 103 | Batch_idx: 50 |  Loss_1: (0.2247) | Acc_1: (91.74%) (5989/6528)\n",
      "Epoch: 103 | Batch_idx: 60 |  Loss_1: (0.2234) | Acc_1: (91.85%) (7172/7808)\n",
      "Epoch: 103 | Batch_idx: 70 |  Loss_1: (0.2213) | Acc_1: (91.98%) (8359/9088)\n",
      "Epoch: 103 | Batch_idx: 80 |  Loss_1: (0.2232) | Acc_1: (91.94%) (9532/10368)\n",
      "Epoch: 103 | Batch_idx: 90 |  Loss_1: (0.2260) | Acc_1: (91.85%) (10699/11648)\n",
      "Epoch: 103 | Batch_idx: 100 |  Loss_1: (0.2256) | Acc_1: (91.88%) (11878/12928)\n",
      "Epoch: 103 | Batch_idx: 110 |  Loss_1: (0.2303) | Acc_1: (91.77%) (13038/14208)\n",
      "Epoch: 103 | Batch_idx: 120 |  Loss_1: (0.2306) | Acc_1: (91.74%) (14209/15488)\n",
      "Epoch: 103 | Batch_idx: 130 |  Loss_1: (0.2307) | Acc_1: (91.79%) (15392/16768)\n",
      "Epoch: 103 | Batch_idx: 140 |  Loss_1: (0.2321) | Acc_1: (91.73%) (16556/18048)\n",
      "Epoch: 103 | Batch_idx: 150 |  Loss_1: (0.2302) | Acc_1: (91.81%) (17746/19328)\n",
      "Epoch: 103 | Batch_idx: 160 |  Loss_1: (0.2288) | Acc_1: (91.85%) (18928/20608)\n",
      "Epoch: 103 | Batch_idx: 170 |  Loss_1: (0.2286) | Acc_1: (91.85%) (20105/21888)\n",
      "Epoch: 103 | Batch_idx: 180 |  Loss_1: (0.2298) | Acc_1: (91.74%) (21255/23168)\n",
      "Epoch: 103 | Batch_idx: 190 |  Loss_1: (0.2323) | Acc_1: (91.70%) (22420/24448)\n",
      "Epoch: 103 | Batch_idx: 200 |  Loss_1: (0.2300) | Acc_1: (91.80%) (23619/25728)\n",
      "Epoch: 103 | Batch_idx: 210 |  Loss_1: (0.2297) | Acc_1: (91.81%) (24797/27008)\n",
      "Epoch: 103 | Batch_idx: 220 |  Loss_1: (0.2296) | Acc_1: (91.82%) (25973/28288)\n",
      "Epoch: 103 | Batch_idx: 230 |  Loss_1: (0.2311) | Acc_1: (91.76%) (27131/29568)\n",
      "Epoch: 103 | Batch_idx: 240 |  Loss_1: (0.2313) | Acc_1: (91.73%) (28296/30848)\n",
      "Epoch: 103 | Batch_idx: 250 |  Loss_1: (0.2310) | Acc_1: (91.71%) (29465/32128)\n",
      "Epoch: 103 | Batch_idx: 260 |  Loss_1: (0.2310) | Acc_1: (91.73%) (30645/33408)\n",
      "Epoch: 103 | Batch_idx: 270 |  Loss_1: (0.2304) | Acc_1: (91.75%) (31825/34688)\n",
      "Epoch: 103 | Batch_idx: 280 |  Loss_1: (0.2315) | Acc_1: (91.72%) (32991/35968)\n",
      "Epoch: 103 | Batch_idx: 290 |  Loss_1: (0.2314) | Acc_1: (91.73%) (34168/37248)\n",
      "Epoch: 103 | Batch_idx: 300 |  Loss_1: (0.2321) | Acc_1: (91.70%) (35330/38528)\n",
      "Epoch: 103 | Batch_idx: 310 |  Loss_1: (0.2317) | Acc_1: (91.70%) (36503/39808)\n",
      "Epoch: 103 | Batch_idx: 320 |  Loss_1: (0.2305) | Acc_1: (91.73%) (37689/41088)\n",
      "Epoch: 103 | Batch_idx: 330 |  Loss_1: (0.2305) | Acc_1: (91.73%) (38865/42368)\n",
      "Epoch: 103 | Batch_idx: 340 |  Loss_1: (0.2311) | Acc_1: (91.72%) (40035/43648)\n",
      "Epoch: 103 | Batch_idx: 350 |  Loss_1: (0.2322) | Acc_1: (91.69%) (41195/44928)\n",
      "Epoch: 103 | Batch_idx: 360 |  Loss_1: (0.2322) | Acc_1: (91.71%) (42377/46208)\n",
      "Epoch: 103 | Batch_idx: 370 |  Loss_1: (0.2321) | Acc_1: (91.72%) (43556/47488)\n",
      "Epoch: 103 | Batch_idx: 380 |  Loss_1: (0.2329) | Acc_1: (91.68%) (44709/48768)\n",
      "Epoch: 103 | Batch_idx: 390 |  Loss_1: (0.2330) | Acc_1: (91.68%) (45838/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2980) | Acc: (90.75%) (9075/10000)\n",
      "Epoch: 104 | Batch_idx: 0 |  Loss_1: (0.2663) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 104 | Batch_idx: 10 |  Loss_1: (0.2138) | Acc_1: (91.90%) (1294/1408)\n",
      "Epoch: 104 | Batch_idx: 20 |  Loss_1: (0.2319) | Acc_1: (91.33%) (2455/2688)\n",
      "Epoch: 104 | Batch_idx: 30 |  Loss_1: (0.2262) | Acc_1: (91.58%) (3634/3968)\n",
      "Epoch: 104 | Batch_idx: 40 |  Loss_1: (0.2264) | Acc_1: (91.58%) (4806/5248)\n",
      "Epoch: 104 | Batch_idx: 50 |  Loss_1: (0.2230) | Acc_1: (91.84%) (5995/6528)\n",
      "Epoch: 104 | Batch_idx: 60 |  Loss_1: (0.2266) | Acc_1: (91.70%) (7160/7808)\n",
      "Epoch: 104 | Batch_idx: 70 |  Loss_1: (0.2246) | Acc_1: (91.81%) (8344/9088)\n",
      "Epoch: 104 | Batch_idx: 80 |  Loss_1: (0.2198) | Acc_1: (92.06%) (9545/10368)\n",
      "Epoch: 104 | Batch_idx: 90 |  Loss_1: (0.2211) | Acc_1: (92.07%) (10724/11648)\n",
      "Epoch: 104 | Batch_idx: 100 |  Loss_1: (0.2186) | Acc_1: (92.11%) (11908/12928)\n",
      "Epoch: 104 | Batch_idx: 110 |  Loss_1: (0.2190) | Acc_1: (92.12%) (13089/14208)\n",
      "Epoch: 104 | Batch_idx: 120 |  Loss_1: (0.2178) | Acc_1: (92.14%) (14270/15488)\n",
      "Epoch: 104 | Batch_idx: 130 |  Loss_1: (0.2174) | Acc_1: (92.15%) (15452/16768)\n",
      "Epoch: 104 | Batch_idx: 140 |  Loss_1: (0.2186) | Acc_1: (92.11%) (16624/18048)\n",
      "Epoch: 104 | Batch_idx: 150 |  Loss_1: (0.2184) | Acc_1: (92.10%) (17802/19328)\n",
      "Epoch: 104 | Batch_idx: 160 |  Loss_1: (0.2206) | Acc_1: (92.04%) (18967/20608)\n",
      "Epoch: 104 | Batch_idx: 170 |  Loss_1: (0.2218) | Acc_1: (92.03%) (20143/21888)\n",
      "Epoch: 104 | Batch_idx: 180 |  Loss_1: (0.2200) | Acc_1: (92.10%) (21338/23168)\n",
      "Epoch: 104 | Batch_idx: 190 |  Loss_1: (0.2191) | Acc_1: (92.13%) (22523/24448)\n",
      "Epoch: 104 | Batch_idx: 200 |  Loss_1: (0.2182) | Acc_1: (92.14%) (23706/25728)\n",
      "Epoch: 104 | Batch_idx: 210 |  Loss_1: (0.2186) | Acc_1: (92.11%) (24878/27008)\n",
      "Epoch: 104 | Batch_idx: 220 |  Loss_1: (0.2184) | Acc_1: (92.16%) (26069/28288)\n",
      "Epoch: 104 | Batch_idx: 230 |  Loss_1: (0.2180) | Acc_1: (92.15%) (27246/29568)\n",
      "Epoch: 104 | Batch_idx: 240 |  Loss_1: (0.2182) | Acc_1: (92.16%) (28428/30848)\n",
      "Epoch: 104 | Batch_idx: 250 |  Loss_1: (0.2197) | Acc_1: (92.10%) (29589/32128)\n",
      "Epoch: 104 | Batch_idx: 260 |  Loss_1: (0.2197) | Acc_1: (92.09%) (30764/33408)\n",
      "Epoch: 104 | Batch_idx: 270 |  Loss_1: (0.2222) | Acc_1: (91.99%) (31911/34688)\n",
      "Epoch: 104 | Batch_idx: 280 |  Loss_1: (0.2226) | Acc_1: (91.98%) (33084/35968)\n",
      "Epoch: 104 | Batch_idx: 290 |  Loss_1: (0.2228) | Acc_1: (92.00%) (34269/37248)\n",
      "Epoch: 104 | Batch_idx: 300 |  Loss_1: (0.2240) | Acc_1: (91.94%) (35422/38528)\n",
      "Epoch: 104 | Batch_idx: 310 |  Loss_1: (0.2236) | Acc_1: (91.95%) (36605/39808)\n",
      "Epoch: 104 | Batch_idx: 320 |  Loss_1: (0.2231) | Acc_1: (91.99%) (37797/41088)\n",
      "Epoch: 104 | Batch_idx: 330 |  Loss_1: (0.2234) | Acc_1: (91.98%) (38972/42368)\n",
      "Epoch: 104 | Batch_idx: 340 |  Loss_1: (0.2229) | Acc_1: (92.00%) (40155/43648)\n",
      "Epoch: 104 | Batch_idx: 350 |  Loss_1: (0.2229) | Acc_1: (92.00%) (41333/44928)\n",
      "Epoch: 104 | Batch_idx: 360 |  Loss_1: (0.2220) | Acc_1: (92.04%) (42529/46208)\n",
      "Epoch: 104 | Batch_idx: 370 |  Loss_1: (0.2207) | Acc_1: (92.08%) (43725/47488)\n",
      "Epoch: 104 | Batch_idx: 380 |  Loss_1: (0.2214) | Acc_1: (92.04%) (44885/48768)\n",
      "Epoch: 104 | Batch_idx: 390 |  Loss_1: (0.2216) | Acc_1: (92.03%) (46017/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3024) | Acc: (90.98%) (9098/10000)\n",
      "Epoch: 105 | Batch_idx: 0 |  Loss_1: (0.2009) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 105 | Batch_idx: 10 |  Loss_1: (0.2046) | Acc_1: (92.19%) (1298/1408)\n",
      "Epoch: 105 | Batch_idx: 20 |  Loss_1: (0.2262) | Acc_1: (91.93%) (2471/2688)\n",
      "Epoch: 105 | Batch_idx: 30 |  Loss_1: (0.2208) | Acc_1: (92.14%) (3656/3968)\n",
      "Epoch: 105 | Batch_idx: 40 |  Loss_1: (0.2177) | Acc_1: (92.15%) (4836/5248)\n",
      "Epoch: 105 | Batch_idx: 50 |  Loss_1: (0.2156) | Acc_1: (92.28%) (6024/6528)\n",
      "Epoch: 105 | Batch_idx: 60 |  Loss_1: (0.2112) | Acc_1: (92.49%) (7222/7808)\n",
      "Epoch: 105 | Batch_idx: 70 |  Loss_1: (0.2117) | Acc_1: (92.51%) (8407/9088)\n",
      "Epoch: 105 | Batch_idx: 80 |  Loss_1: (0.2173) | Acc_1: (92.19%) (9558/10368)\n",
      "Epoch: 105 | Batch_idx: 90 |  Loss_1: (0.2191) | Acc_1: (92.14%) (10732/11648)\n",
      "Epoch: 105 | Batch_idx: 100 |  Loss_1: (0.2187) | Acc_1: (92.20%) (11919/12928)\n",
      "Epoch: 105 | Batch_idx: 110 |  Loss_1: (0.2191) | Acc_1: (92.14%) (13091/14208)\n",
      "Epoch: 105 | Batch_idx: 120 |  Loss_1: (0.2187) | Acc_1: (92.14%) (14271/15488)\n",
      "Epoch: 105 | Batch_idx: 130 |  Loss_1: (0.2195) | Acc_1: (92.09%) (15442/16768)\n",
      "Epoch: 105 | Batch_idx: 140 |  Loss_1: (0.2181) | Acc_1: (92.11%) (16624/18048)\n",
      "Epoch: 105 | Batch_idx: 150 |  Loss_1: (0.2179) | Acc_1: (92.14%) (17809/19328)\n",
      "Epoch: 105 | Batch_idx: 160 |  Loss_1: (0.2177) | Acc_1: (92.15%) (18990/20608)\n",
      "Epoch: 105 | Batch_idx: 170 |  Loss_1: (0.2185) | Acc_1: (92.15%) (20169/21888)\n",
      "Epoch: 105 | Batch_idx: 180 |  Loss_1: (0.2196) | Acc_1: (92.14%) (21347/23168)\n",
      "Epoch: 105 | Batch_idx: 190 |  Loss_1: (0.2214) | Acc_1: (92.09%) (22514/24448)\n",
      "Epoch: 105 | Batch_idx: 200 |  Loss_1: (0.2211) | Acc_1: (92.13%) (23703/25728)\n",
      "Epoch: 105 | Batch_idx: 210 |  Loss_1: (0.2219) | Acc_1: (92.09%) (24873/27008)\n",
      "Epoch: 105 | Batch_idx: 220 |  Loss_1: (0.2220) | Acc_1: (92.08%) (26048/28288)\n",
      "Epoch: 105 | Batch_idx: 230 |  Loss_1: (0.2219) | Acc_1: (92.10%) (27231/29568)\n",
      "Epoch: 105 | Batch_idx: 240 |  Loss_1: (0.2207) | Acc_1: (92.16%) (28429/30848)\n",
      "Epoch: 105 | Batch_idx: 250 |  Loss_1: (0.2223) | Acc_1: (92.11%) (29593/32128)\n",
      "Epoch: 105 | Batch_idx: 260 |  Loss_1: (0.2219) | Acc_1: (92.11%) (30773/33408)\n",
      "Epoch: 105 | Batch_idx: 270 |  Loss_1: (0.2224) | Acc_1: (92.10%) (31947/34688)\n",
      "Epoch: 105 | Batch_idx: 280 |  Loss_1: (0.2223) | Acc_1: (92.09%) (33123/35968)\n",
      "Epoch: 105 | Batch_idx: 290 |  Loss_1: (0.2214) | Acc_1: (92.12%) (34311/37248)\n",
      "Epoch: 105 | Batch_idx: 300 |  Loss_1: (0.2210) | Acc_1: (92.12%) (35491/38528)\n",
      "Epoch: 105 | Batch_idx: 310 |  Loss_1: (0.2204) | Acc_1: (92.13%) (36677/39808)\n",
      "Epoch: 105 | Batch_idx: 320 |  Loss_1: (0.2194) | Acc_1: (92.17%) (37871/41088)\n",
      "Epoch: 105 | Batch_idx: 330 |  Loss_1: (0.2191) | Acc_1: (92.19%) (39057/42368)\n",
      "Epoch: 105 | Batch_idx: 340 |  Loss_1: (0.2190) | Acc_1: (92.20%) (40245/43648)\n",
      "Epoch: 105 | Batch_idx: 350 |  Loss_1: (0.2197) | Acc_1: (92.17%) (41411/44928)\n",
      "Epoch: 105 | Batch_idx: 360 |  Loss_1: (0.2190) | Acc_1: (92.20%) (42605/46208)\n",
      "Epoch: 105 | Batch_idx: 370 |  Loss_1: (0.2190) | Acc_1: (92.20%) (43786/47488)\n",
      "Epoch: 105 | Batch_idx: 380 |  Loss_1: (0.2194) | Acc_1: (92.19%) (44959/48768)\n",
      "Epoch: 105 | Batch_idx: 390 |  Loss_1: (0.2201) | Acc_1: (92.17%) (46084/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3096) | Acc: (90.52%) (9052/10000)\n",
      "Epoch: 106 | Batch_idx: 0 |  Loss_1: (0.3053) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 106 | Batch_idx: 10 |  Loss_1: (0.2039) | Acc_1: (92.76%) (1306/1408)\n",
      "Epoch: 106 | Batch_idx: 20 |  Loss_1: (0.2196) | Acc_1: (92.08%) (2475/2688)\n",
      "Epoch: 106 | Batch_idx: 30 |  Loss_1: (0.2203) | Acc_1: (92.14%) (3656/3968)\n",
      "Epoch: 106 | Batch_idx: 40 |  Loss_1: (0.2134) | Acc_1: (92.45%) (4852/5248)\n",
      "Epoch: 106 | Batch_idx: 50 |  Loss_1: (0.2160) | Acc_1: (92.39%) (6031/6528)\n",
      "Epoch: 106 | Batch_idx: 60 |  Loss_1: (0.2125) | Acc_1: (92.57%) (7228/7808)\n",
      "Epoch: 106 | Batch_idx: 70 |  Loss_1: (0.2130) | Acc_1: (92.51%) (8407/9088)\n",
      "Epoch: 106 | Batch_idx: 80 |  Loss_1: (0.2153) | Acc_1: (92.27%) (9567/10368)\n",
      "Epoch: 106 | Batch_idx: 90 |  Loss_1: (0.2130) | Acc_1: (92.26%) (10747/11648)\n",
      "Epoch: 106 | Batch_idx: 100 |  Loss_1: (0.2114) | Acc_1: (92.33%) (11937/12928)\n",
      "Epoch: 106 | Batch_idx: 110 |  Loss_1: (0.2096) | Acc_1: (92.42%) (13131/14208)\n",
      "Epoch: 106 | Batch_idx: 120 |  Loss_1: (0.2092) | Acc_1: (92.43%) (14316/15488)\n",
      "Epoch: 106 | Batch_idx: 130 |  Loss_1: (0.2106) | Acc_1: (92.43%) (15499/16768)\n",
      "Epoch: 106 | Batch_idx: 140 |  Loss_1: (0.2096) | Acc_1: (92.43%) (16682/18048)\n",
      "Epoch: 106 | Batch_idx: 150 |  Loss_1: (0.2102) | Acc_1: (92.39%) (17858/19328)\n",
      "Epoch: 106 | Batch_idx: 160 |  Loss_1: (0.2088) | Acc_1: (92.43%) (19049/20608)\n",
      "Epoch: 106 | Batch_idx: 170 |  Loss_1: (0.2093) | Acc_1: (92.45%) (20236/21888)\n",
      "Epoch: 106 | Batch_idx: 180 |  Loss_1: (0.2106) | Acc_1: (92.39%) (21404/23168)\n",
      "Epoch: 106 | Batch_idx: 190 |  Loss_1: (0.2111) | Acc_1: (92.36%) (22581/24448)\n",
      "Epoch: 106 | Batch_idx: 200 |  Loss_1: (0.2095) | Acc_1: (92.42%) (23779/25728)\n",
      "Epoch: 106 | Batch_idx: 210 |  Loss_1: (0.2095) | Acc_1: (92.43%) (24964/27008)\n",
      "Epoch: 106 | Batch_idx: 220 |  Loss_1: (0.2101) | Acc_1: (92.40%) (26137/28288)\n",
      "Epoch: 106 | Batch_idx: 230 |  Loss_1: (0.2116) | Acc_1: (92.36%) (27309/29568)\n",
      "Epoch: 106 | Batch_idx: 240 |  Loss_1: (0.2118) | Acc_1: (92.39%) (28499/30848)\n",
      "Epoch: 106 | Batch_idx: 250 |  Loss_1: (0.2116) | Acc_1: (92.42%) (29693/32128)\n",
      "Epoch: 106 | Batch_idx: 260 |  Loss_1: (0.2123) | Acc_1: (92.38%) (30862/33408)\n",
      "Epoch: 106 | Batch_idx: 270 |  Loss_1: (0.2124) | Acc_1: (92.39%) (32047/34688)\n",
      "Epoch: 106 | Batch_idx: 280 |  Loss_1: (0.2122) | Acc_1: (92.41%) (33238/35968)\n",
      "Epoch: 106 | Batch_idx: 290 |  Loss_1: (0.2113) | Acc_1: (92.42%) (34426/37248)\n",
      "Epoch: 106 | Batch_idx: 300 |  Loss_1: (0.2125) | Acc_1: (92.38%) (35594/38528)\n",
      "Epoch: 106 | Batch_idx: 310 |  Loss_1: (0.2128) | Acc_1: (92.36%) (36766/39808)\n",
      "Epoch: 106 | Batch_idx: 320 |  Loss_1: (0.2126) | Acc_1: (92.39%) (37960/41088)\n",
      "Epoch: 106 | Batch_idx: 330 |  Loss_1: (0.2120) | Acc_1: (92.40%) (39149/42368)\n",
      "Epoch: 106 | Batch_idx: 340 |  Loss_1: (0.2115) | Acc_1: (92.43%) (40342/43648)\n",
      "Epoch: 106 | Batch_idx: 350 |  Loss_1: (0.2126) | Acc_1: (92.37%) (41499/44928)\n",
      "Epoch: 106 | Batch_idx: 360 |  Loss_1: (0.2123) | Acc_1: (92.39%) (42690/46208)\n",
      "Epoch: 106 | Batch_idx: 370 |  Loss_1: (0.2127) | Acc_1: (92.37%) (43867/47488)\n",
      "Epoch: 106 | Batch_idx: 380 |  Loss_1: (0.2122) | Acc_1: (92.38%) (45051/48768)\n",
      "Epoch: 106 | Batch_idx: 390 |  Loss_1: (0.2113) | Acc_1: (92.41%) (46206/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2844) | Acc: (91.40%) (9140/10000)\n",
      "Epoch: 107 | Batch_idx: 0 |  Loss_1: (0.1355) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 107 | Batch_idx: 10 |  Loss_1: (0.1777) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 107 | Batch_idx: 20 |  Loss_1: (0.1933) | Acc_1: (93.60%) (2516/2688)\n",
      "Epoch: 107 | Batch_idx: 30 |  Loss_1: (0.1977) | Acc_1: (93.27%) (3701/3968)\n",
      "Epoch: 107 | Batch_idx: 40 |  Loss_1: (0.2041) | Acc_1: (92.99%) (4880/5248)\n",
      "Epoch: 107 | Batch_idx: 50 |  Loss_1: (0.2071) | Acc_1: (93.00%) (6071/6528)\n",
      "Epoch: 107 | Batch_idx: 60 |  Loss_1: (0.2130) | Acc_1: (92.76%) (7243/7808)\n",
      "Epoch: 107 | Batch_idx: 70 |  Loss_1: (0.2091) | Acc_1: (92.76%) (8430/9088)\n",
      "Epoch: 107 | Batch_idx: 80 |  Loss_1: (0.2094) | Acc_1: (92.68%) (9609/10368)\n",
      "Epoch: 107 | Batch_idx: 90 |  Loss_1: (0.2094) | Acc_1: (92.65%) (10792/11648)\n",
      "Epoch: 107 | Batch_idx: 100 |  Loss_1: (0.2122) | Acc_1: (92.54%) (11963/12928)\n",
      "Epoch: 107 | Batch_idx: 110 |  Loss_1: (0.2111) | Acc_1: (92.55%) (13150/14208)\n",
      "Epoch: 107 | Batch_idx: 120 |  Loss_1: (0.2106) | Acc_1: (92.56%) (14335/15488)\n",
      "Epoch: 107 | Batch_idx: 130 |  Loss_1: (0.2107) | Acc_1: (92.58%) (15523/16768)\n",
      "Epoch: 107 | Batch_idx: 140 |  Loss_1: (0.2103) | Acc_1: (92.54%) (16702/18048)\n",
      "Epoch: 107 | Batch_idx: 150 |  Loss_1: (0.2114) | Acc_1: (92.47%) (17872/19328)\n",
      "Epoch: 107 | Batch_idx: 160 |  Loss_1: (0.2112) | Acc_1: (92.46%) (19055/20608)\n",
      "Epoch: 107 | Batch_idx: 170 |  Loss_1: (0.2108) | Acc_1: (92.49%) (20244/21888)\n",
      "Epoch: 107 | Batch_idx: 180 |  Loss_1: (0.2097) | Acc_1: (92.52%) (21435/23168)\n",
      "Epoch: 107 | Batch_idx: 190 |  Loss_1: (0.2104) | Acc_1: (92.49%) (22611/24448)\n",
      "Epoch: 107 | Batch_idx: 200 |  Loss_1: (0.2126) | Acc_1: (92.41%) (23776/25728)\n",
      "Epoch: 107 | Batch_idx: 210 |  Loss_1: (0.2124) | Acc_1: (92.44%) (24965/27008)\n",
      "Epoch: 107 | Batch_idx: 220 |  Loss_1: (0.2125) | Acc_1: (92.43%) (26148/28288)\n",
      "Epoch: 107 | Batch_idx: 230 |  Loss_1: (0.2130) | Acc_1: (92.41%) (27324/29568)\n",
      "Epoch: 107 | Batch_idx: 240 |  Loss_1: (0.2127) | Acc_1: (92.41%) (28508/30848)\n",
      "Epoch: 107 | Batch_idx: 250 |  Loss_1: (0.2128) | Acc_1: (92.41%) (29689/32128)\n",
      "Epoch: 107 | Batch_idx: 260 |  Loss_1: (0.2127) | Acc_1: (92.39%) (30864/33408)\n",
      "Epoch: 107 | Batch_idx: 270 |  Loss_1: (0.2145) | Acc_1: (92.32%) (32024/34688)\n",
      "Epoch: 107 | Batch_idx: 280 |  Loss_1: (0.2147) | Acc_1: (92.34%) (33212/35968)\n",
      "Epoch: 107 | Batch_idx: 290 |  Loss_1: (0.2144) | Acc_1: (92.36%) (34402/37248)\n",
      "Epoch: 107 | Batch_idx: 300 |  Loss_1: (0.2146) | Acc_1: (92.34%) (35578/38528)\n",
      "Epoch: 107 | Batch_idx: 310 |  Loss_1: (0.2147) | Acc_1: (92.34%) (36758/39808)\n",
      "Epoch: 107 | Batch_idx: 320 |  Loss_1: (0.2145) | Acc_1: (92.35%) (37944/41088)\n",
      "Epoch: 107 | Batch_idx: 330 |  Loss_1: (0.2147) | Acc_1: (92.34%) (39124/42368)\n",
      "Epoch: 107 | Batch_idx: 340 |  Loss_1: (0.2149) | Acc_1: (92.33%) (40302/43648)\n",
      "Epoch: 107 | Batch_idx: 350 |  Loss_1: (0.2146) | Acc_1: (92.35%) (41490/44928)\n",
      "Epoch: 107 | Batch_idx: 360 |  Loss_1: (0.2142) | Acc_1: (92.36%) (42680/46208)\n",
      "Epoch: 107 | Batch_idx: 370 |  Loss_1: (0.2156) | Acc_1: (92.32%) (43841/47488)\n",
      "Epoch: 107 | Batch_idx: 380 |  Loss_1: (0.2160) | Acc_1: (92.29%) (45007/48768)\n",
      "Epoch: 107 | Batch_idx: 390 |  Loss_1: (0.2160) | Acc_1: (92.28%) (46142/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2921) | Acc: (91.40%) (9140/10000)\n",
      "Epoch: 108 | Batch_idx: 0 |  Loss_1: (0.2123) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 108 | Batch_idx: 10 |  Loss_1: (0.2058) | Acc_1: (93.04%) (1310/1408)\n",
      "Epoch: 108 | Batch_idx: 20 |  Loss_1: (0.2026) | Acc_1: (92.89%) (2497/2688)\n",
      "Epoch: 108 | Batch_idx: 30 |  Loss_1: (0.2000) | Acc_1: (93.02%) (3691/3968)\n",
      "Epoch: 108 | Batch_idx: 40 |  Loss_1: (0.2037) | Acc_1: (92.97%) (4879/5248)\n",
      "Epoch: 108 | Batch_idx: 50 |  Loss_1: (0.2047) | Acc_1: (92.89%) (6064/6528)\n",
      "Epoch: 108 | Batch_idx: 60 |  Loss_1: (0.2023) | Acc_1: (92.99%) (7261/7808)\n",
      "Epoch: 108 | Batch_idx: 70 |  Loss_1: (0.2023) | Acc_1: (93.03%) (8455/9088)\n",
      "Epoch: 108 | Batch_idx: 80 |  Loss_1: (0.2094) | Acc_1: (92.79%) (9620/10368)\n",
      "Epoch: 108 | Batch_idx: 90 |  Loss_1: (0.2087) | Acc_1: (92.81%) (10811/11648)\n",
      "Epoch: 108 | Batch_idx: 100 |  Loss_1: (0.2108) | Acc_1: (92.78%) (11995/12928)\n",
      "Epoch: 108 | Batch_idx: 110 |  Loss_1: (0.2127) | Acc_1: (92.66%) (13165/14208)\n",
      "Epoch: 108 | Batch_idx: 120 |  Loss_1: (0.2130) | Acc_1: (92.63%) (14346/15488)\n",
      "Epoch: 108 | Batch_idx: 130 |  Loss_1: (0.2117) | Acc_1: (92.64%) (15534/16768)\n",
      "Epoch: 108 | Batch_idx: 140 |  Loss_1: (0.2112) | Acc_1: (92.63%) (16717/18048)\n",
      "Epoch: 108 | Batch_idx: 150 |  Loss_1: (0.2125) | Acc_1: (92.57%) (17891/19328)\n",
      "Epoch: 108 | Batch_idx: 160 |  Loss_1: (0.2130) | Acc_1: (92.54%) (19070/20608)\n",
      "Epoch: 108 | Batch_idx: 170 |  Loss_1: (0.2130) | Acc_1: (92.53%) (20253/21888)\n",
      "Epoch: 108 | Batch_idx: 180 |  Loss_1: (0.2127) | Acc_1: (92.52%) (21436/23168)\n",
      "Epoch: 108 | Batch_idx: 190 |  Loss_1: (0.2132) | Acc_1: (92.50%) (22615/24448)\n",
      "Epoch: 108 | Batch_idx: 200 |  Loss_1: (0.2135) | Acc_1: (92.48%) (23793/25728)\n",
      "Epoch: 108 | Batch_idx: 210 |  Loss_1: (0.2129) | Acc_1: (92.50%) (24982/27008)\n",
      "Epoch: 108 | Batch_idx: 220 |  Loss_1: (0.2129) | Acc_1: (92.50%) (26165/28288)\n",
      "Epoch: 108 | Batch_idx: 230 |  Loss_1: (0.2123) | Acc_1: (92.50%) (27351/29568)\n",
      "Epoch: 108 | Batch_idx: 240 |  Loss_1: (0.2116) | Acc_1: (92.51%) (28539/30848)\n",
      "Epoch: 108 | Batch_idx: 250 |  Loss_1: (0.2105) | Acc_1: (92.57%) (29740/32128)\n",
      "Epoch: 108 | Batch_idx: 260 |  Loss_1: (0.2103) | Acc_1: (92.56%) (30924/33408)\n",
      "Epoch: 108 | Batch_idx: 270 |  Loss_1: (0.2104) | Acc_1: (92.55%) (32105/34688)\n",
      "Epoch: 108 | Batch_idx: 280 |  Loss_1: (0.2106) | Acc_1: (92.52%) (33278/35968)\n",
      "Epoch: 108 | Batch_idx: 290 |  Loss_1: (0.2114) | Acc_1: (92.50%) (34454/37248)\n",
      "Epoch: 108 | Batch_idx: 300 |  Loss_1: (0.2122) | Acc_1: (92.47%) (35626/38528)\n",
      "Epoch: 108 | Batch_idx: 310 |  Loss_1: (0.2123) | Acc_1: (92.47%) (36810/39808)\n",
      "Epoch: 108 | Batch_idx: 320 |  Loss_1: (0.2125) | Acc_1: (92.45%) (37984/41088)\n",
      "Epoch: 108 | Batch_idx: 330 |  Loss_1: (0.2128) | Acc_1: (92.44%) (39164/42368)\n",
      "Epoch: 108 | Batch_idx: 340 |  Loss_1: (0.2126) | Acc_1: (92.43%) (40342/43648)\n",
      "Epoch: 108 | Batch_idx: 350 |  Loss_1: (0.2127) | Acc_1: (92.42%) (41521/44928)\n",
      "Epoch: 108 | Batch_idx: 360 |  Loss_1: (0.2118) | Acc_1: (92.45%) (42718/46208)\n",
      "Epoch: 108 | Batch_idx: 370 |  Loss_1: (0.2121) | Acc_1: (92.42%) (43888/47488)\n",
      "Epoch: 108 | Batch_idx: 380 |  Loss_1: (0.2114) | Acc_1: (92.44%) (45082/48768)\n",
      "Epoch: 108 | Batch_idx: 390 |  Loss_1: (0.2107) | Acc_1: (92.46%) (46229/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3032) | Acc: (91.22%) (9122/10000)\n",
      "Epoch: 109 | Batch_idx: 0 |  Loss_1: (0.2979) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 109 | Batch_idx: 10 |  Loss_1: (0.1871) | Acc_1: (92.90%) (1308/1408)\n",
      "Epoch: 109 | Batch_idx: 20 |  Loss_1: (0.1776) | Acc_1: (93.64%) (2517/2688)\n",
      "Epoch: 109 | Batch_idx: 30 |  Loss_1: (0.1863) | Acc_1: (93.47%) (3709/3968)\n",
      "Epoch: 109 | Batch_idx: 40 |  Loss_1: (0.1910) | Acc_1: (93.33%) (4898/5248)\n",
      "Epoch: 109 | Batch_idx: 50 |  Loss_1: (0.1866) | Acc_1: (93.41%) (6098/6528)\n",
      "Epoch: 109 | Batch_idx: 60 |  Loss_1: (0.1904) | Acc_1: (93.26%) (7282/7808)\n",
      "Epoch: 109 | Batch_idx: 70 |  Loss_1: (0.1902) | Acc_1: (93.25%) (8475/9088)\n",
      "Epoch: 109 | Batch_idx: 80 |  Loss_1: (0.1956) | Acc_1: (93.07%) (9650/10368)\n",
      "Epoch: 109 | Batch_idx: 90 |  Loss_1: (0.1984) | Acc_1: (92.96%) (10828/11648)\n",
      "Epoch: 109 | Batch_idx: 100 |  Loss_1: (0.1963) | Acc_1: (93.02%) (12025/12928)\n",
      "Epoch: 109 | Batch_idx: 110 |  Loss_1: (0.1975) | Acc_1: (92.96%) (13208/14208)\n",
      "Epoch: 109 | Batch_idx: 120 |  Loss_1: (0.1983) | Acc_1: (92.92%) (14392/15488)\n",
      "Epoch: 109 | Batch_idx: 130 |  Loss_1: (0.1986) | Acc_1: (92.90%) (15577/16768)\n",
      "Epoch: 109 | Batch_idx: 140 |  Loss_1: (0.2004) | Acc_1: (92.81%) (16751/18048)\n",
      "Epoch: 109 | Batch_idx: 150 |  Loss_1: (0.1999) | Acc_1: (92.83%) (17943/19328)\n",
      "Epoch: 109 | Batch_idx: 160 |  Loss_1: (0.2002) | Acc_1: (92.84%) (19132/20608)\n",
      "Epoch: 109 | Batch_idx: 170 |  Loss_1: (0.2002) | Acc_1: (92.85%) (20324/21888)\n",
      "Epoch: 109 | Batch_idx: 180 |  Loss_1: (0.2034) | Acc_1: (92.72%) (21482/23168)\n",
      "Epoch: 109 | Batch_idx: 190 |  Loss_1: (0.2035) | Acc_1: (92.73%) (22671/24448)\n",
      "Epoch: 109 | Batch_idx: 200 |  Loss_1: (0.2065) | Acc_1: (92.65%) (23836/25728)\n",
      "Epoch: 109 | Batch_idx: 210 |  Loss_1: (0.2077) | Acc_1: (92.55%) (24997/27008)\n",
      "Epoch: 109 | Batch_idx: 220 |  Loss_1: (0.2092) | Acc_1: (92.50%) (26166/28288)\n",
      "Epoch: 109 | Batch_idx: 230 |  Loss_1: (0.2080) | Acc_1: (92.53%) (27359/29568)\n",
      "Epoch: 109 | Batch_idx: 240 |  Loss_1: (0.2101) | Acc_1: (92.47%) (28525/30848)\n",
      "Epoch: 109 | Batch_idx: 250 |  Loss_1: (0.2099) | Acc_1: (92.50%) (29717/32128)\n",
      "Epoch: 109 | Batch_idx: 260 |  Loss_1: (0.2105) | Acc_1: (92.48%) (30896/33408)\n",
      "Epoch: 109 | Batch_idx: 270 |  Loss_1: (0.2093) | Acc_1: (92.54%) (32099/34688)\n",
      "Epoch: 109 | Batch_idx: 280 |  Loss_1: (0.2092) | Acc_1: (92.55%) (33289/35968)\n",
      "Epoch: 109 | Batch_idx: 290 |  Loss_1: (0.2096) | Acc_1: (92.53%) (34467/37248)\n",
      "Epoch: 109 | Batch_idx: 300 |  Loss_1: (0.2098) | Acc_1: (92.52%) (35648/38528)\n",
      "Epoch: 109 | Batch_idx: 310 |  Loss_1: (0.2085) | Acc_1: (92.57%) (36852/39808)\n",
      "Epoch: 109 | Batch_idx: 320 |  Loss_1: (0.2084) | Acc_1: (92.58%) (38039/41088)\n",
      "Epoch: 109 | Batch_idx: 330 |  Loss_1: (0.2084) | Acc_1: (92.57%) (39220/42368)\n",
      "Epoch: 109 | Batch_idx: 340 |  Loss_1: (0.2098) | Acc_1: (92.51%) (40378/43648)\n",
      "Epoch: 109 | Batch_idx: 350 |  Loss_1: (0.2101) | Acc_1: (92.48%) (41549/44928)\n",
      "Epoch: 109 | Batch_idx: 360 |  Loss_1: (0.2104) | Acc_1: (92.46%) (42723/46208)\n",
      "Epoch: 109 | Batch_idx: 370 |  Loss_1: (0.2108) | Acc_1: (92.43%) (43894/47488)\n",
      "Epoch: 109 | Batch_idx: 380 |  Loss_1: (0.2102) | Acc_1: (92.45%) (45088/48768)\n",
      "Epoch: 109 | Batch_idx: 390 |  Loss_1: (0.2105) | Acc_1: (92.44%) (46218/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3140) | Acc: (90.56%) (9056/10000)\n",
      "Epoch: 110 | Batch_idx: 0 |  Loss_1: (0.1754) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 110 | Batch_idx: 10 |  Loss_1: (0.2148) | Acc_1: (92.12%) (1297/1408)\n",
      "Epoch: 110 | Batch_idx: 20 |  Loss_1: (0.2127) | Acc_1: (92.22%) (2479/2688)\n",
      "Epoch: 110 | Batch_idx: 30 |  Loss_1: (0.2110) | Acc_1: (92.54%) (3672/3968)\n",
      "Epoch: 110 | Batch_idx: 40 |  Loss_1: (0.2148) | Acc_1: (92.36%) (4847/5248)\n",
      "Epoch: 110 | Batch_idx: 50 |  Loss_1: (0.2198) | Acc_1: (92.25%) (6022/6528)\n",
      "Epoch: 110 | Batch_idx: 60 |  Loss_1: (0.2172) | Acc_1: (92.38%) (7213/7808)\n",
      "Epoch: 110 | Batch_idx: 70 |  Loss_1: (0.2157) | Acc_1: (92.29%) (8387/9088)\n",
      "Epoch: 110 | Batch_idx: 80 |  Loss_1: (0.2160) | Acc_1: (92.33%) (9573/10368)\n",
      "Epoch: 110 | Batch_idx: 90 |  Loss_1: (0.2130) | Acc_1: (92.41%) (10764/11648)\n",
      "Epoch: 110 | Batch_idx: 100 |  Loss_1: (0.2123) | Acc_1: (92.44%) (11951/12928)\n",
      "Epoch: 110 | Batch_idx: 110 |  Loss_1: (0.2143) | Acc_1: (92.40%) (13128/14208)\n",
      "Epoch: 110 | Batch_idx: 120 |  Loss_1: (0.2131) | Acc_1: (92.45%) (14318/15488)\n",
      "Epoch: 110 | Batch_idx: 130 |  Loss_1: (0.2113) | Acc_1: (92.53%) (15516/16768)\n",
      "Epoch: 110 | Batch_idx: 140 |  Loss_1: (0.2094) | Acc_1: (92.58%) (16708/18048)\n",
      "Epoch: 110 | Batch_idx: 150 |  Loss_1: (0.2077) | Acc_1: (92.64%) (17906/19328)\n",
      "Epoch: 110 | Batch_idx: 160 |  Loss_1: (0.2082) | Acc_1: (92.61%) (19086/20608)\n",
      "Epoch: 110 | Batch_idx: 170 |  Loss_1: (0.2070) | Acc_1: (92.64%) (20276/21888)\n",
      "Epoch: 110 | Batch_idx: 180 |  Loss_1: (0.2069) | Acc_1: (92.66%) (21467/23168)\n",
      "Epoch: 110 | Batch_idx: 190 |  Loss_1: (0.2083) | Acc_1: (92.59%) (22636/24448)\n",
      "Epoch: 110 | Batch_idx: 200 |  Loss_1: (0.2091) | Acc_1: (92.55%) (23812/25728)\n",
      "Epoch: 110 | Batch_idx: 210 |  Loss_1: (0.2084) | Acc_1: (92.58%) (25003/27008)\n",
      "Epoch: 110 | Batch_idx: 220 |  Loss_1: (0.2093) | Acc_1: (92.53%) (26174/28288)\n",
      "Epoch: 110 | Batch_idx: 230 |  Loss_1: (0.2084) | Acc_1: (92.56%) (27367/29568)\n",
      "Epoch: 110 | Batch_idx: 240 |  Loss_1: (0.2090) | Acc_1: (92.53%) (28543/30848)\n",
      "Epoch: 110 | Batch_idx: 250 |  Loss_1: (0.2099) | Acc_1: (92.49%) (29714/32128)\n",
      "Epoch: 110 | Batch_idx: 260 |  Loss_1: (0.2098) | Acc_1: (92.51%) (30907/33408)\n",
      "Epoch: 110 | Batch_idx: 270 |  Loss_1: (0.2095) | Acc_1: (92.52%) (32094/34688)\n",
      "Epoch: 110 | Batch_idx: 280 |  Loss_1: (0.2094) | Acc_1: (92.54%) (33286/35968)\n",
      "Epoch: 110 | Batch_idx: 290 |  Loss_1: (0.2094) | Acc_1: (92.57%) (34480/37248)\n",
      "Epoch: 110 | Batch_idx: 300 |  Loss_1: (0.2092) | Acc_1: (92.58%) (35670/38528)\n",
      "Epoch: 110 | Batch_idx: 310 |  Loss_1: (0.2092) | Acc_1: (92.57%) (36851/39808)\n",
      "Epoch: 110 | Batch_idx: 320 |  Loss_1: (0.2096) | Acc_1: (92.53%) (38020/41088)\n",
      "Epoch: 110 | Batch_idx: 330 |  Loss_1: (0.2101) | Acc_1: (92.52%) (39200/42368)\n",
      "Epoch: 110 | Batch_idx: 340 |  Loss_1: (0.2111) | Acc_1: (92.49%) (40372/43648)\n",
      "Epoch: 110 | Batch_idx: 350 |  Loss_1: (0.2107) | Acc_1: (92.50%) (41557/44928)\n",
      "Epoch: 110 | Batch_idx: 360 |  Loss_1: (0.2103) | Acc_1: (92.51%) (42747/46208)\n",
      "Epoch: 110 | Batch_idx: 370 |  Loss_1: (0.2100) | Acc_1: (92.51%) (43929/47488)\n",
      "Epoch: 110 | Batch_idx: 380 |  Loss_1: (0.2104) | Acc_1: (92.48%) (45103/48768)\n",
      "Epoch: 110 | Batch_idx: 390 |  Loss_1: (0.2096) | Acc_1: (92.52%) (46258/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3027) | Acc: (90.99%) (9099/10000)\n",
      "Epoch: 111 | Batch_idx: 0 |  Loss_1: (0.2933) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 111 | Batch_idx: 10 |  Loss_1: (0.1811) | Acc_1: (93.04%) (1310/1408)\n",
      "Epoch: 111 | Batch_idx: 20 |  Loss_1: (0.2030) | Acc_1: (92.82%) (2495/2688)\n",
      "Epoch: 111 | Batch_idx: 30 |  Loss_1: (0.1957) | Acc_1: (92.87%) (3685/3968)\n",
      "Epoch: 111 | Batch_idx: 40 |  Loss_1: (0.1896) | Acc_1: (93.04%) (4883/5248)\n",
      "Epoch: 111 | Batch_idx: 50 |  Loss_1: (0.1943) | Acc_1: (92.89%) (6064/6528)\n",
      "Epoch: 111 | Batch_idx: 60 |  Loss_1: (0.1948) | Acc_1: (92.93%) (7256/7808)\n",
      "Epoch: 111 | Batch_idx: 70 |  Loss_1: (0.1938) | Acc_1: (92.96%) (8448/9088)\n",
      "Epoch: 111 | Batch_idx: 80 |  Loss_1: (0.1933) | Acc_1: (93.00%) (9642/10368)\n",
      "Epoch: 111 | Batch_idx: 90 |  Loss_1: (0.1959) | Acc_1: (92.88%) (10819/11648)\n",
      "Epoch: 111 | Batch_idx: 100 |  Loss_1: (0.1982) | Acc_1: (92.84%) (12002/12928)\n",
      "Epoch: 111 | Batch_idx: 110 |  Loss_1: (0.1993) | Acc_1: (92.78%) (13182/14208)\n",
      "Epoch: 111 | Batch_idx: 120 |  Loss_1: (0.1982) | Acc_1: (92.83%) (14378/15488)\n",
      "Epoch: 111 | Batch_idx: 130 |  Loss_1: (0.2005) | Acc_1: (92.82%) (15564/16768)\n",
      "Epoch: 111 | Batch_idx: 140 |  Loss_1: (0.2001) | Acc_1: (92.82%) (16752/18048)\n",
      "Epoch: 111 | Batch_idx: 150 |  Loss_1: (0.1992) | Acc_1: (92.87%) (17949/19328)\n",
      "Epoch: 111 | Batch_idx: 160 |  Loss_1: (0.2003) | Acc_1: (92.80%) (19124/20608)\n",
      "Epoch: 111 | Batch_idx: 170 |  Loss_1: (0.2020) | Acc_1: (92.77%) (20305/21888)\n",
      "Epoch: 111 | Batch_idx: 180 |  Loss_1: (0.2017) | Acc_1: (92.78%) (21495/23168)\n",
      "Epoch: 111 | Batch_idx: 190 |  Loss_1: (0.2043) | Acc_1: (92.69%) (22662/24448)\n",
      "Epoch: 111 | Batch_idx: 200 |  Loss_1: (0.2051) | Acc_1: (92.67%) (23843/25728)\n",
      "Epoch: 111 | Batch_idx: 210 |  Loss_1: (0.2052) | Acc_1: (92.64%) (25021/27008)\n",
      "Epoch: 111 | Batch_idx: 220 |  Loss_1: (0.2047) | Acc_1: (92.68%) (26216/28288)\n",
      "Epoch: 111 | Batch_idx: 230 |  Loss_1: (0.2053) | Acc_1: (92.65%) (27394/29568)\n",
      "Epoch: 111 | Batch_idx: 240 |  Loss_1: (0.2061) | Acc_1: (92.59%) (28563/30848)\n",
      "Epoch: 111 | Batch_idx: 250 |  Loss_1: (0.2062) | Acc_1: (92.59%) (29748/32128)\n",
      "Epoch: 111 | Batch_idx: 260 |  Loss_1: (0.2061) | Acc_1: (92.60%) (30936/33408)\n",
      "Epoch: 111 | Batch_idx: 270 |  Loss_1: (0.2053) | Acc_1: (92.61%) (32126/34688)\n",
      "Epoch: 111 | Batch_idx: 280 |  Loss_1: (0.2052) | Acc_1: (92.62%) (33315/35968)\n",
      "Epoch: 111 | Batch_idx: 290 |  Loss_1: (0.2051) | Acc_1: (92.64%) (34505/37248)\n",
      "Epoch: 111 | Batch_idx: 300 |  Loss_1: (0.2053) | Acc_1: (92.60%) (35677/38528)\n",
      "Epoch: 111 | Batch_idx: 310 |  Loss_1: (0.2044) | Acc_1: (92.64%) (36879/39808)\n",
      "Epoch: 111 | Batch_idx: 320 |  Loss_1: (0.2051) | Acc_1: (92.63%) (38060/41088)\n",
      "Epoch: 111 | Batch_idx: 330 |  Loss_1: (0.2062) | Acc_1: (92.58%) (39224/42368)\n",
      "Epoch: 111 | Batch_idx: 340 |  Loss_1: (0.2058) | Acc_1: (92.59%) (40414/43648)\n",
      "Epoch: 111 | Batch_idx: 350 |  Loss_1: (0.2058) | Acc_1: (92.59%) (41600/44928)\n",
      "Epoch: 111 | Batch_idx: 360 |  Loss_1: (0.2064) | Acc_1: (92.56%) (42769/46208)\n",
      "Epoch: 111 | Batch_idx: 370 |  Loss_1: (0.2075) | Acc_1: (92.51%) (43930/47488)\n",
      "Epoch: 111 | Batch_idx: 380 |  Loss_1: (0.2068) | Acc_1: (92.53%) (45127/48768)\n",
      "Epoch: 111 | Batch_idx: 390 |  Loss_1: (0.2067) | Acc_1: (92.54%) (46272/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3203) | Acc: (90.70%) (9070/10000)\n",
      "Epoch: 112 | Batch_idx: 0 |  Loss_1: (0.1536) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 112 | Batch_idx: 10 |  Loss_1: (0.2115) | Acc_1: (91.97%) (1295/1408)\n",
      "Epoch: 112 | Batch_idx: 20 |  Loss_1: (0.2047) | Acc_1: (92.56%) (2488/2688)\n",
      "Epoch: 112 | Batch_idx: 30 |  Loss_1: (0.2094) | Acc_1: (92.39%) (3666/3968)\n",
      "Epoch: 112 | Batch_idx: 40 |  Loss_1: (0.2161) | Acc_1: (92.13%) (4835/5248)\n",
      "Epoch: 112 | Batch_idx: 50 |  Loss_1: (0.2155) | Acc_1: (92.17%) (6017/6528)\n",
      "Epoch: 112 | Batch_idx: 60 |  Loss_1: (0.2167) | Acc_1: (92.10%) (7191/7808)\n",
      "Epoch: 112 | Batch_idx: 70 |  Loss_1: (0.2142) | Acc_1: (92.24%) (8383/9088)\n",
      "Epoch: 112 | Batch_idx: 80 |  Loss_1: (0.2064) | Acc_1: (92.55%) (9596/10368)\n",
      "Epoch: 112 | Batch_idx: 90 |  Loss_1: (0.2069) | Acc_1: (92.53%) (10778/11648)\n",
      "Epoch: 112 | Batch_idx: 100 |  Loss_1: (0.2069) | Acc_1: (92.58%) (11969/12928)\n",
      "Epoch: 112 | Batch_idx: 110 |  Loss_1: (0.2038) | Acc_1: (92.73%) (13175/14208)\n",
      "Epoch: 112 | Batch_idx: 120 |  Loss_1: (0.2028) | Acc_1: (92.72%) (14360/15488)\n",
      "Epoch: 112 | Batch_idx: 130 |  Loss_1: (0.2012) | Acc_1: (92.80%) (15561/16768)\n",
      "Epoch: 112 | Batch_idx: 140 |  Loss_1: (0.2009) | Acc_1: (92.83%) (16754/18048)\n",
      "Epoch: 112 | Batch_idx: 150 |  Loss_1: (0.1988) | Acc_1: (92.91%) (17958/19328)\n",
      "Epoch: 112 | Batch_idx: 160 |  Loss_1: (0.1997) | Acc_1: (92.86%) (19136/20608)\n",
      "Epoch: 112 | Batch_idx: 170 |  Loss_1: (0.1986) | Acc_1: (92.88%) (20330/21888)\n",
      "Epoch: 112 | Batch_idx: 180 |  Loss_1: (0.1980) | Acc_1: (92.87%) (21517/23168)\n",
      "Epoch: 112 | Batch_idx: 190 |  Loss_1: (0.1985) | Acc_1: (92.85%) (22699/24448)\n",
      "Epoch: 112 | Batch_idx: 200 |  Loss_1: (0.1991) | Acc_1: (92.82%) (23880/25728)\n",
      "Epoch: 112 | Batch_idx: 210 |  Loss_1: (0.1980) | Acc_1: (92.87%) (25081/27008)\n",
      "Epoch: 112 | Batch_idx: 220 |  Loss_1: (0.1976) | Acc_1: (92.87%) (26272/28288)\n",
      "Epoch: 112 | Batch_idx: 230 |  Loss_1: (0.1995) | Acc_1: (92.79%) (27435/29568)\n",
      "Epoch: 112 | Batch_idx: 240 |  Loss_1: (0.2001) | Acc_1: (92.79%) (28623/30848)\n",
      "Epoch: 112 | Batch_idx: 250 |  Loss_1: (0.2005) | Acc_1: (92.79%) (29810/32128)\n",
      "Epoch: 112 | Batch_idx: 260 |  Loss_1: (0.2004) | Acc_1: (92.79%) (31000/33408)\n",
      "Epoch: 112 | Batch_idx: 270 |  Loss_1: (0.2002) | Acc_1: (92.80%) (32190/34688)\n",
      "Epoch: 112 | Batch_idx: 280 |  Loss_1: (0.2008) | Acc_1: (92.78%) (33370/35968)\n",
      "Epoch: 112 | Batch_idx: 290 |  Loss_1: (0.2009) | Acc_1: (92.75%) (34549/37248)\n",
      "Epoch: 112 | Batch_idx: 300 |  Loss_1: (0.2014) | Acc_1: (92.73%) (35727/38528)\n",
      "Epoch: 112 | Batch_idx: 310 |  Loss_1: (0.2015) | Acc_1: (92.72%) (36910/39808)\n",
      "Epoch: 112 | Batch_idx: 320 |  Loss_1: (0.2019) | Acc_1: (92.71%) (38092/41088)\n",
      "Epoch: 112 | Batch_idx: 330 |  Loss_1: (0.2021) | Acc_1: (92.71%) (39279/42368)\n",
      "Epoch: 112 | Batch_idx: 340 |  Loss_1: (0.2018) | Acc_1: (92.72%) (40471/43648)\n",
      "Epoch: 112 | Batch_idx: 350 |  Loss_1: (0.2020) | Acc_1: (92.72%) (41655/44928)\n",
      "Epoch: 112 | Batch_idx: 360 |  Loss_1: (0.2022) | Acc_1: (92.71%) (42840/46208)\n",
      "Epoch: 112 | Batch_idx: 370 |  Loss_1: (0.2022) | Acc_1: (92.72%) (44029/47488)\n",
      "Epoch: 112 | Batch_idx: 380 |  Loss_1: (0.2014) | Acc_1: (92.73%) (45225/48768)\n",
      "Epoch: 112 | Batch_idx: 390 |  Loss_1: (0.2016) | Acc_1: (92.76%) (46378/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3187) | Acc: (91.02%) (9102/10000)\n",
      "Epoch: 113 | Batch_idx: 0 |  Loss_1: (0.1997) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 113 | Batch_idx: 10 |  Loss_1: (0.1686) | Acc_1: (93.82%) (1321/1408)\n",
      "Epoch: 113 | Batch_idx: 20 |  Loss_1: (0.1964) | Acc_1: (92.60%) (2489/2688)\n",
      "Epoch: 113 | Batch_idx: 30 |  Loss_1: (0.1957) | Acc_1: (92.74%) (3680/3968)\n",
      "Epoch: 113 | Batch_idx: 40 |  Loss_1: (0.1933) | Acc_1: (92.99%) (4880/5248)\n",
      "Epoch: 113 | Batch_idx: 50 |  Loss_1: (0.1953) | Acc_1: (92.95%) (6068/6528)\n",
      "Epoch: 113 | Batch_idx: 60 |  Loss_1: (0.1991) | Acc_1: (92.80%) (7246/7808)\n",
      "Epoch: 113 | Batch_idx: 70 |  Loss_1: (0.2013) | Acc_1: (92.69%) (8424/9088)\n",
      "Epoch: 113 | Batch_idx: 80 |  Loss_1: (0.2048) | Acc_1: (92.58%) (9599/10368)\n",
      "Epoch: 113 | Batch_idx: 90 |  Loss_1: (0.2038) | Acc_1: (92.57%) (10783/11648)\n",
      "Epoch: 113 | Batch_idx: 100 |  Loss_1: (0.2034) | Acc_1: (92.58%) (11969/12928)\n",
      "Epoch: 113 | Batch_idx: 110 |  Loss_1: (0.2034) | Acc_1: (92.54%) (13148/14208)\n",
      "Epoch: 113 | Batch_idx: 120 |  Loss_1: (0.2052) | Acc_1: (92.50%) (14327/15488)\n",
      "Epoch: 113 | Batch_idx: 130 |  Loss_1: (0.2036) | Acc_1: (92.58%) (15523/16768)\n",
      "Epoch: 113 | Batch_idx: 140 |  Loss_1: (0.2018) | Acc_1: (92.65%) (16722/18048)\n",
      "Epoch: 113 | Batch_idx: 150 |  Loss_1: (0.2017) | Acc_1: (92.64%) (17905/19328)\n",
      "Epoch: 113 | Batch_idx: 160 |  Loss_1: (0.2005) | Acc_1: (92.65%) (19093/20608)\n",
      "Epoch: 113 | Batch_idx: 170 |  Loss_1: (0.2001) | Acc_1: (92.64%) (20277/21888)\n",
      "Epoch: 113 | Batch_idx: 180 |  Loss_1: (0.2026) | Acc_1: (92.60%) (21454/23168)\n",
      "Epoch: 113 | Batch_idx: 190 |  Loss_1: (0.2042) | Acc_1: (92.53%) (22622/24448)\n",
      "Epoch: 113 | Batch_idx: 200 |  Loss_1: (0.2032) | Acc_1: (92.56%) (23813/25728)\n",
      "Epoch: 113 | Batch_idx: 210 |  Loss_1: (0.2022) | Acc_1: (92.58%) (25004/27008)\n",
      "Epoch: 113 | Batch_idx: 220 |  Loss_1: (0.2026) | Acc_1: (92.59%) (26193/28288)\n",
      "Epoch: 113 | Batch_idx: 230 |  Loss_1: (0.2010) | Acc_1: (92.64%) (27392/29568)\n",
      "Epoch: 113 | Batch_idx: 240 |  Loss_1: (0.2006) | Acc_1: (92.65%) (28580/30848)\n",
      "Epoch: 113 | Batch_idx: 250 |  Loss_1: (0.2011) | Acc_1: (92.63%) (29761/32128)\n",
      "Epoch: 113 | Batch_idx: 260 |  Loss_1: (0.2005) | Acc_1: (92.67%) (30958/33408)\n",
      "Epoch: 113 | Batch_idx: 270 |  Loss_1: (0.2001) | Acc_1: (92.70%) (32155/34688)\n",
      "Epoch: 113 | Batch_idx: 280 |  Loss_1: (0.2003) | Acc_1: (92.70%) (33341/35968)\n",
      "Epoch: 113 | Batch_idx: 290 |  Loss_1: (0.2004) | Acc_1: (92.68%) (34522/37248)\n",
      "Epoch: 113 | Batch_idx: 300 |  Loss_1: (0.1994) | Acc_1: (92.74%) (35730/38528)\n",
      "Epoch: 113 | Batch_idx: 310 |  Loss_1: (0.1994) | Acc_1: (92.73%) (36915/39808)\n",
      "Epoch: 113 | Batch_idx: 320 |  Loss_1: (0.1993) | Acc_1: (92.75%) (38111/41088)\n",
      "Epoch: 113 | Batch_idx: 330 |  Loss_1: (0.1991) | Acc_1: (92.74%) (39294/42368)\n",
      "Epoch: 113 | Batch_idx: 340 |  Loss_1: (0.1987) | Acc_1: (92.77%) (40494/43648)\n",
      "Epoch: 113 | Batch_idx: 350 |  Loss_1: (0.1972) | Acc_1: (92.84%) (41711/44928)\n",
      "Epoch: 113 | Batch_idx: 360 |  Loss_1: (0.1975) | Acc_1: (92.83%) (42895/46208)\n",
      "Epoch: 113 | Batch_idx: 370 |  Loss_1: (0.1976) | Acc_1: (92.83%) (44083/47488)\n",
      "Epoch: 113 | Batch_idx: 380 |  Loss_1: (0.1973) | Acc_1: (92.83%) (45272/48768)\n",
      "Epoch: 113 | Batch_idx: 390 |  Loss_1: (0.1971) | Acc_1: (92.83%) (46414/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3169) | Acc: (91.05%) (9105/10000)\n",
      "Epoch: 114 | Batch_idx: 0 |  Loss_1: (0.1727) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 114 | Batch_idx: 10 |  Loss_1: (0.1819) | Acc_1: (93.25%) (1313/1408)\n",
      "Epoch: 114 | Batch_idx: 20 |  Loss_1: (0.1841) | Acc_1: (93.30%) (2508/2688)\n",
      "Epoch: 114 | Batch_idx: 30 |  Loss_1: (0.1782) | Acc_1: (93.42%) (3707/3968)\n",
      "Epoch: 114 | Batch_idx: 40 |  Loss_1: (0.1792) | Acc_1: (93.37%) (4900/5248)\n",
      "Epoch: 114 | Batch_idx: 50 |  Loss_1: (0.1808) | Acc_1: (93.29%) (6090/6528)\n",
      "Epoch: 114 | Batch_idx: 60 |  Loss_1: (0.1820) | Acc_1: (93.30%) (7285/7808)\n",
      "Epoch: 114 | Batch_idx: 70 |  Loss_1: (0.1807) | Acc_1: (93.42%) (8490/9088)\n",
      "Epoch: 114 | Batch_idx: 80 |  Loss_1: (0.1808) | Acc_1: (93.38%) (9682/10368)\n",
      "Epoch: 114 | Batch_idx: 90 |  Loss_1: (0.1794) | Acc_1: (93.47%) (10887/11648)\n",
      "Epoch: 114 | Batch_idx: 100 |  Loss_1: (0.1799) | Acc_1: (93.48%) (12085/12928)\n",
      "Epoch: 114 | Batch_idx: 110 |  Loss_1: (0.1823) | Acc_1: (93.46%) (13279/14208)\n",
      "Epoch: 114 | Batch_idx: 120 |  Loss_1: (0.1821) | Acc_1: (93.45%) (14474/15488)\n",
      "Epoch: 114 | Batch_idx: 130 |  Loss_1: (0.1870) | Acc_1: (93.31%) (15647/16768)\n",
      "Epoch: 114 | Batch_idx: 140 |  Loss_1: (0.1872) | Acc_1: (93.31%) (16840/18048)\n",
      "Epoch: 114 | Batch_idx: 150 |  Loss_1: (0.1873) | Acc_1: (93.31%) (18034/19328)\n",
      "Epoch: 114 | Batch_idx: 160 |  Loss_1: (0.1893) | Acc_1: (93.25%) (19217/20608)\n",
      "Epoch: 114 | Batch_idx: 170 |  Loss_1: (0.1889) | Acc_1: (93.24%) (20408/21888)\n",
      "Epoch: 114 | Batch_idx: 180 |  Loss_1: (0.1899) | Acc_1: (93.19%) (21591/23168)\n",
      "Epoch: 114 | Batch_idx: 190 |  Loss_1: (0.1896) | Acc_1: (93.20%) (22785/24448)\n",
      "Epoch: 114 | Batch_idx: 200 |  Loss_1: (0.1880) | Acc_1: (93.24%) (23990/25728)\n",
      "Epoch: 114 | Batch_idx: 210 |  Loss_1: (0.1880) | Acc_1: (93.23%) (25180/27008)\n",
      "Epoch: 114 | Batch_idx: 220 |  Loss_1: (0.1889) | Acc_1: (93.22%) (26371/28288)\n",
      "Epoch: 114 | Batch_idx: 230 |  Loss_1: (0.1885) | Acc_1: (93.23%) (27565/29568)\n",
      "Epoch: 114 | Batch_idx: 240 |  Loss_1: (0.1889) | Acc_1: (93.18%) (28745/30848)\n",
      "Epoch: 114 | Batch_idx: 250 |  Loss_1: (0.1895) | Acc_1: (93.20%) (29942/32128)\n",
      "Epoch: 114 | Batch_idx: 260 |  Loss_1: (0.1892) | Acc_1: (93.21%) (31139/33408)\n",
      "Epoch: 114 | Batch_idx: 270 |  Loss_1: (0.1894) | Acc_1: (93.21%) (32334/34688)\n",
      "Epoch: 114 | Batch_idx: 280 |  Loss_1: (0.1914) | Acc_1: (93.16%) (33507/35968)\n",
      "Epoch: 114 | Batch_idx: 290 |  Loss_1: (0.1920) | Acc_1: (93.12%) (34685/37248)\n",
      "Epoch: 114 | Batch_idx: 300 |  Loss_1: (0.1920) | Acc_1: (93.12%) (35878/38528)\n",
      "Epoch: 114 | Batch_idx: 310 |  Loss_1: (0.1918) | Acc_1: (93.14%) (37076/39808)\n",
      "Epoch: 114 | Batch_idx: 320 |  Loss_1: (0.1923) | Acc_1: (93.12%) (38260/41088)\n",
      "Epoch: 114 | Batch_idx: 330 |  Loss_1: (0.1932) | Acc_1: (93.09%) (39439/42368)\n",
      "Epoch: 114 | Batch_idx: 340 |  Loss_1: (0.1930) | Acc_1: (93.09%) (40634/43648)\n",
      "Epoch: 114 | Batch_idx: 350 |  Loss_1: (0.1926) | Acc_1: (93.11%) (41833/44928)\n",
      "Epoch: 114 | Batch_idx: 360 |  Loss_1: (0.1926) | Acc_1: (93.12%) (43028/46208)\n",
      "Epoch: 114 | Batch_idx: 370 |  Loss_1: (0.1926) | Acc_1: (93.11%) (44215/47488)\n",
      "Epoch: 114 | Batch_idx: 380 |  Loss_1: (0.1920) | Acc_1: (93.14%) (45421/48768)\n",
      "Epoch: 114 | Batch_idx: 390 |  Loss_1: (0.1921) | Acc_1: (93.14%) (46568/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3309) | Acc: (91.13%) (9113/10000)\n",
      "Epoch: 115 | Batch_idx: 0 |  Loss_1: (0.1359) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 115 | Batch_idx: 10 |  Loss_1: (0.1921) | Acc_1: (93.32%) (1314/1408)\n",
      "Epoch: 115 | Batch_idx: 20 |  Loss_1: (0.1961) | Acc_1: (93.34%) (2509/2688)\n",
      "Epoch: 115 | Batch_idx: 30 |  Loss_1: (0.1925) | Acc_1: (93.35%) (3704/3968)\n",
      "Epoch: 115 | Batch_idx: 40 |  Loss_1: (0.1977) | Acc_1: (93.04%) (4883/5248)\n",
      "Epoch: 115 | Batch_idx: 50 |  Loss_1: (0.2050) | Acc_1: (92.89%) (6064/6528)\n",
      "Epoch: 115 | Batch_idx: 60 |  Loss_1: (0.2055) | Acc_1: (92.87%) (7251/7808)\n",
      "Epoch: 115 | Batch_idx: 70 |  Loss_1: (0.2043) | Acc_1: (92.89%) (8442/9088)\n",
      "Epoch: 115 | Batch_idx: 80 |  Loss_1: (0.2024) | Acc_1: (92.96%) (9638/10368)\n",
      "Epoch: 115 | Batch_idx: 90 |  Loss_1: (0.2022) | Acc_1: (92.92%) (10823/11648)\n",
      "Epoch: 115 | Batch_idx: 100 |  Loss_1: (0.2020) | Acc_1: (92.95%) (12017/12928)\n",
      "Epoch: 115 | Batch_idx: 110 |  Loss_1: (0.2010) | Acc_1: (92.94%) (13205/14208)\n",
      "Epoch: 115 | Batch_idx: 120 |  Loss_1: (0.2038) | Acc_1: (92.87%) (14383/15488)\n",
      "Epoch: 115 | Batch_idx: 130 |  Loss_1: (0.2034) | Acc_1: (92.92%) (15581/16768)\n",
      "Epoch: 115 | Batch_idx: 140 |  Loss_1: (0.2017) | Acc_1: (92.96%) (16777/18048)\n",
      "Epoch: 115 | Batch_idx: 150 |  Loss_1: (0.1984) | Acc_1: (93.09%) (17992/19328)\n",
      "Epoch: 115 | Batch_idx: 160 |  Loss_1: (0.1975) | Acc_1: (93.19%) (19204/20608)\n",
      "Epoch: 115 | Batch_idx: 170 |  Loss_1: (0.1984) | Acc_1: (93.15%) (20388/21888)\n",
      "Epoch: 115 | Batch_idx: 180 |  Loss_1: (0.1987) | Acc_1: (93.12%) (21573/23168)\n",
      "Epoch: 115 | Batch_idx: 190 |  Loss_1: (0.1977) | Acc_1: (93.14%) (22772/24448)\n",
      "Epoch: 115 | Batch_idx: 200 |  Loss_1: (0.1970) | Acc_1: (93.15%) (23965/25728)\n",
      "Epoch: 115 | Batch_idx: 210 |  Loss_1: (0.1960) | Acc_1: (93.15%) (25158/27008)\n",
      "Epoch: 115 | Batch_idx: 220 |  Loss_1: (0.1962) | Acc_1: (93.14%) (26348/28288)\n",
      "Epoch: 115 | Batch_idx: 230 |  Loss_1: (0.1965) | Acc_1: (93.12%) (27533/29568)\n",
      "Epoch: 115 | Batch_idx: 240 |  Loss_1: (0.1964) | Acc_1: (93.12%) (28725/30848)\n",
      "Epoch: 115 | Batch_idx: 250 |  Loss_1: (0.1970) | Acc_1: (93.10%) (29910/32128)\n",
      "Epoch: 115 | Batch_idx: 260 |  Loss_1: (0.1967) | Acc_1: (93.09%) (31100/33408)\n",
      "Epoch: 115 | Batch_idx: 270 |  Loss_1: (0.1960) | Acc_1: (93.12%) (32302/34688)\n",
      "Epoch: 115 | Batch_idx: 280 |  Loss_1: (0.1953) | Acc_1: (93.14%) (33502/35968)\n",
      "Epoch: 115 | Batch_idx: 290 |  Loss_1: (0.1953) | Acc_1: (93.12%) (34685/37248)\n",
      "Epoch: 115 | Batch_idx: 300 |  Loss_1: (0.1966) | Acc_1: (93.08%) (35862/38528)\n",
      "Epoch: 115 | Batch_idx: 310 |  Loss_1: (0.1965) | Acc_1: (93.08%) (37054/39808)\n",
      "Epoch: 115 | Batch_idx: 320 |  Loss_1: (0.1965) | Acc_1: (93.08%) (38245/41088)\n",
      "Epoch: 115 | Batch_idx: 330 |  Loss_1: (0.1966) | Acc_1: (93.08%) (39436/42368)\n",
      "Epoch: 115 | Batch_idx: 340 |  Loss_1: (0.1976) | Acc_1: (93.06%) (40617/43648)\n",
      "Epoch: 115 | Batch_idx: 350 |  Loss_1: (0.1981) | Acc_1: (93.04%) (41799/44928)\n",
      "Epoch: 115 | Batch_idx: 360 |  Loss_1: (0.1990) | Acc_1: (92.98%) (42966/46208)\n",
      "Epoch: 115 | Batch_idx: 370 |  Loss_1: (0.1994) | Acc_1: (92.96%) (44147/47488)\n",
      "Epoch: 115 | Batch_idx: 380 |  Loss_1: (0.1992) | Acc_1: (92.96%) (45333/48768)\n",
      "Epoch: 115 | Batch_idx: 390 |  Loss_1: (0.1995) | Acc_1: (92.95%) (46473/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2978) | Acc: (91.10%) (9110/10000)\n",
      "Epoch: 116 | Batch_idx: 0 |  Loss_1: (0.1752) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 116 | Batch_idx: 10 |  Loss_1: (0.1895) | Acc_1: (93.61%) (1318/1408)\n",
      "Epoch: 116 | Batch_idx: 20 |  Loss_1: (0.1938) | Acc_1: (93.23%) (2506/2688)\n",
      "Epoch: 116 | Batch_idx: 30 |  Loss_1: (0.1976) | Acc_1: (92.74%) (3680/3968)\n",
      "Epoch: 116 | Batch_idx: 40 |  Loss_1: (0.1971) | Acc_1: (92.91%) (4876/5248)\n",
      "Epoch: 116 | Batch_idx: 50 |  Loss_1: (0.2002) | Acc_1: (92.72%) (6053/6528)\n",
      "Epoch: 116 | Batch_idx: 60 |  Loss_1: (0.1971) | Acc_1: (92.87%) (7251/7808)\n",
      "Epoch: 116 | Batch_idx: 70 |  Loss_1: (0.1998) | Acc_1: (92.79%) (8433/9088)\n",
      "Epoch: 116 | Batch_idx: 80 |  Loss_1: (0.1976) | Acc_1: (92.88%) (9630/10368)\n",
      "Epoch: 116 | Batch_idx: 90 |  Loss_1: (0.1989) | Acc_1: (92.80%) (10809/11648)\n",
      "Epoch: 116 | Batch_idx: 100 |  Loss_1: (0.2006) | Acc_1: (92.74%) (11990/12928)\n",
      "Epoch: 116 | Batch_idx: 110 |  Loss_1: (0.1999) | Acc_1: (92.79%) (13184/14208)\n",
      "Epoch: 116 | Batch_idx: 120 |  Loss_1: (0.1970) | Acc_1: (92.90%) (14389/15488)\n",
      "Epoch: 116 | Batch_idx: 130 |  Loss_1: (0.1955) | Acc_1: (92.96%) (15587/16768)\n",
      "Epoch: 116 | Batch_idx: 140 |  Loss_1: (0.1955) | Acc_1: (92.98%) (16781/18048)\n",
      "Epoch: 116 | Batch_idx: 150 |  Loss_1: (0.1946) | Acc_1: (93.03%) (17980/19328)\n",
      "Epoch: 116 | Batch_idx: 160 |  Loss_1: (0.1947) | Acc_1: (93.01%) (19168/20608)\n",
      "Epoch: 116 | Batch_idx: 170 |  Loss_1: (0.1937) | Acc_1: (93.05%) (20367/21888)\n",
      "Epoch: 116 | Batch_idx: 180 |  Loss_1: (0.1928) | Acc_1: (93.07%) (21563/23168)\n",
      "Epoch: 116 | Batch_idx: 190 |  Loss_1: (0.1909) | Acc_1: (93.13%) (22768/24448)\n",
      "Epoch: 116 | Batch_idx: 200 |  Loss_1: (0.1905) | Acc_1: (93.10%) (23953/25728)\n",
      "Epoch: 116 | Batch_idx: 210 |  Loss_1: (0.1912) | Acc_1: (93.08%) (25138/27008)\n",
      "Epoch: 116 | Batch_idx: 220 |  Loss_1: (0.1905) | Acc_1: (93.13%) (26344/28288)\n",
      "Epoch: 116 | Batch_idx: 230 |  Loss_1: (0.1912) | Acc_1: (93.08%) (27523/29568)\n",
      "Epoch: 116 | Batch_idx: 240 |  Loss_1: (0.1906) | Acc_1: (93.12%) (28725/30848)\n",
      "Epoch: 116 | Batch_idx: 250 |  Loss_1: (0.1897) | Acc_1: (93.16%) (29930/32128)\n",
      "Epoch: 116 | Batch_idx: 260 |  Loss_1: (0.1886) | Acc_1: (93.20%) (31137/33408)\n",
      "Epoch: 116 | Batch_idx: 270 |  Loss_1: (0.1891) | Acc_1: (93.23%) (32338/34688)\n",
      "Epoch: 116 | Batch_idx: 280 |  Loss_1: (0.1886) | Acc_1: (93.24%) (33538/35968)\n",
      "Epoch: 116 | Batch_idx: 290 |  Loss_1: (0.1884) | Acc_1: (93.27%) (34740/37248)\n",
      "Epoch: 116 | Batch_idx: 300 |  Loss_1: (0.1885) | Acc_1: (93.26%) (35932/38528)\n",
      "Epoch: 116 | Batch_idx: 310 |  Loss_1: (0.1883) | Acc_1: (93.25%) (37121/39808)\n",
      "Epoch: 116 | Batch_idx: 320 |  Loss_1: (0.1885) | Acc_1: (93.24%) (38311/41088)\n",
      "Epoch: 116 | Batch_idx: 330 |  Loss_1: (0.1891) | Acc_1: (93.24%) (39503/42368)\n",
      "Epoch: 116 | Batch_idx: 340 |  Loss_1: (0.1899) | Acc_1: (93.21%) (40683/43648)\n",
      "Epoch: 116 | Batch_idx: 350 |  Loss_1: (0.1897) | Acc_1: (93.21%) (41877/44928)\n",
      "Epoch: 116 | Batch_idx: 360 |  Loss_1: (0.1896) | Acc_1: (93.20%) (43065/46208)\n",
      "Epoch: 116 | Batch_idx: 370 |  Loss_1: (0.1894) | Acc_1: (93.21%) (44265/47488)\n",
      "Epoch: 116 | Batch_idx: 380 |  Loss_1: (0.1894) | Acc_1: (93.21%) (45456/48768)\n",
      "Epoch: 116 | Batch_idx: 390 |  Loss_1: (0.1897) | Acc_1: (93.22%) (46608/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3201) | Acc: (91.28%) (9128/10000)\n",
      "Epoch: 117 | Batch_idx: 0 |  Loss_1: (0.1511) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 117 | Batch_idx: 10 |  Loss_1: (0.1843) | Acc_1: (93.61%) (1318/1408)\n",
      "Epoch: 117 | Batch_idx: 20 |  Loss_1: (0.1795) | Acc_1: (93.90%) (2524/2688)\n",
      "Epoch: 117 | Batch_idx: 30 |  Loss_1: (0.1812) | Acc_1: (93.52%) (3711/3968)\n",
      "Epoch: 117 | Batch_idx: 40 |  Loss_1: (0.1802) | Acc_1: (93.37%) (4900/5248)\n",
      "Epoch: 117 | Batch_idx: 50 |  Loss_1: (0.1784) | Acc_1: (93.57%) (6108/6528)\n",
      "Epoch: 117 | Batch_idx: 60 |  Loss_1: (0.1762) | Acc_1: (93.67%) (7314/7808)\n",
      "Epoch: 117 | Batch_idx: 70 |  Loss_1: (0.1787) | Acc_1: (93.55%) (8502/9088)\n",
      "Epoch: 117 | Batch_idx: 80 |  Loss_1: (0.1769) | Acc_1: (93.66%) (9711/10368)\n",
      "Epoch: 117 | Batch_idx: 90 |  Loss_1: (0.1773) | Acc_1: (93.71%) (10915/11648)\n",
      "Epoch: 117 | Batch_idx: 100 |  Loss_1: (0.1795) | Acc_1: (93.66%) (12109/12928)\n",
      "Epoch: 117 | Batch_idx: 110 |  Loss_1: (0.1825) | Acc_1: (93.50%) (13285/14208)\n",
      "Epoch: 117 | Batch_idx: 120 |  Loss_1: (0.1839) | Acc_1: (93.42%) (14469/15488)\n",
      "Epoch: 117 | Batch_idx: 130 |  Loss_1: (0.1848) | Acc_1: (93.36%) (15655/16768)\n",
      "Epoch: 117 | Batch_idx: 140 |  Loss_1: (0.1850) | Acc_1: (93.37%) (16851/18048)\n",
      "Epoch: 117 | Batch_idx: 150 |  Loss_1: (0.1837) | Acc_1: (93.41%) (18055/19328)\n",
      "Epoch: 117 | Batch_idx: 160 |  Loss_1: (0.1828) | Acc_1: (93.43%) (19255/20608)\n",
      "Epoch: 117 | Batch_idx: 170 |  Loss_1: (0.1834) | Acc_1: (93.44%) (20453/21888)\n",
      "Epoch: 117 | Batch_idx: 180 |  Loss_1: (0.1840) | Acc_1: (93.41%) (21641/23168)\n",
      "Epoch: 117 | Batch_idx: 190 |  Loss_1: (0.1847) | Acc_1: (93.38%) (22829/24448)\n",
      "Epoch: 117 | Batch_idx: 200 |  Loss_1: (0.1852) | Acc_1: (93.34%) (24015/25728)\n",
      "Epoch: 117 | Batch_idx: 210 |  Loss_1: (0.1843) | Acc_1: (93.36%) (25216/27008)\n",
      "Epoch: 117 | Batch_idx: 220 |  Loss_1: (0.1855) | Acc_1: (93.30%) (26394/28288)\n",
      "Epoch: 117 | Batch_idx: 230 |  Loss_1: (0.1852) | Acc_1: (93.32%) (27592/29568)\n",
      "Epoch: 117 | Batch_idx: 240 |  Loss_1: (0.1854) | Acc_1: (93.33%) (28791/30848)\n",
      "Epoch: 117 | Batch_idx: 250 |  Loss_1: (0.1852) | Acc_1: (93.34%) (29987/32128)\n",
      "Epoch: 117 | Batch_idx: 260 |  Loss_1: (0.1853) | Acc_1: (93.33%) (31180/33408)\n",
      "Epoch: 117 | Batch_idx: 270 |  Loss_1: (0.1863) | Acc_1: (93.30%) (32365/34688)\n",
      "Epoch: 117 | Batch_idx: 280 |  Loss_1: (0.1869) | Acc_1: (93.27%) (33546/35968)\n",
      "Epoch: 117 | Batch_idx: 290 |  Loss_1: (0.1861) | Acc_1: (93.28%) (34746/37248)\n",
      "Epoch: 117 | Batch_idx: 300 |  Loss_1: (0.1873) | Acc_1: (93.23%) (35921/38528)\n",
      "Epoch: 117 | Batch_idx: 310 |  Loss_1: (0.1873) | Acc_1: (93.21%) (37107/39808)\n",
      "Epoch: 117 | Batch_idx: 320 |  Loss_1: (0.1870) | Acc_1: (93.24%) (38310/41088)\n",
      "Epoch: 117 | Batch_idx: 330 |  Loss_1: (0.1872) | Acc_1: (93.24%) (39503/42368)\n",
      "Epoch: 117 | Batch_idx: 340 |  Loss_1: (0.1885) | Acc_1: (93.16%) (40664/43648)\n",
      "Epoch: 117 | Batch_idx: 350 |  Loss_1: (0.1882) | Acc_1: (93.18%) (41865/44928)\n",
      "Epoch: 117 | Batch_idx: 360 |  Loss_1: (0.1887) | Acc_1: (93.17%) (43053/46208)\n",
      "Epoch: 117 | Batch_idx: 370 |  Loss_1: (0.1889) | Acc_1: (93.18%) (44251/47488)\n",
      "Epoch: 117 | Batch_idx: 380 |  Loss_1: (0.1890) | Acc_1: (93.19%) (45449/48768)\n",
      "Epoch: 117 | Batch_idx: 390 |  Loss_1: (0.1889) | Acc_1: (93.20%) (46601/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3107) | Acc: (91.07%) (9107/10000)\n",
      "Epoch: 118 | Batch_idx: 0 |  Loss_1: (0.1925) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 118 | Batch_idx: 10 |  Loss_1: (0.1785) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 118 | Batch_idx: 20 |  Loss_1: (0.1922) | Acc_1: (93.12%) (2503/2688)\n",
      "Epoch: 118 | Batch_idx: 30 |  Loss_1: (0.1814) | Acc_1: (93.52%) (3711/3968)\n",
      "Epoch: 118 | Batch_idx: 40 |  Loss_1: (0.1799) | Acc_1: (93.48%) (4906/5248)\n",
      "Epoch: 118 | Batch_idx: 50 |  Loss_1: (0.1809) | Acc_1: (93.58%) (6109/6528)\n",
      "Epoch: 118 | Batch_idx: 60 |  Loss_1: (0.1797) | Acc_1: (93.67%) (7314/7808)\n",
      "Epoch: 118 | Batch_idx: 70 |  Loss_1: (0.1772) | Acc_1: (93.74%) (8519/9088)\n",
      "Epoch: 118 | Batch_idx: 80 |  Loss_1: (0.1765) | Acc_1: (93.79%) (9724/10368)\n",
      "Epoch: 118 | Batch_idx: 90 |  Loss_1: (0.1757) | Acc_1: (93.77%) (10922/11648)\n",
      "Epoch: 118 | Batch_idx: 100 |  Loss_1: (0.1745) | Acc_1: (93.83%) (12130/12928)\n",
      "Epoch: 118 | Batch_idx: 110 |  Loss_1: (0.1730) | Acc_1: (93.87%) (13337/14208)\n",
      "Epoch: 118 | Batch_idx: 120 |  Loss_1: (0.1720) | Acc_1: (93.89%) (14542/15488)\n",
      "Epoch: 118 | Batch_idx: 130 |  Loss_1: (0.1726) | Acc_1: (93.87%) (15740/16768)\n",
      "Epoch: 118 | Batch_idx: 140 |  Loss_1: (0.1741) | Acc_1: (93.80%) (16929/18048)\n",
      "Epoch: 118 | Batch_idx: 150 |  Loss_1: (0.1761) | Acc_1: (93.71%) (18113/19328)\n",
      "Epoch: 118 | Batch_idx: 160 |  Loss_1: (0.1758) | Acc_1: (93.75%) (19319/20608)\n",
      "Epoch: 118 | Batch_idx: 170 |  Loss_1: (0.1771) | Acc_1: (93.69%) (20506/21888)\n",
      "Epoch: 118 | Batch_idx: 180 |  Loss_1: (0.1794) | Acc_1: (93.59%) (21683/23168)\n",
      "Epoch: 118 | Batch_idx: 190 |  Loss_1: (0.1787) | Acc_1: (93.64%) (22892/24448)\n",
      "Epoch: 118 | Batch_idx: 200 |  Loss_1: (0.1788) | Acc_1: (93.63%) (24089/25728)\n",
      "Epoch: 118 | Batch_idx: 210 |  Loss_1: (0.1803) | Acc_1: (93.57%) (25271/27008)\n",
      "Epoch: 118 | Batch_idx: 220 |  Loss_1: (0.1810) | Acc_1: (93.56%) (26467/28288)\n",
      "Epoch: 118 | Batch_idx: 230 |  Loss_1: (0.1819) | Acc_1: (93.55%) (27661/29568)\n",
      "Epoch: 118 | Batch_idx: 240 |  Loss_1: (0.1829) | Acc_1: (93.49%) (28839/30848)\n",
      "Epoch: 118 | Batch_idx: 250 |  Loss_1: (0.1830) | Acc_1: (93.46%) (30027/32128)\n",
      "Epoch: 118 | Batch_idx: 260 |  Loss_1: (0.1828) | Acc_1: (93.47%) (31225/33408)\n",
      "Epoch: 118 | Batch_idx: 270 |  Loss_1: (0.1828) | Acc_1: (93.46%) (32419/34688)\n",
      "Epoch: 118 | Batch_idx: 280 |  Loss_1: (0.1828) | Acc_1: (93.46%) (33616/35968)\n",
      "Epoch: 118 | Batch_idx: 290 |  Loss_1: (0.1825) | Acc_1: (93.47%) (34815/37248)\n",
      "Epoch: 118 | Batch_idx: 300 |  Loss_1: (0.1830) | Acc_1: (93.44%) (36001/38528)\n",
      "Epoch: 118 | Batch_idx: 310 |  Loss_1: (0.1841) | Acc_1: (93.40%) (37182/39808)\n",
      "Epoch: 118 | Batch_idx: 320 |  Loss_1: (0.1840) | Acc_1: (93.41%) (38379/41088)\n",
      "Epoch: 118 | Batch_idx: 330 |  Loss_1: (0.1842) | Acc_1: (93.40%) (39572/42368)\n",
      "Epoch: 118 | Batch_idx: 340 |  Loss_1: (0.1840) | Acc_1: (93.43%) (40781/43648)\n",
      "Epoch: 118 | Batch_idx: 350 |  Loss_1: (0.1840) | Acc_1: (93.43%) (41975/44928)\n",
      "Epoch: 118 | Batch_idx: 360 |  Loss_1: (0.1836) | Acc_1: (93.42%) (43169/46208)\n",
      "Epoch: 118 | Batch_idx: 370 |  Loss_1: (0.1834) | Acc_1: (93.43%) (44369/47488)\n",
      "Epoch: 118 | Batch_idx: 380 |  Loss_1: (0.1837) | Acc_1: (93.42%) (45560/48768)\n",
      "Epoch: 118 | Batch_idx: 390 |  Loss_1: (0.1836) | Acc_1: (93.43%) (46714/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3222) | Acc: (91.06%) (9106/10000)\n",
      "Epoch: 119 | Batch_idx: 0 |  Loss_1: (0.1770) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 119 | Batch_idx: 10 |  Loss_1: (0.1994) | Acc_1: (93.68%) (1319/1408)\n",
      "Epoch: 119 | Batch_idx: 20 |  Loss_1: (0.1851) | Acc_1: (93.64%) (2517/2688)\n",
      "Epoch: 119 | Batch_idx: 30 |  Loss_1: (0.1846) | Acc_1: (93.60%) (3714/3968)\n",
      "Epoch: 119 | Batch_idx: 40 |  Loss_1: (0.1812) | Acc_1: (93.67%) (4916/5248)\n",
      "Epoch: 119 | Batch_idx: 50 |  Loss_1: (0.1789) | Acc_1: (93.66%) (6114/6528)\n",
      "Epoch: 119 | Batch_idx: 60 |  Loss_1: (0.1821) | Acc_1: (93.51%) (7301/7808)\n",
      "Epoch: 119 | Batch_idx: 70 |  Loss_1: (0.1809) | Acc_1: (93.45%) (8493/9088)\n",
      "Epoch: 119 | Batch_idx: 80 |  Loss_1: (0.1820) | Acc_1: (93.41%) (9685/10368)\n",
      "Epoch: 119 | Batch_idx: 90 |  Loss_1: (0.1829) | Acc_1: (93.42%) (10882/11648)\n",
      "Epoch: 119 | Batch_idx: 100 |  Loss_1: (0.1828) | Acc_1: (93.39%) (12074/12928)\n",
      "Epoch: 119 | Batch_idx: 110 |  Loss_1: (0.1825) | Acc_1: (93.43%) (13275/14208)\n",
      "Epoch: 119 | Batch_idx: 120 |  Loss_1: (0.1830) | Acc_1: (93.34%) (14457/15488)\n",
      "Epoch: 119 | Batch_idx: 130 |  Loss_1: (0.1816) | Acc_1: (93.38%) (15658/16768)\n",
      "Epoch: 119 | Batch_idx: 140 |  Loss_1: (0.1804) | Acc_1: (93.48%) (16872/18048)\n",
      "Epoch: 119 | Batch_idx: 150 |  Loss_1: (0.1809) | Acc_1: (93.48%) (18068/19328)\n",
      "Epoch: 119 | Batch_idx: 160 |  Loss_1: (0.1818) | Acc_1: (93.45%) (19258/20608)\n",
      "Epoch: 119 | Batch_idx: 170 |  Loss_1: (0.1827) | Acc_1: (93.41%) (20445/21888)\n",
      "Epoch: 119 | Batch_idx: 180 |  Loss_1: (0.1848) | Acc_1: (93.34%) (21626/23168)\n",
      "Epoch: 119 | Batch_idx: 190 |  Loss_1: (0.1855) | Acc_1: (93.34%) (22819/24448)\n",
      "Epoch: 119 | Batch_idx: 200 |  Loss_1: (0.1850) | Acc_1: (93.35%) (24017/25728)\n",
      "Epoch: 119 | Batch_idx: 210 |  Loss_1: (0.1848) | Acc_1: (93.33%) (25207/27008)\n",
      "Epoch: 119 | Batch_idx: 220 |  Loss_1: (0.1859) | Acc_1: (93.32%) (26397/28288)\n",
      "Epoch: 119 | Batch_idx: 230 |  Loss_1: (0.1857) | Acc_1: (93.32%) (27594/29568)\n",
      "Epoch: 119 | Batch_idx: 240 |  Loss_1: (0.1858) | Acc_1: (93.32%) (28788/30848)\n",
      "Epoch: 119 | Batch_idx: 250 |  Loss_1: (0.1862) | Acc_1: (93.30%) (29974/32128)\n",
      "Epoch: 119 | Batch_idx: 260 |  Loss_1: (0.1854) | Acc_1: (93.31%) (31173/33408)\n",
      "Epoch: 119 | Batch_idx: 270 |  Loss_1: (0.1857) | Acc_1: (93.31%) (32366/34688)\n",
      "Epoch: 119 | Batch_idx: 280 |  Loss_1: (0.1854) | Acc_1: (93.34%) (33571/35968)\n",
      "Epoch: 119 | Batch_idx: 290 |  Loss_1: (0.1866) | Acc_1: (93.29%) (34749/37248)\n",
      "Epoch: 119 | Batch_idx: 300 |  Loss_1: (0.1871) | Acc_1: (93.27%) (35935/38528)\n",
      "Epoch: 119 | Batch_idx: 310 |  Loss_1: (0.1872) | Acc_1: (93.26%) (37126/39808)\n",
      "Epoch: 119 | Batch_idx: 320 |  Loss_1: (0.1871) | Acc_1: (93.26%) (38320/41088)\n",
      "Epoch: 119 | Batch_idx: 330 |  Loss_1: (0.1869) | Acc_1: (93.28%) (39522/42368)\n",
      "Epoch: 119 | Batch_idx: 340 |  Loss_1: (0.1864) | Acc_1: (93.29%) (40718/43648)\n",
      "Epoch: 119 | Batch_idx: 350 |  Loss_1: (0.1866) | Acc_1: (93.27%) (41904/44928)\n",
      "Epoch: 119 | Batch_idx: 360 |  Loss_1: (0.1867) | Acc_1: (93.28%) (43103/46208)\n",
      "Epoch: 119 | Batch_idx: 370 |  Loss_1: (0.1867) | Acc_1: (93.28%) (44295/47488)\n",
      "Epoch: 119 | Batch_idx: 380 |  Loss_1: (0.1864) | Acc_1: (93.26%) (45483/48768)\n",
      "Epoch: 119 | Batch_idx: 390 |  Loss_1: (0.1863) | Acc_1: (93.25%) (46627/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3130) | Acc: (91.22%) (9122/10000)\n",
      "Epoch: 120 | Batch_idx: 0 |  Loss_1: (0.1252) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 120 | Batch_idx: 10 |  Loss_1: (0.1916) | Acc_1: (92.76%) (1306/1408)\n",
      "Epoch: 120 | Batch_idx: 20 |  Loss_1: (0.1957) | Acc_1: (92.75%) (2493/2688)\n",
      "Epoch: 120 | Batch_idx: 30 |  Loss_1: (0.1948) | Acc_1: (92.79%) (3682/3968)\n",
      "Epoch: 120 | Batch_idx: 40 |  Loss_1: (0.1924) | Acc_1: (92.84%) (4872/5248)\n",
      "Epoch: 120 | Batch_idx: 50 |  Loss_1: (0.1978) | Acc_1: (92.74%) (6054/6528)\n",
      "Epoch: 120 | Batch_idx: 60 |  Loss_1: (0.1957) | Acc_1: (92.85%) (7250/7808)\n",
      "Epoch: 120 | Batch_idx: 70 |  Loss_1: (0.1974) | Acc_1: (92.84%) (8437/9088)\n",
      "Epoch: 120 | Batch_idx: 80 |  Loss_1: (0.1957) | Acc_1: (93.01%) (9643/10368)\n",
      "Epoch: 120 | Batch_idx: 90 |  Loss_1: (0.1941) | Acc_1: (93.08%) (10842/11648)\n",
      "Epoch: 120 | Batch_idx: 100 |  Loss_1: (0.1919) | Acc_1: (93.16%) (12044/12928)\n",
      "Epoch: 120 | Batch_idx: 110 |  Loss_1: (0.1934) | Acc_1: (93.09%) (13226/14208)\n",
      "Epoch: 120 | Batch_idx: 120 |  Loss_1: (0.1928) | Acc_1: (93.12%) (14422/15488)\n",
      "Epoch: 120 | Batch_idx: 130 |  Loss_1: (0.1929) | Acc_1: (93.12%) (15615/16768)\n",
      "Epoch: 120 | Batch_idx: 140 |  Loss_1: (0.1931) | Acc_1: (93.12%) (16806/18048)\n",
      "Epoch: 120 | Batch_idx: 150 |  Loss_1: (0.1938) | Acc_1: (93.06%) (17986/19328)\n",
      "Epoch: 120 | Batch_idx: 160 |  Loss_1: (0.1928) | Acc_1: (93.08%) (19182/20608)\n",
      "Epoch: 120 | Batch_idx: 170 |  Loss_1: (0.1918) | Acc_1: (93.09%) (20376/21888)\n",
      "Epoch: 120 | Batch_idx: 180 |  Loss_1: (0.1905) | Acc_1: (93.12%) (21575/23168)\n",
      "Epoch: 120 | Batch_idx: 190 |  Loss_1: (0.1891) | Acc_1: (93.17%) (22777/24448)\n",
      "Epoch: 120 | Batch_idx: 200 |  Loss_1: (0.1885) | Acc_1: (93.16%) (23969/25728)\n",
      "Epoch: 120 | Batch_idx: 210 |  Loss_1: (0.1892) | Acc_1: (93.15%) (25157/27008)\n",
      "Epoch: 120 | Batch_idx: 220 |  Loss_1: (0.1885) | Acc_1: (93.18%) (26359/28288)\n",
      "Epoch: 120 | Batch_idx: 230 |  Loss_1: (0.1884) | Acc_1: (93.20%) (27557/29568)\n",
      "Epoch: 120 | Batch_idx: 240 |  Loss_1: (0.1878) | Acc_1: (93.23%) (28760/30848)\n",
      "Epoch: 120 | Batch_idx: 250 |  Loss_1: (0.1886) | Acc_1: (93.22%) (29951/32128)\n",
      "Epoch: 120 | Batch_idx: 260 |  Loss_1: (0.1881) | Acc_1: (93.22%) (31142/33408)\n",
      "Epoch: 120 | Batch_idx: 270 |  Loss_1: (0.1865) | Acc_1: (93.27%) (32355/34688)\n",
      "Epoch: 120 | Batch_idx: 280 |  Loss_1: (0.1869) | Acc_1: (93.29%) (33553/35968)\n",
      "Epoch: 120 | Batch_idx: 290 |  Loss_1: (0.1866) | Acc_1: (93.29%) (34750/37248)\n",
      "Epoch: 120 | Batch_idx: 300 |  Loss_1: (0.1865) | Acc_1: (93.29%) (35944/38528)\n",
      "Epoch: 120 | Batch_idx: 310 |  Loss_1: (0.1865) | Acc_1: (93.29%) (37135/39808)\n",
      "Epoch: 120 | Batch_idx: 320 |  Loss_1: (0.1863) | Acc_1: (93.29%) (38329/41088)\n",
      "Epoch: 120 | Batch_idx: 330 |  Loss_1: (0.1865) | Acc_1: (93.26%) (39514/42368)\n",
      "Epoch: 120 | Batch_idx: 340 |  Loss_1: (0.1858) | Acc_1: (93.29%) (40720/43648)\n",
      "Epoch: 120 | Batch_idx: 350 |  Loss_1: (0.1858) | Acc_1: (93.28%) (41911/44928)\n",
      "Epoch: 120 | Batch_idx: 360 |  Loss_1: (0.1863) | Acc_1: (93.26%) (43093/46208)\n",
      "Epoch: 120 | Batch_idx: 370 |  Loss_1: (0.1861) | Acc_1: (93.27%) (44294/47488)\n",
      "Epoch: 120 | Batch_idx: 380 |  Loss_1: (0.1863) | Acc_1: (93.25%) (45477/48768)\n",
      "Epoch: 120 | Batch_idx: 390 |  Loss_1: (0.1871) | Acc_1: (93.25%) (46623/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3328) | Acc: (90.76%) (9076/10000)\n",
      "Epoch: 121 | Batch_idx: 0 |  Loss_1: (0.1675) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 121 | Batch_idx: 10 |  Loss_1: (0.1696) | Acc_1: (93.54%) (1317/1408)\n",
      "Epoch: 121 | Batch_idx: 20 |  Loss_1: (0.1836) | Acc_1: (93.34%) (2509/2688)\n",
      "Epoch: 121 | Batch_idx: 30 |  Loss_1: (0.1806) | Acc_1: (93.42%) (3707/3968)\n",
      "Epoch: 121 | Batch_idx: 40 |  Loss_1: (0.1753) | Acc_1: (93.54%) (4909/5248)\n",
      "Epoch: 121 | Batch_idx: 50 |  Loss_1: (0.1730) | Acc_1: (93.57%) (6108/6528)\n",
      "Epoch: 121 | Batch_idx: 60 |  Loss_1: (0.1715) | Acc_1: (93.66%) (7313/7808)\n",
      "Epoch: 121 | Batch_idx: 70 |  Loss_1: (0.1741) | Acc_1: (93.56%) (8503/9088)\n",
      "Epoch: 121 | Batch_idx: 80 |  Loss_1: (0.1748) | Acc_1: (93.60%) (9704/10368)\n",
      "Epoch: 121 | Batch_idx: 90 |  Loss_1: (0.1746) | Acc_1: (93.62%) (10905/11648)\n",
      "Epoch: 121 | Batch_idx: 100 |  Loss_1: (0.1783) | Acc_1: (93.48%) (12085/12928)\n",
      "Epoch: 121 | Batch_idx: 110 |  Loss_1: (0.1818) | Acc_1: (93.35%) (13263/14208)\n",
      "Epoch: 121 | Batch_idx: 120 |  Loss_1: (0.1825) | Acc_1: (93.31%) (14452/15488)\n",
      "Epoch: 121 | Batch_idx: 130 |  Loss_1: (0.1802) | Acc_1: (93.42%) (15664/16768)\n",
      "Epoch: 121 | Batch_idx: 140 |  Loss_1: (0.1815) | Acc_1: (93.38%) (16853/18048)\n",
      "Epoch: 121 | Batch_idx: 150 |  Loss_1: (0.1810) | Acc_1: (93.42%) (18056/19328)\n",
      "Epoch: 121 | Batch_idx: 160 |  Loss_1: (0.1826) | Acc_1: (93.38%) (19243/20608)\n",
      "Epoch: 121 | Batch_idx: 170 |  Loss_1: (0.1834) | Acc_1: (93.35%) (20432/21888)\n",
      "Epoch: 121 | Batch_idx: 180 |  Loss_1: (0.1827) | Acc_1: (93.36%) (21630/23168)\n",
      "Epoch: 121 | Batch_idx: 190 |  Loss_1: (0.1834) | Acc_1: (93.35%) (22821/24448)\n",
      "Epoch: 121 | Batch_idx: 200 |  Loss_1: (0.1837) | Acc_1: (93.35%) (24016/25728)\n",
      "Epoch: 121 | Batch_idx: 210 |  Loss_1: (0.1835) | Acc_1: (93.36%) (25214/27008)\n",
      "Epoch: 121 | Batch_idx: 220 |  Loss_1: (0.1832) | Acc_1: (93.38%) (26414/28288)\n",
      "Epoch: 121 | Batch_idx: 230 |  Loss_1: (0.1811) | Acc_1: (93.46%) (27634/29568)\n",
      "Epoch: 121 | Batch_idx: 240 |  Loss_1: (0.1811) | Acc_1: (93.46%) (28832/30848)\n",
      "Epoch: 121 | Batch_idx: 250 |  Loss_1: (0.1799) | Acc_1: (93.50%) (30040/32128)\n",
      "Epoch: 121 | Batch_idx: 260 |  Loss_1: (0.1804) | Acc_1: (93.47%) (31226/33408)\n",
      "Epoch: 121 | Batch_idx: 270 |  Loss_1: (0.1810) | Acc_1: (93.44%) (32411/34688)\n",
      "Epoch: 121 | Batch_idx: 280 |  Loss_1: (0.1821) | Acc_1: (93.42%) (33602/35968)\n",
      "Epoch: 121 | Batch_idx: 290 |  Loss_1: (0.1824) | Acc_1: (93.41%) (34793/37248)\n",
      "Epoch: 121 | Batch_idx: 300 |  Loss_1: (0.1822) | Acc_1: (93.40%) (35984/38528)\n",
      "Epoch: 121 | Batch_idx: 310 |  Loss_1: (0.1825) | Acc_1: (93.40%) (37179/39808)\n",
      "Epoch: 121 | Batch_idx: 320 |  Loss_1: (0.1828) | Acc_1: (93.37%) (38364/41088)\n",
      "Epoch: 121 | Batch_idx: 330 |  Loss_1: (0.1835) | Acc_1: (93.37%) (39557/42368)\n",
      "Epoch: 121 | Batch_idx: 340 |  Loss_1: (0.1834) | Acc_1: (93.36%) (40751/43648)\n",
      "Epoch: 121 | Batch_idx: 350 |  Loss_1: (0.1828) | Acc_1: (93.38%) (41953/44928)\n",
      "Epoch: 121 | Batch_idx: 360 |  Loss_1: (0.1833) | Acc_1: (93.37%) (43146/46208)\n",
      "Epoch: 121 | Batch_idx: 370 |  Loss_1: (0.1840) | Acc_1: (93.35%) (44331/47488)\n",
      "Epoch: 121 | Batch_idx: 380 |  Loss_1: (0.1834) | Acc_1: (93.37%) (45534/48768)\n",
      "Epoch: 121 | Batch_idx: 390 |  Loss_1: (0.1842) | Acc_1: (93.35%) (46677/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3119) | Acc: (91.32%) (9132/10000)\n",
      "Epoch: 122 | Batch_idx: 0 |  Loss_1: (0.1142) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 122 | Batch_idx: 10 |  Loss_1: (0.1796) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 122 | Batch_idx: 20 |  Loss_1: (0.1881) | Acc_1: (93.30%) (2508/2688)\n",
      "Epoch: 122 | Batch_idx: 30 |  Loss_1: (0.1826) | Acc_1: (93.42%) (3707/3968)\n",
      "Epoch: 122 | Batch_idx: 40 |  Loss_1: (0.1812) | Acc_1: (93.50%) (4907/5248)\n",
      "Epoch: 122 | Batch_idx: 50 |  Loss_1: (0.1782) | Acc_1: (93.66%) (6114/6528)\n",
      "Epoch: 122 | Batch_idx: 60 |  Loss_1: (0.1783) | Acc_1: (93.57%) (7306/7808)\n",
      "Epoch: 122 | Batch_idx: 70 |  Loss_1: (0.1735) | Acc_1: (93.76%) (8521/9088)\n",
      "Epoch: 122 | Batch_idx: 80 |  Loss_1: (0.1762) | Acc_1: (93.65%) (9710/10368)\n",
      "Epoch: 122 | Batch_idx: 90 |  Loss_1: (0.1763) | Acc_1: (93.60%) (10903/11648)\n",
      "Epoch: 122 | Batch_idx: 100 |  Loss_1: (0.1789) | Acc_1: (93.49%) (12087/12928)\n",
      "Epoch: 122 | Batch_idx: 110 |  Loss_1: (0.1785) | Acc_1: (93.50%) (13285/14208)\n",
      "Epoch: 122 | Batch_idx: 120 |  Loss_1: (0.1800) | Acc_1: (93.48%) (14478/15488)\n",
      "Epoch: 122 | Batch_idx: 130 |  Loss_1: (0.1818) | Acc_1: (93.45%) (15669/16768)\n",
      "Epoch: 122 | Batch_idx: 140 |  Loss_1: (0.1823) | Acc_1: (93.42%) (16861/18048)\n",
      "Epoch: 122 | Batch_idx: 150 |  Loss_1: (0.1834) | Acc_1: (93.38%) (18049/19328)\n",
      "Epoch: 122 | Batch_idx: 160 |  Loss_1: (0.1823) | Acc_1: (93.41%) (19250/20608)\n",
      "Epoch: 122 | Batch_idx: 170 |  Loss_1: (0.1807) | Acc_1: (93.49%) (20464/21888)\n",
      "Epoch: 122 | Batch_idx: 180 |  Loss_1: (0.1795) | Acc_1: (93.56%) (21675/23168)\n",
      "Epoch: 122 | Batch_idx: 190 |  Loss_1: (0.1799) | Acc_1: (93.55%) (22870/24448)\n",
      "Epoch: 122 | Batch_idx: 200 |  Loss_1: (0.1817) | Acc_1: (93.49%) (24053/25728)\n",
      "Epoch: 122 | Batch_idx: 210 |  Loss_1: (0.1818) | Acc_1: (93.48%) (25247/27008)\n",
      "Epoch: 122 | Batch_idx: 220 |  Loss_1: (0.1814) | Acc_1: (93.48%) (26443/28288)\n",
      "Epoch: 122 | Batch_idx: 230 |  Loss_1: (0.1805) | Acc_1: (93.51%) (27649/29568)\n",
      "Epoch: 122 | Batch_idx: 240 |  Loss_1: (0.1804) | Acc_1: (93.53%) (28853/30848)\n",
      "Epoch: 122 | Batch_idx: 250 |  Loss_1: (0.1812) | Acc_1: (93.47%) (30031/32128)\n",
      "Epoch: 122 | Batch_idx: 260 |  Loss_1: (0.1812) | Acc_1: (93.48%) (31229/33408)\n",
      "Epoch: 122 | Batch_idx: 270 |  Loss_1: (0.1803) | Acc_1: (93.51%) (32438/34688)\n",
      "Epoch: 122 | Batch_idx: 280 |  Loss_1: (0.1804) | Acc_1: (93.53%) (33641/35968)\n",
      "Epoch: 122 | Batch_idx: 290 |  Loss_1: (0.1807) | Acc_1: (93.52%) (34835/37248)\n",
      "Epoch: 122 | Batch_idx: 300 |  Loss_1: (0.1808) | Acc_1: (93.50%) (36025/38528)\n",
      "Epoch: 122 | Batch_idx: 310 |  Loss_1: (0.1810) | Acc_1: (93.50%) (37219/39808)\n",
      "Epoch: 122 | Batch_idx: 320 |  Loss_1: (0.1804) | Acc_1: (93.52%) (38426/41088)\n",
      "Epoch: 122 | Batch_idx: 330 |  Loss_1: (0.1808) | Acc_1: (93.51%) (39617/42368)\n",
      "Epoch: 122 | Batch_idx: 340 |  Loss_1: (0.1813) | Acc_1: (93.49%) (40807/43648)\n",
      "Epoch: 122 | Batch_idx: 350 |  Loss_1: (0.1811) | Acc_1: (93.50%) (42007/44928)\n",
      "Epoch: 122 | Batch_idx: 360 |  Loss_1: (0.1815) | Acc_1: (93.50%) (43203/46208)\n",
      "Epoch: 122 | Batch_idx: 370 |  Loss_1: (0.1819) | Acc_1: (93.47%) (44389/47488)\n",
      "Epoch: 122 | Batch_idx: 380 |  Loss_1: (0.1820) | Acc_1: (93.47%) (45582/48768)\n",
      "Epoch: 122 | Batch_idx: 390 |  Loss_1: (0.1826) | Acc_1: (93.45%) (46723/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3173) | Acc: (91.04%) (9104/10000)\n",
      "Epoch: 123 | Batch_idx: 0 |  Loss_1: (0.1855) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 123 | Batch_idx: 10 |  Loss_1: (0.1739) | Acc_1: (93.96%) (1323/1408)\n",
      "Epoch: 123 | Batch_idx: 20 |  Loss_1: (0.1651) | Acc_1: (94.01%) (2527/2688)\n",
      "Epoch: 123 | Batch_idx: 30 |  Loss_1: (0.1740) | Acc_1: (93.47%) (3709/3968)\n",
      "Epoch: 123 | Batch_idx: 40 |  Loss_1: (0.1751) | Acc_1: (93.43%) (4903/5248)\n",
      "Epoch: 123 | Batch_idx: 50 |  Loss_1: (0.1758) | Acc_1: (93.43%) (6099/6528)\n",
      "Epoch: 123 | Batch_idx: 60 |  Loss_1: (0.1781) | Acc_1: (93.39%) (7292/7808)\n",
      "Epoch: 123 | Batch_idx: 70 |  Loss_1: (0.1791) | Acc_1: (93.44%) (8492/9088)\n",
      "Epoch: 123 | Batch_idx: 80 |  Loss_1: (0.1825) | Acc_1: (93.36%) (9680/10368)\n",
      "Epoch: 123 | Batch_idx: 90 |  Loss_1: (0.1836) | Acc_1: (93.30%) (10868/11648)\n",
      "Epoch: 123 | Batch_idx: 100 |  Loss_1: (0.1804) | Acc_1: (93.41%) (12076/12928)\n",
      "Epoch: 123 | Batch_idx: 110 |  Loss_1: (0.1780) | Acc_1: (93.55%) (13291/14208)\n",
      "Epoch: 123 | Batch_idx: 120 |  Loss_1: (0.1754) | Acc_1: (93.66%) (14506/15488)\n",
      "Epoch: 123 | Batch_idx: 130 |  Loss_1: (0.1785) | Acc_1: (93.54%) (15684/16768)\n",
      "Epoch: 123 | Batch_idx: 140 |  Loss_1: (0.1784) | Acc_1: (93.52%) (16878/18048)\n",
      "Epoch: 123 | Batch_idx: 150 |  Loss_1: (0.1776) | Acc_1: (93.55%) (18082/19328)\n",
      "Epoch: 123 | Batch_idx: 160 |  Loss_1: (0.1786) | Acc_1: (93.52%) (19273/20608)\n",
      "Epoch: 123 | Batch_idx: 170 |  Loss_1: (0.1784) | Acc_1: (93.52%) (20469/21888)\n",
      "Epoch: 123 | Batch_idx: 180 |  Loss_1: (0.1791) | Acc_1: (93.47%) (21656/23168)\n",
      "Epoch: 123 | Batch_idx: 190 |  Loss_1: (0.1786) | Acc_1: (93.48%) (22854/24448)\n",
      "Epoch: 123 | Batch_idx: 200 |  Loss_1: (0.1787) | Acc_1: (93.49%) (24052/25728)\n",
      "Epoch: 123 | Batch_idx: 210 |  Loss_1: (0.1784) | Acc_1: (93.51%) (25256/27008)\n",
      "Epoch: 123 | Batch_idx: 220 |  Loss_1: (0.1794) | Acc_1: (93.46%) (26437/28288)\n",
      "Epoch: 123 | Batch_idx: 230 |  Loss_1: (0.1801) | Acc_1: (93.44%) (27629/29568)\n",
      "Epoch: 123 | Batch_idx: 240 |  Loss_1: (0.1803) | Acc_1: (93.43%) (28822/30848)\n",
      "Epoch: 123 | Batch_idx: 250 |  Loss_1: (0.1797) | Acc_1: (93.47%) (30031/32128)\n",
      "Epoch: 123 | Batch_idx: 260 |  Loss_1: (0.1805) | Acc_1: (93.46%) (31223/33408)\n",
      "Epoch: 123 | Batch_idx: 270 |  Loss_1: (0.1809) | Acc_1: (93.45%) (32417/34688)\n",
      "Epoch: 123 | Batch_idx: 280 |  Loss_1: (0.1809) | Acc_1: (93.47%) (33619/35968)\n",
      "Epoch: 123 | Batch_idx: 290 |  Loss_1: (0.1812) | Acc_1: (93.47%) (34815/37248)\n",
      "Epoch: 123 | Batch_idx: 300 |  Loss_1: (0.1814) | Acc_1: (93.45%) (36004/38528)\n",
      "Epoch: 123 | Batch_idx: 310 |  Loss_1: (0.1824) | Acc_1: (93.41%) (37185/39808)\n",
      "Epoch: 123 | Batch_idx: 320 |  Loss_1: (0.1820) | Acc_1: (93.43%) (38387/41088)\n",
      "Epoch: 123 | Batch_idx: 330 |  Loss_1: (0.1826) | Acc_1: (93.38%) (39565/42368)\n",
      "Epoch: 123 | Batch_idx: 340 |  Loss_1: (0.1828) | Acc_1: (93.39%) (40763/43648)\n",
      "Epoch: 123 | Batch_idx: 350 |  Loss_1: (0.1824) | Acc_1: (93.40%) (41961/44928)\n",
      "Epoch: 123 | Batch_idx: 360 |  Loss_1: (0.1820) | Acc_1: (93.41%) (43162/46208)\n",
      "Epoch: 123 | Batch_idx: 370 |  Loss_1: (0.1823) | Acc_1: (93.39%) (44350/47488)\n",
      "Epoch: 123 | Batch_idx: 380 |  Loss_1: (0.1818) | Acc_1: (93.41%) (45554/48768)\n",
      "Epoch: 123 | Batch_idx: 390 |  Loss_1: (0.1817) | Acc_1: (93.41%) (46705/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3140) | Acc: (91.27%) (9127/10000)\n",
      "Epoch: 124 | Batch_idx: 0 |  Loss_1: (0.1982) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 124 | Batch_idx: 10 |  Loss_1: (0.1811) | Acc_1: (93.47%) (1316/1408)\n",
      "Epoch: 124 | Batch_idx: 20 |  Loss_1: (0.1842) | Acc_1: (93.68%) (2518/2688)\n",
      "Epoch: 124 | Batch_idx: 30 |  Loss_1: (0.1726) | Acc_1: (94.08%) (3733/3968)\n",
      "Epoch: 124 | Batch_idx: 40 |  Loss_1: (0.1777) | Acc_1: (93.90%) (4928/5248)\n",
      "Epoch: 124 | Batch_idx: 50 |  Loss_1: (0.1770) | Acc_1: (93.78%) (6122/6528)\n",
      "Epoch: 124 | Batch_idx: 60 |  Loss_1: (0.1753) | Acc_1: (93.71%) (7317/7808)\n",
      "Epoch: 124 | Batch_idx: 70 |  Loss_1: (0.1745) | Acc_1: (93.79%) (8524/9088)\n",
      "Epoch: 124 | Batch_idx: 80 |  Loss_1: (0.1775) | Acc_1: (93.64%) (9709/10368)\n",
      "Epoch: 124 | Batch_idx: 90 |  Loss_1: (0.1769) | Acc_1: (93.61%) (10904/11648)\n",
      "Epoch: 124 | Batch_idx: 100 |  Loss_1: (0.1744) | Acc_1: (93.69%) (12112/12928)\n",
      "Epoch: 124 | Batch_idx: 110 |  Loss_1: (0.1751) | Acc_1: (93.66%) (13307/14208)\n",
      "Epoch: 124 | Batch_idx: 120 |  Loss_1: (0.1756) | Acc_1: (93.59%) (14495/15488)\n",
      "Epoch: 124 | Batch_idx: 130 |  Loss_1: (0.1748) | Acc_1: (93.59%) (15694/16768)\n",
      "Epoch: 124 | Batch_idx: 140 |  Loss_1: (0.1767) | Acc_1: (93.52%) (16879/18048)\n",
      "Epoch: 124 | Batch_idx: 150 |  Loss_1: (0.1791) | Acc_1: (93.43%) (18059/19328)\n",
      "Epoch: 124 | Batch_idx: 160 |  Loss_1: (0.1776) | Acc_1: (93.51%) (19270/20608)\n",
      "Epoch: 124 | Batch_idx: 170 |  Loss_1: (0.1769) | Acc_1: (93.53%) (20471/21888)\n",
      "Epoch: 124 | Batch_idx: 180 |  Loss_1: (0.1763) | Acc_1: (93.55%) (21674/23168)\n",
      "Epoch: 124 | Batch_idx: 190 |  Loss_1: (0.1755) | Acc_1: (93.58%) (22879/24448)\n",
      "Epoch: 124 | Batch_idx: 200 |  Loss_1: (0.1753) | Acc_1: (93.59%) (24079/25728)\n",
      "Epoch: 124 | Batch_idx: 210 |  Loss_1: (0.1751) | Acc_1: (93.61%) (25283/27008)\n",
      "Epoch: 124 | Batch_idx: 220 |  Loss_1: (0.1751) | Acc_1: (93.59%) (26475/28288)\n",
      "Epoch: 124 | Batch_idx: 230 |  Loss_1: (0.1763) | Acc_1: (93.54%) (27658/29568)\n",
      "Epoch: 124 | Batch_idx: 240 |  Loss_1: (0.1772) | Acc_1: (93.51%) (28846/30848)\n",
      "Epoch: 124 | Batch_idx: 250 |  Loss_1: (0.1778) | Acc_1: (93.49%) (30035/32128)\n",
      "Epoch: 124 | Batch_idx: 260 |  Loss_1: (0.1768) | Acc_1: (93.53%) (31246/33408)\n",
      "Epoch: 124 | Batch_idx: 270 |  Loss_1: (0.1777) | Acc_1: (93.50%) (32435/34688)\n",
      "Epoch: 124 | Batch_idx: 280 |  Loss_1: (0.1769) | Acc_1: (93.54%) (33644/35968)\n",
      "Epoch: 124 | Batch_idx: 290 |  Loss_1: (0.1769) | Acc_1: (93.53%) (34838/37248)\n",
      "Epoch: 124 | Batch_idx: 300 |  Loss_1: (0.1768) | Acc_1: (93.55%) (36042/38528)\n",
      "Epoch: 124 | Batch_idx: 310 |  Loss_1: (0.1760) | Acc_1: (93.57%) (37250/39808)\n",
      "Epoch: 124 | Batch_idx: 320 |  Loss_1: (0.1759) | Acc_1: (93.57%) (38448/41088)\n",
      "Epoch: 124 | Batch_idx: 330 |  Loss_1: (0.1755) | Acc_1: (93.59%) (39651/42368)\n",
      "Epoch: 124 | Batch_idx: 340 |  Loss_1: (0.1746) | Acc_1: (93.61%) (40859/43648)\n",
      "Epoch: 124 | Batch_idx: 350 |  Loss_1: (0.1751) | Acc_1: (93.60%) (42052/44928)\n",
      "Epoch: 124 | Batch_idx: 360 |  Loss_1: (0.1745) | Acc_1: (93.62%) (43262/46208)\n",
      "Epoch: 124 | Batch_idx: 370 |  Loss_1: (0.1738) | Acc_1: (93.65%) (44474/47488)\n",
      "Epoch: 124 | Batch_idx: 380 |  Loss_1: (0.1747) | Acc_1: (93.62%) (45655/48768)\n",
      "Epoch: 124 | Batch_idx: 390 |  Loss_1: (0.1748) | Acc_1: (93.62%) (46808/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3303) | Acc: (91.10%) (9110/10000)\n",
      "Epoch: 125 | Batch_idx: 0 |  Loss_1: (0.1652) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 125 | Batch_idx: 10 |  Loss_1: (0.1800) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 125 | Batch_idx: 20 |  Loss_1: (0.1822) | Acc_1: (93.15%) (2504/2688)\n",
      "Epoch: 125 | Batch_idx: 30 |  Loss_1: (0.1825) | Acc_1: (93.32%) (3703/3968)\n",
      "Epoch: 125 | Batch_idx: 40 |  Loss_1: (0.1810) | Acc_1: (93.35%) (4899/5248)\n",
      "Epoch: 125 | Batch_idx: 50 |  Loss_1: (0.1821) | Acc_1: (93.47%) (6102/6528)\n",
      "Epoch: 125 | Batch_idx: 60 |  Loss_1: (0.1832) | Acc_1: (93.49%) (7300/7808)\n",
      "Epoch: 125 | Batch_idx: 70 |  Loss_1: (0.1837) | Acc_1: (93.50%) (8497/9088)\n",
      "Epoch: 125 | Batch_idx: 80 |  Loss_1: (0.1817) | Acc_1: (93.58%) (9702/10368)\n",
      "Epoch: 125 | Batch_idx: 90 |  Loss_1: (0.1820) | Acc_1: (93.51%) (10892/11648)\n",
      "Epoch: 125 | Batch_idx: 100 |  Loss_1: (0.1817) | Acc_1: (93.50%) (12088/12928)\n",
      "Epoch: 125 | Batch_idx: 110 |  Loss_1: (0.1805) | Acc_1: (93.55%) (13292/14208)\n",
      "Epoch: 125 | Batch_idx: 120 |  Loss_1: (0.1790) | Acc_1: (93.56%) (14490/15488)\n",
      "Epoch: 125 | Batch_idx: 130 |  Loss_1: (0.1779) | Acc_1: (93.57%) (15689/16768)\n",
      "Epoch: 125 | Batch_idx: 140 |  Loss_1: (0.1775) | Acc_1: (93.59%) (16892/18048)\n",
      "Epoch: 125 | Batch_idx: 150 |  Loss_1: (0.1767) | Acc_1: (93.61%) (18093/19328)\n",
      "Epoch: 125 | Batch_idx: 160 |  Loss_1: (0.1762) | Acc_1: (93.61%) (19292/20608)\n",
      "Epoch: 125 | Batch_idx: 170 |  Loss_1: (0.1769) | Acc_1: (93.56%) (20478/21888)\n",
      "Epoch: 125 | Batch_idx: 180 |  Loss_1: (0.1773) | Acc_1: (93.57%) (21678/23168)\n",
      "Epoch: 125 | Batch_idx: 190 |  Loss_1: (0.1769) | Acc_1: (93.56%) (22873/24448)\n",
      "Epoch: 125 | Batch_idx: 200 |  Loss_1: (0.1777) | Acc_1: (93.52%) (24060/25728)\n",
      "Epoch: 125 | Batch_idx: 210 |  Loss_1: (0.1772) | Acc_1: (93.53%) (25260/27008)\n",
      "Epoch: 125 | Batch_idx: 220 |  Loss_1: (0.1773) | Acc_1: (93.53%) (26457/28288)\n",
      "Epoch: 125 | Batch_idx: 230 |  Loss_1: (0.1794) | Acc_1: (93.43%) (27624/29568)\n",
      "Epoch: 125 | Batch_idx: 240 |  Loss_1: (0.1789) | Acc_1: (93.43%) (28822/30848)\n",
      "Epoch: 125 | Batch_idx: 250 |  Loss_1: (0.1792) | Acc_1: (93.44%) (30019/32128)\n",
      "Epoch: 125 | Batch_idx: 260 |  Loss_1: (0.1796) | Acc_1: (93.41%) (31207/33408)\n",
      "Epoch: 125 | Batch_idx: 270 |  Loss_1: (0.1794) | Acc_1: (93.44%) (32413/34688)\n",
      "Epoch: 125 | Batch_idx: 280 |  Loss_1: (0.1788) | Acc_1: (93.47%) (33619/35968)\n",
      "Epoch: 125 | Batch_idx: 290 |  Loss_1: (0.1800) | Acc_1: (93.45%) (34807/37248)\n",
      "Epoch: 125 | Batch_idx: 300 |  Loss_1: (0.1801) | Acc_1: (93.44%) (35999/38528)\n",
      "Epoch: 125 | Batch_idx: 310 |  Loss_1: (0.1802) | Acc_1: (93.43%) (37194/39808)\n",
      "Epoch: 125 | Batch_idx: 320 |  Loss_1: (0.1804) | Acc_1: (93.44%) (38392/41088)\n",
      "Epoch: 125 | Batch_idx: 330 |  Loss_1: (0.1799) | Acc_1: (93.45%) (39593/42368)\n",
      "Epoch: 125 | Batch_idx: 340 |  Loss_1: (0.1799) | Acc_1: (93.45%) (40788/43648)\n",
      "Epoch: 125 | Batch_idx: 350 |  Loss_1: (0.1798) | Acc_1: (93.45%) (41986/44928)\n",
      "Epoch: 125 | Batch_idx: 360 |  Loss_1: (0.1795) | Acc_1: (93.46%) (43185/46208)\n",
      "Epoch: 125 | Batch_idx: 370 |  Loss_1: (0.1796) | Acc_1: (93.46%) (44380/47488)\n",
      "Epoch: 125 | Batch_idx: 380 |  Loss_1: (0.1791) | Acc_1: (93.48%) (45588/48768)\n",
      "Epoch: 125 | Batch_idx: 390 |  Loss_1: (0.1795) | Acc_1: (93.47%) (46737/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2965) | Acc: (91.42%) (9142/10000)\n",
      "Epoch: 126 | Batch_idx: 0 |  Loss_1: (0.1470) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 126 | Batch_idx: 10 |  Loss_1: (0.1683) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 126 | Batch_idx: 20 |  Loss_1: (0.1738) | Acc_1: (93.64%) (2517/2688)\n",
      "Epoch: 126 | Batch_idx: 30 |  Loss_1: (0.1654) | Acc_1: (93.88%) (3725/3968)\n",
      "Epoch: 126 | Batch_idx: 40 |  Loss_1: (0.1691) | Acc_1: (93.79%) (4922/5248)\n",
      "Epoch: 126 | Batch_idx: 50 |  Loss_1: (0.1666) | Acc_1: (93.83%) (6125/6528)\n",
      "Epoch: 126 | Batch_idx: 60 |  Loss_1: (0.1645) | Acc_1: (93.90%) (7332/7808)\n",
      "Epoch: 126 | Batch_idx: 70 |  Loss_1: (0.1633) | Acc_1: (93.96%) (8539/9088)\n",
      "Epoch: 126 | Batch_idx: 80 |  Loss_1: (0.1622) | Acc_1: (93.99%) (9745/10368)\n",
      "Epoch: 126 | Batch_idx: 90 |  Loss_1: (0.1603) | Acc_1: (94.07%) (10957/11648)\n",
      "Epoch: 126 | Batch_idx: 100 |  Loss_1: (0.1602) | Acc_1: (94.07%) (12162/12928)\n",
      "Epoch: 126 | Batch_idx: 110 |  Loss_1: (0.1651) | Acc_1: (93.92%) (13344/14208)\n",
      "Epoch: 126 | Batch_idx: 120 |  Loss_1: (0.1657) | Acc_1: (93.92%) (14546/15488)\n",
      "Epoch: 126 | Batch_idx: 130 |  Loss_1: (0.1657) | Acc_1: (93.99%) (15761/16768)\n",
      "Epoch: 126 | Batch_idx: 140 |  Loss_1: (0.1644) | Acc_1: (94.05%) (16975/18048)\n",
      "Epoch: 126 | Batch_idx: 150 |  Loss_1: (0.1660) | Acc_1: (93.98%) (18164/19328)\n",
      "Epoch: 126 | Batch_idx: 160 |  Loss_1: (0.1665) | Acc_1: (93.97%) (19365/20608)\n",
      "Epoch: 126 | Batch_idx: 170 |  Loss_1: (0.1663) | Acc_1: (94.00%) (20574/21888)\n",
      "Epoch: 126 | Batch_idx: 180 |  Loss_1: (0.1678) | Acc_1: (93.96%) (21769/23168)\n",
      "Epoch: 126 | Batch_idx: 190 |  Loss_1: (0.1690) | Acc_1: (93.93%) (22963/24448)\n",
      "Epoch: 126 | Batch_idx: 200 |  Loss_1: (0.1684) | Acc_1: (93.94%) (24168/25728)\n",
      "Epoch: 126 | Batch_idx: 210 |  Loss_1: (0.1682) | Acc_1: (93.95%) (25374/27008)\n",
      "Epoch: 126 | Batch_idx: 220 |  Loss_1: (0.1677) | Acc_1: (93.97%) (26582/28288)\n",
      "Epoch: 126 | Batch_idx: 230 |  Loss_1: (0.1690) | Acc_1: (93.90%) (27763/29568)\n",
      "Epoch: 126 | Batch_idx: 240 |  Loss_1: (0.1694) | Acc_1: (93.89%) (28963/30848)\n",
      "Epoch: 126 | Batch_idx: 250 |  Loss_1: (0.1699) | Acc_1: (93.87%) (30160/32128)\n",
      "Epoch: 126 | Batch_idx: 260 |  Loss_1: (0.1692) | Acc_1: (93.89%) (31367/33408)\n",
      "Epoch: 126 | Batch_idx: 270 |  Loss_1: (0.1690) | Acc_1: (93.89%) (32570/34688)\n",
      "Epoch: 126 | Batch_idx: 280 |  Loss_1: (0.1677) | Acc_1: (93.94%) (33789/35968)\n",
      "Epoch: 126 | Batch_idx: 290 |  Loss_1: (0.1686) | Acc_1: (93.93%) (34988/37248)\n",
      "Epoch: 126 | Batch_idx: 300 |  Loss_1: (0.1694) | Acc_1: (93.90%) (36179/38528)\n",
      "Epoch: 126 | Batch_idx: 310 |  Loss_1: (0.1702) | Acc_1: (93.88%) (37373/39808)\n",
      "Epoch: 126 | Batch_idx: 320 |  Loss_1: (0.1701) | Acc_1: (93.88%) (38574/41088)\n",
      "Epoch: 126 | Batch_idx: 330 |  Loss_1: (0.1700) | Acc_1: (93.89%) (39778/42368)\n",
      "Epoch: 126 | Batch_idx: 340 |  Loss_1: (0.1701) | Acc_1: (93.88%) (40976/43648)\n",
      "Epoch: 126 | Batch_idx: 350 |  Loss_1: (0.1704) | Acc_1: (93.87%) (42174/44928)\n",
      "Epoch: 126 | Batch_idx: 360 |  Loss_1: (0.1705) | Acc_1: (93.87%) (43376/46208)\n",
      "Epoch: 126 | Batch_idx: 370 |  Loss_1: (0.1712) | Acc_1: (93.86%) (44573/47488)\n",
      "Epoch: 126 | Batch_idx: 380 |  Loss_1: (0.1707) | Acc_1: (93.89%) (45789/48768)\n",
      "Epoch: 126 | Batch_idx: 390 |  Loss_1: (0.1712) | Acc_1: (93.88%) (46938/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3131) | Acc: (91.28%) (9128/10000)\n",
      "Epoch: 127 | Batch_idx: 0 |  Loss_1: (0.1640) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 127 | Batch_idx: 10 |  Loss_1: (0.1762) | Acc_1: (93.75%) (1320/1408)\n",
      "Epoch: 127 | Batch_idx: 20 |  Loss_1: (0.1775) | Acc_1: (93.94%) (2525/2688)\n",
      "Epoch: 127 | Batch_idx: 30 |  Loss_1: (0.1810) | Acc_1: (93.72%) (3719/3968)\n",
      "Epoch: 127 | Batch_idx: 40 |  Loss_1: (0.1769) | Acc_1: (93.83%) (4924/5248)\n",
      "Epoch: 127 | Batch_idx: 50 |  Loss_1: (0.1751) | Acc_1: (93.86%) (6127/6528)\n",
      "Epoch: 127 | Batch_idx: 60 |  Loss_1: (0.1791) | Acc_1: (93.62%) (7310/7808)\n",
      "Epoch: 127 | Batch_idx: 70 |  Loss_1: (0.1780) | Acc_1: (93.65%) (8511/9088)\n",
      "Epoch: 127 | Batch_idx: 80 |  Loss_1: (0.1723) | Acc_1: (93.82%) (9727/10368)\n",
      "Epoch: 127 | Batch_idx: 90 |  Loss_1: (0.1706) | Acc_1: (93.86%) (10933/11648)\n",
      "Epoch: 127 | Batch_idx: 100 |  Loss_1: (0.1716) | Acc_1: (93.84%) (12131/12928)\n",
      "Epoch: 127 | Batch_idx: 110 |  Loss_1: (0.1697) | Acc_1: (93.89%) (13340/14208)\n",
      "Epoch: 127 | Batch_idx: 120 |  Loss_1: (0.1699) | Acc_1: (93.84%) (14534/15488)\n",
      "Epoch: 127 | Batch_idx: 130 |  Loss_1: (0.1705) | Acc_1: (93.83%) (15734/16768)\n",
      "Epoch: 127 | Batch_idx: 140 |  Loss_1: (0.1710) | Acc_1: (93.79%) (16927/18048)\n",
      "Epoch: 127 | Batch_idx: 150 |  Loss_1: (0.1703) | Acc_1: (93.79%) (18128/19328)\n",
      "Epoch: 127 | Batch_idx: 160 |  Loss_1: (0.1695) | Acc_1: (93.84%) (19339/20608)\n",
      "Epoch: 127 | Batch_idx: 170 |  Loss_1: (0.1692) | Acc_1: (93.85%) (20542/21888)\n",
      "Epoch: 127 | Batch_idx: 180 |  Loss_1: (0.1697) | Acc_1: (93.86%) (21745/23168)\n",
      "Epoch: 127 | Batch_idx: 190 |  Loss_1: (0.1690) | Acc_1: (93.89%) (22954/24448)\n",
      "Epoch: 127 | Batch_idx: 200 |  Loss_1: (0.1686) | Acc_1: (93.89%) (24157/25728)\n",
      "Epoch: 127 | Batch_idx: 210 |  Loss_1: (0.1672) | Acc_1: (93.96%) (25377/27008)\n",
      "Epoch: 127 | Batch_idx: 220 |  Loss_1: (0.1687) | Acc_1: (93.92%) (26567/28288)\n",
      "Epoch: 127 | Batch_idx: 230 |  Loss_1: (0.1692) | Acc_1: (93.92%) (27771/29568)\n",
      "Epoch: 127 | Batch_idx: 240 |  Loss_1: (0.1694) | Acc_1: (93.90%) (28965/30848)\n",
      "Epoch: 127 | Batch_idx: 250 |  Loss_1: (0.1689) | Acc_1: (93.92%) (30175/32128)\n",
      "Epoch: 127 | Batch_idx: 260 |  Loss_1: (0.1691) | Acc_1: (93.91%) (31374/33408)\n",
      "Epoch: 127 | Batch_idx: 270 |  Loss_1: (0.1709) | Acc_1: (93.86%) (32557/34688)\n",
      "Epoch: 127 | Batch_idx: 280 |  Loss_1: (0.1715) | Acc_1: (93.83%) (33750/35968)\n",
      "Epoch: 127 | Batch_idx: 290 |  Loss_1: (0.1717) | Acc_1: (93.83%) (34948/37248)\n",
      "Epoch: 127 | Batch_idx: 300 |  Loss_1: (0.1715) | Acc_1: (93.84%) (36153/38528)\n",
      "Epoch: 127 | Batch_idx: 310 |  Loss_1: (0.1712) | Acc_1: (93.84%) (37356/39808)\n",
      "Epoch: 127 | Batch_idx: 320 |  Loss_1: (0.1715) | Acc_1: (93.82%) (38547/41088)\n",
      "Epoch: 127 | Batch_idx: 330 |  Loss_1: (0.1724) | Acc_1: (93.78%) (39732/42368)\n",
      "Epoch: 127 | Batch_idx: 340 |  Loss_1: (0.1733) | Acc_1: (93.73%) (40913/43648)\n",
      "Epoch: 127 | Batch_idx: 350 |  Loss_1: (0.1735) | Acc_1: (93.73%) (42110/44928)\n",
      "Epoch: 127 | Batch_idx: 360 |  Loss_1: (0.1733) | Acc_1: (93.72%) (43308/46208)\n",
      "Epoch: 127 | Batch_idx: 370 |  Loss_1: (0.1732) | Acc_1: (93.73%) (44510/47488)\n",
      "Epoch: 127 | Batch_idx: 380 |  Loss_1: (0.1735) | Acc_1: (93.72%) (45704/48768)\n",
      "Epoch: 127 | Batch_idx: 390 |  Loss_1: (0.1737) | Acc_1: (93.71%) (46855/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3171) | Acc: (91.49%) (9149/10000)\n",
      "Epoch: 128 | Batch_idx: 0 |  Loss_1: (0.2817) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 128 | Batch_idx: 10 |  Loss_1: (0.1802) | Acc_1: (93.61%) (1318/1408)\n",
      "Epoch: 128 | Batch_idx: 20 |  Loss_1: (0.1798) | Acc_1: (93.15%) (2504/2688)\n",
      "Epoch: 128 | Batch_idx: 30 |  Loss_1: (0.1716) | Acc_1: (93.60%) (3714/3968)\n",
      "Epoch: 128 | Batch_idx: 40 |  Loss_1: (0.1647) | Acc_1: (93.79%) (4922/5248)\n",
      "Epoch: 128 | Batch_idx: 50 |  Loss_1: (0.1627) | Acc_1: (93.90%) (6130/6528)\n",
      "Epoch: 128 | Batch_idx: 60 |  Loss_1: (0.1622) | Acc_1: (93.93%) (7334/7808)\n",
      "Epoch: 128 | Batch_idx: 70 |  Loss_1: (0.1640) | Acc_1: (93.95%) (8538/9088)\n",
      "Epoch: 128 | Batch_idx: 80 |  Loss_1: (0.1642) | Acc_1: (93.95%) (9741/10368)\n",
      "Epoch: 128 | Batch_idx: 90 |  Loss_1: (0.1657) | Acc_1: (93.84%) (10930/11648)\n",
      "Epoch: 128 | Batch_idx: 100 |  Loss_1: (0.1644) | Acc_1: (93.87%) (12136/12928)\n",
      "Epoch: 128 | Batch_idx: 110 |  Loss_1: (0.1652) | Acc_1: (93.88%) (13339/14208)\n",
      "Epoch: 128 | Batch_idx: 120 |  Loss_1: (0.1677) | Acc_1: (93.85%) (14535/15488)\n",
      "Epoch: 128 | Batch_idx: 130 |  Loss_1: (0.1696) | Acc_1: (93.79%) (15726/16768)\n",
      "Epoch: 128 | Batch_idx: 140 |  Loss_1: (0.1708) | Acc_1: (93.74%) (16919/18048)\n",
      "Epoch: 128 | Batch_idx: 150 |  Loss_1: (0.1707) | Acc_1: (93.76%) (18122/19328)\n",
      "Epoch: 128 | Batch_idx: 160 |  Loss_1: (0.1702) | Acc_1: (93.77%) (19324/20608)\n",
      "Epoch: 128 | Batch_idx: 170 |  Loss_1: (0.1696) | Acc_1: (93.80%) (20530/21888)\n",
      "Epoch: 128 | Batch_idx: 180 |  Loss_1: (0.1700) | Acc_1: (93.78%) (21728/23168)\n",
      "Epoch: 128 | Batch_idx: 190 |  Loss_1: (0.1692) | Acc_1: (93.82%) (22936/24448)\n",
      "Epoch: 128 | Batch_idx: 200 |  Loss_1: (0.1699) | Acc_1: (93.78%) (24129/25728)\n",
      "Epoch: 128 | Batch_idx: 210 |  Loss_1: (0.1716) | Acc_1: (93.72%) (25313/27008)\n",
      "Epoch: 128 | Batch_idx: 220 |  Loss_1: (0.1718) | Acc_1: (93.69%) (26503/28288)\n",
      "Epoch: 128 | Batch_idx: 230 |  Loss_1: (0.1723) | Acc_1: (93.71%) (27709/29568)\n",
      "Epoch: 128 | Batch_idx: 240 |  Loss_1: (0.1717) | Acc_1: (93.74%) (28917/30848)\n",
      "Epoch: 128 | Batch_idx: 250 |  Loss_1: (0.1710) | Acc_1: (93.80%) (30135/32128)\n",
      "Epoch: 128 | Batch_idx: 260 |  Loss_1: (0.1713) | Acc_1: (93.79%) (31335/33408)\n",
      "Epoch: 128 | Batch_idx: 270 |  Loss_1: (0.1718) | Acc_1: (93.78%) (32529/34688)\n",
      "Epoch: 128 | Batch_idx: 280 |  Loss_1: (0.1715) | Acc_1: (93.77%) (33727/35968)\n",
      "Epoch: 128 | Batch_idx: 290 |  Loss_1: (0.1703) | Acc_1: (93.81%) (34941/37248)\n",
      "Epoch: 128 | Batch_idx: 300 |  Loss_1: (0.1698) | Acc_1: (93.84%) (36153/38528)\n",
      "Epoch: 128 | Batch_idx: 310 |  Loss_1: (0.1689) | Acc_1: (93.86%) (37365/39808)\n",
      "Epoch: 128 | Batch_idx: 320 |  Loss_1: (0.1686) | Acc_1: (93.87%) (38568/41088)\n",
      "Epoch: 128 | Batch_idx: 330 |  Loss_1: (0.1684) | Acc_1: (93.88%) (39775/42368)\n",
      "Epoch: 128 | Batch_idx: 340 |  Loss_1: (0.1685) | Acc_1: (93.89%) (40979/43648)\n",
      "Epoch: 128 | Batch_idx: 350 |  Loss_1: (0.1685) | Acc_1: (93.88%) (42180/44928)\n",
      "Epoch: 128 | Batch_idx: 360 |  Loss_1: (0.1683) | Acc_1: (93.90%) (43391/46208)\n",
      "Epoch: 128 | Batch_idx: 370 |  Loss_1: (0.1681) | Acc_1: (93.91%) (44596/47488)\n",
      "Epoch: 128 | Batch_idx: 380 |  Loss_1: (0.1683) | Acc_1: (93.88%) (45784/48768)\n",
      "Epoch: 128 | Batch_idx: 390 |  Loss_1: (0.1684) | Acc_1: (93.86%) (46931/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3190) | Acc: (91.33%) (9133/10000)\n",
      "Epoch: 129 | Batch_idx: 0 |  Loss_1: (0.1652) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 129 | Batch_idx: 10 |  Loss_1: (0.1618) | Acc_1: (94.03%) (1324/1408)\n",
      "Epoch: 129 | Batch_idx: 20 |  Loss_1: (0.1590) | Acc_1: (94.08%) (2529/2688)\n",
      "Epoch: 129 | Batch_idx: 30 |  Loss_1: (0.1625) | Acc_1: (94.05%) (3732/3968)\n",
      "Epoch: 129 | Batch_idx: 40 |  Loss_1: (0.1626) | Acc_1: (94.02%) (4934/5248)\n",
      "Epoch: 129 | Batch_idx: 50 |  Loss_1: (0.1661) | Acc_1: (93.89%) (6129/6528)\n",
      "Epoch: 129 | Batch_idx: 60 |  Loss_1: (0.1645) | Acc_1: (93.94%) (7335/7808)\n",
      "Epoch: 129 | Batch_idx: 70 |  Loss_1: (0.1633) | Acc_1: (93.97%) (8540/9088)\n",
      "Epoch: 129 | Batch_idx: 80 |  Loss_1: (0.1622) | Acc_1: (94.04%) (9750/10368)\n",
      "Epoch: 129 | Batch_idx: 90 |  Loss_1: (0.1609) | Acc_1: (94.04%) (10954/11648)\n",
      "Epoch: 129 | Batch_idx: 100 |  Loss_1: (0.1618) | Acc_1: (94.01%) (12154/12928)\n",
      "Epoch: 129 | Batch_idx: 110 |  Loss_1: (0.1618) | Acc_1: (94.02%) (13358/14208)\n",
      "Epoch: 129 | Batch_idx: 120 |  Loss_1: (0.1612) | Acc_1: (94.09%) (14573/15488)\n",
      "Epoch: 129 | Batch_idx: 130 |  Loss_1: (0.1621) | Acc_1: (94.08%) (15775/16768)\n",
      "Epoch: 129 | Batch_idx: 140 |  Loss_1: (0.1632) | Acc_1: (94.04%) (16973/18048)\n",
      "Epoch: 129 | Batch_idx: 150 |  Loss_1: (0.1629) | Acc_1: (94.01%) (18171/19328)\n",
      "Epoch: 129 | Batch_idx: 160 |  Loss_1: (0.1641) | Acc_1: (93.97%) (19365/20608)\n",
      "Epoch: 129 | Batch_idx: 170 |  Loss_1: (0.1674) | Acc_1: (93.85%) (20542/21888)\n",
      "Epoch: 129 | Batch_idx: 180 |  Loss_1: (0.1688) | Acc_1: (93.84%) (21741/23168)\n",
      "Epoch: 129 | Batch_idx: 190 |  Loss_1: (0.1681) | Acc_1: (93.87%) (22950/24448)\n",
      "Epoch: 129 | Batch_idx: 200 |  Loss_1: (0.1670) | Acc_1: (93.92%) (24164/25728)\n",
      "Epoch: 129 | Batch_idx: 210 |  Loss_1: (0.1681) | Acc_1: (93.88%) (25356/27008)\n",
      "Epoch: 129 | Batch_idx: 220 |  Loss_1: (0.1688) | Acc_1: (93.86%) (26551/28288)\n",
      "Epoch: 129 | Batch_idx: 230 |  Loss_1: (0.1683) | Acc_1: (93.86%) (27754/29568)\n",
      "Epoch: 129 | Batch_idx: 240 |  Loss_1: (0.1682) | Acc_1: (93.88%) (28959/30848)\n",
      "Epoch: 129 | Batch_idx: 250 |  Loss_1: (0.1683) | Acc_1: (93.89%) (30166/32128)\n",
      "Epoch: 129 | Batch_idx: 260 |  Loss_1: (0.1675) | Acc_1: (93.92%) (31378/33408)\n",
      "Epoch: 129 | Batch_idx: 270 |  Loss_1: (0.1671) | Acc_1: (93.94%) (32587/34688)\n",
      "Epoch: 129 | Batch_idx: 280 |  Loss_1: (0.1668) | Acc_1: (93.95%) (33792/35968)\n",
      "Epoch: 129 | Batch_idx: 290 |  Loss_1: (0.1659) | Acc_1: (93.99%) (35010/37248)\n",
      "Epoch: 129 | Batch_idx: 300 |  Loss_1: (0.1656) | Acc_1: (94.00%) (36218/38528)\n",
      "Epoch: 129 | Batch_idx: 310 |  Loss_1: (0.1656) | Acc_1: (94.00%) (37421/39808)\n",
      "Epoch: 129 | Batch_idx: 320 |  Loss_1: (0.1662) | Acc_1: (93.98%) (38616/41088)\n",
      "Epoch: 129 | Batch_idx: 330 |  Loss_1: (0.1662) | Acc_1: (93.97%) (39813/42368)\n",
      "Epoch: 129 | Batch_idx: 340 |  Loss_1: (0.1663) | Acc_1: (93.96%) (41011/43648)\n",
      "Epoch: 129 | Batch_idx: 350 |  Loss_1: (0.1668) | Acc_1: (93.95%) (42210/44928)\n",
      "Epoch: 129 | Batch_idx: 360 |  Loss_1: (0.1673) | Acc_1: (93.94%) (43408/46208)\n",
      "Epoch: 129 | Batch_idx: 370 |  Loss_1: (0.1671) | Acc_1: (93.95%) (44614/47488)\n",
      "Epoch: 129 | Batch_idx: 380 |  Loss_1: (0.1671) | Acc_1: (93.94%) (45813/48768)\n",
      "Epoch: 129 | Batch_idx: 390 |  Loss_1: (0.1676) | Acc_1: (93.92%) (46958/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3015) | Acc: (91.93%) (9193/10000)\n",
      "Epoch: 130 | Batch_idx: 0 |  Loss_1: (0.1979) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 130 | Batch_idx: 10 |  Loss_1: (0.1559) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 130 | Batch_idx: 20 |  Loss_1: (0.1664) | Acc_1: (93.86%) (2523/2688)\n",
      "Epoch: 130 | Batch_idx: 30 |  Loss_1: (0.1676) | Acc_1: (93.85%) (3724/3968)\n",
      "Epoch: 130 | Batch_idx: 40 |  Loss_1: (0.1553) | Acc_1: (94.38%) (4953/5248)\n",
      "Epoch: 130 | Batch_idx: 50 |  Loss_1: (0.1548) | Acc_1: (94.41%) (6163/6528)\n",
      "Epoch: 130 | Batch_idx: 60 |  Loss_1: (0.1562) | Acc_1: (94.40%) (7371/7808)\n",
      "Epoch: 130 | Batch_idx: 70 |  Loss_1: (0.1586) | Acc_1: (94.32%) (8572/9088)\n",
      "Epoch: 130 | Batch_idx: 80 |  Loss_1: (0.1592) | Acc_1: (94.27%) (9774/10368)\n",
      "Epoch: 130 | Batch_idx: 90 |  Loss_1: (0.1632) | Acc_1: (94.09%) (10960/11648)\n",
      "Epoch: 130 | Batch_idx: 100 |  Loss_1: (0.1637) | Acc_1: (94.11%) (12167/12928)\n",
      "Epoch: 130 | Batch_idx: 110 |  Loss_1: (0.1634) | Acc_1: (94.11%) (13371/14208)\n",
      "Epoch: 130 | Batch_idx: 120 |  Loss_1: (0.1610) | Acc_1: (94.20%) (14590/15488)\n",
      "Epoch: 130 | Batch_idx: 130 |  Loss_1: (0.1613) | Acc_1: (94.19%) (15794/16768)\n",
      "Epoch: 130 | Batch_idx: 140 |  Loss_1: (0.1617) | Acc_1: (94.14%) (16991/18048)\n",
      "Epoch: 130 | Batch_idx: 150 |  Loss_1: (0.1621) | Acc_1: (94.10%) (18188/19328)\n",
      "Epoch: 130 | Batch_idx: 160 |  Loss_1: (0.1606) | Acc_1: (94.15%) (19402/20608)\n",
      "Epoch: 130 | Batch_idx: 170 |  Loss_1: (0.1616) | Acc_1: (94.13%) (20604/21888)\n",
      "Epoch: 130 | Batch_idx: 180 |  Loss_1: (0.1622) | Acc_1: (94.09%) (21799/23168)\n",
      "Epoch: 130 | Batch_idx: 190 |  Loss_1: (0.1628) | Acc_1: (94.10%) (23006/24448)\n",
      "Epoch: 130 | Batch_idx: 200 |  Loss_1: (0.1636) | Acc_1: (94.08%) (24204/25728)\n",
      "Epoch: 130 | Batch_idx: 210 |  Loss_1: (0.1630) | Acc_1: (94.11%) (25416/27008)\n",
      "Epoch: 130 | Batch_idx: 220 |  Loss_1: (0.1652) | Acc_1: (94.03%) (26600/28288)\n",
      "Epoch: 130 | Batch_idx: 230 |  Loss_1: (0.1652) | Acc_1: (94.05%) (27808/29568)\n",
      "Epoch: 130 | Batch_idx: 240 |  Loss_1: (0.1657) | Acc_1: (94.03%) (29005/30848)\n",
      "Epoch: 130 | Batch_idx: 250 |  Loss_1: (0.1665) | Acc_1: (94.01%) (30202/32128)\n",
      "Epoch: 130 | Batch_idx: 260 |  Loss_1: (0.1668) | Acc_1: (93.98%) (31397/33408)\n",
      "Epoch: 130 | Batch_idx: 270 |  Loss_1: (0.1669) | Acc_1: (93.97%) (32597/34688)\n",
      "Epoch: 130 | Batch_idx: 280 |  Loss_1: (0.1664) | Acc_1: (93.99%) (33808/35968)\n",
      "Epoch: 130 | Batch_idx: 290 |  Loss_1: (0.1654) | Acc_1: (94.03%) (35026/37248)\n",
      "Epoch: 130 | Batch_idx: 300 |  Loss_1: (0.1668) | Acc_1: (93.97%) (36206/38528)\n",
      "Epoch: 130 | Batch_idx: 310 |  Loss_1: (0.1661) | Acc_1: (94.00%) (37419/39808)\n",
      "Epoch: 130 | Batch_idx: 320 |  Loss_1: (0.1670) | Acc_1: (93.95%) (38604/41088)\n",
      "Epoch: 130 | Batch_idx: 330 |  Loss_1: (0.1667) | Acc_1: (93.97%) (39814/42368)\n",
      "Epoch: 130 | Batch_idx: 340 |  Loss_1: (0.1664) | Acc_1: (93.98%) (41019/43648)\n",
      "Epoch: 130 | Batch_idx: 350 |  Loss_1: (0.1663) | Acc_1: (93.98%) (42222/44928)\n",
      "Epoch: 130 | Batch_idx: 360 |  Loss_1: (0.1660) | Acc_1: (93.99%) (43431/46208)\n",
      "Epoch: 130 | Batch_idx: 370 |  Loss_1: (0.1662) | Acc_1: (93.98%) (44631/47488)\n",
      "Epoch: 130 | Batch_idx: 380 |  Loss_1: (0.1668) | Acc_1: (93.95%) (45817/48768)\n",
      "Epoch: 130 | Batch_idx: 390 |  Loss_1: (0.1668) | Acc_1: (93.95%) (46977/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3110) | Acc: (91.44%) (9144/10000)\n",
      "Epoch: 131 | Batch_idx: 0 |  Loss_1: (0.2199) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 131 | Batch_idx: 10 |  Loss_1: (0.1520) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 131 | Batch_idx: 20 |  Loss_1: (0.1455) | Acc_1: (95.24%) (2560/2688)\n",
      "Epoch: 131 | Batch_idx: 30 |  Loss_1: (0.1553) | Acc_1: (94.66%) (3756/3968)\n",
      "Epoch: 131 | Batch_idx: 40 |  Loss_1: (0.1542) | Acc_1: (94.49%) (4959/5248)\n",
      "Epoch: 131 | Batch_idx: 50 |  Loss_1: (0.1596) | Acc_1: (94.24%) (6152/6528)\n",
      "Epoch: 131 | Batch_idx: 60 |  Loss_1: (0.1615) | Acc_1: (94.19%) (7354/7808)\n",
      "Epoch: 131 | Batch_idx: 70 |  Loss_1: (0.1642) | Acc_1: (94.08%) (8550/9088)\n",
      "Epoch: 131 | Batch_idx: 80 |  Loss_1: (0.1625) | Acc_1: (94.14%) (9760/10368)\n",
      "Epoch: 131 | Batch_idx: 90 |  Loss_1: (0.1626) | Acc_1: (94.10%) (10961/11648)\n",
      "Epoch: 131 | Batch_idx: 100 |  Loss_1: (0.1640) | Acc_1: (94.01%) (12154/12928)\n",
      "Epoch: 131 | Batch_idx: 110 |  Loss_1: (0.1632) | Acc_1: (94.07%) (13365/14208)\n",
      "Epoch: 131 | Batch_idx: 120 |  Loss_1: (0.1595) | Acc_1: (94.21%) (14592/15488)\n",
      "Epoch: 131 | Batch_idx: 130 |  Loss_1: (0.1596) | Acc_1: (94.16%) (15789/16768)\n",
      "Epoch: 131 | Batch_idx: 140 |  Loss_1: (0.1611) | Acc_1: (94.12%) (16986/18048)\n",
      "Epoch: 131 | Batch_idx: 150 |  Loss_1: (0.1613) | Acc_1: (94.12%) (18191/19328)\n",
      "Epoch: 131 | Batch_idx: 160 |  Loss_1: (0.1616) | Acc_1: (94.12%) (19396/20608)\n",
      "Epoch: 131 | Batch_idx: 170 |  Loss_1: (0.1619) | Acc_1: (94.10%) (20596/21888)\n",
      "Epoch: 131 | Batch_idx: 180 |  Loss_1: (0.1632) | Acc_1: (94.07%) (21793/23168)\n",
      "Epoch: 131 | Batch_idx: 190 |  Loss_1: (0.1636) | Acc_1: (94.06%) (22997/24448)\n",
      "Epoch: 131 | Batch_idx: 200 |  Loss_1: (0.1653) | Acc_1: (93.99%) (24183/25728)\n",
      "Epoch: 131 | Batch_idx: 210 |  Loss_1: (0.1654) | Acc_1: (93.97%) (25380/27008)\n",
      "Epoch: 131 | Batch_idx: 220 |  Loss_1: (0.1647) | Acc_1: (94.01%) (26593/28288)\n",
      "Epoch: 131 | Batch_idx: 230 |  Loss_1: (0.1639) | Acc_1: (94.05%) (27808/29568)\n",
      "Epoch: 131 | Batch_idx: 240 |  Loss_1: (0.1627) | Acc_1: (94.09%) (29024/30848)\n",
      "Epoch: 131 | Batch_idx: 250 |  Loss_1: (0.1633) | Acc_1: (94.08%) (30226/32128)\n",
      "Epoch: 131 | Batch_idx: 260 |  Loss_1: (0.1630) | Acc_1: (94.09%) (31435/33408)\n",
      "Epoch: 131 | Batch_idx: 270 |  Loss_1: (0.1629) | Acc_1: (94.10%) (32641/34688)\n",
      "Epoch: 131 | Batch_idx: 280 |  Loss_1: (0.1621) | Acc_1: (94.12%) (33854/35968)\n",
      "Epoch: 131 | Batch_idx: 290 |  Loss_1: (0.1626) | Acc_1: (94.10%) (35052/37248)\n",
      "Epoch: 131 | Batch_idx: 300 |  Loss_1: (0.1631) | Acc_1: (94.09%) (36251/38528)\n",
      "Epoch: 131 | Batch_idx: 310 |  Loss_1: (0.1630) | Acc_1: (94.09%) (37456/39808)\n",
      "Epoch: 131 | Batch_idx: 320 |  Loss_1: (0.1631) | Acc_1: (94.07%) (38652/41088)\n",
      "Epoch: 131 | Batch_idx: 330 |  Loss_1: (0.1621) | Acc_1: (94.11%) (39873/42368)\n",
      "Epoch: 131 | Batch_idx: 340 |  Loss_1: (0.1610) | Acc_1: (94.14%) (41092/43648)\n",
      "Epoch: 131 | Batch_idx: 350 |  Loss_1: (0.1608) | Acc_1: (94.17%) (42307/44928)\n",
      "Epoch: 131 | Batch_idx: 360 |  Loss_1: (0.1610) | Acc_1: (94.17%) (43513/46208)\n",
      "Epoch: 131 | Batch_idx: 370 |  Loss_1: (0.1610) | Acc_1: (94.15%) (44712/47488)\n",
      "Epoch: 131 | Batch_idx: 380 |  Loss_1: (0.1606) | Acc_1: (94.17%) (45923/48768)\n",
      "Epoch: 131 | Batch_idx: 390 |  Loss_1: (0.1604) | Acc_1: (94.16%) (47081/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3049) | Acc: (91.38%) (9138/10000)\n",
      "Epoch: 132 | Batch_idx: 0 |  Loss_1: (0.1010) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 132 | Batch_idx: 10 |  Loss_1: (0.1585) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 132 | Batch_idx: 20 |  Loss_1: (0.1428) | Acc_1: (94.90%) (2551/2688)\n",
      "Epoch: 132 | Batch_idx: 30 |  Loss_1: (0.1447) | Acc_1: (94.76%) (3760/3968)\n",
      "Epoch: 132 | Batch_idx: 40 |  Loss_1: (0.1452) | Acc_1: (94.78%) (4974/5248)\n",
      "Epoch: 132 | Batch_idx: 50 |  Loss_1: (0.1506) | Acc_1: (94.53%) (6171/6528)\n",
      "Epoch: 132 | Batch_idx: 60 |  Loss_1: (0.1477) | Acc_1: (94.63%) (7389/7808)\n",
      "Epoch: 132 | Batch_idx: 70 |  Loss_1: (0.1463) | Acc_1: (94.71%) (8607/9088)\n",
      "Epoch: 132 | Batch_idx: 80 |  Loss_1: (0.1474) | Acc_1: (94.64%) (9812/10368)\n",
      "Epoch: 132 | Batch_idx: 90 |  Loss_1: (0.1463) | Acc_1: (94.67%) (11027/11648)\n",
      "Epoch: 132 | Batch_idx: 100 |  Loss_1: (0.1472) | Acc_1: (94.66%) (12238/12928)\n",
      "Epoch: 132 | Batch_idx: 110 |  Loss_1: (0.1482) | Acc_1: (94.67%) (13451/14208)\n",
      "Epoch: 132 | Batch_idx: 120 |  Loss_1: (0.1486) | Acc_1: (94.67%) (14663/15488)\n",
      "Epoch: 132 | Batch_idx: 130 |  Loss_1: (0.1501) | Acc_1: (94.61%) (15864/16768)\n",
      "Epoch: 132 | Batch_idx: 140 |  Loss_1: (0.1502) | Acc_1: (94.58%) (17069/18048)\n",
      "Epoch: 132 | Batch_idx: 150 |  Loss_1: (0.1492) | Acc_1: (94.64%) (18292/19328)\n",
      "Epoch: 132 | Batch_idx: 160 |  Loss_1: (0.1512) | Acc_1: (94.57%) (19490/20608)\n",
      "Epoch: 132 | Batch_idx: 170 |  Loss_1: (0.1507) | Acc_1: (94.60%) (20705/21888)\n",
      "Epoch: 132 | Batch_idx: 180 |  Loss_1: (0.1493) | Acc_1: (94.65%) (21929/23168)\n",
      "Epoch: 132 | Batch_idx: 190 |  Loss_1: (0.1494) | Acc_1: (94.65%) (23139/24448)\n",
      "Epoch: 132 | Batch_idx: 200 |  Loss_1: (0.1503) | Acc_1: (94.61%) (24341/25728)\n",
      "Epoch: 132 | Batch_idx: 210 |  Loss_1: (0.1512) | Acc_1: (94.55%) (25535/27008)\n",
      "Epoch: 132 | Batch_idx: 220 |  Loss_1: (0.1514) | Acc_1: (94.53%) (26741/28288)\n",
      "Epoch: 132 | Batch_idx: 230 |  Loss_1: (0.1513) | Acc_1: (94.53%) (27951/29568)\n",
      "Epoch: 132 | Batch_idx: 240 |  Loss_1: (0.1523) | Acc_1: (94.50%) (29151/30848)\n",
      "Epoch: 132 | Batch_idx: 250 |  Loss_1: (0.1535) | Acc_1: (94.46%) (30347/32128)\n",
      "Epoch: 132 | Batch_idx: 260 |  Loss_1: (0.1549) | Acc_1: (94.39%) (31535/33408)\n",
      "Epoch: 132 | Batch_idx: 270 |  Loss_1: (0.1557) | Acc_1: (94.36%) (32731/34688)\n",
      "Epoch: 132 | Batch_idx: 280 |  Loss_1: (0.1562) | Acc_1: (94.34%) (33934/35968)\n",
      "Epoch: 132 | Batch_idx: 290 |  Loss_1: (0.1556) | Acc_1: (94.36%) (35149/37248)\n",
      "Epoch: 132 | Batch_idx: 300 |  Loss_1: (0.1562) | Acc_1: (94.35%) (36351/38528)\n",
      "Epoch: 132 | Batch_idx: 310 |  Loss_1: (0.1570) | Acc_1: (94.32%) (37545/39808)\n",
      "Epoch: 132 | Batch_idx: 320 |  Loss_1: (0.1579) | Acc_1: (94.28%) (38736/41088)\n",
      "Epoch: 132 | Batch_idx: 330 |  Loss_1: (0.1582) | Acc_1: (94.26%) (39938/42368)\n",
      "Epoch: 132 | Batch_idx: 340 |  Loss_1: (0.1582) | Acc_1: (94.25%) (41140/43648)\n",
      "Epoch: 132 | Batch_idx: 350 |  Loss_1: (0.1587) | Acc_1: (94.22%) (42330/44928)\n",
      "Epoch: 132 | Batch_idx: 360 |  Loss_1: (0.1592) | Acc_1: (94.19%) (43522/46208)\n",
      "Epoch: 132 | Batch_idx: 370 |  Loss_1: (0.1588) | Acc_1: (94.20%) (44733/47488)\n",
      "Epoch: 132 | Batch_idx: 380 |  Loss_1: (0.1583) | Acc_1: (94.22%) (45947/48768)\n",
      "Epoch: 132 | Batch_idx: 390 |  Loss_1: (0.1586) | Acc_1: (94.21%) (47107/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3207) | Acc: (91.28%) (9128/10000)\n",
      "Epoch: 133 | Batch_idx: 0 |  Loss_1: (0.1618) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 133 | Batch_idx: 10 |  Loss_1: (0.1749) | Acc_1: (93.54%) (1317/1408)\n",
      "Epoch: 133 | Batch_idx: 20 |  Loss_1: (0.1651) | Acc_1: (94.08%) (2529/2688)\n",
      "Epoch: 133 | Batch_idx: 30 |  Loss_1: (0.1687) | Acc_1: (93.90%) (3726/3968)\n",
      "Epoch: 133 | Batch_idx: 40 |  Loss_1: (0.1679) | Acc_1: (93.90%) (4928/5248)\n",
      "Epoch: 133 | Batch_idx: 50 |  Loss_1: (0.1639) | Acc_1: (93.89%) (6129/6528)\n",
      "Epoch: 133 | Batch_idx: 60 |  Loss_1: (0.1621) | Acc_1: (93.97%) (7337/7808)\n",
      "Epoch: 133 | Batch_idx: 70 |  Loss_1: (0.1637) | Acc_1: (93.88%) (8532/9088)\n",
      "Epoch: 133 | Batch_idx: 80 |  Loss_1: (0.1614) | Acc_1: (94.02%) (9748/10368)\n",
      "Epoch: 133 | Batch_idx: 90 |  Loss_1: (0.1592) | Acc_1: (94.11%) (10962/11648)\n",
      "Epoch: 133 | Batch_idx: 100 |  Loss_1: (0.1615) | Acc_1: (94.07%) (12161/12928)\n",
      "Epoch: 133 | Batch_idx: 110 |  Loss_1: (0.1638) | Acc_1: (94.00%) (13356/14208)\n",
      "Epoch: 133 | Batch_idx: 120 |  Loss_1: (0.1667) | Acc_1: (93.91%) (14545/15488)\n",
      "Epoch: 133 | Batch_idx: 130 |  Loss_1: (0.1649) | Acc_1: (93.98%) (15758/16768)\n",
      "Epoch: 133 | Batch_idx: 140 |  Loss_1: (0.1647) | Acc_1: (93.97%) (16960/18048)\n",
      "Epoch: 133 | Batch_idx: 150 |  Loss_1: (0.1645) | Acc_1: (93.99%) (18166/19328)\n",
      "Epoch: 133 | Batch_idx: 160 |  Loss_1: (0.1640) | Acc_1: (94.03%) (19378/20608)\n",
      "Epoch: 133 | Batch_idx: 170 |  Loss_1: (0.1640) | Acc_1: (94.01%) (20577/21888)\n",
      "Epoch: 133 | Batch_idx: 180 |  Loss_1: (0.1634) | Acc_1: (94.03%) (21784/23168)\n",
      "Epoch: 133 | Batch_idx: 190 |  Loss_1: (0.1630) | Acc_1: (94.04%) (22990/24448)\n",
      "Epoch: 133 | Batch_idx: 200 |  Loss_1: (0.1635) | Acc_1: (94.03%) (24192/25728)\n",
      "Epoch: 133 | Batch_idx: 210 |  Loss_1: (0.1621) | Acc_1: (94.08%) (25408/27008)\n",
      "Epoch: 133 | Batch_idx: 220 |  Loss_1: (0.1628) | Acc_1: (94.05%) (26605/28288)\n",
      "Epoch: 133 | Batch_idx: 230 |  Loss_1: (0.1622) | Acc_1: (94.05%) (27810/29568)\n",
      "Epoch: 133 | Batch_idx: 240 |  Loss_1: (0.1627) | Acc_1: (94.04%) (29009/30848)\n",
      "Epoch: 133 | Batch_idx: 250 |  Loss_1: (0.1620) | Acc_1: (94.07%) (30222/32128)\n",
      "Epoch: 133 | Batch_idx: 260 |  Loss_1: (0.1611) | Acc_1: (94.12%) (31442/33408)\n",
      "Epoch: 133 | Batch_idx: 270 |  Loss_1: (0.1610) | Acc_1: (94.12%) (32647/34688)\n",
      "Epoch: 133 | Batch_idx: 280 |  Loss_1: (0.1612) | Acc_1: (94.11%) (33851/35968)\n",
      "Epoch: 133 | Batch_idx: 290 |  Loss_1: (0.1610) | Acc_1: (94.13%) (35060/37248)\n",
      "Epoch: 133 | Batch_idx: 300 |  Loss_1: (0.1604) | Acc_1: (94.14%) (36269/38528)\n",
      "Epoch: 133 | Batch_idx: 310 |  Loss_1: (0.1604) | Acc_1: (94.13%) (37473/39808)\n",
      "Epoch: 133 | Batch_idx: 320 |  Loss_1: (0.1605) | Acc_1: (94.13%) (38677/41088)\n",
      "Epoch: 133 | Batch_idx: 330 |  Loss_1: (0.1611) | Acc_1: (94.11%) (39872/42368)\n",
      "Epoch: 133 | Batch_idx: 340 |  Loss_1: (0.1604) | Acc_1: (94.13%) (41088/43648)\n",
      "Epoch: 133 | Batch_idx: 350 |  Loss_1: (0.1610) | Acc_1: (94.12%) (42284/44928)\n",
      "Epoch: 133 | Batch_idx: 360 |  Loss_1: (0.1609) | Acc_1: (94.13%) (43494/46208)\n",
      "Epoch: 133 | Batch_idx: 370 |  Loss_1: (0.1600) | Acc_1: (94.16%) (44715/47488)\n",
      "Epoch: 133 | Batch_idx: 380 |  Loss_1: (0.1604) | Acc_1: (94.14%) (45910/48768)\n",
      "Epoch: 133 | Batch_idx: 390 |  Loss_1: (0.1597) | Acc_1: (94.17%) (47087/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3235) | Acc: (91.60%) (9160/10000)\n",
      "Epoch: 134 | Batch_idx: 0 |  Loss_1: (0.0963) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 134 | Batch_idx: 10 |  Loss_1: (0.1560) | Acc_1: (93.54%) (1317/1408)\n",
      "Epoch: 134 | Batch_idx: 20 |  Loss_1: (0.1533) | Acc_1: (94.01%) (2527/2688)\n",
      "Epoch: 134 | Batch_idx: 30 |  Loss_1: (0.1554) | Acc_1: (94.03%) (3731/3968)\n",
      "Epoch: 134 | Batch_idx: 40 |  Loss_1: (0.1523) | Acc_1: (94.19%) (4943/5248)\n",
      "Epoch: 134 | Batch_idx: 50 |  Loss_1: (0.1516) | Acc_1: (94.26%) (6153/6528)\n",
      "Epoch: 134 | Batch_idx: 60 |  Loss_1: (0.1561) | Acc_1: (94.25%) (7359/7808)\n",
      "Epoch: 134 | Batch_idx: 70 |  Loss_1: (0.1582) | Acc_1: (94.20%) (8561/9088)\n",
      "Epoch: 134 | Batch_idx: 80 |  Loss_1: (0.1569) | Acc_1: (94.27%) (9774/10368)\n",
      "Epoch: 134 | Batch_idx: 90 |  Loss_1: (0.1594) | Acc_1: (94.20%) (10972/11648)\n",
      "Epoch: 134 | Batch_idx: 100 |  Loss_1: (0.1594) | Acc_1: (94.21%) (12180/12928)\n",
      "Epoch: 134 | Batch_idx: 110 |  Loss_1: (0.1587) | Acc_1: (94.26%) (13393/14208)\n",
      "Epoch: 134 | Batch_idx: 120 |  Loss_1: (0.1599) | Acc_1: (94.21%) (14592/15488)\n",
      "Epoch: 134 | Batch_idx: 130 |  Loss_1: (0.1611) | Acc_1: (94.15%) (15787/16768)\n",
      "Epoch: 134 | Batch_idx: 140 |  Loss_1: (0.1597) | Acc_1: (94.19%) (17000/18048)\n",
      "Epoch: 134 | Batch_idx: 150 |  Loss_1: (0.1601) | Acc_1: (94.19%) (18205/19328)\n",
      "Epoch: 134 | Batch_idx: 160 |  Loss_1: (0.1596) | Acc_1: (94.19%) (19411/20608)\n",
      "Epoch: 134 | Batch_idx: 170 |  Loss_1: (0.1584) | Acc_1: (94.24%) (20627/21888)\n",
      "Epoch: 134 | Batch_idx: 180 |  Loss_1: (0.1591) | Acc_1: (94.25%) (21835/23168)\n",
      "Epoch: 134 | Batch_idx: 190 |  Loss_1: (0.1584) | Acc_1: (94.27%) (23046/24448)\n",
      "Epoch: 134 | Batch_idx: 200 |  Loss_1: (0.1589) | Acc_1: (94.22%) (24242/25728)\n",
      "Epoch: 134 | Batch_idx: 210 |  Loss_1: (0.1580) | Acc_1: (94.26%) (25459/27008)\n",
      "Epoch: 134 | Batch_idx: 220 |  Loss_1: (0.1597) | Acc_1: (94.20%) (26646/28288)\n",
      "Epoch: 134 | Batch_idx: 230 |  Loss_1: (0.1595) | Acc_1: (94.22%) (27858/29568)\n",
      "Epoch: 134 | Batch_idx: 240 |  Loss_1: (0.1602) | Acc_1: (94.20%) (29058/30848)\n",
      "Epoch: 134 | Batch_idx: 250 |  Loss_1: (0.1602) | Acc_1: (94.19%) (30262/32128)\n",
      "Epoch: 134 | Batch_idx: 260 |  Loss_1: (0.1594) | Acc_1: (94.21%) (31475/33408)\n",
      "Epoch: 134 | Batch_idx: 270 |  Loss_1: (0.1591) | Acc_1: (94.23%) (32688/34688)\n",
      "Epoch: 134 | Batch_idx: 280 |  Loss_1: (0.1586) | Acc_1: (94.26%) (33905/35968)\n",
      "Epoch: 134 | Batch_idx: 290 |  Loss_1: (0.1588) | Acc_1: (94.24%) (35102/37248)\n",
      "Epoch: 134 | Batch_idx: 300 |  Loss_1: (0.1586) | Acc_1: (94.25%) (36314/38528)\n",
      "Epoch: 134 | Batch_idx: 310 |  Loss_1: (0.1583) | Acc_1: (94.27%) (37526/39808)\n",
      "Epoch: 134 | Batch_idx: 320 |  Loss_1: (0.1592) | Acc_1: (94.22%) (38713/41088)\n",
      "Epoch: 134 | Batch_idx: 330 |  Loss_1: (0.1593) | Acc_1: (94.21%) (39913/42368)\n",
      "Epoch: 134 | Batch_idx: 340 |  Loss_1: (0.1595) | Acc_1: (94.19%) (41113/43648)\n",
      "Epoch: 134 | Batch_idx: 350 |  Loss_1: (0.1597) | Acc_1: (94.20%) (42323/44928)\n",
      "Epoch: 134 | Batch_idx: 360 |  Loss_1: (0.1602) | Acc_1: (94.18%) (43520/46208)\n",
      "Epoch: 134 | Batch_idx: 370 |  Loss_1: (0.1597) | Acc_1: (94.19%) (44729/47488)\n",
      "Epoch: 134 | Batch_idx: 380 |  Loss_1: (0.1596) | Acc_1: (94.19%) (45935/48768)\n",
      "Epoch: 134 | Batch_idx: 390 |  Loss_1: (0.1597) | Acc_1: (94.20%) (47098/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3441) | Acc: (91.26%) (9126/10000)\n",
      "Epoch: 135 | Batch_idx: 0 |  Loss_1: (0.1202) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 135 | Batch_idx: 10 |  Loss_1: (0.1408) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 135 | Batch_idx: 20 |  Loss_1: (0.1469) | Acc_1: (94.49%) (2540/2688)\n",
      "Epoch: 135 | Batch_idx: 30 |  Loss_1: (0.1405) | Acc_1: (94.68%) (3757/3968)\n",
      "Epoch: 135 | Batch_idx: 40 |  Loss_1: (0.1413) | Acc_1: (94.70%) (4970/5248)\n",
      "Epoch: 135 | Batch_idx: 50 |  Loss_1: (0.1382) | Acc_1: (94.85%) (6192/6528)\n",
      "Epoch: 135 | Batch_idx: 60 |  Loss_1: (0.1385) | Acc_1: (94.84%) (7405/7808)\n",
      "Epoch: 135 | Batch_idx: 70 |  Loss_1: (0.1408) | Acc_1: (94.78%) (8614/9088)\n",
      "Epoch: 135 | Batch_idx: 80 |  Loss_1: (0.1413) | Acc_1: (94.77%) (9826/10368)\n",
      "Epoch: 135 | Batch_idx: 90 |  Loss_1: (0.1411) | Acc_1: (94.75%) (11037/11648)\n",
      "Epoch: 135 | Batch_idx: 100 |  Loss_1: (0.1442) | Acc_1: (94.68%) (12240/12928)\n",
      "Epoch: 135 | Batch_idx: 110 |  Loss_1: (0.1439) | Acc_1: (94.69%) (13453/14208)\n",
      "Epoch: 135 | Batch_idx: 120 |  Loss_1: (0.1416) | Acc_1: (94.75%) (14675/15488)\n",
      "Epoch: 135 | Batch_idx: 130 |  Loss_1: (0.1445) | Acc_1: (94.63%) (15868/16768)\n",
      "Epoch: 135 | Batch_idx: 140 |  Loss_1: (0.1483) | Acc_1: (94.47%) (17050/18048)\n",
      "Epoch: 135 | Batch_idx: 150 |  Loss_1: (0.1497) | Acc_1: (94.45%) (18255/19328)\n",
      "Epoch: 135 | Batch_idx: 160 |  Loss_1: (0.1497) | Acc_1: (94.45%) (19464/20608)\n",
      "Epoch: 135 | Batch_idx: 170 |  Loss_1: (0.1510) | Acc_1: (94.39%) (20659/21888)\n",
      "Epoch: 135 | Batch_idx: 180 |  Loss_1: (0.1509) | Acc_1: (94.39%) (21869/23168)\n",
      "Epoch: 135 | Batch_idx: 190 |  Loss_1: (0.1513) | Acc_1: (94.38%) (23075/24448)\n",
      "Epoch: 135 | Batch_idx: 200 |  Loss_1: (0.1516) | Acc_1: (94.39%) (24285/25728)\n",
      "Epoch: 135 | Batch_idx: 210 |  Loss_1: (0.1538) | Acc_1: (94.32%) (25474/27008)\n",
      "Epoch: 135 | Batch_idx: 220 |  Loss_1: (0.1536) | Acc_1: (94.33%) (26684/28288)\n",
      "Epoch: 135 | Batch_idx: 230 |  Loss_1: (0.1524) | Acc_1: (94.38%) (27907/29568)\n",
      "Epoch: 135 | Batch_idx: 240 |  Loss_1: (0.1522) | Acc_1: (94.38%) (29113/30848)\n",
      "Epoch: 135 | Batch_idx: 250 |  Loss_1: (0.1521) | Acc_1: (94.39%) (30327/32128)\n",
      "Epoch: 135 | Batch_idx: 260 |  Loss_1: (0.1539) | Acc_1: (94.32%) (31510/33408)\n",
      "Epoch: 135 | Batch_idx: 270 |  Loss_1: (0.1535) | Acc_1: (94.34%) (32726/34688)\n",
      "Epoch: 135 | Batch_idx: 280 |  Loss_1: (0.1538) | Acc_1: (94.33%) (33930/35968)\n",
      "Epoch: 135 | Batch_idx: 290 |  Loss_1: (0.1545) | Acc_1: (94.30%) (35126/37248)\n",
      "Epoch: 135 | Batch_idx: 300 |  Loss_1: (0.1551) | Acc_1: (94.30%) (36331/38528)\n",
      "Epoch: 135 | Batch_idx: 310 |  Loss_1: (0.1553) | Acc_1: (94.30%) (37540/39808)\n",
      "Epoch: 135 | Batch_idx: 320 |  Loss_1: (0.1547) | Acc_1: (94.33%) (38757/41088)\n",
      "Epoch: 135 | Batch_idx: 330 |  Loss_1: (0.1550) | Acc_1: (94.32%) (39961/42368)\n",
      "Epoch: 135 | Batch_idx: 340 |  Loss_1: (0.1551) | Acc_1: (94.32%) (41168/43648)\n",
      "Epoch: 135 | Batch_idx: 350 |  Loss_1: (0.1548) | Acc_1: (94.34%) (42385/44928)\n",
      "Epoch: 135 | Batch_idx: 360 |  Loss_1: (0.1543) | Acc_1: (94.36%) (43601/46208)\n",
      "Epoch: 135 | Batch_idx: 370 |  Loss_1: (0.1536) | Acc_1: (94.39%) (44822/47488)\n",
      "Epoch: 135 | Batch_idx: 380 |  Loss_1: (0.1539) | Acc_1: (94.37%) (46021/48768)\n",
      "Epoch: 135 | Batch_idx: 390 |  Loss_1: (0.1539) | Acc_1: (94.38%) (47188/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3124) | Acc: (91.74%) (9174/10000)\n",
      "Epoch: 136 | Batch_idx: 0 |  Loss_1: (0.1506) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 136 | Batch_idx: 10 |  Loss_1: (0.1494) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 136 | Batch_idx: 20 |  Loss_1: (0.1530) | Acc_1: (94.53%) (2541/2688)\n",
      "Epoch: 136 | Batch_idx: 30 |  Loss_1: (0.1562) | Acc_1: (94.28%) (3741/3968)\n",
      "Epoch: 136 | Batch_idx: 40 |  Loss_1: (0.1558) | Acc_1: (94.38%) (4953/5248)\n",
      "Epoch: 136 | Batch_idx: 50 |  Loss_1: (0.1481) | Acc_1: (94.64%) (6178/6528)\n",
      "Epoch: 136 | Batch_idx: 60 |  Loss_1: (0.1486) | Acc_1: (94.62%) (7388/7808)\n",
      "Epoch: 136 | Batch_idx: 70 |  Loss_1: (0.1490) | Acc_1: (94.59%) (8596/9088)\n",
      "Epoch: 136 | Batch_idx: 80 |  Loss_1: (0.1462) | Acc_1: (94.67%) (9815/10368)\n",
      "Epoch: 136 | Batch_idx: 90 |  Loss_1: (0.1489) | Acc_1: (94.55%) (11013/11648)\n",
      "Epoch: 136 | Batch_idx: 100 |  Loss_1: (0.1516) | Acc_1: (94.43%) (12208/12928)\n",
      "Epoch: 136 | Batch_idx: 110 |  Loss_1: (0.1517) | Acc_1: (94.40%) (13412/14208)\n",
      "Epoch: 136 | Batch_idx: 120 |  Loss_1: (0.1553) | Acc_1: (94.23%) (14594/15488)\n",
      "Epoch: 136 | Batch_idx: 130 |  Loss_1: (0.1563) | Acc_1: (94.17%) (15791/16768)\n",
      "Epoch: 136 | Batch_idx: 140 |  Loss_1: (0.1572) | Acc_1: (94.17%) (16996/18048)\n",
      "Epoch: 136 | Batch_idx: 150 |  Loss_1: (0.1576) | Acc_1: (94.18%) (18203/19328)\n",
      "Epoch: 136 | Batch_idx: 160 |  Loss_1: (0.1570) | Acc_1: (94.20%) (19413/20608)\n",
      "Epoch: 136 | Batch_idx: 170 |  Loss_1: (0.1563) | Acc_1: (94.23%) (20624/21888)\n",
      "Epoch: 136 | Batch_idx: 180 |  Loss_1: (0.1586) | Acc_1: (94.14%) (21810/23168)\n",
      "Epoch: 136 | Batch_idx: 190 |  Loss_1: (0.1588) | Acc_1: (94.13%) (23012/24448)\n",
      "Epoch: 136 | Batch_idx: 200 |  Loss_1: (0.1592) | Acc_1: (94.12%) (24214/25728)\n",
      "Epoch: 136 | Batch_idx: 210 |  Loss_1: (0.1579) | Acc_1: (94.14%) (25426/27008)\n",
      "Epoch: 136 | Batch_idx: 220 |  Loss_1: (0.1581) | Acc_1: (94.12%) (26626/28288)\n",
      "Epoch: 136 | Batch_idx: 230 |  Loss_1: (0.1582) | Acc_1: (94.13%) (27831/29568)\n",
      "Epoch: 136 | Batch_idx: 240 |  Loss_1: (0.1575) | Acc_1: (94.16%) (29047/30848)\n",
      "Epoch: 136 | Batch_idx: 250 |  Loss_1: (0.1574) | Acc_1: (94.17%) (30254/32128)\n",
      "Epoch: 136 | Batch_idx: 260 |  Loss_1: (0.1563) | Acc_1: (94.21%) (31475/33408)\n",
      "Epoch: 136 | Batch_idx: 270 |  Loss_1: (0.1548) | Acc_1: (94.27%) (32701/34688)\n",
      "Epoch: 136 | Batch_idx: 280 |  Loss_1: (0.1543) | Acc_1: (94.29%) (33914/35968)\n",
      "Epoch: 136 | Batch_idx: 290 |  Loss_1: (0.1545) | Acc_1: (94.29%) (35121/37248)\n",
      "Epoch: 136 | Batch_idx: 300 |  Loss_1: (0.1545) | Acc_1: (94.28%) (36326/38528)\n",
      "Epoch: 136 | Batch_idx: 310 |  Loss_1: (0.1550) | Acc_1: (94.24%) (37517/39808)\n",
      "Epoch: 136 | Batch_idx: 320 |  Loss_1: (0.1557) | Acc_1: (94.22%) (38715/41088)\n",
      "Epoch: 136 | Batch_idx: 330 |  Loss_1: (0.1562) | Acc_1: (94.21%) (39916/42368)\n",
      "Epoch: 136 | Batch_idx: 340 |  Loss_1: (0.1564) | Acc_1: (94.21%) (41121/43648)\n",
      "Epoch: 136 | Batch_idx: 350 |  Loss_1: (0.1557) | Acc_1: (94.22%) (42332/44928)\n",
      "Epoch: 136 | Batch_idx: 360 |  Loss_1: (0.1559) | Acc_1: (94.21%) (43534/46208)\n",
      "Epoch: 136 | Batch_idx: 370 |  Loss_1: (0.1561) | Acc_1: (94.21%) (44737/47488)\n",
      "Epoch: 136 | Batch_idx: 380 |  Loss_1: (0.1561) | Acc_1: (94.22%) (45947/48768)\n",
      "Epoch: 136 | Batch_idx: 390 |  Loss_1: (0.1563) | Acc_1: (94.19%) (47097/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3069) | Acc: (91.72%) (9172/10000)\n",
      "Epoch: 137 | Batch_idx: 0 |  Loss_1: (0.2227) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 137 | Batch_idx: 10 |  Loss_1: (0.1607) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 137 | Batch_idx: 20 |  Loss_1: (0.1697) | Acc_1: (94.12%) (2530/2688)\n",
      "Epoch: 137 | Batch_idx: 30 |  Loss_1: (0.1676) | Acc_1: (93.95%) (3728/3968)\n",
      "Epoch: 137 | Batch_idx: 40 |  Loss_1: (0.1591) | Acc_1: (94.25%) (4946/5248)\n",
      "Epoch: 137 | Batch_idx: 50 |  Loss_1: (0.1581) | Acc_1: (94.27%) (6154/6528)\n",
      "Epoch: 137 | Batch_idx: 60 |  Loss_1: (0.1600) | Acc_1: (94.19%) (7354/7808)\n",
      "Epoch: 137 | Batch_idx: 70 |  Loss_1: (0.1682) | Acc_1: (93.87%) (8531/9088)\n",
      "Epoch: 137 | Batch_idx: 80 |  Loss_1: (0.1666) | Acc_1: (93.92%) (9738/10368)\n",
      "Epoch: 137 | Batch_idx: 90 |  Loss_1: (0.1645) | Acc_1: (93.99%) (10948/11648)\n",
      "Epoch: 137 | Batch_idx: 100 |  Loss_1: (0.1609) | Acc_1: (94.10%) (12165/12928)\n",
      "Epoch: 137 | Batch_idx: 110 |  Loss_1: (0.1587) | Acc_1: (94.19%) (13383/14208)\n",
      "Epoch: 137 | Batch_idx: 120 |  Loss_1: (0.1588) | Acc_1: (94.21%) (14592/15488)\n",
      "Epoch: 137 | Batch_idx: 130 |  Loss_1: (0.1574) | Acc_1: (94.27%) (15807/16768)\n",
      "Epoch: 137 | Batch_idx: 140 |  Loss_1: (0.1574) | Acc_1: (94.27%) (17013/18048)\n",
      "Epoch: 137 | Batch_idx: 150 |  Loss_1: (0.1574) | Acc_1: (94.25%) (18216/19328)\n",
      "Epoch: 137 | Batch_idx: 160 |  Loss_1: (0.1581) | Acc_1: (94.19%) (19411/20608)\n",
      "Epoch: 137 | Batch_idx: 170 |  Loss_1: (0.1573) | Acc_1: (94.23%) (20625/21888)\n",
      "Epoch: 137 | Batch_idx: 180 |  Loss_1: (0.1570) | Acc_1: (94.24%) (21833/23168)\n",
      "Epoch: 137 | Batch_idx: 190 |  Loss_1: (0.1558) | Acc_1: (94.27%) (23046/24448)\n",
      "Epoch: 137 | Batch_idx: 200 |  Loss_1: (0.1559) | Acc_1: (94.25%) (24248/25728)\n",
      "Epoch: 137 | Batch_idx: 210 |  Loss_1: (0.1559) | Acc_1: (94.24%) (25452/27008)\n",
      "Epoch: 137 | Batch_idx: 220 |  Loss_1: (0.1554) | Acc_1: (94.25%) (26662/28288)\n",
      "Epoch: 137 | Batch_idx: 230 |  Loss_1: (0.1563) | Acc_1: (94.23%) (27862/29568)\n",
      "Epoch: 137 | Batch_idx: 240 |  Loss_1: (0.1570) | Acc_1: (94.21%) (29063/30848)\n",
      "Epoch: 137 | Batch_idx: 250 |  Loss_1: (0.1569) | Acc_1: (94.21%) (30269/32128)\n",
      "Epoch: 137 | Batch_idx: 260 |  Loss_1: (0.1567) | Acc_1: (94.22%) (31478/33408)\n",
      "Epoch: 137 | Batch_idx: 270 |  Loss_1: (0.1563) | Acc_1: (94.25%) (32693/34688)\n",
      "Epoch: 137 | Batch_idx: 280 |  Loss_1: (0.1553) | Acc_1: (94.29%) (33913/35968)\n",
      "Epoch: 137 | Batch_idx: 290 |  Loss_1: (0.1560) | Acc_1: (94.25%) (35108/37248)\n",
      "Epoch: 137 | Batch_idx: 300 |  Loss_1: (0.1556) | Acc_1: (94.27%) (36319/38528)\n",
      "Epoch: 137 | Batch_idx: 310 |  Loss_1: (0.1557) | Acc_1: (94.27%) (37527/39808)\n",
      "Epoch: 137 | Batch_idx: 320 |  Loss_1: (0.1565) | Acc_1: (94.25%) (38726/41088)\n",
      "Epoch: 137 | Batch_idx: 330 |  Loss_1: (0.1554) | Acc_1: (94.30%) (39953/42368)\n",
      "Epoch: 137 | Batch_idx: 340 |  Loss_1: (0.1548) | Acc_1: (94.32%) (41169/43648)\n",
      "Epoch: 137 | Batch_idx: 350 |  Loss_1: (0.1547) | Acc_1: (94.33%) (42381/44928)\n",
      "Epoch: 137 | Batch_idx: 360 |  Loss_1: (0.1545) | Acc_1: (94.34%) (43591/46208)\n",
      "Epoch: 137 | Batch_idx: 370 |  Loss_1: (0.1548) | Acc_1: (94.32%) (44792/47488)\n",
      "Epoch: 137 | Batch_idx: 380 |  Loss_1: (0.1552) | Acc_1: (94.31%) (45995/48768)\n",
      "Epoch: 137 | Batch_idx: 390 |  Loss_1: (0.1553) | Acc_1: (94.32%) (47160/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3312) | Acc: (91.47%) (9147/10000)\n",
      "Epoch: 138 | Batch_idx: 0 |  Loss_1: (0.1682) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 138 | Batch_idx: 10 |  Loss_1: (0.1430) | Acc_1: (94.18%) (1326/1408)\n",
      "Epoch: 138 | Batch_idx: 20 |  Loss_1: (0.1520) | Acc_1: (94.05%) (2528/2688)\n",
      "Epoch: 138 | Batch_idx: 30 |  Loss_1: (0.1517) | Acc_1: (94.23%) (3739/3968)\n",
      "Epoch: 138 | Batch_idx: 40 |  Loss_1: (0.1498) | Acc_1: (94.34%) (4951/5248)\n",
      "Epoch: 138 | Batch_idx: 50 |  Loss_1: (0.1512) | Acc_1: (94.22%) (6151/6528)\n",
      "Epoch: 138 | Batch_idx: 60 |  Loss_1: (0.1475) | Acc_1: (94.35%) (7367/7808)\n",
      "Epoch: 138 | Batch_idx: 70 |  Loss_1: (0.1465) | Acc_1: (94.55%) (8593/9088)\n",
      "Epoch: 138 | Batch_idx: 80 |  Loss_1: (0.1461) | Acc_1: (94.55%) (9803/10368)\n",
      "Epoch: 138 | Batch_idx: 90 |  Loss_1: (0.1447) | Acc_1: (94.62%) (11021/11648)\n",
      "Epoch: 138 | Batch_idx: 100 |  Loss_1: (0.1440) | Acc_1: (94.66%) (12238/12928)\n",
      "Epoch: 138 | Batch_idx: 110 |  Loss_1: (0.1456) | Acc_1: (94.62%) (13444/14208)\n",
      "Epoch: 138 | Batch_idx: 120 |  Loss_1: (0.1450) | Acc_1: (94.65%) (14659/15488)\n",
      "Epoch: 138 | Batch_idx: 130 |  Loss_1: (0.1424) | Acc_1: (94.75%) (15887/16768)\n",
      "Epoch: 138 | Batch_idx: 140 |  Loss_1: (0.1424) | Acc_1: (94.79%) (17108/18048)\n",
      "Epoch: 138 | Batch_idx: 150 |  Loss_1: (0.1430) | Acc_1: (94.82%) (18326/19328)\n",
      "Epoch: 138 | Batch_idx: 160 |  Loss_1: (0.1425) | Acc_1: (94.83%) (19542/20608)\n",
      "Epoch: 138 | Batch_idx: 170 |  Loss_1: (0.1445) | Acc_1: (94.74%) (20736/21888)\n",
      "Epoch: 138 | Batch_idx: 180 |  Loss_1: (0.1471) | Acc_1: (94.63%) (21925/23168)\n",
      "Epoch: 138 | Batch_idx: 190 |  Loss_1: (0.1469) | Acc_1: (94.64%) (23137/24448)\n",
      "Epoch: 138 | Batch_idx: 200 |  Loss_1: (0.1463) | Acc_1: (94.66%) (24355/25728)\n",
      "Epoch: 138 | Batch_idx: 210 |  Loss_1: (0.1468) | Acc_1: (94.62%) (25555/27008)\n",
      "Epoch: 138 | Batch_idx: 220 |  Loss_1: (0.1481) | Acc_1: (94.57%) (26751/28288)\n",
      "Epoch: 138 | Batch_idx: 230 |  Loss_1: (0.1474) | Acc_1: (94.59%) (27969/29568)\n",
      "Epoch: 138 | Batch_idx: 240 |  Loss_1: (0.1481) | Acc_1: (94.55%) (29168/30848)\n",
      "Epoch: 138 | Batch_idx: 250 |  Loss_1: (0.1487) | Acc_1: (94.51%) (30365/32128)\n",
      "Epoch: 138 | Batch_idx: 260 |  Loss_1: (0.1486) | Acc_1: (94.53%) (31580/33408)\n",
      "Epoch: 138 | Batch_idx: 270 |  Loss_1: (0.1489) | Acc_1: (94.52%) (32788/34688)\n",
      "Epoch: 138 | Batch_idx: 280 |  Loss_1: (0.1491) | Acc_1: (94.51%) (33995/35968)\n",
      "Epoch: 138 | Batch_idx: 290 |  Loss_1: (0.1506) | Acc_1: (94.46%) (35186/37248)\n",
      "Epoch: 138 | Batch_idx: 300 |  Loss_1: (0.1509) | Acc_1: (94.47%) (36396/38528)\n",
      "Epoch: 138 | Batch_idx: 310 |  Loss_1: (0.1519) | Acc_1: (94.43%) (37592/39808)\n",
      "Epoch: 138 | Batch_idx: 320 |  Loss_1: (0.1515) | Acc_1: (94.44%) (38803/41088)\n",
      "Epoch: 138 | Batch_idx: 330 |  Loss_1: (0.1517) | Acc_1: (94.43%) (40007/42368)\n",
      "Epoch: 138 | Batch_idx: 340 |  Loss_1: (0.1521) | Acc_1: (94.42%) (41213/43648)\n",
      "Epoch: 138 | Batch_idx: 350 |  Loss_1: (0.1522) | Acc_1: (94.42%) (42420/44928)\n",
      "Epoch: 138 | Batch_idx: 360 |  Loss_1: (0.1520) | Acc_1: (94.42%) (43629/46208)\n",
      "Epoch: 138 | Batch_idx: 370 |  Loss_1: (0.1522) | Acc_1: (94.40%) (44829/47488)\n",
      "Epoch: 138 | Batch_idx: 380 |  Loss_1: (0.1512) | Acc_1: (94.46%) (46064/48768)\n",
      "Epoch: 138 | Batch_idx: 390 |  Loss_1: (0.1511) | Acc_1: (94.46%) (47231/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3301) | Acc: (91.77%) (9177/10000)\n",
      "Epoch: 139 | Batch_idx: 0 |  Loss_1: (0.1598) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 139 | Batch_idx: 10 |  Loss_1: (0.1553) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 139 | Batch_idx: 20 |  Loss_1: (0.1572) | Acc_1: (94.01%) (2527/2688)\n",
      "Epoch: 139 | Batch_idx: 30 |  Loss_1: (0.1550) | Acc_1: (94.25%) (3740/3968)\n",
      "Epoch: 139 | Batch_idx: 40 |  Loss_1: (0.1586) | Acc_1: (94.19%) (4943/5248)\n",
      "Epoch: 139 | Batch_idx: 50 |  Loss_1: (0.1577) | Acc_1: (94.18%) (6148/6528)\n",
      "Epoch: 139 | Batch_idx: 60 |  Loss_1: (0.1549) | Acc_1: (94.29%) (7362/7808)\n",
      "Epoch: 139 | Batch_idx: 70 |  Loss_1: (0.1569) | Acc_1: (94.19%) (8560/9088)\n",
      "Epoch: 139 | Batch_idx: 80 |  Loss_1: (0.1552) | Acc_1: (94.25%) (9772/10368)\n",
      "Epoch: 139 | Batch_idx: 90 |  Loss_1: (0.1558) | Acc_1: (94.21%) (10974/11648)\n",
      "Epoch: 139 | Batch_idx: 100 |  Loss_1: (0.1596) | Acc_1: (94.10%) (12165/12928)\n",
      "Epoch: 139 | Batch_idx: 110 |  Loss_1: (0.1564) | Acc_1: (94.20%) (13384/14208)\n",
      "Epoch: 139 | Batch_idx: 120 |  Loss_1: (0.1564) | Acc_1: (94.23%) (14594/15488)\n",
      "Epoch: 139 | Batch_idx: 130 |  Loss_1: (0.1563) | Acc_1: (94.24%) (15803/16768)\n",
      "Epoch: 139 | Batch_idx: 140 |  Loss_1: (0.1560) | Acc_1: (94.26%) (17012/18048)\n",
      "Epoch: 139 | Batch_idx: 150 |  Loss_1: (0.1569) | Acc_1: (94.25%) (18217/19328)\n",
      "Epoch: 139 | Batch_idx: 160 |  Loss_1: (0.1572) | Acc_1: (94.21%) (19414/20608)\n",
      "Epoch: 139 | Batch_idx: 170 |  Loss_1: (0.1564) | Acc_1: (94.20%) (20618/21888)\n",
      "Epoch: 139 | Batch_idx: 180 |  Loss_1: (0.1569) | Acc_1: (94.19%) (21822/23168)\n",
      "Epoch: 139 | Batch_idx: 190 |  Loss_1: (0.1560) | Acc_1: (94.22%) (23036/24448)\n",
      "Epoch: 139 | Batch_idx: 200 |  Loss_1: (0.1551) | Acc_1: (94.26%) (24250/25728)\n",
      "Epoch: 139 | Batch_idx: 210 |  Loss_1: (0.1551) | Acc_1: (94.26%) (25458/27008)\n",
      "Epoch: 139 | Batch_idx: 220 |  Loss_1: (0.1548) | Acc_1: (94.29%) (26673/28288)\n",
      "Epoch: 139 | Batch_idx: 230 |  Loss_1: (0.1545) | Acc_1: (94.29%) (27881/29568)\n",
      "Epoch: 139 | Batch_idx: 240 |  Loss_1: (0.1548) | Acc_1: (94.27%) (29080/30848)\n",
      "Epoch: 139 | Batch_idx: 250 |  Loss_1: (0.1553) | Acc_1: (94.27%) (30286/32128)\n",
      "Epoch: 139 | Batch_idx: 260 |  Loss_1: (0.1547) | Acc_1: (94.31%) (31507/33408)\n",
      "Epoch: 139 | Batch_idx: 270 |  Loss_1: (0.1544) | Acc_1: (94.33%) (32722/34688)\n",
      "Epoch: 139 | Batch_idx: 280 |  Loss_1: (0.1540) | Acc_1: (94.34%) (33933/35968)\n",
      "Epoch: 139 | Batch_idx: 290 |  Loss_1: (0.1540) | Acc_1: (94.35%) (35142/37248)\n",
      "Epoch: 139 | Batch_idx: 300 |  Loss_1: (0.1537) | Acc_1: (94.35%) (36353/38528)\n",
      "Epoch: 139 | Batch_idx: 310 |  Loss_1: (0.1536) | Acc_1: (94.36%) (37564/39808)\n",
      "Epoch: 139 | Batch_idx: 320 |  Loss_1: (0.1535) | Acc_1: (94.37%) (38773/41088)\n",
      "Epoch: 139 | Batch_idx: 330 |  Loss_1: (0.1540) | Acc_1: (94.34%) (39972/42368)\n",
      "Epoch: 139 | Batch_idx: 340 |  Loss_1: (0.1531) | Acc_1: (94.38%) (41197/43648)\n",
      "Epoch: 139 | Batch_idx: 350 |  Loss_1: (0.1531) | Acc_1: (94.39%) (42408/44928)\n",
      "Epoch: 139 | Batch_idx: 360 |  Loss_1: (0.1536) | Acc_1: (94.38%) (43609/46208)\n",
      "Epoch: 139 | Batch_idx: 370 |  Loss_1: (0.1537) | Acc_1: (94.37%) (44815/47488)\n",
      "Epoch: 139 | Batch_idx: 380 |  Loss_1: (0.1539) | Acc_1: (94.36%) (46018/48768)\n",
      "Epoch: 139 | Batch_idx: 390 |  Loss_1: (0.1546) | Acc_1: (94.33%) (47167/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3008) | Acc: (92.02%) (9202/10000)\n",
      "Epoch: 140 | Batch_idx: 0 |  Loss_1: (0.1146) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 140 | Batch_idx: 10 |  Loss_1: (0.1620) | Acc_1: (93.96%) (1323/1408)\n",
      "Epoch: 140 | Batch_idx: 20 |  Loss_1: (0.1448) | Acc_1: (94.38%) (2537/2688)\n",
      "Epoch: 140 | Batch_idx: 30 |  Loss_1: (0.1533) | Acc_1: (94.23%) (3739/3968)\n",
      "Epoch: 140 | Batch_idx: 40 |  Loss_1: (0.1513) | Acc_1: (94.47%) (4958/5248)\n",
      "Epoch: 140 | Batch_idx: 50 |  Loss_1: (0.1579) | Acc_1: (94.32%) (6157/6528)\n",
      "Epoch: 140 | Batch_idx: 60 |  Loss_1: (0.1527) | Acc_1: (94.45%) (7375/7808)\n",
      "Epoch: 140 | Batch_idx: 70 |  Loss_1: (0.1532) | Acc_1: (94.42%) (8581/9088)\n",
      "Epoch: 140 | Batch_idx: 80 |  Loss_1: (0.1527) | Acc_1: (94.39%) (9786/10368)\n",
      "Epoch: 140 | Batch_idx: 90 |  Loss_1: (0.1509) | Acc_1: (94.43%) (10999/11648)\n",
      "Epoch: 140 | Batch_idx: 100 |  Loss_1: (0.1483) | Acc_1: (94.51%) (12218/12928)\n",
      "Epoch: 140 | Batch_idx: 110 |  Loss_1: (0.1476) | Acc_1: (94.49%) (13425/14208)\n",
      "Epoch: 140 | Batch_idx: 120 |  Loss_1: (0.1492) | Acc_1: (94.45%) (14629/15488)\n",
      "Epoch: 140 | Batch_idx: 130 |  Loss_1: (0.1490) | Acc_1: (94.42%) (15833/16768)\n",
      "Epoch: 140 | Batch_idx: 140 |  Loss_1: (0.1475) | Acc_1: (94.47%) (17050/18048)\n",
      "Epoch: 140 | Batch_idx: 150 |  Loss_1: (0.1465) | Acc_1: (94.52%) (18269/19328)\n",
      "Epoch: 140 | Batch_idx: 160 |  Loss_1: (0.1463) | Acc_1: (94.53%) (19481/20608)\n",
      "Epoch: 140 | Batch_idx: 170 |  Loss_1: (0.1452) | Acc_1: (94.60%) (20707/21888)\n",
      "Epoch: 140 | Batch_idx: 180 |  Loss_1: (0.1451) | Acc_1: (94.59%) (21914/23168)\n",
      "Epoch: 140 | Batch_idx: 190 |  Loss_1: (0.1441) | Acc_1: (94.65%) (23140/24448)\n",
      "Epoch: 140 | Batch_idx: 200 |  Loss_1: (0.1446) | Acc_1: (94.63%) (24347/25728)\n",
      "Epoch: 140 | Batch_idx: 210 |  Loss_1: (0.1464) | Acc_1: (94.57%) (25541/27008)\n",
      "Epoch: 140 | Batch_idx: 220 |  Loss_1: (0.1466) | Acc_1: (94.58%) (26756/28288)\n",
      "Epoch: 140 | Batch_idx: 230 |  Loss_1: (0.1467) | Acc_1: (94.59%) (27968/29568)\n",
      "Epoch: 140 | Batch_idx: 240 |  Loss_1: (0.1470) | Acc_1: (94.57%) (29173/30848)\n",
      "Epoch: 140 | Batch_idx: 250 |  Loss_1: (0.1464) | Acc_1: (94.61%) (30396/32128)\n",
      "Epoch: 140 | Batch_idx: 260 |  Loss_1: (0.1471) | Acc_1: (94.58%) (31597/33408)\n",
      "Epoch: 140 | Batch_idx: 270 |  Loss_1: (0.1472) | Acc_1: (94.57%) (32806/34688)\n",
      "Epoch: 140 | Batch_idx: 280 |  Loss_1: (0.1469) | Acc_1: (94.59%) (34023/35968)\n",
      "Epoch: 140 | Batch_idx: 290 |  Loss_1: (0.1463) | Acc_1: (94.61%) (35240/37248)\n",
      "Epoch: 140 | Batch_idx: 300 |  Loss_1: (0.1461) | Acc_1: (94.63%) (36459/38528)\n",
      "Epoch: 140 | Batch_idx: 310 |  Loss_1: (0.1472) | Acc_1: (94.59%) (37655/39808)\n",
      "Epoch: 140 | Batch_idx: 320 |  Loss_1: (0.1482) | Acc_1: (94.56%) (38852/41088)\n",
      "Epoch: 140 | Batch_idx: 330 |  Loss_1: (0.1484) | Acc_1: (94.56%) (40065/42368)\n",
      "Epoch: 140 | Batch_idx: 340 |  Loss_1: (0.1484) | Acc_1: (94.57%) (41278/43648)\n",
      "Epoch: 140 | Batch_idx: 350 |  Loss_1: (0.1489) | Acc_1: (94.55%) (42479/44928)\n",
      "Epoch: 140 | Batch_idx: 360 |  Loss_1: (0.1490) | Acc_1: (94.54%) (43683/46208)\n",
      "Epoch: 140 | Batch_idx: 370 |  Loss_1: (0.1491) | Acc_1: (94.53%) (44889/47488)\n",
      "Epoch: 140 | Batch_idx: 380 |  Loss_1: (0.1492) | Acc_1: (94.52%) (46096/48768)\n",
      "Epoch: 140 | Batch_idx: 390 |  Loss_1: (0.1494) | Acc_1: (94.52%) (47260/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3051) | Acc: (91.93%) (9193/10000)\n",
      "Epoch: 141 | Batch_idx: 0 |  Loss_1: (0.2239) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 141 | Batch_idx: 10 |  Loss_1: (0.1608) | Acc_1: (93.75%) (1320/1408)\n",
      "Epoch: 141 | Batch_idx: 20 |  Loss_1: (0.1599) | Acc_1: (93.97%) (2526/2688)\n",
      "Epoch: 141 | Batch_idx: 30 |  Loss_1: (0.1613) | Acc_1: (93.90%) (3726/3968)\n",
      "Epoch: 141 | Batch_idx: 40 |  Loss_1: (0.1584) | Acc_1: (94.00%) (4933/5248)\n",
      "Epoch: 141 | Batch_idx: 50 |  Loss_1: (0.1545) | Acc_1: (94.30%) (6156/6528)\n",
      "Epoch: 141 | Batch_idx: 60 |  Loss_1: (0.1521) | Acc_1: (94.39%) (7370/7808)\n",
      "Epoch: 141 | Batch_idx: 70 |  Loss_1: (0.1527) | Acc_1: (94.41%) (8580/9088)\n",
      "Epoch: 141 | Batch_idx: 80 |  Loss_1: (0.1521) | Acc_1: (94.45%) (9793/10368)\n",
      "Epoch: 141 | Batch_idx: 90 |  Loss_1: (0.1512) | Acc_1: (94.47%) (11004/11648)\n",
      "Epoch: 141 | Batch_idx: 100 |  Loss_1: (0.1529) | Acc_1: (94.39%) (12203/12928)\n",
      "Epoch: 141 | Batch_idx: 110 |  Loss_1: (0.1536) | Acc_1: (94.38%) (13409/14208)\n",
      "Epoch: 141 | Batch_idx: 120 |  Loss_1: (0.1550) | Acc_1: (94.31%) (14607/15488)\n",
      "Epoch: 141 | Batch_idx: 130 |  Loss_1: (0.1535) | Acc_1: (94.41%) (15830/16768)\n",
      "Epoch: 141 | Batch_idx: 140 |  Loss_1: (0.1523) | Acc_1: (94.44%) (17044/18048)\n",
      "Epoch: 141 | Batch_idx: 150 |  Loss_1: (0.1535) | Acc_1: (94.39%) (18244/19328)\n",
      "Epoch: 141 | Batch_idx: 160 |  Loss_1: (0.1533) | Acc_1: (94.39%) (19452/20608)\n",
      "Epoch: 141 | Batch_idx: 170 |  Loss_1: (0.1531) | Acc_1: (94.40%) (20662/21888)\n",
      "Epoch: 141 | Batch_idx: 180 |  Loss_1: (0.1530) | Acc_1: (94.39%) (21868/23168)\n",
      "Epoch: 141 | Batch_idx: 190 |  Loss_1: (0.1534) | Acc_1: (94.36%) (23069/24448)\n",
      "Epoch: 141 | Batch_idx: 200 |  Loss_1: (0.1538) | Acc_1: (94.34%) (24272/25728)\n",
      "Epoch: 141 | Batch_idx: 210 |  Loss_1: (0.1540) | Acc_1: (94.34%) (25480/27008)\n",
      "Epoch: 141 | Batch_idx: 220 |  Loss_1: (0.1547) | Acc_1: (94.31%) (26677/28288)\n",
      "Epoch: 141 | Batch_idx: 230 |  Loss_1: (0.1541) | Acc_1: (94.32%) (27890/29568)\n",
      "Epoch: 141 | Batch_idx: 240 |  Loss_1: (0.1543) | Acc_1: (94.33%) (29098/30848)\n",
      "Epoch: 141 | Batch_idx: 250 |  Loss_1: (0.1535) | Acc_1: (94.35%) (30314/32128)\n",
      "Epoch: 141 | Batch_idx: 260 |  Loss_1: (0.1532) | Acc_1: (94.36%) (31524/33408)\n",
      "Epoch: 141 | Batch_idx: 270 |  Loss_1: (0.1525) | Acc_1: (94.38%) (32740/34688)\n",
      "Epoch: 141 | Batch_idx: 280 |  Loss_1: (0.1521) | Acc_1: (94.38%) (33945/35968)\n",
      "Epoch: 141 | Batch_idx: 290 |  Loss_1: (0.1524) | Acc_1: (94.36%) (35146/37248)\n",
      "Epoch: 141 | Batch_idx: 300 |  Loss_1: (0.1520) | Acc_1: (94.37%) (36358/38528)\n",
      "Epoch: 141 | Batch_idx: 310 |  Loss_1: (0.1517) | Acc_1: (94.38%) (37571/39808)\n",
      "Epoch: 141 | Batch_idx: 320 |  Loss_1: (0.1513) | Acc_1: (94.41%) (38790/41088)\n",
      "Epoch: 141 | Batch_idx: 330 |  Loss_1: (0.1513) | Acc_1: (94.41%) (40001/42368)\n",
      "Epoch: 141 | Batch_idx: 340 |  Loss_1: (0.1509) | Acc_1: (94.43%) (41216/43648)\n",
      "Epoch: 141 | Batch_idx: 350 |  Loss_1: (0.1501) | Acc_1: (94.45%) (42436/44928)\n",
      "Epoch: 141 | Batch_idx: 360 |  Loss_1: (0.1496) | Acc_1: (94.47%) (43654/46208)\n",
      "Epoch: 141 | Batch_idx: 370 |  Loss_1: (0.1498) | Acc_1: (94.47%) (44860/47488)\n",
      "Epoch: 141 | Batch_idx: 380 |  Loss_1: (0.1502) | Acc_1: (94.46%) (46066/48768)\n",
      "Epoch: 141 | Batch_idx: 390 |  Loss_1: (0.1501) | Acc_1: (94.45%) (47226/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3214) | Acc: (91.93%) (9193/10000)\n",
      "Epoch: 142 | Batch_idx: 0 |  Loss_1: (0.2255) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 142 | Batch_idx: 10 |  Loss_1: (0.1982) | Acc_1: (93.11%) (1311/1408)\n",
      "Epoch: 142 | Batch_idx: 20 |  Loss_1: (0.1666) | Acc_1: (94.20%) (2532/2688)\n",
      "Epoch: 142 | Batch_idx: 30 |  Loss_1: (0.1718) | Acc_1: (93.93%) (3727/3968)\n",
      "Epoch: 142 | Batch_idx: 40 |  Loss_1: (0.1673) | Acc_1: (94.07%) (4937/5248)\n",
      "Epoch: 142 | Batch_idx: 50 |  Loss_1: (0.1635) | Acc_1: (94.18%) (6148/6528)\n",
      "Epoch: 142 | Batch_idx: 60 |  Loss_1: (0.1585) | Acc_1: (94.33%) (7365/7808)\n",
      "Epoch: 142 | Batch_idx: 70 |  Loss_1: (0.1536) | Acc_1: (94.44%) (8583/9088)\n",
      "Epoch: 142 | Batch_idx: 80 |  Loss_1: (0.1507) | Acc_1: (94.58%) (9806/10368)\n",
      "Epoch: 142 | Batch_idx: 90 |  Loss_1: (0.1501) | Acc_1: (94.64%) (11024/11648)\n",
      "Epoch: 142 | Batch_idx: 100 |  Loss_1: (0.1523) | Acc_1: (94.55%) (12223/12928)\n",
      "Epoch: 142 | Batch_idx: 110 |  Loss_1: (0.1529) | Acc_1: (94.50%) (13427/14208)\n",
      "Epoch: 142 | Batch_idx: 120 |  Loss_1: (0.1511) | Acc_1: (94.52%) (14639/15488)\n",
      "Epoch: 142 | Batch_idx: 130 |  Loss_1: (0.1500) | Acc_1: (94.56%) (15856/16768)\n",
      "Epoch: 142 | Batch_idx: 140 |  Loss_1: (0.1516) | Acc_1: (94.49%) (17054/18048)\n",
      "Epoch: 142 | Batch_idx: 150 |  Loss_1: (0.1509) | Acc_1: (94.52%) (18268/19328)\n",
      "Epoch: 142 | Batch_idx: 160 |  Loss_1: (0.1512) | Acc_1: (94.53%) (19480/20608)\n",
      "Epoch: 142 | Batch_idx: 170 |  Loss_1: (0.1521) | Acc_1: (94.49%) (20683/21888)\n",
      "Epoch: 142 | Batch_idx: 180 |  Loss_1: (0.1517) | Acc_1: (94.50%) (21894/23168)\n",
      "Epoch: 142 | Batch_idx: 190 |  Loss_1: (0.1515) | Acc_1: (94.51%) (23105/24448)\n",
      "Epoch: 142 | Batch_idx: 200 |  Loss_1: (0.1515) | Acc_1: (94.49%) (24311/25728)\n",
      "Epoch: 142 | Batch_idx: 210 |  Loss_1: (0.1519) | Acc_1: (94.48%) (25518/27008)\n",
      "Epoch: 142 | Batch_idx: 220 |  Loss_1: (0.1518) | Acc_1: (94.47%) (26724/28288)\n",
      "Epoch: 142 | Batch_idx: 230 |  Loss_1: (0.1525) | Acc_1: (94.44%) (27924/29568)\n",
      "Epoch: 142 | Batch_idx: 240 |  Loss_1: (0.1521) | Acc_1: (94.44%) (29133/30848)\n",
      "Epoch: 142 | Batch_idx: 250 |  Loss_1: (0.1526) | Acc_1: (94.41%) (30332/32128)\n",
      "Epoch: 142 | Batch_idx: 260 |  Loss_1: (0.1530) | Acc_1: (94.41%) (31540/33408)\n",
      "Epoch: 142 | Batch_idx: 270 |  Loss_1: (0.1527) | Acc_1: (94.41%) (32750/34688)\n",
      "Epoch: 142 | Batch_idx: 280 |  Loss_1: (0.1525) | Acc_1: (94.42%) (33960/35968)\n",
      "Epoch: 142 | Batch_idx: 290 |  Loss_1: (0.1526) | Acc_1: (94.42%) (35170/37248)\n",
      "Epoch: 142 | Batch_idx: 300 |  Loss_1: (0.1517) | Acc_1: (94.44%) (36385/38528)\n",
      "Epoch: 142 | Batch_idx: 310 |  Loss_1: (0.1515) | Acc_1: (94.46%) (37601/39808)\n",
      "Epoch: 142 | Batch_idx: 320 |  Loss_1: (0.1512) | Acc_1: (94.48%) (38818/41088)\n",
      "Epoch: 142 | Batch_idx: 330 |  Loss_1: (0.1503) | Acc_1: (94.51%) (40042/42368)\n",
      "Epoch: 142 | Batch_idx: 340 |  Loss_1: (0.1497) | Acc_1: (94.53%) (41261/43648)\n",
      "Epoch: 142 | Batch_idx: 350 |  Loss_1: (0.1492) | Acc_1: (94.55%) (42478/44928)\n",
      "Epoch: 142 | Batch_idx: 360 |  Loss_1: (0.1493) | Acc_1: (94.53%) (43681/46208)\n",
      "Epoch: 142 | Batch_idx: 370 |  Loss_1: (0.1492) | Acc_1: (94.54%) (44896/47488)\n",
      "Epoch: 142 | Batch_idx: 380 |  Loss_1: (0.1493) | Acc_1: (94.55%) (46108/48768)\n",
      "Epoch: 142 | Batch_idx: 390 |  Loss_1: (0.1496) | Acc_1: (94.53%) (47265/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3325) | Acc: (91.86%) (9186/10000)\n",
      "Epoch: 143 | Batch_idx: 0 |  Loss_1: (0.1127) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 143 | Batch_idx: 10 |  Loss_1: (0.1604) | Acc_1: (94.18%) (1326/1408)\n",
      "Epoch: 143 | Batch_idx: 20 |  Loss_1: (0.1538) | Acc_1: (94.53%) (2541/2688)\n",
      "Epoch: 143 | Batch_idx: 30 |  Loss_1: (0.1527) | Acc_1: (94.61%) (3754/3968)\n",
      "Epoch: 143 | Batch_idx: 40 |  Loss_1: (0.1526) | Acc_1: (94.49%) (4959/5248)\n",
      "Epoch: 143 | Batch_idx: 50 |  Loss_1: (0.1524) | Acc_1: (94.47%) (6167/6528)\n",
      "Epoch: 143 | Batch_idx: 60 |  Loss_1: (0.1510) | Acc_1: (94.47%) (7376/7808)\n",
      "Epoch: 143 | Batch_idx: 70 |  Loss_1: (0.1523) | Acc_1: (94.38%) (8577/9088)\n",
      "Epoch: 143 | Batch_idx: 80 |  Loss_1: (0.1510) | Acc_1: (94.42%) (9789/10368)\n",
      "Epoch: 143 | Batch_idx: 90 |  Loss_1: (0.1495) | Acc_1: (94.49%) (11006/11648)\n",
      "Epoch: 143 | Batch_idx: 100 |  Loss_1: (0.1473) | Acc_1: (94.57%) (12226/12928)\n",
      "Epoch: 143 | Batch_idx: 110 |  Loss_1: (0.1494) | Acc_1: (94.51%) (13428/14208)\n",
      "Epoch: 143 | Batch_idx: 120 |  Loss_1: (0.1494) | Acc_1: (94.51%) (14638/15488)\n",
      "Epoch: 143 | Batch_idx: 130 |  Loss_1: (0.1493) | Acc_1: (94.50%) (15845/16768)\n",
      "Epoch: 143 | Batch_idx: 140 |  Loss_1: (0.1491) | Acc_1: (94.53%) (17060/18048)\n",
      "Epoch: 143 | Batch_idx: 150 |  Loss_1: (0.1494) | Acc_1: (94.51%) (18267/19328)\n",
      "Epoch: 143 | Batch_idx: 160 |  Loss_1: (0.1476) | Acc_1: (94.56%) (19487/20608)\n",
      "Epoch: 143 | Batch_idx: 170 |  Loss_1: (0.1472) | Acc_1: (94.55%) (20695/21888)\n",
      "Epoch: 143 | Batch_idx: 180 |  Loss_1: (0.1478) | Acc_1: (94.52%) (21899/23168)\n",
      "Epoch: 143 | Batch_idx: 190 |  Loss_1: (0.1477) | Acc_1: (94.51%) (23106/24448)\n",
      "Epoch: 143 | Batch_idx: 200 |  Loss_1: (0.1486) | Acc_1: (94.48%) (24309/25728)\n",
      "Epoch: 143 | Batch_idx: 210 |  Loss_1: (0.1494) | Acc_1: (94.43%) (25504/27008)\n",
      "Epoch: 143 | Batch_idx: 220 |  Loss_1: (0.1500) | Acc_1: (94.43%) (26712/28288)\n",
      "Epoch: 143 | Batch_idx: 230 |  Loss_1: (0.1497) | Acc_1: (94.40%) (27913/29568)\n",
      "Epoch: 143 | Batch_idx: 240 |  Loss_1: (0.1502) | Acc_1: (94.41%) (29124/30848)\n",
      "Epoch: 143 | Batch_idx: 250 |  Loss_1: (0.1497) | Acc_1: (94.44%) (30341/32128)\n",
      "Epoch: 143 | Batch_idx: 260 |  Loss_1: (0.1501) | Acc_1: (94.42%) (31544/33408)\n",
      "Epoch: 143 | Batch_idx: 270 |  Loss_1: (0.1502) | Acc_1: (94.42%) (32753/34688)\n",
      "Epoch: 143 | Batch_idx: 280 |  Loss_1: (0.1503) | Acc_1: (94.41%) (33958/35968)\n",
      "Epoch: 143 | Batch_idx: 290 |  Loss_1: (0.1508) | Acc_1: (94.40%) (35162/37248)\n",
      "Epoch: 143 | Batch_idx: 300 |  Loss_1: (0.1509) | Acc_1: (94.40%) (36371/38528)\n",
      "Epoch: 143 | Batch_idx: 310 |  Loss_1: (0.1500) | Acc_1: (94.43%) (37592/39808)\n",
      "Epoch: 143 | Batch_idx: 320 |  Loss_1: (0.1498) | Acc_1: (94.45%) (38806/41088)\n",
      "Epoch: 143 | Batch_idx: 330 |  Loss_1: (0.1500) | Acc_1: (94.43%) (40009/42368)\n",
      "Epoch: 143 | Batch_idx: 340 |  Loss_1: (0.1505) | Acc_1: (94.41%) (41207/43648)\n",
      "Epoch: 143 | Batch_idx: 350 |  Loss_1: (0.1503) | Acc_1: (94.42%) (42420/44928)\n",
      "Epoch: 143 | Batch_idx: 360 |  Loss_1: (0.1508) | Acc_1: (94.41%) (43626/46208)\n",
      "Epoch: 143 | Batch_idx: 370 |  Loss_1: (0.1503) | Acc_1: (94.44%) (44846/47488)\n",
      "Epoch: 143 | Batch_idx: 380 |  Loss_1: (0.1503) | Acc_1: (94.45%) (46060/48768)\n",
      "Epoch: 143 | Batch_idx: 390 |  Loss_1: (0.1502) | Acc_1: (94.44%) (47219/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3121) | Acc: (92.16%) (9216/10000)\n",
      "Epoch: 144 | Batch_idx: 0 |  Loss_1: (0.2096) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 144 | Batch_idx: 10 |  Loss_1: (0.1417) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 144 | Batch_idx: 20 |  Loss_1: (0.1443) | Acc_1: (94.42%) (2538/2688)\n",
      "Epoch: 144 | Batch_idx: 30 |  Loss_1: (0.1446) | Acc_1: (94.46%) (3748/3968)\n",
      "Epoch: 144 | Batch_idx: 40 |  Loss_1: (0.1507) | Acc_1: (94.34%) (4951/5248)\n",
      "Epoch: 144 | Batch_idx: 50 |  Loss_1: (0.1494) | Acc_1: (94.39%) (6162/6528)\n",
      "Epoch: 144 | Batch_idx: 60 |  Loss_1: (0.1464) | Acc_1: (94.49%) (7378/7808)\n",
      "Epoch: 144 | Batch_idx: 70 |  Loss_1: (0.1435) | Acc_1: (94.63%) (8600/9088)\n",
      "Epoch: 144 | Batch_idx: 80 |  Loss_1: (0.1429) | Acc_1: (94.64%) (9812/10368)\n",
      "Epoch: 144 | Batch_idx: 90 |  Loss_1: (0.1448) | Acc_1: (94.52%) (11010/11648)\n",
      "Epoch: 144 | Batch_idx: 100 |  Loss_1: (0.1469) | Acc_1: (94.41%) (12205/12928)\n",
      "Epoch: 144 | Batch_idx: 110 |  Loss_1: (0.1450) | Acc_1: (94.55%) (13433/14208)\n",
      "Epoch: 144 | Batch_idx: 120 |  Loss_1: (0.1450) | Acc_1: (94.53%) (14641/15488)\n",
      "Epoch: 144 | Batch_idx: 130 |  Loss_1: (0.1440) | Acc_1: (94.56%) (15855/16768)\n",
      "Epoch: 144 | Batch_idx: 140 |  Loss_1: (0.1446) | Acc_1: (94.59%) (17071/18048)\n",
      "Epoch: 144 | Batch_idx: 150 |  Loss_1: (0.1444) | Acc_1: (94.63%) (18290/19328)\n",
      "Epoch: 144 | Batch_idx: 160 |  Loss_1: (0.1452) | Acc_1: (94.59%) (19494/20608)\n",
      "Epoch: 144 | Batch_idx: 170 |  Loss_1: (0.1464) | Acc_1: (94.55%) (20696/21888)\n",
      "Epoch: 144 | Batch_idx: 180 |  Loss_1: (0.1475) | Acc_1: (94.52%) (21898/23168)\n",
      "Epoch: 144 | Batch_idx: 190 |  Loss_1: (0.1477) | Acc_1: (94.53%) (23110/24448)\n",
      "Epoch: 144 | Batch_idx: 200 |  Loss_1: (0.1461) | Acc_1: (94.62%) (24343/25728)\n",
      "Epoch: 144 | Batch_idx: 210 |  Loss_1: (0.1466) | Acc_1: (94.59%) (25548/27008)\n",
      "Epoch: 144 | Batch_idx: 220 |  Loss_1: (0.1463) | Acc_1: (94.60%) (26760/28288)\n",
      "Epoch: 144 | Batch_idx: 230 |  Loss_1: (0.1460) | Acc_1: (94.61%) (27975/29568)\n",
      "Epoch: 144 | Batch_idx: 240 |  Loss_1: (0.1451) | Acc_1: (94.64%) (29195/30848)\n",
      "Epoch: 144 | Batch_idx: 250 |  Loss_1: (0.1444) | Acc_1: (94.66%) (30412/32128)\n",
      "Epoch: 144 | Batch_idx: 260 |  Loss_1: (0.1451) | Acc_1: (94.65%) (31621/33408)\n",
      "Epoch: 144 | Batch_idx: 270 |  Loss_1: (0.1462) | Acc_1: (94.62%) (32822/34688)\n",
      "Epoch: 144 | Batch_idx: 280 |  Loss_1: (0.1473) | Acc_1: (94.55%) (34008/35968)\n",
      "Epoch: 144 | Batch_idx: 290 |  Loss_1: (0.1481) | Acc_1: (94.52%) (35207/37248)\n",
      "Epoch: 144 | Batch_idx: 300 |  Loss_1: (0.1479) | Acc_1: (94.53%) (36419/38528)\n",
      "Epoch: 144 | Batch_idx: 310 |  Loss_1: (0.1480) | Acc_1: (94.51%) (37624/39808)\n",
      "Epoch: 144 | Batch_idx: 320 |  Loss_1: (0.1479) | Acc_1: (94.51%) (38834/41088)\n",
      "Epoch: 144 | Batch_idx: 330 |  Loss_1: (0.1486) | Acc_1: (94.49%) (40032/42368)\n",
      "Epoch: 144 | Batch_idx: 340 |  Loss_1: (0.1491) | Acc_1: (94.46%) (41232/43648)\n",
      "Epoch: 144 | Batch_idx: 350 |  Loss_1: (0.1493) | Acc_1: (94.44%) (42431/44928)\n",
      "Epoch: 144 | Batch_idx: 360 |  Loss_1: (0.1498) | Acc_1: (94.42%) (43628/46208)\n",
      "Epoch: 144 | Batch_idx: 370 |  Loss_1: (0.1496) | Acc_1: (94.44%) (44846/47488)\n",
      "Epoch: 144 | Batch_idx: 380 |  Loss_1: (0.1497) | Acc_1: (94.43%) (46052/48768)\n",
      "Epoch: 144 | Batch_idx: 390 |  Loss_1: (0.1499) | Acc_1: (94.42%) (47212/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3299) | Acc: (92.05%) (9205/10000)\n",
      "Epoch: 145 | Batch_idx: 0 |  Loss_1: (0.2118) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 145 | Batch_idx: 10 |  Loss_1: (0.1334) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 145 | Batch_idx: 20 |  Loss_1: (0.1309) | Acc_1: (95.20%) (2559/2688)\n",
      "Epoch: 145 | Batch_idx: 30 |  Loss_1: (0.1372) | Acc_1: (95.04%) (3771/3968)\n",
      "Epoch: 145 | Batch_idx: 40 |  Loss_1: (0.1390) | Acc_1: (95.05%) (4988/5248)\n",
      "Epoch: 145 | Batch_idx: 50 |  Loss_1: (0.1346) | Acc_1: (95.22%) (6216/6528)\n",
      "Epoch: 145 | Batch_idx: 60 |  Loss_1: (0.1356) | Acc_1: (95.15%) (7429/7808)\n",
      "Epoch: 145 | Batch_idx: 70 |  Loss_1: (0.1389) | Acc_1: (94.97%) (8631/9088)\n",
      "Epoch: 145 | Batch_idx: 80 |  Loss_1: (0.1371) | Acc_1: (95.03%) (9853/10368)\n",
      "Epoch: 145 | Batch_idx: 90 |  Loss_1: (0.1432) | Acc_1: (94.81%) (11043/11648)\n",
      "Epoch: 145 | Batch_idx: 100 |  Loss_1: (0.1446) | Acc_1: (94.79%) (12254/12928)\n",
      "Epoch: 145 | Batch_idx: 110 |  Loss_1: (0.1447) | Acc_1: (94.76%) (13463/14208)\n",
      "Epoch: 145 | Batch_idx: 120 |  Loss_1: (0.1455) | Acc_1: (94.71%) (14669/15488)\n",
      "Epoch: 145 | Batch_idx: 130 |  Loss_1: (0.1449) | Acc_1: (94.75%) (15887/16768)\n",
      "Epoch: 145 | Batch_idx: 140 |  Loss_1: (0.1449) | Acc_1: (94.75%) (17100/18048)\n",
      "Epoch: 145 | Batch_idx: 150 |  Loss_1: (0.1450) | Acc_1: (94.72%) (18308/19328)\n",
      "Epoch: 145 | Batch_idx: 160 |  Loss_1: (0.1449) | Acc_1: (94.75%) (19526/20608)\n",
      "Epoch: 145 | Batch_idx: 170 |  Loss_1: (0.1447) | Acc_1: (94.73%) (20735/21888)\n",
      "Epoch: 145 | Batch_idx: 180 |  Loss_1: (0.1429) | Acc_1: (94.79%) (21962/23168)\n",
      "Epoch: 145 | Batch_idx: 190 |  Loss_1: (0.1428) | Acc_1: (94.76%) (23166/24448)\n",
      "Epoch: 145 | Batch_idx: 200 |  Loss_1: (0.1441) | Acc_1: (94.71%) (24368/25728)\n",
      "Epoch: 145 | Batch_idx: 210 |  Loss_1: (0.1425) | Acc_1: (94.78%) (25599/27008)\n",
      "Epoch: 145 | Batch_idx: 220 |  Loss_1: (0.1420) | Acc_1: (94.80%) (26818/28288)\n",
      "Epoch: 145 | Batch_idx: 230 |  Loss_1: (0.1427) | Acc_1: (94.77%) (28022/29568)\n",
      "Epoch: 145 | Batch_idx: 240 |  Loss_1: (0.1431) | Acc_1: (94.75%) (29229/30848)\n",
      "Epoch: 145 | Batch_idx: 250 |  Loss_1: (0.1424) | Acc_1: (94.77%) (30448/32128)\n",
      "Epoch: 145 | Batch_idx: 260 |  Loss_1: (0.1417) | Acc_1: (94.79%) (31668/33408)\n",
      "Epoch: 145 | Batch_idx: 270 |  Loss_1: (0.1412) | Acc_1: (94.80%) (32884/34688)\n",
      "Epoch: 145 | Batch_idx: 280 |  Loss_1: (0.1406) | Acc_1: (94.82%) (34105/35968)\n",
      "Epoch: 145 | Batch_idx: 290 |  Loss_1: (0.1413) | Acc_1: (94.81%) (35313/37248)\n",
      "Epoch: 145 | Batch_idx: 300 |  Loss_1: (0.1422) | Acc_1: (94.76%) (36509/38528)\n",
      "Epoch: 145 | Batch_idx: 310 |  Loss_1: (0.1429) | Acc_1: (94.74%) (37713/39808)\n",
      "Epoch: 145 | Batch_idx: 320 |  Loss_1: (0.1438) | Acc_1: (94.72%) (38917/41088)\n",
      "Epoch: 145 | Batch_idx: 330 |  Loss_1: (0.1440) | Acc_1: (94.70%) (40122/42368)\n",
      "Epoch: 145 | Batch_idx: 340 |  Loss_1: (0.1433) | Acc_1: (94.73%) (41349/43648)\n",
      "Epoch: 145 | Batch_idx: 350 |  Loss_1: (0.1428) | Acc_1: (94.76%) (42572/44928)\n",
      "Epoch: 145 | Batch_idx: 360 |  Loss_1: (0.1431) | Acc_1: (94.75%) (43783/46208)\n",
      "Epoch: 145 | Batch_idx: 370 |  Loss_1: (0.1435) | Acc_1: (94.74%) (44992/47488)\n",
      "Epoch: 145 | Batch_idx: 380 |  Loss_1: (0.1438) | Acc_1: (94.73%) (46196/48768)\n",
      "Epoch: 145 | Batch_idx: 390 |  Loss_1: (0.1434) | Acc_1: (94.73%) (47367/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3221) | Acc: (92.10%) (9210/10000)\n",
      "Epoch: 146 | Batch_idx: 0 |  Loss_1: (0.1637) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 146 | Batch_idx: 10 |  Loss_1: (0.1586) | Acc_1: (94.25%) (1327/1408)\n",
      "Epoch: 146 | Batch_idx: 20 |  Loss_1: (0.1419) | Acc_1: (94.57%) (2542/2688)\n",
      "Epoch: 146 | Batch_idx: 30 |  Loss_1: (0.1419) | Acc_1: (94.63%) (3755/3968)\n",
      "Epoch: 146 | Batch_idx: 40 |  Loss_1: (0.1424) | Acc_1: (94.61%) (4965/5248)\n",
      "Epoch: 146 | Batch_idx: 50 |  Loss_1: (0.1435) | Acc_1: (94.62%) (6177/6528)\n",
      "Epoch: 146 | Batch_idx: 60 |  Loss_1: (0.1351) | Acc_1: (94.95%) (7414/7808)\n",
      "Epoch: 146 | Batch_idx: 70 |  Loss_1: (0.1362) | Acc_1: (94.91%) (8625/9088)\n",
      "Epoch: 146 | Batch_idx: 80 |  Loss_1: (0.1358) | Acc_1: (94.93%) (9842/10368)\n",
      "Epoch: 146 | Batch_idx: 90 |  Loss_1: (0.1355) | Acc_1: (94.97%) (11062/11648)\n",
      "Epoch: 146 | Batch_idx: 100 |  Loss_1: (0.1342) | Acc_1: (94.99%) (12280/12928)\n",
      "Epoch: 146 | Batch_idx: 110 |  Loss_1: (0.1328) | Acc_1: (95.06%) (13506/14208)\n",
      "Epoch: 146 | Batch_idx: 120 |  Loss_1: (0.1350) | Acc_1: (94.98%) (14710/15488)\n",
      "Epoch: 146 | Batch_idx: 130 |  Loss_1: (0.1336) | Acc_1: (95.00%) (15930/16768)\n",
      "Epoch: 146 | Batch_idx: 140 |  Loss_1: (0.1335) | Acc_1: (95.01%) (17148/18048)\n",
      "Epoch: 146 | Batch_idx: 150 |  Loss_1: (0.1326) | Acc_1: (95.03%) (18368/19328)\n",
      "Epoch: 146 | Batch_idx: 160 |  Loss_1: (0.1321) | Acc_1: (95.04%) (19586/20608)\n",
      "Epoch: 146 | Batch_idx: 170 |  Loss_1: (0.1332) | Acc_1: (95.01%) (20796/21888)\n",
      "Epoch: 146 | Batch_idx: 180 |  Loss_1: (0.1331) | Acc_1: (95.04%) (22019/23168)\n",
      "Epoch: 146 | Batch_idx: 190 |  Loss_1: (0.1338) | Acc_1: (95.01%) (23229/24448)\n",
      "Epoch: 146 | Batch_idx: 200 |  Loss_1: (0.1342) | Acc_1: (95.00%) (24441/25728)\n",
      "Epoch: 146 | Batch_idx: 210 |  Loss_1: (0.1352) | Acc_1: (94.98%) (25651/27008)\n",
      "Epoch: 146 | Batch_idx: 220 |  Loss_1: (0.1364) | Acc_1: (94.94%) (26858/28288)\n",
      "Epoch: 146 | Batch_idx: 230 |  Loss_1: (0.1361) | Acc_1: (94.95%) (28076/29568)\n",
      "Epoch: 146 | Batch_idx: 240 |  Loss_1: (0.1359) | Acc_1: (94.96%) (29293/30848)\n",
      "Epoch: 146 | Batch_idx: 250 |  Loss_1: (0.1375) | Acc_1: (94.89%) (30486/32128)\n",
      "Epoch: 146 | Batch_idx: 260 |  Loss_1: (0.1368) | Acc_1: (94.90%) (31705/33408)\n",
      "Epoch: 146 | Batch_idx: 270 |  Loss_1: (0.1362) | Acc_1: (94.91%) (32921/34688)\n",
      "Epoch: 146 | Batch_idx: 280 |  Loss_1: (0.1358) | Acc_1: (94.95%) (34153/35968)\n",
      "Epoch: 146 | Batch_idx: 290 |  Loss_1: (0.1360) | Acc_1: (94.94%) (35363/37248)\n",
      "Epoch: 146 | Batch_idx: 300 |  Loss_1: (0.1358) | Acc_1: (94.95%) (36581/38528)\n",
      "Epoch: 146 | Batch_idx: 310 |  Loss_1: (0.1356) | Acc_1: (94.95%) (37797/39808)\n",
      "Epoch: 146 | Batch_idx: 320 |  Loss_1: (0.1361) | Acc_1: (94.94%) (39007/41088)\n",
      "Epoch: 146 | Batch_idx: 330 |  Loss_1: (0.1363) | Acc_1: (94.93%) (40220/42368)\n",
      "Epoch: 146 | Batch_idx: 340 |  Loss_1: (0.1362) | Acc_1: (94.93%) (41435/43648)\n",
      "Epoch: 146 | Batch_idx: 350 |  Loss_1: (0.1359) | Acc_1: (94.94%) (42656/44928)\n",
      "Epoch: 146 | Batch_idx: 360 |  Loss_1: (0.1355) | Acc_1: (94.96%) (43880/46208)\n",
      "Epoch: 146 | Batch_idx: 370 |  Loss_1: (0.1358) | Acc_1: (94.95%) (45092/47488)\n",
      "Epoch: 146 | Batch_idx: 380 |  Loss_1: (0.1366) | Acc_1: (94.92%) (46292/48768)\n",
      "Epoch: 146 | Batch_idx: 390 |  Loss_1: (0.1376) | Acc_1: (94.90%) (47451/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3461) | Acc: (91.73%) (9173/10000)\n",
      "Epoch: 147 | Batch_idx: 0 |  Loss_1: (0.1667) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 147 | Batch_idx: 10 |  Loss_1: (0.1589) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 147 | Batch_idx: 20 |  Loss_1: (0.1530) | Acc_1: (94.42%) (2538/2688)\n",
      "Epoch: 147 | Batch_idx: 30 |  Loss_1: (0.1549) | Acc_1: (94.41%) (3746/3968)\n",
      "Epoch: 147 | Batch_idx: 40 |  Loss_1: (0.1556) | Acc_1: (94.38%) (4953/5248)\n",
      "Epoch: 147 | Batch_idx: 50 |  Loss_1: (0.1512) | Acc_1: (94.53%) (6171/6528)\n",
      "Epoch: 147 | Batch_idx: 60 |  Loss_1: (0.1483) | Acc_1: (94.68%) (7393/7808)\n",
      "Epoch: 147 | Batch_idx: 70 |  Loss_1: (0.1473) | Acc_1: (94.75%) (8611/9088)\n",
      "Epoch: 147 | Batch_idx: 80 |  Loss_1: (0.1472) | Acc_1: (94.75%) (9824/10368)\n",
      "Epoch: 147 | Batch_idx: 90 |  Loss_1: (0.1457) | Acc_1: (94.80%) (11042/11648)\n",
      "Epoch: 147 | Batch_idx: 100 |  Loss_1: (0.1443) | Acc_1: (94.83%) (12260/12928)\n",
      "Epoch: 147 | Batch_idx: 110 |  Loss_1: (0.1433) | Acc_1: (94.83%) (13474/14208)\n",
      "Epoch: 147 | Batch_idx: 120 |  Loss_1: (0.1443) | Acc_1: (94.75%) (14675/15488)\n",
      "Epoch: 147 | Batch_idx: 130 |  Loss_1: (0.1446) | Acc_1: (94.70%) (15879/16768)\n",
      "Epoch: 147 | Batch_idx: 140 |  Loss_1: (0.1450) | Acc_1: (94.65%) (17083/18048)\n",
      "Epoch: 147 | Batch_idx: 150 |  Loss_1: (0.1439) | Acc_1: (94.70%) (18304/19328)\n",
      "Epoch: 147 | Batch_idx: 160 |  Loss_1: (0.1432) | Acc_1: (94.71%) (19518/20608)\n",
      "Epoch: 147 | Batch_idx: 170 |  Loss_1: (0.1440) | Acc_1: (94.68%) (20724/21888)\n",
      "Epoch: 147 | Batch_idx: 180 |  Loss_1: (0.1452) | Acc_1: (94.61%) (21920/23168)\n",
      "Epoch: 147 | Batch_idx: 190 |  Loss_1: (0.1433) | Acc_1: (94.69%) (23149/24448)\n",
      "Epoch: 147 | Batch_idx: 200 |  Loss_1: (0.1450) | Acc_1: (94.64%) (24349/25728)\n",
      "Epoch: 147 | Batch_idx: 210 |  Loss_1: (0.1457) | Acc_1: (94.61%) (25552/27008)\n",
      "Epoch: 147 | Batch_idx: 220 |  Loss_1: (0.1463) | Acc_1: (94.58%) (26755/28288)\n",
      "Epoch: 147 | Batch_idx: 230 |  Loss_1: (0.1456) | Acc_1: (94.60%) (27972/29568)\n",
      "Epoch: 147 | Batch_idx: 240 |  Loss_1: (0.1455) | Acc_1: (94.62%) (29189/30848)\n",
      "Epoch: 147 | Batch_idx: 250 |  Loss_1: (0.1456) | Acc_1: (94.62%) (30401/32128)\n",
      "Epoch: 147 | Batch_idx: 260 |  Loss_1: (0.1461) | Acc_1: (94.59%) (31599/33408)\n",
      "Epoch: 147 | Batch_idx: 270 |  Loss_1: (0.1468) | Acc_1: (94.57%) (32806/34688)\n",
      "Epoch: 147 | Batch_idx: 280 |  Loss_1: (0.1465) | Acc_1: (94.59%) (34021/35968)\n",
      "Epoch: 147 | Batch_idx: 290 |  Loss_1: (0.1465) | Acc_1: (94.60%) (35235/37248)\n",
      "Epoch: 147 | Batch_idx: 300 |  Loss_1: (0.1462) | Acc_1: (94.60%) (36447/38528)\n",
      "Epoch: 147 | Batch_idx: 310 |  Loss_1: (0.1472) | Acc_1: (94.56%) (37643/39808)\n",
      "Epoch: 147 | Batch_idx: 320 |  Loss_1: (0.1469) | Acc_1: (94.58%) (38860/41088)\n",
      "Epoch: 147 | Batch_idx: 330 |  Loss_1: (0.1465) | Acc_1: (94.59%) (40076/42368)\n",
      "Epoch: 147 | Batch_idx: 340 |  Loss_1: (0.1463) | Acc_1: (94.60%) (41293/43648)\n",
      "Epoch: 147 | Batch_idx: 350 |  Loss_1: (0.1463) | Acc_1: (94.62%) (42510/44928)\n",
      "Epoch: 147 | Batch_idx: 360 |  Loss_1: (0.1469) | Acc_1: (94.60%) (43714/46208)\n",
      "Epoch: 147 | Batch_idx: 370 |  Loss_1: (0.1469) | Acc_1: (94.59%) (44920/47488)\n",
      "Epoch: 147 | Batch_idx: 380 |  Loss_1: (0.1467) | Acc_1: (94.60%) (46135/48768)\n",
      "Epoch: 147 | Batch_idx: 390 |  Loss_1: (0.1471) | Acc_1: (94.58%) (47288/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3210) | Acc: (92.13%) (9213/10000)\n",
      "Epoch: 148 | Batch_idx: 0 |  Loss_1: (0.1048) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 148 | Batch_idx: 10 |  Loss_1: (0.1287) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 148 | Batch_idx: 20 |  Loss_1: (0.1330) | Acc_1: (95.01%) (2554/2688)\n",
      "Epoch: 148 | Batch_idx: 30 |  Loss_1: (0.1321) | Acc_1: (95.01%) (3770/3968)\n",
      "Epoch: 148 | Batch_idx: 40 |  Loss_1: (0.1370) | Acc_1: (94.70%) (4970/5248)\n",
      "Epoch: 148 | Batch_idx: 50 |  Loss_1: (0.1378) | Acc_1: (94.79%) (6188/6528)\n",
      "Epoch: 148 | Batch_idx: 60 |  Loss_1: (0.1358) | Acc_1: (94.86%) (7407/7808)\n",
      "Epoch: 148 | Batch_idx: 70 |  Loss_1: (0.1350) | Acc_1: (94.88%) (8623/9088)\n",
      "Epoch: 148 | Batch_idx: 80 |  Loss_1: (0.1356) | Acc_1: (94.86%) (9835/10368)\n",
      "Epoch: 148 | Batch_idx: 90 |  Loss_1: (0.1379) | Acc_1: (94.77%) (11039/11648)\n",
      "Epoch: 148 | Batch_idx: 100 |  Loss_1: (0.1394) | Acc_1: (94.74%) (12248/12928)\n",
      "Epoch: 148 | Batch_idx: 110 |  Loss_1: (0.1421) | Acc_1: (94.69%) (13454/14208)\n",
      "Epoch: 148 | Batch_idx: 120 |  Loss_1: (0.1402) | Acc_1: (94.76%) (14677/15488)\n",
      "Epoch: 148 | Batch_idx: 130 |  Loss_1: (0.1418) | Acc_1: (94.67%) (15874/16768)\n",
      "Epoch: 148 | Batch_idx: 140 |  Loss_1: (0.1422) | Acc_1: (94.66%) (17085/18048)\n",
      "Epoch: 148 | Batch_idx: 150 |  Loss_1: (0.1407) | Acc_1: (94.75%) (18313/19328)\n",
      "Epoch: 148 | Batch_idx: 160 |  Loss_1: (0.1428) | Acc_1: (94.67%) (19510/20608)\n",
      "Epoch: 148 | Batch_idx: 170 |  Loss_1: (0.1425) | Acc_1: (94.68%) (20723/21888)\n",
      "Epoch: 148 | Batch_idx: 180 |  Loss_1: (0.1417) | Acc_1: (94.70%) (21941/23168)\n",
      "Epoch: 148 | Batch_idx: 190 |  Loss_1: (0.1433) | Acc_1: (94.63%) (23134/24448)\n",
      "Epoch: 148 | Batch_idx: 200 |  Loss_1: (0.1430) | Acc_1: (94.64%) (24350/25728)\n",
      "Epoch: 148 | Batch_idx: 210 |  Loss_1: (0.1422) | Acc_1: (94.69%) (25573/27008)\n",
      "Epoch: 148 | Batch_idx: 220 |  Loss_1: (0.1425) | Acc_1: (94.66%) (26777/28288)\n",
      "Epoch: 148 | Batch_idx: 230 |  Loss_1: (0.1418) | Acc_1: (94.70%) (28000/29568)\n",
      "Epoch: 148 | Batch_idx: 240 |  Loss_1: (0.1419) | Acc_1: (94.68%) (29208/30848)\n",
      "Epoch: 148 | Batch_idx: 250 |  Loss_1: (0.1415) | Acc_1: (94.70%) (30424/32128)\n",
      "Epoch: 148 | Batch_idx: 260 |  Loss_1: (0.1413) | Acc_1: (94.69%) (31635/33408)\n",
      "Epoch: 148 | Batch_idx: 270 |  Loss_1: (0.1419) | Acc_1: (94.68%) (32842/34688)\n",
      "Epoch: 148 | Batch_idx: 280 |  Loss_1: (0.1422) | Acc_1: (94.67%) (34051/35968)\n",
      "Epoch: 148 | Batch_idx: 290 |  Loss_1: (0.1426) | Acc_1: (94.67%) (35261/37248)\n",
      "Epoch: 148 | Batch_idx: 300 |  Loss_1: (0.1429) | Acc_1: (94.66%) (36471/38528)\n",
      "Epoch: 148 | Batch_idx: 310 |  Loss_1: (0.1429) | Acc_1: (94.65%) (37680/39808)\n",
      "Epoch: 148 | Batch_idx: 320 |  Loss_1: (0.1436) | Acc_1: (94.63%) (38883/41088)\n",
      "Epoch: 148 | Batch_idx: 330 |  Loss_1: (0.1440) | Acc_1: (94.63%) (40092/42368)\n",
      "Epoch: 148 | Batch_idx: 340 |  Loss_1: (0.1443) | Acc_1: (94.63%) (41303/43648)\n",
      "Epoch: 148 | Batch_idx: 350 |  Loss_1: (0.1445) | Acc_1: (94.63%) (42515/44928)\n",
      "Epoch: 148 | Batch_idx: 360 |  Loss_1: (0.1446) | Acc_1: (94.63%) (43726/46208)\n",
      "Epoch: 148 | Batch_idx: 370 |  Loss_1: (0.1441) | Acc_1: (94.65%) (44949/47488)\n",
      "Epoch: 148 | Batch_idx: 380 |  Loss_1: (0.1445) | Acc_1: (94.64%) (46152/48768)\n",
      "Epoch: 148 | Batch_idx: 390 |  Loss_1: (0.1447) | Acc_1: (94.64%) (47319/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3097) | Acc: (92.20%) (9220/10000)\n",
      "Epoch: 149 | Batch_idx: 0 |  Loss_1: (0.2359) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 149 | Batch_idx: 10 |  Loss_1: (0.1609) | Acc_1: (94.03%) (1324/1408)\n",
      "Epoch: 149 | Batch_idx: 20 |  Loss_1: (0.1453) | Acc_1: (94.72%) (2546/2688)\n",
      "Epoch: 149 | Batch_idx: 30 |  Loss_1: (0.1413) | Acc_1: (94.71%) (3758/3968)\n",
      "Epoch: 149 | Batch_idx: 40 |  Loss_1: (0.1381) | Acc_1: (94.82%) (4976/5248)\n",
      "Epoch: 149 | Batch_idx: 50 |  Loss_1: (0.1360) | Acc_1: (94.88%) (6194/6528)\n",
      "Epoch: 149 | Batch_idx: 60 |  Loss_1: (0.1369) | Acc_1: (94.85%) (7406/7808)\n",
      "Epoch: 149 | Batch_idx: 70 |  Loss_1: (0.1392) | Acc_1: (94.76%) (8612/9088)\n",
      "Epoch: 149 | Batch_idx: 80 |  Loss_1: (0.1369) | Acc_1: (94.87%) (9836/10368)\n",
      "Epoch: 149 | Batch_idx: 90 |  Loss_1: (0.1355) | Acc_1: (94.94%) (11059/11648)\n",
      "Epoch: 149 | Batch_idx: 100 |  Loss_1: (0.1327) | Acc_1: (95.05%) (12288/12928)\n",
      "Epoch: 149 | Batch_idx: 110 |  Loss_1: (0.1324) | Acc_1: (95.09%) (13510/14208)\n",
      "Epoch: 149 | Batch_idx: 120 |  Loss_1: (0.1348) | Acc_1: (95.00%) (14714/15488)\n",
      "Epoch: 149 | Batch_idx: 130 |  Loss_1: (0.1345) | Acc_1: (95.02%) (15933/16768)\n",
      "Epoch: 149 | Batch_idx: 140 |  Loss_1: (0.1362) | Acc_1: (94.98%) (17142/18048)\n",
      "Epoch: 149 | Batch_idx: 150 |  Loss_1: (0.1352) | Acc_1: (95.03%) (18367/19328)\n",
      "Epoch: 149 | Batch_idx: 160 |  Loss_1: (0.1372) | Acc_1: (94.95%) (19567/20608)\n",
      "Epoch: 149 | Batch_idx: 170 |  Loss_1: (0.1368) | Acc_1: (94.96%) (20785/21888)\n",
      "Epoch: 149 | Batch_idx: 180 |  Loss_1: (0.1367) | Acc_1: (94.97%) (22002/23168)\n",
      "Epoch: 149 | Batch_idx: 190 |  Loss_1: (0.1358) | Acc_1: (95.01%) (23228/24448)\n",
      "Epoch: 149 | Batch_idx: 200 |  Loss_1: (0.1351) | Acc_1: (95.03%) (24450/25728)\n",
      "Epoch: 149 | Batch_idx: 210 |  Loss_1: (0.1360) | Acc_1: (95.01%) (25661/27008)\n",
      "Epoch: 149 | Batch_idx: 220 |  Loss_1: (0.1357) | Acc_1: (95.02%) (26879/28288)\n",
      "Epoch: 149 | Batch_idx: 230 |  Loss_1: (0.1366) | Acc_1: (94.99%) (28086/29568)\n",
      "Epoch: 149 | Batch_idx: 240 |  Loss_1: (0.1372) | Acc_1: (94.97%) (29297/30848)\n",
      "Epoch: 149 | Batch_idx: 250 |  Loss_1: (0.1374) | Acc_1: (94.97%) (30511/32128)\n",
      "Epoch: 149 | Batch_idx: 260 |  Loss_1: (0.1373) | Acc_1: (94.96%) (31724/33408)\n",
      "Epoch: 149 | Batch_idx: 270 |  Loss_1: (0.1372) | Acc_1: (94.97%) (32944/34688)\n",
      "Epoch: 149 | Batch_idx: 280 |  Loss_1: (0.1376) | Acc_1: (94.96%) (34157/35968)\n",
      "Epoch: 149 | Batch_idx: 290 |  Loss_1: (0.1374) | Acc_1: (94.97%) (35374/37248)\n",
      "Epoch: 149 | Batch_idx: 300 |  Loss_1: (0.1388) | Acc_1: (94.92%) (36569/38528)\n",
      "Epoch: 149 | Batch_idx: 310 |  Loss_1: (0.1380) | Acc_1: (94.94%) (37794/39808)\n",
      "Epoch: 149 | Batch_idx: 320 |  Loss_1: (0.1376) | Acc_1: (94.97%) (39020/41088)\n",
      "Epoch: 149 | Batch_idx: 330 |  Loss_1: (0.1375) | Acc_1: (94.97%) (40236/42368)\n",
      "Epoch: 149 | Batch_idx: 340 |  Loss_1: (0.1382) | Acc_1: (94.94%) (41439/43648)\n",
      "Epoch: 149 | Batch_idx: 350 |  Loss_1: (0.1382) | Acc_1: (94.93%) (42651/44928)\n",
      "Epoch: 149 | Batch_idx: 360 |  Loss_1: (0.1376) | Acc_1: (94.95%) (43873/46208)\n",
      "Epoch: 149 | Batch_idx: 370 |  Loss_1: (0.1380) | Acc_1: (94.92%) (45077/47488)\n",
      "Epoch: 149 | Batch_idx: 380 |  Loss_1: (0.1382) | Acc_1: (94.92%) (46290/48768)\n",
      "Epoch: 149 | Batch_idx: 390 |  Loss_1: (0.1385) | Acc_1: (94.91%) (47457/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3176) | Acc: (92.11%) (9211/10000)\n",
      "Epoch: 150 | Batch_idx: 0 |  Loss_1: (0.1420) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 150 | Batch_idx: 10 |  Loss_1: (0.1444) | Acc_1: (94.53%) (1331/1408)\n",
      "Epoch: 150 | Batch_idx: 20 |  Loss_1: (0.1486) | Acc_1: (94.42%) (2538/2688)\n",
      "Epoch: 150 | Batch_idx: 30 |  Loss_1: (0.1449) | Acc_1: (94.68%) (3757/3968)\n",
      "Epoch: 150 | Batch_idx: 40 |  Loss_1: (0.1425) | Acc_1: (94.68%) (4969/5248)\n",
      "Epoch: 150 | Batch_idx: 50 |  Loss_1: (0.1434) | Acc_1: (94.67%) (6180/6528)\n",
      "Epoch: 150 | Batch_idx: 60 |  Loss_1: (0.1416) | Acc_1: (94.71%) (7395/7808)\n",
      "Epoch: 150 | Batch_idx: 70 |  Loss_1: (0.1449) | Acc_1: (94.56%) (8594/9088)\n",
      "Epoch: 150 | Batch_idx: 80 |  Loss_1: (0.1453) | Acc_1: (94.53%) (9801/10368)\n",
      "Epoch: 150 | Batch_idx: 90 |  Loss_1: (0.1437) | Acc_1: (94.58%) (11017/11648)\n",
      "Epoch: 150 | Batch_idx: 100 |  Loss_1: (0.1437) | Acc_1: (94.59%) (12228/12928)\n",
      "Epoch: 150 | Batch_idx: 110 |  Loss_1: (0.1417) | Acc_1: (94.71%) (13457/14208)\n",
      "Epoch: 150 | Batch_idx: 120 |  Loss_1: (0.1439) | Acc_1: (94.63%) (14657/15488)\n",
      "Epoch: 150 | Batch_idx: 130 |  Loss_1: (0.1420) | Acc_1: (94.70%) (15880/16768)\n",
      "Epoch: 150 | Batch_idx: 140 |  Loss_1: (0.1394) | Acc_1: (94.81%) (17111/18048)\n",
      "Epoch: 150 | Batch_idx: 150 |  Loss_1: (0.1381) | Acc_1: (94.85%) (18333/19328)\n",
      "Epoch: 150 | Batch_idx: 160 |  Loss_1: (0.1383) | Acc_1: (94.84%) (19545/20608)\n",
      "Epoch: 150 | Batch_idx: 170 |  Loss_1: (0.1380) | Acc_1: (94.85%) (20760/21888)\n",
      "Epoch: 150 | Batch_idx: 180 |  Loss_1: (0.1382) | Acc_1: (94.85%) (21975/23168)\n",
      "Epoch: 150 | Batch_idx: 190 |  Loss_1: (0.1374) | Acc_1: (94.88%) (23196/24448)\n",
      "Epoch: 150 | Batch_idx: 200 |  Loss_1: (0.1373) | Acc_1: (94.88%) (24412/25728)\n",
      "Epoch: 150 | Batch_idx: 210 |  Loss_1: (0.1370) | Acc_1: (94.89%) (25627/27008)\n",
      "Epoch: 150 | Batch_idx: 220 |  Loss_1: (0.1371) | Acc_1: (94.89%) (26843/28288)\n",
      "Epoch: 150 | Batch_idx: 230 |  Loss_1: (0.1367) | Acc_1: (94.91%) (28062/29568)\n",
      "Epoch: 150 | Batch_idx: 240 |  Loss_1: (0.1367) | Acc_1: (94.90%) (29274/30848)\n",
      "Epoch: 150 | Batch_idx: 250 |  Loss_1: (0.1372) | Acc_1: (94.89%) (30486/32128)\n",
      "Epoch: 150 | Batch_idx: 260 |  Loss_1: (0.1370) | Acc_1: (94.90%) (31705/33408)\n",
      "Epoch: 150 | Batch_idx: 270 |  Loss_1: (0.1373) | Acc_1: (94.89%) (32914/34688)\n",
      "Epoch: 150 | Batch_idx: 280 |  Loss_1: (0.1368) | Acc_1: (94.90%) (34133/35968)\n",
      "Epoch: 150 | Batch_idx: 290 |  Loss_1: (0.1372) | Acc_1: (94.90%) (35350/37248)\n",
      "Epoch: 150 | Batch_idx: 300 |  Loss_1: (0.1382) | Acc_1: (94.85%) (36542/38528)\n",
      "Epoch: 150 | Batch_idx: 310 |  Loss_1: (0.1381) | Acc_1: (94.87%) (37766/39808)\n",
      "Epoch: 150 | Batch_idx: 320 |  Loss_1: (0.1383) | Acc_1: (94.85%) (38973/41088)\n",
      "Epoch: 150 | Batch_idx: 330 |  Loss_1: (0.1383) | Acc_1: (94.86%) (40192/42368)\n",
      "Epoch: 150 | Batch_idx: 340 |  Loss_1: (0.1379) | Acc_1: (94.88%) (41414/43648)\n",
      "Epoch: 150 | Batch_idx: 350 |  Loss_1: (0.1380) | Acc_1: (94.86%) (42620/44928)\n",
      "Epoch: 150 | Batch_idx: 360 |  Loss_1: (0.1371) | Acc_1: (94.90%) (43851/46208)\n",
      "Epoch: 150 | Batch_idx: 370 |  Loss_1: (0.1378) | Acc_1: (94.88%) (45056/47488)\n",
      "Epoch: 150 | Batch_idx: 380 |  Loss_1: (0.1384) | Acc_1: (94.86%) (46261/48768)\n",
      "Epoch: 150 | Batch_idx: 390 |  Loss_1: (0.1381) | Acc_1: (94.87%) (47437/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3271) | Acc: (92.14%) (9214/10000)\n",
      "Epoch: 151 | Batch_idx: 0 |  Loss_1: (0.0756) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 151 | Batch_idx: 10 |  Loss_1: (0.1116) | Acc_1: (96.24%) (1355/1408)\n",
      "Epoch: 151 | Batch_idx: 20 |  Loss_1: (0.1391) | Acc_1: (95.05%) (2555/2688)\n",
      "Epoch: 151 | Batch_idx: 30 |  Loss_1: (0.1401) | Acc_1: (95.01%) (3770/3968)\n",
      "Epoch: 151 | Batch_idx: 40 |  Loss_1: (0.1362) | Acc_1: (95.10%) (4991/5248)\n",
      "Epoch: 151 | Batch_idx: 50 |  Loss_1: (0.1385) | Acc_1: (95.05%) (6205/6528)\n",
      "Epoch: 151 | Batch_idx: 60 |  Loss_1: (0.1377) | Acc_1: (95.13%) (7428/7808)\n",
      "Epoch: 151 | Batch_idx: 70 |  Loss_1: (0.1355) | Acc_1: (95.18%) (8650/9088)\n",
      "Epoch: 151 | Batch_idx: 80 |  Loss_1: (0.1345) | Acc_1: (95.13%) (9863/10368)\n",
      "Epoch: 151 | Batch_idx: 90 |  Loss_1: (0.1340) | Acc_1: (95.13%) (11081/11648)\n",
      "Epoch: 151 | Batch_idx: 100 |  Loss_1: (0.1387) | Acc_1: (94.94%) (12274/12928)\n",
      "Epoch: 151 | Batch_idx: 110 |  Loss_1: (0.1386) | Acc_1: (94.97%) (13493/14208)\n",
      "Epoch: 151 | Batch_idx: 120 |  Loss_1: (0.1388) | Acc_1: (94.96%) (14707/15488)\n",
      "Epoch: 151 | Batch_idx: 130 |  Loss_1: (0.1383) | Acc_1: (95.00%) (15930/16768)\n",
      "Epoch: 151 | Batch_idx: 140 |  Loss_1: (0.1374) | Acc_1: (94.99%) (17144/18048)\n",
      "Epoch: 151 | Batch_idx: 150 |  Loss_1: (0.1382) | Acc_1: (94.95%) (18351/19328)\n",
      "Epoch: 151 | Batch_idx: 160 |  Loss_1: (0.1384) | Acc_1: (94.95%) (19568/20608)\n",
      "Epoch: 151 | Batch_idx: 170 |  Loss_1: (0.1379) | Acc_1: (94.95%) (20783/21888)\n",
      "Epoch: 151 | Batch_idx: 180 |  Loss_1: (0.1392) | Acc_1: (94.89%) (21984/23168)\n",
      "Epoch: 151 | Batch_idx: 190 |  Loss_1: (0.1385) | Acc_1: (94.91%) (23204/24448)\n",
      "Epoch: 151 | Batch_idx: 200 |  Loss_1: (0.1380) | Acc_1: (94.92%) (24422/25728)\n",
      "Epoch: 151 | Batch_idx: 210 |  Loss_1: (0.1371) | Acc_1: (94.95%) (25644/27008)\n",
      "Epoch: 151 | Batch_idx: 220 |  Loss_1: (0.1367) | Acc_1: (94.96%) (26863/28288)\n",
      "Epoch: 151 | Batch_idx: 230 |  Loss_1: (0.1380) | Acc_1: (94.91%) (28064/29568)\n",
      "Epoch: 151 | Batch_idx: 240 |  Loss_1: (0.1376) | Acc_1: (94.93%) (29283/30848)\n",
      "Epoch: 151 | Batch_idx: 250 |  Loss_1: (0.1381) | Acc_1: (94.92%) (30496/32128)\n",
      "Epoch: 151 | Batch_idx: 260 |  Loss_1: (0.1373) | Acc_1: (94.95%) (31722/33408)\n",
      "Epoch: 151 | Batch_idx: 270 |  Loss_1: (0.1367) | Acc_1: (94.97%) (32942/34688)\n",
      "Epoch: 151 | Batch_idx: 280 |  Loss_1: (0.1367) | Acc_1: (94.97%) (34160/35968)\n",
      "Epoch: 151 | Batch_idx: 290 |  Loss_1: (0.1362) | Acc_1: (94.99%) (35383/37248)\n",
      "Epoch: 151 | Batch_idx: 300 |  Loss_1: (0.1357) | Acc_1: (95.01%) (36607/38528)\n",
      "Epoch: 151 | Batch_idx: 310 |  Loss_1: (0.1363) | Acc_1: (94.97%) (37805/39808)\n",
      "Epoch: 151 | Batch_idx: 320 |  Loss_1: (0.1363) | Acc_1: (94.97%) (39021/41088)\n",
      "Epoch: 151 | Batch_idx: 330 |  Loss_1: (0.1363) | Acc_1: (94.97%) (40237/42368)\n",
      "Epoch: 151 | Batch_idx: 340 |  Loss_1: (0.1359) | Acc_1: (94.98%) (41457/43648)\n",
      "Epoch: 151 | Batch_idx: 350 |  Loss_1: (0.1363) | Acc_1: (94.96%) (42663/44928)\n",
      "Epoch: 151 | Batch_idx: 360 |  Loss_1: (0.1360) | Acc_1: (94.96%) (43881/46208)\n",
      "Epoch: 151 | Batch_idx: 370 |  Loss_1: (0.1364) | Acc_1: (94.95%) (45089/47488)\n",
      "Epoch: 151 | Batch_idx: 380 |  Loss_1: (0.1368) | Acc_1: (94.94%) (46299/48768)\n",
      "Epoch: 151 | Batch_idx: 390 |  Loss_1: (0.1367) | Acc_1: (94.93%) (47466/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3570) | Acc: (91.55%) (9155/10000)\n",
      "Epoch: 152 | Batch_idx: 0 |  Loss_1: (0.2009) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 152 | Batch_idx: 10 |  Loss_1: (0.1627) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 152 | Batch_idx: 20 |  Loss_1: (0.1438) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 152 | Batch_idx: 30 |  Loss_1: (0.1432) | Acc_1: (94.86%) (3764/3968)\n",
      "Epoch: 152 | Batch_idx: 40 |  Loss_1: (0.1402) | Acc_1: (94.87%) (4979/5248)\n",
      "Epoch: 152 | Batch_idx: 50 |  Loss_1: (0.1386) | Acc_1: (94.90%) (6195/6528)\n",
      "Epoch: 152 | Batch_idx: 60 |  Loss_1: (0.1367) | Acc_1: (94.98%) (7416/7808)\n",
      "Epoch: 152 | Batch_idx: 70 |  Loss_1: (0.1412) | Acc_1: (94.77%) (8613/9088)\n",
      "Epoch: 152 | Batch_idx: 80 |  Loss_1: (0.1416) | Acc_1: (94.70%) (9819/10368)\n",
      "Epoch: 152 | Batch_idx: 90 |  Loss_1: (0.1389) | Acc_1: (94.78%) (11040/11648)\n",
      "Epoch: 152 | Batch_idx: 100 |  Loss_1: (0.1383) | Acc_1: (94.76%) (12251/12928)\n",
      "Epoch: 152 | Batch_idx: 110 |  Loss_1: (0.1400) | Acc_1: (94.67%) (13451/14208)\n",
      "Epoch: 152 | Batch_idx: 120 |  Loss_1: (0.1379) | Acc_1: (94.76%) (14677/15488)\n",
      "Epoch: 152 | Batch_idx: 130 |  Loss_1: (0.1388) | Acc_1: (94.70%) (15880/16768)\n",
      "Epoch: 152 | Batch_idx: 140 |  Loss_1: (0.1383) | Acc_1: (94.73%) (17097/18048)\n",
      "Epoch: 152 | Batch_idx: 150 |  Loss_1: (0.1387) | Acc_1: (94.71%) (18306/19328)\n",
      "Epoch: 152 | Batch_idx: 160 |  Loss_1: (0.1390) | Acc_1: (94.71%) (19518/20608)\n",
      "Epoch: 152 | Batch_idx: 170 |  Loss_1: (0.1378) | Acc_1: (94.76%) (20741/21888)\n",
      "Epoch: 152 | Batch_idx: 180 |  Loss_1: (0.1385) | Acc_1: (94.74%) (21949/23168)\n",
      "Epoch: 152 | Batch_idx: 190 |  Loss_1: (0.1371) | Acc_1: (94.79%) (23174/24448)\n",
      "Epoch: 152 | Batch_idx: 200 |  Loss_1: (0.1373) | Acc_1: (94.79%) (24387/25728)\n",
      "Epoch: 152 | Batch_idx: 210 |  Loss_1: (0.1361) | Acc_1: (94.86%) (25619/27008)\n",
      "Epoch: 152 | Batch_idx: 220 |  Loss_1: (0.1383) | Acc_1: (94.79%) (26813/28288)\n",
      "Epoch: 152 | Batch_idx: 230 |  Loss_1: (0.1386) | Acc_1: (94.78%) (28025/29568)\n",
      "Epoch: 152 | Batch_idx: 240 |  Loss_1: (0.1372) | Acc_1: (94.83%) (29254/30848)\n",
      "Epoch: 152 | Batch_idx: 250 |  Loss_1: (0.1376) | Acc_1: (94.82%) (30464/32128)\n",
      "Epoch: 152 | Batch_idx: 260 |  Loss_1: (0.1379) | Acc_1: (94.82%) (31678/33408)\n",
      "Epoch: 152 | Batch_idx: 270 |  Loss_1: (0.1387) | Acc_1: (94.80%) (32883/34688)\n",
      "Epoch: 152 | Batch_idx: 280 |  Loss_1: (0.1389) | Acc_1: (94.79%) (34095/35968)\n",
      "Epoch: 152 | Batch_idx: 290 |  Loss_1: (0.1390) | Acc_1: (94.79%) (35307/37248)\n",
      "Epoch: 152 | Batch_idx: 300 |  Loss_1: (0.1397) | Acc_1: (94.78%) (36518/38528)\n",
      "Epoch: 152 | Batch_idx: 310 |  Loss_1: (0.1405) | Acc_1: (94.76%) (37722/39808)\n",
      "Epoch: 152 | Batch_idx: 320 |  Loss_1: (0.1409) | Acc_1: (94.74%) (38927/41088)\n",
      "Epoch: 152 | Batch_idx: 330 |  Loss_1: (0.1411) | Acc_1: (94.74%) (40140/42368)\n",
      "Epoch: 152 | Batch_idx: 340 |  Loss_1: (0.1410) | Acc_1: (94.75%) (41355/43648)\n",
      "Epoch: 152 | Batch_idx: 350 |  Loss_1: (0.1408) | Acc_1: (94.75%) (42568/44928)\n",
      "Epoch: 152 | Batch_idx: 360 |  Loss_1: (0.1405) | Acc_1: (94.75%) (43782/46208)\n",
      "Epoch: 152 | Batch_idx: 370 |  Loss_1: (0.1406) | Acc_1: (94.75%) (44996/47488)\n",
      "Epoch: 152 | Batch_idx: 380 |  Loss_1: (0.1403) | Acc_1: (94.77%) (46216/48768)\n",
      "Epoch: 152 | Batch_idx: 390 |  Loss_1: (0.1409) | Acc_1: (94.75%) (47376/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3259) | Acc: (92.07%) (9207/10000)\n",
      "Epoch: 153 | Batch_idx: 0 |  Loss_1: (0.1569) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 153 | Batch_idx: 10 |  Loss_1: (0.1363) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 153 | Batch_idx: 20 |  Loss_1: (0.1291) | Acc_1: (95.31%) (2562/2688)\n",
      "Epoch: 153 | Batch_idx: 30 |  Loss_1: (0.1343) | Acc_1: (95.11%) (3774/3968)\n",
      "Epoch: 153 | Batch_idx: 40 |  Loss_1: (0.1418) | Acc_1: (94.86%) (4978/5248)\n",
      "Epoch: 153 | Batch_idx: 50 |  Loss_1: (0.1430) | Acc_1: (94.76%) (6186/6528)\n",
      "Epoch: 153 | Batch_idx: 60 |  Loss_1: (0.1402) | Acc_1: (94.81%) (7403/7808)\n",
      "Epoch: 153 | Batch_idx: 70 |  Loss_1: (0.1361) | Acc_1: (94.98%) (8632/9088)\n",
      "Epoch: 153 | Batch_idx: 80 |  Loss_1: (0.1321) | Acc_1: (95.13%) (9863/10368)\n",
      "Epoch: 153 | Batch_idx: 90 |  Loss_1: (0.1326) | Acc_1: (95.14%) (11082/11648)\n",
      "Epoch: 153 | Batch_idx: 100 |  Loss_1: (0.1336) | Acc_1: (95.10%) (12295/12928)\n",
      "Epoch: 153 | Batch_idx: 110 |  Loss_1: (0.1333) | Acc_1: (95.14%) (13517/14208)\n",
      "Epoch: 153 | Batch_idx: 120 |  Loss_1: (0.1333) | Acc_1: (95.14%) (14735/15488)\n",
      "Epoch: 153 | Batch_idx: 130 |  Loss_1: (0.1354) | Acc_1: (95.07%) (15942/16768)\n",
      "Epoch: 153 | Batch_idx: 140 |  Loss_1: (0.1355) | Acc_1: (95.08%) (17160/18048)\n",
      "Epoch: 153 | Batch_idx: 150 |  Loss_1: (0.1370) | Acc_1: (95.01%) (18364/19328)\n",
      "Epoch: 153 | Batch_idx: 160 |  Loss_1: (0.1368) | Acc_1: (95.01%) (19579/20608)\n",
      "Epoch: 153 | Batch_idx: 170 |  Loss_1: (0.1356) | Acc_1: (95.04%) (20803/21888)\n",
      "Epoch: 153 | Batch_idx: 180 |  Loss_1: (0.1371) | Acc_1: (94.98%) (22006/23168)\n",
      "Epoch: 153 | Batch_idx: 190 |  Loss_1: (0.1381) | Acc_1: (94.94%) (23210/24448)\n",
      "Epoch: 153 | Batch_idx: 200 |  Loss_1: (0.1379) | Acc_1: (94.92%) (24421/25728)\n",
      "Epoch: 153 | Batch_idx: 210 |  Loss_1: (0.1377) | Acc_1: (94.93%) (25638/27008)\n",
      "Epoch: 153 | Batch_idx: 220 |  Loss_1: (0.1387) | Acc_1: (94.90%) (26845/28288)\n",
      "Epoch: 153 | Batch_idx: 230 |  Loss_1: (0.1377) | Acc_1: (94.94%) (28073/29568)\n",
      "Epoch: 153 | Batch_idx: 240 |  Loss_1: (0.1375) | Acc_1: (94.95%) (29290/30848)\n",
      "Epoch: 153 | Batch_idx: 250 |  Loss_1: (0.1382) | Acc_1: (94.94%) (30502/32128)\n",
      "Epoch: 153 | Batch_idx: 260 |  Loss_1: (0.1374) | Acc_1: (94.94%) (31717/33408)\n",
      "Epoch: 153 | Batch_idx: 270 |  Loss_1: (0.1370) | Acc_1: (94.95%) (32937/34688)\n",
      "Epoch: 153 | Batch_idx: 280 |  Loss_1: (0.1370) | Acc_1: (94.95%) (34153/35968)\n",
      "Epoch: 153 | Batch_idx: 290 |  Loss_1: (0.1365) | Acc_1: (94.97%) (35373/37248)\n",
      "Epoch: 153 | Batch_idx: 300 |  Loss_1: (0.1370) | Acc_1: (94.95%) (36583/38528)\n",
      "Epoch: 153 | Batch_idx: 310 |  Loss_1: (0.1375) | Acc_1: (94.91%) (37782/39808)\n",
      "Epoch: 153 | Batch_idx: 320 |  Loss_1: (0.1371) | Acc_1: (94.92%) (38999/41088)\n",
      "Epoch: 153 | Batch_idx: 330 |  Loss_1: (0.1374) | Acc_1: (94.91%) (40211/42368)\n",
      "Epoch: 153 | Batch_idx: 340 |  Loss_1: (0.1372) | Acc_1: (94.90%) (41424/43648)\n",
      "Epoch: 153 | Batch_idx: 350 |  Loss_1: (0.1376) | Acc_1: (94.90%) (42635/44928)\n",
      "Epoch: 153 | Batch_idx: 360 |  Loss_1: (0.1376) | Acc_1: (94.90%) (43850/46208)\n",
      "Epoch: 153 | Batch_idx: 370 |  Loss_1: (0.1370) | Acc_1: (94.92%) (45077/47488)\n",
      "Epoch: 153 | Batch_idx: 380 |  Loss_1: (0.1371) | Acc_1: (94.93%) (46294/48768)\n",
      "Epoch: 153 | Batch_idx: 390 |  Loss_1: (0.1377) | Acc_1: (94.93%) (47465/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3441) | Acc: (91.91%) (9191/10000)\n",
      "Epoch: 154 | Batch_idx: 0 |  Loss_1: (0.2187) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 154 | Batch_idx: 10 |  Loss_1: (0.1531) | Acc_1: (94.53%) (1331/1408)\n",
      "Epoch: 154 | Batch_idx: 20 |  Loss_1: (0.1600) | Acc_1: (94.49%) (2540/2688)\n",
      "Epoch: 154 | Batch_idx: 30 |  Loss_1: (0.1487) | Acc_1: (94.68%) (3757/3968)\n",
      "Epoch: 154 | Batch_idx: 40 |  Loss_1: (0.1456) | Acc_1: (94.74%) (4972/5248)\n",
      "Epoch: 154 | Batch_idx: 50 |  Loss_1: (0.1434) | Acc_1: (94.81%) (6189/6528)\n",
      "Epoch: 154 | Batch_idx: 60 |  Loss_1: (0.1426) | Acc_1: (94.85%) (7406/7808)\n",
      "Epoch: 154 | Batch_idx: 70 |  Loss_1: (0.1415) | Acc_1: (94.84%) (8619/9088)\n",
      "Epoch: 154 | Batch_idx: 80 |  Loss_1: (0.1377) | Acc_1: (94.95%) (9844/10368)\n",
      "Epoch: 154 | Batch_idx: 90 |  Loss_1: (0.1387) | Acc_1: (94.90%) (11054/11648)\n",
      "Epoch: 154 | Batch_idx: 100 |  Loss_1: (0.1363) | Acc_1: (94.94%) (12274/12928)\n",
      "Epoch: 154 | Batch_idx: 110 |  Loss_1: (0.1363) | Acc_1: (94.93%) (13488/14208)\n",
      "Epoch: 154 | Batch_idx: 120 |  Loss_1: (0.1365) | Acc_1: (94.90%) (14698/15488)\n",
      "Epoch: 154 | Batch_idx: 130 |  Loss_1: (0.1353) | Acc_1: (94.95%) (15921/16768)\n",
      "Epoch: 154 | Batch_idx: 140 |  Loss_1: (0.1346) | Acc_1: (94.97%) (17140/18048)\n",
      "Epoch: 154 | Batch_idx: 150 |  Loss_1: (0.1364) | Acc_1: (94.89%) (18341/19328)\n",
      "Epoch: 154 | Batch_idx: 160 |  Loss_1: (0.1363) | Acc_1: (94.91%) (19559/20608)\n",
      "Epoch: 154 | Batch_idx: 170 |  Loss_1: (0.1357) | Acc_1: (94.92%) (20777/21888)\n",
      "Epoch: 154 | Batch_idx: 180 |  Loss_1: (0.1352) | Acc_1: (94.94%) (21996/23168)\n",
      "Epoch: 154 | Batch_idx: 190 |  Loss_1: (0.1353) | Acc_1: (94.94%) (23211/24448)\n",
      "Epoch: 154 | Batch_idx: 200 |  Loss_1: (0.1351) | Acc_1: (94.94%) (24427/25728)\n",
      "Epoch: 154 | Batch_idx: 210 |  Loss_1: (0.1350) | Acc_1: (94.94%) (25641/27008)\n",
      "Epoch: 154 | Batch_idx: 220 |  Loss_1: (0.1343) | Acc_1: (94.95%) (26860/28288)\n",
      "Epoch: 154 | Batch_idx: 230 |  Loss_1: (0.1338) | Acc_1: (94.97%) (28080/29568)\n",
      "Epoch: 154 | Batch_idx: 240 |  Loss_1: (0.1339) | Acc_1: (94.98%) (29298/30848)\n",
      "Epoch: 154 | Batch_idx: 250 |  Loss_1: (0.1345) | Acc_1: (94.95%) (30507/32128)\n",
      "Epoch: 154 | Batch_idx: 260 |  Loss_1: (0.1352) | Acc_1: (94.94%) (31716/33408)\n",
      "Epoch: 154 | Batch_idx: 270 |  Loss_1: (0.1352) | Acc_1: (94.95%) (32935/34688)\n",
      "Epoch: 154 | Batch_idx: 280 |  Loss_1: (0.1352) | Acc_1: (94.95%) (34153/35968)\n",
      "Epoch: 154 | Batch_idx: 290 |  Loss_1: (0.1355) | Acc_1: (94.96%) (35370/37248)\n",
      "Epoch: 154 | Batch_idx: 300 |  Loss_1: (0.1355) | Acc_1: (94.95%) (36583/38528)\n",
      "Epoch: 154 | Batch_idx: 310 |  Loss_1: (0.1357) | Acc_1: (94.94%) (37794/39808)\n",
      "Epoch: 154 | Batch_idx: 320 |  Loss_1: (0.1352) | Acc_1: (94.96%) (39018/41088)\n",
      "Epoch: 154 | Batch_idx: 330 |  Loss_1: (0.1350) | Acc_1: (94.97%) (40236/42368)\n",
      "Epoch: 154 | Batch_idx: 340 |  Loss_1: (0.1343) | Acc_1: (94.99%) (41462/43648)\n",
      "Epoch: 154 | Batch_idx: 350 |  Loss_1: (0.1346) | Acc_1: (94.99%) (42678/44928)\n",
      "Epoch: 154 | Batch_idx: 360 |  Loss_1: (0.1353) | Acc_1: (94.97%) (43882/46208)\n",
      "Epoch: 154 | Batch_idx: 370 |  Loss_1: (0.1353) | Acc_1: (94.97%) (45098/47488)\n",
      "Epoch: 154 | Batch_idx: 380 |  Loss_1: (0.1348) | Acc_1: (94.99%) (46325/48768)\n",
      "Epoch: 154 | Batch_idx: 390 |  Loss_1: (0.1345) | Acc_1: (95.01%) (47504/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3631) | Acc: (91.69%) (9169/10000)\n",
      "Epoch: 155 | Batch_idx: 0 |  Loss_1: (0.1094) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 155 | Batch_idx: 10 |  Loss_1: (0.1554) | Acc_1: (94.25%) (1327/1408)\n",
      "Epoch: 155 | Batch_idx: 20 |  Loss_1: (0.1495) | Acc_1: (94.38%) (2537/2688)\n",
      "Epoch: 155 | Batch_idx: 30 |  Loss_1: (0.1455) | Acc_1: (94.51%) (3750/3968)\n",
      "Epoch: 155 | Batch_idx: 40 |  Loss_1: (0.1441) | Acc_1: (94.61%) (4965/5248)\n",
      "Epoch: 155 | Batch_idx: 50 |  Loss_1: (0.1437) | Acc_1: (94.65%) (6179/6528)\n",
      "Epoch: 155 | Batch_idx: 60 |  Loss_1: (0.1421) | Acc_1: (94.71%) (7395/7808)\n",
      "Epoch: 155 | Batch_idx: 70 |  Loss_1: (0.1435) | Acc_1: (94.64%) (8601/9088)\n",
      "Epoch: 155 | Batch_idx: 80 |  Loss_1: (0.1435) | Acc_1: (94.65%) (9813/10368)\n",
      "Epoch: 155 | Batch_idx: 90 |  Loss_1: (0.1432) | Acc_1: (94.68%) (11028/11648)\n",
      "Epoch: 155 | Batch_idx: 100 |  Loss_1: (0.1385) | Acc_1: (94.84%) (12261/12928)\n",
      "Epoch: 155 | Batch_idx: 110 |  Loss_1: (0.1382) | Acc_1: (94.87%) (13479/14208)\n",
      "Epoch: 155 | Batch_idx: 120 |  Loss_1: (0.1382) | Acc_1: (94.83%) (14688/15488)\n",
      "Epoch: 155 | Batch_idx: 130 |  Loss_1: (0.1380) | Acc_1: (94.83%) (15901/16768)\n",
      "Epoch: 155 | Batch_idx: 140 |  Loss_1: (0.1386) | Acc_1: (94.81%) (17112/18048)\n",
      "Epoch: 155 | Batch_idx: 150 |  Loss_1: (0.1370) | Acc_1: (94.88%) (18339/19328)\n",
      "Epoch: 155 | Batch_idx: 160 |  Loss_1: (0.1384) | Acc_1: (94.87%) (19550/20608)\n",
      "Epoch: 155 | Batch_idx: 170 |  Loss_1: (0.1376) | Acc_1: (94.89%) (20769/21888)\n",
      "Epoch: 155 | Batch_idx: 180 |  Loss_1: (0.1364) | Acc_1: (94.92%) (21992/23168)\n",
      "Epoch: 155 | Batch_idx: 190 |  Loss_1: (0.1367) | Acc_1: (94.90%) (23201/24448)\n",
      "Epoch: 155 | Batch_idx: 200 |  Loss_1: (0.1363) | Acc_1: (94.92%) (24421/25728)\n",
      "Epoch: 155 | Batch_idx: 210 |  Loss_1: (0.1360) | Acc_1: (94.92%) (25637/27008)\n",
      "Epoch: 155 | Batch_idx: 220 |  Loss_1: (0.1365) | Acc_1: (94.92%) (26850/28288)\n",
      "Epoch: 155 | Batch_idx: 230 |  Loss_1: (0.1368) | Acc_1: (94.91%) (28062/29568)\n",
      "Epoch: 155 | Batch_idx: 240 |  Loss_1: (0.1361) | Acc_1: (94.93%) (29285/30848)\n",
      "Epoch: 155 | Batch_idx: 250 |  Loss_1: (0.1371) | Acc_1: (94.89%) (30487/32128)\n",
      "Epoch: 155 | Batch_idx: 260 |  Loss_1: (0.1372) | Acc_1: (94.89%) (31702/33408)\n",
      "Epoch: 155 | Batch_idx: 270 |  Loss_1: (0.1368) | Acc_1: (94.93%) (32929/34688)\n",
      "Epoch: 155 | Batch_idx: 280 |  Loss_1: (0.1364) | Acc_1: (94.92%) (34142/35968)\n",
      "Epoch: 155 | Batch_idx: 290 |  Loss_1: (0.1364) | Acc_1: (94.91%) (35353/37248)\n",
      "Epoch: 155 | Batch_idx: 300 |  Loss_1: (0.1368) | Acc_1: (94.90%) (36564/38528)\n",
      "Epoch: 155 | Batch_idx: 310 |  Loss_1: (0.1367) | Acc_1: (94.90%) (37779/39808)\n",
      "Epoch: 155 | Batch_idx: 320 |  Loss_1: (0.1364) | Acc_1: (94.92%) (39001/41088)\n",
      "Epoch: 155 | Batch_idx: 330 |  Loss_1: (0.1361) | Acc_1: (94.92%) (40215/42368)\n",
      "Epoch: 155 | Batch_idx: 340 |  Loss_1: (0.1357) | Acc_1: (94.94%) (41439/43648)\n",
      "Epoch: 155 | Batch_idx: 350 |  Loss_1: (0.1354) | Acc_1: (94.94%) (42656/44928)\n",
      "Epoch: 155 | Batch_idx: 360 |  Loss_1: (0.1352) | Acc_1: (94.96%) (43877/46208)\n",
      "Epoch: 155 | Batch_idx: 370 |  Loss_1: (0.1356) | Acc_1: (94.95%) (45092/47488)\n",
      "Epoch: 155 | Batch_idx: 380 |  Loss_1: (0.1360) | Acc_1: (94.93%) (46297/48768)\n",
      "Epoch: 155 | Batch_idx: 390 |  Loss_1: (0.1363) | Acc_1: (94.91%) (47453/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3351) | Acc: (91.97%) (9197/10000)\n",
      "Epoch: 156 | Batch_idx: 0 |  Loss_1: (0.1445) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 156 | Batch_idx: 10 |  Loss_1: (0.1316) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 156 | Batch_idx: 20 |  Loss_1: (0.1214) | Acc_1: (95.50%) (2567/2688)\n",
      "Epoch: 156 | Batch_idx: 30 |  Loss_1: (0.1258) | Acc_1: (95.36%) (3784/3968)\n",
      "Epoch: 156 | Batch_idx: 40 |  Loss_1: (0.1328) | Acc_1: (95.22%) (4997/5248)\n",
      "Epoch: 156 | Batch_idx: 50 |  Loss_1: (0.1291) | Acc_1: (95.30%) (6221/6528)\n",
      "Epoch: 156 | Batch_idx: 60 |  Loss_1: (0.1317) | Acc_1: (95.08%) (7424/7808)\n",
      "Epoch: 156 | Batch_idx: 70 |  Loss_1: (0.1344) | Acc_1: (94.99%) (8633/9088)\n",
      "Epoch: 156 | Batch_idx: 80 |  Loss_1: (0.1339) | Acc_1: (95.00%) (9850/10368)\n",
      "Epoch: 156 | Batch_idx: 90 |  Loss_1: (0.1333) | Acc_1: (94.99%) (11065/11648)\n",
      "Epoch: 156 | Batch_idx: 100 |  Loss_1: (0.1348) | Acc_1: (94.89%) (12267/12928)\n",
      "Epoch: 156 | Batch_idx: 110 |  Loss_1: (0.1351) | Acc_1: (94.86%) (13477/14208)\n",
      "Epoch: 156 | Batch_idx: 120 |  Loss_1: (0.1353) | Acc_1: (94.88%) (14695/15488)\n",
      "Epoch: 156 | Batch_idx: 130 |  Loss_1: (0.1342) | Acc_1: (94.94%) (15919/16768)\n",
      "Epoch: 156 | Batch_idx: 140 |  Loss_1: (0.1338) | Acc_1: (95.00%) (17145/18048)\n",
      "Epoch: 156 | Batch_idx: 150 |  Loss_1: (0.1344) | Acc_1: (94.99%) (18359/19328)\n",
      "Epoch: 156 | Batch_idx: 160 |  Loss_1: (0.1335) | Acc_1: (94.99%) (19576/20608)\n",
      "Epoch: 156 | Batch_idx: 170 |  Loss_1: (0.1334) | Acc_1: (94.99%) (20791/21888)\n",
      "Epoch: 156 | Batch_idx: 180 |  Loss_1: (0.1318) | Acc_1: (95.03%) (22017/23168)\n",
      "Epoch: 156 | Batch_idx: 190 |  Loss_1: (0.1318) | Acc_1: (95.05%) (23237/24448)\n",
      "Epoch: 156 | Batch_idx: 200 |  Loss_1: (0.1314) | Acc_1: (95.07%) (24460/25728)\n",
      "Epoch: 156 | Batch_idx: 210 |  Loss_1: (0.1310) | Acc_1: (95.09%) (25683/27008)\n",
      "Epoch: 156 | Batch_idx: 220 |  Loss_1: (0.1324) | Acc_1: (95.05%) (26889/28288)\n",
      "Epoch: 156 | Batch_idx: 230 |  Loss_1: (0.1324) | Acc_1: (95.06%) (28108/29568)\n",
      "Epoch: 156 | Batch_idx: 240 |  Loss_1: (0.1327) | Acc_1: (95.05%) (29321/30848)\n",
      "Epoch: 156 | Batch_idx: 250 |  Loss_1: (0.1339) | Acc_1: (95.01%) (30525/32128)\n",
      "Epoch: 156 | Batch_idx: 260 |  Loss_1: (0.1331) | Acc_1: (95.04%) (31750/33408)\n",
      "Epoch: 156 | Batch_idx: 270 |  Loss_1: (0.1334) | Acc_1: (95.01%) (32956/34688)\n",
      "Epoch: 156 | Batch_idx: 280 |  Loss_1: (0.1328) | Acc_1: (95.02%) (34178/35968)\n",
      "Epoch: 156 | Batch_idx: 290 |  Loss_1: (0.1327) | Acc_1: (95.02%) (35394/37248)\n",
      "Epoch: 156 | Batch_idx: 300 |  Loss_1: (0.1327) | Acc_1: (95.02%) (36611/38528)\n",
      "Epoch: 156 | Batch_idx: 310 |  Loss_1: (0.1319) | Acc_1: (95.03%) (37831/39808)\n",
      "Epoch: 156 | Batch_idx: 320 |  Loss_1: (0.1323) | Acc_1: (95.01%) (39039/41088)\n",
      "Epoch: 156 | Batch_idx: 330 |  Loss_1: (0.1324) | Acc_1: (95.01%) (40253/42368)\n",
      "Epoch: 156 | Batch_idx: 340 |  Loss_1: (0.1322) | Acc_1: (95.02%) (41474/43648)\n",
      "Epoch: 156 | Batch_idx: 350 |  Loss_1: (0.1320) | Acc_1: (95.03%) (42695/44928)\n",
      "Epoch: 156 | Batch_idx: 360 |  Loss_1: (0.1313) | Acc_1: (95.05%) (43923/46208)\n",
      "Epoch: 156 | Batch_idx: 370 |  Loss_1: (0.1324) | Acc_1: (95.02%) (45125/47488)\n",
      "Epoch: 156 | Batch_idx: 380 |  Loss_1: (0.1320) | Acc_1: (95.05%) (46352/48768)\n",
      "Epoch: 156 | Batch_idx: 390 |  Loss_1: (0.1316) | Acc_1: (95.07%) (47535/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3406) | Acc: (92.24%) (9224/10000)\n",
      "Epoch: 157 | Batch_idx: 0 |  Loss_1: (0.1488) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 157 | Batch_idx: 10 |  Loss_1: (0.1146) | Acc_1: (95.81%) (1349/1408)\n",
      "Epoch: 157 | Batch_idx: 20 |  Loss_1: (0.1188) | Acc_1: (95.83%) (2576/2688)\n",
      "Epoch: 157 | Batch_idx: 30 |  Loss_1: (0.1146) | Acc_1: (95.92%) (3806/3968)\n",
      "Epoch: 157 | Batch_idx: 40 |  Loss_1: (0.1162) | Acc_1: (95.85%) (5030/5248)\n",
      "Epoch: 157 | Batch_idx: 50 |  Loss_1: (0.1200) | Acc_1: (95.66%) (6245/6528)\n",
      "Epoch: 157 | Batch_idx: 60 |  Loss_1: (0.1202) | Acc_1: (95.59%) (7464/7808)\n",
      "Epoch: 157 | Batch_idx: 70 |  Loss_1: (0.1234) | Acc_1: (95.47%) (8676/9088)\n",
      "Epoch: 157 | Batch_idx: 80 |  Loss_1: (0.1274) | Acc_1: (95.33%) (9884/10368)\n",
      "Epoch: 157 | Batch_idx: 90 |  Loss_1: (0.1253) | Acc_1: (95.43%) (11116/11648)\n",
      "Epoch: 157 | Batch_idx: 100 |  Loss_1: (0.1264) | Acc_1: (95.38%) (12331/12928)\n",
      "Epoch: 157 | Batch_idx: 110 |  Loss_1: (0.1268) | Acc_1: (95.33%) (13545/14208)\n",
      "Epoch: 157 | Batch_idx: 120 |  Loss_1: (0.1290) | Acc_1: (95.26%) (14754/15488)\n",
      "Epoch: 157 | Batch_idx: 130 |  Loss_1: (0.1281) | Acc_1: (95.28%) (15977/16768)\n",
      "Epoch: 157 | Batch_idx: 140 |  Loss_1: (0.1272) | Acc_1: (95.31%) (17201/18048)\n",
      "Epoch: 157 | Batch_idx: 150 |  Loss_1: (0.1273) | Acc_1: (95.28%) (18416/19328)\n",
      "Epoch: 157 | Batch_idx: 160 |  Loss_1: (0.1270) | Acc_1: (95.28%) (19635/20608)\n",
      "Epoch: 157 | Batch_idx: 170 |  Loss_1: (0.1280) | Acc_1: (95.21%) (20839/21888)\n",
      "Epoch: 157 | Batch_idx: 180 |  Loss_1: (0.1279) | Acc_1: (95.21%) (22058/23168)\n",
      "Epoch: 157 | Batch_idx: 190 |  Loss_1: (0.1284) | Acc_1: (95.18%) (23269/24448)\n",
      "Epoch: 157 | Batch_idx: 200 |  Loss_1: (0.1289) | Acc_1: (95.17%) (24485/25728)\n",
      "Epoch: 157 | Batch_idx: 210 |  Loss_1: (0.1305) | Acc_1: (95.12%) (25690/27008)\n",
      "Epoch: 157 | Batch_idx: 220 |  Loss_1: (0.1303) | Acc_1: (95.13%) (26909/28288)\n",
      "Epoch: 157 | Batch_idx: 230 |  Loss_1: (0.1303) | Acc_1: (95.12%) (28126/29568)\n",
      "Epoch: 157 | Batch_idx: 240 |  Loss_1: (0.1303) | Acc_1: (95.12%) (29342/30848)\n",
      "Epoch: 157 | Batch_idx: 250 |  Loss_1: (0.1299) | Acc_1: (95.12%) (30561/32128)\n",
      "Epoch: 157 | Batch_idx: 260 |  Loss_1: (0.1300) | Acc_1: (95.13%) (31780/33408)\n",
      "Epoch: 157 | Batch_idx: 270 |  Loss_1: (0.1289) | Acc_1: (95.17%) (33014/34688)\n",
      "Epoch: 157 | Batch_idx: 280 |  Loss_1: (0.1280) | Acc_1: (95.22%) (34247/35968)\n",
      "Epoch: 157 | Batch_idx: 290 |  Loss_1: (0.1284) | Acc_1: (95.20%) (35460/37248)\n",
      "Epoch: 157 | Batch_idx: 300 |  Loss_1: (0.1290) | Acc_1: (95.16%) (36664/38528)\n",
      "Epoch: 157 | Batch_idx: 310 |  Loss_1: (0.1294) | Acc_1: (95.15%) (37878/39808)\n",
      "Epoch: 157 | Batch_idx: 320 |  Loss_1: (0.1295) | Acc_1: (95.14%) (39093/41088)\n",
      "Epoch: 157 | Batch_idx: 330 |  Loss_1: (0.1291) | Acc_1: (95.16%) (40317/42368)\n",
      "Epoch: 157 | Batch_idx: 340 |  Loss_1: (0.1287) | Acc_1: (95.17%) (41540/43648)\n",
      "Epoch: 157 | Batch_idx: 350 |  Loss_1: (0.1291) | Acc_1: (95.16%) (42755/44928)\n",
      "Epoch: 157 | Batch_idx: 360 |  Loss_1: (0.1290) | Acc_1: (95.16%) (43973/46208)\n",
      "Epoch: 157 | Batch_idx: 370 |  Loss_1: (0.1294) | Acc_1: (95.15%) (45184/47488)\n",
      "Epoch: 157 | Batch_idx: 380 |  Loss_1: (0.1289) | Acc_1: (95.17%) (46414/48768)\n",
      "Epoch: 157 | Batch_idx: 390 |  Loss_1: (0.1297) | Acc_1: (95.15%) (47576/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3606) | Acc: (91.80%) (9180/10000)\n",
      "Epoch: 158 | Batch_idx: 0 |  Loss_1: (0.1382) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 158 | Batch_idx: 10 |  Loss_1: (0.1409) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 158 | Batch_idx: 20 |  Loss_1: (0.1316) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 158 | Batch_idx: 30 |  Loss_1: (0.1261) | Acc_1: (95.41%) (3786/3968)\n",
      "Epoch: 158 | Batch_idx: 40 |  Loss_1: (0.1255) | Acc_1: (95.35%) (5004/5248)\n",
      "Epoch: 158 | Batch_idx: 50 |  Loss_1: (0.1267) | Acc_1: (95.30%) (6221/6528)\n",
      "Epoch: 158 | Batch_idx: 60 |  Loss_1: (0.1303) | Acc_1: (95.16%) (7430/7808)\n",
      "Epoch: 158 | Batch_idx: 70 |  Loss_1: (0.1330) | Acc_1: (95.08%) (8641/9088)\n",
      "Epoch: 158 | Batch_idx: 80 |  Loss_1: (0.1315) | Acc_1: (95.18%) (9868/10368)\n",
      "Epoch: 158 | Batch_idx: 90 |  Loss_1: (0.1307) | Acc_1: (95.18%) (11087/11648)\n",
      "Epoch: 158 | Batch_idx: 100 |  Loss_1: (0.1305) | Acc_1: (95.20%) (12307/12928)\n",
      "Epoch: 158 | Batch_idx: 110 |  Loss_1: (0.1308) | Acc_1: (95.15%) (13519/14208)\n",
      "Epoch: 158 | Batch_idx: 120 |  Loss_1: (0.1313) | Acc_1: (95.14%) (14736/15488)\n",
      "Epoch: 158 | Batch_idx: 130 |  Loss_1: (0.1317) | Acc_1: (95.12%) (15950/16768)\n",
      "Epoch: 158 | Batch_idx: 140 |  Loss_1: (0.1311) | Acc_1: (95.12%) (17167/18048)\n",
      "Epoch: 158 | Batch_idx: 150 |  Loss_1: (0.1299) | Acc_1: (95.17%) (18394/19328)\n",
      "Epoch: 158 | Batch_idx: 160 |  Loss_1: (0.1297) | Acc_1: (95.18%) (19615/20608)\n",
      "Epoch: 158 | Batch_idx: 170 |  Loss_1: (0.1297) | Acc_1: (95.19%) (20836/21888)\n",
      "Epoch: 158 | Batch_idx: 180 |  Loss_1: (0.1297) | Acc_1: (95.19%) (22053/23168)\n",
      "Epoch: 158 | Batch_idx: 190 |  Loss_1: (0.1302) | Acc_1: (95.19%) (23271/24448)\n",
      "Epoch: 158 | Batch_idx: 200 |  Loss_1: (0.1305) | Acc_1: (95.17%) (24486/25728)\n",
      "Epoch: 158 | Batch_idx: 210 |  Loss_1: (0.1302) | Acc_1: (95.19%) (25709/27008)\n",
      "Epoch: 158 | Batch_idx: 220 |  Loss_1: (0.1300) | Acc_1: (95.19%) (26926/28288)\n",
      "Epoch: 158 | Batch_idx: 230 |  Loss_1: (0.1287) | Acc_1: (95.22%) (28156/29568)\n",
      "Epoch: 158 | Batch_idx: 240 |  Loss_1: (0.1279) | Acc_1: (95.25%) (29384/30848)\n",
      "Epoch: 158 | Batch_idx: 250 |  Loss_1: (0.1278) | Acc_1: (95.27%) (30607/32128)\n",
      "Epoch: 158 | Batch_idx: 260 |  Loss_1: (0.1277) | Acc_1: (95.27%) (31827/33408)\n",
      "Epoch: 158 | Batch_idx: 270 |  Loss_1: (0.1273) | Acc_1: (95.30%) (33056/34688)\n",
      "Epoch: 158 | Batch_idx: 280 |  Loss_1: (0.1280) | Acc_1: (95.27%) (34268/35968)\n",
      "Epoch: 158 | Batch_idx: 290 |  Loss_1: (0.1276) | Acc_1: (95.27%) (35487/37248)\n",
      "Epoch: 158 | Batch_idx: 300 |  Loss_1: (0.1266) | Acc_1: (95.30%) (36719/38528)\n",
      "Epoch: 158 | Batch_idx: 310 |  Loss_1: (0.1270) | Acc_1: (95.28%) (37928/39808)\n",
      "Epoch: 158 | Batch_idx: 320 |  Loss_1: (0.1277) | Acc_1: (95.25%) (39137/41088)\n",
      "Epoch: 158 | Batch_idx: 330 |  Loss_1: (0.1280) | Acc_1: (95.24%) (40352/42368)\n",
      "Epoch: 158 | Batch_idx: 340 |  Loss_1: (0.1286) | Acc_1: (95.23%) (41564/43648)\n",
      "Epoch: 158 | Batch_idx: 350 |  Loss_1: (0.1284) | Acc_1: (95.24%) (42790/44928)\n",
      "Epoch: 158 | Batch_idx: 360 |  Loss_1: (0.1283) | Acc_1: (95.24%) (44008/46208)\n",
      "Epoch: 158 | Batch_idx: 370 |  Loss_1: (0.1279) | Acc_1: (95.25%) (45233/47488)\n",
      "Epoch: 158 | Batch_idx: 380 |  Loss_1: (0.1287) | Acc_1: (95.22%) (46436/48768)\n",
      "Epoch: 158 | Batch_idx: 390 |  Loss_1: (0.1286) | Acc_1: (95.22%) (47612/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3499) | Acc: (92.12%) (9212/10000)\n",
      "Epoch: 159 | Batch_idx: 0 |  Loss_1: (0.1416) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 159 | Batch_idx: 10 |  Loss_1: (0.1097) | Acc_1: (95.95%) (1351/1408)\n",
      "Epoch: 159 | Batch_idx: 20 |  Loss_1: (0.1140) | Acc_1: (95.94%) (2579/2688)\n",
      "Epoch: 159 | Batch_idx: 30 |  Loss_1: (0.1247) | Acc_1: (95.51%) (3790/3968)\n",
      "Epoch: 159 | Batch_idx: 40 |  Loss_1: (0.1269) | Acc_1: (95.27%) (5000/5248)\n",
      "Epoch: 159 | Batch_idx: 50 |  Loss_1: (0.1336) | Acc_1: (95.02%) (6203/6528)\n",
      "Epoch: 159 | Batch_idx: 60 |  Loss_1: (0.1322) | Acc_1: (95.11%) (7426/7808)\n",
      "Epoch: 159 | Batch_idx: 70 |  Loss_1: (0.1322) | Acc_1: (95.07%) (8640/9088)\n",
      "Epoch: 159 | Batch_idx: 80 |  Loss_1: (0.1280) | Acc_1: (95.16%) (9866/10368)\n",
      "Epoch: 159 | Batch_idx: 90 |  Loss_1: (0.1294) | Acc_1: (95.12%) (11080/11648)\n",
      "Epoch: 159 | Batch_idx: 100 |  Loss_1: (0.1294) | Acc_1: (95.10%) (12295/12928)\n",
      "Epoch: 159 | Batch_idx: 110 |  Loss_1: (0.1299) | Acc_1: (95.09%) (13510/14208)\n",
      "Epoch: 159 | Batch_idx: 120 |  Loss_1: (0.1305) | Acc_1: (95.03%) (14718/15488)\n",
      "Epoch: 159 | Batch_idx: 130 |  Loss_1: (0.1300) | Acc_1: (95.05%) (15938/16768)\n",
      "Epoch: 159 | Batch_idx: 140 |  Loss_1: (0.1314) | Acc_1: (94.99%) (17144/18048)\n",
      "Epoch: 159 | Batch_idx: 150 |  Loss_1: (0.1316) | Acc_1: (95.00%) (18361/19328)\n",
      "Epoch: 159 | Batch_idx: 160 |  Loss_1: (0.1306) | Acc_1: (95.07%) (19591/20608)\n",
      "Epoch: 159 | Batch_idx: 170 |  Loss_1: (0.1313) | Acc_1: (95.05%) (20804/21888)\n",
      "Epoch: 159 | Batch_idx: 180 |  Loss_1: (0.1323) | Acc_1: (95.03%) (22016/23168)\n",
      "Epoch: 159 | Batch_idx: 190 |  Loss_1: (0.1313) | Acc_1: (95.05%) (23239/24448)\n",
      "Epoch: 159 | Batch_idx: 200 |  Loss_1: (0.1317) | Acc_1: (95.05%) (24454/25728)\n",
      "Epoch: 159 | Batch_idx: 210 |  Loss_1: (0.1305) | Acc_1: (95.10%) (25684/27008)\n",
      "Epoch: 159 | Batch_idx: 220 |  Loss_1: (0.1300) | Acc_1: (95.12%) (26908/28288)\n",
      "Epoch: 159 | Batch_idx: 230 |  Loss_1: (0.1301) | Acc_1: (95.12%) (28126/29568)\n",
      "Epoch: 159 | Batch_idx: 240 |  Loss_1: (0.1302) | Acc_1: (95.11%) (29341/30848)\n",
      "Epoch: 159 | Batch_idx: 250 |  Loss_1: (0.1315) | Acc_1: (95.07%) (30544/32128)\n",
      "Epoch: 159 | Batch_idx: 260 |  Loss_1: (0.1310) | Acc_1: (95.08%) (31763/33408)\n",
      "Epoch: 159 | Batch_idx: 270 |  Loss_1: (0.1310) | Acc_1: (95.08%) (32981/34688)\n",
      "Epoch: 159 | Batch_idx: 280 |  Loss_1: (0.1309) | Acc_1: (95.08%) (34199/35968)\n",
      "Epoch: 159 | Batch_idx: 290 |  Loss_1: (0.1309) | Acc_1: (95.08%) (35417/37248)\n",
      "Epoch: 159 | Batch_idx: 300 |  Loss_1: (0.1315) | Acc_1: (95.06%) (36623/38528)\n",
      "Epoch: 159 | Batch_idx: 310 |  Loss_1: (0.1319) | Acc_1: (95.04%) (37834/39808)\n",
      "Epoch: 159 | Batch_idx: 320 |  Loss_1: (0.1317) | Acc_1: (95.05%) (39055/41088)\n",
      "Epoch: 159 | Batch_idx: 330 |  Loss_1: (0.1321) | Acc_1: (95.02%) (40260/42368)\n",
      "Epoch: 159 | Batch_idx: 340 |  Loss_1: (0.1320) | Acc_1: (95.04%) (41484/43648)\n",
      "Epoch: 159 | Batch_idx: 350 |  Loss_1: (0.1320) | Acc_1: (95.04%) (42698/44928)\n",
      "Epoch: 159 | Batch_idx: 360 |  Loss_1: (0.1325) | Acc_1: (95.01%) (43903/46208)\n",
      "Epoch: 159 | Batch_idx: 370 |  Loss_1: (0.1324) | Acc_1: (95.01%) (45120/47488)\n",
      "Epoch: 159 | Batch_idx: 380 |  Loss_1: (0.1323) | Acc_1: (95.01%) (46336/48768)\n",
      "Epoch: 159 | Batch_idx: 390 |  Loss_1: (0.1318) | Acc_1: (95.04%) (47522/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3385) | Acc: (92.13%) (9213/10000)\n",
      "Epoch: 160 | Batch_idx: 0 |  Loss_1: (0.0996) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 160 | Batch_idx: 10 |  Loss_1: (0.1093) | Acc_1: (95.81%) (1349/1408)\n",
      "Epoch: 160 | Batch_idx: 20 |  Loss_1: (0.1328) | Acc_1: (94.83%) (2549/2688)\n",
      "Epoch: 160 | Batch_idx: 30 |  Loss_1: (0.1277) | Acc_1: (95.09%) (3773/3968)\n",
      "Epoch: 160 | Batch_idx: 40 |  Loss_1: (0.1314) | Acc_1: (94.97%) (4984/5248)\n",
      "Epoch: 160 | Batch_idx: 50 |  Loss_1: (0.1295) | Acc_1: (95.07%) (6206/6528)\n",
      "Epoch: 160 | Batch_idx: 60 |  Loss_1: (0.1290) | Acc_1: (95.09%) (7425/7808)\n",
      "Epoch: 160 | Batch_idx: 70 |  Loss_1: (0.1296) | Acc_1: (95.10%) (8643/9088)\n",
      "Epoch: 160 | Batch_idx: 80 |  Loss_1: (0.1278) | Acc_1: (95.16%) (9866/10368)\n",
      "Epoch: 160 | Batch_idx: 90 |  Loss_1: (0.1285) | Acc_1: (95.12%) (11080/11648)\n",
      "Epoch: 160 | Batch_idx: 100 |  Loss_1: (0.1293) | Acc_1: (95.10%) (12294/12928)\n",
      "Epoch: 160 | Batch_idx: 110 |  Loss_1: (0.1304) | Acc_1: (95.09%) (13510/14208)\n",
      "Epoch: 160 | Batch_idx: 120 |  Loss_1: (0.1302) | Acc_1: (95.11%) (14731/15488)\n",
      "Epoch: 160 | Batch_idx: 130 |  Loss_1: (0.1313) | Acc_1: (95.09%) (15944/16768)\n",
      "Epoch: 160 | Batch_idx: 140 |  Loss_1: (0.1306) | Acc_1: (95.10%) (17163/18048)\n",
      "Epoch: 160 | Batch_idx: 150 |  Loss_1: (0.1316) | Acc_1: (95.10%) (18380/19328)\n",
      "Epoch: 160 | Batch_idx: 160 |  Loss_1: (0.1314) | Acc_1: (95.13%) (19604/20608)\n",
      "Epoch: 160 | Batch_idx: 170 |  Loss_1: (0.1310) | Acc_1: (95.14%) (20824/21888)\n",
      "Epoch: 160 | Batch_idx: 180 |  Loss_1: (0.1309) | Acc_1: (95.12%) (22038/23168)\n",
      "Epoch: 160 | Batch_idx: 190 |  Loss_1: (0.1316) | Acc_1: (95.08%) (23244/24448)\n",
      "Epoch: 160 | Batch_idx: 200 |  Loss_1: (0.1319) | Acc_1: (95.04%) (24452/25728)\n",
      "Epoch: 160 | Batch_idx: 210 |  Loss_1: (0.1325) | Acc_1: (95.02%) (25663/27008)\n",
      "Epoch: 160 | Batch_idx: 220 |  Loss_1: (0.1330) | Acc_1: (94.99%) (26872/28288)\n",
      "Epoch: 160 | Batch_idx: 230 |  Loss_1: (0.1336) | Acc_1: (94.96%) (28078/29568)\n",
      "Epoch: 160 | Batch_idx: 240 |  Loss_1: (0.1331) | Acc_1: (95.00%) (29305/30848)\n",
      "Epoch: 160 | Batch_idx: 250 |  Loss_1: (0.1326) | Acc_1: (95.02%) (30529/32128)\n",
      "Epoch: 160 | Batch_idx: 260 |  Loss_1: (0.1326) | Acc_1: (95.02%) (31743/33408)\n",
      "Epoch: 160 | Batch_idx: 270 |  Loss_1: (0.1321) | Acc_1: (95.04%) (32966/34688)\n",
      "Epoch: 160 | Batch_idx: 280 |  Loss_1: (0.1316) | Acc_1: (95.05%) (34188/35968)\n",
      "Epoch: 160 | Batch_idx: 290 |  Loss_1: (0.1313) | Acc_1: (95.06%) (35407/37248)\n",
      "Epoch: 160 | Batch_idx: 300 |  Loss_1: (0.1311) | Acc_1: (95.07%) (36630/38528)\n",
      "Epoch: 160 | Batch_idx: 310 |  Loss_1: (0.1310) | Acc_1: (95.07%) (37844/39808)\n",
      "Epoch: 160 | Batch_idx: 320 |  Loss_1: (0.1304) | Acc_1: (95.09%) (39071/41088)\n",
      "Epoch: 160 | Batch_idx: 330 |  Loss_1: (0.1300) | Acc_1: (95.11%) (40295/42368)\n",
      "Epoch: 160 | Batch_idx: 340 |  Loss_1: (0.1295) | Acc_1: (95.12%) (41517/43648)\n",
      "Epoch: 160 | Batch_idx: 350 |  Loss_1: (0.1290) | Acc_1: (95.14%) (42746/44928)\n",
      "Epoch: 160 | Batch_idx: 360 |  Loss_1: (0.1292) | Acc_1: (95.13%) (43958/46208)\n",
      "Epoch: 160 | Batch_idx: 370 |  Loss_1: (0.1290) | Acc_1: (95.14%) (45180/47488)\n",
      "Epoch: 160 | Batch_idx: 380 |  Loss_1: (0.1296) | Acc_1: (95.11%) (46382/48768)\n",
      "Epoch: 160 | Batch_idx: 390 |  Loss_1: (0.1302) | Acc_1: (95.10%) (47548/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3444) | Acc: (92.34%) (9234/10000)\n",
      "Epoch: 161 | Batch_idx: 0 |  Loss_1: (0.1758) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 161 | Batch_idx: 10 |  Loss_1: (0.1154) | Acc_1: (95.67%) (1347/1408)\n",
      "Epoch: 161 | Batch_idx: 20 |  Loss_1: (0.1259) | Acc_1: (95.24%) (2560/2688)\n",
      "Epoch: 161 | Batch_idx: 30 |  Loss_1: (0.1288) | Acc_1: (95.29%) (3781/3968)\n",
      "Epoch: 161 | Batch_idx: 40 |  Loss_1: (0.1416) | Acc_1: (94.80%) (4975/5248)\n",
      "Epoch: 161 | Batch_idx: 50 |  Loss_1: (0.1381) | Acc_1: (94.91%) (6196/6528)\n",
      "Epoch: 161 | Batch_idx: 60 |  Loss_1: (0.1344) | Acc_1: (94.99%) (7417/7808)\n",
      "Epoch: 161 | Batch_idx: 70 |  Loss_1: (0.1333) | Acc_1: (95.04%) (8637/9088)\n",
      "Epoch: 161 | Batch_idx: 80 |  Loss_1: (0.1304) | Acc_1: (95.16%) (9866/10368)\n",
      "Epoch: 161 | Batch_idx: 90 |  Loss_1: (0.1302) | Acc_1: (95.16%) (11084/11648)\n",
      "Epoch: 161 | Batch_idx: 100 |  Loss_1: (0.1313) | Acc_1: (95.12%) (12297/12928)\n",
      "Epoch: 161 | Batch_idx: 110 |  Loss_1: (0.1309) | Acc_1: (95.12%) (13514/14208)\n",
      "Epoch: 161 | Batch_idx: 120 |  Loss_1: (0.1316) | Acc_1: (95.11%) (14730/15488)\n",
      "Epoch: 161 | Batch_idx: 130 |  Loss_1: (0.1326) | Acc_1: (95.10%) (15946/16768)\n",
      "Epoch: 161 | Batch_idx: 140 |  Loss_1: (0.1313) | Acc_1: (95.15%) (17172/18048)\n",
      "Epoch: 161 | Batch_idx: 150 |  Loss_1: (0.1312) | Acc_1: (95.15%) (18391/19328)\n",
      "Epoch: 161 | Batch_idx: 160 |  Loss_1: (0.1312) | Acc_1: (95.14%) (19607/20608)\n",
      "Epoch: 161 | Batch_idx: 170 |  Loss_1: (0.1322) | Acc_1: (95.09%) (20813/21888)\n",
      "Epoch: 161 | Batch_idx: 180 |  Loss_1: (0.1323) | Acc_1: (95.07%) (22026/23168)\n",
      "Epoch: 161 | Batch_idx: 190 |  Loss_1: (0.1323) | Acc_1: (95.07%) (23242/24448)\n",
      "Epoch: 161 | Batch_idx: 200 |  Loss_1: (0.1324) | Acc_1: (95.06%) (24456/25728)\n",
      "Epoch: 161 | Batch_idx: 210 |  Loss_1: (0.1326) | Acc_1: (95.04%) (25669/27008)\n",
      "Epoch: 161 | Batch_idx: 220 |  Loss_1: (0.1323) | Acc_1: (95.07%) (26892/28288)\n",
      "Epoch: 161 | Batch_idx: 230 |  Loss_1: (0.1332) | Acc_1: (95.03%) (28097/29568)\n",
      "Epoch: 161 | Batch_idx: 240 |  Loss_1: (0.1332) | Acc_1: (95.02%) (29311/30848)\n",
      "Epoch: 161 | Batch_idx: 250 |  Loss_1: (0.1342) | Acc_1: (94.97%) (30513/32128)\n",
      "Epoch: 161 | Batch_idx: 260 |  Loss_1: (0.1342) | Acc_1: (94.98%) (31731/33408)\n",
      "Epoch: 161 | Batch_idx: 270 |  Loss_1: (0.1336) | Acc_1: (95.00%) (32952/34688)\n",
      "Epoch: 161 | Batch_idx: 280 |  Loss_1: (0.1336) | Acc_1: (94.98%) (34162/35968)\n",
      "Epoch: 161 | Batch_idx: 290 |  Loss_1: (0.1338) | Acc_1: (94.97%) (35373/37248)\n",
      "Epoch: 161 | Batch_idx: 300 |  Loss_1: (0.1339) | Acc_1: (94.99%) (36596/38528)\n",
      "Epoch: 161 | Batch_idx: 310 |  Loss_1: (0.1343) | Acc_1: (94.97%) (37804/39808)\n",
      "Epoch: 161 | Batch_idx: 320 |  Loss_1: (0.1340) | Acc_1: (94.98%) (39024/41088)\n",
      "Epoch: 161 | Batch_idx: 330 |  Loss_1: (0.1338) | Acc_1: (94.97%) (40239/42368)\n",
      "Epoch: 161 | Batch_idx: 340 |  Loss_1: (0.1339) | Acc_1: (94.97%) (41452/43648)\n",
      "Epoch: 161 | Batch_idx: 350 |  Loss_1: (0.1325) | Acc_1: (95.01%) (42686/44928)\n",
      "Epoch: 161 | Batch_idx: 360 |  Loss_1: (0.1337) | Acc_1: (94.96%) (43880/46208)\n",
      "Epoch: 161 | Batch_idx: 370 |  Loss_1: (0.1323) | Acc_1: (95.02%) (45122/47488)\n",
      "Epoch: 161 | Batch_idx: 380 |  Loss_1: (0.1319) | Acc_1: (95.03%) (46343/48768)\n",
      "Epoch: 161 | Batch_idx: 390 |  Loss_1: (0.1317) | Acc_1: (95.03%) (47514/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3485) | Acc: (92.17%) (9217/10000)\n",
      "Epoch: 162 | Batch_idx: 0 |  Loss_1: (0.0722) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 162 | Batch_idx: 10 |  Loss_1: (0.1036) | Acc_1: (96.16%) (1354/1408)\n",
      "Epoch: 162 | Batch_idx: 20 |  Loss_1: (0.1093) | Acc_1: (95.98%) (2580/2688)\n",
      "Epoch: 162 | Batch_idx: 30 |  Loss_1: (0.1074) | Acc_1: (96.02%) (3810/3968)\n",
      "Epoch: 162 | Batch_idx: 40 |  Loss_1: (0.1079) | Acc_1: (96.00%) (5038/5248)\n",
      "Epoch: 162 | Batch_idx: 50 |  Loss_1: (0.1087) | Acc_1: (95.99%) (6266/6528)\n",
      "Epoch: 162 | Batch_idx: 60 |  Loss_1: (0.1124) | Acc_1: (95.91%) (7489/7808)\n",
      "Epoch: 162 | Batch_idx: 70 |  Loss_1: (0.1125) | Acc_1: (95.91%) (8716/9088)\n",
      "Epoch: 162 | Batch_idx: 80 |  Loss_1: (0.1173) | Acc_1: (95.70%) (9922/10368)\n",
      "Epoch: 162 | Batch_idx: 90 |  Loss_1: (0.1172) | Acc_1: (95.72%) (11149/11648)\n",
      "Epoch: 162 | Batch_idx: 100 |  Loss_1: (0.1186) | Acc_1: (95.61%) (12361/12928)\n",
      "Epoch: 162 | Batch_idx: 110 |  Loss_1: (0.1187) | Acc_1: (95.59%) (13582/14208)\n",
      "Epoch: 162 | Batch_idx: 120 |  Loss_1: (0.1193) | Acc_1: (95.57%) (14802/15488)\n",
      "Epoch: 162 | Batch_idx: 130 |  Loss_1: (0.1204) | Acc_1: (95.52%) (16016/16768)\n",
      "Epoch: 162 | Batch_idx: 140 |  Loss_1: (0.1223) | Acc_1: (95.41%) (17220/18048)\n",
      "Epoch: 162 | Batch_idx: 150 |  Loss_1: (0.1228) | Acc_1: (95.42%) (18442/19328)\n",
      "Epoch: 162 | Batch_idx: 160 |  Loss_1: (0.1227) | Acc_1: (95.41%) (19662/20608)\n",
      "Epoch: 162 | Batch_idx: 170 |  Loss_1: (0.1224) | Acc_1: (95.46%) (20894/21888)\n",
      "Epoch: 162 | Batch_idx: 180 |  Loss_1: (0.1230) | Acc_1: (95.44%) (22111/23168)\n",
      "Epoch: 162 | Batch_idx: 190 |  Loss_1: (0.1215) | Acc_1: (95.49%) (23346/24448)\n",
      "Epoch: 162 | Batch_idx: 200 |  Loss_1: (0.1211) | Acc_1: (95.52%) (24575/25728)\n",
      "Epoch: 162 | Batch_idx: 210 |  Loss_1: (0.1217) | Acc_1: (95.50%) (25793/27008)\n",
      "Epoch: 162 | Batch_idx: 220 |  Loss_1: (0.1228) | Acc_1: (95.46%) (27005/28288)\n",
      "Epoch: 162 | Batch_idx: 230 |  Loss_1: (0.1238) | Acc_1: (95.44%) (28219/29568)\n",
      "Epoch: 162 | Batch_idx: 240 |  Loss_1: (0.1246) | Acc_1: (95.39%) (29427/30848)\n",
      "Epoch: 162 | Batch_idx: 250 |  Loss_1: (0.1248) | Acc_1: (95.38%) (30644/32128)\n",
      "Epoch: 162 | Batch_idx: 260 |  Loss_1: (0.1252) | Acc_1: (95.37%) (31861/33408)\n",
      "Epoch: 162 | Batch_idx: 270 |  Loss_1: (0.1252) | Acc_1: (95.38%) (33084/34688)\n",
      "Epoch: 162 | Batch_idx: 280 |  Loss_1: (0.1256) | Acc_1: (95.36%) (34299/35968)\n",
      "Epoch: 162 | Batch_idx: 290 |  Loss_1: (0.1265) | Acc_1: (95.33%) (35508/37248)\n",
      "Epoch: 162 | Batch_idx: 300 |  Loss_1: (0.1267) | Acc_1: (95.33%) (36729/38528)\n",
      "Epoch: 162 | Batch_idx: 310 |  Loss_1: (0.1262) | Acc_1: (95.34%) (37953/39808)\n",
      "Epoch: 162 | Batch_idx: 320 |  Loss_1: (0.1265) | Acc_1: (95.33%) (39169/41088)\n",
      "Epoch: 162 | Batch_idx: 330 |  Loss_1: (0.1258) | Acc_1: (95.37%) (40406/42368)\n",
      "Epoch: 162 | Batch_idx: 340 |  Loss_1: (0.1260) | Acc_1: (95.35%) (41617/43648)\n",
      "Epoch: 162 | Batch_idx: 350 |  Loss_1: (0.1265) | Acc_1: (95.32%) (42827/44928)\n",
      "Epoch: 162 | Batch_idx: 360 |  Loss_1: (0.1268) | Acc_1: (95.31%) (44039/46208)\n",
      "Epoch: 162 | Batch_idx: 370 |  Loss_1: (0.1267) | Acc_1: (95.32%) (45264/47488)\n",
      "Epoch: 162 | Batch_idx: 380 |  Loss_1: (0.1268) | Acc_1: (95.31%) (46480/48768)\n",
      "Epoch: 162 | Batch_idx: 390 |  Loss_1: (0.1265) | Acc_1: (95.32%) (47661/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3394) | Acc: (92.26%) (9226/10000)\n",
      "Epoch: 163 | Batch_idx: 0 |  Loss_1: (0.0826) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 163 | Batch_idx: 10 |  Loss_1: (0.1088) | Acc_1: (95.31%) (1342/1408)\n",
      "Epoch: 163 | Batch_idx: 20 |  Loss_1: (0.1224) | Acc_1: (95.01%) (2554/2688)\n",
      "Epoch: 163 | Batch_idx: 30 |  Loss_1: (0.1142) | Acc_1: (95.44%) (3787/3968)\n",
      "Epoch: 163 | Batch_idx: 40 |  Loss_1: (0.1174) | Acc_1: (95.45%) (5009/5248)\n",
      "Epoch: 163 | Batch_idx: 50 |  Loss_1: (0.1190) | Acc_1: (95.42%) (6229/6528)\n",
      "Epoch: 163 | Batch_idx: 60 |  Loss_1: (0.1232) | Acc_1: (95.31%) (7442/7808)\n",
      "Epoch: 163 | Batch_idx: 70 |  Loss_1: (0.1247) | Acc_1: (95.26%) (8657/9088)\n",
      "Epoch: 163 | Batch_idx: 80 |  Loss_1: (0.1212) | Acc_1: (95.41%) (9892/10368)\n",
      "Epoch: 163 | Batch_idx: 90 |  Loss_1: (0.1242) | Acc_1: (95.28%) (11098/11648)\n",
      "Epoch: 163 | Batch_idx: 100 |  Loss_1: (0.1227) | Acc_1: (95.36%) (12328/12928)\n",
      "Epoch: 163 | Batch_idx: 110 |  Loss_1: (0.1231) | Acc_1: (95.36%) (13549/14208)\n",
      "Epoch: 163 | Batch_idx: 120 |  Loss_1: (0.1242) | Acc_1: (95.34%) (14766/15488)\n",
      "Epoch: 163 | Batch_idx: 130 |  Loss_1: (0.1243) | Acc_1: (95.31%) (15982/16768)\n",
      "Epoch: 163 | Batch_idx: 140 |  Loss_1: (0.1230) | Acc_1: (95.36%) (17211/18048)\n",
      "Epoch: 163 | Batch_idx: 150 |  Loss_1: (0.1249) | Acc_1: (95.29%) (18417/19328)\n",
      "Epoch: 163 | Batch_idx: 160 |  Loss_1: (0.1253) | Acc_1: (95.28%) (19636/20608)\n",
      "Epoch: 163 | Batch_idx: 170 |  Loss_1: (0.1245) | Acc_1: (95.36%) (20872/21888)\n",
      "Epoch: 163 | Batch_idx: 180 |  Loss_1: (0.1250) | Acc_1: (95.33%) (22087/23168)\n",
      "Epoch: 163 | Batch_idx: 190 |  Loss_1: (0.1249) | Acc_1: (95.34%) (23309/24448)\n",
      "Epoch: 163 | Batch_idx: 200 |  Loss_1: (0.1240) | Acc_1: (95.37%) (24538/25728)\n",
      "Epoch: 163 | Batch_idx: 210 |  Loss_1: (0.1236) | Acc_1: (95.39%) (25764/27008)\n",
      "Epoch: 163 | Batch_idx: 220 |  Loss_1: (0.1230) | Acc_1: (95.41%) (26989/28288)\n",
      "Epoch: 163 | Batch_idx: 230 |  Loss_1: (0.1225) | Acc_1: (95.44%) (28219/29568)\n",
      "Epoch: 163 | Batch_idx: 240 |  Loss_1: (0.1237) | Acc_1: (95.39%) (29425/30848)\n",
      "Epoch: 163 | Batch_idx: 250 |  Loss_1: (0.1236) | Acc_1: (95.39%) (30647/32128)\n",
      "Epoch: 163 | Batch_idx: 260 |  Loss_1: (0.1240) | Acc_1: (95.37%) (31861/33408)\n",
      "Epoch: 163 | Batch_idx: 270 |  Loss_1: (0.1237) | Acc_1: (95.38%) (33087/34688)\n",
      "Epoch: 163 | Batch_idx: 280 |  Loss_1: (0.1246) | Acc_1: (95.35%) (34295/35968)\n",
      "Epoch: 163 | Batch_idx: 290 |  Loss_1: (0.1246) | Acc_1: (95.34%) (35513/37248)\n",
      "Epoch: 163 | Batch_idx: 300 |  Loss_1: (0.1250) | Acc_1: (95.33%) (36728/38528)\n",
      "Epoch: 163 | Batch_idx: 310 |  Loss_1: (0.1246) | Acc_1: (95.34%) (37953/39808)\n",
      "Epoch: 163 | Batch_idx: 320 |  Loss_1: (0.1249) | Acc_1: (95.33%) (39169/41088)\n",
      "Epoch: 163 | Batch_idx: 330 |  Loss_1: (0.1252) | Acc_1: (95.31%) (40381/42368)\n",
      "Epoch: 163 | Batch_idx: 340 |  Loss_1: (0.1254) | Acc_1: (95.29%) (41594/43648)\n",
      "Epoch: 163 | Batch_idx: 350 |  Loss_1: (0.1261) | Acc_1: (95.27%) (42804/44928)\n",
      "Epoch: 163 | Batch_idx: 360 |  Loss_1: (0.1255) | Acc_1: (95.30%) (44037/46208)\n",
      "Epoch: 163 | Batch_idx: 370 |  Loss_1: (0.1257) | Acc_1: (95.29%) (45252/47488)\n",
      "Epoch: 163 | Batch_idx: 380 |  Loss_1: (0.1260) | Acc_1: (95.27%) (46462/48768)\n",
      "Epoch: 163 | Batch_idx: 390 |  Loss_1: (0.1260) | Acc_1: (95.26%) (47630/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3496) | Acc: (92.08%) (9208/10000)\n",
      "Epoch: 164 | Batch_idx: 0 |  Loss_1: (0.0767) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 164 | Batch_idx: 10 |  Loss_1: (0.1191) | Acc_1: (95.38%) (1343/1408)\n",
      "Epoch: 164 | Batch_idx: 20 |  Loss_1: (0.1258) | Acc_1: (95.09%) (2556/2688)\n",
      "Epoch: 164 | Batch_idx: 30 |  Loss_1: (0.1281) | Acc_1: (95.14%) (3775/3968)\n",
      "Epoch: 164 | Batch_idx: 40 |  Loss_1: (0.1239) | Acc_1: (95.31%) (5002/5248)\n",
      "Epoch: 164 | Batch_idx: 50 |  Loss_1: (0.1233) | Acc_1: (95.31%) (6222/6528)\n",
      "Epoch: 164 | Batch_idx: 60 |  Loss_1: (0.1199) | Acc_1: (95.44%) (7452/7808)\n",
      "Epoch: 164 | Batch_idx: 70 |  Loss_1: (0.1257) | Acc_1: (95.22%) (8654/9088)\n",
      "Epoch: 164 | Batch_idx: 80 |  Loss_1: (0.1280) | Acc_1: (95.14%) (9864/10368)\n",
      "Epoch: 164 | Batch_idx: 90 |  Loss_1: (0.1300) | Acc_1: (95.10%) (11077/11648)\n",
      "Epoch: 164 | Batch_idx: 100 |  Loss_1: (0.1310) | Acc_1: (95.08%) (12292/12928)\n",
      "Epoch: 164 | Batch_idx: 110 |  Loss_1: (0.1305) | Acc_1: (95.10%) (13512/14208)\n",
      "Epoch: 164 | Batch_idx: 120 |  Loss_1: (0.1313) | Acc_1: (95.07%) (14725/15488)\n",
      "Epoch: 164 | Batch_idx: 130 |  Loss_1: (0.1303) | Acc_1: (95.12%) (15949/16768)\n",
      "Epoch: 164 | Batch_idx: 140 |  Loss_1: (0.1330) | Acc_1: (95.06%) (17156/18048)\n",
      "Epoch: 164 | Batch_idx: 150 |  Loss_1: (0.1325) | Acc_1: (95.08%) (18377/19328)\n",
      "Epoch: 164 | Batch_idx: 160 |  Loss_1: (0.1303) | Acc_1: (95.16%) (19610/20608)\n",
      "Epoch: 164 | Batch_idx: 170 |  Loss_1: (0.1302) | Acc_1: (95.17%) (20830/21888)\n",
      "Epoch: 164 | Batch_idx: 180 |  Loss_1: (0.1313) | Acc_1: (95.11%) (22036/23168)\n",
      "Epoch: 164 | Batch_idx: 190 |  Loss_1: (0.1310) | Acc_1: (95.12%) (23256/24448)\n",
      "Epoch: 164 | Batch_idx: 200 |  Loss_1: (0.1299) | Acc_1: (95.17%) (24486/25728)\n",
      "Epoch: 164 | Batch_idx: 210 |  Loss_1: (0.1294) | Acc_1: (95.18%) (25705/27008)\n",
      "Epoch: 164 | Batch_idx: 220 |  Loss_1: (0.1296) | Acc_1: (95.21%) (26933/28288)\n",
      "Epoch: 164 | Batch_idx: 230 |  Loss_1: (0.1290) | Acc_1: (95.23%) (28157/29568)\n",
      "Epoch: 164 | Batch_idx: 240 |  Loss_1: (0.1291) | Acc_1: (95.22%) (29372/30848)\n",
      "Epoch: 164 | Batch_idx: 250 |  Loss_1: (0.1284) | Acc_1: (95.24%) (30599/32128)\n",
      "Epoch: 164 | Batch_idx: 260 |  Loss_1: (0.1295) | Acc_1: (95.20%) (31805/33408)\n",
      "Epoch: 164 | Batch_idx: 270 |  Loss_1: (0.1282) | Acc_1: (95.25%) (33040/34688)\n",
      "Epoch: 164 | Batch_idx: 280 |  Loss_1: (0.1281) | Acc_1: (95.27%) (34266/35968)\n",
      "Epoch: 164 | Batch_idx: 290 |  Loss_1: (0.1275) | Acc_1: (95.28%) (35490/37248)\n",
      "Epoch: 164 | Batch_idx: 300 |  Loss_1: (0.1274) | Acc_1: (95.28%) (36711/38528)\n",
      "Epoch: 164 | Batch_idx: 310 |  Loss_1: (0.1266) | Acc_1: (95.32%) (37946/39808)\n",
      "Epoch: 164 | Batch_idx: 320 |  Loss_1: (0.1275) | Acc_1: (95.28%) (39148/41088)\n",
      "Epoch: 164 | Batch_idx: 330 |  Loss_1: (0.1270) | Acc_1: (95.29%) (40374/42368)\n",
      "Epoch: 164 | Batch_idx: 340 |  Loss_1: (0.1269) | Acc_1: (95.29%) (41593/43648)\n",
      "Epoch: 164 | Batch_idx: 350 |  Loss_1: (0.1279) | Acc_1: (95.26%) (42798/44928)\n",
      "Epoch: 164 | Batch_idx: 360 |  Loss_1: (0.1283) | Acc_1: (95.25%) (44015/46208)\n",
      "Epoch: 164 | Batch_idx: 370 |  Loss_1: (0.1277) | Acc_1: (95.27%) (45243/47488)\n",
      "Epoch: 164 | Batch_idx: 380 |  Loss_1: (0.1275) | Acc_1: (95.27%) (46463/48768)\n",
      "Epoch: 164 | Batch_idx: 390 |  Loss_1: (0.1278) | Acc_1: (95.26%) (47630/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3432) | Acc: (92.11%) (9211/10000)\n",
      "Epoch: 165 | Batch_idx: 0 |  Loss_1: (0.0893) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 165 | Batch_idx: 10 |  Loss_1: (0.1363) | Acc_1: (94.96%) (1337/1408)\n",
      "Epoch: 165 | Batch_idx: 20 |  Loss_1: (0.1223) | Acc_1: (95.54%) (2568/2688)\n",
      "Epoch: 165 | Batch_idx: 30 |  Loss_1: (0.1218) | Acc_1: (95.54%) (3791/3968)\n",
      "Epoch: 165 | Batch_idx: 40 |  Loss_1: (0.1172) | Acc_1: (95.67%) (5021/5248)\n",
      "Epoch: 165 | Batch_idx: 50 |  Loss_1: (0.1158) | Acc_1: (95.65%) (6244/6528)\n",
      "Epoch: 165 | Batch_idx: 60 |  Loss_1: (0.1178) | Acc_1: (95.67%) (7470/7808)\n",
      "Epoch: 165 | Batch_idx: 70 |  Loss_1: (0.1168) | Acc_1: (95.69%) (8696/9088)\n",
      "Epoch: 165 | Batch_idx: 80 |  Loss_1: (0.1189) | Acc_1: (95.57%) (9909/10368)\n",
      "Epoch: 165 | Batch_idx: 90 |  Loss_1: (0.1196) | Acc_1: (95.60%) (11135/11648)\n",
      "Epoch: 165 | Batch_idx: 100 |  Loss_1: (0.1208) | Acc_1: (95.51%) (12347/12928)\n",
      "Epoch: 165 | Batch_idx: 110 |  Loss_1: (0.1208) | Acc_1: (95.50%) (13568/14208)\n",
      "Epoch: 165 | Batch_idx: 120 |  Loss_1: (0.1202) | Acc_1: (95.51%) (14793/15488)\n",
      "Epoch: 165 | Batch_idx: 130 |  Loss_1: (0.1194) | Acc_1: (95.53%) (16018/16768)\n",
      "Epoch: 165 | Batch_idx: 140 |  Loss_1: (0.1193) | Acc_1: (95.54%) (17243/18048)\n",
      "Epoch: 165 | Batch_idx: 150 |  Loss_1: (0.1200) | Acc_1: (95.53%) (18464/19328)\n",
      "Epoch: 165 | Batch_idx: 160 |  Loss_1: (0.1219) | Acc_1: (95.45%) (19670/20608)\n",
      "Epoch: 165 | Batch_idx: 170 |  Loss_1: (0.1226) | Acc_1: (95.42%) (20885/21888)\n",
      "Epoch: 165 | Batch_idx: 180 |  Loss_1: (0.1231) | Acc_1: (95.42%) (22108/23168)\n",
      "Epoch: 165 | Batch_idx: 190 |  Loss_1: (0.1224) | Acc_1: (95.44%) (23334/24448)\n",
      "Epoch: 165 | Batch_idx: 200 |  Loss_1: (0.1233) | Acc_1: (95.41%) (24546/25728)\n",
      "Epoch: 165 | Batch_idx: 210 |  Loss_1: (0.1241) | Acc_1: (95.37%) (25757/27008)\n",
      "Epoch: 165 | Batch_idx: 220 |  Loss_1: (0.1250) | Acc_1: (95.33%) (26967/28288)\n",
      "Epoch: 165 | Batch_idx: 230 |  Loss_1: (0.1257) | Acc_1: (95.31%) (28180/29568)\n",
      "Epoch: 165 | Batch_idx: 240 |  Loss_1: (0.1255) | Acc_1: (95.32%) (29403/30848)\n",
      "Epoch: 165 | Batch_idx: 250 |  Loss_1: (0.1258) | Acc_1: (95.31%) (30620/32128)\n",
      "Epoch: 165 | Batch_idx: 260 |  Loss_1: (0.1253) | Acc_1: (95.33%) (31848/33408)\n",
      "Epoch: 165 | Batch_idx: 270 |  Loss_1: (0.1253) | Acc_1: (95.32%) (33065/34688)\n",
      "Epoch: 165 | Batch_idx: 280 |  Loss_1: (0.1257) | Acc_1: (95.31%) (34280/35968)\n",
      "Epoch: 165 | Batch_idx: 290 |  Loss_1: (0.1260) | Acc_1: (95.31%) (35501/37248)\n",
      "Epoch: 165 | Batch_idx: 300 |  Loss_1: (0.1266) | Acc_1: (95.28%) (36711/38528)\n",
      "Epoch: 165 | Batch_idx: 310 |  Loss_1: (0.1268) | Acc_1: (95.27%) (37925/39808)\n",
      "Epoch: 165 | Batch_idx: 320 |  Loss_1: (0.1264) | Acc_1: (95.28%) (39149/41088)\n",
      "Epoch: 165 | Batch_idx: 330 |  Loss_1: (0.1263) | Acc_1: (95.29%) (40372/42368)\n",
      "Epoch: 165 | Batch_idx: 340 |  Loss_1: (0.1259) | Acc_1: (95.29%) (41594/43648)\n",
      "Epoch: 165 | Batch_idx: 350 |  Loss_1: (0.1259) | Acc_1: (95.30%) (42815/44928)\n",
      "Epoch: 165 | Batch_idx: 360 |  Loss_1: (0.1261) | Acc_1: (95.30%) (44034/46208)\n",
      "Epoch: 165 | Batch_idx: 370 |  Loss_1: (0.1256) | Acc_1: (95.31%) (45261/47488)\n",
      "Epoch: 165 | Batch_idx: 380 |  Loss_1: (0.1258) | Acc_1: (95.31%) (46482/48768)\n",
      "Epoch: 165 | Batch_idx: 390 |  Loss_1: (0.1258) | Acc_1: (95.30%) (47650/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3418) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 166 | Batch_idx: 0 |  Loss_1: (0.1502) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 166 | Batch_idx: 10 |  Loss_1: (0.1288) | Acc_1: (95.53%) (1345/1408)\n",
      "Epoch: 166 | Batch_idx: 20 |  Loss_1: (0.1115) | Acc_1: (96.24%) (2587/2688)\n",
      "Epoch: 166 | Batch_idx: 30 |  Loss_1: (0.1211) | Acc_1: (95.77%) (3800/3968)\n",
      "Epoch: 166 | Batch_idx: 40 |  Loss_1: (0.1139) | Acc_1: (95.98%) (5037/5248)\n",
      "Epoch: 166 | Batch_idx: 50 |  Loss_1: (0.1130) | Acc_1: (95.97%) (6265/6528)\n",
      "Epoch: 166 | Batch_idx: 60 |  Loss_1: (0.1147) | Acc_1: (95.88%) (7486/7808)\n",
      "Epoch: 166 | Batch_idx: 70 |  Loss_1: (0.1160) | Acc_1: (95.82%) (8708/9088)\n",
      "Epoch: 166 | Batch_idx: 80 |  Loss_1: (0.1207) | Acc_1: (95.57%) (9909/10368)\n",
      "Epoch: 166 | Batch_idx: 90 |  Loss_1: (0.1216) | Acc_1: (95.51%) (11125/11648)\n",
      "Epoch: 166 | Batch_idx: 100 |  Loss_1: (0.1209) | Acc_1: (95.53%) (12350/12928)\n",
      "Epoch: 166 | Batch_idx: 110 |  Loss_1: (0.1217) | Acc_1: (95.50%) (13569/14208)\n",
      "Epoch: 166 | Batch_idx: 120 |  Loss_1: (0.1217) | Acc_1: (95.49%) (14790/15488)\n",
      "Epoch: 166 | Batch_idx: 130 |  Loss_1: (0.1221) | Acc_1: (95.51%) (16015/16768)\n",
      "Epoch: 166 | Batch_idx: 140 |  Loss_1: (0.1213) | Acc_1: (95.51%) (17238/18048)\n",
      "Epoch: 166 | Batch_idx: 150 |  Loss_1: (0.1221) | Acc_1: (95.46%) (18450/19328)\n",
      "Epoch: 166 | Batch_idx: 160 |  Loss_1: (0.1212) | Acc_1: (95.47%) (19674/20608)\n",
      "Epoch: 166 | Batch_idx: 170 |  Loss_1: (0.1198) | Acc_1: (95.54%) (20912/21888)\n",
      "Epoch: 166 | Batch_idx: 180 |  Loss_1: (0.1206) | Acc_1: (95.50%) (22125/23168)\n",
      "Epoch: 166 | Batch_idx: 190 |  Loss_1: (0.1207) | Acc_1: (95.48%) (23344/24448)\n",
      "Epoch: 166 | Batch_idx: 200 |  Loss_1: (0.1198) | Acc_1: (95.51%) (24574/25728)\n",
      "Epoch: 166 | Batch_idx: 210 |  Loss_1: (0.1208) | Acc_1: (95.49%) (25789/27008)\n",
      "Epoch: 166 | Batch_idx: 220 |  Loss_1: (0.1215) | Acc_1: (95.45%) (27002/28288)\n",
      "Epoch: 166 | Batch_idx: 230 |  Loss_1: (0.1207) | Acc_1: (95.49%) (28235/29568)\n",
      "Epoch: 166 | Batch_idx: 240 |  Loss_1: (0.1216) | Acc_1: (95.46%) (29447/30848)\n",
      "Epoch: 166 | Batch_idx: 250 |  Loss_1: (0.1219) | Acc_1: (95.45%) (30665/32128)\n",
      "Epoch: 166 | Batch_idx: 260 |  Loss_1: (0.1211) | Acc_1: (95.48%) (31897/33408)\n",
      "Epoch: 166 | Batch_idx: 270 |  Loss_1: (0.1213) | Acc_1: (95.47%) (33117/34688)\n",
      "Epoch: 166 | Batch_idx: 280 |  Loss_1: (0.1211) | Acc_1: (95.47%) (34340/35968)\n",
      "Epoch: 166 | Batch_idx: 290 |  Loss_1: (0.1211) | Acc_1: (95.47%) (35560/37248)\n",
      "Epoch: 166 | Batch_idx: 300 |  Loss_1: (0.1215) | Acc_1: (95.47%) (36781/38528)\n",
      "Epoch: 166 | Batch_idx: 310 |  Loss_1: (0.1222) | Acc_1: (95.43%) (37990/39808)\n",
      "Epoch: 166 | Batch_idx: 320 |  Loss_1: (0.1224) | Acc_1: (95.42%) (39207/41088)\n",
      "Epoch: 166 | Batch_idx: 330 |  Loss_1: (0.1216) | Acc_1: (95.45%) (40440/42368)\n",
      "Epoch: 166 | Batch_idx: 340 |  Loss_1: (0.1217) | Acc_1: (95.45%) (41663/43648)\n",
      "Epoch: 166 | Batch_idx: 350 |  Loss_1: (0.1225) | Acc_1: (95.43%) (42876/44928)\n",
      "Epoch: 166 | Batch_idx: 360 |  Loss_1: (0.1234) | Acc_1: (95.39%) (44077/46208)\n",
      "Epoch: 166 | Batch_idx: 370 |  Loss_1: (0.1229) | Acc_1: (95.40%) (45304/47488)\n",
      "Epoch: 166 | Batch_idx: 380 |  Loss_1: (0.1229) | Acc_1: (95.40%) (46526/48768)\n",
      "Epoch: 166 | Batch_idx: 390 |  Loss_1: (0.1229) | Acc_1: (95.41%) (47705/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3682) | Acc: (91.97%) (9197/10000)\n",
      "Epoch: 167 | Batch_idx: 0 |  Loss_1: (0.1538) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 167 | Batch_idx: 10 |  Loss_1: (0.1358) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 167 | Batch_idx: 20 |  Loss_1: (0.1386) | Acc_1: (94.75%) (2547/2688)\n",
      "Epoch: 167 | Batch_idx: 30 |  Loss_1: (0.1350) | Acc_1: (94.91%) (3766/3968)\n",
      "Epoch: 167 | Batch_idx: 40 |  Loss_1: (0.1292) | Acc_1: (95.12%) (4992/5248)\n",
      "Epoch: 167 | Batch_idx: 50 |  Loss_1: (0.1280) | Acc_1: (95.13%) (6210/6528)\n",
      "Epoch: 167 | Batch_idx: 60 |  Loss_1: (0.1242) | Acc_1: (95.21%) (7434/7808)\n",
      "Epoch: 167 | Batch_idx: 70 |  Loss_1: (0.1261) | Acc_1: (95.16%) (8648/9088)\n",
      "Epoch: 167 | Batch_idx: 80 |  Loss_1: (0.1258) | Acc_1: (95.21%) (9871/10368)\n",
      "Epoch: 167 | Batch_idx: 90 |  Loss_1: (0.1241) | Acc_1: (95.27%) (11097/11648)\n",
      "Epoch: 167 | Batch_idx: 100 |  Loss_1: (0.1232) | Acc_1: (95.34%) (12325/12928)\n",
      "Epoch: 167 | Batch_idx: 110 |  Loss_1: (0.1244) | Acc_1: (95.35%) (13547/14208)\n",
      "Epoch: 167 | Batch_idx: 120 |  Loss_1: (0.1245) | Acc_1: (95.34%) (14766/15488)\n",
      "Epoch: 167 | Batch_idx: 130 |  Loss_1: (0.1223) | Acc_1: (95.43%) (16001/16768)\n",
      "Epoch: 167 | Batch_idx: 140 |  Loss_1: (0.1223) | Acc_1: (95.41%) (17219/18048)\n",
      "Epoch: 167 | Batch_idx: 150 |  Loss_1: (0.1225) | Acc_1: (95.41%) (18441/19328)\n",
      "Epoch: 167 | Batch_idx: 160 |  Loss_1: (0.1215) | Acc_1: (95.44%) (19668/20608)\n",
      "Epoch: 167 | Batch_idx: 170 |  Loss_1: (0.1221) | Acc_1: (95.43%) (20888/21888)\n",
      "Epoch: 167 | Batch_idx: 180 |  Loss_1: (0.1225) | Acc_1: (95.42%) (22107/23168)\n",
      "Epoch: 167 | Batch_idx: 190 |  Loss_1: (0.1225) | Acc_1: (95.40%) (23324/24448)\n",
      "Epoch: 167 | Batch_idx: 200 |  Loss_1: (0.1234) | Acc_1: (95.37%) (24537/25728)\n",
      "Epoch: 167 | Batch_idx: 210 |  Loss_1: (0.1240) | Acc_1: (95.33%) (25746/27008)\n",
      "Epoch: 167 | Batch_idx: 220 |  Loss_1: (0.1239) | Acc_1: (95.33%) (26967/28288)\n",
      "Epoch: 167 | Batch_idx: 230 |  Loss_1: (0.1246) | Acc_1: (95.30%) (28178/29568)\n",
      "Epoch: 167 | Batch_idx: 240 |  Loss_1: (0.1246) | Acc_1: (95.30%) (29397/30848)\n",
      "Epoch: 167 | Batch_idx: 250 |  Loss_1: (0.1243) | Acc_1: (95.30%) (30618/32128)\n",
      "Epoch: 167 | Batch_idx: 260 |  Loss_1: (0.1248) | Acc_1: (95.29%) (31834/33408)\n",
      "Epoch: 167 | Batch_idx: 270 |  Loss_1: (0.1245) | Acc_1: (95.29%) (33054/34688)\n",
      "Epoch: 167 | Batch_idx: 280 |  Loss_1: (0.1236) | Acc_1: (95.33%) (34290/35968)\n",
      "Epoch: 167 | Batch_idx: 290 |  Loss_1: (0.1233) | Acc_1: (95.35%) (35515/37248)\n",
      "Epoch: 167 | Batch_idx: 300 |  Loss_1: (0.1234) | Acc_1: (95.34%) (36732/38528)\n",
      "Epoch: 167 | Batch_idx: 310 |  Loss_1: (0.1237) | Acc_1: (95.33%) (37948/39808)\n",
      "Epoch: 167 | Batch_idx: 320 |  Loss_1: (0.1230) | Acc_1: (95.35%) (39179/41088)\n",
      "Epoch: 167 | Batch_idx: 330 |  Loss_1: (0.1228) | Acc_1: (95.38%) (40409/42368)\n",
      "Epoch: 167 | Batch_idx: 340 |  Loss_1: (0.1229) | Acc_1: (95.37%) (41626/43648)\n",
      "Epoch: 167 | Batch_idx: 350 |  Loss_1: (0.1229) | Acc_1: (95.37%) (42849/44928)\n",
      "Epoch: 167 | Batch_idx: 360 |  Loss_1: (0.1239) | Acc_1: (95.32%) (44046/46208)\n",
      "Epoch: 167 | Batch_idx: 370 |  Loss_1: (0.1242) | Acc_1: (95.31%) (45260/47488)\n",
      "Epoch: 167 | Batch_idx: 380 |  Loss_1: (0.1245) | Acc_1: (95.30%) (46477/48768)\n",
      "Epoch: 167 | Batch_idx: 390 |  Loss_1: (0.1244) | Acc_1: (95.30%) (47648/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3517) | Acc: (92.12%) (9212/10000)\n",
      "Epoch: 168 | Batch_idx: 0 |  Loss_1: (0.0951) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 168 | Batch_idx: 10 |  Loss_1: (0.1286) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 168 | Batch_idx: 20 |  Loss_1: (0.1286) | Acc_1: (94.75%) (2547/2688)\n",
      "Epoch: 168 | Batch_idx: 30 |  Loss_1: (0.1271) | Acc_1: (94.96%) (3768/3968)\n",
      "Epoch: 168 | Batch_idx: 40 |  Loss_1: (0.1256) | Acc_1: (94.97%) (4984/5248)\n",
      "Epoch: 168 | Batch_idx: 50 |  Loss_1: (0.1265) | Acc_1: (94.98%) (6200/6528)\n",
      "Epoch: 168 | Batch_idx: 60 |  Loss_1: (0.1245) | Acc_1: (95.11%) (7426/7808)\n",
      "Epoch: 168 | Batch_idx: 70 |  Loss_1: (0.1252) | Acc_1: (95.15%) (8647/9088)\n",
      "Epoch: 168 | Batch_idx: 80 |  Loss_1: (0.1250) | Acc_1: (95.16%) (9866/10368)\n",
      "Epoch: 168 | Batch_idx: 90 |  Loss_1: (0.1246) | Acc_1: (95.18%) (11086/11648)\n",
      "Epoch: 168 | Batch_idx: 100 |  Loss_1: (0.1266) | Acc_1: (95.06%) (12290/12928)\n",
      "Epoch: 168 | Batch_idx: 110 |  Loss_1: (0.1271) | Acc_1: (95.09%) (13511/14208)\n",
      "Epoch: 168 | Batch_idx: 120 |  Loss_1: (0.1275) | Acc_1: (95.13%) (14733/15488)\n",
      "Epoch: 168 | Batch_idx: 130 |  Loss_1: (0.1274) | Acc_1: (95.14%) (15953/16768)\n",
      "Epoch: 168 | Batch_idx: 140 |  Loss_1: (0.1271) | Acc_1: (95.14%) (17171/18048)\n",
      "Epoch: 168 | Batch_idx: 150 |  Loss_1: (0.1253) | Acc_1: (95.22%) (18404/19328)\n",
      "Epoch: 168 | Batch_idx: 160 |  Loss_1: (0.1261) | Acc_1: (95.20%) (19618/20608)\n",
      "Epoch: 168 | Batch_idx: 170 |  Loss_1: (0.1263) | Acc_1: (95.20%) (20838/21888)\n",
      "Epoch: 168 | Batch_idx: 180 |  Loss_1: (0.1262) | Acc_1: (95.20%) (22056/23168)\n",
      "Epoch: 168 | Batch_idx: 190 |  Loss_1: (0.1266) | Acc_1: (95.19%) (23273/24448)\n",
      "Epoch: 168 | Batch_idx: 200 |  Loss_1: (0.1266) | Acc_1: (95.18%) (24487/25728)\n",
      "Epoch: 168 | Batch_idx: 210 |  Loss_1: (0.1258) | Acc_1: (95.22%) (25716/27008)\n",
      "Epoch: 168 | Batch_idx: 220 |  Loss_1: (0.1254) | Acc_1: (95.23%) (26939/28288)\n",
      "Epoch: 168 | Batch_idx: 230 |  Loss_1: (0.1264) | Acc_1: (95.19%) (28146/29568)\n",
      "Epoch: 168 | Batch_idx: 240 |  Loss_1: (0.1259) | Acc_1: (95.21%) (29369/30848)\n",
      "Epoch: 168 | Batch_idx: 250 |  Loss_1: (0.1265) | Acc_1: (95.21%) (30588/32128)\n",
      "Epoch: 168 | Batch_idx: 260 |  Loss_1: (0.1269) | Acc_1: (95.19%) (31800/33408)\n",
      "Epoch: 168 | Batch_idx: 270 |  Loss_1: (0.1275) | Acc_1: (95.16%) (33008/34688)\n",
      "Epoch: 168 | Batch_idx: 280 |  Loss_1: (0.1278) | Acc_1: (95.14%) (34221/35968)\n",
      "Epoch: 168 | Batch_idx: 290 |  Loss_1: (0.1278) | Acc_1: (95.15%) (35441/37248)\n",
      "Epoch: 168 | Batch_idx: 300 |  Loss_1: (0.1269) | Acc_1: (95.19%) (36674/38528)\n",
      "Epoch: 168 | Batch_idx: 310 |  Loss_1: (0.1276) | Acc_1: (95.15%) (37879/39808)\n",
      "Epoch: 168 | Batch_idx: 320 |  Loss_1: (0.1277) | Acc_1: (95.16%) (39100/41088)\n",
      "Epoch: 168 | Batch_idx: 330 |  Loss_1: (0.1282) | Acc_1: (95.15%) (40312/42368)\n",
      "Epoch: 168 | Batch_idx: 340 |  Loss_1: (0.1287) | Acc_1: (95.12%) (41518/43648)\n",
      "Epoch: 168 | Batch_idx: 350 |  Loss_1: (0.1279) | Acc_1: (95.15%) (42750/44928)\n",
      "Epoch: 168 | Batch_idx: 360 |  Loss_1: (0.1276) | Acc_1: (95.17%) (43977/46208)\n",
      "Epoch: 168 | Batch_idx: 370 |  Loss_1: (0.1275) | Acc_1: (95.18%) (45197/47488)\n",
      "Epoch: 168 | Batch_idx: 380 |  Loss_1: (0.1270) | Acc_1: (95.20%) (46427/48768)\n",
      "Epoch: 168 | Batch_idx: 390 |  Loss_1: (0.1276) | Acc_1: (95.18%) (47589/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3778) | Acc: (91.92%) (9192/10000)\n",
      "Epoch: 169 | Batch_idx: 0 |  Loss_1: (0.1544) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 169 | Batch_idx: 10 |  Loss_1: (0.1213) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 169 | Batch_idx: 20 |  Loss_1: (0.1187) | Acc_1: (95.42%) (2565/2688)\n",
      "Epoch: 169 | Batch_idx: 30 |  Loss_1: (0.1265) | Acc_1: (95.24%) (3779/3968)\n",
      "Epoch: 169 | Batch_idx: 40 |  Loss_1: (0.1243) | Acc_1: (95.37%) (5005/5248)\n",
      "Epoch: 169 | Batch_idx: 50 |  Loss_1: (0.1263) | Acc_1: (95.27%) (6219/6528)\n",
      "Epoch: 169 | Batch_idx: 60 |  Loss_1: (0.1274) | Acc_1: (95.16%) (7430/7808)\n",
      "Epoch: 169 | Batch_idx: 70 |  Loss_1: (0.1286) | Acc_1: (95.19%) (8651/9088)\n",
      "Epoch: 169 | Batch_idx: 80 |  Loss_1: (0.1286) | Acc_1: (95.20%) (9870/10368)\n",
      "Epoch: 169 | Batch_idx: 90 |  Loss_1: (0.1275) | Acc_1: (95.24%) (11094/11648)\n",
      "Epoch: 169 | Batch_idx: 100 |  Loss_1: (0.1270) | Acc_1: (95.27%) (12316/12928)\n",
      "Epoch: 169 | Batch_idx: 110 |  Loss_1: (0.1247) | Acc_1: (95.33%) (13544/14208)\n",
      "Epoch: 169 | Batch_idx: 120 |  Loss_1: (0.1248) | Acc_1: (95.32%) (14763/15488)\n",
      "Epoch: 169 | Batch_idx: 130 |  Loss_1: (0.1247) | Acc_1: (95.33%) (15985/16768)\n",
      "Epoch: 169 | Batch_idx: 140 |  Loss_1: (0.1243) | Acc_1: (95.36%) (17210/18048)\n",
      "Epoch: 169 | Batch_idx: 150 |  Loss_1: (0.1247) | Acc_1: (95.34%) (18428/19328)\n",
      "Epoch: 169 | Batch_idx: 160 |  Loss_1: (0.1231) | Acc_1: (95.39%) (19658/20608)\n",
      "Epoch: 169 | Batch_idx: 170 |  Loss_1: (0.1223) | Acc_1: (95.40%) (20882/21888)\n",
      "Epoch: 169 | Batch_idx: 180 |  Loss_1: (0.1231) | Acc_1: (95.37%) (22096/23168)\n",
      "Epoch: 169 | Batch_idx: 190 |  Loss_1: (0.1248) | Acc_1: (95.33%) (23306/24448)\n",
      "Epoch: 169 | Batch_idx: 200 |  Loss_1: (0.1254) | Acc_1: (95.30%) (24518/25728)\n",
      "Epoch: 169 | Batch_idx: 210 |  Loss_1: (0.1241) | Acc_1: (95.33%) (25747/27008)\n",
      "Epoch: 169 | Batch_idx: 220 |  Loss_1: (0.1240) | Acc_1: (95.34%) (26971/28288)\n",
      "Epoch: 169 | Batch_idx: 230 |  Loss_1: (0.1252) | Acc_1: (95.32%) (28185/29568)\n",
      "Epoch: 169 | Batch_idx: 240 |  Loss_1: (0.1250) | Acc_1: (95.34%) (29409/30848)\n",
      "Epoch: 169 | Batch_idx: 250 |  Loss_1: (0.1264) | Acc_1: (95.26%) (30606/32128)\n",
      "Epoch: 169 | Batch_idx: 260 |  Loss_1: (0.1261) | Acc_1: (95.28%) (31830/33408)\n",
      "Epoch: 169 | Batch_idx: 270 |  Loss_1: (0.1266) | Acc_1: (95.25%) (33040/34688)\n",
      "Epoch: 169 | Batch_idx: 280 |  Loss_1: (0.1274) | Acc_1: (95.22%) (34247/35968)\n",
      "Epoch: 169 | Batch_idx: 290 |  Loss_1: (0.1263) | Acc_1: (95.26%) (35484/37248)\n",
      "Epoch: 169 | Batch_idx: 300 |  Loss_1: (0.1262) | Acc_1: (95.25%) (36697/38528)\n",
      "Epoch: 169 | Batch_idx: 310 |  Loss_1: (0.1257) | Acc_1: (95.26%) (37923/39808)\n",
      "Epoch: 169 | Batch_idx: 320 |  Loss_1: (0.1251) | Acc_1: (95.28%) (39148/41088)\n",
      "Epoch: 169 | Batch_idx: 330 |  Loss_1: (0.1247) | Acc_1: (95.30%) (40375/42368)\n",
      "Epoch: 169 | Batch_idx: 340 |  Loss_1: (0.1250) | Acc_1: (95.29%) (41591/43648)\n",
      "Epoch: 169 | Batch_idx: 350 |  Loss_1: (0.1253) | Acc_1: (95.27%) (42803/44928)\n",
      "Epoch: 169 | Batch_idx: 360 |  Loss_1: (0.1250) | Acc_1: (95.28%) (44029/46208)\n",
      "Epoch: 169 | Batch_idx: 370 |  Loss_1: (0.1257) | Acc_1: (95.26%) (45236/47488)\n",
      "Epoch: 169 | Batch_idx: 380 |  Loss_1: (0.1259) | Acc_1: (95.26%) (46457/48768)\n",
      "Epoch: 169 | Batch_idx: 390 |  Loss_1: (0.1255) | Acc_1: (95.27%) (47635/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3527) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 170 | Batch_idx: 0 |  Loss_1: (0.1980) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 170 | Batch_idx: 10 |  Loss_1: (0.1269) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 170 | Batch_idx: 20 |  Loss_1: (0.1264) | Acc_1: (94.90%) (2551/2688)\n",
      "Epoch: 170 | Batch_idx: 30 |  Loss_1: (0.1174) | Acc_1: (95.29%) (3781/3968)\n",
      "Epoch: 170 | Batch_idx: 40 |  Loss_1: (0.1185) | Acc_1: (95.43%) (5008/5248)\n",
      "Epoch: 170 | Batch_idx: 50 |  Loss_1: (0.1187) | Acc_1: (95.40%) (6228/6528)\n",
      "Epoch: 170 | Batch_idx: 60 |  Loss_1: (0.1221) | Acc_1: (95.31%) (7442/7808)\n",
      "Epoch: 170 | Batch_idx: 70 |  Loss_1: (0.1210) | Acc_1: (95.39%) (8669/9088)\n",
      "Epoch: 170 | Batch_idx: 80 |  Loss_1: (0.1210) | Acc_1: (95.41%) (9892/10368)\n",
      "Epoch: 170 | Batch_idx: 90 |  Loss_1: (0.1195) | Acc_1: (95.48%) (11121/11648)\n",
      "Epoch: 170 | Batch_idx: 100 |  Loss_1: (0.1211) | Acc_1: (95.41%) (12335/12928)\n",
      "Epoch: 170 | Batch_idx: 110 |  Loss_1: (0.1241) | Acc_1: (95.30%) (13540/14208)\n",
      "Epoch: 170 | Batch_idx: 120 |  Loss_1: (0.1238) | Acc_1: (95.33%) (14765/15488)\n",
      "Epoch: 170 | Batch_idx: 130 |  Loss_1: (0.1235) | Acc_1: (95.31%) (15982/16768)\n",
      "Epoch: 170 | Batch_idx: 140 |  Loss_1: (0.1219) | Acc_1: (95.36%) (17210/18048)\n",
      "Epoch: 170 | Batch_idx: 150 |  Loss_1: (0.1211) | Acc_1: (95.40%) (18438/19328)\n",
      "Epoch: 170 | Batch_idx: 160 |  Loss_1: (0.1219) | Acc_1: (95.37%) (19654/20608)\n",
      "Epoch: 170 | Batch_idx: 170 |  Loss_1: (0.1212) | Acc_1: (95.41%) (20883/21888)\n",
      "Epoch: 170 | Batch_idx: 180 |  Loss_1: (0.1223) | Acc_1: (95.36%) (22092/23168)\n",
      "Epoch: 170 | Batch_idx: 190 |  Loss_1: (0.1231) | Acc_1: (95.34%) (23309/24448)\n",
      "Epoch: 170 | Batch_idx: 200 |  Loss_1: (0.1245) | Acc_1: (95.30%) (24518/25728)\n",
      "Epoch: 170 | Batch_idx: 210 |  Loss_1: (0.1240) | Acc_1: (95.31%) (25741/27008)\n",
      "Epoch: 170 | Batch_idx: 220 |  Loss_1: (0.1235) | Acc_1: (95.33%) (26967/28288)\n",
      "Epoch: 170 | Batch_idx: 230 |  Loss_1: (0.1235) | Acc_1: (95.33%) (28188/29568)\n",
      "Epoch: 170 | Batch_idx: 240 |  Loss_1: (0.1231) | Acc_1: (95.34%) (29410/30848)\n",
      "Epoch: 170 | Batch_idx: 250 |  Loss_1: (0.1237) | Acc_1: (95.32%) (30626/32128)\n",
      "Epoch: 170 | Batch_idx: 260 |  Loss_1: (0.1240) | Acc_1: (95.32%) (31846/33408)\n",
      "Epoch: 170 | Batch_idx: 270 |  Loss_1: (0.1246) | Acc_1: (95.31%) (33060/34688)\n",
      "Epoch: 170 | Batch_idx: 280 |  Loss_1: (0.1254) | Acc_1: (95.28%) (34271/35968)\n",
      "Epoch: 170 | Batch_idx: 290 |  Loss_1: (0.1251) | Acc_1: (95.30%) (35498/37248)\n",
      "Epoch: 170 | Batch_idx: 300 |  Loss_1: (0.1250) | Acc_1: (95.30%) (36719/38528)\n",
      "Epoch: 170 | Batch_idx: 310 |  Loss_1: (0.1244) | Acc_1: (95.34%) (37953/39808)\n",
      "Epoch: 170 | Batch_idx: 320 |  Loss_1: (0.1241) | Acc_1: (95.35%) (39178/41088)\n",
      "Epoch: 170 | Batch_idx: 330 |  Loss_1: (0.1240) | Acc_1: (95.35%) (40397/42368)\n",
      "Epoch: 170 | Batch_idx: 340 |  Loss_1: (0.1240) | Acc_1: (95.34%) (41614/43648)\n",
      "Epoch: 170 | Batch_idx: 350 |  Loss_1: (0.1235) | Acc_1: (95.34%) (42836/44928)\n",
      "Epoch: 170 | Batch_idx: 360 |  Loss_1: (0.1234) | Acc_1: (95.35%) (44059/46208)\n",
      "Epoch: 170 | Batch_idx: 370 |  Loss_1: (0.1234) | Acc_1: (95.35%) (45280/47488)\n",
      "Epoch: 170 | Batch_idx: 380 |  Loss_1: (0.1240) | Acc_1: (95.33%) (46490/48768)\n",
      "Epoch: 170 | Batch_idx: 390 |  Loss_1: (0.1240) | Acc_1: (95.32%) (47661/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3537) | Acc: (92.30%) (9230/10000)\n",
      "Epoch: 171 | Batch_idx: 0 |  Loss_1: (0.1631) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 171 | Batch_idx: 10 |  Loss_1: (0.1002) | Acc_1: (96.38%) (1357/1408)\n",
      "Epoch: 171 | Batch_idx: 20 |  Loss_1: (0.1152) | Acc_1: (95.57%) (2569/2688)\n",
      "Epoch: 171 | Batch_idx: 30 |  Loss_1: (0.1143) | Acc_1: (95.72%) (3798/3968)\n",
      "Epoch: 171 | Batch_idx: 40 |  Loss_1: (0.1184) | Acc_1: (95.60%) (5017/5248)\n",
      "Epoch: 171 | Batch_idx: 50 |  Loss_1: (0.1218) | Acc_1: (95.50%) (6234/6528)\n",
      "Epoch: 171 | Batch_idx: 60 |  Loss_1: (0.1247) | Acc_1: (95.36%) (7446/7808)\n",
      "Epoch: 171 | Batch_idx: 70 |  Loss_1: (0.1220) | Acc_1: (95.48%) (8677/9088)\n",
      "Epoch: 171 | Batch_idx: 80 |  Loss_1: (0.1183) | Acc_1: (95.60%) (9912/10368)\n",
      "Epoch: 171 | Batch_idx: 90 |  Loss_1: (0.1164) | Acc_1: (95.67%) (11144/11648)\n",
      "Epoch: 171 | Batch_idx: 100 |  Loss_1: (0.1171) | Acc_1: (95.65%) (12365/12928)\n",
      "Epoch: 171 | Batch_idx: 110 |  Loss_1: (0.1173) | Acc_1: (95.63%) (13587/14208)\n",
      "Epoch: 171 | Batch_idx: 120 |  Loss_1: (0.1185) | Acc_1: (95.58%) (14804/15488)\n",
      "Epoch: 171 | Batch_idx: 130 |  Loss_1: (0.1212) | Acc_1: (95.48%) (16010/16768)\n",
      "Epoch: 171 | Batch_idx: 140 |  Loss_1: (0.1234) | Acc_1: (95.40%) (17218/18048)\n",
      "Epoch: 171 | Batch_idx: 150 |  Loss_1: (0.1222) | Acc_1: (95.45%) (18449/19328)\n",
      "Epoch: 171 | Batch_idx: 160 |  Loss_1: (0.1224) | Acc_1: (95.48%) (19676/20608)\n",
      "Epoch: 171 | Batch_idx: 170 |  Loss_1: (0.1227) | Acc_1: (95.46%) (20894/21888)\n",
      "Epoch: 171 | Batch_idx: 180 |  Loss_1: (0.1230) | Acc_1: (95.43%) (22110/23168)\n",
      "Epoch: 171 | Batch_idx: 190 |  Loss_1: (0.1232) | Acc_1: (95.42%) (23329/24448)\n",
      "Epoch: 171 | Batch_idx: 200 |  Loss_1: (0.1231) | Acc_1: (95.42%) (24549/25728)\n",
      "Epoch: 171 | Batch_idx: 210 |  Loss_1: (0.1238) | Acc_1: (95.41%) (25769/27008)\n",
      "Epoch: 171 | Batch_idx: 220 |  Loss_1: (0.1236) | Acc_1: (95.40%) (26988/28288)\n",
      "Epoch: 171 | Batch_idx: 230 |  Loss_1: (0.1234) | Acc_1: (95.40%) (28207/29568)\n",
      "Epoch: 171 | Batch_idx: 240 |  Loss_1: (0.1224) | Acc_1: (95.43%) (29438/30848)\n",
      "Epoch: 171 | Batch_idx: 250 |  Loss_1: (0.1221) | Acc_1: (95.43%) (30661/32128)\n",
      "Epoch: 171 | Batch_idx: 260 |  Loss_1: (0.1226) | Acc_1: (95.41%) (31876/33408)\n",
      "Epoch: 171 | Batch_idx: 270 |  Loss_1: (0.1217) | Acc_1: (95.45%) (33111/34688)\n",
      "Epoch: 171 | Batch_idx: 280 |  Loss_1: (0.1214) | Acc_1: (95.45%) (34332/35968)\n",
      "Epoch: 171 | Batch_idx: 290 |  Loss_1: (0.1217) | Acc_1: (95.43%) (35546/37248)\n",
      "Epoch: 171 | Batch_idx: 300 |  Loss_1: (0.1213) | Acc_1: (95.46%) (36777/38528)\n",
      "Epoch: 171 | Batch_idx: 310 |  Loss_1: (0.1209) | Acc_1: (95.48%) (38010/39808)\n",
      "Epoch: 171 | Batch_idx: 320 |  Loss_1: (0.1203) | Acc_1: (95.50%) (39239/41088)\n",
      "Epoch: 171 | Batch_idx: 330 |  Loss_1: (0.1211) | Acc_1: (95.47%) (40449/42368)\n",
      "Epoch: 171 | Batch_idx: 340 |  Loss_1: (0.1212) | Acc_1: (95.47%) (41669/43648)\n",
      "Epoch: 171 | Batch_idx: 350 |  Loss_1: (0.1219) | Acc_1: (95.44%) (42879/44928)\n",
      "Epoch: 171 | Batch_idx: 360 |  Loss_1: (0.1221) | Acc_1: (95.42%) (44093/46208)\n",
      "Epoch: 171 | Batch_idx: 370 |  Loss_1: (0.1222) | Acc_1: (95.42%) (45313/47488)\n",
      "Epoch: 171 | Batch_idx: 380 |  Loss_1: (0.1225) | Acc_1: (95.42%) (46532/48768)\n",
      "Epoch: 171 | Batch_idx: 390 |  Loss_1: (0.1225) | Acc_1: (95.43%) (47714/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3512) | Acc: (92.30%) (9230/10000)\n",
      "Epoch: 172 | Batch_idx: 0 |  Loss_1: (0.1913) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 172 | Batch_idx: 10 |  Loss_1: (0.1360) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 172 | Batch_idx: 20 |  Loss_1: (0.1279) | Acc_1: (95.05%) (2555/2688)\n",
      "Epoch: 172 | Batch_idx: 30 |  Loss_1: (0.1273) | Acc_1: (95.21%) (3778/3968)\n",
      "Epoch: 172 | Batch_idx: 40 |  Loss_1: (0.1316) | Acc_1: (95.08%) (4990/5248)\n",
      "Epoch: 172 | Batch_idx: 50 |  Loss_1: (0.1292) | Acc_1: (95.24%) (6217/6528)\n",
      "Epoch: 172 | Batch_idx: 60 |  Loss_1: (0.1297) | Acc_1: (95.22%) (7435/7808)\n",
      "Epoch: 172 | Batch_idx: 70 |  Loss_1: (0.1262) | Acc_1: (95.31%) (8662/9088)\n",
      "Epoch: 172 | Batch_idx: 80 |  Loss_1: (0.1265) | Acc_1: (95.29%) (9880/10368)\n",
      "Epoch: 172 | Batch_idx: 90 |  Loss_1: (0.1258) | Acc_1: (95.29%) (11099/11648)\n",
      "Epoch: 172 | Batch_idx: 100 |  Loss_1: (0.1245) | Acc_1: (95.36%) (12328/12928)\n",
      "Epoch: 172 | Batch_idx: 110 |  Loss_1: (0.1245) | Acc_1: (95.34%) (13546/14208)\n",
      "Epoch: 172 | Batch_idx: 120 |  Loss_1: (0.1252) | Acc_1: (95.30%) (14760/15488)\n",
      "Epoch: 172 | Batch_idx: 130 |  Loss_1: (0.1241) | Acc_1: (95.34%) (15986/16768)\n",
      "Epoch: 172 | Batch_idx: 140 |  Loss_1: (0.1254) | Acc_1: (95.30%) (17199/18048)\n",
      "Epoch: 172 | Batch_idx: 150 |  Loss_1: (0.1259) | Acc_1: (95.25%) (18409/19328)\n",
      "Epoch: 172 | Batch_idx: 160 |  Loss_1: (0.1259) | Acc_1: (95.22%) (19623/20608)\n",
      "Epoch: 172 | Batch_idx: 170 |  Loss_1: (0.1258) | Acc_1: (95.22%) (20842/21888)\n",
      "Epoch: 172 | Batch_idx: 180 |  Loss_1: (0.1262) | Acc_1: (95.21%) (22058/23168)\n",
      "Epoch: 172 | Batch_idx: 190 |  Loss_1: (0.1262) | Acc_1: (95.21%) (23278/24448)\n",
      "Epoch: 172 | Batch_idx: 200 |  Loss_1: (0.1260) | Acc_1: (95.23%) (24502/25728)\n",
      "Epoch: 172 | Batch_idx: 210 |  Loss_1: (0.1267) | Acc_1: (95.21%) (25714/27008)\n",
      "Epoch: 172 | Batch_idx: 220 |  Loss_1: (0.1268) | Acc_1: (95.20%) (26931/28288)\n",
      "Epoch: 172 | Batch_idx: 230 |  Loss_1: (0.1274) | Acc_1: (95.19%) (28147/29568)\n",
      "Epoch: 172 | Batch_idx: 240 |  Loss_1: (0.1278) | Acc_1: (95.17%) (29359/30848)\n",
      "Epoch: 172 | Batch_idx: 250 |  Loss_1: (0.1280) | Acc_1: (95.16%) (30573/32128)\n",
      "Epoch: 172 | Batch_idx: 260 |  Loss_1: (0.1279) | Acc_1: (95.15%) (31788/33408)\n",
      "Epoch: 172 | Batch_idx: 270 |  Loss_1: (0.1282) | Acc_1: (95.15%) (33004/34688)\n",
      "Epoch: 172 | Batch_idx: 280 |  Loss_1: (0.1279) | Acc_1: (95.15%) (34223/35968)\n",
      "Epoch: 172 | Batch_idx: 290 |  Loss_1: (0.1281) | Acc_1: (95.15%) (35440/37248)\n",
      "Epoch: 172 | Batch_idx: 300 |  Loss_1: (0.1288) | Acc_1: (95.11%) (36645/38528)\n",
      "Epoch: 172 | Batch_idx: 310 |  Loss_1: (0.1277) | Acc_1: (95.16%) (37882/39808)\n",
      "Epoch: 172 | Batch_idx: 320 |  Loss_1: (0.1278) | Acc_1: (95.16%) (39100/41088)\n",
      "Epoch: 172 | Batch_idx: 330 |  Loss_1: (0.1282) | Acc_1: (95.15%) (40314/42368)\n",
      "Epoch: 172 | Batch_idx: 340 |  Loss_1: (0.1286) | Acc_1: (95.13%) (41524/43648)\n",
      "Epoch: 172 | Batch_idx: 350 |  Loss_1: (0.1279) | Acc_1: (95.16%) (42754/44928)\n",
      "Epoch: 172 | Batch_idx: 360 |  Loss_1: (0.1287) | Acc_1: (95.12%) (43954/46208)\n",
      "Epoch: 172 | Batch_idx: 370 |  Loss_1: (0.1284) | Acc_1: (95.13%) (45173/47488)\n",
      "Epoch: 172 | Batch_idx: 380 |  Loss_1: (0.1275) | Acc_1: (95.16%) (46408/48768)\n",
      "Epoch: 172 | Batch_idx: 390 |  Loss_1: (0.1272) | Acc_1: (95.17%) (47583/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3445) | Acc: (92.18%) (9218/10000)\n",
      "Epoch: 173 | Batch_idx: 0 |  Loss_1: (0.0627) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 173 | Batch_idx: 10 |  Loss_1: (0.1116) | Acc_1: (96.09%) (1353/1408)\n",
      "Epoch: 173 | Batch_idx: 20 |  Loss_1: (0.1115) | Acc_1: (95.98%) (2580/2688)\n",
      "Epoch: 173 | Batch_idx: 30 |  Loss_1: (0.1156) | Acc_1: (95.72%) (3798/3968)\n",
      "Epoch: 173 | Batch_idx: 40 |  Loss_1: (0.1172) | Acc_1: (95.66%) (5020/5248)\n",
      "Epoch: 173 | Batch_idx: 50 |  Loss_1: (0.1196) | Acc_1: (95.56%) (6238/6528)\n",
      "Epoch: 173 | Batch_idx: 60 |  Loss_1: (0.1159) | Acc_1: (95.71%) (7473/7808)\n",
      "Epoch: 173 | Batch_idx: 70 |  Loss_1: (0.1197) | Acc_1: (95.60%) (8688/9088)\n",
      "Epoch: 173 | Batch_idx: 80 |  Loss_1: (0.1206) | Acc_1: (95.55%) (9907/10368)\n",
      "Epoch: 173 | Batch_idx: 90 |  Loss_1: (0.1211) | Acc_1: (95.49%) (11123/11648)\n",
      "Epoch: 173 | Batch_idx: 100 |  Loss_1: (0.1237) | Acc_1: (95.44%) (12339/12928)\n",
      "Epoch: 173 | Batch_idx: 110 |  Loss_1: (0.1246) | Acc_1: (95.40%) (13555/14208)\n",
      "Epoch: 173 | Batch_idx: 120 |  Loss_1: (0.1228) | Acc_1: (95.49%) (14790/15488)\n",
      "Epoch: 173 | Batch_idx: 130 |  Loss_1: (0.1211) | Acc_1: (95.56%) (16024/16768)\n",
      "Epoch: 173 | Batch_idx: 140 |  Loss_1: (0.1209) | Acc_1: (95.57%) (17248/18048)\n",
      "Epoch: 173 | Batch_idx: 150 |  Loss_1: (0.1217) | Acc_1: (95.51%) (18461/19328)\n",
      "Epoch: 173 | Batch_idx: 160 |  Loss_1: (0.1193) | Acc_1: (95.60%) (19701/20608)\n",
      "Epoch: 173 | Batch_idx: 170 |  Loss_1: (0.1192) | Acc_1: (95.61%) (20927/21888)\n",
      "Epoch: 173 | Batch_idx: 180 |  Loss_1: (0.1197) | Acc_1: (95.60%) (22148/23168)\n",
      "Epoch: 173 | Batch_idx: 190 |  Loss_1: (0.1204) | Acc_1: (95.57%) (23364/24448)\n",
      "Epoch: 173 | Batch_idx: 200 |  Loss_1: (0.1208) | Acc_1: (95.56%) (24585/25728)\n",
      "Epoch: 173 | Batch_idx: 210 |  Loss_1: (0.1200) | Acc_1: (95.56%) (25810/27008)\n",
      "Epoch: 173 | Batch_idx: 220 |  Loss_1: (0.1198) | Acc_1: (95.57%) (27035/28288)\n",
      "Epoch: 173 | Batch_idx: 230 |  Loss_1: (0.1213) | Acc_1: (95.52%) (28243/29568)\n",
      "Epoch: 173 | Batch_idx: 240 |  Loss_1: (0.1211) | Acc_1: (95.52%) (29466/30848)\n",
      "Epoch: 173 | Batch_idx: 250 |  Loss_1: (0.1202) | Acc_1: (95.54%) (30695/32128)\n",
      "Epoch: 173 | Batch_idx: 260 |  Loss_1: (0.1207) | Acc_1: (95.53%) (31913/33408)\n",
      "Epoch: 173 | Batch_idx: 270 |  Loss_1: (0.1212) | Acc_1: (95.50%) (33128/34688)\n",
      "Epoch: 173 | Batch_idx: 280 |  Loss_1: (0.1215) | Acc_1: (95.49%) (34346/35968)\n",
      "Epoch: 173 | Batch_idx: 290 |  Loss_1: (0.1229) | Acc_1: (95.46%) (35556/37248)\n",
      "Epoch: 173 | Batch_idx: 300 |  Loss_1: (0.1231) | Acc_1: (95.43%) (36769/38528)\n",
      "Epoch: 173 | Batch_idx: 310 |  Loss_1: (0.1233) | Acc_1: (95.42%) (37986/39808)\n",
      "Epoch: 173 | Batch_idx: 320 |  Loss_1: (0.1230) | Acc_1: (95.42%) (39206/41088)\n",
      "Epoch: 173 | Batch_idx: 330 |  Loss_1: (0.1233) | Acc_1: (95.41%) (40423/42368)\n",
      "Epoch: 173 | Batch_idx: 340 |  Loss_1: (0.1227) | Acc_1: (95.44%) (41657/43648)\n",
      "Epoch: 173 | Batch_idx: 350 |  Loss_1: (0.1230) | Acc_1: (95.43%) (42873/44928)\n",
      "Epoch: 173 | Batch_idx: 360 |  Loss_1: (0.1233) | Acc_1: (95.41%) (44089/46208)\n",
      "Epoch: 173 | Batch_idx: 370 |  Loss_1: (0.1231) | Acc_1: (95.42%) (45313/47488)\n",
      "Epoch: 173 | Batch_idx: 380 |  Loss_1: (0.1229) | Acc_1: (95.43%) (46538/48768)\n",
      "Epoch: 173 | Batch_idx: 390 |  Loss_1: (0.1225) | Acc_1: (95.44%) (47719/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3601) | Acc: (92.24%) (9224/10000)\n",
      "Epoch: 174 | Batch_idx: 0 |  Loss_1: (0.2092) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 174 | Batch_idx: 10 |  Loss_1: (0.1423) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 174 | Batch_idx: 20 |  Loss_1: (0.1312) | Acc_1: (95.01%) (2554/2688)\n",
      "Epoch: 174 | Batch_idx: 30 |  Loss_1: (0.1269) | Acc_1: (95.19%) (3777/3968)\n",
      "Epoch: 174 | Batch_idx: 40 |  Loss_1: (0.1266) | Acc_1: (95.29%) (5001/5248)\n",
      "Epoch: 174 | Batch_idx: 50 |  Loss_1: (0.1218) | Acc_1: (95.42%) (6229/6528)\n",
      "Epoch: 174 | Batch_idx: 60 |  Loss_1: (0.1229) | Acc_1: (95.40%) (7449/7808)\n",
      "Epoch: 174 | Batch_idx: 70 |  Loss_1: (0.1217) | Acc_1: (95.43%) (8673/9088)\n",
      "Epoch: 174 | Batch_idx: 80 |  Loss_1: (0.1203) | Acc_1: (95.54%) (9906/10368)\n",
      "Epoch: 174 | Batch_idx: 90 |  Loss_1: (0.1219) | Acc_1: (95.51%) (11125/11648)\n",
      "Epoch: 174 | Batch_idx: 100 |  Loss_1: (0.1228) | Acc_1: (95.43%) (12337/12928)\n",
      "Epoch: 174 | Batch_idx: 110 |  Loss_1: (0.1227) | Acc_1: (95.40%) (13555/14208)\n",
      "Epoch: 174 | Batch_idx: 120 |  Loss_1: (0.1217) | Acc_1: (95.46%) (14785/15488)\n",
      "Epoch: 174 | Batch_idx: 130 |  Loss_1: (0.1203) | Acc_1: (95.53%) (16019/16768)\n",
      "Epoch: 174 | Batch_idx: 140 |  Loss_1: (0.1198) | Acc_1: (95.54%) (17243/18048)\n",
      "Epoch: 174 | Batch_idx: 150 |  Loss_1: (0.1197) | Acc_1: (95.54%) (18466/19328)\n",
      "Epoch: 174 | Batch_idx: 160 |  Loss_1: (0.1189) | Acc_1: (95.56%) (19694/20608)\n",
      "Epoch: 174 | Batch_idx: 170 |  Loss_1: (0.1186) | Acc_1: (95.56%) (20917/21888)\n",
      "Epoch: 174 | Batch_idx: 180 |  Loss_1: (0.1182) | Acc_1: (95.59%) (22146/23168)\n",
      "Epoch: 174 | Batch_idx: 190 |  Loss_1: (0.1183) | Acc_1: (95.59%) (23370/24448)\n",
      "Epoch: 174 | Batch_idx: 200 |  Loss_1: (0.1184) | Acc_1: (95.60%) (24597/25728)\n",
      "Epoch: 174 | Batch_idx: 210 |  Loss_1: (0.1183) | Acc_1: (95.61%) (25821/27008)\n",
      "Epoch: 174 | Batch_idx: 220 |  Loss_1: (0.1184) | Acc_1: (95.62%) (27048/28288)\n",
      "Epoch: 174 | Batch_idx: 230 |  Loss_1: (0.1182) | Acc_1: (95.62%) (28272/29568)\n",
      "Epoch: 174 | Batch_idx: 240 |  Loss_1: (0.1181) | Acc_1: (95.62%) (29496/30848)\n",
      "Epoch: 174 | Batch_idx: 250 |  Loss_1: (0.1182) | Acc_1: (95.62%) (30720/32128)\n",
      "Epoch: 174 | Batch_idx: 260 |  Loss_1: (0.1191) | Acc_1: (95.58%) (31933/33408)\n",
      "Epoch: 174 | Batch_idx: 270 |  Loss_1: (0.1195) | Acc_1: (95.55%) (33145/34688)\n",
      "Epoch: 174 | Batch_idx: 280 |  Loss_1: (0.1202) | Acc_1: (95.52%) (34358/35968)\n",
      "Epoch: 174 | Batch_idx: 290 |  Loss_1: (0.1202) | Acc_1: (95.54%) (35585/37248)\n",
      "Epoch: 174 | Batch_idx: 300 |  Loss_1: (0.1201) | Acc_1: (95.53%) (36805/38528)\n",
      "Epoch: 174 | Batch_idx: 310 |  Loss_1: (0.1199) | Acc_1: (95.53%) (38030/39808)\n",
      "Epoch: 174 | Batch_idx: 320 |  Loss_1: (0.1200) | Acc_1: (95.53%) (39252/41088)\n",
      "Epoch: 174 | Batch_idx: 330 |  Loss_1: (0.1209) | Acc_1: (95.51%) (40466/42368)\n",
      "Epoch: 174 | Batch_idx: 340 |  Loss_1: (0.1211) | Acc_1: (95.49%) (41681/43648)\n",
      "Epoch: 174 | Batch_idx: 350 |  Loss_1: (0.1210) | Acc_1: (95.52%) (42913/44928)\n",
      "Epoch: 174 | Batch_idx: 360 |  Loss_1: (0.1212) | Acc_1: (95.50%) (44129/46208)\n",
      "Epoch: 174 | Batch_idx: 370 |  Loss_1: (0.1207) | Acc_1: (95.52%) (45360/47488)\n",
      "Epoch: 174 | Batch_idx: 380 |  Loss_1: (0.1213) | Acc_1: (95.49%) (46567/48768)\n",
      "Epoch: 174 | Batch_idx: 390 |  Loss_1: (0.1213) | Acc_1: (95.49%) (47744/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3505) | Acc: (92.33%) (9233/10000)\n",
      "Epoch: 175 | Batch_idx: 0 |  Loss_1: (0.0579) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 175 | Batch_idx: 10 |  Loss_1: (0.1046) | Acc_1: (96.02%) (1352/1408)\n",
      "Epoch: 175 | Batch_idx: 20 |  Loss_1: (0.1258) | Acc_1: (95.05%) (2555/2688)\n",
      "Epoch: 175 | Batch_idx: 30 |  Loss_1: (0.1272) | Acc_1: (95.14%) (3775/3968)\n",
      "Epoch: 175 | Batch_idx: 40 |  Loss_1: (0.1289) | Acc_1: (95.16%) (4994/5248)\n",
      "Epoch: 175 | Batch_idx: 50 |  Loss_1: (0.1259) | Acc_1: (95.24%) (6217/6528)\n",
      "Epoch: 175 | Batch_idx: 60 |  Loss_1: (0.1266) | Acc_1: (95.16%) (7430/7808)\n",
      "Epoch: 175 | Batch_idx: 70 |  Loss_1: (0.1236) | Acc_1: (95.30%) (8661/9088)\n",
      "Epoch: 175 | Batch_idx: 80 |  Loss_1: (0.1284) | Acc_1: (95.14%) (9864/10368)\n",
      "Epoch: 175 | Batch_idx: 90 |  Loss_1: (0.1280) | Acc_1: (95.12%) (11080/11648)\n",
      "Epoch: 175 | Batch_idx: 100 |  Loss_1: (0.1272) | Acc_1: (95.17%) (12304/12928)\n",
      "Epoch: 175 | Batch_idx: 110 |  Loss_1: (0.1278) | Acc_1: (95.14%) (13517/14208)\n",
      "Epoch: 175 | Batch_idx: 120 |  Loss_1: (0.1285) | Acc_1: (95.11%) (14730/15488)\n",
      "Epoch: 175 | Batch_idx: 130 |  Loss_1: (0.1278) | Acc_1: (95.13%) (15952/16768)\n",
      "Epoch: 175 | Batch_idx: 140 |  Loss_1: (0.1277) | Acc_1: (95.14%) (17170/18048)\n",
      "Epoch: 175 | Batch_idx: 150 |  Loss_1: (0.1277) | Acc_1: (95.15%) (18391/19328)\n",
      "Epoch: 175 | Batch_idx: 160 |  Loss_1: (0.1263) | Acc_1: (95.20%) (19619/20608)\n",
      "Epoch: 175 | Batch_idx: 170 |  Loss_1: (0.1262) | Acc_1: (95.22%) (20841/21888)\n",
      "Epoch: 175 | Batch_idx: 180 |  Loss_1: (0.1263) | Acc_1: (95.24%) (22065/23168)\n",
      "Epoch: 175 | Batch_idx: 190 |  Loss_1: (0.1259) | Acc_1: (95.26%) (23288/24448)\n",
      "Epoch: 175 | Batch_idx: 200 |  Loss_1: (0.1257) | Acc_1: (95.25%) (24505/25728)\n",
      "Epoch: 175 | Batch_idx: 210 |  Loss_1: (0.1256) | Acc_1: (95.24%) (25723/27008)\n",
      "Epoch: 175 | Batch_idx: 220 |  Loss_1: (0.1262) | Acc_1: (95.20%) (26929/28288)\n",
      "Epoch: 175 | Batch_idx: 230 |  Loss_1: (0.1251) | Acc_1: (95.24%) (28162/29568)\n",
      "Epoch: 175 | Batch_idx: 240 |  Loss_1: (0.1244) | Acc_1: (95.26%) (29386/30848)\n",
      "Epoch: 175 | Batch_idx: 250 |  Loss_1: (0.1245) | Acc_1: (95.24%) (30600/32128)\n",
      "Epoch: 175 | Batch_idx: 260 |  Loss_1: (0.1246) | Acc_1: (95.25%) (31822/33408)\n",
      "Epoch: 175 | Batch_idx: 270 |  Loss_1: (0.1246) | Acc_1: (95.25%) (33042/34688)\n",
      "Epoch: 175 | Batch_idx: 280 |  Loss_1: (0.1253) | Acc_1: (95.23%) (34254/35968)\n",
      "Epoch: 175 | Batch_idx: 290 |  Loss_1: (0.1250) | Acc_1: (95.23%) (35472/37248)\n",
      "Epoch: 175 | Batch_idx: 300 |  Loss_1: (0.1242) | Acc_1: (95.26%) (36700/38528)\n",
      "Epoch: 175 | Batch_idx: 310 |  Loss_1: (0.1238) | Acc_1: (95.27%) (37924/39808)\n",
      "Epoch: 175 | Batch_idx: 320 |  Loss_1: (0.1236) | Acc_1: (95.27%) (39146/41088)\n",
      "Epoch: 175 | Batch_idx: 330 |  Loss_1: (0.1233) | Acc_1: (95.28%) (40368/42368)\n",
      "Epoch: 175 | Batch_idx: 340 |  Loss_1: (0.1235) | Acc_1: (95.28%) (41586/43648)\n",
      "Epoch: 175 | Batch_idx: 350 |  Loss_1: (0.1232) | Acc_1: (95.29%) (42812/44928)\n",
      "Epoch: 175 | Batch_idx: 360 |  Loss_1: (0.1232) | Acc_1: (95.30%) (44035/46208)\n",
      "Epoch: 175 | Batch_idx: 370 |  Loss_1: (0.1221) | Acc_1: (95.33%) (45272/47488)\n",
      "Epoch: 175 | Batch_idx: 380 |  Loss_1: (0.1219) | Acc_1: (95.34%) (46493/48768)\n",
      "Epoch: 175 | Batch_idx: 390 |  Loss_1: (0.1224) | Acc_1: (95.32%) (47659/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3418) | Acc: (92.64%) (9264/10000)\n",
      "Epoch: 176 | Batch_idx: 0 |  Loss_1: (0.1052) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 176 | Batch_idx: 10 |  Loss_1: (0.1256) | Acc_1: (95.24%) (1341/1408)\n",
      "Epoch: 176 | Batch_idx: 20 |  Loss_1: (0.1203) | Acc_1: (95.39%) (2564/2688)\n",
      "Epoch: 176 | Batch_idx: 30 |  Loss_1: (0.1154) | Acc_1: (95.67%) (3796/3968)\n",
      "Epoch: 176 | Batch_idx: 40 |  Loss_1: (0.1173) | Acc_1: (95.64%) (5019/5248)\n",
      "Epoch: 176 | Batch_idx: 50 |  Loss_1: (0.1160) | Acc_1: (95.73%) (6249/6528)\n",
      "Epoch: 176 | Batch_idx: 60 |  Loss_1: (0.1166) | Acc_1: (95.65%) (7468/7808)\n",
      "Epoch: 176 | Batch_idx: 70 |  Loss_1: (0.1157) | Acc_1: (95.68%) (8695/9088)\n",
      "Epoch: 176 | Batch_idx: 80 |  Loss_1: (0.1151) | Acc_1: (95.71%) (9923/10368)\n",
      "Epoch: 176 | Batch_idx: 90 |  Loss_1: (0.1140) | Acc_1: (95.73%) (11151/11648)\n",
      "Epoch: 176 | Batch_idx: 100 |  Loss_1: (0.1146) | Acc_1: (95.70%) (12372/12928)\n",
      "Epoch: 176 | Batch_idx: 110 |  Loss_1: (0.1153) | Acc_1: (95.72%) (13600/14208)\n",
      "Epoch: 176 | Batch_idx: 120 |  Loss_1: (0.1165) | Acc_1: (95.69%) (14820/15488)\n",
      "Epoch: 176 | Batch_idx: 130 |  Loss_1: (0.1163) | Acc_1: (95.71%) (16048/16768)\n",
      "Epoch: 176 | Batch_idx: 140 |  Loss_1: (0.1171) | Acc_1: (95.67%) (17266/18048)\n",
      "Epoch: 176 | Batch_idx: 150 |  Loss_1: (0.1178) | Acc_1: (95.62%) (18481/19328)\n",
      "Epoch: 176 | Batch_idx: 160 |  Loss_1: (0.1183) | Acc_1: (95.58%) (19697/20608)\n",
      "Epoch: 176 | Batch_idx: 170 |  Loss_1: (0.1198) | Acc_1: (95.52%) (20907/21888)\n",
      "Epoch: 176 | Batch_idx: 180 |  Loss_1: (0.1194) | Acc_1: (95.53%) (22132/23168)\n",
      "Epoch: 176 | Batch_idx: 190 |  Loss_1: (0.1196) | Acc_1: (95.52%) (23352/24448)\n",
      "Epoch: 176 | Batch_idx: 200 |  Loss_1: (0.1207) | Acc_1: (95.47%) (24563/25728)\n",
      "Epoch: 176 | Batch_idx: 210 |  Loss_1: (0.1209) | Acc_1: (95.46%) (25781/27008)\n",
      "Epoch: 176 | Batch_idx: 220 |  Loss_1: (0.1200) | Acc_1: (95.47%) (27007/28288)\n",
      "Epoch: 176 | Batch_idx: 230 |  Loss_1: (0.1204) | Acc_1: (95.47%) (28229/29568)\n",
      "Epoch: 176 | Batch_idx: 240 |  Loss_1: (0.1200) | Acc_1: (95.49%) (29456/30848)\n",
      "Epoch: 176 | Batch_idx: 250 |  Loss_1: (0.1196) | Acc_1: (95.50%) (30681/32128)\n",
      "Epoch: 176 | Batch_idx: 260 |  Loss_1: (0.1196) | Acc_1: (95.50%) (31903/33408)\n",
      "Epoch: 176 | Batch_idx: 270 |  Loss_1: (0.1195) | Acc_1: (95.51%) (33129/34688)\n",
      "Epoch: 176 | Batch_idx: 280 |  Loss_1: (0.1196) | Acc_1: (95.47%) (34339/35968)\n",
      "Epoch: 176 | Batch_idx: 290 |  Loss_1: (0.1198) | Acc_1: (95.45%) (35555/37248)\n",
      "Epoch: 176 | Batch_idx: 300 |  Loss_1: (0.1204) | Acc_1: (95.42%) (36765/38528)\n",
      "Epoch: 176 | Batch_idx: 310 |  Loss_1: (0.1200) | Acc_1: (95.44%) (37994/39808)\n",
      "Epoch: 176 | Batch_idx: 320 |  Loss_1: (0.1207) | Acc_1: (95.41%) (39204/41088)\n",
      "Epoch: 176 | Batch_idx: 330 |  Loss_1: (0.1203) | Acc_1: (95.44%) (40434/42368)\n",
      "Epoch: 176 | Batch_idx: 340 |  Loss_1: (0.1200) | Acc_1: (95.45%) (41664/43648)\n",
      "Epoch: 176 | Batch_idx: 350 |  Loss_1: (0.1197) | Acc_1: (95.46%) (42890/44928)\n",
      "Epoch: 176 | Batch_idx: 360 |  Loss_1: (0.1193) | Acc_1: (95.47%) (44117/46208)\n",
      "Epoch: 176 | Batch_idx: 370 |  Loss_1: (0.1190) | Acc_1: (95.48%) (45342/47488)\n",
      "Epoch: 176 | Batch_idx: 380 |  Loss_1: (0.1195) | Acc_1: (95.46%) (46555/48768)\n",
      "Epoch: 176 | Batch_idx: 390 |  Loss_1: (0.1188) | Acc_1: (95.48%) (47742/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3496) | Acc: (92.41%) (9241/10000)\n",
      "Epoch: 177 | Batch_idx: 0 |  Loss_1: (0.1545) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 177 | Batch_idx: 10 |  Loss_1: (0.1059) | Acc_1: (95.81%) (1349/1408)\n",
      "Epoch: 177 | Batch_idx: 20 |  Loss_1: (0.1114) | Acc_1: (95.54%) (2568/2688)\n",
      "Epoch: 177 | Batch_idx: 30 |  Loss_1: (0.1106) | Acc_1: (95.59%) (3793/3968)\n",
      "Epoch: 177 | Batch_idx: 40 |  Loss_1: (0.1123) | Acc_1: (95.46%) (5010/5248)\n",
      "Epoch: 177 | Batch_idx: 50 |  Loss_1: (0.1159) | Acc_1: (95.36%) (6225/6528)\n",
      "Epoch: 177 | Batch_idx: 60 |  Loss_1: (0.1182) | Acc_1: (95.31%) (7442/7808)\n",
      "Epoch: 177 | Batch_idx: 70 |  Loss_1: (0.1232) | Acc_1: (95.25%) (8656/9088)\n",
      "Epoch: 177 | Batch_idx: 80 |  Loss_1: (0.1242) | Acc_1: (95.19%) (9869/10368)\n",
      "Epoch: 177 | Batch_idx: 90 |  Loss_1: (0.1229) | Acc_1: (95.29%) (11099/11648)\n",
      "Epoch: 177 | Batch_idx: 100 |  Loss_1: (0.1227) | Acc_1: (95.30%) (12321/12928)\n",
      "Epoch: 177 | Batch_idx: 110 |  Loss_1: (0.1237) | Acc_1: (95.24%) (13532/14208)\n",
      "Epoch: 177 | Batch_idx: 120 |  Loss_1: (0.1226) | Acc_1: (95.32%) (14763/15488)\n",
      "Epoch: 177 | Batch_idx: 130 |  Loss_1: (0.1233) | Acc_1: (95.32%) (15983/16768)\n",
      "Epoch: 177 | Batch_idx: 140 |  Loss_1: (0.1231) | Acc_1: (95.36%) (17211/18048)\n",
      "Epoch: 177 | Batch_idx: 150 |  Loss_1: (0.1233) | Acc_1: (95.36%) (18432/19328)\n",
      "Epoch: 177 | Batch_idx: 160 |  Loss_1: (0.1246) | Acc_1: (95.32%) (19643/20608)\n",
      "Epoch: 177 | Batch_idx: 170 |  Loss_1: (0.1256) | Acc_1: (95.27%) (20853/21888)\n",
      "Epoch: 177 | Batch_idx: 180 |  Loss_1: (0.1259) | Acc_1: (95.23%) (22063/23168)\n",
      "Epoch: 177 | Batch_idx: 190 |  Loss_1: (0.1256) | Acc_1: (95.23%) (23282/24448)\n",
      "Epoch: 177 | Batch_idx: 200 |  Loss_1: (0.1257) | Acc_1: (95.23%) (24501/25728)\n",
      "Epoch: 177 | Batch_idx: 210 |  Loss_1: (0.1262) | Acc_1: (95.22%) (25717/27008)\n",
      "Epoch: 177 | Batch_idx: 220 |  Loss_1: (0.1250) | Acc_1: (95.27%) (26949/28288)\n",
      "Epoch: 177 | Batch_idx: 230 |  Loss_1: (0.1246) | Acc_1: (95.29%) (28174/29568)\n",
      "Epoch: 177 | Batch_idx: 240 |  Loss_1: (0.1238) | Acc_1: (95.31%) (29400/30848)\n",
      "Epoch: 177 | Batch_idx: 250 |  Loss_1: (0.1239) | Acc_1: (95.29%) (30615/32128)\n",
      "Epoch: 177 | Batch_idx: 260 |  Loss_1: (0.1242) | Acc_1: (95.27%) (31829/33408)\n",
      "Epoch: 177 | Batch_idx: 270 |  Loss_1: (0.1243) | Acc_1: (95.27%) (33046/34688)\n",
      "Epoch: 177 | Batch_idx: 280 |  Loss_1: (0.1237) | Acc_1: (95.29%) (34275/35968)\n",
      "Epoch: 177 | Batch_idx: 290 |  Loss_1: (0.1240) | Acc_1: (95.29%) (35492/37248)\n",
      "Epoch: 177 | Batch_idx: 300 |  Loss_1: (0.1241) | Acc_1: (95.29%) (36714/38528)\n",
      "Epoch: 177 | Batch_idx: 310 |  Loss_1: (0.1239) | Acc_1: (95.30%) (37937/39808)\n",
      "Epoch: 177 | Batch_idx: 320 |  Loss_1: (0.1241) | Acc_1: (95.30%) (39155/41088)\n",
      "Epoch: 177 | Batch_idx: 330 |  Loss_1: (0.1246) | Acc_1: (95.28%) (40367/42368)\n",
      "Epoch: 177 | Batch_idx: 340 |  Loss_1: (0.1236) | Acc_1: (95.32%) (41607/43648)\n",
      "Epoch: 177 | Batch_idx: 350 |  Loss_1: (0.1231) | Acc_1: (95.35%) (42838/44928)\n",
      "Epoch: 177 | Batch_idx: 360 |  Loss_1: (0.1232) | Acc_1: (95.35%) (44059/46208)\n",
      "Epoch: 177 | Batch_idx: 370 |  Loss_1: (0.1233) | Acc_1: (95.34%) (45274/47488)\n",
      "Epoch: 177 | Batch_idx: 380 |  Loss_1: (0.1234) | Acc_1: (95.33%) (46490/48768)\n",
      "Epoch: 177 | Batch_idx: 390 |  Loss_1: (0.1231) | Acc_1: (95.35%) (47674/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3516) | Acc: (92.42%) (9242/10000)\n",
      "Epoch: 178 | Batch_idx: 0 |  Loss_1: (0.1126) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 178 | Batch_idx: 10 |  Loss_1: (0.1444) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 178 | Batch_idx: 20 |  Loss_1: (0.1269) | Acc_1: (95.50%) (2567/2688)\n",
      "Epoch: 178 | Batch_idx: 30 |  Loss_1: (0.1282) | Acc_1: (95.41%) (3786/3968)\n",
      "Epoch: 178 | Batch_idx: 40 |  Loss_1: (0.1296) | Acc_1: (95.24%) (4998/5248)\n",
      "Epoch: 178 | Batch_idx: 50 |  Loss_1: (0.1280) | Acc_1: (95.28%) (6220/6528)\n",
      "Epoch: 178 | Batch_idx: 60 |  Loss_1: (0.1291) | Acc_1: (95.22%) (7435/7808)\n",
      "Epoch: 178 | Batch_idx: 70 |  Loss_1: (0.1262) | Acc_1: (95.33%) (8664/9088)\n",
      "Epoch: 178 | Batch_idx: 80 |  Loss_1: (0.1259) | Acc_1: (95.30%) (9881/10368)\n",
      "Epoch: 178 | Batch_idx: 90 |  Loss_1: (0.1269) | Acc_1: (95.26%) (11096/11648)\n",
      "Epoch: 178 | Batch_idx: 100 |  Loss_1: (0.1267) | Acc_1: (95.27%) (12316/12928)\n",
      "Epoch: 178 | Batch_idx: 110 |  Loss_1: (0.1284) | Acc_1: (95.23%) (13530/14208)\n",
      "Epoch: 178 | Batch_idx: 120 |  Loss_1: (0.1266) | Acc_1: (95.29%) (14758/15488)\n",
      "Epoch: 178 | Batch_idx: 130 |  Loss_1: (0.1267) | Acc_1: (95.28%) (15976/16768)\n",
      "Epoch: 178 | Batch_idx: 140 |  Loss_1: (0.1274) | Acc_1: (95.25%) (17190/18048)\n",
      "Epoch: 178 | Batch_idx: 150 |  Loss_1: (0.1277) | Acc_1: (95.24%) (18408/19328)\n",
      "Epoch: 178 | Batch_idx: 160 |  Loss_1: (0.1269) | Acc_1: (95.25%) (19629/20608)\n",
      "Epoch: 178 | Batch_idx: 170 |  Loss_1: (0.1277) | Acc_1: (95.22%) (20842/21888)\n",
      "Epoch: 178 | Batch_idx: 180 |  Loss_1: (0.1279) | Acc_1: (95.21%) (22058/23168)\n",
      "Epoch: 178 | Batch_idx: 190 |  Loss_1: (0.1289) | Acc_1: (95.17%) (23268/24448)\n",
      "Epoch: 178 | Batch_idx: 200 |  Loss_1: (0.1291) | Acc_1: (95.16%) (24483/25728)\n",
      "Epoch: 178 | Batch_idx: 210 |  Loss_1: (0.1287) | Acc_1: (95.17%) (25704/27008)\n",
      "Epoch: 178 | Batch_idx: 220 |  Loss_1: (0.1281) | Acc_1: (95.20%) (26930/28288)\n",
      "Epoch: 178 | Batch_idx: 230 |  Loss_1: (0.1282) | Acc_1: (95.19%) (28145/29568)\n",
      "Epoch: 178 | Batch_idx: 240 |  Loss_1: (0.1266) | Acc_1: (95.25%) (29383/30848)\n",
      "Epoch: 178 | Batch_idx: 250 |  Loss_1: (0.1267) | Acc_1: (95.25%) (30601/32128)\n",
      "Epoch: 178 | Batch_idx: 260 |  Loss_1: (0.1262) | Acc_1: (95.27%) (31828/33408)\n",
      "Epoch: 178 | Batch_idx: 270 |  Loss_1: (0.1253) | Acc_1: (95.32%) (33063/34688)\n",
      "Epoch: 178 | Batch_idx: 280 |  Loss_1: (0.1246) | Acc_1: (95.35%) (34295/35968)\n",
      "Epoch: 178 | Batch_idx: 290 |  Loss_1: (0.1245) | Acc_1: (95.35%) (35517/37248)\n",
      "Epoch: 178 | Batch_idx: 300 |  Loss_1: (0.1241) | Acc_1: (95.37%) (36745/38528)\n",
      "Epoch: 178 | Batch_idx: 310 |  Loss_1: (0.1237) | Acc_1: (95.39%) (37973/39808)\n",
      "Epoch: 178 | Batch_idx: 320 |  Loss_1: (0.1243) | Acc_1: (95.35%) (39179/41088)\n",
      "Epoch: 178 | Batch_idx: 330 |  Loss_1: (0.1243) | Acc_1: (95.36%) (40402/42368)\n",
      "Epoch: 178 | Batch_idx: 340 |  Loss_1: (0.1239) | Acc_1: (95.37%) (41627/43648)\n",
      "Epoch: 178 | Batch_idx: 350 |  Loss_1: (0.1241) | Acc_1: (95.36%) (42845/44928)\n",
      "Epoch: 178 | Batch_idx: 360 |  Loss_1: (0.1238) | Acc_1: (95.38%) (44072/46208)\n",
      "Epoch: 178 | Batch_idx: 370 |  Loss_1: (0.1241) | Acc_1: (95.37%) (45289/47488)\n",
      "Epoch: 178 | Batch_idx: 380 |  Loss_1: (0.1233) | Acc_1: (95.40%) (46525/48768)\n",
      "Epoch: 178 | Batch_idx: 390 |  Loss_1: (0.1227) | Acc_1: (95.42%) (47711/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3320) | Acc: (92.51%) (9251/10000)\n",
      "Epoch: 179 | Batch_idx: 0 |  Loss_1: (0.1191) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 179 | Batch_idx: 10 |  Loss_1: (0.1080) | Acc_1: (96.02%) (1352/1408)\n",
      "Epoch: 179 | Batch_idx: 20 |  Loss_1: (0.1147) | Acc_1: (95.68%) (2572/2688)\n",
      "Epoch: 179 | Batch_idx: 30 |  Loss_1: (0.1131) | Acc_1: (95.79%) (3801/3968)\n",
      "Epoch: 179 | Batch_idx: 40 |  Loss_1: (0.1121) | Acc_1: (95.75%) (5025/5248)\n",
      "Epoch: 179 | Batch_idx: 50 |  Loss_1: (0.1134) | Acc_1: (95.70%) (6247/6528)\n",
      "Epoch: 179 | Batch_idx: 60 |  Loss_1: (0.1188) | Acc_1: (95.45%) (7453/7808)\n",
      "Epoch: 179 | Batch_idx: 70 |  Loss_1: (0.1140) | Acc_1: (95.61%) (8689/9088)\n",
      "Epoch: 179 | Batch_idx: 80 |  Loss_1: (0.1180) | Acc_1: (95.50%) (9901/10368)\n",
      "Epoch: 179 | Batch_idx: 90 |  Loss_1: (0.1203) | Acc_1: (95.39%) (11111/11648)\n",
      "Epoch: 179 | Batch_idx: 100 |  Loss_1: (0.1203) | Acc_1: (95.43%) (12337/12928)\n",
      "Epoch: 179 | Batch_idx: 110 |  Loss_1: (0.1203) | Acc_1: (95.39%) (13553/14208)\n",
      "Epoch: 179 | Batch_idx: 120 |  Loss_1: (0.1192) | Acc_1: (95.43%) (14780/15488)\n",
      "Epoch: 179 | Batch_idx: 130 |  Loss_1: (0.1178) | Acc_1: (95.46%) (16006/16768)\n",
      "Epoch: 179 | Batch_idx: 140 |  Loss_1: (0.1181) | Acc_1: (95.44%) (17225/18048)\n",
      "Epoch: 179 | Batch_idx: 150 |  Loss_1: (0.1171) | Acc_1: (95.50%) (18458/19328)\n",
      "Epoch: 179 | Batch_idx: 160 |  Loss_1: (0.1180) | Acc_1: (95.47%) (19675/20608)\n",
      "Epoch: 179 | Batch_idx: 170 |  Loss_1: (0.1177) | Acc_1: (95.48%) (20899/21888)\n",
      "Epoch: 179 | Batch_idx: 180 |  Loss_1: (0.1189) | Acc_1: (95.45%) (22114/23168)\n",
      "Epoch: 179 | Batch_idx: 190 |  Loss_1: (0.1191) | Acc_1: (95.44%) (23334/24448)\n",
      "Epoch: 179 | Batch_idx: 200 |  Loss_1: (0.1191) | Acc_1: (95.43%) (24553/25728)\n",
      "Epoch: 179 | Batch_idx: 210 |  Loss_1: (0.1194) | Acc_1: (95.44%) (25776/27008)\n",
      "Epoch: 179 | Batch_idx: 220 |  Loss_1: (0.1201) | Acc_1: (95.42%) (26991/28288)\n",
      "Epoch: 179 | Batch_idx: 230 |  Loss_1: (0.1203) | Acc_1: (95.41%) (28212/29568)\n",
      "Epoch: 179 | Batch_idx: 240 |  Loss_1: (0.1200) | Acc_1: (95.42%) (29435/30848)\n",
      "Epoch: 179 | Batch_idx: 250 |  Loss_1: (0.1203) | Acc_1: (95.41%) (30652/32128)\n",
      "Epoch: 179 | Batch_idx: 260 |  Loss_1: (0.1201) | Acc_1: (95.41%) (31876/33408)\n",
      "Epoch: 179 | Batch_idx: 270 |  Loss_1: (0.1202) | Acc_1: (95.40%) (33091/34688)\n",
      "Epoch: 179 | Batch_idx: 280 |  Loss_1: (0.1201) | Acc_1: (95.39%) (34311/35968)\n",
      "Epoch: 179 | Batch_idx: 290 |  Loss_1: (0.1206) | Acc_1: (95.40%) (35533/37248)\n",
      "Epoch: 179 | Batch_idx: 300 |  Loss_1: (0.1202) | Acc_1: (95.41%) (36761/38528)\n",
      "Epoch: 179 | Batch_idx: 310 |  Loss_1: (0.1203) | Acc_1: (95.41%) (37979/39808)\n",
      "Epoch: 179 | Batch_idx: 320 |  Loss_1: (0.1210) | Acc_1: (95.39%) (39192/41088)\n",
      "Epoch: 179 | Batch_idx: 330 |  Loss_1: (0.1206) | Acc_1: (95.40%) (40418/42368)\n",
      "Epoch: 179 | Batch_idx: 340 |  Loss_1: (0.1203) | Acc_1: (95.40%) (41639/43648)\n",
      "Epoch: 179 | Batch_idx: 350 |  Loss_1: (0.1208) | Acc_1: (95.37%) (42850/44928)\n",
      "Epoch: 179 | Batch_idx: 360 |  Loss_1: (0.1204) | Acc_1: (95.39%) (44080/46208)\n",
      "Epoch: 179 | Batch_idx: 370 |  Loss_1: (0.1203) | Acc_1: (95.40%) (45302/47488)\n",
      "Epoch: 179 | Batch_idx: 380 |  Loss_1: (0.1202) | Acc_1: (95.40%) (46526/48768)\n",
      "Epoch: 179 | Batch_idx: 390 |  Loss_1: (0.1206) | Acc_1: (95.38%) (47690/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3375) | Acc: (92.44%) (9244/10000)\n",
      "Epoch: 180 | Batch_idx: 0 |  Loss_1: (0.1460) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 180 | Batch_idx: 10 |  Loss_1: (0.1126) | Acc_1: (95.67%) (1347/1408)\n",
      "Epoch: 180 | Batch_idx: 20 |  Loss_1: (0.1174) | Acc_1: (95.31%) (2562/2688)\n",
      "Epoch: 180 | Batch_idx: 30 |  Loss_1: (0.1229) | Acc_1: (95.11%) (3774/3968)\n",
      "Epoch: 180 | Batch_idx: 40 |  Loss_1: (0.1239) | Acc_1: (95.20%) (4996/5248)\n",
      "Epoch: 180 | Batch_idx: 50 |  Loss_1: (0.1247) | Acc_1: (95.24%) (6217/6528)\n",
      "Epoch: 180 | Batch_idx: 60 |  Loss_1: (0.1261) | Acc_1: (95.18%) (7432/7808)\n",
      "Epoch: 180 | Batch_idx: 70 |  Loss_1: (0.1261) | Acc_1: (95.20%) (8652/9088)\n",
      "Epoch: 180 | Batch_idx: 80 |  Loss_1: (0.1259) | Acc_1: (95.21%) (9871/10368)\n",
      "Epoch: 180 | Batch_idx: 90 |  Loss_1: (0.1225) | Acc_1: (95.32%) (11103/11648)\n",
      "Epoch: 180 | Batch_idx: 100 |  Loss_1: (0.1234) | Acc_1: (95.25%) (12314/12928)\n",
      "Epoch: 180 | Batch_idx: 110 |  Loss_1: (0.1237) | Acc_1: (95.25%) (13533/14208)\n",
      "Epoch: 180 | Batch_idx: 120 |  Loss_1: (0.1237) | Acc_1: (95.24%) (14751/15488)\n",
      "Epoch: 180 | Batch_idx: 130 |  Loss_1: (0.1230) | Acc_1: (95.29%) (15978/16768)\n",
      "Epoch: 180 | Batch_idx: 140 |  Loss_1: (0.1223) | Acc_1: (95.34%) (17207/18048)\n",
      "Epoch: 180 | Batch_idx: 150 |  Loss_1: (0.1212) | Acc_1: (95.40%) (18439/19328)\n",
      "Epoch: 180 | Batch_idx: 160 |  Loss_1: (0.1215) | Acc_1: (95.41%) (19663/20608)\n",
      "Epoch: 180 | Batch_idx: 170 |  Loss_1: (0.1219) | Acc_1: (95.40%) (20881/21888)\n",
      "Epoch: 180 | Batch_idx: 180 |  Loss_1: (0.1212) | Acc_1: (95.43%) (22110/23168)\n",
      "Epoch: 180 | Batch_idx: 190 |  Loss_1: (0.1209) | Acc_1: (95.44%) (23334/24448)\n",
      "Epoch: 180 | Batch_idx: 200 |  Loss_1: (0.1213) | Acc_1: (95.43%) (24553/25728)\n",
      "Epoch: 180 | Batch_idx: 210 |  Loss_1: (0.1210) | Acc_1: (95.46%) (25781/27008)\n",
      "Epoch: 180 | Batch_idx: 220 |  Loss_1: (0.1202) | Acc_1: (95.50%) (27016/28288)\n",
      "Epoch: 180 | Batch_idx: 230 |  Loss_1: (0.1201) | Acc_1: (95.50%) (28237/29568)\n",
      "Epoch: 180 | Batch_idx: 240 |  Loss_1: (0.1200) | Acc_1: (95.50%) (29459/30848)\n",
      "Epoch: 180 | Batch_idx: 250 |  Loss_1: (0.1195) | Acc_1: (95.52%) (30689/32128)\n",
      "Epoch: 180 | Batch_idx: 260 |  Loss_1: (0.1197) | Acc_1: (95.53%) (31913/33408)\n",
      "Epoch: 180 | Batch_idx: 270 |  Loss_1: (0.1190) | Acc_1: (95.55%) (33146/34688)\n",
      "Epoch: 180 | Batch_idx: 280 |  Loss_1: (0.1191) | Acc_1: (95.57%) (34376/35968)\n",
      "Epoch: 180 | Batch_idx: 290 |  Loss_1: (0.1196) | Acc_1: (95.56%) (35595/37248)\n",
      "Epoch: 180 | Batch_idx: 300 |  Loss_1: (0.1200) | Acc_1: (95.55%) (36815/38528)\n",
      "Epoch: 180 | Batch_idx: 310 |  Loss_1: (0.1200) | Acc_1: (95.55%) (38037/39808)\n",
      "Epoch: 180 | Batch_idx: 320 |  Loss_1: (0.1196) | Acc_1: (95.55%) (39260/41088)\n",
      "Epoch: 180 | Batch_idx: 330 |  Loss_1: (0.1201) | Acc_1: (95.54%) (40477/42368)\n",
      "Epoch: 180 | Batch_idx: 340 |  Loss_1: (0.1196) | Acc_1: (95.56%) (41709/43648)\n",
      "Epoch: 180 | Batch_idx: 350 |  Loss_1: (0.1200) | Acc_1: (95.55%) (42927/44928)\n",
      "Epoch: 180 | Batch_idx: 360 |  Loss_1: (0.1203) | Acc_1: (95.53%) (44143/46208)\n",
      "Epoch: 180 | Batch_idx: 370 |  Loss_1: (0.1206) | Acc_1: (95.52%) (45362/47488)\n",
      "Epoch: 180 | Batch_idx: 380 |  Loss_1: (0.1206) | Acc_1: (95.51%) (46580/48768)\n",
      "Epoch: 180 | Batch_idx: 390 |  Loss_1: (0.1208) | Acc_1: (95.50%) (47751/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3533) | Acc: (92.28%) (9228/10000)\n",
      "Epoch: 181 | Batch_idx: 0 |  Loss_1: (0.1554) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 181 | Batch_idx: 10 |  Loss_1: (0.0947) | Acc_1: (96.24%) (1355/1408)\n",
      "Epoch: 181 | Batch_idx: 20 |  Loss_1: (0.1145) | Acc_1: (95.54%) (2568/2688)\n",
      "Epoch: 181 | Batch_idx: 30 |  Loss_1: (0.1219) | Acc_1: (95.26%) (3780/3968)\n",
      "Epoch: 181 | Batch_idx: 40 |  Loss_1: (0.1211) | Acc_1: (95.29%) (5001/5248)\n",
      "Epoch: 181 | Batch_idx: 50 |  Loss_1: (0.1253) | Acc_1: (95.08%) (6207/6528)\n",
      "Epoch: 181 | Batch_idx: 60 |  Loss_1: (0.1260) | Acc_1: (95.09%) (7425/7808)\n",
      "Epoch: 181 | Batch_idx: 70 |  Loss_1: (0.1249) | Acc_1: (95.15%) (8647/9088)\n",
      "Epoch: 181 | Batch_idx: 80 |  Loss_1: (0.1248) | Acc_1: (95.16%) (9866/10368)\n",
      "Epoch: 181 | Batch_idx: 90 |  Loss_1: (0.1248) | Acc_1: (95.23%) (11092/11648)\n",
      "Epoch: 181 | Batch_idx: 100 |  Loss_1: (0.1241) | Acc_1: (95.30%) (12320/12928)\n",
      "Epoch: 181 | Batch_idx: 110 |  Loss_1: (0.1260) | Acc_1: (95.22%) (13529/14208)\n",
      "Epoch: 181 | Batch_idx: 120 |  Loss_1: (0.1258) | Acc_1: (95.24%) (14750/15488)\n",
      "Epoch: 181 | Batch_idx: 130 |  Loss_1: (0.1240) | Acc_1: (95.30%) (15980/16768)\n",
      "Epoch: 181 | Batch_idx: 140 |  Loss_1: (0.1239) | Acc_1: (95.31%) (17201/18048)\n",
      "Epoch: 181 | Batch_idx: 150 |  Loss_1: (0.1229) | Acc_1: (95.34%) (18427/19328)\n",
      "Epoch: 181 | Batch_idx: 160 |  Loss_1: (0.1220) | Acc_1: (95.37%) (19653/20608)\n",
      "Epoch: 181 | Batch_idx: 170 |  Loss_1: (0.1230) | Acc_1: (95.33%) (20865/21888)\n",
      "Epoch: 181 | Batch_idx: 180 |  Loss_1: (0.1234) | Acc_1: (95.32%) (22083/23168)\n",
      "Epoch: 181 | Batch_idx: 190 |  Loss_1: (0.1239) | Acc_1: (95.31%) (23301/24448)\n",
      "Epoch: 181 | Batch_idx: 200 |  Loss_1: (0.1243) | Acc_1: (95.30%) (24519/25728)\n",
      "Epoch: 181 | Batch_idx: 210 |  Loss_1: (0.1234) | Acc_1: (95.33%) (25748/27008)\n",
      "Epoch: 181 | Batch_idx: 220 |  Loss_1: (0.1233) | Acc_1: (95.34%) (26971/28288)\n",
      "Epoch: 181 | Batch_idx: 230 |  Loss_1: (0.1230) | Acc_1: (95.35%) (28192/29568)\n",
      "Epoch: 181 | Batch_idx: 240 |  Loss_1: (0.1222) | Acc_1: (95.37%) (29420/30848)\n",
      "Epoch: 181 | Batch_idx: 250 |  Loss_1: (0.1219) | Acc_1: (95.37%) (30642/32128)\n",
      "Epoch: 181 | Batch_idx: 260 |  Loss_1: (0.1223) | Acc_1: (95.35%) (31855/33408)\n",
      "Epoch: 181 | Batch_idx: 270 |  Loss_1: (0.1231) | Acc_1: (95.32%) (33064/34688)\n",
      "Epoch: 181 | Batch_idx: 280 |  Loss_1: (0.1241) | Acc_1: (95.27%) (34268/35968)\n",
      "Epoch: 181 | Batch_idx: 290 |  Loss_1: (0.1238) | Acc_1: (95.27%) (35488/37248)\n",
      "Epoch: 181 | Batch_idx: 300 |  Loss_1: (0.1244) | Acc_1: (95.25%) (36697/38528)\n",
      "Epoch: 181 | Batch_idx: 310 |  Loss_1: (0.1244) | Acc_1: (95.25%) (37919/39808)\n",
      "Epoch: 181 | Batch_idx: 320 |  Loss_1: (0.1237) | Acc_1: (95.28%) (39149/41088)\n",
      "Epoch: 181 | Batch_idx: 330 |  Loss_1: (0.1232) | Acc_1: (95.29%) (40373/42368)\n",
      "Epoch: 181 | Batch_idx: 340 |  Loss_1: (0.1229) | Acc_1: (95.31%) (41601/43648)\n",
      "Epoch: 181 | Batch_idx: 350 |  Loss_1: (0.1228) | Acc_1: (95.30%) (42818/44928)\n",
      "Epoch: 181 | Batch_idx: 360 |  Loss_1: (0.1229) | Acc_1: (95.31%) (44039/46208)\n",
      "Epoch: 181 | Batch_idx: 370 |  Loss_1: (0.1224) | Acc_1: (95.33%) (45269/47488)\n",
      "Epoch: 181 | Batch_idx: 380 |  Loss_1: (0.1223) | Acc_1: (95.32%) (46486/48768)\n",
      "Epoch: 181 | Batch_idx: 390 |  Loss_1: (0.1225) | Acc_1: (95.31%) (47657/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3631) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 182 | Batch_idx: 0 |  Loss_1: (0.0736) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 182 | Batch_idx: 10 |  Loss_1: (0.0924) | Acc_1: (96.59%) (1360/1408)\n",
      "Epoch: 182 | Batch_idx: 20 |  Loss_1: (0.1037) | Acc_1: (96.02%) (2581/2688)\n",
      "Epoch: 182 | Batch_idx: 30 |  Loss_1: (0.1147) | Acc_1: (95.59%) (3793/3968)\n",
      "Epoch: 182 | Batch_idx: 40 |  Loss_1: (0.1208) | Acc_1: (95.33%) (5003/5248)\n",
      "Epoch: 182 | Batch_idx: 50 |  Loss_1: (0.1214) | Acc_1: (95.30%) (6221/6528)\n",
      "Epoch: 182 | Batch_idx: 60 |  Loss_1: (0.1187) | Acc_1: (95.41%) (7450/7808)\n",
      "Epoch: 182 | Batch_idx: 70 |  Loss_1: (0.1163) | Acc_1: (95.50%) (8679/9088)\n",
      "Epoch: 182 | Batch_idx: 80 |  Loss_1: (0.1187) | Acc_1: (95.40%) (9891/10368)\n",
      "Epoch: 182 | Batch_idx: 90 |  Loss_1: (0.1182) | Acc_1: (95.40%) (11112/11648)\n",
      "Epoch: 182 | Batch_idx: 100 |  Loss_1: (0.1191) | Acc_1: (95.38%) (12331/12928)\n",
      "Epoch: 182 | Batch_idx: 110 |  Loss_1: (0.1176) | Acc_1: (95.44%) (13560/14208)\n",
      "Epoch: 182 | Batch_idx: 120 |  Loss_1: (0.1170) | Acc_1: (95.49%) (14790/15488)\n",
      "Epoch: 182 | Batch_idx: 130 |  Loss_1: (0.1153) | Acc_1: (95.56%) (16024/16768)\n",
      "Epoch: 182 | Batch_idx: 140 |  Loss_1: (0.1164) | Acc_1: (95.51%) (17238/18048)\n",
      "Epoch: 182 | Batch_idx: 150 |  Loss_1: (0.1174) | Acc_1: (95.49%) (18457/19328)\n",
      "Epoch: 182 | Batch_idx: 160 |  Loss_1: (0.1184) | Acc_1: (95.45%) (19671/20608)\n",
      "Epoch: 182 | Batch_idx: 170 |  Loss_1: (0.1191) | Acc_1: (95.42%) (20885/21888)\n",
      "Epoch: 182 | Batch_idx: 180 |  Loss_1: (0.1183) | Acc_1: (95.43%) (22110/23168)\n",
      "Epoch: 182 | Batch_idx: 190 |  Loss_1: (0.1194) | Acc_1: (95.41%) (23326/24448)\n",
      "Epoch: 182 | Batch_idx: 200 |  Loss_1: (0.1203) | Acc_1: (95.39%) (24543/25728)\n",
      "Epoch: 182 | Batch_idx: 210 |  Loss_1: (0.1198) | Acc_1: (95.42%) (25770/27008)\n",
      "Epoch: 182 | Batch_idx: 220 |  Loss_1: (0.1206) | Acc_1: (95.38%) (26981/28288)\n",
      "Epoch: 182 | Batch_idx: 230 |  Loss_1: (0.1198) | Acc_1: (95.40%) (28209/29568)\n",
      "Epoch: 182 | Batch_idx: 240 |  Loss_1: (0.1203) | Acc_1: (95.39%) (29426/30848)\n",
      "Epoch: 182 | Batch_idx: 250 |  Loss_1: (0.1201) | Acc_1: (95.39%) (30648/32128)\n",
      "Epoch: 182 | Batch_idx: 260 |  Loss_1: (0.1206) | Acc_1: (95.37%) (31862/33408)\n",
      "Epoch: 182 | Batch_idx: 270 |  Loss_1: (0.1203) | Acc_1: (95.39%) (33090/34688)\n",
      "Epoch: 182 | Batch_idx: 280 |  Loss_1: (0.1206) | Acc_1: (95.37%) (34303/35968)\n",
      "Epoch: 182 | Batch_idx: 290 |  Loss_1: (0.1199) | Acc_1: (95.40%) (35534/37248)\n",
      "Epoch: 182 | Batch_idx: 300 |  Loss_1: (0.1199) | Acc_1: (95.40%) (36756/38528)\n",
      "Epoch: 182 | Batch_idx: 310 |  Loss_1: (0.1200) | Acc_1: (95.41%) (37981/39808)\n",
      "Epoch: 182 | Batch_idx: 320 |  Loss_1: (0.1198) | Acc_1: (95.41%) (39202/41088)\n",
      "Epoch: 182 | Batch_idx: 330 |  Loss_1: (0.1191) | Acc_1: (95.44%) (40437/42368)\n",
      "Epoch: 182 | Batch_idx: 340 |  Loss_1: (0.1187) | Acc_1: (95.45%) (41661/43648)\n",
      "Epoch: 182 | Batch_idx: 350 |  Loss_1: (0.1187) | Acc_1: (95.44%) (42881/44928)\n",
      "Epoch: 182 | Batch_idx: 360 |  Loss_1: (0.1186) | Acc_1: (95.45%) (44104/46208)\n",
      "Epoch: 182 | Batch_idx: 370 |  Loss_1: (0.1182) | Acc_1: (95.47%) (45335/47488)\n",
      "Epoch: 182 | Batch_idx: 380 |  Loss_1: (0.1184) | Acc_1: (95.46%) (46554/48768)\n",
      "Epoch: 182 | Batch_idx: 390 |  Loss_1: (0.1183) | Acc_1: (95.47%) (47735/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3485) | Acc: (92.30%) (9230/10000)\n",
      "Epoch: 183 | Batch_idx: 0 |  Loss_1: (0.0764) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 183 | Batch_idx: 10 |  Loss_1: (0.1102) | Acc_1: (95.95%) (1351/1408)\n",
      "Epoch: 183 | Batch_idx: 20 |  Loss_1: (0.1084) | Acc_1: (96.02%) (2581/2688)\n",
      "Epoch: 183 | Batch_idx: 30 |  Loss_1: (0.1090) | Acc_1: (95.97%) (3808/3968)\n",
      "Epoch: 183 | Batch_idx: 40 |  Loss_1: (0.1076) | Acc_1: (96.07%) (5042/5248)\n",
      "Epoch: 183 | Batch_idx: 50 |  Loss_1: (0.1090) | Acc_1: (95.99%) (6266/6528)\n",
      "Epoch: 183 | Batch_idx: 60 |  Loss_1: (0.1130) | Acc_1: (95.80%) (7480/7808)\n",
      "Epoch: 183 | Batch_idx: 70 |  Loss_1: (0.1122) | Acc_1: (95.81%) (8707/9088)\n",
      "Epoch: 183 | Batch_idx: 80 |  Loss_1: (0.1157) | Acc_1: (95.65%) (9917/10368)\n",
      "Epoch: 183 | Batch_idx: 90 |  Loss_1: (0.1176) | Acc_1: (95.64%) (11140/11648)\n",
      "Epoch: 183 | Batch_idx: 100 |  Loss_1: (0.1166) | Acc_1: (95.66%) (12367/12928)\n",
      "Epoch: 183 | Batch_idx: 110 |  Loss_1: (0.1181) | Acc_1: (95.62%) (13586/14208)\n",
      "Epoch: 183 | Batch_idx: 120 |  Loss_1: (0.1167) | Acc_1: (95.67%) (14817/15488)\n",
      "Epoch: 183 | Batch_idx: 130 |  Loss_1: (0.1171) | Acc_1: (95.65%) (16038/16768)\n",
      "Epoch: 183 | Batch_idx: 140 |  Loss_1: (0.1165) | Acc_1: (95.66%) (17265/18048)\n",
      "Epoch: 183 | Batch_idx: 150 |  Loss_1: (0.1152) | Acc_1: (95.71%) (18498/19328)\n",
      "Epoch: 183 | Batch_idx: 160 |  Loss_1: (0.1156) | Acc_1: (95.69%) (19720/20608)\n",
      "Epoch: 183 | Batch_idx: 170 |  Loss_1: (0.1158) | Acc_1: (95.68%) (20943/21888)\n",
      "Epoch: 183 | Batch_idx: 180 |  Loss_1: (0.1141) | Acc_1: (95.76%) (22185/23168)\n",
      "Epoch: 183 | Batch_idx: 190 |  Loss_1: (0.1156) | Acc_1: (95.71%) (23398/24448)\n",
      "Epoch: 183 | Batch_idx: 200 |  Loss_1: (0.1158) | Acc_1: (95.70%) (24622/25728)\n",
      "Epoch: 183 | Batch_idx: 210 |  Loss_1: (0.1158) | Acc_1: (95.69%) (25843/27008)\n",
      "Epoch: 183 | Batch_idx: 220 |  Loss_1: (0.1164) | Acc_1: (95.67%) (27062/28288)\n",
      "Epoch: 183 | Batch_idx: 230 |  Loss_1: (0.1165) | Acc_1: (95.65%) (28283/29568)\n",
      "Epoch: 183 | Batch_idx: 240 |  Loss_1: (0.1166) | Acc_1: (95.66%) (29508/30848)\n",
      "Epoch: 183 | Batch_idx: 250 |  Loss_1: (0.1172) | Acc_1: (95.64%) (30728/32128)\n",
      "Epoch: 183 | Batch_idx: 260 |  Loss_1: (0.1171) | Acc_1: (95.65%) (31955/33408)\n",
      "Epoch: 183 | Batch_idx: 270 |  Loss_1: (0.1175) | Acc_1: (95.63%) (33171/34688)\n",
      "Epoch: 183 | Batch_idx: 280 |  Loss_1: (0.1180) | Acc_1: (95.61%) (34388/35968)\n",
      "Epoch: 183 | Batch_idx: 290 |  Loss_1: (0.1184) | Acc_1: (95.59%) (35607/37248)\n",
      "Epoch: 183 | Batch_idx: 300 |  Loss_1: (0.1184) | Acc_1: (95.59%) (36828/38528)\n",
      "Epoch: 183 | Batch_idx: 310 |  Loss_1: (0.1190) | Acc_1: (95.57%) (38045/39808)\n",
      "Epoch: 183 | Batch_idx: 320 |  Loss_1: (0.1184) | Acc_1: (95.59%) (39274/41088)\n",
      "Epoch: 183 | Batch_idx: 330 |  Loss_1: (0.1174) | Acc_1: (95.62%) (40511/42368)\n",
      "Epoch: 183 | Batch_idx: 340 |  Loss_1: (0.1173) | Acc_1: (95.61%) (41733/43648)\n",
      "Epoch: 183 | Batch_idx: 350 |  Loss_1: (0.1178) | Acc_1: (95.59%) (42948/44928)\n",
      "Epoch: 183 | Batch_idx: 360 |  Loss_1: (0.1178) | Acc_1: (95.59%) (44168/46208)\n",
      "Epoch: 183 | Batch_idx: 370 |  Loss_1: (0.1180) | Acc_1: (95.57%) (45384/47488)\n",
      "Epoch: 183 | Batch_idx: 380 |  Loss_1: (0.1180) | Acc_1: (95.57%) (46608/48768)\n",
      "Epoch: 183 | Batch_idx: 390 |  Loss_1: (0.1188) | Acc_1: (95.53%) (47764/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3625) | Acc: (92.20%) (9220/10000)\n",
      "Epoch: 184 | Batch_idx: 0 |  Loss_1: (0.1287) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 184 | Batch_idx: 10 |  Loss_1: (0.1401) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 184 | Batch_idx: 20 |  Loss_1: (0.1247) | Acc_1: (95.28%) (2561/2688)\n",
      "Epoch: 184 | Batch_idx: 30 |  Loss_1: (0.1271) | Acc_1: (95.34%) (3783/3968)\n",
      "Epoch: 184 | Batch_idx: 40 |  Loss_1: (0.1256) | Acc_1: (95.46%) (5010/5248)\n",
      "Epoch: 184 | Batch_idx: 50 |  Loss_1: (0.1251) | Acc_1: (95.42%) (6229/6528)\n",
      "Epoch: 184 | Batch_idx: 60 |  Loss_1: (0.1273) | Acc_1: (95.27%) (7439/7808)\n",
      "Epoch: 184 | Batch_idx: 70 |  Loss_1: (0.1288) | Acc_1: (95.20%) (8652/9088)\n",
      "Epoch: 184 | Batch_idx: 80 |  Loss_1: (0.1274) | Acc_1: (95.24%) (9875/10368)\n",
      "Epoch: 184 | Batch_idx: 90 |  Loss_1: (0.1277) | Acc_1: (95.26%) (11096/11648)\n",
      "Epoch: 184 | Batch_idx: 100 |  Loss_1: (0.1282) | Acc_1: (95.23%) (12311/12928)\n",
      "Epoch: 184 | Batch_idx: 110 |  Loss_1: (0.1274) | Acc_1: (95.26%) (13534/14208)\n",
      "Epoch: 184 | Batch_idx: 120 |  Loss_1: (0.1271) | Acc_1: (95.24%) (14750/15488)\n",
      "Epoch: 184 | Batch_idx: 130 |  Loss_1: (0.1277) | Acc_1: (95.19%) (15961/16768)\n",
      "Epoch: 184 | Batch_idx: 140 |  Loss_1: (0.1278) | Acc_1: (95.20%) (17181/18048)\n",
      "Epoch: 184 | Batch_idx: 150 |  Loss_1: (0.1268) | Acc_1: (95.23%) (18407/19328)\n",
      "Epoch: 184 | Batch_idx: 160 |  Loss_1: (0.1256) | Acc_1: (95.27%) (19634/20608)\n",
      "Epoch: 184 | Batch_idx: 170 |  Loss_1: (0.1258) | Acc_1: (95.26%) (20850/21888)\n",
      "Epoch: 184 | Batch_idx: 180 |  Loss_1: (0.1254) | Acc_1: (95.29%) (22076/23168)\n",
      "Epoch: 184 | Batch_idx: 190 |  Loss_1: (0.1260) | Acc_1: (95.28%) (23293/24448)\n",
      "Epoch: 184 | Batch_idx: 200 |  Loss_1: (0.1258) | Acc_1: (95.27%) (24512/25728)\n",
      "Epoch: 184 | Batch_idx: 210 |  Loss_1: (0.1251) | Acc_1: (95.31%) (25742/27008)\n",
      "Epoch: 184 | Batch_idx: 220 |  Loss_1: (0.1252) | Acc_1: (95.30%) (26959/28288)\n",
      "Epoch: 184 | Batch_idx: 230 |  Loss_1: (0.1246) | Acc_1: (95.31%) (28180/29568)\n",
      "Epoch: 184 | Batch_idx: 240 |  Loss_1: (0.1242) | Acc_1: (95.30%) (29398/30848)\n",
      "Epoch: 184 | Batch_idx: 250 |  Loss_1: (0.1238) | Acc_1: (95.32%) (30625/32128)\n",
      "Epoch: 184 | Batch_idx: 260 |  Loss_1: (0.1233) | Acc_1: (95.34%) (31850/33408)\n",
      "Epoch: 184 | Batch_idx: 270 |  Loss_1: (0.1237) | Acc_1: (95.31%) (33062/34688)\n",
      "Epoch: 184 | Batch_idx: 280 |  Loss_1: (0.1223) | Acc_1: (95.37%) (34302/35968)\n",
      "Epoch: 184 | Batch_idx: 290 |  Loss_1: (0.1223) | Acc_1: (95.37%) (35522/37248)\n",
      "Epoch: 184 | Batch_idx: 300 |  Loss_1: (0.1217) | Acc_1: (95.37%) (36746/38528)\n",
      "Epoch: 184 | Batch_idx: 310 |  Loss_1: (0.1221) | Acc_1: (95.36%) (37962/39808)\n",
      "Epoch: 184 | Batch_idx: 320 |  Loss_1: (0.1215) | Acc_1: (95.38%) (39190/41088)\n",
      "Epoch: 184 | Batch_idx: 330 |  Loss_1: (0.1213) | Acc_1: (95.39%) (40416/42368)\n",
      "Epoch: 184 | Batch_idx: 340 |  Loss_1: (0.1209) | Acc_1: (95.42%) (41650/43648)\n",
      "Epoch: 184 | Batch_idx: 350 |  Loss_1: (0.1204) | Acc_1: (95.44%) (42879/44928)\n",
      "Epoch: 184 | Batch_idx: 360 |  Loss_1: (0.1200) | Acc_1: (95.46%) (44108/46208)\n",
      "Epoch: 184 | Batch_idx: 370 |  Loss_1: (0.1195) | Acc_1: (95.47%) (45337/47488)\n",
      "Epoch: 184 | Batch_idx: 380 |  Loss_1: (0.1195) | Acc_1: (95.47%) (46557/48768)\n",
      "Epoch: 184 | Batch_idx: 390 |  Loss_1: (0.1199) | Acc_1: (95.45%) (47727/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3550) | Acc: (92.52%) (9252/10000)\n",
      "Epoch: 185 | Batch_idx: 0 |  Loss_1: (0.0422) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 185 | Batch_idx: 10 |  Loss_1: (0.0989) | Acc_1: (96.31%) (1356/1408)\n",
      "Epoch: 185 | Batch_idx: 20 |  Loss_1: (0.0955) | Acc_1: (96.54%) (2595/2688)\n",
      "Epoch: 185 | Batch_idx: 30 |  Loss_1: (0.1103) | Acc_1: (95.89%) (3805/3968)\n",
      "Epoch: 185 | Batch_idx: 40 |  Loss_1: (0.1055) | Acc_1: (96.02%) (5039/5248)\n",
      "Epoch: 185 | Batch_idx: 50 |  Loss_1: (0.1048) | Acc_1: (96.00%) (6267/6528)\n",
      "Epoch: 185 | Batch_idx: 60 |  Loss_1: (0.1083) | Acc_1: (95.95%) (7492/7808)\n",
      "Epoch: 185 | Batch_idx: 70 |  Loss_1: (0.1112) | Acc_1: (95.79%) (8705/9088)\n",
      "Epoch: 185 | Batch_idx: 80 |  Loss_1: (0.1121) | Acc_1: (95.73%) (9925/10368)\n",
      "Epoch: 185 | Batch_idx: 90 |  Loss_1: (0.1135) | Acc_1: (95.68%) (11145/11648)\n",
      "Epoch: 185 | Batch_idx: 100 |  Loss_1: (0.1156) | Acc_1: (95.65%) (12365/12928)\n",
      "Epoch: 185 | Batch_idx: 110 |  Loss_1: (0.1152) | Acc_1: (95.63%) (13587/14208)\n",
      "Epoch: 185 | Batch_idx: 120 |  Loss_1: (0.1143) | Acc_1: (95.67%) (14818/15488)\n",
      "Epoch: 185 | Batch_idx: 130 |  Loss_1: (0.1143) | Acc_1: (95.68%) (16043/16768)\n",
      "Epoch: 185 | Batch_idx: 140 |  Loss_1: (0.1175) | Acc_1: (95.54%) (17243/18048)\n",
      "Epoch: 185 | Batch_idx: 150 |  Loss_1: (0.1157) | Acc_1: (95.63%) (18483/19328)\n",
      "Epoch: 185 | Batch_idx: 160 |  Loss_1: (0.1153) | Acc_1: (95.61%) (19704/20608)\n",
      "Epoch: 185 | Batch_idx: 170 |  Loss_1: (0.1149) | Acc_1: (95.63%) (20931/21888)\n",
      "Epoch: 185 | Batch_idx: 180 |  Loss_1: (0.1148) | Acc_1: (95.62%) (22154/23168)\n",
      "Epoch: 185 | Batch_idx: 190 |  Loss_1: (0.1138) | Acc_1: (95.66%) (23388/24448)\n",
      "Epoch: 185 | Batch_idx: 200 |  Loss_1: (0.1133) | Acc_1: (95.68%) (24617/25728)\n",
      "Epoch: 185 | Batch_idx: 210 |  Loss_1: (0.1134) | Acc_1: (95.67%) (25839/27008)\n",
      "Epoch: 185 | Batch_idx: 220 |  Loss_1: (0.1133) | Acc_1: (95.67%) (27062/28288)\n",
      "Epoch: 185 | Batch_idx: 230 |  Loss_1: (0.1130) | Acc_1: (95.68%) (28292/29568)\n",
      "Epoch: 185 | Batch_idx: 240 |  Loss_1: (0.1149) | Acc_1: (95.60%) (29492/30848)\n",
      "Epoch: 185 | Batch_idx: 250 |  Loss_1: (0.1148) | Acc_1: (95.61%) (30718/32128)\n",
      "Epoch: 185 | Batch_idx: 260 |  Loss_1: (0.1145) | Acc_1: (95.64%) (31953/33408)\n",
      "Epoch: 185 | Batch_idx: 270 |  Loss_1: (0.1137) | Acc_1: (95.67%) (33186/34688)\n",
      "Epoch: 185 | Batch_idx: 280 |  Loss_1: (0.1137) | Acc_1: (95.68%) (34413/35968)\n",
      "Epoch: 185 | Batch_idx: 290 |  Loss_1: (0.1136) | Acc_1: (95.67%) (35636/37248)\n",
      "Epoch: 185 | Batch_idx: 300 |  Loss_1: (0.1134) | Acc_1: (95.67%) (36861/38528)\n",
      "Epoch: 185 | Batch_idx: 310 |  Loss_1: (0.1136) | Acc_1: (95.67%) (38083/39808)\n",
      "Epoch: 185 | Batch_idx: 320 |  Loss_1: (0.1135) | Acc_1: (95.67%) (39309/41088)\n",
      "Epoch: 185 | Batch_idx: 330 |  Loss_1: (0.1134) | Acc_1: (95.69%) (40542/42368)\n",
      "Epoch: 185 | Batch_idx: 340 |  Loss_1: (0.1136) | Acc_1: (95.68%) (41762/43648)\n",
      "Epoch: 185 | Batch_idx: 350 |  Loss_1: (0.1130) | Acc_1: (95.70%) (42997/44928)\n",
      "Epoch: 185 | Batch_idx: 360 |  Loss_1: (0.1130) | Acc_1: (95.70%) (44223/46208)\n",
      "Epoch: 185 | Batch_idx: 370 |  Loss_1: (0.1130) | Acc_1: (95.71%) (45452/47488)\n",
      "Epoch: 185 | Batch_idx: 380 |  Loss_1: (0.1133) | Acc_1: (95.70%) (46672/48768)\n",
      "Epoch: 185 | Batch_idx: 390 |  Loss_1: (0.1137) | Acc_1: (95.68%) (47840/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3668) | Acc: (92.20%) (9220/10000)\n",
      "Epoch: 186 | Batch_idx: 0 |  Loss_1: (0.1480) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 186 | Batch_idx: 10 |  Loss_1: (0.1093) | Acc_1: (95.88%) (1350/1408)\n",
      "Epoch: 186 | Batch_idx: 20 |  Loss_1: (0.1140) | Acc_1: (95.57%) (2569/2688)\n",
      "Epoch: 186 | Batch_idx: 30 |  Loss_1: (0.1097) | Acc_1: (95.77%) (3800/3968)\n",
      "Epoch: 186 | Batch_idx: 40 |  Loss_1: (0.1115) | Acc_1: (95.66%) (5020/5248)\n",
      "Epoch: 186 | Batch_idx: 50 |  Loss_1: (0.1137) | Acc_1: (95.59%) (6240/6528)\n",
      "Epoch: 186 | Batch_idx: 60 |  Loss_1: (0.1150) | Acc_1: (95.57%) (7462/7808)\n",
      "Epoch: 186 | Batch_idx: 70 |  Loss_1: (0.1141) | Acc_1: (95.61%) (8689/9088)\n",
      "Epoch: 186 | Batch_idx: 80 |  Loss_1: (0.1136) | Acc_1: (95.65%) (9917/10368)\n",
      "Epoch: 186 | Batch_idx: 90 |  Loss_1: (0.1152) | Acc_1: (95.63%) (11139/11648)\n",
      "Epoch: 186 | Batch_idx: 100 |  Loss_1: (0.1155) | Acc_1: (95.64%) (12364/12928)\n",
      "Epoch: 186 | Batch_idx: 110 |  Loss_1: (0.1150) | Acc_1: (95.66%) (13591/14208)\n",
      "Epoch: 186 | Batch_idx: 120 |  Loss_1: (0.1155) | Acc_1: (95.66%) (14816/15488)\n",
      "Epoch: 186 | Batch_idx: 130 |  Loss_1: (0.1166) | Acc_1: (95.59%) (16028/16768)\n",
      "Epoch: 186 | Batch_idx: 140 |  Loss_1: (0.1168) | Acc_1: (95.56%) (17247/18048)\n",
      "Epoch: 186 | Batch_idx: 150 |  Loss_1: (0.1168) | Acc_1: (95.56%) (18470/19328)\n",
      "Epoch: 186 | Batch_idx: 160 |  Loss_1: (0.1173) | Acc_1: (95.56%) (19694/20608)\n",
      "Epoch: 186 | Batch_idx: 170 |  Loss_1: (0.1179) | Acc_1: (95.53%) (20910/21888)\n",
      "Epoch: 186 | Batch_idx: 180 |  Loss_1: (0.1172) | Acc_1: (95.54%) (22134/23168)\n",
      "Epoch: 186 | Batch_idx: 190 |  Loss_1: (0.1162) | Acc_1: (95.57%) (23366/24448)\n",
      "Epoch: 186 | Batch_idx: 200 |  Loss_1: (0.1155) | Acc_1: (95.59%) (24594/25728)\n",
      "Epoch: 186 | Batch_idx: 210 |  Loss_1: (0.1168) | Acc_1: (95.55%) (25806/27008)\n",
      "Epoch: 186 | Batch_idx: 220 |  Loss_1: (0.1164) | Acc_1: (95.55%) (27029/28288)\n",
      "Epoch: 186 | Batch_idx: 230 |  Loss_1: (0.1158) | Acc_1: (95.56%) (28256/29568)\n",
      "Epoch: 186 | Batch_idx: 240 |  Loss_1: (0.1159) | Acc_1: (95.56%) (29477/30848)\n",
      "Epoch: 186 | Batch_idx: 250 |  Loss_1: (0.1157) | Acc_1: (95.56%) (30703/32128)\n",
      "Epoch: 186 | Batch_idx: 260 |  Loss_1: (0.1153) | Acc_1: (95.57%) (31929/33408)\n",
      "Epoch: 186 | Batch_idx: 270 |  Loss_1: (0.1157) | Acc_1: (95.56%) (33148/34688)\n",
      "Epoch: 186 | Batch_idx: 280 |  Loss_1: (0.1160) | Acc_1: (95.56%) (34370/35968)\n",
      "Epoch: 186 | Batch_idx: 290 |  Loss_1: (0.1160) | Acc_1: (95.57%) (35597/37248)\n",
      "Epoch: 186 | Batch_idx: 300 |  Loss_1: (0.1163) | Acc_1: (95.56%) (36818/38528)\n",
      "Epoch: 186 | Batch_idx: 310 |  Loss_1: (0.1163) | Acc_1: (95.56%) (38039/39808)\n",
      "Epoch: 186 | Batch_idx: 320 |  Loss_1: (0.1159) | Acc_1: (95.58%) (39270/41088)\n",
      "Epoch: 186 | Batch_idx: 330 |  Loss_1: (0.1163) | Acc_1: (95.56%) (40485/42368)\n",
      "Epoch: 186 | Batch_idx: 340 |  Loss_1: (0.1168) | Acc_1: (95.53%) (41699/43648)\n",
      "Epoch: 186 | Batch_idx: 350 |  Loss_1: (0.1163) | Acc_1: (95.55%) (42929/44928)\n",
      "Epoch: 186 | Batch_idx: 360 |  Loss_1: (0.1166) | Acc_1: (95.55%) (44151/46208)\n",
      "Epoch: 186 | Batch_idx: 370 |  Loss_1: (0.1160) | Acc_1: (95.58%) (45390/47488)\n",
      "Epoch: 186 | Batch_idx: 380 |  Loss_1: (0.1163) | Acc_1: (95.57%) (46608/48768)\n",
      "Epoch: 186 | Batch_idx: 390 |  Loss_1: (0.1162) | Acc_1: (95.58%) (47789/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3603) | Acc: (92.48%) (9248/10000)\n",
      "Epoch: 187 | Batch_idx: 0 |  Loss_1: (0.1258) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 187 | Batch_idx: 10 |  Loss_1: (0.1077) | Acc_1: (95.88%) (1350/1408)\n",
      "Epoch: 187 | Batch_idx: 20 |  Loss_1: (0.1094) | Acc_1: (95.91%) (2578/2688)\n",
      "Epoch: 187 | Batch_idx: 30 |  Loss_1: (0.1096) | Acc_1: (95.92%) (3806/3968)\n",
      "Epoch: 187 | Batch_idx: 40 |  Loss_1: (0.1087) | Acc_1: (95.87%) (5031/5248)\n",
      "Epoch: 187 | Batch_idx: 50 |  Loss_1: (0.1131) | Acc_1: (95.73%) (6249/6528)\n",
      "Epoch: 187 | Batch_idx: 60 |  Loss_1: (0.1188) | Acc_1: (95.54%) (7460/7808)\n",
      "Epoch: 187 | Batch_idx: 70 |  Loss_1: (0.1191) | Acc_1: (95.49%) (8678/9088)\n",
      "Epoch: 187 | Batch_idx: 80 |  Loss_1: (0.1243) | Acc_1: (95.29%) (9880/10368)\n",
      "Epoch: 187 | Batch_idx: 90 |  Loss_1: (0.1230) | Acc_1: (95.36%) (11108/11648)\n",
      "Epoch: 187 | Batch_idx: 100 |  Loss_1: (0.1222) | Acc_1: (95.38%) (12331/12928)\n",
      "Epoch: 187 | Batch_idx: 110 |  Loss_1: (0.1197) | Acc_1: (95.47%) (13564/14208)\n",
      "Epoch: 187 | Batch_idx: 120 |  Loss_1: (0.1183) | Acc_1: (95.49%) (14789/15488)\n",
      "Epoch: 187 | Batch_idx: 130 |  Loss_1: (0.1193) | Acc_1: (95.44%) (16003/16768)\n",
      "Epoch: 187 | Batch_idx: 140 |  Loss_1: (0.1181) | Acc_1: (95.47%) (17230/18048)\n",
      "Epoch: 187 | Batch_idx: 150 |  Loss_1: (0.1166) | Acc_1: (95.56%) (18469/19328)\n",
      "Epoch: 187 | Batch_idx: 160 |  Loss_1: (0.1174) | Acc_1: (95.55%) (19690/20608)\n",
      "Epoch: 187 | Batch_idx: 170 |  Loss_1: (0.1159) | Acc_1: (95.60%) (20926/21888)\n",
      "Epoch: 187 | Batch_idx: 180 |  Loss_1: (0.1163) | Acc_1: (95.61%) (22151/23168)\n",
      "Epoch: 187 | Batch_idx: 190 |  Loss_1: (0.1164) | Acc_1: (95.60%) (23373/24448)\n",
      "Epoch: 187 | Batch_idx: 200 |  Loss_1: (0.1165) | Acc_1: (95.59%) (24594/25728)\n",
      "Epoch: 187 | Batch_idx: 210 |  Loss_1: (0.1157) | Acc_1: (95.62%) (25826/27008)\n",
      "Epoch: 187 | Batch_idx: 220 |  Loss_1: (0.1162) | Acc_1: (95.58%) (27039/28288)\n",
      "Epoch: 187 | Batch_idx: 230 |  Loss_1: (0.1164) | Acc_1: (95.58%) (28262/29568)\n",
      "Epoch: 187 | Batch_idx: 240 |  Loss_1: (0.1158) | Acc_1: (95.60%) (29492/30848)\n",
      "Epoch: 187 | Batch_idx: 250 |  Loss_1: (0.1166) | Acc_1: (95.55%) (30699/32128)\n",
      "Epoch: 187 | Batch_idx: 260 |  Loss_1: (0.1168) | Acc_1: (95.54%) (31919/33408)\n",
      "Epoch: 187 | Batch_idx: 270 |  Loss_1: (0.1166) | Acc_1: (95.56%) (33148/34688)\n",
      "Epoch: 187 | Batch_idx: 280 |  Loss_1: (0.1165) | Acc_1: (95.57%) (34375/35968)\n",
      "Epoch: 187 | Batch_idx: 290 |  Loss_1: (0.1164) | Acc_1: (95.56%) (35593/37248)\n",
      "Epoch: 187 | Batch_idx: 300 |  Loss_1: (0.1164) | Acc_1: (95.57%) (36821/38528)\n",
      "Epoch: 187 | Batch_idx: 310 |  Loss_1: (0.1160) | Acc_1: (95.58%) (38050/39808)\n",
      "Epoch: 187 | Batch_idx: 320 |  Loss_1: (0.1152) | Acc_1: (95.61%) (39285/41088)\n",
      "Epoch: 187 | Batch_idx: 330 |  Loss_1: (0.1149) | Acc_1: (95.62%) (40514/42368)\n",
      "Epoch: 187 | Batch_idx: 340 |  Loss_1: (0.1150) | Acc_1: (95.61%) (41732/43648)\n",
      "Epoch: 187 | Batch_idx: 350 |  Loss_1: (0.1151) | Acc_1: (95.61%) (42957/44928)\n",
      "Epoch: 187 | Batch_idx: 360 |  Loss_1: (0.1149) | Acc_1: (95.62%) (44183/46208)\n",
      "Epoch: 187 | Batch_idx: 370 |  Loss_1: (0.1143) | Acc_1: (95.63%) (45414/47488)\n",
      "Epoch: 187 | Batch_idx: 380 |  Loss_1: (0.1144) | Acc_1: (95.63%) (46638/48768)\n",
      "Epoch: 187 | Batch_idx: 390 |  Loss_1: (0.1147) | Acc_1: (95.62%) (47812/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3546) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 188 | Batch_idx: 0 |  Loss_1: (0.0897) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 188 | Batch_idx: 10 |  Loss_1: (0.1293) | Acc_1: (94.89%) (1336/1408)\n",
      "Epoch: 188 | Batch_idx: 20 |  Loss_1: (0.1326) | Acc_1: (95.13%) (2557/2688)\n",
      "Epoch: 188 | Batch_idx: 30 |  Loss_1: (0.1373) | Acc_1: (94.78%) (3761/3968)\n",
      "Epoch: 188 | Batch_idx: 40 |  Loss_1: (0.1310) | Acc_1: (95.08%) (4990/5248)\n",
      "Epoch: 188 | Batch_idx: 50 |  Loss_1: (0.1281) | Acc_1: (95.21%) (6215/6528)\n",
      "Epoch: 188 | Batch_idx: 60 |  Loss_1: (0.1243) | Acc_1: (95.30%) (7441/7808)\n",
      "Epoch: 188 | Batch_idx: 70 |  Loss_1: (0.1157) | Acc_1: (95.59%) (8687/9088)\n",
      "Epoch: 188 | Batch_idx: 80 |  Loss_1: (0.1153) | Acc_1: (95.59%) (9911/10368)\n",
      "Epoch: 188 | Batch_idx: 90 |  Loss_1: (0.1137) | Acc_1: (95.67%) (11144/11648)\n",
      "Epoch: 188 | Batch_idx: 100 |  Loss_1: (0.1121) | Acc_1: (95.71%) (12374/12928)\n",
      "Epoch: 188 | Batch_idx: 110 |  Loss_1: (0.1126) | Acc_1: (95.73%) (13601/14208)\n",
      "Epoch: 188 | Batch_idx: 120 |  Loss_1: (0.1117) | Acc_1: (95.76%) (14831/15488)\n",
      "Epoch: 188 | Batch_idx: 130 |  Loss_1: (0.1112) | Acc_1: (95.77%) (16058/16768)\n",
      "Epoch: 188 | Batch_idx: 140 |  Loss_1: (0.1124) | Acc_1: (95.72%) (17276/18048)\n",
      "Epoch: 188 | Batch_idx: 150 |  Loss_1: (0.1121) | Acc_1: (95.75%) (18506/19328)\n",
      "Epoch: 188 | Batch_idx: 160 |  Loss_1: (0.1141) | Acc_1: (95.66%) (19714/20608)\n",
      "Epoch: 188 | Batch_idx: 170 |  Loss_1: (0.1149) | Acc_1: (95.64%) (20933/21888)\n",
      "Epoch: 188 | Batch_idx: 180 |  Loss_1: (0.1168) | Acc_1: (95.55%) (22137/23168)\n",
      "Epoch: 188 | Batch_idx: 190 |  Loss_1: (0.1178) | Acc_1: (95.51%) (23350/24448)\n",
      "Epoch: 188 | Batch_idx: 200 |  Loss_1: (0.1185) | Acc_1: (95.47%) (24562/25728)\n",
      "Epoch: 188 | Batch_idx: 210 |  Loss_1: (0.1202) | Acc_1: (95.43%) (25773/27008)\n",
      "Epoch: 188 | Batch_idx: 220 |  Loss_1: (0.1197) | Acc_1: (95.45%) (27001/28288)\n",
      "Epoch: 188 | Batch_idx: 230 |  Loss_1: (0.1194) | Acc_1: (95.44%) (28221/29568)\n",
      "Epoch: 188 | Batch_idx: 240 |  Loss_1: (0.1194) | Acc_1: (95.47%) (29451/30848)\n",
      "Epoch: 188 | Batch_idx: 250 |  Loss_1: (0.1190) | Acc_1: (95.48%) (30675/32128)\n",
      "Epoch: 188 | Batch_idx: 260 |  Loss_1: (0.1187) | Acc_1: (95.49%) (31902/33408)\n",
      "Epoch: 188 | Batch_idx: 270 |  Loss_1: (0.1189) | Acc_1: (95.51%) (33130/34688)\n",
      "Epoch: 188 | Batch_idx: 280 |  Loss_1: (0.1185) | Acc_1: (95.52%) (34355/35968)\n",
      "Epoch: 188 | Batch_idx: 290 |  Loss_1: (0.1183) | Acc_1: (95.52%) (35578/37248)\n",
      "Epoch: 188 | Batch_idx: 300 |  Loss_1: (0.1178) | Acc_1: (95.54%) (36808/38528)\n",
      "Epoch: 188 | Batch_idx: 310 |  Loss_1: (0.1184) | Acc_1: (95.51%) (38022/39808)\n",
      "Epoch: 188 | Batch_idx: 320 |  Loss_1: (0.1185) | Acc_1: (95.52%) (39247/41088)\n",
      "Epoch: 188 | Batch_idx: 330 |  Loss_1: (0.1192) | Acc_1: (95.50%) (40460/42368)\n",
      "Epoch: 188 | Batch_idx: 340 |  Loss_1: (0.1185) | Acc_1: (95.53%) (41695/43648)\n",
      "Epoch: 188 | Batch_idx: 350 |  Loss_1: (0.1183) | Acc_1: (95.53%) (42921/44928)\n",
      "Epoch: 188 | Batch_idx: 360 |  Loss_1: (0.1181) | Acc_1: (95.54%) (44146/46208)\n",
      "Epoch: 188 | Batch_idx: 370 |  Loss_1: (0.1184) | Acc_1: (95.53%) (45364/47488)\n",
      "Epoch: 188 | Batch_idx: 380 |  Loss_1: (0.1181) | Acc_1: (95.53%) (46587/48768)\n",
      "Epoch: 188 | Batch_idx: 390 |  Loss_1: (0.1183) | Acc_1: (95.51%) (47757/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3779) | Acc: (92.11%) (9211/10000)\n",
      "Epoch: 189 | Batch_idx: 0 |  Loss_1: (0.0948) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 189 | Batch_idx: 10 |  Loss_1: (0.1311) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 189 | Batch_idx: 20 |  Loss_1: (0.1196) | Acc_1: (95.35%) (2563/2688)\n",
      "Epoch: 189 | Batch_idx: 30 |  Loss_1: (0.1201) | Acc_1: (95.39%) (3785/3968)\n",
      "Epoch: 189 | Batch_idx: 40 |  Loss_1: (0.1194) | Acc_1: (95.45%) (5009/5248)\n",
      "Epoch: 189 | Batch_idx: 50 |  Loss_1: (0.1181) | Acc_1: (95.47%) (6232/6528)\n",
      "Epoch: 189 | Batch_idx: 60 |  Loss_1: (0.1163) | Acc_1: (95.56%) (7461/7808)\n",
      "Epoch: 189 | Batch_idx: 70 |  Loss_1: (0.1196) | Acc_1: (95.40%) (8670/9088)\n",
      "Epoch: 189 | Batch_idx: 80 |  Loss_1: (0.1169) | Acc_1: (95.48%) (9899/10368)\n",
      "Epoch: 189 | Batch_idx: 90 |  Loss_1: (0.1173) | Acc_1: (95.52%) (11126/11648)\n",
      "Epoch: 189 | Batch_idx: 100 |  Loss_1: (0.1159) | Acc_1: (95.55%) (12353/12928)\n",
      "Epoch: 189 | Batch_idx: 110 |  Loss_1: (0.1170) | Acc_1: (95.52%) (13571/14208)\n",
      "Epoch: 189 | Batch_idx: 120 |  Loss_1: (0.1184) | Acc_1: (95.48%) (14788/15488)\n",
      "Epoch: 189 | Batch_idx: 130 |  Loss_1: (0.1169) | Acc_1: (95.55%) (16021/16768)\n",
      "Epoch: 189 | Batch_idx: 140 |  Loss_1: (0.1171) | Acc_1: (95.56%) (17247/18048)\n",
      "Epoch: 189 | Batch_idx: 150 |  Loss_1: (0.1171) | Acc_1: (95.53%) (18465/19328)\n",
      "Epoch: 189 | Batch_idx: 160 |  Loss_1: (0.1179) | Acc_1: (95.50%) (19681/20608)\n",
      "Epoch: 189 | Batch_idx: 170 |  Loss_1: (0.1184) | Acc_1: (95.49%) (20900/21888)\n",
      "Epoch: 189 | Batch_idx: 180 |  Loss_1: (0.1196) | Acc_1: (95.42%) (22108/23168)\n",
      "Epoch: 189 | Batch_idx: 190 |  Loss_1: (0.1189) | Acc_1: (95.46%) (23337/24448)\n",
      "Epoch: 189 | Batch_idx: 200 |  Loss_1: (0.1185) | Acc_1: (95.46%) (24560/25728)\n",
      "Epoch: 189 | Batch_idx: 210 |  Loss_1: (0.1188) | Acc_1: (95.46%) (25781/27008)\n",
      "Epoch: 189 | Batch_idx: 220 |  Loss_1: (0.1197) | Acc_1: (95.42%) (26992/28288)\n",
      "Epoch: 189 | Batch_idx: 230 |  Loss_1: (0.1201) | Acc_1: (95.39%) (28204/29568)\n",
      "Epoch: 189 | Batch_idx: 240 |  Loss_1: (0.1199) | Acc_1: (95.41%) (29433/30848)\n",
      "Epoch: 189 | Batch_idx: 250 |  Loss_1: (0.1191) | Acc_1: (95.44%) (30664/32128)\n",
      "Epoch: 189 | Batch_idx: 260 |  Loss_1: (0.1200) | Acc_1: (95.42%) (31877/33408)\n",
      "Epoch: 189 | Batch_idx: 270 |  Loss_1: (0.1199) | Acc_1: (95.41%) (33097/34688)\n",
      "Epoch: 189 | Batch_idx: 280 |  Loss_1: (0.1198) | Acc_1: (95.42%) (34320/35968)\n",
      "Epoch: 189 | Batch_idx: 290 |  Loss_1: (0.1196) | Acc_1: (95.42%) (35543/37248)\n",
      "Epoch: 189 | Batch_idx: 300 |  Loss_1: (0.1200) | Acc_1: (95.40%) (36757/38528)\n",
      "Epoch: 189 | Batch_idx: 310 |  Loss_1: (0.1197) | Acc_1: (95.42%) (37983/39808)\n",
      "Epoch: 189 | Batch_idx: 320 |  Loss_1: (0.1191) | Acc_1: (95.44%) (39213/41088)\n",
      "Epoch: 189 | Batch_idx: 330 |  Loss_1: (0.1189) | Acc_1: (95.45%) (40441/42368)\n",
      "Epoch: 189 | Batch_idx: 340 |  Loss_1: (0.1189) | Acc_1: (95.46%) (41665/43648)\n",
      "Epoch: 189 | Batch_idx: 350 |  Loss_1: (0.1186) | Acc_1: (95.47%) (42892/44928)\n",
      "Epoch: 189 | Batch_idx: 360 |  Loss_1: (0.1192) | Acc_1: (95.43%) (44098/46208)\n",
      "Epoch: 189 | Batch_idx: 370 |  Loss_1: (0.1195) | Acc_1: (95.43%) (45317/47488)\n",
      "Epoch: 189 | Batch_idx: 380 |  Loss_1: (0.1198) | Acc_1: (95.42%) (46533/48768)\n",
      "Epoch: 189 | Batch_idx: 390 |  Loss_1: (0.1205) | Acc_1: (95.40%) (47700/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3507) | Acc: (92.59%) (9259/10000)\n",
      "Epoch: 190 | Batch_idx: 0 |  Loss_1: (0.1208) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 190 | Batch_idx: 10 |  Loss_1: (0.1161) | Acc_1: (95.60%) (1346/1408)\n",
      "Epoch: 190 | Batch_idx: 20 |  Loss_1: (0.1212) | Acc_1: (95.46%) (2566/2688)\n",
      "Epoch: 190 | Batch_idx: 30 |  Loss_1: (0.1172) | Acc_1: (95.59%) (3793/3968)\n",
      "Epoch: 190 | Batch_idx: 40 |  Loss_1: (0.1121) | Acc_1: (95.69%) (5022/5248)\n",
      "Epoch: 190 | Batch_idx: 50 |  Loss_1: (0.1105) | Acc_1: (95.73%) (6249/6528)\n",
      "Epoch: 190 | Batch_idx: 60 |  Loss_1: (0.1103) | Acc_1: (95.75%) (7476/7808)\n",
      "Epoch: 190 | Batch_idx: 70 |  Loss_1: (0.1096) | Acc_1: (95.82%) (8708/9088)\n",
      "Epoch: 190 | Batch_idx: 80 |  Loss_1: (0.1078) | Acc_1: (95.88%) (9941/10368)\n",
      "Epoch: 190 | Batch_idx: 90 |  Loss_1: (0.1119) | Acc_1: (95.72%) (11149/11648)\n",
      "Epoch: 190 | Batch_idx: 100 |  Loss_1: (0.1121) | Acc_1: (95.68%) (12370/12928)\n",
      "Epoch: 190 | Batch_idx: 110 |  Loss_1: (0.1145) | Acc_1: (95.54%) (13575/14208)\n",
      "Epoch: 190 | Batch_idx: 120 |  Loss_1: (0.1155) | Acc_1: (95.52%) (14794/15488)\n",
      "Epoch: 190 | Batch_idx: 130 |  Loss_1: (0.1156) | Acc_1: (95.53%) (16018/16768)\n",
      "Epoch: 190 | Batch_idx: 140 |  Loss_1: (0.1159) | Acc_1: (95.52%) (17239/18048)\n",
      "Epoch: 190 | Batch_idx: 150 |  Loss_1: (0.1151) | Acc_1: (95.56%) (18470/19328)\n",
      "Epoch: 190 | Batch_idx: 160 |  Loss_1: (0.1153) | Acc_1: (95.56%) (19692/20608)\n",
      "Epoch: 190 | Batch_idx: 170 |  Loss_1: (0.1141) | Acc_1: (95.61%) (20927/21888)\n",
      "Epoch: 190 | Batch_idx: 180 |  Loss_1: (0.1136) | Acc_1: (95.64%) (22157/23168)\n",
      "Epoch: 190 | Batch_idx: 190 |  Loss_1: (0.1131) | Acc_1: (95.65%) (23385/24448)\n",
      "Epoch: 190 | Batch_idx: 200 |  Loss_1: (0.1141) | Acc_1: (95.62%) (24601/25728)\n",
      "Epoch: 190 | Batch_idx: 210 |  Loss_1: (0.1153) | Acc_1: (95.57%) (25811/27008)\n",
      "Epoch: 190 | Batch_idx: 220 |  Loss_1: (0.1159) | Acc_1: (95.56%) (27033/28288)\n",
      "Epoch: 190 | Batch_idx: 230 |  Loss_1: (0.1155) | Acc_1: (95.58%) (28260/29568)\n",
      "Epoch: 190 | Batch_idx: 240 |  Loss_1: (0.1150) | Acc_1: (95.60%) (29490/30848)\n",
      "Epoch: 190 | Batch_idx: 250 |  Loss_1: (0.1156) | Acc_1: (95.57%) (30705/32128)\n",
      "Epoch: 190 | Batch_idx: 260 |  Loss_1: (0.1161) | Acc_1: (95.53%) (31915/33408)\n",
      "Epoch: 190 | Batch_idx: 270 |  Loss_1: (0.1167) | Acc_1: (95.52%) (33135/34688)\n",
      "Epoch: 190 | Batch_idx: 280 |  Loss_1: (0.1168) | Acc_1: (95.52%) (34358/35968)\n",
      "Epoch: 190 | Batch_idx: 290 |  Loss_1: (0.1174) | Acc_1: (95.49%) (35568/37248)\n",
      "Epoch: 190 | Batch_idx: 300 |  Loss_1: (0.1175) | Acc_1: (95.49%) (36789/38528)\n",
      "Epoch: 190 | Batch_idx: 310 |  Loss_1: (0.1176) | Acc_1: (95.49%) (38011/39808)\n",
      "Epoch: 190 | Batch_idx: 320 |  Loss_1: (0.1182) | Acc_1: (95.46%) (39224/41088)\n",
      "Epoch: 190 | Batch_idx: 330 |  Loss_1: (0.1178) | Acc_1: (95.48%) (40455/42368)\n",
      "Epoch: 190 | Batch_idx: 340 |  Loss_1: (0.1180) | Acc_1: (95.48%) (41674/43648)\n",
      "Epoch: 190 | Batch_idx: 350 |  Loss_1: (0.1180) | Acc_1: (95.47%) (42891/44928)\n",
      "Epoch: 190 | Batch_idx: 360 |  Loss_1: (0.1180) | Acc_1: (95.47%) (44113/46208)\n",
      "Epoch: 190 | Batch_idx: 370 |  Loss_1: (0.1183) | Acc_1: (95.46%) (45331/47488)\n",
      "Epoch: 190 | Batch_idx: 380 |  Loss_1: (0.1185) | Acc_1: (95.45%) (46550/48768)\n",
      "Epoch: 190 | Batch_idx: 390 |  Loss_1: (0.1184) | Acc_1: (95.47%) (47734/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3539) | Acc: (92.44%) (9244/10000)\n",
      "Epoch: 191 | Batch_idx: 0 |  Loss_1: (0.1095) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 191 | Batch_idx: 10 |  Loss_1: (0.1096) | Acc_1: (96.09%) (1353/1408)\n",
      "Epoch: 191 | Batch_idx: 20 |  Loss_1: (0.1130) | Acc_1: (95.76%) (2574/2688)\n",
      "Epoch: 191 | Batch_idx: 30 |  Loss_1: (0.1103) | Acc_1: (95.82%) (3802/3968)\n",
      "Epoch: 191 | Batch_idx: 40 |  Loss_1: (0.1186) | Acc_1: (95.64%) (5019/5248)\n",
      "Epoch: 191 | Batch_idx: 50 |  Loss_1: (0.1176) | Acc_1: (95.60%) (6241/6528)\n",
      "Epoch: 191 | Batch_idx: 60 |  Loss_1: (0.1178) | Acc_1: (95.57%) (7462/7808)\n",
      "Epoch: 191 | Batch_idx: 70 |  Loss_1: (0.1194) | Acc_1: (95.49%) (8678/9088)\n",
      "Epoch: 191 | Batch_idx: 80 |  Loss_1: (0.1213) | Acc_1: (95.41%) (9892/10368)\n",
      "Epoch: 191 | Batch_idx: 90 |  Loss_1: (0.1220) | Acc_1: (95.41%) (11113/11648)\n",
      "Epoch: 191 | Batch_idx: 100 |  Loss_1: (0.1207) | Acc_1: (95.45%) (12340/12928)\n",
      "Epoch: 191 | Batch_idx: 110 |  Loss_1: (0.1210) | Acc_1: (95.47%) (13564/14208)\n",
      "Epoch: 191 | Batch_idx: 120 |  Loss_1: (0.1207) | Acc_1: (95.46%) (14785/15488)\n",
      "Epoch: 191 | Batch_idx: 130 |  Loss_1: (0.1194) | Acc_1: (95.49%) (16012/16768)\n",
      "Epoch: 191 | Batch_idx: 140 |  Loss_1: (0.1200) | Acc_1: (95.48%) (17233/18048)\n",
      "Epoch: 191 | Batch_idx: 150 |  Loss_1: (0.1200) | Acc_1: (95.50%) (18458/19328)\n",
      "Epoch: 191 | Batch_idx: 160 |  Loss_1: (0.1207) | Acc_1: (95.49%) (19678/20608)\n",
      "Epoch: 191 | Batch_idx: 170 |  Loss_1: (0.1209) | Acc_1: (95.47%) (20897/21888)\n",
      "Epoch: 191 | Batch_idx: 180 |  Loss_1: (0.1202) | Acc_1: (95.50%) (22125/23168)\n",
      "Epoch: 191 | Batch_idx: 190 |  Loss_1: (0.1196) | Acc_1: (95.51%) (23350/24448)\n",
      "Epoch: 191 | Batch_idx: 200 |  Loss_1: (0.1203) | Acc_1: (95.48%) (24565/25728)\n",
      "Epoch: 191 | Batch_idx: 210 |  Loss_1: (0.1222) | Acc_1: (95.42%) (25770/27008)\n",
      "Epoch: 191 | Batch_idx: 220 |  Loss_1: (0.1211) | Acc_1: (95.46%) (27004/28288)\n",
      "Epoch: 191 | Batch_idx: 230 |  Loss_1: (0.1209) | Acc_1: (95.46%) (28227/29568)\n",
      "Epoch: 191 | Batch_idx: 240 |  Loss_1: (0.1205) | Acc_1: (95.48%) (29454/30848)\n",
      "Epoch: 191 | Batch_idx: 250 |  Loss_1: (0.1201) | Acc_1: (95.51%) (30685/32128)\n",
      "Epoch: 191 | Batch_idx: 260 |  Loss_1: (0.1195) | Acc_1: (95.54%) (31918/33408)\n",
      "Epoch: 191 | Batch_idx: 270 |  Loss_1: (0.1194) | Acc_1: (95.55%) (33143/34688)\n",
      "Epoch: 191 | Batch_idx: 280 |  Loss_1: (0.1183) | Acc_1: (95.58%) (34380/35968)\n",
      "Epoch: 191 | Batch_idx: 290 |  Loss_1: (0.1173) | Acc_1: (95.62%) (35615/37248)\n",
      "Epoch: 191 | Batch_idx: 300 |  Loss_1: (0.1167) | Acc_1: (95.64%) (36849/38528)\n",
      "Epoch: 191 | Batch_idx: 310 |  Loss_1: (0.1168) | Acc_1: (95.65%) (38076/39808)\n",
      "Epoch: 191 | Batch_idx: 320 |  Loss_1: (0.1169) | Acc_1: (95.64%) (39297/41088)\n",
      "Epoch: 191 | Batch_idx: 330 |  Loss_1: (0.1171) | Acc_1: (95.62%) (40514/42368)\n",
      "Epoch: 191 | Batch_idx: 340 |  Loss_1: (0.1168) | Acc_1: (95.63%) (41742/43648)\n",
      "Epoch: 191 | Batch_idx: 350 |  Loss_1: (0.1159) | Acc_1: (95.67%) (42981/44928)\n",
      "Epoch: 191 | Batch_idx: 360 |  Loss_1: (0.1164) | Acc_1: (95.66%) (44203/46208)\n",
      "Epoch: 191 | Batch_idx: 370 |  Loss_1: (0.1166) | Acc_1: (95.66%) (45428/47488)\n",
      "Epoch: 191 | Batch_idx: 380 |  Loss_1: (0.1161) | Acc_1: (95.68%) (46660/48768)\n",
      "Epoch: 191 | Batch_idx: 390 |  Loss_1: (0.1167) | Acc_1: (95.66%) (47831/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3462) | Acc: (92.28%) (9228/10000)\n",
      "Epoch: 192 | Batch_idx: 0 |  Loss_1: (0.0781) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 192 | Batch_idx: 10 |  Loss_1: (0.1257) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 192 | Batch_idx: 20 |  Loss_1: (0.1154) | Acc_1: (95.72%) (2573/2688)\n",
      "Epoch: 192 | Batch_idx: 30 |  Loss_1: (0.1224) | Acc_1: (95.44%) (3787/3968)\n",
      "Epoch: 192 | Batch_idx: 40 |  Loss_1: (0.1220) | Acc_1: (95.50%) (5012/5248)\n",
      "Epoch: 192 | Batch_idx: 50 |  Loss_1: (0.1294) | Acc_1: (95.14%) (6211/6528)\n",
      "Epoch: 192 | Batch_idx: 60 |  Loss_1: (0.1315) | Acc_1: (95.07%) (7423/7808)\n",
      "Epoch: 192 | Batch_idx: 70 |  Loss_1: (0.1303) | Acc_1: (95.14%) (8646/9088)\n",
      "Epoch: 192 | Batch_idx: 80 |  Loss_1: (0.1276) | Acc_1: (95.19%) (9869/10368)\n",
      "Epoch: 192 | Batch_idx: 90 |  Loss_1: (0.1250) | Acc_1: (95.33%) (11104/11648)\n",
      "Epoch: 192 | Batch_idx: 100 |  Loss_1: (0.1253) | Acc_1: (95.31%) (12322/12928)\n",
      "Epoch: 192 | Batch_idx: 110 |  Loss_1: (0.1265) | Acc_1: (95.26%) (13534/14208)\n",
      "Epoch: 192 | Batch_idx: 120 |  Loss_1: (0.1246) | Acc_1: (95.32%) (14763/15488)\n",
      "Epoch: 192 | Batch_idx: 130 |  Loss_1: (0.1257) | Acc_1: (95.29%) (15978/16768)\n",
      "Epoch: 192 | Batch_idx: 140 |  Loss_1: (0.1253) | Acc_1: (95.31%) (17202/18048)\n",
      "Epoch: 192 | Batch_idx: 150 |  Loss_1: (0.1254) | Acc_1: (95.31%) (18421/19328)\n",
      "Epoch: 192 | Batch_idx: 160 |  Loss_1: (0.1243) | Acc_1: (95.34%) (19648/20608)\n",
      "Epoch: 192 | Batch_idx: 170 |  Loss_1: (0.1231) | Acc_1: (95.39%) (20880/21888)\n",
      "Epoch: 192 | Batch_idx: 180 |  Loss_1: (0.1221) | Acc_1: (95.42%) (22108/23168)\n",
      "Epoch: 192 | Batch_idx: 190 |  Loss_1: (0.1215) | Acc_1: (95.48%) (23342/24448)\n",
      "Epoch: 192 | Batch_idx: 200 |  Loss_1: (0.1198) | Acc_1: (95.55%) (24582/25728)\n",
      "Epoch: 192 | Batch_idx: 210 |  Loss_1: (0.1193) | Acc_1: (95.56%) (25809/27008)\n",
      "Epoch: 192 | Batch_idx: 220 |  Loss_1: (0.1178) | Acc_1: (95.62%) (27050/28288)\n",
      "Epoch: 192 | Batch_idx: 230 |  Loss_1: (0.1176) | Acc_1: (95.62%) (28274/29568)\n",
      "Epoch: 192 | Batch_idx: 240 |  Loss_1: (0.1175) | Acc_1: (95.62%) (29498/30848)\n",
      "Epoch: 192 | Batch_idx: 250 |  Loss_1: (0.1171) | Acc_1: (95.62%) (30721/32128)\n",
      "Epoch: 192 | Batch_idx: 260 |  Loss_1: (0.1169) | Acc_1: (95.63%) (31948/33408)\n",
      "Epoch: 192 | Batch_idx: 270 |  Loss_1: (0.1176) | Acc_1: (95.60%) (33163/34688)\n",
      "Epoch: 192 | Batch_idx: 280 |  Loss_1: (0.1178) | Acc_1: (95.60%) (34386/35968)\n",
      "Epoch: 192 | Batch_idx: 290 |  Loss_1: (0.1170) | Acc_1: (95.62%) (35618/37248)\n",
      "Epoch: 192 | Batch_idx: 300 |  Loss_1: (0.1177) | Acc_1: (95.60%) (36833/38528)\n",
      "Epoch: 192 | Batch_idx: 310 |  Loss_1: (0.1174) | Acc_1: (95.61%) (38062/39808)\n",
      "Epoch: 192 | Batch_idx: 320 |  Loss_1: (0.1179) | Acc_1: (95.59%) (39276/41088)\n",
      "Epoch: 192 | Batch_idx: 330 |  Loss_1: (0.1183) | Acc_1: (95.58%) (40494/42368)\n",
      "Epoch: 192 | Batch_idx: 340 |  Loss_1: (0.1183) | Acc_1: (95.57%) (41716/43648)\n",
      "Epoch: 192 | Batch_idx: 350 |  Loss_1: (0.1183) | Acc_1: (95.58%) (42941/44928)\n",
      "Epoch: 192 | Batch_idx: 360 |  Loss_1: (0.1186) | Acc_1: (95.56%) (44156/46208)\n",
      "Epoch: 192 | Batch_idx: 370 |  Loss_1: (0.1186) | Acc_1: (95.56%) (45380/47488)\n",
      "Epoch: 192 | Batch_idx: 380 |  Loss_1: (0.1173) | Acc_1: (95.61%) (46626/48768)\n",
      "Epoch: 192 | Batch_idx: 390 |  Loss_1: (0.1165) | Acc_1: (95.64%) (47819/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3465) | Acc: (92.44%) (9244/10000)\n",
      "Epoch: 193 | Batch_idx: 0 |  Loss_1: (0.1283) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 193 | Batch_idx: 10 |  Loss_1: (0.1202) | Acc_1: (95.17%) (1340/1408)\n",
      "Epoch: 193 | Batch_idx: 20 |  Loss_1: (0.1190) | Acc_1: (95.24%) (2560/2688)\n",
      "Epoch: 193 | Batch_idx: 30 |  Loss_1: (0.1160) | Acc_1: (95.44%) (3787/3968)\n",
      "Epoch: 193 | Batch_idx: 40 |  Loss_1: (0.1160) | Acc_1: (95.41%) (5007/5248)\n",
      "Epoch: 193 | Batch_idx: 50 |  Loss_1: (0.1164) | Acc_1: (95.48%) (6233/6528)\n",
      "Epoch: 193 | Batch_idx: 60 |  Loss_1: (0.1193) | Acc_1: (95.36%) (7446/7808)\n",
      "Epoch: 193 | Batch_idx: 70 |  Loss_1: (0.1176) | Acc_1: (95.47%) (8676/9088)\n",
      "Epoch: 193 | Batch_idx: 80 |  Loss_1: (0.1175) | Acc_1: (95.48%) (9899/10368)\n",
      "Epoch: 193 | Batch_idx: 90 |  Loss_1: (0.1163) | Acc_1: (95.50%) (11124/11648)\n",
      "Epoch: 193 | Batch_idx: 100 |  Loss_1: (0.1165) | Acc_1: (95.50%) (12346/12928)\n",
      "Epoch: 193 | Batch_idx: 110 |  Loss_1: (0.1134) | Acc_1: (95.65%) (13590/14208)\n",
      "Epoch: 193 | Batch_idx: 120 |  Loss_1: (0.1143) | Acc_1: (95.64%) (14812/15488)\n",
      "Epoch: 193 | Batch_idx: 130 |  Loss_1: (0.1156) | Acc_1: (95.60%) (16030/16768)\n",
      "Epoch: 193 | Batch_idx: 140 |  Loss_1: (0.1162) | Acc_1: (95.59%) (17252/18048)\n",
      "Epoch: 193 | Batch_idx: 150 |  Loss_1: (0.1167) | Acc_1: (95.59%) (18476/19328)\n",
      "Epoch: 193 | Batch_idx: 160 |  Loss_1: (0.1162) | Acc_1: (95.64%) (19710/20608)\n",
      "Epoch: 193 | Batch_idx: 170 |  Loss_1: (0.1172) | Acc_1: (95.59%) (20923/21888)\n",
      "Epoch: 193 | Batch_idx: 180 |  Loss_1: (0.1166) | Acc_1: (95.59%) (22147/23168)\n",
      "Epoch: 193 | Batch_idx: 190 |  Loss_1: (0.1171) | Acc_1: (95.59%) (23370/24448)\n",
      "Epoch: 193 | Batch_idx: 200 |  Loss_1: (0.1158) | Acc_1: (95.63%) (24604/25728)\n",
      "Epoch: 193 | Batch_idx: 210 |  Loss_1: (0.1156) | Acc_1: (95.62%) (25826/27008)\n",
      "Epoch: 193 | Batch_idx: 220 |  Loss_1: (0.1151) | Acc_1: (95.65%) (27057/28288)\n",
      "Epoch: 193 | Batch_idx: 230 |  Loss_1: (0.1158) | Acc_1: (95.62%) (28274/29568)\n",
      "Epoch: 193 | Batch_idx: 240 |  Loss_1: (0.1157) | Acc_1: (95.62%) (29498/30848)\n",
      "Epoch: 193 | Batch_idx: 250 |  Loss_1: (0.1162) | Acc_1: (95.60%) (30715/32128)\n",
      "Epoch: 193 | Batch_idx: 260 |  Loss_1: (0.1163) | Acc_1: (95.60%) (31938/33408)\n",
      "Epoch: 193 | Batch_idx: 270 |  Loss_1: (0.1164) | Acc_1: (95.60%) (33160/34688)\n",
      "Epoch: 193 | Batch_idx: 280 |  Loss_1: (0.1159) | Acc_1: (95.61%) (34390/35968)\n",
      "Epoch: 193 | Batch_idx: 290 |  Loss_1: (0.1153) | Acc_1: (95.64%) (35624/37248)\n",
      "Epoch: 193 | Batch_idx: 300 |  Loss_1: (0.1151) | Acc_1: (95.64%) (36850/38528)\n",
      "Epoch: 193 | Batch_idx: 310 |  Loss_1: (0.1144) | Acc_1: (95.67%) (38083/39808)\n",
      "Epoch: 193 | Batch_idx: 320 |  Loss_1: (0.1146) | Acc_1: (95.67%) (39307/41088)\n",
      "Epoch: 193 | Batch_idx: 330 |  Loss_1: (0.1146) | Acc_1: (95.68%) (40536/42368)\n",
      "Epoch: 193 | Batch_idx: 340 |  Loss_1: (0.1147) | Acc_1: (95.67%) (41760/43648)\n",
      "Epoch: 193 | Batch_idx: 350 |  Loss_1: (0.1150) | Acc_1: (95.66%) (42978/44928)\n",
      "Epoch: 193 | Batch_idx: 360 |  Loss_1: (0.1148) | Acc_1: (95.67%) (44205/46208)\n",
      "Epoch: 193 | Batch_idx: 370 |  Loss_1: (0.1148) | Acc_1: (95.66%) (45429/47488)\n",
      "Epoch: 193 | Batch_idx: 380 |  Loss_1: (0.1146) | Acc_1: (95.68%) (46659/48768)\n",
      "Epoch: 193 | Batch_idx: 390 |  Loss_1: (0.1148) | Acc_1: (95.67%) (47833/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3542) | Acc: (92.38%) (9238/10000)\n",
      "Epoch: 194 | Batch_idx: 0 |  Loss_1: (0.0949) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 194 | Batch_idx: 10 |  Loss_1: (0.1175) | Acc_1: (95.45%) (1344/1408)\n",
      "Epoch: 194 | Batch_idx: 20 |  Loss_1: (0.1110) | Acc_1: (95.83%) (2576/2688)\n",
      "Epoch: 194 | Batch_idx: 30 |  Loss_1: (0.1131) | Acc_1: (95.69%) (3797/3968)\n",
      "Epoch: 194 | Batch_idx: 40 |  Loss_1: (0.1143) | Acc_1: (95.69%) (5022/5248)\n",
      "Epoch: 194 | Batch_idx: 50 |  Loss_1: (0.1172) | Acc_1: (95.53%) (6236/6528)\n",
      "Epoch: 194 | Batch_idx: 60 |  Loss_1: (0.1164) | Acc_1: (95.58%) (7463/7808)\n",
      "Epoch: 194 | Batch_idx: 70 |  Loss_1: (0.1194) | Acc_1: (95.41%) (8671/9088)\n",
      "Epoch: 194 | Batch_idx: 80 |  Loss_1: (0.1198) | Acc_1: (95.39%) (9890/10368)\n",
      "Epoch: 194 | Batch_idx: 90 |  Loss_1: (0.1233) | Acc_1: (95.24%) (11093/11648)\n",
      "Epoch: 194 | Batch_idx: 100 |  Loss_1: (0.1249) | Acc_1: (95.19%) (12306/12928)\n",
      "Epoch: 194 | Batch_idx: 110 |  Loss_1: (0.1227) | Acc_1: (95.27%) (13536/14208)\n",
      "Epoch: 194 | Batch_idx: 120 |  Loss_1: (0.1213) | Acc_1: (95.34%) (14766/15488)\n",
      "Epoch: 194 | Batch_idx: 130 |  Loss_1: (0.1223) | Acc_1: (95.27%) (15975/16768)\n",
      "Epoch: 194 | Batch_idx: 140 |  Loss_1: (0.1225) | Acc_1: (95.28%) (17197/18048)\n",
      "Epoch: 194 | Batch_idx: 150 |  Loss_1: (0.1230) | Acc_1: (95.25%) (18409/19328)\n",
      "Epoch: 194 | Batch_idx: 160 |  Loss_1: (0.1233) | Acc_1: (95.24%) (19627/20608)\n",
      "Epoch: 194 | Batch_idx: 170 |  Loss_1: (0.1223) | Acc_1: (95.27%) (20853/21888)\n",
      "Epoch: 194 | Batch_idx: 180 |  Loss_1: (0.1214) | Acc_1: (95.28%) (22074/23168)\n",
      "Epoch: 194 | Batch_idx: 190 |  Loss_1: (0.1213) | Acc_1: (95.27%) (23291/24448)\n",
      "Epoch: 194 | Batch_idx: 200 |  Loss_1: (0.1212) | Acc_1: (95.29%) (24517/25728)\n",
      "Epoch: 194 | Batch_idx: 210 |  Loss_1: (0.1216) | Acc_1: (95.28%) (25733/27008)\n",
      "Epoch: 194 | Batch_idx: 220 |  Loss_1: (0.1219) | Acc_1: (95.26%) (26948/28288)\n",
      "Epoch: 194 | Batch_idx: 230 |  Loss_1: (0.1219) | Acc_1: (95.27%) (28170/29568)\n",
      "Epoch: 194 | Batch_idx: 240 |  Loss_1: (0.1226) | Acc_1: (95.24%) (29381/30848)\n",
      "Epoch: 194 | Batch_idx: 250 |  Loss_1: (0.1218) | Acc_1: (95.28%) (30611/32128)\n",
      "Epoch: 194 | Batch_idx: 260 |  Loss_1: (0.1211) | Acc_1: (95.30%) (31839/33408)\n",
      "Epoch: 194 | Batch_idx: 270 |  Loss_1: (0.1211) | Acc_1: (95.33%) (33067/34688)\n",
      "Epoch: 194 | Batch_idx: 280 |  Loss_1: (0.1212) | Acc_1: (95.32%) (34283/35968)\n",
      "Epoch: 194 | Batch_idx: 290 |  Loss_1: (0.1208) | Acc_1: (95.34%) (35511/37248)\n",
      "Epoch: 194 | Batch_idx: 300 |  Loss_1: (0.1214) | Acc_1: (95.33%) (36728/38528)\n",
      "Epoch: 194 | Batch_idx: 310 |  Loss_1: (0.1217) | Acc_1: (95.32%) (37946/39808)\n",
      "Epoch: 194 | Batch_idx: 320 |  Loss_1: (0.1218) | Acc_1: (95.33%) (39169/41088)\n",
      "Epoch: 194 | Batch_idx: 330 |  Loss_1: (0.1213) | Acc_1: (95.35%) (40396/42368)\n",
      "Epoch: 194 | Batch_idx: 340 |  Loss_1: (0.1209) | Acc_1: (95.37%) (41628/43648)\n",
      "Epoch: 194 | Batch_idx: 350 |  Loss_1: (0.1197) | Acc_1: (95.43%) (42873/44928)\n",
      "Epoch: 194 | Batch_idx: 360 |  Loss_1: (0.1200) | Acc_1: (95.41%) (44089/46208)\n",
      "Epoch: 194 | Batch_idx: 370 |  Loss_1: (0.1204) | Acc_1: (95.39%) (45300/47488)\n",
      "Epoch: 194 | Batch_idx: 380 |  Loss_1: (0.1199) | Acc_1: (95.41%) (46529/48768)\n",
      "Epoch: 194 | Batch_idx: 390 |  Loss_1: (0.1201) | Acc_1: (95.41%) (47703/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3483) | Acc: (92.83%) (9283/10000)\n",
      "Epoch: 195 | Batch_idx: 0 |  Loss_1: (0.1749) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 195 | Batch_idx: 10 |  Loss_1: (0.1295) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 195 | Batch_idx: 20 |  Loss_1: (0.1259) | Acc_1: (95.01%) (2554/2688)\n",
      "Epoch: 195 | Batch_idx: 30 |  Loss_1: (0.1180) | Acc_1: (95.36%) (3784/3968)\n",
      "Epoch: 195 | Batch_idx: 40 |  Loss_1: (0.1180) | Acc_1: (95.39%) (5006/5248)\n",
      "Epoch: 195 | Batch_idx: 50 |  Loss_1: (0.1159) | Acc_1: (95.51%) (6235/6528)\n",
      "Epoch: 195 | Batch_idx: 60 |  Loss_1: (0.1148) | Acc_1: (95.59%) (7464/7808)\n",
      "Epoch: 195 | Batch_idx: 70 |  Loss_1: (0.1193) | Acc_1: (95.40%) (8670/9088)\n",
      "Epoch: 195 | Batch_idx: 80 |  Loss_1: (0.1190) | Acc_1: (95.39%) (9890/10368)\n",
      "Epoch: 195 | Batch_idx: 90 |  Loss_1: (0.1160) | Acc_1: (95.52%) (11126/11648)\n",
      "Epoch: 195 | Batch_idx: 100 |  Loss_1: (0.1148) | Acc_1: (95.57%) (12355/12928)\n",
      "Epoch: 195 | Batch_idx: 110 |  Loss_1: (0.1156) | Acc_1: (95.54%) (13575/14208)\n",
      "Epoch: 195 | Batch_idx: 120 |  Loss_1: (0.1162) | Acc_1: (95.57%) (14802/15488)\n",
      "Epoch: 195 | Batch_idx: 130 |  Loss_1: (0.1143) | Acc_1: (95.63%) (16035/16768)\n",
      "Epoch: 195 | Batch_idx: 140 |  Loss_1: (0.1147) | Acc_1: (95.62%) (17258/18048)\n",
      "Epoch: 195 | Batch_idx: 150 |  Loss_1: (0.1152) | Acc_1: (95.61%) (18479/19328)\n",
      "Epoch: 195 | Batch_idx: 160 |  Loss_1: (0.1156) | Acc_1: (95.57%) (19696/20608)\n",
      "Epoch: 195 | Batch_idx: 170 |  Loss_1: (0.1155) | Acc_1: (95.56%) (20917/21888)\n",
      "Epoch: 195 | Batch_idx: 180 |  Loss_1: (0.1146) | Acc_1: (95.61%) (22151/23168)\n",
      "Epoch: 195 | Batch_idx: 190 |  Loss_1: (0.1144) | Acc_1: (95.60%) (23373/24448)\n",
      "Epoch: 195 | Batch_idx: 200 |  Loss_1: (0.1137) | Acc_1: (95.61%) (24598/25728)\n",
      "Epoch: 195 | Batch_idx: 210 |  Loss_1: (0.1152) | Acc_1: (95.57%) (25812/27008)\n",
      "Epoch: 195 | Batch_idx: 220 |  Loss_1: (0.1147) | Acc_1: (95.60%) (27044/28288)\n",
      "Epoch: 195 | Batch_idx: 230 |  Loss_1: (0.1141) | Acc_1: (95.62%) (28274/29568)\n",
      "Epoch: 195 | Batch_idx: 240 |  Loss_1: (0.1143) | Acc_1: (95.62%) (29496/30848)\n",
      "Epoch: 195 | Batch_idx: 250 |  Loss_1: (0.1145) | Acc_1: (95.61%) (30718/32128)\n",
      "Epoch: 195 | Batch_idx: 260 |  Loss_1: (0.1146) | Acc_1: (95.62%) (31945/33408)\n",
      "Epoch: 195 | Batch_idx: 270 |  Loss_1: (0.1144) | Acc_1: (95.64%) (33174/34688)\n",
      "Epoch: 195 | Batch_idx: 280 |  Loss_1: (0.1148) | Acc_1: (95.63%) (34395/35968)\n",
      "Epoch: 195 | Batch_idx: 290 |  Loss_1: (0.1146) | Acc_1: (95.63%) (35621/37248)\n",
      "Epoch: 195 | Batch_idx: 300 |  Loss_1: (0.1151) | Acc_1: (95.63%) (36843/38528)\n",
      "Epoch: 195 | Batch_idx: 310 |  Loss_1: (0.1147) | Acc_1: (95.63%) (38070/39808)\n",
      "Epoch: 195 | Batch_idx: 320 |  Loss_1: (0.1145) | Acc_1: (95.63%) (39294/41088)\n",
      "Epoch: 195 | Batch_idx: 330 |  Loss_1: (0.1148) | Acc_1: (95.62%) (40513/42368)\n",
      "Epoch: 195 | Batch_idx: 340 |  Loss_1: (0.1146) | Acc_1: (95.63%) (41742/43648)\n",
      "Epoch: 195 | Batch_idx: 350 |  Loss_1: (0.1144) | Acc_1: (95.64%) (42969/44928)\n",
      "Epoch: 195 | Batch_idx: 360 |  Loss_1: (0.1144) | Acc_1: (95.64%) (44195/46208)\n",
      "Epoch: 195 | Batch_idx: 370 |  Loss_1: (0.1136) | Acc_1: (95.67%) (45434/47488)\n",
      "Epoch: 195 | Batch_idx: 380 |  Loss_1: (0.1141) | Acc_1: (95.65%) (46645/48768)\n",
      "Epoch: 195 | Batch_idx: 390 |  Loss_1: (0.1140) | Acc_1: (95.66%) (47828/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3545) | Acc: (92.40%) (9240/10000)\n",
      "Epoch: 196 | Batch_idx: 0 |  Loss_1: (0.1155) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 196 | Batch_idx: 10 |  Loss_1: (0.1198) | Acc_1: (95.67%) (1347/1408)\n",
      "Epoch: 196 | Batch_idx: 20 |  Loss_1: (0.1166) | Acc_1: (95.72%) (2573/2688)\n",
      "Epoch: 196 | Batch_idx: 30 |  Loss_1: (0.1267) | Acc_1: (95.41%) (3786/3968)\n",
      "Epoch: 196 | Batch_idx: 40 |  Loss_1: (0.1261) | Acc_1: (95.46%) (5010/5248)\n",
      "Epoch: 196 | Batch_idx: 50 |  Loss_1: (0.1218) | Acc_1: (95.57%) (6239/6528)\n",
      "Epoch: 196 | Batch_idx: 60 |  Loss_1: (0.1232) | Acc_1: (95.56%) (7461/7808)\n",
      "Epoch: 196 | Batch_idx: 70 |  Loss_1: (0.1225) | Acc_1: (95.57%) (8685/9088)\n",
      "Epoch: 196 | Batch_idx: 80 |  Loss_1: (0.1244) | Acc_1: (95.49%) (9900/10368)\n",
      "Epoch: 196 | Batch_idx: 90 |  Loss_1: (0.1233) | Acc_1: (95.48%) (11121/11648)\n",
      "Epoch: 196 | Batch_idx: 100 |  Loss_1: (0.1225) | Acc_1: (95.49%) (12345/12928)\n",
      "Epoch: 196 | Batch_idx: 110 |  Loss_1: (0.1242) | Acc_1: (95.40%) (13554/14208)\n",
      "Epoch: 196 | Batch_idx: 120 |  Loss_1: (0.1252) | Acc_1: (95.34%) (14766/15488)\n",
      "Epoch: 196 | Batch_idx: 130 |  Loss_1: (0.1248) | Acc_1: (95.34%) (15987/16768)\n",
      "Epoch: 196 | Batch_idx: 140 |  Loss_1: (0.1242) | Acc_1: (95.37%) (17212/18048)\n",
      "Epoch: 196 | Batch_idx: 150 |  Loss_1: (0.1230) | Acc_1: (95.40%) (18439/19328)\n",
      "Epoch: 196 | Batch_idx: 160 |  Loss_1: (0.1220) | Acc_1: (95.43%) (19667/20608)\n",
      "Epoch: 196 | Batch_idx: 170 |  Loss_1: (0.1215) | Acc_1: (95.45%) (20893/21888)\n",
      "Epoch: 196 | Batch_idx: 180 |  Loss_1: (0.1220) | Acc_1: (95.43%) (22110/23168)\n",
      "Epoch: 196 | Batch_idx: 190 |  Loss_1: (0.1207) | Acc_1: (95.46%) (23339/24448)\n",
      "Epoch: 196 | Batch_idx: 200 |  Loss_1: (0.1211) | Acc_1: (95.45%) (24557/25728)\n",
      "Epoch: 196 | Batch_idx: 210 |  Loss_1: (0.1211) | Acc_1: (95.44%) (25777/27008)\n",
      "Epoch: 196 | Batch_idx: 220 |  Loss_1: (0.1207) | Acc_1: (95.46%) (27003/28288)\n",
      "Epoch: 196 | Batch_idx: 230 |  Loss_1: (0.1208) | Acc_1: (95.45%) (28222/29568)\n",
      "Epoch: 196 | Batch_idx: 240 |  Loss_1: (0.1225) | Acc_1: (95.38%) (29423/30848)\n",
      "Epoch: 196 | Batch_idx: 250 |  Loss_1: (0.1226) | Acc_1: (95.37%) (30642/32128)\n",
      "Epoch: 196 | Batch_idx: 260 |  Loss_1: (0.1223) | Acc_1: (95.38%) (31864/33408)\n",
      "Epoch: 196 | Batch_idx: 270 |  Loss_1: (0.1223) | Acc_1: (95.39%) (33089/34688)\n",
      "Epoch: 196 | Batch_idx: 280 |  Loss_1: (0.1221) | Acc_1: (95.40%) (34314/35968)\n",
      "Epoch: 196 | Batch_idx: 290 |  Loss_1: (0.1229) | Acc_1: (95.37%) (35523/37248)\n",
      "Epoch: 196 | Batch_idx: 300 |  Loss_1: (0.1225) | Acc_1: (95.39%) (36751/38528)\n",
      "Epoch: 196 | Batch_idx: 310 |  Loss_1: (0.1223) | Acc_1: (95.39%) (37971/39808)\n",
      "Epoch: 196 | Batch_idx: 320 |  Loss_1: (0.1225) | Acc_1: (95.38%) (39190/41088)\n",
      "Epoch: 196 | Batch_idx: 330 |  Loss_1: (0.1228) | Acc_1: (95.36%) (40403/42368)\n",
      "Epoch: 196 | Batch_idx: 340 |  Loss_1: (0.1235) | Acc_1: (95.34%) (41614/43648)\n",
      "Epoch: 196 | Batch_idx: 350 |  Loss_1: (0.1229) | Acc_1: (95.37%) (42849/44928)\n",
      "Epoch: 196 | Batch_idx: 360 |  Loss_1: (0.1228) | Acc_1: (95.38%) (44073/46208)\n",
      "Epoch: 196 | Batch_idx: 370 |  Loss_1: (0.1230) | Acc_1: (95.37%) (45290/47488)\n",
      "Epoch: 196 | Batch_idx: 380 |  Loss_1: (0.1231) | Acc_1: (95.36%) (46505/48768)\n",
      "Epoch: 196 | Batch_idx: 390 |  Loss_1: (0.1233) | Acc_1: (95.35%) (47677/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3477) | Acc: (92.55%) (9255/10000)\n",
      "Epoch: 197 | Batch_idx: 0 |  Loss_1: (0.1274) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 197 | Batch_idx: 10 |  Loss_1: (0.1155) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 197 | Batch_idx: 20 |  Loss_1: (0.1150) | Acc_1: (95.72%) (2573/2688)\n",
      "Epoch: 197 | Batch_idx: 30 |  Loss_1: (0.1103) | Acc_1: (95.92%) (3806/3968)\n",
      "Epoch: 197 | Batch_idx: 40 |  Loss_1: (0.1184) | Acc_1: (95.56%) (5015/5248)\n",
      "Epoch: 197 | Batch_idx: 50 |  Loss_1: (0.1229) | Acc_1: (95.39%) (6227/6528)\n",
      "Epoch: 197 | Batch_idx: 60 |  Loss_1: (0.1238) | Acc_1: (95.31%) (7442/7808)\n",
      "Epoch: 197 | Batch_idx: 70 |  Loss_1: (0.1238) | Acc_1: (95.30%) (8661/9088)\n",
      "Epoch: 197 | Batch_idx: 80 |  Loss_1: (0.1200) | Acc_1: (95.45%) (9896/10368)\n",
      "Epoch: 197 | Batch_idx: 90 |  Loss_1: (0.1202) | Acc_1: (95.48%) (11121/11648)\n",
      "Epoch: 197 | Batch_idx: 100 |  Loss_1: (0.1189) | Acc_1: (95.56%) (12354/12928)\n",
      "Epoch: 197 | Batch_idx: 110 |  Loss_1: (0.1196) | Acc_1: (95.52%) (13572/14208)\n",
      "Epoch: 197 | Batch_idx: 120 |  Loss_1: (0.1188) | Acc_1: (95.56%) (14801/15488)\n",
      "Epoch: 197 | Batch_idx: 130 |  Loss_1: (0.1197) | Acc_1: (95.50%) (16014/16768)\n",
      "Epoch: 197 | Batch_idx: 140 |  Loss_1: (0.1199) | Acc_1: (95.50%) (17235/18048)\n",
      "Epoch: 197 | Batch_idx: 150 |  Loss_1: (0.1198) | Acc_1: (95.50%) (18459/19328)\n",
      "Epoch: 197 | Batch_idx: 160 |  Loss_1: (0.1191) | Acc_1: (95.55%) (19690/20608)\n",
      "Epoch: 197 | Batch_idx: 170 |  Loss_1: (0.1186) | Acc_1: (95.56%) (20916/21888)\n",
      "Epoch: 197 | Batch_idx: 180 |  Loss_1: (0.1189) | Acc_1: (95.53%) (22133/23168)\n",
      "Epoch: 197 | Batch_idx: 190 |  Loss_1: (0.1198) | Acc_1: (95.50%) (23347/24448)\n",
      "Epoch: 197 | Batch_idx: 200 |  Loss_1: (0.1203) | Acc_1: (95.46%) (24561/25728)\n",
      "Epoch: 197 | Batch_idx: 210 |  Loss_1: (0.1196) | Acc_1: (95.50%) (25792/27008)\n",
      "Epoch: 197 | Batch_idx: 220 |  Loss_1: (0.1192) | Acc_1: (95.51%) (27019/28288)\n",
      "Epoch: 197 | Batch_idx: 230 |  Loss_1: (0.1199) | Acc_1: (95.48%) (28232/29568)\n",
      "Epoch: 197 | Batch_idx: 240 |  Loss_1: (0.1189) | Acc_1: (95.53%) (29469/30848)\n",
      "Epoch: 197 | Batch_idx: 250 |  Loss_1: (0.1193) | Acc_1: (95.50%) (30681/32128)\n",
      "Epoch: 197 | Batch_idx: 260 |  Loss_1: (0.1196) | Acc_1: (95.48%) (31898/33408)\n",
      "Epoch: 197 | Batch_idx: 270 |  Loss_1: (0.1186) | Acc_1: (95.52%) (33134/34688)\n",
      "Epoch: 197 | Batch_idx: 280 |  Loss_1: (0.1182) | Acc_1: (95.53%) (34360/35968)\n",
      "Epoch: 197 | Batch_idx: 290 |  Loss_1: (0.1178) | Acc_1: (95.55%) (35589/37248)\n",
      "Epoch: 197 | Batch_idx: 300 |  Loss_1: (0.1177) | Acc_1: (95.56%) (36816/38528)\n",
      "Epoch: 197 | Batch_idx: 310 |  Loss_1: (0.1170) | Acc_1: (95.57%) (38045/39808)\n",
      "Epoch: 197 | Batch_idx: 320 |  Loss_1: (0.1174) | Acc_1: (95.56%) (39264/41088)\n",
      "Epoch: 197 | Batch_idx: 330 |  Loss_1: (0.1172) | Acc_1: (95.57%) (40489/42368)\n",
      "Epoch: 197 | Batch_idx: 340 |  Loss_1: (0.1166) | Acc_1: (95.60%) (41726/43648)\n",
      "Epoch: 197 | Batch_idx: 350 |  Loss_1: (0.1161) | Acc_1: (95.61%) (42954/44928)\n",
      "Epoch: 197 | Batch_idx: 360 |  Loss_1: (0.1160) | Acc_1: (95.60%) (44176/46208)\n",
      "Epoch: 197 | Batch_idx: 370 |  Loss_1: (0.1158) | Acc_1: (95.60%) (45399/47488)\n",
      "Epoch: 197 | Batch_idx: 380 |  Loss_1: (0.1156) | Acc_1: (95.60%) (46623/48768)\n",
      "Epoch: 197 | Batch_idx: 390 |  Loss_1: (0.1153) | Acc_1: (95.60%) (47798/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3559) | Acc: (92.54%) (9254/10000)\n",
      "Epoch: 198 | Batch_idx: 0 |  Loss_1: (0.1119) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 198 | Batch_idx: 10 |  Loss_1: (0.1054) | Acc_1: (95.95%) (1351/1408)\n",
      "Epoch: 198 | Batch_idx: 20 |  Loss_1: (0.1201) | Acc_1: (95.31%) (2562/2688)\n",
      "Epoch: 198 | Batch_idx: 30 |  Loss_1: (0.1152) | Acc_1: (95.49%) (3789/3968)\n",
      "Epoch: 198 | Batch_idx: 40 |  Loss_1: (0.1179) | Acc_1: (95.35%) (5004/5248)\n",
      "Epoch: 198 | Batch_idx: 50 |  Loss_1: (0.1154) | Acc_1: (95.47%) (6232/6528)\n",
      "Epoch: 198 | Batch_idx: 60 |  Loss_1: (0.1155) | Acc_1: (95.49%) (7456/7808)\n",
      "Epoch: 198 | Batch_idx: 70 |  Loss_1: (0.1150) | Acc_1: (95.50%) (8679/9088)\n",
      "Epoch: 198 | Batch_idx: 80 |  Loss_1: (0.1144) | Acc_1: (95.55%) (9907/10368)\n",
      "Epoch: 198 | Batch_idx: 90 |  Loss_1: (0.1164) | Acc_1: (95.46%) (11119/11648)\n",
      "Epoch: 198 | Batch_idx: 100 |  Loss_1: (0.1158) | Acc_1: (95.49%) (12345/12928)\n",
      "Epoch: 198 | Batch_idx: 110 |  Loss_1: (0.1155) | Acc_1: (95.50%) (13569/14208)\n",
      "Epoch: 198 | Batch_idx: 120 |  Loss_1: (0.1146) | Acc_1: (95.55%) (14799/15488)\n",
      "Epoch: 198 | Batch_idx: 130 |  Loss_1: (0.1146) | Acc_1: (95.59%) (16028/16768)\n",
      "Epoch: 198 | Batch_idx: 140 |  Loss_1: (0.1148) | Acc_1: (95.58%) (17251/18048)\n",
      "Epoch: 198 | Batch_idx: 150 |  Loss_1: (0.1148) | Acc_1: (95.59%) (18476/19328)\n",
      "Epoch: 198 | Batch_idx: 160 |  Loss_1: (0.1150) | Acc_1: (95.61%) (19703/20608)\n",
      "Epoch: 198 | Batch_idx: 170 |  Loss_1: (0.1155) | Acc_1: (95.60%) (20924/21888)\n",
      "Epoch: 198 | Batch_idx: 180 |  Loss_1: (0.1158) | Acc_1: (95.59%) (22146/23168)\n",
      "Epoch: 198 | Batch_idx: 190 |  Loss_1: (0.1170) | Acc_1: (95.55%) (23359/24448)\n",
      "Epoch: 198 | Batch_idx: 200 |  Loss_1: (0.1169) | Acc_1: (95.55%) (24582/25728)\n",
      "Epoch: 198 | Batch_idx: 210 |  Loss_1: (0.1165) | Acc_1: (95.57%) (25811/27008)\n",
      "Epoch: 198 | Batch_idx: 220 |  Loss_1: (0.1158) | Acc_1: (95.60%) (27044/28288)\n",
      "Epoch: 198 | Batch_idx: 230 |  Loss_1: (0.1162) | Acc_1: (95.58%) (28262/29568)\n",
      "Epoch: 198 | Batch_idx: 240 |  Loss_1: (0.1167) | Acc_1: (95.55%) (29476/30848)\n",
      "Epoch: 198 | Batch_idx: 250 |  Loss_1: (0.1166) | Acc_1: (95.55%) (30698/32128)\n",
      "Epoch: 198 | Batch_idx: 260 |  Loss_1: (0.1172) | Acc_1: (95.53%) (31915/33408)\n",
      "Epoch: 198 | Batch_idx: 270 |  Loss_1: (0.1176) | Acc_1: (95.53%) (33138/34688)\n",
      "Epoch: 198 | Batch_idx: 280 |  Loss_1: (0.1169) | Acc_1: (95.55%) (34369/35968)\n",
      "Epoch: 198 | Batch_idx: 290 |  Loss_1: (0.1162) | Acc_1: (95.58%) (35602/37248)\n",
      "Epoch: 198 | Batch_idx: 300 |  Loss_1: (0.1163) | Acc_1: (95.60%) (36831/38528)\n",
      "Epoch: 198 | Batch_idx: 310 |  Loss_1: (0.1164) | Acc_1: (95.59%) (38051/39808)\n",
      "Epoch: 198 | Batch_idx: 320 |  Loss_1: (0.1172) | Acc_1: (95.55%) (39261/41088)\n",
      "Epoch: 198 | Batch_idx: 330 |  Loss_1: (0.1170) | Acc_1: (95.56%) (40488/42368)\n",
      "Epoch: 198 | Batch_idx: 340 |  Loss_1: (0.1162) | Acc_1: (95.60%) (41729/43648)\n",
      "Epoch: 198 | Batch_idx: 350 |  Loss_1: (0.1155) | Acc_1: (95.63%) (42963/44928)\n",
      "Epoch: 198 | Batch_idx: 360 |  Loss_1: (0.1150) | Acc_1: (95.65%) (44196/46208)\n",
      "Epoch: 198 | Batch_idx: 370 |  Loss_1: (0.1145) | Acc_1: (95.66%) (45427/47488)\n",
      "Epoch: 198 | Batch_idx: 380 |  Loss_1: (0.1150) | Acc_1: (95.64%) (46643/48768)\n",
      "Epoch: 198 | Batch_idx: 390 |  Loss_1: (0.1151) | Acc_1: (95.64%) (47820/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3521) | Acc: (92.50%) (9250/10000)\n",
      "Epoch: 199 | Batch_idx: 0 |  Loss_1: (0.1635) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 199 | Batch_idx: 10 |  Loss_1: (0.1320) | Acc_1: (94.96%) (1337/1408)\n",
      "Epoch: 199 | Batch_idx: 20 |  Loss_1: (0.1265) | Acc_1: (95.20%) (2559/2688)\n",
      "Epoch: 199 | Batch_idx: 30 |  Loss_1: (0.1267) | Acc_1: (95.14%) (3775/3968)\n",
      "Epoch: 199 | Batch_idx: 40 |  Loss_1: (0.1256) | Acc_1: (95.16%) (4994/5248)\n",
      "Epoch: 199 | Batch_idx: 50 |  Loss_1: (0.1194) | Acc_1: (95.37%) (6226/6528)\n",
      "Epoch: 199 | Batch_idx: 60 |  Loss_1: (0.1175) | Acc_1: (95.57%) (7462/7808)\n",
      "Epoch: 199 | Batch_idx: 70 |  Loss_1: (0.1164) | Acc_1: (95.64%) (8692/9088)\n",
      "Epoch: 199 | Batch_idx: 80 |  Loss_1: (0.1154) | Acc_1: (95.67%) (9919/10368)\n",
      "Epoch: 199 | Batch_idx: 90 |  Loss_1: (0.1157) | Acc_1: (95.67%) (11144/11648)\n",
      "Epoch: 199 | Batch_idx: 100 |  Loss_1: (0.1135) | Acc_1: (95.76%) (12380/12928)\n",
      "Epoch: 199 | Batch_idx: 110 |  Loss_1: (0.1142) | Acc_1: (95.70%) (13597/14208)\n",
      "Epoch: 199 | Batch_idx: 120 |  Loss_1: (0.1141) | Acc_1: (95.72%) (14825/15488)\n",
      "Epoch: 199 | Batch_idx: 130 |  Loss_1: (0.1148) | Acc_1: (95.68%) (16043/16768)\n",
      "Epoch: 199 | Batch_idx: 140 |  Loss_1: (0.1158) | Acc_1: (95.63%) (17259/18048)\n",
      "Epoch: 199 | Batch_idx: 150 |  Loss_1: (0.1167) | Acc_1: (95.60%) (18477/19328)\n",
      "Epoch: 199 | Batch_idx: 160 |  Loss_1: (0.1176) | Acc_1: (95.56%) (19693/20608)\n",
      "Epoch: 199 | Batch_idx: 170 |  Loss_1: (0.1186) | Acc_1: (95.50%) (20903/21888)\n",
      "Epoch: 199 | Batch_idx: 180 |  Loss_1: (0.1190) | Acc_1: (95.49%) (22123/23168)\n",
      "Epoch: 199 | Batch_idx: 190 |  Loss_1: (0.1202) | Acc_1: (95.45%) (23336/24448)\n",
      "Epoch: 199 | Batch_idx: 200 |  Loss_1: (0.1200) | Acc_1: (95.47%) (24562/25728)\n",
      "Epoch: 199 | Batch_idx: 210 |  Loss_1: (0.1200) | Acc_1: (95.48%) (25788/27008)\n",
      "Epoch: 199 | Batch_idx: 220 |  Loss_1: (0.1197) | Acc_1: (95.49%) (27011/28288)\n",
      "Epoch: 199 | Batch_idx: 230 |  Loss_1: (0.1198) | Acc_1: (95.48%) (28232/29568)\n",
      "Epoch: 199 | Batch_idx: 240 |  Loss_1: (0.1206) | Acc_1: (95.44%) (29441/30848)\n",
      "Epoch: 199 | Batch_idx: 250 |  Loss_1: (0.1214) | Acc_1: (95.41%) (30652/32128)\n",
      "Epoch: 199 | Batch_idx: 260 |  Loss_1: (0.1214) | Acc_1: (95.41%) (31873/33408)\n",
      "Epoch: 199 | Batch_idx: 270 |  Loss_1: (0.1222) | Acc_1: (95.38%) (33085/34688)\n",
      "Epoch: 199 | Batch_idx: 280 |  Loss_1: (0.1211) | Acc_1: (95.42%) (34320/35968)\n",
      "Epoch: 199 | Batch_idx: 290 |  Loss_1: (0.1212) | Acc_1: (95.41%) (35537/37248)\n",
      "Epoch: 199 | Batch_idx: 300 |  Loss_1: (0.1209) | Acc_1: (95.42%) (36764/38528)\n",
      "Epoch: 199 | Batch_idx: 310 |  Loss_1: (0.1220) | Acc_1: (95.39%) (37972/39808)\n",
      "Epoch: 199 | Batch_idx: 320 |  Loss_1: (0.1216) | Acc_1: (95.39%) (39195/41088)\n",
      "Epoch: 199 | Batch_idx: 330 |  Loss_1: (0.1220) | Acc_1: (95.39%) (40413/42368)\n",
      "Epoch: 199 | Batch_idx: 340 |  Loss_1: (0.1217) | Acc_1: (95.40%) (41639/43648)\n",
      "Epoch: 199 | Batch_idx: 350 |  Loss_1: (0.1214) | Acc_1: (95.40%) (42863/44928)\n",
      "Epoch: 199 | Batch_idx: 360 |  Loss_1: (0.1223) | Acc_1: (95.37%) (44069/46208)\n",
      "Epoch: 199 | Batch_idx: 370 |  Loss_1: (0.1221) | Acc_1: (95.39%) (45298/47488)\n",
      "Epoch: 199 | Batch_idx: 380 |  Loss_1: (0.1221) | Acc_1: (95.40%) (46524/48768)\n",
      "Epoch: 199 | Batch_idx: 390 |  Loss_1: (0.1221) | Acc_1: (95.39%) (47697/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3545) | Acc: (92.58%) (9258/10000)\n",
      "6 hours 45 mins 37 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet_101_relu.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 200):\n",
    "\n",
    "    # if epoch < 80:\n",
    "    #     lr = learning_rate\n",
    "    # elif epoch < 120:\n",
    "    #     lr = learning_rate * 0.1\n",
    "    # else:\n",
    "    #     lr = learning_rate * 0.01\n",
    "    # for param_group in optimizer.param_groups:\n",
    "    #     param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet_101_relu.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8766a17f2584f17ae9875767170f3464b2a051bfe2b6423fb227ac503acbc200"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hw2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
