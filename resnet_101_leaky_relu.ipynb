{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParmeter 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.006\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet_101_leaky') #!#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformer_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "validation_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlock2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, drop_prob, block_size):\n",
    "        super(DropBlock2D, self).__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        assert x.dim() == 4, \\\n",
    "            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n",
    "\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        else:\n",
    "            # get gamma value\n",
    "            gamma = self._compute_gamma(x)\n",
    "\n",
    "            # sample mask\n",
    "            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n",
    "\n",
    "            # place mask on input device\n",
    "            mask = mask.to(x.device)\n",
    "\n",
    "            # compute block mask\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "\n",
    "            # apply block mask\n",
    "            out = x * block_mask[:, None, :, :]\n",
    "\n",
    "            # scale output\n",
    "            out = out * block_mask.numel() / block_mask.sum()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n",
    "                                  kernel_size=(self.block_size, self.block_size),\n",
    "                                  stride=(1, 1),\n",
    "                                  padding=self.block_size // 2)\n",
    "\n",
    "        if self.block_size % 2 == 0:\n",
    "            block_mask = block_mask[:, :, :-1, :-1]\n",
    "\n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "\n",
    "        return block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob / (self.block_size ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(nn.Module):\n",
    "    def __init__(self, dropblock, start_value, stop_value, nr_steps):\n",
    "        super(LinearScheduler, self).__init__()\n",
    "        self.dropblock = dropblock\n",
    "        self.i = 0\n",
    "        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=int(nr_steps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropblock(x)\n",
    "\n",
    "    def step(self):\n",
    "        if self.i < len(self.drop_values):\n",
    "            self.dropblock.drop_prob = self.drop_values[self.i]\n",
    "\n",
    "        self.i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, start_prob, stop_prob, block_size, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        self.start_prob = start_prob\n",
    "        self.stop_prob = stop_prob\n",
    "        self.block_size = block_size\n",
    "        self.dropblock = LinearScheduler(DropBlock2D(drop_prob=self.start_prob, block_size=self.block_size), start_value=self.start_prob, stop_value=self.start_prob, nr_steps=60000)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropblock.step()\n",
    "        out = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.dropblock(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.dropblock(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BottleNeck, [3, 4, 23, 3], 0.6, 0.8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 병렬연산 사용 유무 및 GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 1 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer & Scheduler & Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)               \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(training_loader_1)\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(training_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(validation_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(validation_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheckPoint (model save and model load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (2.4981) | Acc_1: (9.38%) (12/128)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (4.7870) | Acc_1: (9.66%) (136/1408)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (3.9913) | Acc_1: (9.75%) (262/2688)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (3.7462) | Acc_1: (9.73%) (386/3968)\n",
      "Epoch: 0 | Batch_idx: 40 |  Loss_1: (3.4981) | Acc_1: (9.98%) (524/5248)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss_1: (3.3080) | Acc_1: (10.22%) (667/6528)\n",
      "Epoch: 0 | Batch_idx: 60 |  Loss_1: (3.1517) | Acc_1: (10.27%) (802/7808)\n",
      "Epoch: 0 | Batch_idx: 70 |  Loss_1: (3.0395) | Acc_1: (10.29%) (935/9088)\n",
      "Epoch: 0 | Batch_idx: 80 |  Loss_1: (2.9537) | Acc_1: (10.36%) (1074/10368)\n",
      "Epoch: 0 | Batch_idx: 90 |  Loss_1: (2.8908) | Acc_1: (10.22%) (1190/11648)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss_1: (2.8424) | Acc_1: (10.33%) (1336/12928)\n",
      "Epoch: 0 | Batch_idx: 110 |  Loss_1: (2.7969) | Acc_1: (10.30%) (1463/14208)\n",
      "Epoch: 0 | Batch_idx: 120 |  Loss_1: (2.7580) | Acc_1: (10.31%) (1597/15488)\n",
      "Epoch: 0 | Batch_idx: 130 |  Loss_1: (2.7249) | Acc_1: (10.34%) (1734/16768)\n",
      "Epoch: 0 | Batch_idx: 140 |  Loss_1: (2.6955) | Acc_1: (10.39%) (1875/18048)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss_1: (2.6700) | Acc_1: (10.37%) (2005/19328)\n",
      "Epoch: 0 | Batch_idx: 160 |  Loss_1: (2.6477) | Acc_1: (10.37%) (2138/20608)\n",
      "Epoch: 0 | Batch_idx: 170 |  Loss_1: (2.6271) | Acc_1: (10.50%) (2298/21888)\n",
      "Epoch: 0 | Batch_idx: 180 |  Loss_1: (2.6093) | Acc_1: (10.64%) (2466/23168)\n",
      "Epoch: 0 | Batch_idx: 190 |  Loss_1: (2.5923) | Acc_1: (10.80%) (2641/24448)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss_1: (2.5759) | Acc_1: (10.95%) (2816/25728)\n",
      "Epoch: 0 | Batch_idx: 210 |  Loss_1: (2.5591) | Acc_1: (11.20%) (3025/27008)\n",
      "Epoch: 0 | Batch_idx: 220 |  Loss_1: (2.5407) | Acc_1: (11.38%) (3219/28288)\n",
      "Epoch: 0 | Batch_idx: 230 |  Loss_1: (2.5240) | Acc_1: (11.55%) (3414/29568)\n",
      "Epoch: 0 | Batch_idx: 240 |  Loss_1: (2.5077) | Acc_1: (11.81%) (3643/30848)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss_1: (2.4912) | Acc_1: (12.15%) (3903/32128)\n",
      "Epoch: 0 | Batch_idx: 260 |  Loss_1: (2.4740) | Acc_1: (12.39%) (4140/33408)\n",
      "Epoch: 0 | Batch_idx: 270 |  Loss_1: (2.4586) | Acc_1: (12.66%) (4392/34688)\n",
      "Epoch: 0 | Batch_idx: 280 |  Loss_1: (2.4442) | Acc_1: (12.93%) (4651/35968)\n",
      "Epoch: 0 | Batch_idx: 290 |  Loss_1: (2.4309) | Acc_1: (13.13%) (4891/37248)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss_1: (2.4180) | Acc_1: (13.40%) (5161/38528)\n",
      "Epoch: 0 | Batch_idx: 310 |  Loss_1: (2.4063) | Acc_1: (13.63%) (5426/39808)\n",
      "Epoch: 0 | Batch_idx: 320 |  Loss_1: (2.3951) | Acc_1: (13.90%) (5713/41088)\n",
      "Epoch: 0 | Batch_idx: 330 |  Loss_1: (2.3855) | Acc_1: (14.08%) (5966/42368)\n",
      "Epoch: 0 | Batch_idx: 340 |  Loss_1: (2.3761) | Acc_1: (14.23%) (6213/43648)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss_1: (2.3670) | Acc_1: (14.39%) (6467/44928)\n",
      "Epoch: 0 | Batch_idx: 360 |  Loss_1: (2.3576) | Acc_1: (14.57%) (6733/46208)\n",
      "Epoch: 0 | Batch_idx: 370 |  Loss_1: (2.3491) | Acc_1: (14.80%) (7027/47488)\n",
      "Epoch: 0 | Batch_idx: 380 |  Loss_1: (2.3409) | Acc_1: (14.91%) (7269/48768)\n",
      "Epoch: 0 | Batch_idx: 390 |  Loss_1: (2.3326) | Acc_1: (15.05%) (7527/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.9215) | Acc: (25.67%) (2567/10000)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.9117) | Acc_1: (26.56%) (34/128)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (2.0146) | Acc_1: (22.30%) (314/1408)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (2.0364) | Acc_1: (20.76%) (558/2688)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (2.0163) | Acc_1: (21.88%) (868/3968)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss_1: (2.0293) | Acc_1: (22.01%) (1155/5248)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss_1: (2.0319) | Acc_1: (21.52%) (1405/6528)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss_1: (2.0297) | Acc_1: (21.44%) (1674/7808)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss_1: (2.0266) | Acc_1: (21.82%) (1983/9088)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss_1: (2.0237) | Acc_1: (22.15%) (2297/10368)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss_1: (2.0215) | Acc_1: (22.15%) (2580/11648)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss_1: (2.0153) | Acc_1: (22.30%) (2883/12928)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss_1: (2.0126) | Acc_1: (22.60%) (3211/14208)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss_1: (2.0111) | Acc_1: (22.61%) (3502/15488)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss_1: (2.0065) | Acc_1: (22.64%) (3796/16768)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss_1: (2.0041) | Acc_1: (22.70%) (4097/18048)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss_1: (2.0027) | Acc_1: (22.76%) (4399/19328)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss_1: (2.0001) | Acc_1: (22.78%) (4695/20608)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss_1: (1.9975) | Acc_1: (22.78%) (4985/21888)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss_1: (1.9962) | Acc_1: (22.90%) (5305/23168)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss_1: (1.9950) | Acc_1: (22.89%) (5597/24448)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss_1: (1.9934) | Acc_1: (23.00%) (5917/25728)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss_1: (1.9935) | Acc_1: (22.96%) (6201/27008)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss_1: (1.9915) | Acc_1: (23.00%) (6505/28288)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss_1: (1.9896) | Acc_1: (23.15%) (6844/29568)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss_1: (1.9874) | Acc_1: (23.24%) (7168/30848)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss_1: (1.9857) | Acc_1: (23.28%) (7481/32128)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss_1: (1.9846) | Acc_1: (23.35%) (7800/33408)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss_1: (1.9832) | Acc_1: (23.43%) (8126/34688)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss_1: (1.9819) | Acc_1: (23.54%) (8466/35968)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss_1: (1.9797) | Acc_1: (23.62%) (8799/37248)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss_1: (1.9772) | Acc_1: (23.75%) (9152/38528)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss_1: (1.9741) | Acc_1: (23.89%) (9509/39808)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss_1: (1.9717) | Acc_1: (23.98%) (9853/41088)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss_1: (1.9681) | Acc_1: (24.19%) (10247/42368)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss_1: (1.9644) | Acc_1: (24.30%) (10605/43648)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss_1: (1.9632) | Acc_1: (24.41%) (10966/44928)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss_1: (1.9613) | Acc_1: (24.50%) (11322/46208)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss_1: (1.9590) | Acc_1: (24.61%) (11688/47488)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss_1: (1.9559) | Acc_1: (24.76%) (12073/48768)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss_1: (1.9535) | Acc_1: (24.86%) (12432/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.8337) | Acc: (26.71%) (2671/10000)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.8918) | Acc_1: (29.69%) (38/128)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.8636) | Acc_1: (30.11%) (424/1408)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.8527) | Acc_1: (30.43%) (818/2688)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.8421) | Acc_1: (29.86%) (1185/3968)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss_1: (1.8381) | Acc_1: (30.28%) (1589/5248)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss_1: (1.8451) | Acc_1: (29.90%) (1952/6528)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss_1: (1.8371) | Acc_1: (30.24%) (2361/7808)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss_1: (1.8278) | Acc_1: (30.42%) (2765/9088)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss_1: (1.8222) | Acc_1: (30.66%) (3179/10368)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss_1: (1.8216) | Acc_1: (30.55%) (3559/11648)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss_1: (1.8225) | Acc_1: (30.41%) (3932/12928)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss_1: (1.8227) | Acc_1: (30.34%) (4311/14208)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss_1: (1.8205) | Acc_1: (30.60%) (4740/15488)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss_1: (1.8178) | Acc_1: (30.76%) (5158/16768)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss_1: (1.8140) | Acc_1: (30.98%) (5592/18048)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss_1: (1.8113) | Acc_1: (31.28%) (6045/19328)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss_1: (1.8107) | Acc_1: (31.36%) (6462/20608)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss_1: (1.8090) | Acc_1: (31.31%) (6853/21888)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss_1: (1.8044) | Acc_1: (31.53%) (7305/23168)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss_1: (1.8050) | Acc_1: (31.56%) (7717/24448)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss_1: (1.8048) | Acc_1: (31.61%) (8132/25728)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss_1: (1.8051) | Acc_1: (31.62%) (8539/27008)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss_1: (1.8037) | Acc_1: (31.63%) (8947/28288)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss_1: (1.8030) | Acc_1: (31.66%) (9362/29568)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss_1: (1.8017) | Acc_1: (31.75%) (9794/30848)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss_1: (1.8022) | Acc_1: (31.78%) (10209/32128)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss_1: (1.8027) | Acc_1: (31.73%) (10600/33408)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss_1: (1.8019) | Acc_1: (31.78%) (11023/34688)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss_1: (1.8004) | Acc_1: (31.83%) (11450/35968)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss_1: (1.7996) | Acc_1: (31.82%) (11851/37248)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss_1: (1.7979) | Acc_1: (31.87%) (12277/38528)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss_1: (1.7955) | Acc_1: (31.96%) (12721/39808)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss_1: (1.7927) | Acc_1: (32.07%) (13177/41088)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss_1: (1.7913) | Acc_1: (32.17%) (13628/42368)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss_1: (1.7891) | Acc_1: (32.24%) (14074/43648)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss_1: (1.7864) | Acc_1: (32.34%) (14529/44928)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss_1: (1.7851) | Acc_1: (32.39%) (14968/46208)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss_1: (1.7827) | Acc_1: (32.45%) (15408/47488)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss_1: (1.7795) | Acc_1: (32.60%) (15899/48768)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss_1: (1.7776) | Acc_1: (32.73%) (16363/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.5732) | Acc: (40.91%) (4091/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.7747) | Acc_1: (33.59%) (43/128)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.6848) | Acc_1: (38.42%) (541/1408)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.6935) | Acc_1: (37.28%) (1002/2688)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.7065) | Acc_1: (36.29%) (1440/3968)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss_1: (1.7092) | Acc_1: (36.45%) (1913/5248)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss_1: (1.7111) | Acc_1: (36.47%) (2381/6528)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss_1: (1.7091) | Acc_1: (36.33%) (2837/7808)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss_1: (1.6959) | Acc_1: (36.75%) (3340/9088)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss_1: (1.6941) | Acc_1: (36.84%) (3820/10368)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss_1: (1.6904) | Acc_1: (36.73%) (4278/11648)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss_1: (1.6844) | Acc_1: (36.85%) (4764/12928)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss_1: (1.6857) | Acc_1: (36.93%) (5247/14208)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss_1: (1.6871) | Acc_1: (36.81%) (5701/15488)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss_1: (1.6847) | Acc_1: (36.90%) (6188/16768)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss_1: (1.6809) | Acc_1: (37.16%) (6706/18048)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss_1: (1.6787) | Acc_1: (37.23%) (7195/19328)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss_1: (1.6768) | Acc_1: (37.19%) (7665/20608)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss_1: (1.6717) | Acc_1: (37.48%) (8204/21888)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss_1: (1.6726) | Acc_1: (37.41%) (8666/23168)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss_1: (1.6749) | Acc_1: (37.40%) (9143/24448)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss_1: (1.6747) | Acc_1: (37.40%) (9621/25728)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss_1: (1.6733) | Acc_1: (37.43%) (10109/27008)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss_1: (1.6703) | Acc_1: (37.57%) (10629/28288)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss_1: (1.6670) | Acc_1: (37.70%) (11148/29568)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss_1: (1.6648) | Acc_1: (37.74%) (11642/30848)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss_1: (1.6625) | Acc_1: (37.83%) (12154/32128)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss_1: (1.6602) | Acc_1: (37.90%) (12662/33408)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss_1: (1.6605) | Acc_1: (37.92%) (13155/34688)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss_1: (1.6578) | Acc_1: (38.09%) (13702/35968)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss_1: (1.6548) | Acc_1: (38.18%) (14222/37248)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss_1: (1.6525) | Acc_1: (38.30%) (14758/38528)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss_1: (1.6494) | Acc_1: (38.46%) (15310/39808)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss_1: (1.6472) | Acc_1: (38.52%) (15826/41088)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss_1: (1.6453) | Acc_1: (38.56%) (16339/42368)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss_1: (1.6428) | Acc_1: (38.67%) (16879/43648)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss_1: (1.6421) | Acc_1: (38.77%) (17417/44928)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss_1: (1.6410) | Acc_1: (38.81%) (17934/46208)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss_1: (1.6399) | Acc_1: (38.88%) (18463/47488)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss_1: (1.6377) | Acc_1: (38.93%) (18986/48768)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss_1: (1.6360) | Acc_1: (38.99%) (19497/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.5288) | Acc: (42.26%) (4226/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.5170) | Acc_1: (40.62%) (52/128)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.5594) | Acc_1: (42.54%) (599/1408)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.5611) | Acc_1: (42.89%) (1153/2688)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.5574) | Acc_1: (42.62%) (1691/3968)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss_1: (1.5525) | Acc_1: (42.32%) (2221/5248)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss_1: (1.5455) | Acc_1: (42.54%) (2777/6528)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss_1: (1.5459) | Acc_1: (42.66%) (3331/7808)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss_1: (1.5379) | Acc_1: (43.05%) (3912/9088)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss_1: (1.5414) | Acc_1: (42.87%) (4445/10368)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss_1: (1.5423) | Acc_1: (42.63%) (4966/11648)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss_1: (1.5383) | Acc_1: (42.89%) (5545/12928)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss_1: (1.5407) | Acc_1: (42.70%) (6067/14208)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss_1: (1.5475) | Acc_1: (42.39%) (6565/15488)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss_1: (1.5516) | Acc_1: (42.21%) (7078/16768)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss_1: (1.5501) | Acc_1: (42.19%) (7614/18048)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss_1: (1.5499) | Acc_1: (42.26%) (8168/19328)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss_1: (1.5456) | Acc_1: (42.39%) (8736/20608)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss_1: (1.5410) | Acc_1: (42.53%) (9309/21888)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss_1: (1.5384) | Acc_1: (42.70%) (9893/23168)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss_1: (1.5336) | Acc_1: (42.88%) (10483/24448)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss_1: (1.5310) | Acc_1: (42.94%) (11047/25728)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss_1: (1.5317) | Acc_1: (42.94%) (11598/27008)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss_1: (1.5301) | Acc_1: (43.01%) (12167/28288)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss_1: (1.5294) | Acc_1: (43.09%) (12741/29568)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss_1: (1.5290) | Acc_1: (43.11%) (13300/30848)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss_1: (1.5274) | Acc_1: (43.20%) (13878/32128)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss_1: (1.5242) | Acc_1: (43.34%) (14479/33408)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss_1: (1.5235) | Acc_1: (43.33%) (15029/34688)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss_1: (1.5212) | Acc_1: (43.42%) (15619/35968)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss_1: (1.5183) | Acc_1: (43.52%) (16212/37248)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss_1: (1.5162) | Acc_1: (43.61%) (16803/38528)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss_1: (1.5148) | Acc_1: (43.61%) (17362/39808)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss_1: (1.5125) | Acc_1: (43.67%) (17943/41088)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss_1: (1.5109) | Acc_1: (43.80%) (18557/42368)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss_1: (1.5095) | Acc_1: (43.91%) (19164/43648)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss_1: (1.5090) | Acc_1: (43.93%) (19738/44928)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss_1: (1.5076) | Acc_1: (43.97%) (20319/46208)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss_1: (1.5064) | Acc_1: (43.99%) (20892/47488)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss_1: (1.5052) | Acc_1: (44.10%) (21506/48768)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss_1: (1.5040) | Acc_1: (44.17%) (22083/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3406) | Acc: (50.72%) (5072/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (1.3472) | Acc_1: (50.00%) (64/128)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (1.3900) | Acc_1: (50.07%) (705/1408)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (1.4033) | Acc_1: (49.33%) (1326/2688)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (1.4249) | Acc_1: (47.45%) (1883/3968)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss_1: (1.4130) | Acc_1: (48.06%) (2522/5248)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss_1: (1.4175) | Acc_1: (48.25%) (3150/6528)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss_1: (1.4268) | Acc_1: (47.63%) (3719/7808)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss_1: (1.4293) | Acc_1: (47.38%) (4306/9088)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss_1: (1.4330) | Acc_1: (47.22%) (4896/10368)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss_1: (1.4330) | Acc_1: (47.10%) (5486/11648)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss_1: (1.4356) | Acc_1: (47.10%) (6089/12928)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss_1: (1.4380) | Acc_1: (46.88%) (6660/14208)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss_1: (1.4372) | Acc_1: (46.93%) (7269/15488)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss_1: (1.4356) | Acc_1: (47.11%) (7899/16768)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss_1: (1.4367) | Acc_1: (47.15%) (8510/18048)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss_1: (1.4402) | Acc_1: (47.08%) (9099/19328)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss_1: (1.4424) | Acc_1: (46.99%) (9683/20608)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss_1: (1.4414) | Acc_1: (46.94%) (10275/21888)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss_1: (1.4400) | Acc_1: (47.07%) (10906/23168)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss_1: (1.4379) | Acc_1: (47.20%) (11540/24448)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss_1: (1.4370) | Acc_1: (47.25%) (12157/25728)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss_1: (1.4351) | Acc_1: (47.30%) (12774/27008)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss_1: (1.4342) | Acc_1: (47.36%) (13397/28288)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss_1: (1.4348) | Acc_1: (47.25%) (13972/29568)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss_1: (1.4330) | Acc_1: (47.34%) (14602/30848)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss_1: (1.4293) | Acc_1: (47.39%) (15224/32128)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss_1: (1.4251) | Acc_1: (47.54%) (15882/33408)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss_1: (1.4225) | Acc_1: (47.62%) (16518/34688)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss_1: (1.4202) | Acc_1: (47.68%) (17149/35968)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss_1: (1.4187) | Acc_1: (47.81%) (17808/37248)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss_1: (1.4160) | Acc_1: (47.90%) (18453/38528)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss_1: (1.4145) | Acc_1: (47.95%) (19087/39808)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss_1: (1.4135) | Acc_1: (48.06%) (19745/41088)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss_1: (1.4131) | Acc_1: (48.06%) (20362/42368)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss_1: (1.4104) | Acc_1: (48.16%) (21023/43648)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss_1: (1.4080) | Acc_1: (48.27%) (21686/44928)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss_1: (1.4051) | Acc_1: (48.36%) (22347/46208)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss_1: (1.4037) | Acc_1: (48.38%) (22975/47488)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss_1: (1.4020) | Acc_1: (48.42%) (23614/48768)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss_1: (1.4003) | Acc_1: (48.52%) (24258/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2485) | Acc: (54.64%) (5464/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (1.3585) | Acc_1: (50.78%) (65/128)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (1.3517) | Acc_1: (50.85%) (716/1408)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (1.3660) | Acc_1: (50.67%) (1362/2688)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (1.3582) | Acc_1: (50.78%) (2015/3968)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss_1: (1.3552) | Acc_1: (50.86%) (2669/5248)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss_1: (1.3504) | Acc_1: (51.07%) (3334/6528)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss_1: (1.3564) | Acc_1: (50.85%) (3970/7808)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss_1: (1.3543) | Acc_1: (50.80%) (4617/9088)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss_1: (1.3565) | Acc_1: (50.68%) (5255/10368)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss_1: (1.3506) | Acc_1: (50.84%) (5922/11648)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss_1: (1.3473) | Acc_1: (50.98%) (6591/12928)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss_1: (1.3445) | Acc_1: (50.97%) (7242/14208)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss_1: (1.3431) | Acc_1: (51.05%) (7907/15488)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss_1: (1.3428) | Acc_1: (51.14%) (8575/16768)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss_1: (1.3435) | Acc_1: (51.12%) (9226/18048)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss_1: (1.3441) | Acc_1: (51.12%) (9880/19328)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss_1: (1.3439) | Acc_1: (51.18%) (10547/20608)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss_1: (1.3429) | Acc_1: (51.17%) (11200/21888)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss_1: (1.3447) | Acc_1: (51.07%) (11833/23168)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss_1: (1.3444) | Acc_1: (51.06%) (12482/24448)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss_1: (1.3423) | Acc_1: (51.15%) (13160/25728)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss_1: (1.3421) | Acc_1: (51.17%) (13820/27008)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss_1: (1.3409) | Acc_1: (51.23%) (14492/28288)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss_1: (1.3415) | Acc_1: (51.21%) (15143/29568)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss_1: (1.3394) | Acc_1: (51.27%) (15817/30848)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss_1: (1.3360) | Acc_1: (51.32%) (16489/32128)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss_1: (1.3342) | Acc_1: (51.34%) (17152/33408)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss_1: (1.3345) | Acc_1: (51.30%) (17794/34688)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss_1: (1.3333) | Acc_1: (51.32%) (18458/35968)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss_1: (1.3307) | Acc_1: (51.43%) (19157/37248)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss_1: (1.3275) | Acc_1: (51.52%) (19851/38528)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss_1: (1.3257) | Acc_1: (51.66%) (20565/39808)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss_1: (1.3229) | Acc_1: (51.79%) (21280/41088)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss_1: (1.3215) | Acc_1: (51.84%) (21963/42368)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss_1: (1.3219) | Acc_1: (51.83%) (22624/43648)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss_1: (1.3211) | Acc_1: (51.84%) (23292/44928)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss_1: (1.3205) | Acc_1: (51.91%) (23986/46208)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss_1: (1.3209) | Acc_1: (51.92%) (24658/47488)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss_1: (1.3206) | Acc_1: (51.93%) (25327/48768)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss_1: (1.3196) | Acc_1: (52.00%) (25998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1527) | Acc: (58.60%) (5860/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (1.2651) | Acc_1: (54.69%) (70/128)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (1.3327) | Acc_1: (51.07%) (719/1408)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (1.3373) | Acc_1: (50.97%) (1370/2688)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (1.3264) | Acc_1: (51.71%) (2052/3968)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss_1: (1.3207) | Acc_1: (51.52%) (2704/5248)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss_1: (1.3109) | Acc_1: (52.16%) (3405/6528)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss_1: (1.3025) | Acc_1: (52.50%) (4099/7808)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss_1: (1.3068) | Acc_1: (52.24%) (4748/9088)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss_1: (1.3059) | Acc_1: (52.33%) (5426/10368)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss_1: (1.2996) | Acc_1: (52.56%) (6122/11648)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss_1: (1.2935) | Acc_1: (52.82%) (6828/12928)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss_1: (1.2958) | Acc_1: (52.63%) (7478/14208)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss_1: (1.2909) | Acc_1: (52.86%) (8187/15488)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss_1: (1.2890) | Acc_1: (52.95%) (8879/16768)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss_1: (1.2903) | Acc_1: (52.89%) (9546/18048)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss_1: (1.2877) | Acc_1: (53.01%) (10245/19328)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss_1: (1.2868) | Acc_1: (53.08%) (10938/20608)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss_1: (1.2874) | Acc_1: (53.10%) (11622/21888)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss_1: (1.2801) | Acc_1: (53.42%) (12377/23168)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss_1: (1.2791) | Acc_1: (53.42%) (13061/24448)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss_1: (1.2782) | Acc_1: (53.44%) (13748/25728)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss_1: (1.2752) | Acc_1: (53.50%) (14449/27008)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss_1: (1.2744) | Acc_1: (53.56%) (15150/28288)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss_1: (1.2747) | Acc_1: (53.57%) (15839/29568)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss_1: (1.2721) | Acc_1: (53.71%) (16569/30848)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss_1: (1.2719) | Acc_1: (53.77%) (17274/32128)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss_1: (1.2722) | Acc_1: (53.72%) (17948/33408)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss_1: (1.2712) | Acc_1: (53.70%) (18627/34688)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss_1: (1.2692) | Acc_1: (53.78%) (19344/35968)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss_1: (1.2686) | Acc_1: (53.87%) (20066/37248)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss_1: (1.2679) | Acc_1: (53.91%) (20771/38528)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss_1: (1.2656) | Acc_1: (54.01%) (21499/39808)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss_1: (1.2651) | Acc_1: (54.05%) (22209/41088)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss_1: (1.2642) | Acc_1: (54.09%) (22916/42368)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss_1: (1.2645) | Acc_1: (54.10%) (23614/43648)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss_1: (1.2619) | Acc_1: (54.22%) (24358/44928)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss_1: (1.2616) | Acc_1: (54.23%) (25060/46208)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss_1: (1.2605) | Acc_1: (54.29%) (25781/47488)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss_1: (1.2586) | Acc_1: (54.32%) (26493/48768)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss_1: (1.2582) | Acc_1: (54.33%) (27164/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1265) | Acc: (58.61%) (5861/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (1.2394) | Acc_1: (54.69%) (70/128)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (1.2289) | Acc_1: (54.47%) (767/1408)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (1.2196) | Acc_1: (55.02%) (1479/2688)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (1.2079) | Acc_1: (55.57%) (2205/3968)\n",
      "Epoch: 8 | Batch_idx: 40 |  Loss_1: (1.2102) | Acc_1: (55.34%) (2904/5248)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss_1: (1.1975) | Acc_1: (56.04%) (3658/6528)\n",
      "Epoch: 8 | Batch_idx: 60 |  Loss_1: (1.1997) | Acc_1: (56.19%) (4387/7808)\n",
      "Epoch: 8 | Batch_idx: 70 |  Loss_1: (1.1962) | Acc_1: (56.29%) (5116/9088)\n",
      "Epoch: 8 | Batch_idx: 80 |  Loss_1: (1.1963) | Acc_1: (56.38%) (5845/10368)\n",
      "Epoch: 8 | Batch_idx: 90 |  Loss_1: (1.1951) | Acc_1: (56.41%) (6571/11648)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss_1: (1.1981) | Acc_1: (56.34%) (7283/12928)\n",
      "Epoch: 8 | Batch_idx: 110 |  Loss_1: (1.2013) | Acc_1: (56.30%) (7999/14208)\n",
      "Epoch: 8 | Batch_idx: 120 |  Loss_1: (1.1997) | Acc_1: (56.35%) (8727/15488)\n",
      "Epoch: 8 | Batch_idx: 130 |  Loss_1: (1.1984) | Acc_1: (56.42%) (9461/16768)\n",
      "Epoch: 8 | Batch_idx: 140 |  Loss_1: (1.1973) | Acc_1: (56.44%) (10187/18048)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss_1: (1.1967) | Acc_1: (56.57%) (10934/19328)\n",
      "Epoch: 8 | Batch_idx: 160 |  Loss_1: (1.1971) | Acc_1: (56.54%) (11652/20608)\n",
      "Epoch: 8 | Batch_idx: 170 |  Loss_1: (1.1970) | Acc_1: (56.44%) (12353/21888)\n",
      "Epoch: 8 | Batch_idx: 180 |  Loss_1: (1.1996) | Acc_1: (56.33%) (13051/23168)\n",
      "Epoch: 8 | Batch_idx: 190 |  Loss_1: (1.1989) | Acc_1: (56.33%) (13771/24448)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss_1: (1.2013) | Acc_1: (56.34%) (14494/25728)\n",
      "Epoch: 8 | Batch_idx: 210 |  Loss_1: (1.2020) | Acc_1: (56.32%) (15212/27008)\n",
      "Epoch: 8 | Batch_idx: 220 |  Loss_1: (1.2039) | Acc_1: (56.26%) (15914/28288)\n",
      "Epoch: 8 | Batch_idx: 230 |  Loss_1: (1.2031) | Acc_1: (56.26%) (16636/29568)\n",
      "Epoch: 8 | Batch_idx: 240 |  Loss_1: (1.2033) | Acc_1: (56.25%) (17352/30848)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss_1: (1.2037) | Acc_1: (56.24%) (18070/32128)\n",
      "Epoch: 8 | Batch_idx: 260 |  Loss_1: (1.2021) | Acc_1: (56.35%) (18824/33408)\n",
      "Epoch: 8 | Batch_idx: 270 |  Loss_1: (1.2023) | Acc_1: (56.37%) (19553/34688)\n",
      "Epoch: 8 | Batch_idx: 280 |  Loss_1: (1.1997) | Acc_1: (56.42%) (20293/35968)\n",
      "Epoch: 8 | Batch_idx: 290 |  Loss_1: (1.1982) | Acc_1: (56.53%) (21057/37248)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss_1: (1.1984) | Acc_1: (56.52%) (21775/38528)\n",
      "Epoch: 8 | Batch_idx: 310 |  Loss_1: (1.1980) | Acc_1: (56.51%) (22497/39808)\n",
      "Epoch: 8 | Batch_idx: 320 |  Loss_1: (1.1953) | Acc_1: (56.60%) (23255/41088)\n",
      "Epoch: 8 | Batch_idx: 330 |  Loss_1: (1.1948) | Acc_1: (56.63%) (23994/42368)\n",
      "Epoch: 8 | Batch_idx: 340 |  Loss_1: (1.1929) | Acc_1: (56.76%) (24775/43648)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss_1: (1.1931) | Acc_1: (56.82%) (25530/44928)\n",
      "Epoch: 8 | Batch_idx: 360 |  Loss_1: (1.1924) | Acc_1: (56.87%) (26280/46208)\n",
      "Epoch: 8 | Batch_idx: 370 |  Loss_1: (1.1914) | Acc_1: (56.90%) (27022/47488)\n",
      "Epoch: 8 | Batch_idx: 380 |  Loss_1: (1.1913) | Acc_1: (56.91%) (27752/48768)\n",
      "Epoch: 8 | Batch_idx: 390 |  Loss_1: (1.1915) | Acc_1: (56.88%) (28439/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0441) | Acc: (62.09%) (6209/10000)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (1.0977) | Acc_1: (57.03%) (73/128)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (1.1586) | Acc_1: (58.31%) (821/1408)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (1.1751) | Acc_1: (58.22%) (1565/2688)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (1.1629) | Acc_1: (58.04%) (2303/3968)\n",
      "Epoch: 9 | Batch_idx: 40 |  Loss_1: (1.1629) | Acc_1: (57.87%) (3037/5248)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss_1: (1.1580) | Acc_1: (58.32%) (3807/6528)\n",
      "Epoch: 9 | Batch_idx: 60 |  Loss_1: (1.1597) | Acc_1: (58.15%) (4540/7808)\n",
      "Epoch: 9 | Batch_idx: 70 |  Loss_1: (1.1631) | Acc_1: (58.07%) (5277/9088)\n",
      "Epoch: 9 | Batch_idx: 80 |  Loss_1: (1.1644) | Acc_1: (57.83%) (5996/10368)\n",
      "Epoch: 9 | Batch_idx: 90 |  Loss_1: (1.1605) | Acc_1: (58.06%) (6763/11648)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss_1: (1.1607) | Acc_1: (58.21%) (7526/12928)\n",
      "Epoch: 9 | Batch_idx: 110 |  Loss_1: (1.1561) | Acc_1: (58.28%) (8280/14208)\n",
      "Epoch: 9 | Batch_idx: 120 |  Loss_1: (1.1550) | Acc_1: (58.08%) (8995/15488)\n",
      "Epoch: 9 | Batch_idx: 130 |  Loss_1: (1.1557) | Acc_1: (58.13%) (9748/16768)\n",
      "Epoch: 9 | Batch_idx: 140 |  Loss_1: (1.1548) | Acc_1: (58.19%) (10502/18048)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss_1: (1.1515) | Acc_1: (58.31%) (11271/19328)\n",
      "Epoch: 9 | Batch_idx: 160 |  Loss_1: (1.1497) | Acc_1: (58.31%) (12017/20608)\n",
      "Epoch: 9 | Batch_idx: 170 |  Loss_1: (1.1479) | Acc_1: (58.37%) (12776/21888)\n",
      "Epoch: 9 | Batch_idx: 180 |  Loss_1: (1.1498) | Acc_1: (58.30%) (13508/23168)\n",
      "Epoch: 9 | Batch_idx: 190 |  Loss_1: (1.1502) | Acc_1: (58.29%) (14250/24448)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss_1: (1.1507) | Acc_1: (58.23%) (14981/25728)\n",
      "Epoch: 9 | Batch_idx: 210 |  Loss_1: (1.1492) | Acc_1: (58.31%) (15749/27008)\n",
      "Epoch: 9 | Batch_idx: 220 |  Loss_1: (1.1485) | Acc_1: (58.24%) (16476/28288)\n",
      "Epoch: 9 | Batch_idx: 230 |  Loss_1: (1.1493) | Acc_1: (58.22%) (17214/29568)\n",
      "Epoch: 9 | Batch_idx: 240 |  Loss_1: (1.1489) | Acc_1: (58.18%) (17948/30848)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss_1: (1.1482) | Acc_1: (58.23%) (18709/32128)\n",
      "Epoch: 9 | Batch_idx: 260 |  Loss_1: (1.1492) | Acc_1: (58.24%) (19457/33408)\n",
      "Epoch: 9 | Batch_idx: 270 |  Loss_1: (1.1481) | Acc_1: (58.30%) (20224/34688)\n",
      "Epoch: 9 | Batch_idx: 280 |  Loss_1: (1.1483) | Acc_1: (58.31%) (20972/35968)\n",
      "Epoch: 9 | Batch_idx: 290 |  Loss_1: (1.1481) | Acc_1: (58.29%) (21710/37248)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss_1: (1.1471) | Acc_1: (58.34%) (22477/38528)\n",
      "Epoch: 9 | Batch_idx: 310 |  Loss_1: (1.1452) | Acc_1: (58.46%) (23271/39808)\n",
      "Epoch: 9 | Batch_idx: 320 |  Loss_1: (1.1448) | Acc_1: (58.56%) (24062/41088)\n",
      "Epoch: 9 | Batch_idx: 330 |  Loss_1: (1.1447) | Acc_1: (58.57%) (24815/42368)\n",
      "Epoch: 9 | Batch_idx: 340 |  Loss_1: (1.1433) | Acc_1: (58.63%) (25591/43648)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss_1: (1.1410) | Acc_1: (58.69%) (26370/44928)\n",
      "Epoch: 9 | Batch_idx: 360 |  Loss_1: (1.1392) | Acc_1: (58.79%) (27165/46208)\n",
      "Epoch: 9 | Batch_idx: 370 |  Loss_1: (1.1380) | Acc_1: (58.85%) (27948/47488)\n",
      "Epoch: 9 | Batch_idx: 380 |  Loss_1: (1.1397) | Acc_1: (58.80%) (28675/48768)\n",
      "Epoch: 9 | Batch_idx: 390 |  Loss_1: (1.1397) | Acc_1: (58.82%) (29411/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9673) | Acc: (65.53%) (6553/10000)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (1.4496) | Acc_1: (46.09%) (59/128)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (1.1223) | Acc_1: (60.01%) (845/1408)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (1.1306) | Acc_1: (59.00%) (1586/2688)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (1.1271) | Acc_1: (58.72%) (2330/3968)\n",
      "Epoch: 10 | Batch_idx: 40 |  Loss_1: (1.1228) | Acc_1: (59.45%) (3120/5248)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss_1: (1.1255) | Acc_1: (59.19%) (3864/6528)\n",
      "Epoch: 10 | Batch_idx: 60 |  Loss_1: (1.1318) | Acc_1: (58.82%) (4593/7808)\n",
      "Epoch: 10 | Batch_idx: 70 |  Loss_1: (1.1376) | Acc_1: (58.79%) (5343/9088)\n",
      "Epoch: 10 | Batch_idx: 80 |  Loss_1: (1.1357) | Acc_1: (58.89%) (6106/10368)\n",
      "Epoch: 10 | Batch_idx: 90 |  Loss_1: (1.1287) | Acc_1: (59.23%) (6899/11648)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss_1: (1.1257) | Acc_1: (59.42%) (7682/12928)\n",
      "Epoch: 10 | Batch_idx: 110 |  Loss_1: (1.1214) | Acc_1: (59.61%) (8469/14208)\n",
      "Epoch: 10 | Batch_idx: 120 |  Loss_1: (1.1247) | Acc_1: (59.58%) (9228/15488)\n",
      "Epoch: 10 | Batch_idx: 130 |  Loss_1: (1.1230) | Acc_1: (59.73%) (10016/16768)\n",
      "Epoch: 10 | Batch_idx: 140 |  Loss_1: (1.1220) | Acc_1: (59.72%) (10779/18048)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss_1: (1.1227) | Acc_1: (59.60%) (11519/19328)\n",
      "Epoch: 10 | Batch_idx: 160 |  Loss_1: (1.1225) | Acc_1: (59.60%) (12282/20608)\n",
      "Epoch: 10 | Batch_idx: 170 |  Loss_1: (1.1230) | Acc_1: (59.64%) (13053/21888)\n",
      "Epoch: 10 | Batch_idx: 180 |  Loss_1: (1.1217) | Acc_1: (59.70%) (13832/23168)\n",
      "Epoch: 10 | Batch_idx: 190 |  Loss_1: (1.1200) | Acc_1: (59.76%) (14611/24448)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss_1: (1.1197) | Acc_1: (59.81%) (15389/25728)\n",
      "Epoch: 10 | Batch_idx: 210 |  Loss_1: (1.1202) | Acc_1: (59.73%) (16133/27008)\n",
      "Epoch: 10 | Batch_idx: 220 |  Loss_1: (1.1207) | Acc_1: (59.75%) (16901/28288)\n",
      "Epoch: 10 | Batch_idx: 230 |  Loss_1: (1.1193) | Acc_1: (59.81%) (17686/29568)\n",
      "Epoch: 10 | Batch_idx: 240 |  Loss_1: (1.1191) | Acc_1: (59.80%) (18447/30848)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss_1: (1.1181) | Acc_1: (59.79%) (19210/32128)\n",
      "Epoch: 10 | Batch_idx: 260 |  Loss_1: (1.1165) | Acc_1: (59.82%) (19986/33408)\n",
      "Epoch: 10 | Batch_idx: 270 |  Loss_1: (1.1141) | Acc_1: (59.82%) (20751/34688)\n",
      "Epoch: 10 | Batch_idx: 280 |  Loss_1: (1.1133) | Acc_1: (59.84%) (21525/35968)\n",
      "Epoch: 10 | Batch_idx: 290 |  Loss_1: (1.1118) | Acc_1: (59.94%) (22328/37248)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss_1: (1.1098) | Acc_1: (60.01%) (23122/38528)\n",
      "Epoch: 10 | Batch_idx: 310 |  Loss_1: (1.1110) | Acc_1: (59.97%) (23873/39808)\n",
      "Epoch: 10 | Batch_idx: 320 |  Loss_1: (1.1105) | Acc_1: (60.03%) (24665/41088)\n",
      "Epoch: 10 | Batch_idx: 330 |  Loss_1: (1.1105) | Acc_1: (60.02%) (25429/42368)\n",
      "Epoch: 10 | Batch_idx: 340 |  Loss_1: (1.1105) | Acc_1: (60.03%) (26204/43648)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss_1: (1.1083) | Acc_1: (60.15%) (27022/44928)\n",
      "Epoch: 10 | Batch_idx: 360 |  Loss_1: (1.1069) | Acc_1: (60.21%) (27821/46208)\n",
      "Epoch: 10 | Batch_idx: 370 |  Loss_1: (1.1060) | Acc_1: (60.26%) (28618/47488)\n",
      "Epoch: 10 | Batch_idx: 380 |  Loss_1: (1.1055) | Acc_1: (60.29%) (29404/48768)\n",
      "Epoch: 10 | Batch_idx: 390 |  Loss_1: (1.1044) | Acc_1: (60.34%) (30169/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1301) | Acc: (59.07%) (5907/10000)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (1.1971) | Acc_1: (53.12%) (68/128)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (1.0805) | Acc_1: (62.22%) (876/1408)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (1.0745) | Acc_1: (61.87%) (1663/2688)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (1.0931) | Acc_1: (60.94%) (2418/3968)\n",
      "Epoch: 11 | Batch_idx: 40 |  Loss_1: (1.0911) | Acc_1: (61.07%) (3205/5248)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss_1: (1.0883) | Acc_1: (61.24%) (3998/6528)\n",
      "Epoch: 11 | Batch_idx: 60 |  Loss_1: (1.0795) | Acc_1: (61.42%) (4796/7808)\n",
      "Epoch: 11 | Batch_idx: 70 |  Loss_1: (1.0743) | Acc_1: (61.52%) (5591/9088)\n",
      "Epoch: 11 | Batch_idx: 80 |  Loss_1: (1.0748) | Acc_1: (61.54%) (6380/10368)\n",
      "Epoch: 11 | Batch_idx: 90 |  Loss_1: (1.0714) | Acc_1: (61.73%) (7190/11648)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss_1: (1.0713) | Acc_1: (61.69%) (7975/12928)\n",
      "Epoch: 11 | Batch_idx: 110 |  Loss_1: (1.0666) | Acc_1: (61.89%) (8793/14208)\n",
      "Epoch: 11 | Batch_idx: 120 |  Loss_1: (1.0636) | Acc_1: (62.09%) (9616/15488)\n",
      "Epoch: 11 | Batch_idx: 130 |  Loss_1: (1.0610) | Acc_1: (62.14%) (10419/16768)\n",
      "Epoch: 11 | Batch_idx: 140 |  Loss_1: (1.0632) | Acc_1: (62.03%) (11195/18048)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss_1: (1.0628) | Acc_1: (62.04%) (11991/19328)\n",
      "Epoch: 11 | Batch_idx: 160 |  Loss_1: (1.0626) | Acc_1: (62.05%) (12788/20608)\n",
      "Epoch: 11 | Batch_idx: 170 |  Loss_1: (1.0624) | Acc_1: (62.01%) (13572/21888)\n",
      "Epoch: 11 | Batch_idx: 180 |  Loss_1: (1.0604) | Acc_1: (62.15%) (14398/23168)\n",
      "Epoch: 11 | Batch_idx: 190 |  Loss_1: (1.0604) | Acc_1: (62.13%) (15189/24448)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss_1: (1.0609) | Acc_1: (62.05%) (15965/25728)\n",
      "Epoch: 11 | Batch_idx: 210 |  Loss_1: (1.0626) | Acc_1: (62.04%) (16756/27008)\n",
      "Epoch: 11 | Batch_idx: 220 |  Loss_1: (1.0647) | Acc_1: (61.92%) (17515/28288)\n",
      "Epoch: 11 | Batch_idx: 230 |  Loss_1: (1.0654) | Acc_1: (61.89%) (18300/29568)\n",
      "Epoch: 11 | Batch_idx: 240 |  Loss_1: (1.0663) | Acc_1: (61.88%) (19088/30848)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss_1: (1.0660) | Acc_1: (61.90%) (19886/32128)\n",
      "Epoch: 11 | Batch_idx: 260 |  Loss_1: (1.0656) | Acc_1: (61.91%) (20682/33408)\n",
      "Epoch: 11 | Batch_idx: 270 |  Loss_1: (1.0652) | Acc_1: (61.91%) (21474/34688)\n",
      "Epoch: 11 | Batch_idx: 280 |  Loss_1: (1.0647) | Acc_1: (61.92%) (22272/35968)\n",
      "Epoch: 11 | Batch_idx: 290 |  Loss_1: (1.0645) | Acc_1: (61.94%) (23071/37248)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss_1: (1.0630) | Acc_1: (62.02%) (23894/38528)\n",
      "Epoch: 11 | Batch_idx: 310 |  Loss_1: (1.0613) | Acc_1: (62.08%) (24712/39808)\n",
      "Epoch: 11 | Batch_idx: 320 |  Loss_1: (1.0623) | Acc_1: (62.07%) (25503/41088)\n",
      "Epoch: 11 | Batch_idx: 330 |  Loss_1: (1.0617) | Acc_1: (62.07%) (26298/42368)\n",
      "Epoch: 11 | Batch_idx: 340 |  Loss_1: (1.0620) | Acc_1: (62.07%) (27094/43648)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss_1: (1.0627) | Acc_1: (62.05%) (27877/44928)\n",
      "Epoch: 11 | Batch_idx: 360 |  Loss_1: (1.0614) | Acc_1: (62.08%) (28685/46208)\n",
      "Epoch: 11 | Batch_idx: 370 |  Loss_1: (1.0612) | Acc_1: (62.09%) (29485/47488)\n",
      "Epoch: 11 | Batch_idx: 380 |  Loss_1: (1.0600) | Acc_1: (62.17%) (30317/48768)\n",
      "Epoch: 11 | Batch_idx: 390 |  Loss_1: (1.0588) | Acc_1: (62.18%) (31092/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8296) | Acc: (70.98%) (7098/10000)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (1.2152) | Acc_1: (59.38%) (76/128)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (1.0329) | Acc_1: (63.00%) (887/1408)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (1.0249) | Acc_1: (63.10%) (1696/2688)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (1.0222) | Acc_1: (63.48%) (2519/3968)\n",
      "Epoch: 12 | Batch_idx: 40 |  Loss_1: (1.0186) | Acc_1: (63.85%) (3351/5248)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss_1: (1.0292) | Acc_1: (63.33%) (4134/6528)\n",
      "Epoch: 12 | Batch_idx: 60 |  Loss_1: (1.0354) | Acc_1: (63.01%) (4920/7808)\n",
      "Epoch: 12 | Batch_idx: 70 |  Loss_1: (1.0401) | Acc_1: (62.78%) (5705/9088)\n",
      "Epoch: 12 | Batch_idx: 80 |  Loss_1: (1.0407) | Acc_1: (62.93%) (6525/10368)\n",
      "Epoch: 12 | Batch_idx: 90 |  Loss_1: (1.0439) | Acc_1: (62.65%) (7298/11648)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss_1: (1.0370) | Acc_1: (62.77%) (8115/12928)\n",
      "Epoch: 12 | Batch_idx: 110 |  Loss_1: (1.0363) | Acc_1: (62.83%) (8927/14208)\n",
      "Epoch: 12 | Batch_idx: 120 |  Loss_1: (1.0369) | Acc_1: (62.88%) (9739/15488)\n",
      "Epoch: 12 | Batch_idx: 130 |  Loss_1: (1.0370) | Acc_1: (62.83%) (10535/16768)\n",
      "Epoch: 12 | Batch_idx: 140 |  Loss_1: (1.0375) | Acc_1: (62.82%) (11337/18048)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss_1: (1.0358) | Acc_1: (62.87%) (12152/19328)\n",
      "Epoch: 12 | Batch_idx: 160 |  Loss_1: (1.0405) | Acc_1: (62.61%) (12903/20608)\n",
      "Epoch: 12 | Batch_idx: 170 |  Loss_1: (1.0434) | Acc_1: (62.46%) (13671/21888)\n",
      "Epoch: 12 | Batch_idx: 180 |  Loss_1: (1.0436) | Acc_1: (62.47%) (14474/23168)\n",
      "Epoch: 12 | Batch_idx: 190 |  Loss_1: (1.0431) | Acc_1: (62.50%) (15281/24448)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss_1: (1.0412) | Acc_1: (62.51%) (16082/25728)\n",
      "Epoch: 12 | Batch_idx: 210 |  Loss_1: (1.0414) | Acc_1: (62.54%) (16890/27008)\n",
      "Epoch: 12 | Batch_idx: 220 |  Loss_1: (1.0400) | Acc_1: (62.57%) (17699/28288)\n",
      "Epoch: 12 | Batch_idx: 230 |  Loss_1: (1.0370) | Acc_1: (62.67%) (18531/29568)\n",
      "Epoch: 12 | Batch_idx: 240 |  Loss_1: (1.0353) | Acc_1: (62.71%) (19346/30848)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss_1: (1.0361) | Acc_1: (62.72%) (20150/32128)\n",
      "Epoch: 12 | Batch_idx: 260 |  Loss_1: (1.0341) | Acc_1: (62.78%) (20973/33408)\n",
      "Epoch: 12 | Batch_idx: 270 |  Loss_1: (1.0329) | Acc_1: (62.86%) (21804/34688)\n",
      "Epoch: 12 | Batch_idx: 280 |  Loss_1: (1.0315) | Acc_1: (62.90%) (22625/35968)\n",
      "Epoch: 12 | Batch_idx: 290 |  Loss_1: (1.0294) | Acc_1: (63.00%) (23467/37248)\n",
      "Epoch: 12 | Batch_idx: 300 |  Loss_1: (1.0283) | Acc_1: (63.07%) (24298/38528)\n",
      "Epoch: 12 | Batch_idx: 310 |  Loss_1: (1.0263) | Acc_1: (63.12%) (25127/39808)\n",
      "Epoch: 12 | Batch_idx: 320 |  Loss_1: (1.0263) | Acc_1: (63.09%) (25921/41088)\n",
      "Epoch: 12 | Batch_idx: 330 |  Loss_1: (1.0256) | Acc_1: (63.14%) (26753/42368)\n",
      "Epoch: 12 | Batch_idx: 340 |  Loss_1: (1.0250) | Acc_1: (63.19%) (27581/43648)\n",
      "Epoch: 12 | Batch_idx: 350 |  Loss_1: (1.0247) | Acc_1: (63.19%) (28392/44928)\n",
      "Epoch: 12 | Batch_idx: 360 |  Loss_1: (1.0234) | Acc_1: (63.25%) (29225/46208)\n",
      "Epoch: 12 | Batch_idx: 370 |  Loss_1: (1.0238) | Acc_1: (63.23%) (30029/47488)\n",
      "Epoch: 12 | Batch_idx: 380 |  Loss_1: (1.0244) | Acc_1: (63.22%) (30830/48768)\n",
      "Epoch: 12 | Batch_idx: 390 |  Loss_1: (1.0242) | Acc_1: (63.23%) (31614/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7962) | Acc: (72.24%) (7224/10000)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.8591) | Acc_1: (69.53%) (89/128)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.9671) | Acc_1: (65.06%) (916/1408)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.9818) | Acc_1: (63.95%) (1719/2688)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.9809) | Acc_1: (63.96%) (2538/3968)\n",
      "Epoch: 13 | Batch_idx: 40 |  Loss_1: (0.9996) | Acc_1: (63.78%) (3347/5248)\n",
      "Epoch: 13 | Batch_idx: 50 |  Loss_1: (1.0022) | Acc_1: (63.88%) (4170/6528)\n",
      "Epoch: 13 | Batch_idx: 60 |  Loss_1: (1.0018) | Acc_1: (63.95%) (4993/7808)\n",
      "Epoch: 13 | Batch_idx: 70 |  Loss_1: (1.0009) | Acc_1: (64.02%) (5818/9088)\n",
      "Epoch: 13 | Batch_idx: 80 |  Loss_1: (0.9986) | Acc_1: (64.12%) (6648/10368)\n",
      "Epoch: 13 | Batch_idx: 90 |  Loss_1: (0.9980) | Acc_1: (64.20%) (7478/11648)\n",
      "Epoch: 13 | Batch_idx: 100 |  Loss_1: (0.9944) | Acc_1: (64.34%) (8318/12928)\n",
      "Epoch: 13 | Batch_idx: 110 |  Loss_1: (0.9966) | Acc_1: (64.24%) (9127/14208)\n",
      "Epoch: 13 | Batch_idx: 120 |  Loss_1: (0.9940) | Acc_1: (64.35%) (9966/15488)\n",
      "Epoch: 13 | Batch_idx: 130 |  Loss_1: (0.9934) | Acc_1: (64.42%) (10802/16768)\n",
      "Epoch: 13 | Batch_idx: 140 |  Loss_1: (0.9914) | Acc_1: (64.46%) (11634/18048)\n",
      "Epoch: 13 | Batch_idx: 150 |  Loss_1: (0.9927) | Acc_1: (64.51%) (12468/19328)\n",
      "Epoch: 13 | Batch_idx: 160 |  Loss_1: (0.9933) | Acc_1: (64.43%) (13278/20608)\n",
      "Epoch: 13 | Batch_idx: 170 |  Loss_1: (0.9925) | Acc_1: (64.41%) (14099/21888)\n",
      "Epoch: 13 | Batch_idx: 180 |  Loss_1: (0.9926) | Acc_1: (64.51%) (14946/23168)\n",
      "Epoch: 13 | Batch_idx: 190 |  Loss_1: (0.9921) | Acc_1: (64.51%) (15772/24448)\n",
      "Epoch: 13 | Batch_idx: 200 |  Loss_1: (0.9916) | Acc_1: (64.54%) (16604/25728)\n",
      "Epoch: 13 | Batch_idx: 210 |  Loss_1: (0.9934) | Acc_1: (64.44%) (17405/27008)\n",
      "Epoch: 13 | Batch_idx: 220 |  Loss_1: (0.9929) | Acc_1: (64.40%) (18217/28288)\n",
      "Epoch: 13 | Batch_idx: 230 |  Loss_1: (0.9924) | Acc_1: (64.41%) (19044/29568)\n",
      "Epoch: 13 | Batch_idx: 240 |  Loss_1: (0.9932) | Acc_1: (64.43%) (19876/30848)\n",
      "Epoch: 13 | Batch_idx: 250 |  Loss_1: (0.9897) | Acc_1: (64.52%) (20730/32128)\n",
      "Epoch: 13 | Batch_idx: 260 |  Loss_1: (0.9886) | Acc_1: (64.62%) (21587/33408)\n",
      "Epoch: 13 | Batch_idx: 270 |  Loss_1: (0.9888) | Acc_1: (64.65%) (22427/34688)\n",
      "Epoch: 13 | Batch_idx: 280 |  Loss_1: (0.9868) | Acc_1: (64.72%) (23279/35968)\n",
      "Epoch: 13 | Batch_idx: 290 |  Loss_1: (0.9859) | Acc_1: (64.77%) (24124/37248)\n",
      "Epoch: 13 | Batch_idx: 300 |  Loss_1: (0.9855) | Acc_1: (64.76%) (24950/38528)\n",
      "Epoch: 13 | Batch_idx: 310 |  Loss_1: (0.9878) | Acc_1: (64.67%) (25742/39808)\n",
      "Epoch: 13 | Batch_idx: 320 |  Loss_1: (0.9889) | Acc_1: (64.66%) (26566/41088)\n",
      "Epoch: 13 | Batch_idx: 330 |  Loss_1: (0.9906) | Acc_1: (64.57%) (27359/42368)\n",
      "Epoch: 13 | Batch_idx: 340 |  Loss_1: (0.9915) | Acc_1: (64.58%) (28186/43648)\n",
      "Epoch: 13 | Batch_idx: 350 |  Loss_1: (0.9907) | Acc_1: (64.61%) (29030/44928)\n",
      "Epoch: 13 | Batch_idx: 360 |  Loss_1: (0.9905) | Acc_1: (64.61%) (29855/46208)\n",
      "Epoch: 13 | Batch_idx: 370 |  Loss_1: (0.9901) | Acc_1: (64.63%) (30692/47488)\n",
      "Epoch: 13 | Batch_idx: 380 |  Loss_1: (0.9888) | Acc_1: (64.66%) (31534/48768)\n",
      "Epoch: 13 | Batch_idx: 390 |  Loss_1: (0.9879) | Acc_1: (64.75%) (32375/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7717) | Acc: (73.68%) (7368/10000)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.8027) | Acc_1: (74.22%) (95/128)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.9343) | Acc_1: (67.40%) (949/1408)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.9518) | Acc_1: (66.26%) (1781/2688)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.9410) | Acc_1: (66.96%) (2657/3968)\n",
      "Epoch: 14 | Batch_idx: 40 |  Loss_1: (0.9535) | Acc_1: (66.71%) (3501/5248)\n",
      "Epoch: 14 | Batch_idx: 50 |  Loss_1: (0.9582) | Acc_1: (66.39%) (4334/6528)\n",
      "Epoch: 14 | Batch_idx: 60 |  Loss_1: (0.9709) | Acc_1: (65.84%) (5141/7808)\n",
      "Epoch: 14 | Batch_idx: 70 |  Loss_1: (0.9751) | Acc_1: (65.77%) (5977/9088)\n",
      "Epoch: 14 | Batch_idx: 80 |  Loss_1: (0.9727) | Acc_1: (65.79%) (6821/10368)\n",
      "Epoch: 14 | Batch_idx: 90 |  Loss_1: (0.9730) | Acc_1: (65.70%) (7653/11648)\n",
      "Epoch: 14 | Batch_idx: 100 |  Loss_1: (0.9719) | Acc_1: (65.66%) (8489/12928)\n",
      "Epoch: 14 | Batch_idx: 110 |  Loss_1: (0.9766) | Acc_1: (65.34%) (9283/14208)\n",
      "Epoch: 14 | Batch_idx: 120 |  Loss_1: (0.9776) | Acc_1: (65.22%) (10101/15488)\n",
      "Epoch: 14 | Batch_idx: 130 |  Loss_1: (0.9726) | Acc_1: (65.39%) (10965/16768)\n",
      "Epoch: 14 | Batch_idx: 140 |  Loss_1: (0.9699) | Acc_1: (65.44%) (11810/18048)\n",
      "Epoch: 14 | Batch_idx: 150 |  Loss_1: (0.9674) | Acc_1: (65.59%) (12678/19328)\n",
      "Epoch: 14 | Batch_idx: 160 |  Loss_1: (0.9629) | Acc_1: (65.74%) (13548/20608)\n",
      "Epoch: 14 | Batch_idx: 170 |  Loss_1: (0.9613) | Acc_1: (65.83%) (14409/21888)\n",
      "Epoch: 14 | Batch_idx: 180 |  Loss_1: (0.9574) | Acc_1: (65.93%) (15275/23168)\n",
      "Epoch: 14 | Batch_idx: 190 |  Loss_1: (0.9612) | Acc_1: (65.69%) (16061/24448)\n",
      "Epoch: 14 | Batch_idx: 200 |  Loss_1: (0.9627) | Acc_1: (65.67%) (16895/25728)\n",
      "Epoch: 14 | Batch_idx: 210 |  Loss_1: (0.9650) | Acc_1: (65.57%) (17710/27008)\n",
      "Epoch: 14 | Batch_idx: 220 |  Loss_1: (0.9660) | Acc_1: (65.53%) (18538/28288)\n",
      "Epoch: 14 | Batch_idx: 230 |  Loss_1: (0.9640) | Acc_1: (65.58%) (19392/29568)\n",
      "Epoch: 14 | Batch_idx: 240 |  Loss_1: (0.9628) | Acc_1: (65.63%) (20247/30848)\n",
      "Epoch: 14 | Batch_idx: 250 |  Loss_1: (0.9612) | Acc_1: (65.72%) (21114/32128)\n",
      "Epoch: 14 | Batch_idx: 260 |  Loss_1: (0.9620) | Acc_1: (65.66%) (21937/33408)\n",
      "Epoch: 14 | Batch_idx: 270 |  Loss_1: (0.9603) | Acc_1: (65.69%) (22786/34688)\n",
      "Epoch: 14 | Batch_idx: 280 |  Loss_1: (0.9603) | Acc_1: (65.68%) (23623/35968)\n",
      "Epoch: 14 | Batch_idx: 290 |  Loss_1: (0.9612) | Acc_1: (65.64%) (24449/37248)\n",
      "Epoch: 14 | Batch_idx: 300 |  Loss_1: (0.9593) | Acc_1: (65.67%) (25303/38528)\n",
      "Epoch: 14 | Batch_idx: 310 |  Loss_1: (0.9583) | Acc_1: (65.73%) (26165/39808)\n",
      "Epoch: 14 | Batch_idx: 320 |  Loss_1: (0.9576) | Acc_1: (65.71%) (26997/41088)\n",
      "Epoch: 14 | Batch_idx: 330 |  Loss_1: (0.9578) | Acc_1: (65.70%) (27835/42368)\n",
      "Epoch: 14 | Batch_idx: 340 |  Loss_1: (0.9575) | Acc_1: (65.71%) (28681/43648)\n",
      "Epoch: 14 | Batch_idx: 350 |  Loss_1: (0.9570) | Acc_1: (65.74%) (29535/44928)\n",
      "Epoch: 14 | Batch_idx: 360 |  Loss_1: (0.9576) | Acc_1: (65.71%) (30365/46208)\n",
      "Epoch: 14 | Batch_idx: 370 |  Loss_1: (0.9569) | Acc_1: (65.73%) (31212/47488)\n",
      "Epoch: 14 | Batch_idx: 380 |  Loss_1: (0.9565) | Acc_1: (65.74%) (32062/48768)\n",
      "Epoch: 14 | Batch_idx: 390 |  Loss_1: (0.9576) | Acc_1: (65.73%) (32867/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7837) | Acc: (72.67%) (7267/10000)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.8003) | Acc_1: (73.44%) (94/128)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.9763) | Acc_1: (65.06%) (916/1408)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.9570) | Acc_1: (66.00%) (1774/2688)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.9337) | Acc_1: (67.06%) (2661/3968)\n",
      "Epoch: 15 | Batch_idx: 40 |  Loss_1: (0.9340) | Acc_1: (67.11%) (3522/5248)\n",
      "Epoch: 15 | Batch_idx: 50 |  Loss_1: (0.9311) | Acc_1: (67.29%) (4393/6528)\n",
      "Epoch: 15 | Batch_idx: 60 |  Loss_1: (0.9296) | Acc_1: (67.33%) (5257/7808)\n",
      "Epoch: 15 | Batch_idx: 70 |  Loss_1: (0.9260) | Acc_1: (67.58%) (6142/9088)\n",
      "Epoch: 15 | Batch_idx: 80 |  Loss_1: (0.9245) | Acc_1: (67.58%) (7007/10368)\n",
      "Epoch: 15 | Batch_idx: 90 |  Loss_1: (0.9284) | Acc_1: (67.42%) (7853/11648)\n",
      "Epoch: 15 | Batch_idx: 100 |  Loss_1: (0.9306) | Acc_1: (67.31%) (8702/12928)\n",
      "Epoch: 15 | Batch_idx: 110 |  Loss_1: (0.9272) | Acc_1: (67.41%) (9578/14208)\n",
      "Epoch: 15 | Batch_idx: 120 |  Loss_1: (0.9256) | Acc_1: (67.39%) (10437/15488)\n",
      "Epoch: 15 | Batch_idx: 130 |  Loss_1: (0.9274) | Acc_1: (67.27%) (11280/16768)\n",
      "Epoch: 15 | Batch_idx: 140 |  Loss_1: (0.9328) | Acc_1: (66.99%) (12090/18048)\n",
      "Epoch: 15 | Batch_idx: 150 |  Loss_1: (0.9301) | Acc_1: (67.10%) (12970/19328)\n",
      "Epoch: 15 | Batch_idx: 160 |  Loss_1: (0.9315) | Acc_1: (67.08%) (13824/20608)\n",
      "Epoch: 15 | Batch_idx: 170 |  Loss_1: (0.9305) | Acc_1: (67.12%) (14691/21888)\n",
      "Epoch: 15 | Batch_idx: 180 |  Loss_1: (0.9351) | Acc_1: (66.96%) (15513/23168)\n",
      "Epoch: 15 | Batch_idx: 190 |  Loss_1: (0.9364) | Acc_1: (66.88%) (16351/24448)\n",
      "Epoch: 15 | Batch_idx: 200 |  Loss_1: (0.9370) | Acc_1: (66.85%) (17200/25728)\n",
      "Epoch: 15 | Batch_idx: 210 |  Loss_1: (0.9362) | Acc_1: (66.90%) (18068/27008)\n",
      "Epoch: 15 | Batch_idx: 220 |  Loss_1: (0.9345) | Acc_1: (66.95%) (18938/28288)\n",
      "Epoch: 15 | Batch_idx: 230 |  Loss_1: (0.9335) | Acc_1: (67.00%) (19812/29568)\n",
      "Epoch: 15 | Batch_idx: 240 |  Loss_1: (0.9354) | Acc_1: (66.97%) (20660/30848)\n",
      "Epoch: 15 | Batch_idx: 250 |  Loss_1: (0.9364) | Acc_1: (66.94%) (21506/32128)\n",
      "Epoch: 15 | Batch_idx: 260 |  Loss_1: (0.9355) | Acc_1: (66.97%) (22372/33408)\n",
      "Epoch: 15 | Batch_idx: 270 |  Loss_1: (0.9337) | Acc_1: (66.99%) (23238/34688)\n",
      "Epoch: 15 | Batch_idx: 280 |  Loss_1: (0.9333) | Acc_1: (66.97%) (24087/35968)\n",
      "Epoch: 15 | Batch_idx: 290 |  Loss_1: (0.9316) | Acc_1: (66.96%) (24942/37248)\n",
      "Epoch: 15 | Batch_idx: 300 |  Loss_1: (0.9327) | Acc_1: (66.91%) (25778/38528)\n",
      "Epoch: 15 | Batch_idx: 310 |  Loss_1: (0.9329) | Acc_1: (66.90%) (26632/39808)\n",
      "Epoch: 15 | Batch_idx: 320 |  Loss_1: (0.9307) | Acc_1: (66.98%) (27522/41088)\n",
      "Epoch: 15 | Batch_idx: 330 |  Loss_1: (0.9283) | Acc_1: (67.06%) (28414/42368)\n",
      "Epoch: 15 | Batch_idx: 340 |  Loss_1: (0.9290) | Acc_1: (67.04%) (29261/43648)\n",
      "Epoch: 15 | Batch_idx: 350 |  Loss_1: (0.9292) | Acc_1: (67.03%) (30113/44928)\n",
      "Epoch: 15 | Batch_idx: 360 |  Loss_1: (0.9286) | Acc_1: (67.01%) (30964/46208)\n",
      "Epoch: 15 | Batch_idx: 370 |  Loss_1: (0.9286) | Acc_1: (66.99%) (31811/47488)\n",
      "Epoch: 15 | Batch_idx: 380 |  Loss_1: (0.9291) | Acc_1: (66.97%) (32662/48768)\n",
      "Epoch: 15 | Batch_idx: 390 |  Loss_1: (0.9289) | Acc_1: (66.99%) (33495/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7896) | Acc: (72.03%) (7203/10000)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.8908) | Acc_1: (68.75%) (88/128)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.8862) | Acc_1: (68.75%) (968/1408)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.8853) | Acc_1: (68.34%) (1837/2688)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.8915) | Acc_1: (68.12%) (2703/3968)\n",
      "Epoch: 16 | Batch_idx: 40 |  Loss_1: (0.8991) | Acc_1: (67.76%) (3556/5248)\n",
      "Epoch: 16 | Batch_idx: 50 |  Loss_1: (0.9063) | Acc_1: (67.63%) (4415/6528)\n",
      "Epoch: 16 | Batch_idx: 60 |  Loss_1: (0.9083) | Acc_1: (67.51%) (5271/7808)\n",
      "Epoch: 16 | Batch_idx: 70 |  Loss_1: (0.9035) | Acc_1: (67.74%) (6156/9088)\n",
      "Epoch: 16 | Batch_idx: 80 |  Loss_1: (0.9194) | Acc_1: (67.21%) (6968/10368)\n",
      "Epoch: 16 | Batch_idx: 90 |  Loss_1: (0.9185) | Acc_1: (67.25%) (7833/11648)\n",
      "Epoch: 16 | Batch_idx: 100 |  Loss_1: (0.9112) | Acc_1: (67.50%) (8726/12928)\n",
      "Epoch: 16 | Batch_idx: 110 |  Loss_1: (0.9102) | Acc_1: (67.45%) (9584/14208)\n",
      "Epoch: 16 | Batch_idx: 120 |  Loss_1: (0.9129) | Acc_1: (67.50%) (10455/15488)\n",
      "Epoch: 16 | Batch_idx: 130 |  Loss_1: (0.9187) | Acc_1: (67.34%) (11292/16768)\n",
      "Epoch: 16 | Batch_idx: 140 |  Loss_1: (0.9174) | Acc_1: (67.41%) (12167/18048)\n",
      "Epoch: 16 | Batch_idx: 150 |  Loss_1: (0.9143) | Acc_1: (67.50%) (13047/19328)\n",
      "Epoch: 16 | Batch_idx: 160 |  Loss_1: (0.9137) | Acc_1: (67.43%) (13895/20608)\n",
      "Epoch: 16 | Batch_idx: 170 |  Loss_1: (0.9166) | Acc_1: (67.37%) (14745/21888)\n",
      "Epoch: 16 | Batch_idx: 180 |  Loss_1: (0.9170) | Acc_1: (67.36%) (15606/23168)\n",
      "Epoch: 16 | Batch_idx: 190 |  Loss_1: (0.9143) | Acc_1: (67.39%) (16476/24448)\n",
      "Epoch: 16 | Batch_idx: 200 |  Loss_1: (0.9106) | Acc_1: (67.51%) (17369/25728)\n",
      "Epoch: 16 | Batch_idx: 210 |  Loss_1: (0.9098) | Acc_1: (67.50%) (18230/27008)\n",
      "Epoch: 16 | Batch_idx: 220 |  Loss_1: (0.9117) | Acc_1: (67.49%) (19091/28288)\n",
      "Epoch: 16 | Batch_idx: 230 |  Loss_1: (0.9094) | Acc_1: (67.52%) (19963/29568)\n",
      "Epoch: 16 | Batch_idx: 240 |  Loss_1: (0.9096) | Acc_1: (67.47%) (20812/30848)\n",
      "Epoch: 16 | Batch_idx: 250 |  Loss_1: (0.9083) | Acc_1: (67.53%) (21696/32128)\n",
      "Epoch: 16 | Batch_idx: 260 |  Loss_1: (0.9054) | Acc_1: (67.66%) (22603/33408)\n",
      "Epoch: 16 | Batch_idx: 270 |  Loss_1: (0.9028) | Acc_1: (67.80%) (23517/34688)\n",
      "Epoch: 16 | Batch_idx: 280 |  Loss_1: (0.9005) | Acc_1: (67.89%) (24417/35968)\n",
      "Epoch: 16 | Batch_idx: 290 |  Loss_1: (0.9011) | Acc_1: (67.84%) (25268/37248)\n",
      "Epoch: 16 | Batch_idx: 300 |  Loss_1: (0.9013) | Acc_1: (67.84%) (26138/38528)\n",
      "Epoch: 16 | Batch_idx: 310 |  Loss_1: (0.9023) | Acc_1: (67.76%) (26975/39808)\n",
      "Epoch: 16 | Batch_idx: 320 |  Loss_1: (0.9013) | Acc_1: (67.82%) (27865/41088)\n",
      "Epoch: 16 | Batch_idx: 330 |  Loss_1: (0.9016) | Acc_1: (67.83%) (28737/42368)\n",
      "Epoch: 16 | Batch_idx: 340 |  Loss_1: (0.9008) | Acc_1: (67.83%) (29607/43648)\n",
      "Epoch: 16 | Batch_idx: 350 |  Loss_1: (0.8996) | Acc_1: (67.86%) (30489/44928)\n",
      "Epoch: 16 | Batch_idx: 360 |  Loss_1: (0.8998) | Acc_1: (67.90%) (31376/46208)\n",
      "Epoch: 16 | Batch_idx: 370 |  Loss_1: (0.9001) | Acc_1: (67.87%) (32231/47488)\n",
      "Epoch: 16 | Batch_idx: 380 |  Loss_1: (0.8997) | Acc_1: (67.89%) (33111/48768)\n",
      "Epoch: 16 | Batch_idx: 390 |  Loss_1: (0.9000) | Acc_1: (67.87%) (33935/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8047) | Acc: (72.62%) (7262/10000)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.7579) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.8793) | Acc_1: (68.61%) (966/1408)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.8836) | Acc_1: (68.60%) (1844/2688)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.8886) | Acc_1: (68.42%) (2715/3968)\n",
      "Epoch: 17 | Batch_idx: 40 |  Loss_1: (0.8794) | Acc_1: (68.94%) (3618/5248)\n",
      "Epoch: 17 | Batch_idx: 50 |  Loss_1: (0.8915) | Acc_1: (68.44%) (4468/6528)\n",
      "Epoch: 17 | Batch_idx: 60 |  Loss_1: (0.8902) | Acc_1: (68.63%) (5359/7808)\n",
      "Epoch: 17 | Batch_idx: 70 |  Loss_1: (0.8895) | Acc_1: (68.61%) (6235/9088)\n",
      "Epoch: 17 | Batch_idx: 80 |  Loss_1: (0.8903) | Acc_1: (68.75%) (7128/10368)\n",
      "Epoch: 17 | Batch_idx: 90 |  Loss_1: (0.8926) | Acc_1: (68.75%) (8008/11648)\n",
      "Epoch: 17 | Batch_idx: 100 |  Loss_1: (0.8931) | Acc_1: (68.80%) (8895/12928)\n",
      "Epoch: 17 | Batch_idx: 110 |  Loss_1: (0.8902) | Acc_1: (68.86%) (9783/14208)\n",
      "Epoch: 17 | Batch_idx: 120 |  Loss_1: (0.8872) | Acc_1: (68.94%) (10677/15488)\n",
      "Epoch: 17 | Batch_idx: 130 |  Loss_1: (0.8876) | Acc_1: (68.93%) (11558/16768)\n",
      "Epoch: 17 | Batch_idx: 140 |  Loss_1: (0.8863) | Acc_1: (68.90%) (12435/18048)\n",
      "Epoch: 17 | Batch_idx: 150 |  Loss_1: (0.8868) | Acc_1: (68.85%) (13308/19328)\n",
      "Epoch: 17 | Batch_idx: 160 |  Loss_1: (0.8849) | Acc_1: (68.86%) (14191/20608)\n",
      "Epoch: 17 | Batch_idx: 170 |  Loss_1: (0.8864) | Acc_1: (68.80%) (15059/21888)\n",
      "Epoch: 17 | Batch_idx: 180 |  Loss_1: (0.8836) | Acc_1: (68.84%) (15949/23168)\n",
      "Epoch: 17 | Batch_idx: 190 |  Loss_1: (0.8851) | Acc_1: (68.81%) (16823/24448)\n",
      "Epoch: 17 | Batch_idx: 200 |  Loss_1: (0.8859) | Acc_1: (68.78%) (17697/25728)\n",
      "Epoch: 17 | Batch_idx: 210 |  Loss_1: (0.8870) | Acc_1: (68.74%) (18565/27008)\n",
      "Epoch: 17 | Batch_idx: 220 |  Loss_1: (0.8844) | Acc_1: (68.77%) (19453/28288)\n",
      "Epoch: 17 | Batch_idx: 230 |  Loss_1: (0.8843) | Acc_1: (68.76%) (20330/29568)\n",
      "Epoch: 17 | Batch_idx: 240 |  Loss_1: (0.8851) | Acc_1: (68.73%) (21203/30848)\n",
      "Epoch: 17 | Batch_idx: 250 |  Loss_1: (0.8836) | Acc_1: (68.75%) (22088/32128)\n",
      "Epoch: 17 | Batch_idx: 260 |  Loss_1: (0.8846) | Acc_1: (68.74%) (22965/33408)\n",
      "Epoch: 17 | Batch_idx: 270 |  Loss_1: (0.8827) | Acc_1: (68.82%) (23872/34688)\n",
      "Epoch: 17 | Batch_idx: 280 |  Loss_1: (0.8816) | Acc_1: (68.86%) (24768/35968)\n",
      "Epoch: 17 | Batch_idx: 290 |  Loss_1: (0.8795) | Acc_1: (68.91%) (25669/37248)\n",
      "Epoch: 17 | Batch_idx: 300 |  Loss_1: (0.8786) | Acc_1: (68.96%) (26569/38528)\n",
      "Epoch: 17 | Batch_idx: 310 |  Loss_1: (0.8777) | Acc_1: (68.94%) (27443/39808)\n",
      "Epoch: 17 | Batch_idx: 320 |  Loss_1: (0.8793) | Acc_1: (68.92%) (28316/41088)\n",
      "Epoch: 17 | Batch_idx: 330 |  Loss_1: (0.8792) | Acc_1: (68.89%) (29187/42368)\n",
      "Epoch: 17 | Batch_idx: 340 |  Loss_1: (0.8793) | Acc_1: (68.89%) (30069/43648)\n",
      "Epoch: 17 | Batch_idx: 350 |  Loss_1: (0.8799) | Acc_1: (68.87%) (30940/44928)\n",
      "Epoch: 17 | Batch_idx: 360 |  Loss_1: (0.8791) | Acc_1: (68.87%) (31824/46208)\n",
      "Epoch: 17 | Batch_idx: 370 |  Loss_1: (0.8783) | Acc_1: (68.89%) (32715/47488)\n",
      "Epoch: 17 | Batch_idx: 380 |  Loss_1: (0.8765) | Acc_1: (68.93%) (33615/48768)\n",
      "Epoch: 17 | Batch_idx: 390 |  Loss_1: (0.8768) | Acc_1: (68.92%) (34459/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7276) | Acc: (74.48%) (7448/10000)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.8302) | Acc_1: (68.75%) (88/128)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.8982) | Acc_1: (67.83%) (955/1408)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.8852) | Acc_1: (68.49%) (1841/2688)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.8700) | Acc_1: (68.75%) (2728/3968)\n",
      "Epoch: 18 | Batch_idx: 40 |  Loss_1: (0.8545) | Acc_1: (69.21%) (3632/5248)\n",
      "Epoch: 18 | Batch_idx: 50 |  Loss_1: (0.8453) | Acc_1: (69.44%) (4533/6528)\n",
      "Epoch: 18 | Batch_idx: 60 |  Loss_1: (0.8500) | Acc_1: (69.60%) (5434/7808)\n",
      "Epoch: 18 | Batch_idx: 70 |  Loss_1: (0.8452) | Acc_1: (69.76%) (6340/9088)\n",
      "Epoch: 18 | Batch_idx: 80 |  Loss_1: (0.8439) | Acc_1: (69.80%) (7237/10368)\n",
      "Epoch: 18 | Batch_idx: 90 |  Loss_1: (0.8419) | Acc_1: (69.84%) (8135/11648)\n",
      "Epoch: 18 | Batch_idx: 100 |  Loss_1: (0.8431) | Acc_1: (69.79%) (9023/12928)\n",
      "Epoch: 18 | Batch_idx: 110 |  Loss_1: (0.8383) | Acc_1: (69.95%) (9939/14208)\n",
      "Epoch: 18 | Batch_idx: 120 |  Loss_1: (0.8409) | Acc_1: (69.80%) (10811/15488)\n",
      "Epoch: 18 | Batch_idx: 130 |  Loss_1: (0.8396) | Acc_1: (69.74%) (11694/16768)\n",
      "Epoch: 18 | Batch_idx: 140 |  Loss_1: (0.8370) | Acc_1: (69.84%) (12605/18048)\n",
      "Epoch: 18 | Batch_idx: 150 |  Loss_1: (0.8388) | Acc_1: (69.76%) (13484/19328)\n",
      "Epoch: 18 | Batch_idx: 160 |  Loss_1: (0.8409) | Acc_1: (69.71%) (14365/20608)\n",
      "Epoch: 18 | Batch_idx: 170 |  Loss_1: (0.8444) | Acc_1: (69.59%) (15232/21888)\n",
      "Epoch: 18 | Batch_idx: 180 |  Loss_1: (0.8458) | Acc_1: (69.49%) (16100/23168)\n",
      "Epoch: 18 | Batch_idx: 190 |  Loss_1: (0.8466) | Acc_1: (69.53%) (16999/24448)\n",
      "Epoch: 18 | Batch_idx: 200 |  Loss_1: (0.8481) | Acc_1: (69.49%) (17878/25728)\n",
      "Epoch: 18 | Batch_idx: 210 |  Loss_1: (0.8467) | Acc_1: (69.50%) (18770/27008)\n",
      "Epoch: 18 | Batch_idx: 220 |  Loss_1: (0.8453) | Acc_1: (69.58%) (19684/28288)\n",
      "Epoch: 18 | Batch_idx: 230 |  Loss_1: (0.8462) | Acc_1: (69.53%) (20559/29568)\n",
      "Epoch: 18 | Batch_idx: 240 |  Loss_1: (0.8451) | Acc_1: (69.59%) (21466/30848)\n",
      "Epoch: 18 | Batch_idx: 250 |  Loss_1: (0.8455) | Acc_1: (69.56%) (22348/32128)\n",
      "Epoch: 18 | Batch_idx: 260 |  Loss_1: (0.8463) | Acc_1: (69.60%) (23253/33408)\n",
      "Epoch: 18 | Batch_idx: 270 |  Loss_1: (0.8456) | Acc_1: (69.59%) (24138/34688)\n",
      "Epoch: 18 | Batch_idx: 280 |  Loss_1: (0.8453) | Acc_1: (69.60%) (25035/35968)\n",
      "Epoch: 18 | Batch_idx: 290 |  Loss_1: (0.8466) | Acc_1: (69.57%) (25914/37248)\n",
      "Epoch: 18 | Batch_idx: 300 |  Loss_1: (0.8464) | Acc_1: (69.59%) (26810/38528)\n",
      "Epoch: 18 | Batch_idx: 310 |  Loss_1: (0.8450) | Acc_1: (69.63%) (27719/39808)\n",
      "Epoch: 18 | Batch_idx: 320 |  Loss_1: (0.8442) | Acc_1: (69.66%) (28620/41088)\n",
      "Epoch: 18 | Batch_idx: 330 |  Loss_1: (0.8438) | Acc_1: (69.68%) (29522/42368)\n",
      "Epoch: 18 | Batch_idx: 340 |  Loss_1: (0.8431) | Acc_1: (69.71%) (30426/43648)\n",
      "Epoch: 18 | Batch_idx: 350 |  Loss_1: (0.8444) | Acc_1: (69.72%) (31324/44928)\n",
      "Epoch: 18 | Batch_idx: 360 |  Loss_1: (0.8443) | Acc_1: (69.74%) (32225/46208)\n",
      "Epoch: 18 | Batch_idx: 370 |  Loss_1: (0.8448) | Acc_1: (69.73%) (33113/47488)\n",
      "Epoch: 18 | Batch_idx: 380 |  Loss_1: (0.8448) | Acc_1: (69.74%) (34010/48768)\n",
      "Epoch: 18 | Batch_idx: 390 |  Loss_1: (0.8452) | Acc_1: (69.71%) (34855/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6618) | Acc: (77.21%) (7721/10000)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.5823) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.7748) | Acc_1: (71.80%) (1011/1408)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.7939) | Acc_1: (71.80%) (1930/2688)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.8031) | Acc_1: (71.52%) (2838/3968)\n",
      "Epoch: 19 | Batch_idx: 40 |  Loss_1: (0.8062) | Acc_1: (71.42%) (3748/5248)\n",
      "Epoch: 19 | Batch_idx: 50 |  Loss_1: (0.8034) | Acc_1: (71.43%) (4663/6528)\n",
      "Epoch: 19 | Batch_idx: 60 |  Loss_1: (0.8077) | Acc_1: (71.20%) (5559/7808)\n",
      "Epoch: 19 | Batch_idx: 70 |  Loss_1: (0.8024) | Acc_1: (71.28%) (6478/9088)\n",
      "Epoch: 19 | Batch_idx: 80 |  Loss_1: (0.8019) | Acc_1: (71.44%) (7407/10368)\n",
      "Epoch: 19 | Batch_idx: 90 |  Loss_1: (0.8020) | Acc_1: (71.36%) (8312/11648)\n",
      "Epoch: 19 | Batch_idx: 100 |  Loss_1: (0.8040) | Acc_1: (71.21%) (9206/12928)\n",
      "Epoch: 19 | Batch_idx: 110 |  Loss_1: (0.8063) | Acc_1: (71.08%) (10099/14208)\n",
      "Epoch: 19 | Batch_idx: 120 |  Loss_1: (0.8105) | Acc_1: (70.93%) (10985/15488)\n",
      "Epoch: 19 | Batch_idx: 130 |  Loss_1: (0.8118) | Acc_1: (70.89%) (11886/16768)\n",
      "Epoch: 19 | Batch_idx: 140 |  Loss_1: (0.8182) | Acc_1: (70.73%) (12766/18048)\n",
      "Epoch: 19 | Batch_idx: 150 |  Loss_1: (0.8190) | Acc_1: (70.71%) (13667/19328)\n",
      "Epoch: 19 | Batch_idx: 160 |  Loss_1: (0.8210) | Acc_1: (70.62%) (14553/20608)\n",
      "Epoch: 19 | Batch_idx: 170 |  Loss_1: (0.8198) | Acc_1: (70.76%) (15487/21888)\n",
      "Epoch: 19 | Batch_idx: 180 |  Loss_1: (0.8167) | Acc_1: (70.84%) (16412/23168)\n",
      "Epoch: 19 | Batch_idx: 190 |  Loss_1: (0.8166) | Acc_1: (70.87%) (17327/24448)\n",
      "Epoch: 19 | Batch_idx: 200 |  Loss_1: (0.8156) | Acc_1: (70.91%) (18245/25728)\n",
      "Epoch: 19 | Batch_idx: 210 |  Loss_1: (0.8148) | Acc_1: (70.96%) (19165/27008)\n",
      "Epoch: 19 | Batch_idx: 220 |  Loss_1: (0.8157) | Acc_1: (70.97%) (20075/28288)\n",
      "Epoch: 19 | Batch_idx: 230 |  Loss_1: (0.8175) | Acc_1: (70.91%) (20966/29568)\n",
      "Epoch: 19 | Batch_idx: 240 |  Loss_1: (0.8174) | Acc_1: (70.92%) (21878/30848)\n",
      "Epoch: 19 | Batch_idx: 250 |  Loss_1: (0.8162) | Acc_1: (70.98%) (22803/32128)\n",
      "Epoch: 19 | Batch_idx: 260 |  Loss_1: (0.8182) | Acc_1: (70.91%) (23691/33408)\n",
      "Epoch: 19 | Batch_idx: 270 |  Loss_1: (0.8180) | Acc_1: (70.89%) (24589/34688)\n",
      "Epoch: 19 | Batch_idx: 280 |  Loss_1: (0.8178) | Acc_1: (70.94%) (25514/35968)\n",
      "Epoch: 19 | Batch_idx: 290 |  Loss_1: (0.8178) | Acc_1: (70.93%) (26419/37248)\n",
      "Epoch: 19 | Batch_idx: 300 |  Loss_1: (0.8168) | Acc_1: (70.98%) (27346/38528)\n",
      "Epoch: 19 | Batch_idx: 310 |  Loss_1: (0.8141) | Acc_1: (71.08%) (28294/39808)\n",
      "Epoch: 19 | Batch_idx: 320 |  Loss_1: (0.8147) | Acc_1: (71.07%) (29202/41088)\n",
      "Epoch: 19 | Batch_idx: 330 |  Loss_1: (0.8161) | Acc_1: (71.00%) (30083/42368)\n",
      "Epoch: 19 | Batch_idx: 340 |  Loss_1: (0.8187) | Acc_1: (70.91%) (30950/43648)\n",
      "Epoch: 19 | Batch_idx: 350 |  Loss_1: (0.8183) | Acc_1: (70.95%) (31878/44928)\n",
      "Epoch: 19 | Batch_idx: 360 |  Loss_1: (0.8182) | Acc_1: (70.97%) (32793/46208)\n",
      "Epoch: 19 | Batch_idx: 370 |  Loss_1: (0.8183) | Acc_1: (70.97%) (33704/47488)\n",
      "Epoch: 19 | Batch_idx: 380 |  Loss_1: (0.8181) | Acc_1: (71.02%) (34634/48768)\n",
      "Epoch: 19 | Batch_idx: 390 |  Loss_1: (0.8176) | Acc_1: (71.02%) (35512/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6518) | Acc: (77.89%) (7789/10000)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.8715) | Acc_1: (69.53%) (89/128)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.7856) | Acc_1: (71.24%) (1003/1408)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.8029) | Acc_1: (71.17%) (1913/2688)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.8133) | Acc_1: (71.09%) (2821/3968)\n",
      "Epoch: 20 | Batch_idx: 40 |  Loss_1: (0.8116) | Acc_1: (71.09%) (3731/5248)\n",
      "Epoch: 20 | Batch_idx: 50 |  Loss_1: (0.8094) | Acc_1: (71.32%) (4656/6528)\n",
      "Epoch: 20 | Batch_idx: 60 |  Loss_1: (0.8125) | Acc_1: (71.22%) (5561/7808)\n",
      "Epoch: 20 | Batch_idx: 70 |  Loss_1: (0.8068) | Acc_1: (71.34%) (6483/9088)\n",
      "Epoch: 20 | Batch_idx: 80 |  Loss_1: (0.8066) | Acc_1: (71.32%) (7394/10368)\n",
      "Epoch: 20 | Batch_idx: 90 |  Loss_1: (0.8083) | Acc_1: (71.24%) (8298/11648)\n",
      "Epoch: 20 | Batch_idx: 100 |  Loss_1: (0.8027) | Acc_1: (71.50%) (9243/12928)\n",
      "Epoch: 20 | Batch_idx: 110 |  Loss_1: (0.8009) | Acc_1: (71.57%) (10169/14208)\n",
      "Epoch: 20 | Batch_idx: 120 |  Loss_1: (0.7992) | Acc_1: (71.58%) (11086/15488)\n",
      "Epoch: 20 | Batch_idx: 130 |  Loss_1: (0.8028) | Acc_1: (71.48%) (11985/16768)\n",
      "Epoch: 20 | Batch_idx: 140 |  Loss_1: (0.8042) | Acc_1: (71.38%) (12883/18048)\n",
      "Epoch: 20 | Batch_idx: 150 |  Loss_1: (0.8030) | Acc_1: (71.44%) (13808/19328)\n",
      "Epoch: 20 | Batch_idx: 160 |  Loss_1: (0.8001) | Acc_1: (71.53%) (14740/20608)\n",
      "Epoch: 20 | Batch_idx: 170 |  Loss_1: (0.8017) | Acc_1: (71.47%) (15644/21888)\n",
      "Epoch: 20 | Batch_idx: 180 |  Loss_1: (0.7976) | Acc_1: (71.61%) (16590/23168)\n",
      "Epoch: 20 | Batch_idx: 190 |  Loss_1: (0.7962) | Acc_1: (71.66%) (17520/24448)\n",
      "Epoch: 20 | Batch_idx: 200 |  Loss_1: (0.7966) | Acc_1: (71.65%) (18435/25728)\n",
      "Epoch: 20 | Batch_idx: 210 |  Loss_1: (0.7946) | Acc_1: (71.76%) (19382/27008)\n",
      "Epoch: 20 | Batch_idx: 220 |  Loss_1: (0.7950) | Acc_1: (71.74%) (20295/28288)\n",
      "Epoch: 20 | Batch_idx: 230 |  Loss_1: (0.7959) | Acc_1: (71.72%) (21207/29568)\n",
      "Epoch: 20 | Batch_idx: 240 |  Loss_1: (0.7965) | Acc_1: (71.65%) (22102/30848)\n",
      "Epoch: 20 | Batch_idx: 250 |  Loss_1: (0.7967) | Acc_1: (71.65%) (23020/32128)\n",
      "Epoch: 20 | Batch_idx: 260 |  Loss_1: (0.7960) | Acc_1: (71.66%) (23939/33408)\n",
      "Epoch: 20 | Batch_idx: 270 |  Loss_1: (0.7953) | Acc_1: (71.73%) (24880/34688)\n",
      "Epoch: 20 | Batch_idx: 280 |  Loss_1: (0.7958) | Acc_1: (71.69%) (25786/35968)\n",
      "Epoch: 20 | Batch_idx: 290 |  Loss_1: (0.7949) | Acc_1: (71.73%) (26717/37248)\n",
      "Epoch: 20 | Batch_idx: 300 |  Loss_1: (0.7942) | Acc_1: (71.76%) (27648/38528)\n",
      "Epoch: 20 | Batch_idx: 310 |  Loss_1: (0.7945) | Acc_1: (71.76%) (28566/39808)\n",
      "Epoch: 20 | Batch_idx: 320 |  Loss_1: (0.7947) | Acc_1: (71.73%) (29473/41088)\n",
      "Epoch: 20 | Batch_idx: 330 |  Loss_1: (0.7948) | Acc_1: (71.75%) (30399/42368)\n",
      "Epoch: 20 | Batch_idx: 340 |  Loss_1: (0.7955) | Acc_1: (71.76%) (31321/43648)\n",
      "Epoch: 20 | Batch_idx: 350 |  Loss_1: (0.7948) | Acc_1: (71.75%) (32236/44928)\n",
      "Epoch: 20 | Batch_idx: 360 |  Loss_1: (0.7953) | Acc_1: (71.72%) (33142/46208)\n",
      "Epoch: 20 | Batch_idx: 370 |  Loss_1: (0.7958) | Acc_1: (71.70%) (34048/47488)\n",
      "Epoch: 20 | Batch_idx: 380 |  Loss_1: (0.7951) | Acc_1: (71.75%) (34993/48768)\n",
      "Epoch: 20 | Batch_idx: 390 |  Loss_1: (0.7959) | Acc_1: (71.73%) (35865/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5685) | Acc: (80.85%) (8085/10000)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.9393) | Acc_1: (67.19%) (86/128)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.7738) | Acc_1: (71.66%) (1009/1408)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.7692) | Acc_1: (73.21%) (1968/2688)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.7835) | Acc_1: (72.83%) (2890/3968)\n",
      "Epoch: 21 | Batch_idx: 40 |  Loss_1: (0.7819) | Acc_1: (72.64%) (3812/5248)\n",
      "Epoch: 21 | Batch_idx: 50 |  Loss_1: (0.7901) | Acc_1: (71.92%) (4695/6528)\n",
      "Epoch: 21 | Batch_idx: 60 |  Loss_1: (0.7807) | Acc_1: (72.35%) (5649/7808)\n",
      "Epoch: 21 | Batch_idx: 70 |  Loss_1: (0.7796) | Acc_1: (72.35%) (6575/9088)\n",
      "Epoch: 21 | Batch_idx: 80 |  Loss_1: (0.7794) | Acc_1: (72.47%) (7514/10368)\n",
      "Epoch: 21 | Batch_idx: 90 |  Loss_1: (0.7731) | Acc_1: (72.67%) (8465/11648)\n",
      "Epoch: 21 | Batch_idx: 100 |  Loss_1: (0.7737) | Acc_1: (72.61%) (9387/12928)\n",
      "Epoch: 21 | Batch_idx: 110 |  Loss_1: (0.7714) | Acc_1: (72.60%) (10315/14208)\n",
      "Epoch: 21 | Batch_idx: 120 |  Loss_1: (0.7664) | Acc_1: (72.80%) (11275/15488)\n",
      "Epoch: 21 | Batch_idx: 130 |  Loss_1: (0.7712) | Acc_1: (72.71%) (12192/16768)\n",
      "Epoch: 21 | Batch_idx: 140 |  Loss_1: (0.7731) | Acc_1: (72.67%) (13115/18048)\n",
      "Epoch: 21 | Batch_idx: 150 |  Loss_1: (0.7724) | Acc_1: (72.70%) (14051/19328)\n",
      "Epoch: 21 | Batch_idx: 160 |  Loss_1: (0.7774) | Acc_1: (72.54%) (14950/20608)\n",
      "Epoch: 21 | Batch_idx: 170 |  Loss_1: (0.7806) | Acc_1: (72.44%) (15856/21888)\n",
      "Epoch: 21 | Batch_idx: 180 |  Loss_1: (0.7844) | Acc_1: (72.30%) (16750/23168)\n",
      "Epoch: 21 | Batch_idx: 190 |  Loss_1: (0.7866) | Acc_1: (72.21%) (17654/24448)\n",
      "Epoch: 21 | Batch_idx: 200 |  Loss_1: (0.7885) | Acc_1: (72.16%) (18565/25728)\n",
      "Epoch: 21 | Batch_idx: 210 |  Loss_1: (0.7877) | Acc_1: (72.19%) (19497/27008)\n",
      "Epoch: 21 | Batch_idx: 220 |  Loss_1: (0.7876) | Acc_1: (72.17%) (20416/28288)\n",
      "Epoch: 21 | Batch_idx: 230 |  Loss_1: (0.7853) | Acc_1: (72.24%) (21361/29568)\n",
      "Epoch: 21 | Batch_idx: 240 |  Loss_1: (0.7838) | Acc_1: (72.29%) (22299/30848)\n",
      "Epoch: 21 | Batch_idx: 250 |  Loss_1: (0.7817) | Acc_1: (72.32%) (23235/32128)\n",
      "Epoch: 21 | Batch_idx: 260 |  Loss_1: (0.7807) | Acc_1: (72.30%) (24154/33408)\n",
      "Epoch: 21 | Batch_idx: 270 |  Loss_1: (0.7790) | Acc_1: (72.32%) (25086/34688)\n",
      "Epoch: 21 | Batch_idx: 280 |  Loss_1: (0.7773) | Acc_1: (72.38%) (26032/35968)\n",
      "Epoch: 21 | Batch_idx: 290 |  Loss_1: (0.7775) | Acc_1: (72.42%) (26975/37248)\n",
      "Epoch: 21 | Batch_idx: 300 |  Loss_1: (0.7773) | Acc_1: (72.42%) (27903/38528)\n",
      "Epoch: 21 | Batch_idx: 310 |  Loss_1: (0.7754) | Acc_1: (72.50%) (28860/39808)\n",
      "Epoch: 21 | Batch_idx: 320 |  Loss_1: (0.7771) | Acc_1: (72.45%) (29769/41088)\n",
      "Epoch: 21 | Batch_idx: 330 |  Loss_1: (0.7772) | Acc_1: (72.47%) (30702/42368)\n",
      "Epoch: 21 | Batch_idx: 340 |  Loss_1: (0.7775) | Acc_1: (72.43%) (31615/43648)\n",
      "Epoch: 21 | Batch_idx: 350 |  Loss_1: (0.7766) | Acc_1: (72.48%) (32564/44928)\n",
      "Epoch: 21 | Batch_idx: 360 |  Loss_1: (0.7760) | Acc_1: (72.52%) (33509/46208)\n",
      "Epoch: 21 | Batch_idx: 370 |  Loss_1: (0.7767) | Acc_1: (72.49%) (34423/47488)\n",
      "Epoch: 21 | Batch_idx: 380 |  Loss_1: (0.7759) | Acc_1: (72.53%) (35373/48768)\n",
      "Epoch: 21 | Batch_idx: 390 |  Loss_1: (0.7753) | Acc_1: (72.54%) (36271/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5485) | Acc: (81.43%) (8143/10000)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.8606) | Acc_1: (64.84%) (83/128)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.7674) | Acc_1: (73.22%) (1031/1408)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.7421) | Acc_1: (74.14%) (1993/2688)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.7459) | Acc_1: (73.66%) (2923/3968)\n",
      "Epoch: 22 | Batch_idx: 40 |  Loss_1: (0.7493) | Acc_1: (73.46%) (3855/5248)\n",
      "Epoch: 22 | Batch_idx: 50 |  Loss_1: (0.7403) | Acc_1: (73.81%) (4818/6528)\n",
      "Epoch: 22 | Batch_idx: 60 |  Loss_1: (0.7485) | Acc_1: (73.41%) (5732/7808)\n",
      "Epoch: 22 | Batch_idx: 70 |  Loss_1: (0.7448) | Acc_1: (73.60%) (6689/9088)\n",
      "Epoch: 22 | Batch_idx: 80 |  Loss_1: (0.7412) | Acc_1: (73.75%) (7646/10368)\n",
      "Epoch: 22 | Batch_idx: 90 |  Loss_1: (0.7425) | Acc_1: (73.73%) (8588/11648)\n",
      "Epoch: 22 | Batch_idx: 100 |  Loss_1: (0.7390) | Acc_1: (73.89%) (9552/12928)\n",
      "Epoch: 22 | Batch_idx: 110 |  Loss_1: (0.7376) | Acc_1: (73.87%) (10496/14208)\n",
      "Epoch: 22 | Batch_idx: 120 |  Loss_1: (0.7365) | Acc_1: (73.86%) (11439/15488)\n",
      "Epoch: 22 | Batch_idx: 130 |  Loss_1: (0.7351) | Acc_1: (73.96%) (12401/16768)\n",
      "Epoch: 22 | Batch_idx: 140 |  Loss_1: (0.7399) | Acc_1: (73.80%) (13320/18048)\n",
      "Epoch: 22 | Batch_idx: 150 |  Loss_1: (0.7409) | Acc_1: (73.76%) (14257/19328)\n",
      "Epoch: 22 | Batch_idx: 160 |  Loss_1: (0.7417) | Acc_1: (73.75%) (15199/20608)\n",
      "Epoch: 22 | Batch_idx: 170 |  Loss_1: (0.7425) | Acc_1: (73.78%) (16148/21888)\n",
      "Epoch: 22 | Batch_idx: 180 |  Loss_1: (0.7426) | Acc_1: (73.87%) (17115/23168)\n",
      "Epoch: 22 | Batch_idx: 190 |  Loss_1: (0.7445) | Acc_1: (73.74%) (18028/24448)\n",
      "Epoch: 22 | Batch_idx: 200 |  Loss_1: (0.7449) | Acc_1: (73.67%) (18954/25728)\n",
      "Epoch: 22 | Batch_idx: 210 |  Loss_1: (0.7443) | Acc_1: (73.67%) (19896/27008)\n",
      "Epoch: 22 | Batch_idx: 220 |  Loss_1: (0.7432) | Acc_1: (73.70%) (20848/28288)\n",
      "Epoch: 22 | Batch_idx: 230 |  Loss_1: (0.7467) | Acc_1: (73.54%) (21744/29568)\n",
      "Epoch: 22 | Batch_idx: 240 |  Loss_1: (0.7490) | Acc_1: (73.48%) (22666/30848)\n",
      "Epoch: 22 | Batch_idx: 250 |  Loss_1: (0.7501) | Acc_1: (73.40%) (23581/32128)\n",
      "Epoch: 22 | Batch_idx: 260 |  Loss_1: (0.7510) | Acc_1: (73.36%) (24507/33408)\n",
      "Epoch: 22 | Batch_idx: 270 |  Loss_1: (0.7494) | Acc_1: (73.38%) (25455/34688)\n",
      "Epoch: 22 | Batch_idx: 280 |  Loss_1: (0.7511) | Acc_1: (73.30%) (26365/35968)\n",
      "Epoch: 22 | Batch_idx: 290 |  Loss_1: (0.7503) | Acc_1: (73.32%) (27309/37248)\n",
      "Epoch: 22 | Batch_idx: 300 |  Loss_1: (0.7514) | Acc_1: (73.30%) (28240/38528)\n",
      "Epoch: 22 | Batch_idx: 310 |  Loss_1: (0.7517) | Acc_1: (73.27%) (29168/39808)\n",
      "Epoch: 22 | Batch_idx: 320 |  Loss_1: (0.7526) | Acc_1: (73.22%) (30085/41088)\n",
      "Epoch: 22 | Batch_idx: 330 |  Loss_1: (0.7535) | Acc_1: (73.19%) (31009/42368)\n",
      "Epoch: 22 | Batch_idx: 340 |  Loss_1: (0.7536) | Acc_1: (73.19%) (31945/43648)\n",
      "Epoch: 22 | Batch_idx: 350 |  Loss_1: (0.7540) | Acc_1: (73.10%) (32844/44928)\n",
      "Epoch: 22 | Batch_idx: 360 |  Loss_1: (0.7529) | Acc_1: (73.14%) (33797/46208)\n",
      "Epoch: 22 | Batch_idx: 370 |  Loss_1: (0.7534) | Acc_1: (73.11%) (34717/47488)\n",
      "Epoch: 22 | Batch_idx: 380 |  Loss_1: (0.7534) | Acc_1: (73.15%) (35673/48768)\n",
      "Epoch: 22 | Batch_idx: 390 |  Loss_1: (0.7539) | Acc_1: (73.12%) (36560/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5562) | Acc: (81.05%) (8105/10000)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.8178) | Acc_1: (67.97%) (87/128)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.7832) | Acc_1: (72.23%) (1017/1408)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.7335) | Acc_1: (74.40%) (2000/2688)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.7212) | Acc_1: (74.80%) (2968/3968)\n",
      "Epoch: 23 | Batch_idx: 40 |  Loss_1: (0.7308) | Acc_1: (74.07%) (3887/5248)\n",
      "Epoch: 23 | Batch_idx: 50 |  Loss_1: (0.7402) | Acc_1: (73.79%) (4817/6528)\n",
      "Epoch: 23 | Batch_idx: 60 |  Loss_1: (0.7400) | Acc_1: (73.76%) (5759/7808)\n",
      "Epoch: 23 | Batch_idx: 70 |  Loss_1: (0.7400) | Acc_1: (73.64%) (6692/9088)\n",
      "Epoch: 23 | Batch_idx: 80 |  Loss_1: (0.7361) | Acc_1: (73.64%) (7635/10368)\n",
      "Epoch: 23 | Batch_idx: 90 |  Loss_1: (0.7380) | Acc_1: (73.69%) (8583/11648)\n",
      "Epoch: 23 | Batch_idx: 100 |  Loss_1: (0.7334) | Acc_1: (73.82%) (9544/12928)\n",
      "Epoch: 23 | Batch_idx: 110 |  Loss_1: (0.7280) | Acc_1: (74.09%) (10527/14208)\n",
      "Epoch: 23 | Batch_idx: 120 |  Loss_1: (0.7272) | Acc_1: (74.06%) (11471/15488)\n",
      "Epoch: 23 | Batch_idx: 130 |  Loss_1: (0.7317) | Acc_1: (74.03%) (12414/16768)\n",
      "Epoch: 23 | Batch_idx: 140 |  Loss_1: (0.7346) | Acc_1: (73.93%) (13342/18048)\n",
      "Epoch: 23 | Batch_idx: 150 |  Loss_1: (0.7361) | Acc_1: (73.84%) (14272/19328)\n",
      "Epoch: 23 | Batch_idx: 160 |  Loss_1: (0.7323) | Acc_1: (73.99%) (15248/20608)\n",
      "Epoch: 23 | Batch_idx: 170 |  Loss_1: (0.7300) | Acc_1: (74.01%) (16199/21888)\n",
      "Epoch: 23 | Batch_idx: 180 |  Loss_1: (0.7279) | Acc_1: (74.15%) (17180/23168)\n",
      "Epoch: 23 | Batch_idx: 190 |  Loss_1: (0.7292) | Acc_1: (74.09%) (18114/24448)\n",
      "Epoch: 23 | Batch_idx: 200 |  Loss_1: (0.7285) | Acc_1: (74.12%) (19069/25728)\n",
      "Epoch: 23 | Batch_idx: 210 |  Loss_1: (0.7280) | Acc_1: (74.13%) (20020/27008)\n",
      "Epoch: 23 | Batch_idx: 220 |  Loss_1: (0.7299) | Acc_1: (74.08%) (20955/28288)\n",
      "Epoch: 23 | Batch_idx: 230 |  Loss_1: (0.7299) | Acc_1: (74.08%) (21903/29568)\n",
      "Epoch: 23 | Batch_idx: 240 |  Loss_1: (0.7312) | Acc_1: (74.02%) (22834/30848)\n",
      "Epoch: 23 | Batch_idx: 250 |  Loss_1: (0.7306) | Acc_1: (74.04%) (23786/32128)\n",
      "Epoch: 23 | Batch_idx: 260 |  Loss_1: (0.7306) | Acc_1: (74.04%) (24734/33408)\n",
      "Epoch: 23 | Batch_idx: 270 |  Loss_1: (0.7315) | Acc_1: (74.00%) (25669/34688)\n",
      "Epoch: 23 | Batch_idx: 280 |  Loss_1: (0.7340) | Acc_1: (73.91%) (26584/35968)\n",
      "Epoch: 23 | Batch_idx: 290 |  Loss_1: (0.7339) | Acc_1: (73.92%) (27532/37248)\n",
      "Epoch: 23 | Batch_idx: 300 |  Loss_1: (0.7325) | Acc_1: (73.93%) (28484/38528)\n",
      "Epoch: 23 | Batch_idx: 310 |  Loss_1: (0.7345) | Acc_1: (73.89%) (29413/39808)\n",
      "Epoch: 23 | Batch_idx: 320 |  Loss_1: (0.7353) | Acc_1: (73.87%) (30351/41088)\n",
      "Epoch: 23 | Batch_idx: 330 |  Loss_1: (0.7334) | Acc_1: (73.92%) (31317/42368)\n",
      "Epoch: 23 | Batch_idx: 340 |  Loss_1: (0.7345) | Acc_1: (73.90%) (32254/43648)\n",
      "Epoch: 23 | Batch_idx: 350 |  Loss_1: (0.7355) | Acc_1: (73.83%) (33170/44928)\n",
      "Epoch: 23 | Batch_idx: 360 |  Loss_1: (0.7341) | Acc_1: (73.92%) (34156/46208)\n",
      "Epoch: 23 | Batch_idx: 370 |  Loss_1: (0.7330) | Acc_1: (73.96%) (35123/47488)\n",
      "Epoch: 23 | Batch_idx: 380 |  Loss_1: (0.7316) | Acc_1: (74.03%) (36105/48768)\n",
      "Epoch: 23 | Batch_idx: 390 |  Loss_1: (0.7326) | Acc_1: (74.01%) (37003/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5448) | Acc: (81.40%) (8140/10000)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.7177) | Acc_1: (71.88%) (92/128)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.6898) | Acc_1: (74.08%) (1043/1408)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.7047) | Acc_1: (74.29%) (1997/2688)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.7017) | Acc_1: (74.70%) (2964/3968)\n",
      "Epoch: 24 | Batch_idx: 40 |  Loss_1: (0.6973) | Acc_1: (74.87%) (3929/5248)\n",
      "Epoch: 24 | Batch_idx: 50 |  Loss_1: (0.7048) | Acc_1: (74.89%) (4889/6528)\n",
      "Epoch: 24 | Batch_idx: 60 |  Loss_1: (0.7102) | Acc_1: (74.67%) (5830/7808)\n",
      "Epoch: 24 | Batch_idx: 70 |  Loss_1: (0.7068) | Acc_1: (74.92%) (6809/9088)\n",
      "Epoch: 24 | Batch_idx: 80 |  Loss_1: (0.7128) | Acc_1: (74.67%) (7742/10368)\n",
      "Epoch: 24 | Batch_idx: 90 |  Loss_1: (0.7159) | Acc_1: (74.46%) (8673/11648)\n",
      "Epoch: 24 | Batch_idx: 100 |  Loss_1: (0.7132) | Acc_1: (74.53%) (9635/12928)\n",
      "Epoch: 24 | Batch_idx: 110 |  Loss_1: (0.7141) | Acc_1: (74.58%) (10597/14208)\n",
      "Epoch: 24 | Batch_idx: 120 |  Loss_1: (0.7164) | Acc_1: (74.55%) (11546/15488)\n",
      "Epoch: 24 | Batch_idx: 130 |  Loss_1: (0.7159) | Acc_1: (74.57%) (12504/16768)\n",
      "Epoch: 24 | Batch_idx: 140 |  Loss_1: (0.7120) | Acc_1: (74.71%) (13484/18048)\n",
      "Epoch: 24 | Batch_idx: 150 |  Loss_1: (0.7126) | Acc_1: (74.67%) (14433/19328)\n",
      "Epoch: 24 | Batch_idx: 160 |  Loss_1: (0.7138) | Acc_1: (74.64%) (15382/20608)\n",
      "Epoch: 24 | Batch_idx: 170 |  Loss_1: (0.7110) | Acc_1: (74.68%) (16345/21888)\n",
      "Epoch: 24 | Batch_idx: 180 |  Loss_1: (0.7120) | Acc_1: (74.56%) (17273/23168)\n",
      "Epoch: 24 | Batch_idx: 190 |  Loss_1: (0.7111) | Acc_1: (74.58%) (18233/24448)\n",
      "Epoch: 24 | Batch_idx: 200 |  Loss_1: (0.7098) | Acc_1: (74.69%) (19216/25728)\n",
      "Epoch: 24 | Batch_idx: 210 |  Loss_1: (0.7114) | Acc_1: (74.63%) (20157/27008)\n",
      "Epoch: 24 | Batch_idx: 220 |  Loss_1: (0.7108) | Acc_1: (74.67%) (21123/28288)\n",
      "Epoch: 24 | Batch_idx: 230 |  Loss_1: (0.7113) | Acc_1: (74.61%) (22061/29568)\n",
      "Epoch: 24 | Batch_idx: 240 |  Loss_1: (0.7096) | Acc_1: (74.65%) (23027/30848)\n",
      "Epoch: 24 | Batch_idx: 250 |  Loss_1: (0.7111) | Acc_1: (74.60%) (23969/32128)\n",
      "Epoch: 24 | Batch_idx: 260 |  Loss_1: (0.7100) | Acc_1: (74.67%) (24945/33408)\n",
      "Epoch: 24 | Batch_idx: 270 |  Loss_1: (0.7099) | Acc_1: (74.67%) (25901/34688)\n",
      "Epoch: 24 | Batch_idx: 280 |  Loss_1: (0.7082) | Acc_1: (74.75%) (26887/35968)\n",
      "Epoch: 24 | Batch_idx: 290 |  Loss_1: (0.7080) | Acc_1: (74.74%) (27838/37248)\n",
      "Epoch: 24 | Batch_idx: 300 |  Loss_1: (0.7082) | Acc_1: (74.69%) (28777/38528)\n",
      "Epoch: 24 | Batch_idx: 310 |  Loss_1: (0.7077) | Acc_1: (74.66%) (29721/39808)\n",
      "Epoch: 24 | Batch_idx: 320 |  Loss_1: (0.7076) | Acc_1: (74.66%) (30676/41088)\n",
      "Epoch: 24 | Batch_idx: 330 |  Loss_1: (0.7079) | Acc_1: (74.66%) (31631/42368)\n",
      "Epoch: 24 | Batch_idx: 340 |  Loss_1: (0.7073) | Acc_1: (74.66%) (32588/43648)\n",
      "Epoch: 24 | Batch_idx: 350 |  Loss_1: (0.7080) | Acc_1: (74.63%) (33532/44928)\n",
      "Epoch: 24 | Batch_idx: 360 |  Loss_1: (0.7095) | Acc_1: (74.56%) (34454/46208)\n",
      "Epoch: 24 | Batch_idx: 370 |  Loss_1: (0.7097) | Acc_1: (74.55%) (35403/47488)\n",
      "Epoch: 24 | Batch_idx: 380 |  Loss_1: (0.7083) | Acc_1: (74.61%) (36388/48768)\n",
      "Epoch: 24 | Batch_idx: 390 |  Loss_1: (0.7085) | Acc_1: (74.62%) (37308/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5079) | Acc: (83.69%) (8369/10000)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.7168) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.6793) | Acc_1: (76.14%) (1072/1408)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.7200) | Acc_1: (74.33%) (1998/2688)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.7189) | Acc_1: (74.12%) (2941/3968)\n",
      "Epoch: 25 | Batch_idx: 40 |  Loss_1: (0.7052) | Acc_1: (74.62%) (3916/5248)\n",
      "Epoch: 25 | Batch_idx: 50 |  Loss_1: (0.6996) | Acc_1: (75.02%) (4897/6528)\n",
      "Epoch: 25 | Batch_idx: 60 |  Loss_1: (0.6894) | Acc_1: (75.29%) (5879/7808)\n",
      "Epoch: 25 | Batch_idx: 70 |  Loss_1: (0.6936) | Acc_1: (75.19%) (6833/9088)\n",
      "Epoch: 25 | Batch_idx: 80 |  Loss_1: (0.6922) | Acc_1: (75.32%) (7809/10368)\n",
      "Epoch: 25 | Batch_idx: 90 |  Loss_1: (0.6944) | Acc_1: (75.25%) (8765/11648)\n",
      "Epoch: 25 | Batch_idx: 100 |  Loss_1: (0.6926) | Acc_1: (75.39%) (9747/12928)\n",
      "Epoch: 25 | Batch_idx: 110 |  Loss_1: (0.6902) | Acc_1: (75.47%) (10723/14208)\n",
      "Epoch: 25 | Batch_idx: 120 |  Loss_1: (0.6916) | Acc_1: (75.45%) (11685/15488)\n",
      "Epoch: 25 | Batch_idx: 130 |  Loss_1: (0.6881) | Acc_1: (75.53%) (12665/16768)\n",
      "Epoch: 25 | Batch_idx: 140 |  Loss_1: (0.6845) | Acc_1: (75.62%) (13648/18048)\n",
      "Epoch: 25 | Batch_idx: 150 |  Loss_1: (0.6848) | Acc_1: (75.62%) (14616/19328)\n",
      "Epoch: 25 | Batch_idx: 160 |  Loss_1: (0.6885) | Acc_1: (75.52%) (15563/20608)\n",
      "Epoch: 25 | Batch_idx: 170 |  Loss_1: (0.6909) | Acc_1: (75.40%) (16503/21888)\n",
      "Epoch: 25 | Batch_idx: 180 |  Loss_1: (0.6894) | Acc_1: (75.44%) (17478/23168)\n",
      "Epoch: 25 | Batch_idx: 190 |  Loss_1: (0.6891) | Acc_1: (75.49%) (18456/24448)\n",
      "Epoch: 25 | Batch_idx: 200 |  Loss_1: (0.6886) | Acc_1: (75.44%) (19410/25728)\n",
      "Epoch: 25 | Batch_idx: 210 |  Loss_1: (0.6865) | Acc_1: (75.51%) (20395/27008)\n",
      "Epoch: 25 | Batch_idx: 220 |  Loss_1: (0.6856) | Acc_1: (75.57%) (21377/28288)\n",
      "Epoch: 25 | Batch_idx: 230 |  Loss_1: (0.6857) | Acc_1: (75.51%) (22328/29568)\n",
      "Epoch: 25 | Batch_idx: 240 |  Loss_1: (0.6861) | Acc_1: (75.50%) (23290/30848)\n",
      "Epoch: 25 | Batch_idx: 250 |  Loss_1: (0.6877) | Acc_1: (75.47%) (24247/32128)\n",
      "Epoch: 25 | Batch_idx: 260 |  Loss_1: (0.6860) | Acc_1: (75.55%) (25240/33408)\n",
      "Epoch: 25 | Batch_idx: 270 |  Loss_1: (0.6863) | Acc_1: (75.52%) (26195/34688)\n",
      "Epoch: 25 | Batch_idx: 280 |  Loss_1: (0.6858) | Acc_1: (75.56%) (27179/35968)\n",
      "Epoch: 25 | Batch_idx: 290 |  Loss_1: (0.6860) | Acc_1: (75.55%) (28142/37248)\n",
      "Epoch: 25 | Batch_idx: 300 |  Loss_1: (0.6865) | Acc_1: (75.51%) (29091/38528)\n",
      "Epoch: 25 | Batch_idx: 310 |  Loss_1: (0.6859) | Acc_1: (75.51%) (30058/39808)\n",
      "Epoch: 25 | Batch_idx: 320 |  Loss_1: (0.6873) | Acc_1: (75.44%) (30997/41088)\n",
      "Epoch: 25 | Batch_idx: 330 |  Loss_1: (0.6876) | Acc_1: (75.43%) (31960/42368)\n",
      "Epoch: 25 | Batch_idx: 340 |  Loss_1: (0.6879) | Acc_1: (75.42%) (32919/43648)\n",
      "Epoch: 25 | Batch_idx: 350 |  Loss_1: (0.6890) | Acc_1: (75.37%) (33862/44928)\n",
      "Epoch: 25 | Batch_idx: 360 |  Loss_1: (0.6896) | Acc_1: (75.31%) (34800/46208)\n",
      "Epoch: 25 | Batch_idx: 370 |  Loss_1: (0.6898) | Acc_1: (75.33%) (35775/47488)\n",
      "Epoch: 25 | Batch_idx: 380 |  Loss_1: (0.6892) | Acc_1: (75.36%) (36750/48768)\n",
      "Epoch: 25 | Batch_idx: 390 |  Loss_1: (0.6887) | Acc_1: (75.36%) (37680/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5032) | Acc: (82.50%) (8250/10000)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.5203) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.6358) | Acc_1: (77.84%) (1096/1408)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.6267) | Acc_1: (78.01%) (2097/2688)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.6397) | Acc_1: (77.49%) (3075/3968)\n",
      "Epoch: 26 | Batch_idx: 40 |  Loss_1: (0.6458) | Acc_1: (77.13%) (4048/5248)\n",
      "Epoch: 26 | Batch_idx: 50 |  Loss_1: (0.6450) | Acc_1: (77.08%) (5032/6528)\n",
      "Epoch: 26 | Batch_idx: 60 |  Loss_1: (0.6448) | Acc_1: (77.16%) (6025/7808)\n",
      "Epoch: 26 | Batch_idx: 70 |  Loss_1: (0.6462) | Acc_1: (77.13%) (7010/9088)\n",
      "Epoch: 26 | Batch_idx: 80 |  Loss_1: (0.6551) | Acc_1: (76.74%) (7956/10368)\n",
      "Epoch: 26 | Batch_idx: 90 |  Loss_1: (0.6585) | Acc_1: (76.59%) (8921/11648)\n",
      "Epoch: 26 | Batch_idx: 100 |  Loss_1: (0.6595) | Acc_1: (76.51%) (9891/12928)\n",
      "Epoch: 26 | Batch_idx: 110 |  Loss_1: (0.6624) | Acc_1: (76.45%) (10862/14208)\n",
      "Epoch: 26 | Batch_idx: 120 |  Loss_1: (0.6647) | Acc_1: (76.32%) (11821/15488)\n",
      "Epoch: 26 | Batch_idx: 130 |  Loss_1: (0.6640) | Acc_1: (76.29%) (12793/16768)\n",
      "Epoch: 26 | Batch_idx: 140 |  Loss_1: (0.6644) | Acc_1: (76.29%) (13768/18048)\n",
      "Epoch: 26 | Batch_idx: 150 |  Loss_1: (0.6662) | Acc_1: (76.33%) (14754/19328)\n",
      "Epoch: 26 | Batch_idx: 160 |  Loss_1: (0.6616) | Acc_1: (76.50%) (15766/20608)\n",
      "Epoch: 26 | Batch_idx: 170 |  Loss_1: (0.6621) | Acc_1: (76.49%) (16742/21888)\n",
      "Epoch: 26 | Batch_idx: 180 |  Loss_1: (0.6627) | Acc_1: (76.49%) (17721/23168)\n",
      "Epoch: 26 | Batch_idx: 190 |  Loss_1: (0.6614) | Acc_1: (76.48%) (18699/24448)\n",
      "Epoch: 26 | Batch_idx: 200 |  Loss_1: (0.6610) | Acc_1: (76.45%) (19668/25728)\n",
      "Epoch: 26 | Batch_idx: 210 |  Loss_1: (0.6623) | Acc_1: (76.42%) (20639/27008)\n",
      "Epoch: 26 | Batch_idx: 220 |  Loss_1: (0.6649) | Acc_1: (76.31%) (21587/28288)\n",
      "Epoch: 26 | Batch_idx: 230 |  Loss_1: (0.6650) | Acc_1: (76.33%) (22569/29568)\n",
      "Epoch: 26 | Batch_idx: 240 |  Loss_1: (0.6647) | Acc_1: (76.38%) (23563/30848)\n",
      "Epoch: 26 | Batch_idx: 250 |  Loss_1: (0.6634) | Acc_1: (76.44%) (24560/32128)\n",
      "Epoch: 26 | Batch_idx: 260 |  Loss_1: (0.6617) | Acc_1: (76.55%) (25574/33408)\n",
      "Epoch: 26 | Batch_idx: 270 |  Loss_1: (0.6623) | Acc_1: (76.52%) (26542/34688)\n",
      "Epoch: 26 | Batch_idx: 280 |  Loss_1: (0.6623) | Acc_1: (76.50%) (27517/35968)\n",
      "Epoch: 26 | Batch_idx: 290 |  Loss_1: (0.6627) | Acc_1: (76.50%) (28494/37248)\n",
      "Epoch: 26 | Batch_idx: 300 |  Loss_1: (0.6607) | Acc_1: (76.52%) (29481/38528)\n",
      "Epoch: 26 | Batch_idx: 310 |  Loss_1: (0.6611) | Acc_1: (76.50%) (30452/39808)\n",
      "Epoch: 26 | Batch_idx: 320 |  Loss_1: (0.6625) | Acc_1: (76.44%) (31408/41088)\n",
      "Epoch: 26 | Batch_idx: 330 |  Loss_1: (0.6627) | Acc_1: (76.44%) (32387/42368)\n",
      "Epoch: 26 | Batch_idx: 340 |  Loss_1: (0.6631) | Acc_1: (76.45%) (33370/43648)\n",
      "Epoch: 26 | Batch_idx: 350 |  Loss_1: (0.6634) | Acc_1: (76.42%) (34332/44928)\n",
      "Epoch: 26 | Batch_idx: 360 |  Loss_1: (0.6642) | Acc_1: (76.41%) (35308/46208)\n",
      "Epoch: 26 | Batch_idx: 370 |  Loss_1: (0.6648) | Acc_1: (76.41%) (36284/47488)\n",
      "Epoch: 26 | Batch_idx: 380 |  Loss_1: (0.6650) | Acc_1: (76.45%) (37281/48768)\n",
      "Epoch: 26 | Batch_idx: 390 |  Loss_1: (0.6647) | Acc_1: (76.48%) (38239/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4905) | Acc: (83.50%) (8350/10000)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.6886) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.6661) | Acc_1: (75.64%) (1065/1408)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.6288) | Acc_1: (77.31%) (2078/2688)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.6186) | Acc_1: (77.55%) (3077/3968)\n",
      "Epoch: 27 | Batch_idx: 40 |  Loss_1: (0.6242) | Acc_1: (77.63%) (4074/5248)\n",
      "Epoch: 27 | Batch_idx: 50 |  Loss_1: (0.6374) | Acc_1: (77.33%) (5048/6528)\n",
      "Epoch: 27 | Batch_idx: 60 |  Loss_1: (0.6309) | Acc_1: (77.60%) (6059/7808)\n",
      "Epoch: 27 | Batch_idx: 70 |  Loss_1: (0.6335) | Acc_1: (77.49%) (7042/9088)\n",
      "Epoch: 27 | Batch_idx: 80 |  Loss_1: (0.6348) | Acc_1: (77.47%) (8032/10368)\n",
      "Epoch: 27 | Batch_idx: 90 |  Loss_1: (0.6346) | Acc_1: (77.51%) (9028/11648)\n",
      "Epoch: 27 | Batch_idx: 100 |  Loss_1: (0.6359) | Acc_1: (77.32%) (9996/12928)\n",
      "Epoch: 27 | Batch_idx: 110 |  Loss_1: (0.6360) | Acc_1: (77.32%) (10985/14208)\n",
      "Epoch: 27 | Batch_idx: 120 |  Loss_1: (0.6392) | Acc_1: (77.29%) (11971/15488)\n",
      "Epoch: 27 | Batch_idx: 130 |  Loss_1: (0.6427) | Acc_1: (77.08%) (12925/16768)\n",
      "Epoch: 27 | Batch_idx: 140 |  Loss_1: (0.6429) | Acc_1: (77.02%) (13901/18048)\n",
      "Epoch: 27 | Batch_idx: 150 |  Loss_1: (0.6424) | Acc_1: (77.00%) (14883/19328)\n",
      "Epoch: 27 | Batch_idx: 160 |  Loss_1: (0.6413) | Acc_1: (77.08%) (15885/20608)\n",
      "Epoch: 27 | Batch_idx: 170 |  Loss_1: (0.6421) | Acc_1: (77.11%) (16877/21888)\n",
      "Epoch: 27 | Batch_idx: 180 |  Loss_1: (0.6398) | Acc_1: (77.20%) (17885/23168)\n",
      "Epoch: 27 | Batch_idx: 190 |  Loss_1: (0.6407) | Acc_1: (77.12%) (18855/24448)\n",
      "Epoch: 27 | Batch_idx: 200 |  Loss_1: (0.6425) | Acc_1: (77.01%) (19812/25728)\n",
      "Epoch: 27 | Batch_idx: 210 |  Loss_1: (0.6454) | Acc_1: (76.90%) (20770/27008)\n",
      "Epoch: 27 | Batch_idx: 220 |  Loss_1: (0.6437) | Acc_1: (76.97%) (21774/28288)\n",
      "Epoch: 27 | Batch_idx: 230 |  Loss_1: (0.6461) | Acc_1: (76.90%) (22739/29568)\n",
      "Epoch: 27 | Batch_idx: 240 |  Loss_1: (0.6468) | Acc_1: (76.88%) (23715/30848)\n",
      "Epoch: 27 | Batch_idx: 250 |  Loss_1: (0.6458) | Acc_1: (76.89%) (24702/32128)\n",
      "Epoch: 27 | Batch_idx: 260 |  Loss_1: (0.6455) | Acc_1: (76.88%) (25683/33408)\n",
      "Epoch: 27 | Batch_idx: 270 |  Loss_1: (0.6470) | Acc_1: (76.84%) (26655/34688)\n",
      "Epoch: 27 | Batch_idx: 280 |  Loss_1: (0.6460) | Acc_1: (76.83%) (27633/35968)\n",
      "Epoch: 27 | Batch_idx: 290 |  Loss_1: (0.6469) | Acc_1: (76.82%) (28613/37248)\n",
      "Epoch: 27 | Batch_idx: 300 |  Loss_1: (0.6458) | Acc_1: (76.84%) (29603/38528)\n",
      "Epoch: 27 | Batch_idx: 310 |  Loss_1: (0.6455) | Acc_1: (76.87%) (30601/39808)\n",
      "Epoch: 27 | Batch_idx: 320 |  Loss_1: (0.6461) | Acc_1: (76.84%) (31574/41088)\n",
      "Epoch: 27 | Batch_idx: 330 |  Loss_1: (0.6470) | Acc_1: (76.83%) (32550/42368)\n",
      "Epoch: 27 | Batch_idx: 340 |  Loss_1: (0.6468) | Acc_1: (76.84%) (33538/43648)\n",
      "Epoch: 27 | Batch_idx: 350 |  Loss_1: (0.6470) | Acc_1: (76.84%) (34524/44928)\n",
      "Epoch: 27 | Batch_idx: 360 |  Loss_1: (0.6472) | Acc_1: (76.85%) (35509/46208)\n",
      "Epoch: 27 | Batch_idx: 370 |  Loss_1: (0.6459) | Acc_1: (76.90%) (36516/47488)\n",
      "Epoch: 27 | Batch_idx: 380 |  Loss_1: (0.6470) | Acc_1: (76.87%) (37489/48768)\n",
      "Epoch: 27 | Batch_idx: 390 |  Loss_1: (0.6478) | Acc_1: (76.87%) (38434/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4884) | Acc: (83.95%) (8395/10000)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.5273) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.6031) | Acc_1: (78.34%) (1103/1408)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.6370) | Acc_1: (77.16%) (2074/2688)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.6420) | Acc_1: (76.79%) (3047/3968)\n",
      "Epoch: 28 | Batch_idx: 40 |  Loss_1: (0.6401) | Acc_1: (77.08%) (4045/5248)\n",
      "Epoch: 28 | Batch_idx: 50 |  Loss_1: (0.6392) | Acc_1: (77.24%) (5042/6528)\n",
      "Epoch: 28 | Batch_idx: 60 |  Loss_1: (0.6365) | Acc_1: (77.42%) (6045/7808)\n",
      "Epoch: 28 | Batch_idx: 70 |  Loss_1: (0.6330) | Acc_1: (77.45%) (7039/9088)\n",
      "Epoch: 28 | Batch_idx: 80 |  Loss_1: (0.6292) | Acc_1: (77.54%) (8039/10368)\n",
      "Epoch: 28 | Batch_idx: 90 |  Loss_1: (0.6312) | Acc_1: (77.50%) (9027/11648)\n",
      "Epoch: 28 | Batch_idx: 100 |  Loss_1: (0.6273) | Acc_1: (77.63%) (10036/12928)\n",
      "Epoch: 28 | Batch_idx: 110 |  Loss_1: (0.6261) | Acc_1: (77.63%) (11029/14208)\n",
      "Epoch: 28 | Batch_idx: 120 |  Loss_1: (0.6327) | Acc_1: (77.40%) (11988/15488)\n",
      "Epoch: 28 | Batch_idx: 130 |  Loss_1: (0.6324) | Acc_1: (77.46%) (12988/16768)\n",
      "Epoch: 28 | Batch_idx: 140 |  Loss_1: (0.6316) | Acc_1: (77.48%) (13983/18048)\n",
      "Epoch: 28 | Batch_idx: 150 |  Loss_1: (0.6326) | Acc_1: (77.43%) (14966/19328)\n",
      "Epoch: 28 | Batch_idx: 160 |  Loss_1: (0.6337) | Acc_1: (77.44%) (15959/20608)\n",
      "Epoch: 28 | Batch_idx: 170 |  Loss_1: (0.6313) | Acc_1: (77.55%) (16975/21888)\n",
      "Epoch: 28 | Batch_idx: 180 |  Loss_1: (0.6262) | Acc_1: (77.75%) (18012/23168)\n",
      "Epoch: 28 | Batch_idx: 190 |  Loss_1: (0.6282) | Acc_1: (77.66%) (18986/24448)\n",
      "Epoch: 28 | Batch_idx: 200 |  Loss_1: (0.6287) | Acc_1: (77.56%) (19955/25728)\n",
      "Epoch: 28 | Batch_idx: 210 |  Loss_1: (0.6281) | Acc_1: (77.54%) (20943/27008)\n",
      "Epoch: 28 | Batch_idx: 220 |  Loss_1: (0.6283) | Acc_1: (77.54%) (21935/28288)\n",
      "Epoch: 28 | Batch_idx: 230 |  Loss_1: (0.6290) | Acc_1: (77.54%) (22928/29568)\n",
      "Epoch: 28 | Batch_idx: 240 |  Loss_1: (0.6296) | Acc_1: (77.55%) (23922/30848)\n",
      "Epoch: 28 | Batch_idx: 250 |  Loss_1: (0.6296) | Acc_1: (77.54%) (24913/32128)\n",
      "Epoch: 28 | Batch_idx: 260 |  Loss_1: (0.6269) | Acc_1: (77.60%) (25926/33408)\n",
      "Epoch: 28 | Batch_idx: 270 |  Loss_1: (0.6272) | Acc_1: (77.57%) (26909/34688)\n",
      "Epoch: 28 | Batch_idx: 280 |  Loss_1: (0.6254) | Acc_1: (77.67%) (27936/35968)\n",
      "Epoch: 28 | Batch_idx: 290 |  Loss_1: (0.6252) | Acc_1: (77.68%) (28933/37248)\n",
      "Epoch: 28 | Batch_idx: 300 |  Loss_1: (0.6246) | Acc_1: (77.69%) (29932/38528)\n",
      "Epoch: 28 | Batch_idx: 310 |  Loss_1: (0.6242) | Acc_1: (77.72%) (30937/39808)\n",
      "Epoch: 28 | Batch_idx: 320 |  Loss_1: (0.6252) | Acc_1: (77.69%) (31920/41088)\n",
      "Epoch: 28 | Batch_idx: 330 |  Loss_1: (0.6263) | Acc_1: (77.63%) (32891/42368)\n",
      "Epoch: 28 | Batch_idx: 340 |  Loss_1: (0.6272) | Acc_1: (77.60%) (33869/43648)\n",
      "Epoch: 28 | Batch_idx: 350 |  Loss_1: (0.6279) | Acc_1: (77.55%) (34841/44928)\n",
      "Epoch: 28 | Batch_idx: 360 |  Loss_1: (0.6284) | Acc_1: (77.52%) (35819/46208)\n",
      "Epoch: 28 | Batch_idx: 370 |  Loss_1: (0.6285) | Acc_1: (77.51%) (36808/47488)\n",
      "Epoch: 28 | Batch_idx: 380 |  Loss_1: (0.6284) | Acc_1: (77.53%) (37809/48768)\n",
      "Epoch: 28 | Batch_idx: 390 |  Loss_1: (0.6286) | Acc_1: (77.49%) (38747/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4698) | Acc: (84.27%) (8427/10000)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.4621) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.5764) | Acc_1: (80.11%) (1128/1408)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.5839) | Acc_1: (79.50%) (2137/2688)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.5945) | Acc_1: (78.96%) (3133/3968)\n",
      "Epoch: 29 | Batch_idx: 40 |  Loss_1: (0.6076) | Acc_1: (78.30%) (4109/5248)\n",
      "Epoch: 29 | Batch_idx: 50 |  Loss_1: (0.6121) | Acc_1: (78.05%) (5095/6528)\n",
      "Epoch: 29 | Batch_idx: 60 |  Loss_1: (0.6125) | Acc_1: (77.97%) (6088/7808)\n",
      "Epoch: 29 | Batch_idx: 70 |  Loss_1: (0.6100) | Acc_1: (78.07%) (7095/9088)\n",
      "Epoch: 29 | Batch_idx: 80 |  Loss_1: (0.6118) | Acc_1: (78.00%) (8087/10368)\n",
      "Epoch: 29 | Batch_idx: 90 |  Loss_1: (0.6073) | Acc_1: (78.23%) (9112/11648)\n",
      "Epoch: 29 | Batch_idx: 100 |  Loss_1: (0.6092) | Acc_1: (78.19%) (10108/12928)\n",
      "Epoch: 29 | Batch_idx: 110 |  Loss_1: (0.6080) | Acc_1: (78.15%) (11104/14208)\n",
      "Epoch: 29 | Batch_idx: 120 |  Loss_1: (0.6085) | Acc_1: (78.12%) (12099/15488)\n",
      "Epoch: 29 | Batch_idx: 130 |  Loss_1: (0.6087) | Acc_1: (78.13%) (13101/16768)\n",
      "Epoch: 29 | Batch_idx: 140 |  Loss_1: (0.6094) | Acc_1: (78.12%) (14099/18048)\n",
      "Epoch: 29 | Batch_idx: 150 |  Loss_1: (0.6091) | Acc_1: (78.07%) (15090/19328)\n",
      "Epoch: 29 | Batch_idx: 160 |  Loss_1: (0.6125) | Acc_1: (77.98%) (16071/20608)\n",
      "Epoch: 29 | Batch_idx: 170 |  Loss_1: (0.6117) | Acc_1: (77.97%) (17067/21888)\n",
      "Epoch: 29 | Batch_idx: 180 |  Loss_1: (0.6131) | Acc_1: (77.97%) (18064/23168)\n",
      "Epoch: 29 | Batch_idx: 190 |  Loss_1: (0.6134) | Acc_1: (77.98%) (19065/24448)\n",
      "Epoch: 29 | Batch_idx: 200 |  Loss_1: (0.6151) | Acc_1: (77.92%) (20046/25728)\n",
      "Epoch: 29 | Batch_idx: 210 |  Loss_1: (0.6151) | Acc_1: (77.89%) (21037/27008)\n",
      "Epoch: 29 | Batch_idx: 220 |  Loss_1: (0.6147) | Acc_1: (77.93%) (22045/28288)\n",
      "Epoch: 29 | Batch_idx: 230 |  Loss_1: (0.6151) | Acc_1: (77.93%) (23042/29568)\n",
      "Epoch: 29 | Batch_idx: 240 |  Loss_1: (0.6164) | Acc_1: (77.87%) (24020/30848)\n",
      "Epoch: 29 | Batch_idx: 250 |  Loss_1: (0.6159) | Acc_1: (77.85%) (25013/32128)\n",
      "Epoch: 29 | Batch_idx: 260 |  Loss_1: (0.6147) | Acc_1: (77.94%) (26037/33408)\n",
      "Epoch: 29 | Batch_idx: 270 |  Loss_1: (0.6155) | Acc_1: (77.88%) (27016/34688)\n",
      "Epoch: 29 | Batch_idx: 280 |  Loss_1: (0.6160) | Acc_1: (77.89%) (28016/35968)\n",
      "Epoch: 29 | Batch_idx: 290 |  Loss_1: (0.6169) | Acc_1: (77.85%) (28996/37248)\n",
      "Epoch: 29 | Batch_idx: 300 |  Loss_1: (0.6161) | Acc_1: (77.87%) (30002/38528)\n",
      "Epoch: 29 | Batch_idx: 310 |  Loss_1: (0.6172) | Acc_1: (77.88%) (31001/39808)\n",
      "Epoch: 29 | Batch_idx: 320 |  Loss_1: (0.6173) | Acc_1: (77.90%) (32008/41088)\n",
      "Epoch: 29 | Batch_idx: 330 |  Loss_1: (0.6184) | Acc_1: (77.85%) (32982/42368)\n",
      "Epoch: 29 | Batch_idx: 340 |  Loss_1: (0.6177) | Acc_1: (77.88%) (33993/43648)\n",
      "Epoch: 29 | Batch_idx: 350 |  Loss_1: (0.6161) | Acc_1: (77.92%) (35009/44928)\n",
      "Epoch: 29 | Batch_idx: 360 |  Loss_1: (0.6163) | Acc_1: (77.91%) (36000/46208)\n",
      "Epoch: 29 | Batch_idx: 370 |  Loss_1: (0.6173) | Acc_1: (77.88%) (36986/47488)\n",
      "Epoch: 29 | Batch_idx: 380 |  Loss_1: (0.6163) | Acc_1: (77.94%) (38009/48768)\n",
      "Epoch: 29 | Batch_idx: 390 |  Loss_1: (0.6155) | Acc_1: (77.96%) (38980/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4168) | Acc: (86.11%) (8611/10000)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.6816) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.5813) | Acc_1: (79.83%) (1124/1408)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.6227) | Acc_1: (78.09%) (2099/2688)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.5993) | Acc_1: (78.68%) (3122/3968)\n",
      "Epoch: 30 | Batch_idx: 40 |  Loss_1: (0.6044) | Acc_1: (78.33%) (4111/5248)\n",
      "Epoch: 30 | Batch_idx: 50 |  Loss_1: (0.5997) | Acc_1: (78.49%) (5124/6528)\n",
      "Epoch: 30 | Batch_idx: 60 |  Loss_1: (0.5980) | Acc_1: (78.60%) (6137/7808)\n",
      "Epoch: 30 | Batch_idx: 70 |  Loss_1: (0.5963) | Acc_1: (78.59%) (7142/9088)\n",
      "Epoch: 30 | Batch_idx: 80 |  Loss_1: (0.5887) | Acc_1: (78.75%) (8165/10368)\n",
      "Epoch: 30 | Batch_idx: 90 |  Loss_1: (0.5886) | Acc_1: (78.79%) (9178/11648)\n",
      "Epoch: 30 | Batch_idx: 100 |  Loss_1: (0.5888) | Acc_1: (78.79%) (10186/12928)\n",
      "Epoch: 30 | Batch_idx: 110 |  Loss_1: (0.5923) | Acc_1: (78.67%) (11178/14208)\n",
      "Epoch: 30 | Batch_idx: 120 |  Loss_1: (0.5932) | Acc_1: (78.64%) (12180/15488)\n",
      "Epoch: 30 | Batch_idx: 130 |  Loss_1: (0.5914) | Acc_1: (78.76%) (13206/16768)\n",
      "Epoch: 30 | Batch_idx: 140 |  Loss_1: (0.5911) | Acc_1: (78.78%) (14219/18048)\n",
      "Epoch: 30 | Batch_idx: 150 |  Loss_1: (0.5897) | Acc_1: (78.85%) (15240/19328)\n",
      "Epoch: 30 | Batch_idx: 160 |  Loss_1: (0.5898) | Acc_1: (78.82%) (16243/20608)\n",
      "Epoch: 30 | Batch_idx: 170 |  Loss_1: (0.5963) | Acc_1: (78.66%) (17217/21888)\n",
      "Epoch: 30 | Batch_idx: 180 |  Loss_1: (0.5977) | Acc_1: (78.60%) (18211/23168)\n",
      "Epoch: 30 | Batch_idx: 190 |  Loss_1: (0.5985) | Acc_1: (78.56%) (19206/24448)\n",
      "Epoch: 30 | Batch_idx: 200 |  Loss_1: (0.6000) | Acc_1: (78.47%) (20189/25728)\n",
      "Epoch: 30 | Batch_idx: 210 |  Loss_1: (0.6008) | Acc_1: (78.48%) (21196/27008)\n",
      "Epoch: 30 | Batch_idx: 220 |  Loss_1: (0.6004) | Acc_1: (78.51%) (22209/28288)\n",
      "Epoch: 30 | Batch_idx: 230 |  Loss_1: (0.6011) | Acc_1: (78.46%) (23199/29568)\n",
      "Epoch: 30 | Batch_idx: 240 |  Loss_1: (0.6021) | Acc_1: (78.41%) (24189/30848)\n",
      "Epoch: 30 | Batch_idx: 250 |  Loss_1: (0.6017) | Acc_1: (78.38%) (25181/32128)\n",
      "Epoch: 30 | Batch_idx: 260 |  Loss_1: (0.6000) | Acc_1: (78.41%) (26196/33408)\n",
      "Epoch: 30 | Batch_idx: 270 |  Loss_1: (0.6011) | Acc_1: (78.36%) (27183/34688)\n",
      "Epoch: 30 | Batch_idx: 280 |  Loss_1: (0.6015) | Acc_1: (78.35%) (28181/35968)\n",
      "Epoch: 30 | Batch_idx: 290 |  Loss_1: (0.6026) | Acc_1: (78.32%) (29174/37248)\n",
      "Epoch: 30 | Batch_idx: 300 |  Loss_1: (0.6031) | Acc_1: (78.31%) (30173/38528)\n",
      "Epoch: 30 | Batch_idx: 310 |  Loss_1: (0.6032) | Acc_1: (78.32%) (31177/39808)\n",
      "Epoch: 30 | Batch_idx: 320 |  Loss_1: (0.6037) | Acc_1: (78.31%) (32178/41088)\n",
      "Epoch: 30 | Batch_idx: 330 |  Loss_1: (0.6036) | Acc_1: (78.31%) (33180/42368)\n",
      "Epoch: 30 | Batch_idx: 340 |  Loss_1: (0.6023) | Acc_1: (78.35%) (34198/43648)\n",
      "Epoch: 30 | Batch_idx: 350 |  Loss_1: (0.6025) | Acc_1: (78.34%) (35196/44928)\n",
      "Epoch: 30 | Batch_idx: 360 |  Loss_1: (0.6024) | Acc_1: (78.35%) (36206/46208)\n",
      "Epoch: 30 | Batch_idx: 370 |  Loss_1: (0.6022) | Acc_1: (78.39%) (37225/47488)\n",
      "Epoch: 30 | Batch_idx: 380 |  Loss_1: (0.6011) | Acc_1: (78.41%) (38241/48768)\n",
      "Epoch: 30 | Batch_idx: 390 |  Loss_1: (0.6012) | Acc_1: (78.40%) (39200/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4677) | Acc: (83.78%) (8378/10000)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.5124) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.6249) | Acc_1: (78.12%) (1100/1408)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.6054) | Acc_1: (78.39%) (2107/2688)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.5924) | Acc_1: (78.83%) (3128/3968)\n",
      "Epoch: 31 | Batch_idx: 40 |  Loss_1: (0.5879) | Acc_1: (79.06%) (4149/5248)\n",
      "Epoch: 31 | Batch_idx: 50 |  Loss_1: (0.5834) | Acc_1: (79.30%) (5177/6528)\n",
      "Epoch: 31 | Batch_idx: 60 |  Loss_1: (0.5786) | Acc_1: (79.20%) (6184/7808)\n",
      "Epoch: 31 | Batch_idx: 70 |  Loss_1: (0.5758) | Acc_1: (79.29%) (7206/9088)\n",
      "Epoch: 31 | Batch_idx: 80 |  Loss_1: (0.5764) | Acc_1: (79.21%) (8212/10368)\n",
      "Epoch: 31 | Batch_idx: 90 |  Loss_1: (0.5800) | Acc_1: (79.09%) (9212/11648)\n",
      "Epoch: 31 | Batch_idx: 100 |  Loss_1: (0.5755) | Acc_1: (79.29%) (10251/12928)\n",
      "Epoch: 31 | Batch_idx: 110 |  Loss_1: (0.5754) | Acc_1: (79.29%) (11265/14208)\n",
      "Epoch: 31 | Batch_idx: 120 |  Loss_1: (0.5724) | Acc_1: (79.43%) (12302/15488)\n",
      "Epoch: 31 | Batch_idx: 130 |  Loss_1: (0.5736) | Acc_1: (79.51%) (13332/16768)\n",
      "Epoch: 31 | Batch_idx: 140 |  Loss_1: (0.5718) | Acc_1: (79.61%) (14368/18048)\n",
      "Epoch: 31 | Batch_idx: 150 |  Loss_1: (0.5701) | Acc_1: (79.62%) (15389/19328)\n",
      "Epoch: 31 | Batch_idx: 160 |  Loss_1: (0.5742) | Acc_1: (79.53%) (16389/20608)\n",
      "Epoch: 31 | Batch_idx: 170 |  Loss_1: (0.5715) | Acc_1: (79.63%) (17429/21888)\n",
      "Epoch: 31 | Batch_idx: 180 |  Loss_1: (0.5699) | Acc_1: (79.67%) (18459/23168)\n",
      "Epoch: 31 | Batch_idx: 190 |  Loss_1: (0.5726) | Acc_1: (79.57%) (19454/24448)\n",
      "Epoch: 31 | Batch_idx: 200 |  Loss_1: (0.5723) | Acc_1: (79.61%) (20483/25728)\n",
      "Epoch: 31 | Batch_idx: 210 |  Loss_1: (0.5747) | Acc_1: (79.49%) (21470/27008)\n",
      "Epoch: 31 | Batch_idx: 220 |  Loss_1: (0.5732) | Acc_1: (79.51%) (22493/28288)\n",
      "Epoch: 31 | Batch_idx: 230 |  Loss_1: (0.5730) | Acc_1: (79.56%) (23524/29568)\n",
      "Epoch: 31 | Batch_idx: 240 |  Loss_1: (0.5729) | Acc_1: (79.53%) (24532/30848)\n",
      "Epoch: 31 | Batch_idx: 250 |  Loss_1: (0.5760) | Acc_1: (79.43%) (25519/32128)\n",
      "Epoch: 31 | Batch_idx: 260 |  Loss_1: (0.5765) | Acc_1: (79.36%) (26511/33408)\n",
      "Epoch: 31 | Batch_idx: 270 |  Loss_1: (0.5760) | Acc_1: (79.36%) (27528/34688)\n",
      "Epoch: 31 | Batch_idx: 280 |  Loss_1: (0.5753) | Acc_1: (79.40%) (28557/35968)\n",
      "Epoch: 31 | Batch_idx: 290 |  Loss_1: (0.5753) | Acc_1: (79.42%) (29584/37248)\n",
      "Epoch: 31 | Batch_idx: 300 |  Loss_1: (0.5764) | Acc_1: (79.38%) (30583/38528)\n",
      "Epoch: 31 | Batch_idx: 310 |  Loss_1: (0.5760) | Acc_1: (79.39%) (31605/39808)\n",
      "Epoch: 31 | Batch_idx: 320 |  Loss_1: (0.5773) | Acc_1: (79.31%) (32588/41088)\n",
      "Epoch: 31 | Batch_idx: 330 |  Loss_1: (0.5771) | Acc_1: (79.35%) (33619/42368)\n",
      "Epoch: 31 | Batch_idx: 340 |  Loss_1: (0.5774) | Acc_1: (79.36%) (34637/43648)\n",
      "Epoch: 31 | Batch_idx: 350 |  Loss_1: (0.5779) | Acc_1: (79.33%) (35643/44928)\n",
      "Epoch: 31 | Batch_idx: 360 |  Loss_1: (0.5786) | Acc_1: (79.30%) (36644/46208)\n",
      "Epoch: 31 | Batch_idx: 370 |  Loss_1: (0.5798) | Acc_1: (79.25%) (37635/47488)\n",
      "Epoch: 31 | Batch_idx: 380 |  Loss_1: (0.5797) | Acc_1: (79.25%) (38649/48768)\n",
      "Epoch: 31 | Batch_idx: 390 |  Loss_1: (0.5792) | Acc_1: (79.29%) (39644/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3858) | Acc: (87.01%) (8701/10000)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.6243) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.5513) | Acc_1: (80.26%) (1130/1408)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.5422) | Acc_1: (80.73%) (2170/2688)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.5482) | Acc_1: (80.32%) (3187/3968)\n",
      "Epoch: 32 | Batch_idx: 40 |  Loss_1: (0.5475) | Acc_1: (80.16%) (4207/5248)\n",
      "Epoch: 32 | Batch_idx: 50 |  Loss_1: (0.5474) | Acc_1: (80.02%) (5224/6528)\n",
      "Epoch: 32 | Batch_idx: 60 |  Loss_1: (0.5451) | Acc_1: (80.16%) (6259/7808)\n",
      "Epoch: 32 | Batch_idx: 70 |  Loss_1: (0.5428) | Acc_1: (80.22%) (7290/9088)\n",
      "Epoch: 32 | Batch_idx: 80 |  Loss_1: (0.5473) | Acc_1: (80.07%) (8302/10368)\n",
      "Epoch: 32 | Batch_idx: 90 |  Loss_1: (0.5520) | Acc_1: (79.88%) (9305/11648)\n",
      "Epoch: 32 | Batch_idx: 100 |  Loss_1: (0.5515) | Acc_1: (79.94%) (10334/12928)\n",
      "Epoch: 32 | Batch_idx: 110 |  Loss_1: (0.5513) | Acc_1: (79.94%) (11358/14208)\n",
      "Epoch: 32 | Batch_idx: 120 |  Loss_1: (0.5550) | Acc_1: (79.79%) (12358/15488)\n",
      "Epoch: 32 | Batch_idx: 130 |  Loss_1: (0.5556) | Acc_1: (79.78%) (13377/16768)\n",
      "Epoch: 32 | Batch_idx: 140 |  Loss_1: (0.5585) | Acc_1: (79.70%) (14384/18048)\n",
      "Epoch: 32 | Batch_idx: 150 |  Loss_1: (0.5603) | Acc_1: (79.66%) (15396/19328)\n",
      "Epoch: 32 | Batch_idx: 160 |  Loss_1: (0.5655) | Acc_1: (79.48%) (16380/20608)\n",
      "Epoch: 32 | Batch_idx: 170 |  Loss_1: (0.5680) | Acc_1: (79.44%) (17387/21888)\n",
      "Epoch: 32 | Batch_idx: 180 |  Loss_1: (0.5663) | Acc_1: (79.50%) (18419/23168)\n",
      "Epoch: 32 | Batch_idx: 190 |  Loss_1: (0.5671) | Acc_1: (79.52%) (19442/24448)\n",
      "Epoch: 32 | Batch_idx: 200 |  Loss_1: (0.5682) | Acc_1: (79.54%) (20464/25728)\n",
      "Epoch: 32 | Batch_idx: 210 |  Loss_1: (0.5698) | Acc_1: (79.45%) (21458/27008)\n",
      "Epoch: 32 | Batch_idx: 220 |  Loss_1: (0.5700) | Acc_1: (79.49%) (22486/28288)\n",
      "Epoch: 32 | Batch_idx: 230 |  Loss_1: (0.5691) | Acc_1: (79.52%) (23511/29568)\n",
      "Epoch: 32 | Batch_idx: 240 |  Loss_1: (0.5686) | Acc_1: (79.60%) (24554/30848)\n",
      "Epoch: 32 | Batch_idx: 250 |  Loss_1: (0.5701) | Acc_1: (79.54%) (25556/32128)\n",
      "Epoch: 32 | Batch_idx: 260 |  Loss_1: (0.5690) | Acc_1: (79.62%) (26600/33408)\n",
      "Epoch: 32 | Batch_idx: 270 |  Loss_1: (0.5683) | Acc_1: (79.64%) (27626/34688)\n",
      "Epoch: 32 | Batch_idx: 280 |  Loss_1: (0.5698) | Acc_1: (79.53%) (28606/35968)\n",
      "Epoch: 32 | Batch_idx: 290 |  Loss_1: (0.5700) | Acc_1: (79.52%) (29620/37248)\n",
      "Epoch: 32 | Batch_idx: 300 |  Loss_1: (0.5702) | Acc_1: (79.52%) (30639/38528)\n",
      "Epoch: 32 | Batch_idx: 310 |  Loss_1: (0.5698) | Acc_1: (79.53%) (31660/39808)\n",
      "Epoch: 32 | Batch_idx: 320 |  Loss_1: (0.5678) | Acc_1: (79.59%) (32700/41088)\n",
      "Epoch: 32 | Batch_idx: 330 |  Loss_1: (0.5685) | Acc_1: (79.54%) (33701/42368)\n",
      "Epoch: 32 | Batch_idx: 340 |  Loss_1: (0.5679) | Acc_1: (79.59%) (34741/43648)\n",
      "Epoch: 32 | Batch_idx: 350 |  Loss_1: (0.5678) | Acc_1: (79.64%) (35781/44928)\n",
      "Epoch: 32 | Batch_idx: 360 |  Loss_1: (0.5657) | Acc_1: (79.70%) (36828/46208)\n",
      "Epoch: 32 | Batch_idx: 370 |  Loss_1: (0.5658) | Acc_1: (79.68%) (37838/47488)\n",
      "Epoch: 32 | Batch_idx: 380 |  Loss_1: (0.5666) | Acc_1: (79.66%) (38848/48768)\n",
      "Epoch: 32 | Batch_idx: 390 |  Loss_1: (0.5682) | Acc_1: (79.62%) (39812/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4003) | Acc: (86.70%) (8670/10000)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.4108) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.5645) | Acc_1: (80.26%) (1130/1408)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.5513) | Acc_1: (80.54%) (2165/2688)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.5485) | Acc_1: (80.54%) (3196/3968)\n",
      "Epoch: 33 | Batch_idx: 40 |  Loss_1: (0.5509) | Acc_1: (80.34%) (4216/5248)\n",
      "Epoch: 33 | Batch_idx: 50 |  Loss_1: (0.5652) | Acc_1: (79.66%) (5200/6528)\n",
      "Epoch: 33 | Batch_idx: 60 |  Loss_1: (0.5544) | Acc_1: (80.06%) (6251/7808)\n",
      "Epoch: 33 | Batch_idx: 70 |  Loss_1: (0.5550) | Acc_1: (80.07%) (7277/9088)\n",
      "Epoch: 33 | Batch_idx: 80 |  Loss_1: (0.5534) | Acc_1: (80.08%) (8303/10368)\n",
      "Epoch: 33 | Batch_idx: 90 |  Loss_1: (0.5533) | Acc_1: (79.98%) (9316/11648)\n",
      "Epoch: 33 | Batch_idx: 100 |  Loss_1: (0.5524) | Acc_1: (80.03%) (10346/12928)\n",
      "Epoch: 33 | Batch_idx: 110 |  Loss_1: (0.5500) | Acc_1: (80.22%) (11397/14208)\n",
      "Epoch: 33 | Batch_idx: 120 |  Loss_1: (0.5490) | Acc_1: (80.21%) (12423/15488)\n",
      "Epoch: 33 | Batch_idx: 130 |  Loss_1: (0.5531) | Acc_1: (80.02%) (13417/16768)\n",
      "Epoch: 33 | Batch_idx: 140 |  Loss_1: (0.5555) | Acc_1: (79.93%) (14426/18048)\n",
      "Epoch: 33 | Batch_idx: 150 |  Loss_1: (0.5547) | Acc_1: (80.00%) (15462/19328)\n",
      "Epoch: 33 | Batch_idx: 160 |  Loss_1: (0.5551) | Acc_1: (79.93%) (16473/20608)\n",
      "Epoch: 33 | Batch_idx: 170 |  Loss_1: (0.5563) | Acc_1: (79.87%) (17483/21888)\n",
      "Epoch: 33 | Batch_idx: 180 |  Loss_1: (0.5552) | Acc_1: (79.90%) (18511/23168)\n",
      "Epoch: 33 | Batch_idx: 190 |  Loss_1: (0.5565) | Acc_1: (79.85%) (19522/24448)\n",
      "Epoch: 33 | Batch_idx: 200 |  Loss_1: (0.5558) | Acc_1: (79.86%) (20547/25728)\n",
      "Epoch: 33 | Batch_idx: 210 |  Loss_1: (0.5561) | Acc_1: (79.84%) (21562/27008)\n",
      "Epoch: 33 | Batch_idx: 220 |  Loss_1: (0.5575) | Acc_1: (79.82%) (22580/28288)\n",
      "Epoch: 33 | Batch_idx: 230 |  Loss_1: (0.5564) | Acc_1: (79.84%) (23608/29568)\n",
      "Epoch: 33 | Batch_idx: 240 |  Loss_1: (0.5556) | Acc_1: (79.90%) (24649/30848)\n",
      "Epoch: 33 | Batch_idx: 250 |  Loss_1: (0.5538) | Acc_1: (79.95%) (25687/32128)\n",
      "Epoch: 33 | Batch_idx: 260 |  Loss_1: (0.5532) | Acc_1: (79.98%) (26721/33408)\n",
      "Epoch: 33 | Batch_idx: 270 |  Loss_1: (0.5521) | Acc_1: (80.02%) (27757/34688)\n",
      "Epoch: 33 | Batch_idx: 280 |  Loss_1: (0.5546) | Acc_1: (79.96%) (28760/35968)\n",
      "Epoch: 33 | Batch_idx: 290 |  Loss_1: (0.5536) | Acc_1: (79.99%) (29795/37248)\n",
      "Epoch: 33 | Batch_idx: 300 |  Loss_1: (0.5532) | Acc_1: (80.00%) (30822/38528)\n",
      "Epoch: 33 | Batch_idx: 310 |  Loss_1: (0.5525) | Acc_1: (80.03%) (31860/39808)\n",
      "Epoch: 33 | Batch_idx: 320 |  Loss_1: (0.5519) | Acc_1: (80.06%) (32896/41088)\n",
      "Epoch: 33 | Batch_idx: 330 |  Loss_1: (0.5514) | Acc_1: (80.09%) (33932/42368)\n",
      "Epoch: 33 | Batch_idx: 340 |  Loss_1: (0.5509) | Acc_1: (80.11%) (34968/43648)\n",
      "Epoch: 33 | Batch_idx: 350 |  Loss_1: (0.5531) | Acc_1: (80.03%) (35956/44928)\n",
      "Epoch: 33 | Batch_idx: 360 |  Loss_1: (0.5529) | Acc_1: (80.04%) (36987/46208)\n",
      "Epoch: 33 | Batch_idx: 370 |  Loss_1: (0.5536) | Acc_1: (80.04%) (38008/47488)\n",
      "Epoch: 33 | Batch_idx: 380 |  Loss_1: (0.5535) | Acc_1: (80.03%) (39029/48768)\n",
      "Epoch: 33 | Batch_idx: 390 |  Loss_1: (0.5533) | Acc_1: (80.04%) (40020/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3631) | Acc: (87.97%) (8797/10000)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.4966) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.5383) | Acc_1: (81.25%) (1144/1408)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.5224) | Acc_1: (81.51%) (2191/2688)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.5200) | Acc_1: (81.33%) (3227/3968)\n",
      "Epoch: 34 | Batch_idx: 40 |  Loss_1: (0.5335) | Acc_1: (80.79%) (4240/5248)\n",
      "Epoch: 34 | Batch_idx: 50 |  Loss_1: (0.5418) | Acc_1: (80.48%) (5254/6528)\n",
      "Epoch: 34 | Batch_idx: 60 |  Loss_1: (0.5465) | Acc_1: (80.43%) (6280/7808)\n",
      "Epoch: 34 | Batch_idx: 70 |  Loss_1: (0.5602) | Acc_1: (79.86%) (7258/9088)\n",
      "Epoch: 34 | Batch_idx: 80 |  Loss_1: (0.5533) | Acc_1: (80.13%) (8308/10368)\n",
      "Epoch: 34 | Batch_idx: 90 |  Loss_1: (0.5483) | Acc_1: (80.35%) (9359/11648)\n",
      "Epoch: 34 | Batch_idx: 100 |  Loss_1: (0.5454) | Acc_1: (80.54%) (10412/12928)\n",
      "Epoch: 34 | Batch_idx: 110 |  Loss_1: (0.5474) | Acc_1: (80.42%) (11426/14208)\n",
      "Epoch: 34 | Batch_idx: 120 |  Loss_1: (0.5422) | Acc_1: (80.58%) (12481/15488)\n",
      "Epoch: 34 | Batch_idx: 130 |  Loss_1: (0.5431) | Acc_1: (80.59%) (13514/16768)\n",
      "Epoch: 34 | Batch_idx: 140 |  Loss_1: (0.5422) | Acc_1: (80.62%) (14550/18048)\n",
      "Epoch: 34 | Batch_idx: 150 |  Loss_1: (0.5425) | Acc_1: (80.60%) (15579/19328)\n",
      "Epoch: 34 | Batch_idx: 160 |  Loss_1: (0.5422) | Acc_1: (80.57%) (16603/20608)\n",
      "Epoch: 34 | Batch_idx: 170 |  Loss_1: (0.5420) | Acc_1: (80.55%) (17630/21888)\n",
      "Epoch: 34 | Batch_idx: 180 |  Loss_1: (0.5423) | Acc_1: (80.53%) (18658/23168)\n",
      "Epoch: 34 | Batch_idx: 190 |  Loss_1: (0.5396) | Acc_1: (80.67%) (19722/24448)\n",
      "Epoch: 34 | Batch_idx: 200 |  Loss_1: (0.5381) | Acc_1: (80.78%) (20782/25728)\n",
      "Epoch: 34 | Batch_idx: 210 |  Loss_1: (0.5373) | Acc_1: (80.78%) (21817/27008)\n",
      "Epoch: 34 | Batch_idx: 220 |  Loss_1: (0.5360) | Acc_1: (80.78%) (22850/28288)\n",
      "Epoch: 34 | Batch_idx: 230 |  Loss_1: (0.5369) | Acc_1: (80.72%) (23866/29568)\n",
      "Epoch: 34 | Batch_idx: 240 |  Loss_1: (0.5363) | Acc_1: (80.74%) (24907/30848)\n",
      "Epoch: 34 | Batch_idx: 250 |  Loss_1: (0.5347) | Acc_1: (80.77%) (25949/32128)\n",
      "Epoch: 34 | Batch_idx: 260 |  Loss_1: (0.5350) | Acc_1: (80.77%) (26983/33408)\n",
      "Epoch: 34 | Batch_idx: 270 |  Loss_1: (0.5342) | Acc_1: (80.81%) (28033/34688)\n",
      "Epoch: 34 | Batch_idx: 280 |  Loss_1: (0.5326) | Acc_1: (80.83%) (29074/35968)\n",
      "Epoch: 34 | Batch_idx: 290 |  Loss_1: (0.5322) | Acc_1: (80.83%) (30107/37248)\n",
      "Epoch: 34 | Batch_idx: 300 |  Loss_1: (0.5321) | Acc_1: (80.86%) (31152/38528)\n",
      "Epoch: 34 | Batch_idx: 310 |  Loss_1: (0.5322) | Acc_1: (80.87%) (32193/39808)\n",
      "Epoch: 34 | Batch_idx: 320 |  Loss_1: (0.5308) | Acc_1: (80.94%) (33258/41088)\n",
      "Epoch: 34 | Batch_idx: 330 |  Loss_1: (0.5311) | Acc_1: (80.92%) (34286/42368)\n",
      "Epoch: 34 | Batch_idx: 340 |  Loss_1: (0.5316) | Acc_1: (80.92%) (35320/43648)\n",
      "Epoch: 34 | Batch_idx: 350 |  Loss_1: (0.5313) | Acc_1: (80.94%) (36364/44928)\n",
      "Epoch: 34 | Batch_idx: 360 |  Loss_1: (0.5318) | Acc_1: (80.93%) (37394/46208)\n",
      "Epoch: 34 | Batch_idx: 370 |  Loss_1: (0.5322) | Acc_1: (80.90%) (38417/47488)\n",
      "Epoch: 34 | Batch_idx: 380 |  Loss_1: (0.5318) | Acc_1: (80.93%) (39468/48768)\n",
      "Epoch: 34 | Batch_idx: 390 |  Loss_1: (0.5310) | Acc_1: (80.95%) (40474/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3606) | Acc: (87.82%) (8782/10000)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.4474) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.5474) | Acc_1: (79.83%) (1124/1408)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.5504) | Acc_1: (80.25%) (2157/2688)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.5462) | Acc_1: (79.89%) (3170/3968)\n",
      "Epoch: 35 | Batch_idx: 40 |  Loss_1: (0.5437) | Acc_1: (80.13%) (4205/5248)\n",
      "Epoch: 35 | Batch_idx: 50 |  Loss_1: (0.5529) | Acc_1: (79.87%) (5214/6528)\n",
      "Epoch: 35 | Batch_idx: 60 |  Loss_1: (0.5432) | Acc_1: (80.26%) (6267/7808)\n",
      "Epoch: 35 | Batch_idx: 70 |  Loss_1: (0.5366) | Acc_1: (80.56%) (7321/9088)\n",
      "Epoch: 35 | Batch_idx: 80 |  Loss_1: (0.5313) | Acc_1: (80.79%) (8376/10368)\n",
      "Epoch: 35 | Batch_idx: 90 |  Loss_1: (0.5328) | Acc_1: (80.80%) (9412/11648)\n",
      "Epoch: 35 | Batch_idx: 100 |  Loss_1: (0.5300) | Acc_1: (80.90%) (10459/12928)\n",
      "Epoch: 35 | Batch_idx: 110 |  Loss_1: (0.5288) | Acc_1: (80.86%) (11489/14208)\n",
      "Epoch: 35 | Batch_idx: 120 |  Loss_1: (0.5296) | Acc_1: (80.76%) (12508/15488)\n",
      "Epoch: 35 | Batch_idx: 130 |  Loss_1: (0.5298) | Acc_1: (80.78%) (13545/16768)\n",
      "Epoch: 35 | Batch_idx: 140 |  Loss_1: (0.5311) | Acc_1: (80.65%) (14556/18048)\n",
      "Epoch: 35 | Batch_idx: 150 |  Loss_1: (0.5327) | Acc_1: (80.63%) (15584/19328)\n",
      "Epoch: 35 | Batch_idx: 160 |  Loss_1: (0.5323) | Acc_1: (80.65%) (16621/20608)\n",
      "Epoch: 35 | Batch_idx: 170 |  Loss_1: (0.5314) | Acc_1: (80.67%) (17656/21888)\n",
      "Epoch: 35 | Batch_idx: 180 |  Loss_1: (0.5312) | Acc_1: (80.71%) (18700/23168)\n",
      "Epoch: 35 | Batch_idx: 190 |  Loss_1: (0.5323) | Acc_1: (80.69%) (19726/24448)\n",
      "Epoch: 35 | Batch_idx: 200 |  Loss_1: (0.5316) | Acc_1: (80.70%) (20763/25728)\n",
      "Epoch: 35 | Batch_idx: 210 |  Loss_1: (0.5321) | Acc_1: (80.67%) (21788/27008)\n",
      "Epoch: 35 | Batch_idx: 220 |  Loss_1: (0.5320) | Acc_1: (80.70%) (22828/28288)\n",
      "Epoch: 35 | Batch_idx: 230 |  Loss_1: (0.5323) | Acc_1: (80.71%) (23865/29568)\n",
      "Epoch: 35 | Batch_idx: 240 |  Loss_1: (0.5306) | Acc_1: (80.79%) (24921/30848)\n",
      "Epoch: 35 | Batch_idx: 250 |  Loss_1: (0.5301) | Acc_1: (80.80%) (25960/32128)\n",
      "Epoch: 35 | Batch_idx: 260 |  Loss_1: (0.5307) | Acc_1: (80.79%) (26989/33408)\n",
      "Epoch: 35 | Batch_idx: 270 |  Loss_1: (0.5306) | Acc_1: (80.77%) (28017/34688)\n",
      "Epoch: 35 | Batch_idx: 280 |  Loss_1: (0.5299) | Acc_1: (80.76%) (29048/35968)\n",
      "Epoch: 35 | Batch_idx: 290 |  Loss_1: (0.5302) | Acc_1: (80.74%) (30074/37248)\n",
      "Epoch: 35 | Batch_idx: 300 |  Loss_1: (0.5303) | Acc_1: (80.71%) (31096/38528)\n",
      "Epoch: 35 | Batch_idx: 310 |  Loss_1: (0.5300) | Acc_1: (80.73%) (32136/39808)\n",
      "Epoch: 35 | Batch_idx: 320 |  Loss_1: (0.5288) | Acc_1: (80.76%) (33183/41088)\n",
      "Epoch: 35 | Batch_idx: 330 |  Loss_1: (0.5290) | Acc_1: (80.76%) (34217/42368)\n",
      "Epoch: 35 | Batch_idx: 340 |  Loss_1: (0.5285) | Acc_1: (80.77%) (35253/43648)\n",
      "Epoch: 35 | Batch_idx: 350 |  Loss_1: (0.5284) | Acc_1: (80.77%) (36289/44928)\n",
      "Epoch: 35 | Batch_idx: 360 |  Loss_1: (0.5272) | Acc_1: (80.80%) (37338/46208)\n",
      "Epoch: 35 | Batch_idx: 370 |  Loss_1: (0.5272) | Acc_1: (80.85%) (38393/47488)\n",
      "Epoch: 35 | Batch_idx: 380 |  Loss_1: (0.5271) | Acc_1: (80.85%) (39428/48768)\n",
      "Epoch: 35 | Batch_idx: 390 |  Loss_1: (0.5270) | Acc_1: (80.84%) (40419/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3645) | Acc: (87.71%) (8771/10000)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.4213) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.4797) | Acc_1: (82.67%) (1164/1408)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.4846) | Acc_1: (82.37%) (2214/2688)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.4738) | Acc_1: (82.96%) (3292/3968)\n",
      "Epoch: 36 | Batch_idx: 40 |  Loss_1: (0.4689) | Acc_1: (83.00%) (4356/5248)\n",
      "Epoch: 36 | Batch_idx: 50 |  Loss_1: (0.4754) | Acc_1: (82.54%) (5388/6528)\n",
      "Epoch: 36 | Batch_idx: 60 |  Loss_1: (0.4869) | Acc_1: (82.18%) (6417/7808)\n",
      "Epoch: 36 | Batch_idx: 70 |  Loss_1: (0.4875) | Acc_1: (82.15%) (7466/9088)\n",
      "Epoch: 36 | Batch_idx: 80 |  Loss_1: (0.4920) | Acc_1: (82.11%) (8513/10368)\n",
      "Epoch: 36 | Batch_idx: 90 |  Loss_1: (0.4975) | Acc_1: (81.89%) (9538/11648)\n",
      "Epoch: 36 | Batch_idx: 100 |  Loss_1: (0.4973) | Acc_1: (81.85%) (10582/12928)\n",
      "Epoch: 36 | Batch_idx: 110 |  Loss_1: (0.4944) | Acc_1: (81.94%) (11642/14208)\n",
      "Epoch: 36 | Batch_idx: 120 |  Loss_1: (0.4933) | Acc_1: (81.96%) (12694/15488)\n",
      "Epoch: 36 | Batch_idx: 130 |  Loss_1: (0.4961) | Acc_1: (81.80%) (13716/16768)\n",
      "Epoch: 36 | Batch_idx: 140 |  Loss_1: (0.4952) | Acc_1: (81.90%) (14782/18048)\n",
      "Epoch: 36 | Batch_idx: 150 |  Loss_1: (0.4984) | Acc_1: (81.78%) (15807/19328)\n",
      "Epoch: 36 | Batch_idx: 160 |  Loss_1: (0.4983) | Acc_1: (81.79%) (16855/20608)\n",
      "Epoch: 36 | Batch_idx: 170 |  Loss_1: (0.4997) | Acc_1: (81.68%) (17878/21888)\n",
      "Epoch: 36 | Batch_idx: 180 |  Loss_1: (0.4979) | Acc_1: (81.76%) (18943/23168)\n",
      "Epoch: 36 | Batch_idx: 190 |  Loss_1: (0.4977) | Acc_1: (81.80%) (19999/24448)\n",
      "Epoch: 36 | Batch_idx: 200 |  Loss_1: (0.4979) | Acc_1: (81.78%) (21040/25728)\n",
      "Epoch: 36 | Batch_idx: 210 |  Loss_1: (0.4984) | Acc_1: (81.76%) (22081/27008)\n",
      "Epoch: 36 | Batch_idx: 220 |  Loss_1: (0.4994) | Acc_1: (81.71%) (23113/28288)\n",
      "Epoch: 36 | Batch_idx: 230 |  Loss_1: (0.5006) | Acc_1: (81.66%) (24145/29568)\n",
      "Epoch: 36 | Batch_idx: 240 |  Loss_1: (0.5021) | Acc_1: (81.57%) (25163/30848)\n",
      "Epoch: 36 | Batch_idx: 250 |  Loss_1: (0.5050) | Acc_1: (81.48%) (26177/32128)\n",
      "Epoch: 36 | Batch_idx: 260 |  Loss_1: (0.5056) | Acc_1: (81.45%) (27211/33408)\n",
      "Epoch: 36 | Batch_idx: 270 |  Loss_1: (0.5087) | Acc_1: (81.37%) (28226/34688)\n",
      "Epoch: 36 | Batch_idx: 280 |  Loss_1: (0.5073) | Acc_1: (81.46%) (29300/35968)\n",
      "Epoch: 36 | Batch_idx: 290 |  Loss_1: (0.5060) | Acc_1: (81.51%) (30361/37248)\n",
      "Epoch: 36 | Batch_idx: 300 |  Loss_1: (0.5070) | Acc_1: (81.47%) (31390/38528)\n",
      "Epoch: 36 | Batch_idx: 310 |  Loss_1: (0.5080) | Acc_1: (81.47%) (32430/39808)\n",
      "Epoch: 36 | Batch_idx: 320 |  Loss_1: (0.5092) | Acc_1: (81.44%) (33461/41088)\n",
      "Epoch: 36 | Batch_idx: 330 |  Loss_1: (0.5096) | Acc_1: (81.43%) (34501/42368)\n",
      "Epoch: 36 | Batch_idx: 340 |  Loss_1: (0.5098) | Acc_1: (81.44%) (35549/43648)\n",
      "Epoch: 36 | Batch_idx: 350 |  Loss_1: (0.5101) | Acc_1: (81.43%) (36587/44928)\n",
      "Epoch: 36 | Batch_idx: 360 |  Loss_1: (0.5098) | Acc_1: (81.46%) (37640/46208)\n",
      "Epoch: 36 | Batch_idx: 370 |  Loss_1: (0.5101) | Acc_1: (81.47%) (38688/47488)\n",
      "Epoch: 36 | Batch_idx: 380 |  Loss_1: (0.5107) | Acc_1: (81.45%) (39721/48768)\n",
      "Epoch: 36 | Batch_idx: 390 |  Loss_1: (0.5116) | Acc_1: (81.42%) (40708/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3748) | Acc: (87.16%) (8716/10000)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.6224) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.4800) | Acc_1: (83.03%) (1169/1408)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.4956) | Acc_1: (82.22%) (2210/2688)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.4883) | Acc_1: (82.46%) (3272/3968)\n",
      "Epoch: 37 | Batch_idx: 40 |  Loss_1: (0.4917) | Acc_1: (82.18%) (4313/5248)\n",
      "Epoch: 37 | Batch_idx: 50 |  Loss_1: (0.4887) | Acc_1: (82.31%) (5373/6528)\n",
      "Epoch: 37 | Batch_idx: 60 |  Loss_1: (0.4890) | Acc_1: (82.29%) (6425/7808)\n",
      "Epoch: 37 | Batch_idx: 70 |  Loss_1: (0.4949) | Acc_1: (82.16%) (7467/9088)\n",
      "Epoch: 37 | Batch_idx: 80 |  Loss_1: (0.4985) | Acc_1: (81.96%) (8498/10368)\n",
      "Epoch: 37 | Batch_idx: 90 |  Loss_1: (0.5017) | Acc_1: (81.74%) (9521/11648)\n",
      "Epoch: 37 | Batch_idx: 100 |  Loss_1: (0.4971) | Acc_1: (81.90%) (10588/12928)\n",
      "Epoch: 37 | Batch_idx: 110 |  Loss_1: (0.4960) | Acc_1: (81.89%) (11635/14208)\n",
      "Epoch: 37 | Batch_idx: 120 |  Loss_1: (0.4969) | Acc_1: (81.82%) (12673/15488)\n",
      "Epoch: 37 | Batch_idx: 130 |  Loss_1: (0.4992) | Acc_1: (81.68%) (13696/16768)\n",
      "Epoch: 37 | Batch_idx: 140 |  Loss_1: (0.5007) | Acc_1: (81.64%) (14735/18048)\n",
      "Epoch: 37 | Batch_idx: 150 |  Loss_1: (0.5000) | Acc_1: (81.72%) (15794/19328)\n",
      "Epoch: 37 | Batch_idx: 160 |  Loss_1: (0.4973) | Acc_1: (81.81%) (16859/20608)\n",
      "Epoch: 37 | Batch_idx: 170 |  Loss_1: (0.4971) | Acc_1: (81.80%) (17905/21888)\n",
      "Epoch: 37 | Batch_idx: 180 |  Loss_1: (0.4978) | Acc_1: (81.80%) (18951/23168)\n",
      "Epoch: 37 | Batch_idx: 190 |  Loss_1: (0.4968) | Acc_1: (81.81%) (20001/24448)\n",
      "Epoch: 37 | Batch_idx: 200 |  Loss_1: (0.4956) | Acc_1: (81.86%) (21061/25728)\n",
      "Epoch: 37 | Batch_idx: 210 |  Loss_1: (0.4951) | Acc_1: (81.86%) (22108/27008)\n",
      "Epoch: 37 | Batch_idx: 220 |  Loss_1: (0.4949) | Acc_1: (81.88%) (23161/28288)\n",
      "Epoch: 37 | Batch_idx: 230 |  Loss_1: (0.4961) | Acc_1: (81.84%) (24197/29568)\n",
      "Epoch: 37 | Batch_idx: 240 |  Loss_1: (0.4959) | Acc_1: (81.87%) (25254/30848)\n",
      "Epoch: 37 | Batch_idx: 250 |  Loss_1: (0.4959) | Acc_1: (81.87%) (26303/32128)\n",
      "Epoch: 37 | Batch_idx: 260 |  Loss_1: (0.4952) | Acc_1: (81.94%) (27375/33408)\n",
      "Epoch: 37 | Batch_idx: 270 |  Loss_1: (0.4974) | Acc_1: (81.82%) (28381/34688)\n",
      "Epoch: 37 | Batch_idx: 280 |  Loss_1: (0.4970) | Acc_1: (81.84%) (29437/35968)\n",
      "Epoch: 37 | Batch_idx: 290 |  Loss_1: (0.4967) | Acc_1: (81.84%) (30484/37248)\n",
      "Epoch: 37 | Batch_idx: 300 |  Loss_1: (0.4959) | Acc_1: (81.88%) (31546/38528)\n",
      "Epoch: 37 | Batch_idx: 310 |  Loss_1: (0.4963) | Acc_1: (81.86%) (32586/39808)\n",
      "Epoch: 37 | Batch_idx: 320 |  Loss_1: (0.4956) | Acc_1: (81.87%) (33637/41088)\n",
      "Epoch: 37 | Batch_idx: 330 |  Loss_1: (0.4959) | Acc_1: (81.86%) (34681/42368)\n",
      "Epoch: 37 | Batch_idx: 340 |  Loss_1: (0.4963) | Acc_1: (81.85%) (35725/43648)\n",
      "Epoch: 37 | Batch_idx: 350 |  Loss_1: (0.4976) | Acc_1: (81.81%) (36757/44928)\n",
      "Epoch: 37 | Batch_idx: 360 |  Loss_1: (0.4978) | Acc_1: (81.79%) (37792/46208)\n",
      "Epoch: 37 | Batch_idx: 370 |  Loss_1: (0.4958) | Acc_1: (81.86%) (38873/47488)\n",
      "Epoch: 37 | Batch_idx: 380 |  Loss_1: (0.4967) | Acc_1: (81.84%) (39913/48768)\n",
      "Epoch: 37 | Batch_idx: 390 |  Loss_1: (0.4962) | Acc_1: (81.85%) (40927/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3502) | Acc: (88.21%) (8821/10000)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.4782) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.4987) | Acc_1: (80.97%) (1140/1408)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.4834) | Acc_1: (81.36%) (2187/2688)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.4850) | Acc_1: (81.91%) (3250/3968)\n",
      "Epoch: 38 | Batch_idx: 40 |  Loss_1: (0.4818) | Acc_1: (82.20%) (4314/5248)\n",
      "Epoch: 38 | Batch_idx: 50 |  Loss_1: (0.4856) | Acc_1: (81.91%) (5347/6528)\n",
      "Epoch: 38 | Batch_idx: 60 |  Loss_1: (0.4875) | Acc_1: (81.88%) (6393/7808)\n",
      "Epoch: 38 | Batch_idx: 70 |  Loss_1: (0.4813) | Acc_1: (82.21%) (7471/9088)\n",
      "Epoch: 38 | Batch_idx: 80 |  Loss_1: (0.4770) | Acc_1: (82.41%) (8544/10368)\n",
      "Epoch: 38 | Batch_idx: 90 |  Loss_1: (0.4767) | Acc_1: (82.48%) (9607/11648)\n",
      "Epoch: 38 | Batch_idx: 100 |  Loss_1: (0.4741) | Acc_1: (82.60%) (10679/12928)\n",
      "Epoch: 38 | Batch_idx: 110 |  Loss_1: (0.4719) | Acc_1: (82.74%) (11755/14208)\n",
      "Epoch: 38 | Batch_idx: 120 |  Loss_1: (0.4734) | Acc_1: (82.66%) (12803/15488)\n",
      "Epoch: 38 | Batch_idx: 130 |  Loss_1: (0.4737) | Acc_1: (82.62%) (13854/16768)\n",
      "Epoch: 38 | Batch_idx: 140 |  Loss_1: (0.4755) | Acc_1: (82.58%) (14904/18048)\n",
      "Epoch: 38 | Batch_idx: 150 |  Loss_1: (0.4738) | Acc_1: (82.66%) (15976/19328)\n",
      "Epoch: 38 | Batch_idx: 160 |  Loss_1: (0.4727) | Acc_1: (82.70%) (17042/20608)\n",
      "Epoch: 38 | Batch_idx: 170 |  Loss_1: (0.4734) | Acc_1: (82.68%) (18098/21888)\n",
      "Epoch: 38 | Batch_idx: 180 |  Loss_1: (0.4772) | Acc_1: (82.57%) (19129/23168)\n",
      "Epoch: 38 | Batch_idx: 190 |  Loss_1: (0.4784) | Acc_1: (82.54%) (20180/24448)\n",
      "Epoch: 38 | Batch_idx: 200 |  Loss_1: (0.4792) | Acc_1: (82.54%) (21235/25728)\n",
      "Epoch: 38 | Batch_idx: 210 |  Loss_1: (0.4778) | Acc_1: (82.58%) (22304/27008)\n",
      "Epoch: 38 | Batch_idx: 220 |  Loss_1: (0.4781) | Acc_1: (82.59%) (23364/28288)\n",
      "Epoch: 38 | Batch_idx: 230 |  Loss_1: (0.4800) | Acc_1: (82.53%) (24403/29568)\n",
      "Epoch: 38 | Batch_idx: 240 |  Loss_1: (0.4796) | Acc_1: (82.52%) (25456/30848)\n",
      "Epoch: 38 | Batch_idx: 250 |  Loss_1: (0.4821) | Acc_1: (82.44%) (26485/32128)\n",
      "Epoch: 38 | Batch_idx: 260 |  Loss_1: (0.4825) | Acc_1: (82.44%) (27540/33408)\n",
      "Epoch: 38 | Batch_idx: 270 |  Loss_1: (0.4805) | Acc_1: (82.50%) (28619/34688)\n",
      "Epoch: 38 | Batch_idx: 280 |  Loss_1: (0.4801) | Acc_1: (82.53%) (29684/35968)\n",
      "Epoch: 38 | Batch_idx: 290 |  Loss_1: (0.4792) | Acc_1: (82.55%) (30748/37248)\n",
      "Epoch: 38 | Batch_idx: 300 |  Loss_1: (0.4797) | Acc_1: (82.54%) (31801/38528)\n",
      "Epoch: 38 | Batch_idx: 310 |  Loss_1: (0.4805) | Acc_1: (82.51%) (32847/39808)\n",
      "Epoch: 38 | Batch_idx: 320 |  Loss_1: (0.4804) | Acc_1: (82.51%) (33902/41088)\n",
      "Epoch: 38 | Batch_idx: 330 |  Loss_1: (0.4802) | Acc_1: (82.50%) (34954/42368)\n",
      "Epoch: 38 | Batch_idx: 340 |  Loss_1: (0.4807) | Acc_1: (82.46%) (35994/43648)\n",
      "Epoch: 38 | Batch_idx: 350 |  Loss_1: (0.4799) | Acc_1: (82.49%) (37060/44928)\n",
      "Epoch: 38 | Batch_idx: 360 |  Loss_1: (0.4797) | Acc_1: (82.51%) (38127/46208)\n",
      "Epoch: 38 | Batch_idx: 370 |  Loss_1: (0.4785) | Acc_1: (82.57%) (39212/47488)\n",
      "Epoch: 38 | Batch_idx: 380 |  Loss_1: (0.4793) | Acc_1: (82.53%) (40250/48768)\n",
      "Epoch: 38 | Batch_idx: 390 |  Loss_1: (0.4784) | Acc_1: (82.57%) (41285/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3299) | Acc: (88.38%) (8838/10000)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.4632) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.5075) | Acc_1: (82.81%) (1166/1408)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.4979) | Acc_1: (82.14%) (2208/2688)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.4972) | Acc_1: (81.96%) (3252/3968)\n",
      "Epoch: 39 | Batch_idx: 40 |  Loss_1: (0.4977) | Acc_1: (81.84%) (4295/5248)\n",
      "Epoch: 39 | Batch_idx: 50 |  Loss_1: (0.4927) | Acc_1: (82.05%) (5356/6528)\n",
      "Epoch: 39 | Batch_idx: 60 |  Loss_1: (0.4870) | Acc_1: (82.26%) (6423/7808)\n",
      "Epoch: 39 | Batch_idx: 70 |  Loss_1: (0.4786) | Acc_1: (82.57%) (7504/9088)\n",
      "Epoch: 39 | Batch_idx: 80 |  Loss_1: (0.4810) | Acc_1: (82.46%) (8549/10368)\n",
      "Epoch: 39 | Batch_idx: 90 |  Loss_1: (0.4806) | Acc_1: (82.43%) (9602/11648)\n",
      "Epoch: 39 | Batch_idx: 100 |  Loss_1: (0.4809) | Acc_1: (82.39%) (10651/12928)\n",
      "Epoch: 39 | Batch_idx: 110 |  Loss_1: (0.4811) | Acc_1: (82.40%) (11707/14208)\n",
      "Epoch: 39 | Batch_idx: 120 |  Loss_1: (0.4816) | Acc_1: (82.45%) (12770/15488)\n",
      "Epoch: 39 | Batch_idx: 130 |  Loss_1: (0.4802) | Acc_1: (82.54%) (13840/16768)\n",
      "Epoch: 39 | Batch_idx: 140 |  Loss_1: (0.4821) | Acc_1: (82.49%) (14887/18048)\n",
      "Epoch: 39 | Batch_idx: 150 |  Loss_1: (0.4794) | Acc_1: (82.54%) (15954/19328)\n",
      "Epoch: 39 | Batch_idx: 160 |  Loss_1: (0.4789) | Acc_1: (82.51%) (17004/20608)\n",
      "Epoch: 39 | Batch_idx: 170 |  Loss_1: (0.4768) | Acc_1: (82.58%) (18075/21888)\n",
      "Epoch: 39 | Batch_idx: 180 |  Loss_1: (0.4763) | Acc_1: (82.62%) (19142/23168)\n",
      "Epoch: 39 | Batch_idx: 190 |  Loss_1: (0.4751) | Acc_1: (82.66%) (20209/24448)\n",
      "Epoch: 39 | Batch_idx: 200 |  Loss_1: (0.4743) | Acc_1: (82.66%) (21266/25728)\n",
      "Epoch: 39 | Batch_idx: 210 |  Loss_1: (0.4745) | Acc_1: (82.59%) (22306/27008)\n",
      "Epoch: 39 | Batch_idx: 220 |  Loss_1: (0.4755) | Acc_1: (82.53%) (23346/28288)\n",
      "Epoch: 39 | Batch_idx: 230 |  Loss_1: (0.4786) | Acc_1: (82.43%) (24373/29568)\n",
      "Epoch: 39 | Batch_idx: 240 |  Loss_1: (0.4796) | Acc_1: (82.38%) (25414/30848)\n",
      "Epoch: 39 | Batch_idx: 250 |  Loss_1: (0.4786) | Acc_1: (82.41%) (26477/32128)\n",
      "Epoch: 39 | Batch_idx: 260 |  Loss_1: (0.4777) | Acc_1: (82.44%) (27540/33408)\n",
      "Epoch: 39 | Batch_idx: 270 |  Loss_1: (0.4764) | Acc_1: (82.47%) (28607/34688)\n",
      "Epoch: 39 | Batch_idx: 280 |  Loss_1: (0.4743) | Acc_1: (82.54%) (29687/35968)\n",
      "Epoch: 39 | Batch_idx: 290 |  Loss_1: (0.4737) | Acc_1: (82.57%) (30756/37248)\n",
      "Epoch: 39 | Batch_idx: 300 |  Loss_1: (0.4737) | Acc_1: (82.57%) (31811/38528)\n",
      "Epoch: 39 | Batch_idx: 310 |  Loss_1: (0.4727) | Acc_1: (82.61%) (32885/39808)\n",
      "Epoch: 39 | Batch_idx: 320 |  Loss_1: (0.4732) | Acc_1: (82.63%) (33953/41088)\n",
      "Epoch: 39 | Batch_idx: 330 |  Loss_1: (0.4732) | Acc_1: (82.61%) (35002/42368)\n",
      "Epoch: 39 | Batch_idx: 340 |  Loss_1: (0.4729) | Acc_1: (82.61%) (36059/43648)\n",
      "Epoch: 39 | Batch_idx: 350 |  Loss_1: (0.4734) | Acc_1: (82.61%) (37117/44928)\n",
      "Epoch: 39 | Batch_idx: 360 |  Loss_1: (0.4729) | Acc_1: (82.63%) (38180/46208)\n",
      "Epoch: 39 | Batch_idx: 370 |  Loss_1: (0.4725) | Acc_1: (82.64%) (39246/47488)\n",
      "Epoch: 39 | Batch_idx: 380 |  Loss_1: (0.4717) | Acc_1: (82.68%) (40321/48768)\n",
      "Epoch: 39 | Batch_idx: 390 |  Loss_1: (0.4724) | Acc_1: (82.64%) (41319/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3330) | Acc: (88.61%) (8861/10000)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.3863) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.4464) | Acc_1: (83.95%) (1182/1408)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.4417) | Acc_1: (83.82%) (2253/2688)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.4468) | Acc_1: (83.64%) (3319/3968)\n",
      "Epoch: 40 | Batch_idx: 40 |  Loss_1: (0.4556) | Acc_1: (83.56%) (4385/5248)\n",
      "Epoch: 40 | Batch_idx: 50 |  Loss_1: (0.4532) | Acc_1: (83.46%) (5448/6528)\n",
      "Epoch: 40 | Batch_idx: 60 |  Loss_1: (0.4497) | Acc_1: (83.63%) (6530/7808)\n",
      "Epoch: 40 | Batch_idx: 70 |  Loss_1: (0.4521) | Acc_1: (83.62%) (7599/9088)\n",
      "Epoch: 40 | Batch_idx: 80 |  Loss_1: (0.4504) | Acc_1: (83.63%) (8671/10368)\n",
      "Epoch: 40 | Batch_idx: 90 |  Loss_1: (0.4505) | Acc_1: (83.65%) (9743/11648)\n",
      "Epoch: 40 | Batch_idx: 100 |  Loss_1: (0.4459) | Acc_1: (83.79%) (10832/12928)\n",
      "Epoch: 40 | Batch_idx: 110 |  Loss_1: (0.4476) | Acc_1: (83.75%) (11899/14208)\n",
      "Epoch: 40 | Batch_idx: 120 |  Loss_1: (0.4492) | Acc_1: (83.78%) (12976/15488)\n",
      "Epoch: 40 | Batch_idx: 130 |  Loss_1: (0.4490) | Acc_1: (83.87%) (14064/16768)\n",
      "Epoch: 40 | Batch_idx: 140 |  Loss_1: (0.4527) | Acc_1: (83.69%) (15105/18048)\n",
      "Epoch: 40 | Batch_idx: 150 |  Loss_1: (0.4524) | Acc_1: (83.69%) (16176/19328)\n",
      "Epoch: 40 | Batch_idx: 160 |  Loss_1: (0.4512) | Acc_1: (83.70%) (17248/20608)\n",
      "Epoch: 40 | Batch_idx: 170 |  Loss_1: (0.4507) | Acc_1: (83.67%) (18313/21888)\n",
      "Epoch: 40 | Batch_idx: 180 |  Loss_1: (0.4519) | Acc_1: (83.56%) (19359/23168)\n",
      "Epoch: 40 | Batch_idx: 190 |  Loss_1: (0.4520) | Acc_1: (83.59%) (20437/24448)\n",
      "Epoch: 40 | Batch_idx: 200 |  Loss_1: (0.4521) | Acc_1: (83.61%) (21510/25728)\n",
      "Epoch: 40 | Batch_idx: 210 |  Loss_1: (0.4529) | Acc_1: (83.57%) (22570/27008)\n",
      "Epoch: 40 | Batch_idx: 220 |  Loss_1: (0.4535) | Acc_1: (83.56%) (23637/28288)\n",
      "Epoch: 40 | Batch_idx: 230 |  Loss_1: (0.4522) | Acc_1: (83.57%) (24709/29568)\n",
      "Epoch: 40 | Batch_idx: 240 |  Loss_1: (0.4534) | Acc_1: (83.49%) (25754/30848)\n",
      "Epoch: 40 | Batch_idx: 250 |  Loss_1: (0.4557) | Acc_1: (83.39%) (26791/32128)\n",
      "Epoch: 40 | Batch_idx: 260 |  Loss_1: (0.4573) | Acc_1: (83.29%) (27825/33408)\n",
      "Epoch: 40 | Batch_idx: 270 |  Loss_1: (0.4576) | Acc_1: (83.29%) (28891/34688)\n",
      "Epoch: 40 | Batch_idx: 280 |  Loss_1: (0.4597) | Acc_1: (83.22%) (29931/35968)\n",
      "Epoch: 40 | Batch_idx: 290 |  Loss_1: (0.4595) | Acc_1: (83.23%) (31001/37248)\n",
      "Epoch: 40 | Batch_idx: 300 |  Loss_1: (0.4595) | Acc_1: (83.24%) (32070/38528)\n",
      "Epoch: 40 | Batch_idx: 310 |  Loss_1: (0.4591) | Acc_1: (83.28%) (33152/39808)\n",
      "Epoch: 40 | Batch_idx: 320 |  Loss_1: (0.4587) | Acc_1: (83.26%) (34211/41088)\n",
      "Epoch: 40 | Batch_idx: 330 |  Loss_1: (0.4585) | Acc_1: (83.27%) (35278/42368)\n",
      "Epoch: 40 | Batch_idx: 340 |  Loss_1: (0.4582) | Acc_1: (83.26%) (36340/43648)\n",
      "Epoch: 40 | Batch_idx: 350 |  Loss_1: (0.4576) | Acc_1: (83.28%) (37414/44928)\n",
      "Epoch: 40 | Batch_idx: 360 |  Loss_1: (0.4576) | Acc_1: (83.27%) (38476/46208)\n",
      "Epoch: 40 | Batch_idx: 370 |  Loss_1: (0.4577) | Acc_1: (83.26%) (39540/47488)\n",
      "Epoch: 40 | Batch_idx: 380 |  Loss_1: (0.4571) | Acc_1: (83.29%) (40621/48768)\n",
      "Epoch: 40 | Batch_idx: 390 |  Loss_1: (0.4577) | Acc_1: (83.27%) (41633/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3168) | Acc: (89.16%) (8916/10000)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.4034) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.4604) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.4380) | Acc_1: (84.11%) (2261/2688)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.4262) | Acc_1: (84.35%) (3347/3968)\n",
      "Epoch: 41 | Batch_idx: 40 |  Loss_1: (0.4332) | Acc_1: (84.30%) (4424/5248)\n",
      "Epoch: 41 | Batch_idx: 50 |  Loss_1: (0.4369) | Acc_1: (84.07%) (5488/6528)\n",
      "Epoch: 41 | Batch_idx: 60 |  Loss_1: (0.4423) | Acc_1: (83.80%) (6543/7808)\n",
      "Epoch: 41 | Batch_idx: 70 |  Loss_1: (0.4402) | Acc_1: (83.91%) (7626/9088)\n",
      "Epoch: 41 | Batch_idx: 80 |  Loss_1: (0.4382) | Acc_1: (83.98%) (8707/10368)\n",
      "Epoch: 41 | Batch_idx: 90 |  Loss_1: (0.4436) | Acc_1: (83.73%) (9753/11648)\n",
      "Epoch: 41 | Batch_idx: 100 |  Loss_1: (0.4464) | Acc_1: (83.65%) (10814/12928)\n",
      "Epoch: 41 | Batch_idx: 110 |  Loss_1: (0.4462) | Acc_1: (83.61%) (11879/14208)\n",
      "Epoch: 41 | Batch_idx: 120 |  Loss_1: (0.4476) | Acc_1: (83.63%) (12952/15488)\n",
      "Epoch: 41 | Batch_idx: 130 |  Loss_1: (0.4448) | Acc_1: (83.78%) (14049/16768)\n",
      "Epoch: 41 | Batch_idx: 140 |  Loss_1: (0.4482) | Acc_1: (83.69%) (15105/18048)\n",
      "Epoch: 41 | Batch_idx: 150 |  Loss_1: (0.4464) | Acc_1: (83.79%) (16194/19328)\n",
      "Epoch: 41 | Batch_idx: 160 |  Loss_1: (0.4456) | Acc_1: (83.81%) (17271/20608)\n",
      "Epoch: 41 | Batch_idx: 170 |  Loss_1: (0.4446) | Acc_1: (83.85%) (18353/21888)\n",
      "Epoch: 41 | Batch_idx: 180 |  Loss_1: (0.4455) | Acc_1: (83.81%) (19416/23168)\n",
      "Epoch: 41 | Batch_idx: 190 |  Loss_1: (0.4458) | Acc_1: (83.80%) (20487/24448)\n",
      "Epoch: 41 | Batch_idx: 200 |  Loss_1: (0.4464) | Acc_1: (83.77%) (21553/25728)\n",
      "Epoch: 41 | Batch_idx: 210 |  Loss_1: (0.4470) | Acc_1: (83.73%) (22615/27008)\n",
      "Epoch: 41 | Batch_idx: 220 |  Loss_1: (0.4477) | Acc_1: (83.67%) (23669/28288)\n",
      "Epoch: 41 | Batch_idx: 230 |  Loss_1: (0.4487) | Acc_1: (83.61%) (24721/29568)\n",
      "Epoch: 41 | Batch_idx: 240 |  Loss_1: (0.4486) | Acc_1: (83.61%) (25793/30848)\n",
      "Epoch: 41 | Batch_idx: 250 |  Loss_1: (0.4477) | Acc_1: (83.63%) (26868/32128)\n",
      "Epoch: 41 | Batch_idx: 260 |  Loss_1: (0.4482) | Acc_1: (83.64%) (27944/33408)\n",
      "Epoch: 41 | Batch_idx: 270 |  Loss_1: (0.4469) | Acc_1: (83.68%) (29027/34688)\n",
      "Epoch: 41 | Batch_idx: 280 |  Loss_1: (0.4479) | Acc_1: (83.66%) (30091/35968)\n",
      "Epoch: 41 | Batch_idx: 290 |  Loss_1: (0.4475) | Acc_1: (83.68%) (31168/37248)\n",
      "Epoch: 41 | Batch_idx: 300 |  Loss_1: (0.4477) | Acc_1: (83.69%) (32244/38528)\n",
      "Epoch: 41 | Batch_idx: 310 |  Loss_1: (0.4474) | Acc_1: (83.68%) (33310/39808)\n",
      "Epoch: 41 | Batch_idx: 320 |  Loss_1: (0.4481) | Acc_1: (83.63%) (34361/41088)\n",
      "Epoch: 41 | Batch_idx: 330 |  Loss_1: (0.4491) | Acc_1: (83.57%) (35406/42368)\n",
      "Epoch: 41 | Batch_idx: 340 |  Loss_1: (0.4490) | Acc_1: (83.57%) (36478/43648)\n",
      "Epoch: 41 | Batch_idx: 350 |  Loss_1: (0.4497) | Acc_1: (83.54%) (37531/44928)\n",
      "Epoch: 41 | Batch_idx: 360 |  Loss_1: (0.4500) | Acc_1: (83.51%) (38588/46208)\n",
      "Epoch: 41 | Batch_idx: 370 |  Loss_1: (0.4495) | Acc_1: (83.53%) (39665/47488)\n",
      "Epoch: 41 | Batch_idx: 380 |  Loss_1: (0.4496) | Acc_1: (83.53%) (40735/48768)\n",
      "Epoch: 41 | Batch_idx: 390 |  Loss_1: (0.4502) | Acc_1: (83.51%) (41757/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3369) | Acc: (88.39%) (8839/10000)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.4050) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.4429) | Acc_1: (83.66%) (1178/1408)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.4348) | Acc_1: (84.00%) (2258/2688)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.4456) | Acc_1: (83.59%) (3317/3968)\n",
      "Epoch: 42 | Batch_idx: 40 |  Loss_1: (0.4515) | Acc_1: (83.23%) (4368/5248)\n",
      "Epoch: 42 | Batch_idx: 50 |  Loss_1: (0.4421) | Acc_1: (83.66%) (5461/6528)\n",
      "Epoch: 42 | Batch_idx: 60 |  Loss_1: (0.4352) | Acc_1: (83.95%) (6555/7808)\n",
      "Epoch: 42 | Batch_idx: 70 |  Loss_1: (0.4415) | Acc_1: (83.75%) (7611/9088)\n",
      "Epoch: 42 | Batch_idx: 80 |  Loss_1: (0.4431) | Acc_1: (83.70%) (8678/10368)\n",
      "Epoch: 42 | Batch_idx: 90 |  Loss_1: (0.4446) | Acc_1: (83.70%) (9749/11648)\n",
      "Epoch: 42 | Batch_idx: 100 |  Loss_1: (0.4492) | Acc_1: (83.56%) (10802/12928)\n",
      "Epoch: 42 | Batch_idx: 110 |  Loss_1: (0.4495) | Acc_1: (83.56%) (11872/14208)\n",
      "Epoch: 42 | Batch_idx: 120 |  Loss_1: (0.4454) | Acc_1: (83.67%) (12959/15488)\n",
      "Epoch: 42 | Batch_idx: 130 |  Loss_1: (0.4443) | Acc_1: (83.71%) (14036/16768)\n",
      "Epoch: 42 | Batch_idx: 140 |  Loss_1: (0.4487) | Acc_1: (83.52%) (15073/18048)\n",
      "Epoch: 42 | Batch_idx: 150 |  Loss_1: (0.4471) | Acc_1: (83.55%) (16148/19328)\n",
      "Epoch: 42 | Batch_idx: 160 |  Loss_1: (0.4465) | Acc_1: (83.54%) (17215/20608)\n",
      "Epoch: 42 | Batch_idx: 170 |  Loss_1: (0.4453) | Acc_1: (83.59%) (18296/21888)\n",
      "Epoch: 42 | Batch_idx: 180 |  Loss_1: (0.4443) | Acc_1: (83.61%) (19371/23168)\n",
      "Epoch: 42 | Batch_idx: 190 |  Loss_1: (0.4455) | Acc_1: (83.59%) (20437/24448)\n",
      "Epoch: 42 | Batch_idx: 200 |  Loss_1: (0.4434) | Acc_1: (83.68%) (21530/25728)\n",
      "Epoch: 42 | Batch_idx: 210 |  Loss_1: (0.4435) | Acc_1: (83.68%) (22601/27008)\n",
      "Epoch: 42 | Batch_idx: 220 |  Loss_1: (0.4438) | Acc_1: (83.66%) (23665/28288)\n",
      "Epoch: 42 | Batch_idx: 230 |  Loss_1: (0.4426) | Acc_1: (83.69%) (24745/29568)\n",
      "Epoch: 42 | Batch_idx: 240 |  Loss_1: (0.4424) | Acc_1: (83.71%) (25822/30848)\n",
      "Epoch: 42 | Batch_idx: 250 |  Loss_1: (0.4420) | Acc_1: (83.72%) (26899/32128)\n",
      "Epoch: 42 | Batch_idx: 260 |  Loss_1: (0.4435) | Acc_1: (83.68%) (27955/33408)\n",
      "Epoch: 42 | Batch_idx: 270 |  Loss_1: (0.4439) | Acc_1: (83.68%) (29028/34688)\n",
      "Epoch: 42 | Batch_idx: 280 |  Loss_1: (0.4436) | Acc_1: (83.70%) (30104/35968)\n",
      "Epoch: 42 | Batch_idx: 290 |  Loss_1: (0.4432) | Acc_1: (83.71%) (31181/37248)\n",
      "Epoch: 42 | Batch_idx: 300 |  Loss_1: (0.4428) | Acc_1: (83.73%) (32259/38528)\n",
      "Epoch: 42 | Batch_idx: 310 |  Loss_1: (0.4425) | Acc_1: (83.76%) (33343/39808)\n",
      "Epoch: 42 | Batch_idx: 320 |  Loss_1: (0.4431) | Acc_1: (83.73%) (34401/41088)\n",
      "Epoch: 42 | Batch_idx: 330 |  Loss_1: (0.4428) | Acc_1: (83.74%) (35481/42368)\n",
      "Epoch: 42 | Batch_idx: 340 |  Loss_1: (0.4424) | Acc_1: (83.79%) (36573/43648)\n",
      "Epoch: 42 | Batch_idx: 350 |  Loss_1: (0.4439) | Acc_1: (83.74%) (37623/44928)\n",
      "Epoch: 42 | Batch_idx: 360 |  Loss_1: (0.4445) | Acc_1: (83.72%) (38684/46208)\n",
      "Epoch: 42 | Batch_idx: 370 |  Loss_1: (0.4439) | Acc_1: (83.76%) (39775/47488)\n",
      "Epoch: 42 | Batch_idx: 380 |  Loss_1: (0.4442) | Acc_1: (83.74%) (40840/48768)\n",
      "Epoch: 42 | Batch_idx: 390 |  Loss_1: (0.4447) | Acc_1: (83.71%) (41857/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3054) | Acc: (89.52%) (8952/10000)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.5080) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.4291) | Acc_1: (84.38%) (1188/1408)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.4410) | Acc_1: (83.59%) (2247/2688)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.4436) | Acc_1: (83.49%) (3313/3968)\n",
      "Epoch: 43 | Batch_idx: 40 |  Loss_1: (0.4438) | Acc_1: (83.50%) (4382/5248)\n",
      "Epoch: 43 | Batch_idx: 50 |  Loss_1: (0.4311) | Acc_1: (83.96%) (5481/6528)\n",
      "Epoch: 43 | Batch_idx: 60 |  Loss_1: (0.4298) | Acc_1: (83.97%) (6556/7808)\n",
      "Epoch: 43 | Batch_idx: 70 |  Loss_1: (0.4285) | Acc_1: (84.11%) (7644/9088)\n",
      "Epoch: 43 | Batch_idx: 80 |  Loss_1: (0.4304) | Acc_1: (83.95%) (8704/10368)\n",
      "Epoch: 43 | Batch_idx: 90 |  Loss_1: (0.4283) | Acc_1: (84.01%) (9786/11648)\n",
      "Epoch: 43 | Batch_idx: 100 |  Loss_1: (0.4293) | Acc_1: (83.97%) (10855/12928)\n",
      "Epoch: 43 | Batch_idx: 110 |  Loss_1: (0.4286) | Acc_1: (84.06%) (11943/14208)\n",
      "Epoch: 43 | Batch_idx: 120 |  Loss_1: (0.4267) | Acc_1: (84.19%) (13039/15488)\n",
      "Epoch: 43 | Batch_idx: 130 |  Loss_1: (0.4274) | Acc_1: (84.21%) (14120/16768)\n",
      "Epoch: 43 | Batch_idx: 140 |  Loss_1: (0.4266) | Acc_1: (84.24%) (15204/18048)\n",
      "Epoch: 43 | Batch_idx: 150 |  Loss_1: (0.4255) | Acc_1: (84.30%) (16293/19328)\n",
      "Epoch: 43 | Batch_idx: 160 |  Loss_1: (0.4261) | Acc_1: (84.32%) (17376/20608)\n",
      "Epoch: 43 | Batch_idx: 170 |  Loss_1: (0.4272) | Acc_1: (84.32%) (18455/21888)\n",
      "Epoch: 43 | Batch_idx: 180 |  Loss_1: (0.4284) | Acc_1: (84.23%) (19515/23168)\n",
      "Epoch: 43 | Batch_idx: 190 |  Loss_1: (0.4293) | Acc_1: (84.18%) (20581/24448)\n",
      "Epoch: 43 | Batch_idx: 200 |  Loss_1: (0.4297) | Acc_1: (84.19%) (21661/25728)\n",
      "Epoch: 43 | Batch_idx: 210 |  Loss_1: (0.4307) | Acc_1: (84.12%) (22718/27008)\n",
      "Epoch: 43 | Batch_idx: 220 |  Loss_1: (0.4291) | Acc_1: (84.17%) (23811/28288)\n",
      "Epoch: 43 | Batch_idx: 230 |  Loss_1: (0.4275) | Acc_1: (84.19%) (24892/29568)\n",
      "Epoch: 43 | Batch_idx: 240 |  Loss_1: (0.4292) | Acc_1: (84.14%) (25954/30848)\n",
      "Epoch: 43 | Batch_idx: 250 |  Loss_1: (0.4295) | Acc_1: (84.12%) (27027/32128)\n",
      "Epoch: 43 | Batch_idx: 260 |  Loss_1: (0.4278) | Acc_1: (84.20%) (28130/33408)\n",
      "Epoch: 43 | Batch_idx: 270 |  Loss_1: (0.4273) | Acc_1: (84.18%) (29202/34688)\n",
      "Epoch: 43 | Batch_idx: 280 |  Loss_1: (0.4275) | Acc_1: (84.21%) (30290/35968)\n",
      "Epoch: 43 | Batch_idx: 290 |  Loss_1: (0.4269) | Acc_1: (84.24%) (31377/37248)\n",
      "Epoch: 43 | Batch_idx: 300 |  Loss_1: (0.4261) | Acc_1: (84.29%) (32474/38528)\n",
      "Epoch: 43 | Batch_idx: 310 |  Loss_1: (0.4245) | Acc_1: (84.37%) (33586/39808)\n",
      "Epoch: 43 | Batch_idx: 320 |  Loss_1: (0.4249) | Acc_1: (84.38%) (34670/41088)\n",
      "Epoch: 43 | Batch_idx: 330 |  Loss_1: (0.4258) | Acc_1: (84.34%) (35734/42368)\n",
      "Epoch: 43 | Batch_idx: 340 |  Loss_1: (0.4267) | Acc_1: (84.31%) (36800/43648)\n",
      "Epoch: 43 | Batch_idx: 350 |  Loss_1: (0.4267) | Acc_1: (84.31%) (37877/44928)\n",
      "Epoch: 43 | Batch_idx: 360 |  Loss_1: (0.4279) | Acc_1: (84.27%) (38939/46208)\n",
      "Epoch: 43 | Batch_idx: 370 |  Loss_1: (0.4288) | Acc_1: (84.24%) (40003/47488)\n",
      "Epoch: 43 | Batch_idx: 380 |  Loss_1: (0.4282) | Acc_1: (84.26%) (41092/48768)\n",
      "Epoch: 43 | Batch_idx: 390 |  Loss_1: (0.4295) | Acc_1: (84.21%) (42105/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3626) | Acc: (87.55%) (8755/10000)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.5170) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.4318) | Acc_1: (84.87%) (1195/1408)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.4339) | Acc_1: (84.56%) (2273/2688)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.4308) | Acc_1: (84.55%) (3355/3968)\n",
      "Epoch: 44 | Batch_idx: 40 |  Loss_1: (0.4229) | Acc_1: (84.89%) (4455/5248)\n",
      "Epoch: 44 | Batch_idx: 50 |  Loss_1: (0.4229) | Acc_1: (84.76%) (5533/6528)\n",
      "Epoch: 44 | Batch_idx: 60 |  Loss_1: (0.4249) | Acc_1: (84.72%) (6615/7808)\n",
      "Epoch: 44 | Batch_idx: 70 |  Loss_1: (0.4246) | Acc_1: (84.69%) (7697/9088)\n",
      "Epoch: 44 | Batch_idx: 80 |  Loss_1: (0.4254) | Acc_1: (84.60%) (8771/10368)\n",
      "Epoch: 44 | Batch_idx: 90 |  Loss_1: (0.4169) | Acc_1: (84.83%) (9881/11648)\n",
      "Epoch: 44 | Batch_idx: 100 |  Loss_1: (0.4163) | Acc_1: (84.82%) (10966/12928)\n",
      "Epoch: 44 | Batch_idx: 110 |  Loss_1: (0.4208) | Acc_1: (84.73%) (12038/14208)\n",
      "Epoch: 44 | Batch_idx: 120 |  Loss_1: (0.4203) | Acc_1: (84.85%) (13141/15488)\n",
      "Epoch: 44 | Batch_idx: 130 |  Loss_1: (0.4208) | Acc_1: (84.74%) (14209/16768)\n",
      "Epoch: 44 | Batch_idx: 140 |  Loss_1: (0.4242) | Acc_1: (84.60%) (15268/18048)\n",
      "Epoch: 44 | Batch_idx: 150 |  Loss_1: (0.4238) | Acc_1: (84.57%) (16346/19328)\n",
      "Epoch: 44 | Batch_idx: 160 |  Loss_1: (0.4243) | Acc_1: (84.62%) (17438/20608)\n",
      "Epoch: 44 | Batch_idx: 170 |  Loss_1: (0.4224) | Acc_1: (84.65%) (18528/21888)\n",
      "Epoch: 44 | Batch_idx: 180 |  Loss_1: (0.4251) | Acc_1: (84.52%) (19582/23168)\n",
      "Epoch: 44 | Batch_idx: 190 |  Loss_1: (0.4249) | Acc_1: (84.48%) (20653/24448)\n",
      "Epoch: 44 | Batch_idx: 200 |  Loss_1: (0.4241) | Acc_1: (84.53%) (21747/25728)\n",
      "Epoch: 44 | Batch_idx: 210 |  Loss_1: (0.4252) | Acc_1: (84.46%) (22812/27008)\n",
      "Epoch: 44 | Batch_idx: 220 |  Loss_1: (0.4267) | Acc_1: (84.36%) (23864/28288)\n",
      "Epoch: 44 | Batch_idx: 230 |  Loss_1: (0.4294) | Acc_1: (84.27%) (24917/29568)\n",
      "Epoch: 44 | Batch_idx: 240 |  Loss_1: (0.4292) | Acc_1: (84.25%) (25989/30848)\n",
      "Epoch: 44 | Batch_idx: 250 |  Loss_1: (0.4292) | Acc_1: (84.23%) (27063/32128)\n",
      "Epoch: 44 | Batch_idx: 260 |  Loss_1: (0.4290) | Acc_1: (84.23%) (28139/33408)\n",
      "Epoch: 44 | Batch_idx: 270 |  Loss_1: (0.4295) | Acc_1: (84.19%) (29204/34688)\n",
      "Epoch: 44 | Batch_idx: 280 |  Loss_1: (0.4306) | Acc_1: (84.15%) (30267/35968)\n",
      "Epoch: 44 | Batch_idx: 290 |  Loss_1: (0.4285) | Acc_1: (84.24%) (31378/37248)\n",
      "Epoch: 44 | Batch_idx: 300 |  Loss_1: (0.4283) | Acc_1: (84.28%) (32470/38528)\n",
      "Epoch: 44 | Batch_idx: 310 |  Loss_1: (0.4288) | Acc_1: (84.26%) (33541/39808)\n",
      "Epoch: 44 | Batch_idx: 320 |  Loss_1: (0.4283) | Acc_1: (84.26%) (34619/41088)\n",
      "Epoch: 44 | Batch_idx: 330 |  Loss_1: (0.4281) | Acc_1: (84.29%) (35714/42368)\n",
      "Epoch: 44 | Batch_idx: 340 |  Loss_1: (0.4284) | Acc_1: (84.30%) (36794/43648)\n",
      "Epoch: 44 | Batch_idx: 350 |  Loss_1: (0.4282) | Acc_1: (84.30%) (37873/44928)\n",
      "Epoch: 44 | Batch_idx: 360 |  Loss_1: (0.4293) | Acc_1: (84.29%) (38949/46208)\n",
      "Epoch: 44 | Batch_idx: 370 |  Loss_1: (0.4280) | Acc_1: (84.36%) (40061/47488)\n",
      "Epoch: 44 | Batch_idx: 380 |  Loss_1: (0.4293) | Acc_1: (84.30%) (41113/48768)\n",
      "Epoch: 44 | Batch_idx: 390 |  Loss_1: (0.4293) | Acc_1: (84.30%) (42149/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2919) | Acc: (89.97%) (8997/10000)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.4792) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.4628) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.4519) | Acc_1: (83.33%) (2240/2688)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.4459) | Acc_1: (83.29%) (3305/3968)\n",
      "Epoch: 45 | Batch_idx: 40 |  Loss_1: (0.4360) | Acc_1: (83.82%) (4399/5248)\n",
      "Epoch: 45 | Batch_idx: 50 |  Loss_1: (0.4360) | Acc_1: (83.87%) (5475/6528)\n",
      "Epoch: 45 | Batch_idx: 60 |  Loss_1: (0.4285) | Acc_1: (84.07%) (6564/7808)\n",
      "Epoch: 45 | Batch_idx: 70 |  Loss_1: (0.4285) | Acc_1: (84.18%) (7650/9088)\n",
      "Epoch: 45 | Batch_idx: 80 |  Loss_1: (0.4270) | Acc_1: (84.30%) (8740/10368)\n",
      "Epoch: 45 | Batch_idx: 90 |  Loss_1: (0.4267) | Acc_1: (84.34%) (9824/11648)\n",
      "Epoch: 45 | Batch_idx: 100 |  Loss_1: (0.4223) | Acc_1: (84.44%) (10917/12928)\n",
      "Epoch: 45 | Batch_idx: 110 |  Loss_1: (0.4227) | Acc_1: (84.44%) (11997/14208)\n",
      "Epoch: 45 | Batch_idx: 120 |  Loss_1: (0.4220) | Acc_1: (84.47%) (13082/15488)\n",
      "Epoch: 45 | Batch_idx: 130 |  Loss_1: (0.4224) | Acc_1: (84.40%) (14153/16768)\n",
      "Epoch: 45 | Batch_idx: 140 |  Loss_1: (0.4208) | Acc_1: (84.46%) (15244/18048)\n",
      "Epoch: 45 | Batch_idx: 150 |  Loss_1: (0.4222) | Acc_1: (84.45%) (16323/19328)\n",
      "Epoch: 45 | Batch_idx: 160 |  Loss_1: (0.4218) | Acc_1: (84.44%) (17402/20608)\n",
      "Epoch: 45 | Batch_idx: 170 |  Loss_1: (0.4220) | Acc_1: (84.41%) (18475/21888)\n",
      "Epoch: 45 | Batch_idx: 180 |  Loss_1: (0.4233) | Acc_1: (84.39%) (19552/23168)\n",
      "Epoch: 45 | Batch_idx: 190 |  Loss_1: (0.4244) | Acc_1: (84.37%) (20627/24448)\n",
      "Epoch: 45 | Batch_idx: 200 |  Loss_1: (0.4241) | Acc_1: (84.39%) (21711/25728)\n",
      "Epoch: 45 | Batch_idx: 210 |  Loss_1: (0.4238) | Acc_1: (84.40%) (22794/27008)\n",
      "Epoch: 45 | Batch_idx: 220 |  Loss_1: (0.4226) | Acc_1: (84.42%) (23882/28288)\n",
      "Epoch: 45 | Batch_idx: 230 |  Loss_1: (0.4231) | Acc_1: (84.40%) (24956/29568)\n",
      "Epoch: 45 | Batch_idx: 240 |  Loss_1: (0.4228) | Acc_1: (84.40%) (26036/30848)\n",
      "Epoch: 45 | Batch_idx: 250 |  Loss_1: (0.4228) | Acc_1: (84.37%) (27107/32128)\n",
      "Epoch: 45 | Batch_idx: 260 |  Loss_1: (0.4229) | Acc_1: (84.38%) (28190/33408)\n",
      "Epoch: 45 | Batch_idx: 270 |  Loss_1: (0.4217) | Acc_1: (84.42%) (29283/34688)\n",
      "Epoch: 45 | Batch_idx: 280 |  Loss_1: (0.4212) | Acc_1: (84.46%) (30379/35968)\n",
      "Epoch: 45 | Batch_idx: 290 |  Loss_1: (0.4199) | Acc_1: (84.52%) (31481/37248)\n",
      "Epoch: 45 | Batch_idx: 300 |  Loss_1: (0.4203) | Acc_1: (84.53%) (32567/38528)\n",
      "Epoch: 45 | Batch_idx: 310 |  Loss_1: (0.4196) | Acc_1: (84.56%) (33663/39808)\n",
      "Epoch: 45 | Batch_idx: 320 |  Loss_1: (0.4201) | Acc_1: (84.54%) (34735/41088)\n",
      "Epoch: 45 | Batch_idx: 330 |  Loss_1: (0.4202) | Acc_1: (84.52%) (35810/42368)\n",
      "Epoch: 45 | Batch_idx: 340 |  Loss_1: (0.4203) | Acc_1: (84.53%) (36895/43648)\n",
      "Epoch: 45 | Batch_idx: 350 |  Loss_1: (0.4199) | Acc_1: (84.55%) (37987/44928)\n",
      "Epoch: 45 | Batch_idx: 360 |  Loss_1: (0.4201) | Acc_1: (84.54%) (39064/46208)\n",
      "Epoch: 45 | Batch_idx: 370 |  Loss_1: (0.4198) | Acc_1: (84.55%) (40149/47488)\n",
      "Epoch: 45 | Batch_idx: 380 |  Loss_1: (0.4198) | Acc_1: (84.55%) (41235/48768)\n",
      "Epoch: 45 | Batch_idx: 390 |  Loss_1: (0.4206) | Acc_1: (84.54%) (42268/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3292) | Acc: (88.61%) (8861/10000)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.3560) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.4116) | Acc_1: (85.09%) (1198/1408)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.4435) | Acc_1: (83.97%) (2257/2688)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.4309) | Acc_1: (84.12%) (3338/3968)\n",
      "Epoch: 46 | Batch_idx: 40 |  Loss_1: (0.4303) | Acc_1: (84.11%) (4414/5248)\n",
      "Epoch: 46 | Batch_idx: 50 |  Loss_1: (0.4322) | Acc_1: (83.98%) (5482/6528)\n",
      "Epoch: 46 | Batch_idx: 60 |  Loss_1: (0.4290) | Acc_1: (84.16%) (6571/7808)\n",
      "Epoch: 46 | Batch_idx: 70 |  Loss_1: (0.4217) | Acc_1: (84.41%) (7671/9088)\n",
      "Epoch: 46 | Batch_idx: 80 |  Loss_1: (0.4258) | Acc_1: (84.17%) (8727/10368)\n",
      "Epoch: 46 | Batch_idx: 90 |  Loss_1: (0.4272) | Acc_1: (84.32%) (9822/11648)\n",
      "Epoch: 46 | Batch_idx: 100 |  Loss_1: (0.4241) | Acc_1: (84.44%) (10917/12928)\n",
      "Epoch: 46 | Batch_idx: 110 |  Loss_1: (0.4243) | Acc_1: (84.38%) (11989/14208)\n",
      "Epoch: 46 | Batch_idx: 120 |  Loss_1: (0.4268) | Acc_1: (84.31%) (13058/15488)\n",
      "Epoch: 46 | Batch_idx: 130 |  Loss_1: (0.4227) | Acc_1: (84.39%) (14151/16768)\n",
      "Epoch: 46 | Batch_idx: 140 |  Loss_1: (0.4220) | Acc_1: (84.39%) (15231/18048)\n",
      "Epoch: 46 | Batch_idx: 150 |  Loss_1: (0.4194) | Acc_1: (84.48%) (16328/19328)\n",
      "Epoch: 46 | Batch_idx: 160 |  Loss_1: (0.4211) | Acc_1: (84.46%) (17405/20608)\n",
      "Epoch: 46 | Batch_idx: 170 |  Loss_1: (0.4187) | Acc_1: (84.54%) (18504/21888)\n",
      "Epoch: 46 | Batch_idx: 180 |  Loss_1: (0.4162) | Acc_1: (84.64%) (19609/23168)\n",
      "Epoch: 46 | Batch_idx: 190 |  Loss_1: (0.4152) | Acc_1: (84.67%) (20699/24448)\n",
      "Epoch: 46 | Batch_idx: 200 |  Loss_1: (0.4176) | Acc_1: (84.59%) (21763/25728)\n",
      "Epoch: 46 | Batch_idx: 210 |  Loss_1: (0.4171) | Acc_1: (84.58%) (22844/27008)\n",
      "Epoch: 46 | Batch_idx: 220 |  Loss_1: (0.4203) | Acc_1: (84.46%) (23891/28288)\n",
      "Epoch: 46 | Batch_idx: 230 |  Loss_1: (0.4184) | Acc_1: (84.57%) (25005/29568)\n",
      "Epoch: 46 | Batch_idx: 240 |  Loss_1: (0.4195) | Acc_1: (84.54%) (26079/30848)\n",
      "Epoch: 46 | Batch_idx: 250 |  Loss_1: (0.4186) | Acc_1: (84.60%) (27180/32128)\n",
      "Epoch: 46 | Batch_idx: 260 |  Loss_1: (0.4168) | Acc_1: (84.65%) (28281/33408)\n",
      "Epoch: 46 | Batch_idx: 270 |  Loss_1: (0.4168) | Acc_1: (84.64%) (29361/34688)\n",
      "Epoch: 46 | Batch_idx: 280 |  Loss_1: (0.4175) | Acc_1: (84.61%) (30434/35968)\n",
      "Epoch: 46 | Batch_idx: 290 |  Loss_1: (0.4157) | Acc_1: (84.67%) (31539/37248)\n",
      "Epoch: 46 | Batch_idx: 300 |  Loss_1: (0.4150) | Acc_1: (84.70%) (32635/38528)\n",
      "Epoch: 46 | Batch_idx: 310 |  Loss_1: (0.4138) | Acc_1: (84.72%) (33727/39808)\n",
      "Epoch: 46 | Batch_idx: 320 |  Loss_1: (0.4150) | Acc_1: (84.67%) (34791/41088)\n",
      "Epoch: 46 | Batch_idx: 330 |  Loss_1: (0.4138) | Acc_1: (84.72%) (35896/42368)\n",
      "Epoch: 46 | Batch_idx: 340 |  Loss_1: (0.4137) | Acc_1: (84.71%) (36975/43648)\n",
      "Epoch: 46 | Batch_idx: 350 |  Loss_1: (0.4138) | Acc_1: (84.72%) (38063/44928)\n",
      "Epoch: 46 | Batch_idx: 360 |  Loss_1: (0.4132) | Acc_1: (84.73%) (39154/46208)\n",
      "Epoch: 46 | Batch_idx: 370 |  Loss_1: (0.4126) | Acc_1: (84.75%) (40244/47488)\n",
      "Epoch: 46 | Batch_idx: 380 |  Loss_1: (0.4109) | Acc_1: (84.82%) (41364/48768)\n",
      "Epoch: 46 | Batch_idx: 390 |  Loss_1: (0.4104) | Acc_1: (84.82%) (42411/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2944) | Acc: (89.88%) (8988/10000)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.4296) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.4132) | Acc_1: (84.52%) (1190/1408)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.3953) | Acc_1: (85.31%) (2293/2688)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.4000) | Acc_1: (85.28%) (3384/3968)\n",
      "Epoch: 47 | Batch_idx: 40 |  Loss_1: (0.3983) | Acc_1: (85.23%) (4473/5248)\n",
      "Epoch: 47 | Batch_idx: 50 |  Loss_1: (0.3974) | Acc_1: (85.22%) (5563/6528)\n",
      "Epoch: 47 | Batch_idx: 60 |  Loss_1: (0.3976) | Acc_1: (85.23%) (6655/7808)\n",
      "Epoch: 47 | Batch_idx: 70 |  Loss_1: (0.3981) | Acc_1: (85.24%) (7747/9088)\n",
      "Epoch: 47 | Batch_idx: 80 |  Loss_1: (0.3974) | Acc_1: (85.30%) (8844/10368)\n",
      "Epoch: 47 | Batch_idx: 90 |  Loss_1: (0.3937) | Acc_1: (85.42%) (9950/11648)\n",
      "Epoch: 47 | Batch_idx: 100 |  Loss_1: (0.3942) | Acc_1: (85.47%) (11050/12928)\n",
      "Epoch: 47 | Batch_idx: 110 |  Loss_1: (0.3896) | Acc_1: (85.66%) (12170/14208)\n",
      "Epoch: 47 | Batch_idx: 120 |  Loss_1: (0.3907) | Acc_1: (85.63%) (13262/15488)\n",
      "Epoch: 47 | Batch_idx: 130 |  Loss_1: (0.3938) | Acc_1: (85.49%) (14335/16768)\n",
      "Epoch: 47 | Batch_idx: 140 |  Loss_1: (0.3968) | Acc_1: (85.39%) (15411/18048)\n",
      "Epoch: 47 | Batch_idx: 150 |  Loss_1: (0.3971) | Acc_1: (85.36%) (16498/19328)\n",
      "Epoch: 47 | Batch_idx: 160 |  Loss_1: (0.3980) | Acc_1: (85.36%) (17592/20608)\n",
      "Epoch: 47 | Batch_idx: 170 |  Loss_1: (0.3969) | Acc_1: (85.35%) (18681/21888)\n",
      "Epoch: 47 | Batch_idx: 180 |  Loss_1: (0.3997) | Acc_1: (85.26%) (19754/23168)\n",
      "Epoch: 47 | Batch_idx: 190 |  Loss_1: (0.4002) | Acc_1: (85.27%) (20848/24448)\n",
      "Epoch: 47 | Batch_idx: 200 |  Loss_1: (0.4022) | Acc_1: (85.17%) (21913/25728)\n",
      "Epoch: 47 | Batch_idx: 210 |  Loss_1: (0.4016) | Acc_1: (85.20%) (23010/27008)\n",
      "Epoch: 47 | Batch_idx: 220 |  Loss_1: (0.4043) | Acc_1: (85.11%) (24075/28288)\n",
      "Epoch: 47 | Batch_idx: 230 |  Loss_1: (0.4052) | Acc_1: (85.01%) (25136/29568)\n",
      "Epoch: 47 | Batch_idx: 240 |  Loss_1: (0.4057) | Acc_1: (84.99%) (26218/30848)\n",
      "Epoch: 47 | Batch_idx: 250 |  Loss_1: (0.4058) | Acc_1: (84.98%) (27303/32128)\n",
      "Epoch: 47 | Batch_idx: 260 |  Loss_1: (0.4051) | Acc_1: (84.98%) (28390/33408)\n",
      "Epoch: 47 | Batch_idx: 270 |  Loss_1: (0.4045) | Acc_1: (84.99%) (29482/34688)\n",
      "Epoch: 47 | Batch_idx: 280 |  Loss_1: (0.4046) | Acc_1: (85.01%) (30575/35968)\n",
      "Epoch: 47 | Batch_idx: 290 |  Loss_1: (0.4047) | Acc_1: (85.03%) (31671/37248)\n",
      "Epoch: 47 | Batch_idx: 300 |  Loss_1: (0.4043) | Acc_1: (85.03%) (32760/38528)\n",
      "Epoch: 47 | Batch_idx: 310 |  Loss_1: (0.4047) | Acc_1: (85.02%) (33843/39808)\n",
      "Epoch: 47 | Batch_idx: 320 |  Loss_1: (0.4042) | Acc_1: (85.02%) (34935/41088)\n",
      "Epoch: 47 | Batch_idx: 330 |  Loss_1: (0.4027) | Acc_1: (85.09%) (36049/42368)\n",
      "Epoch: 47 | Batch_idx: 340 |  Loss_1: (0.4041) | Acc_1: (85.04%) (37119/43648)\n",
      "Epoch: 47 | Batch_idx: 350 |  Loss_1: (0.4035) | Acc_1: (85.07%) (38222/44928)\n",
      "Epoch: 47 | Batch_idx: 360 |  Loss_1: (0.4029) | Acc_1: (85.11%) (39329/46208)\n",
      "Epoch: 47 | Batch_idx: 370 |  Loss_1: (0.4033) | Acc_1: (85.09%) (40408/47488)\n",
      "Epoch: 47 | Batch_idx: 380 |  Loss_1: (0.4044) | Acc_1: (85.06%) (41484/48768)\n",
      "Epoch: 47 | Batch_idx: 390 |  Loss_1: (0.4039) | Acc_1: (85.09%) (42547/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2909) | Acc: (90.23%) (9023/10000)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.4174) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.3862) | Acc_1: (85.87%) (1209/1408)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.3827) | Acc_1: (85.53%) (2299/2688)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.3882) | Acc_1: (85.46%) (3391/3968)\n",
      "Epoch: 48 | Batch_idx: 40 |  Loss_1: (0.3858) | Acc_1: (85.63%) (4494/5248)\n",
      "Epoch: 48 | Batch_idx: 50 |  Loss_1: (0.3808) | Acc_1: (85.66%) (5592/6528)\n",
      "Epoch: 48 | Batch_idx: 60 |  Loss_1: (0.3818) | Acc_1: (85.63%) (6686/7808)\n",
      "Epoch: 48 | Batch_idx: 70 |  Loss_1: (0.3837) | Acc_1: (85.64%) (7783/9088)\n",
      "Epoch: 48 | Batch_idx: 80 |  Loss_1: (0.3854) | Acc_1: (85.70%) (8885/10368)\n",
      "Epoch: 48 | Batch_idx: 90 |  Loss_1: (0.3898) | Acc_1: (85.52%) (9961/11648)\n",
      "Epoch: 48 | Batch_idx: 100 |  Loss_1: (0.3925) | Acc_1: (85.37%) (11037/12928)\n",
      "Epoch: 48 | Batch_idx: 110 |  Loss_1: (0.3925) | Acc_1: (85.37%) (12130/14208)\n",
      "Epoch: 48 | Batch_idx: 120 |  Loss_1: (0.3902) | Acc_1: (85.45%) (13234/15488)\n",
      "Epoch: 48 | Batch_idx: 130 |  Loss_1: (0.3905) | Acc_1: (85.40%) (14320/16768)\n",
      "Epoch: 48 | Batch_idx: 140 |  Loss_1: (0.3885) | Acc_1: (85.46%) (15423/18048)\n",
      "Epoch: 48 | Batch_idx: 150 |  Loss_1: (0.3900) | Acc_1: (85.44%) (16513/19328)\n",
      "Epoch: 48 | Batch_idx: 160 |  Loss_1: (0.3918) | Acc_1: (85.40%) (17599/20608)\n",
      "Epoch: 48 | Batch_idx: 170 |  Loss_1: (0.3916) | Acc_1: (85.41%) (18694/21888)\n",
      "Epoch: 48 | Batch_idx: 180 |  Loss_1: (0.3928) | Acc_1: (85.37%) (19778/23168)\n",
      "Epoch: 48 | Batch_idx: 190 |  Loss_1: (0.3933) | Acc_1: (85.37%) (20872/24448)\n",
      "Epoch: 48 | Batch_idx: 200 |  Loss_1: (0.3950) | Acc_1: (85.29%) (21943/25728)\n",
      "Epoch: 48 | Batch_idx: 210 |  Loss_1: (0.3973) | Acc_1: (85.22%) (23017/27008)\n",
      "Epoch: 48 | Batch_idx: 220 |  Loss_1: (0.3966) | Acc_1: (85.27%) (24120/28288)\n",
      "Epoch: 48 | Batch_idx: 230 |  Loss_1: (0.3980) | Acc_1: (85.23%) (25202/29568)\n",
      "Epoch: 48 | Batch_idx: 240 |  Loss_1: (0.3978) | Acc_1: (85.18%) (26276/30848)\n",
      "Epoch: 48 | Batch_idx: 250 |  Loss_1: (0.3969) | Acc_1: (85.19%) (27371/32128)\n",
      "Epoch: 48 | Batch_idx: 260 |  Loss_1: (0.3963) | Acc_1: (85.19%) (28461/33408)\n",
      "Epoch: 48 | Batch_idx: 270 |  Loss_1: (0.3960) | Acc_1: (85.21%) (29556/34688)\n",
      "Epoch: 48 | Batch_idx: 280 |  Loss_1: (0.3960) | Acc_1: (85.21%) (30647/35968)\n",
      "Epoch: 48 | Batch_idx: 290 |  Loss_1: (0.3948) | Acc_1: (85.22%) (31743/37248)\n",
      "Epoch: 48 | Batch_idx: 300 |  Loss_1: (0.3957) | Acc_1: (85.21%) (32829/38528)\n",
      "Epoch: 48 | Batch_idx: 310 |  Loss_1: (0.3959) | Acc_1: (85.21%) (33919/39808)\n",
      "Epoch: 48 | Batch_idx: 320 |  Loss_1: (0.3967) | Acc_1: (85.22%) (35015/41088)\n",
      "Epoch: 48 | Batch_idx: 330 |  Loss_1: (0.3962) | Acc_1: (85.24%) (36115/42368)\n",
      "Epoch: 48 | Batch_idx: 340 |  Loss_1: (0.3960) | Acc_1: (85.25%) (37211/43648)\n",
      "Epoch: 48 | Batch_idx: 350 |  Loss_1: (0.3964) | Acc_1: (85.24%) (38295/44928)\n",
      "Epoch: 48 | Batch_idx: 360 |  Loss_1: (0.3956) | Acc_1: (85.27%) (39400/46208)\n",
      "Epoch: 48 | Batch_idx: 370 |  Loss_1: (0.3962) | Acc_1: (85.24%) (40477/47488)\n",
      "Epoch: 48 | Batch_idx: 380 |  Loss_1: (0.3961) | Acc_1: (85.22%) (41559/48768)\n",
      "Epoch: 48 | Batch_idx: 390 |  Loss_1: (0.3957) | Acc_1: (85.23%) (42615/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2909) | Acc: (90.21%) (9021/10000)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.4694) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.4073) | Acc_1: (84.87%) (1195/1408)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.4040) | Acc_1: (85.19%) (2290/2688)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.3940) | Acc_1: (85.56%) (3395/3968)\n",
      "Epoch: 49 | Batch_idx: 40 |  Loss_1: (0.4010) | Acc_1: (85.37%) (4480/5248)\n",
      "Epoch: 49 | Batch_idx: 50 |  Loss_1: (0.3941) | Acc_1: (85.60%) (5588/6528)\n",
      "Epoch: 49 | Batch_idx: 60 |  Loss_1: (0.3907) | Acc_1: (85.72%) (6693/7808)\n",
      "Epoch: 49 | Batch_idx: 70 |  Loss_1: (0.3921) | Acc_1: (85.53%) (7773/9088)\n",
      "Epoch: 49 | Batch_idx: 80 |  Loss_1: (0.3906) | Acc_1: (85.57%) (8872/10368)\n",
      "Epoch: 49 | Batch_idx: 90 |  Loss_1: (0.3901) | Acc_1: (85.64%) (9975/11648)\n",
      "Epoch: 49 | Batch_idx: 100 |  Loss_1: (0.3897) | Acc_1: (85.65%) (11073/12928)\n",
      "Epoch: 49 | Batch_idx: 110 |  Loss_1: (0.3907) | Acc_1: (85.64%) (12168/14208)\n",
      "Epoch: 49 | Batch_idx: 120 |  Loss_1: (0.3927) | Acc_1: (85.58%) (13254/15488)\n",
      "Epoch: 49 | Batch_idx: 130 |  Loss_1: (0.3931) | Acc_1: (85.57%) (14349/16768)\n",
      "Epoch: 49 | Batch_idx: 140 |  Loss_1: (0.3924) | Acc_1: (85.55%) (15440/18048)\n",
      "Epoch: 49 | Batch_idx: 150 |  Loss_1: (0.3961) | Acc_1: (85.44%) (16513/19328)\n",
      "Epoch: 49 | Batch_idx: 160 |  Loss_1: (0.3967) | Acc_1: (85.41%) (17602/20608)\n",
      "Epoch: 49 | Batch_idx: 170 |  Loss_1: (0.3958) | Acc_1: (85.46%) (18706/21888)\n",
      "Epoch: 49 | Batch_idx: 180 |  Loss_1: (0.3955) | Acc_1: (85.43%) (19792/23168)\n",
      "Epoch: 49 | Batch_idx: 190 |  Loss_1: (0.3953) | Acc_1: (85.45%) (20890/24448)\n",
      "Epoch: 49 | Batch_idx: 200 |  Loss_1: (0.3956) | Acc_1: (85.37%) (21964/25728)\n",
      "Epoch: 49 | Batch_idx: 210 |  Loss_1: (0.3965) | Acc_1: (85.33%) (23045/27008)\n",
      "Epoch: 49 | Batch_idx: 220 |  Loss_1: (0.3951) | Acc_1: (85.39%) (24154/28288)\n",
      "Epoch: 49 | Batch_idx: 230 |  Loss_1: (0.3938) | Acc_1: (85.46%) (25268/29568)\n",
      "Epoch: 49 | Batch_idx: 240 |  Loss_1: (0.3944) | Acc_1: (85.45%) (26360/30848)\n",
      "Epoch: 49 | Batch_idx: 250 |  Loss_1: (0.3953) | Acc_1: (85.41%) (27439/32128)\n",
      "Epoch: 49 | Batch_idx: 260 |  Loss_1: (0.3944) | Acc_1: (85.45%) (28547/33408)\n",
      "Epoch: 49 | Batch_idx: 270 |  Loss_1: (0.3936) | Acc_1: (85.46%) (29643/34688)\n",
      "Epoch: 49 | Batch_idx: 280 |  Loss_1: (0.3947) | Acc_1: (85.44%) (30730/35968)\n",
      "Epoch: 49 | Batch_idx: 290 |  Loss_1: (0.3939) | Acc_1: (85.45%) (31829/37248)\n",
      "Epoch: 49 | Batch_idx: 300 |  Loss_1: (0.3938) | Acc_1: (85.45%) (32921/38528)\n",
      "Epoch: 49 | Batch_idx: 310 |  Loss_1: (0.3946) | Acc_1: (85.41%) (34000/39808)\n",
      "Epoch: 49 | Batch_idx: 320 |  Loss_1: (0.3945) | Acc_1: (85.41%) (35095/41088)\n",
      "Epoch: 49 | Batch_idx: 330 |  Loss_1: (0.3939) | Acc_1: (85.43%) (36197/42368)\n",
      "Epoch: 49 | Batch_idx: 340 |  Loss_1: (0.3942) | Acc_1: (85.41%) (37280/43648)\n",
      "Epoch: 49 | Batch_idx: 350 |  Loss_1: (0.3944) | Acc_1: (85.42%) (38379/44928)\n",
      "Epoch: 49 | Batch_idx: 360 |  Loss_1: (0.3949) | Acc_1: (85.40%) (39462/46208)\n",
      "Epoch: 49 | Batch_idx: 370 |  Loss_1: (0.3954) | Acc_1: (85.39%) (40549/47488)\n",
      "Epoch: 49 | Batch_idx: 380 |  Loss_1: (0.3959) | Acc_1: (85.37%) (41634/48768)\n",
      "Epoch: 49 | Batch_idx: 390 |  Loss_1: (0.3958) | Acc_1: (85.36%) (42679/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2952) | Acc: (90.37%) (9037/10000)\n",
      "Epoch: 50 | Batch_idx: 0 |  Loss_1: (0.3476) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 50 | Batch_idx: 10 |  Loss_1: (0.4515) | Acc_1: (83.59%) (1177/1408)\n",
      "Epoch: 50 | Batch_idx: 20 |  Loss_1: (0.5098) | Acc_1: (81.85%) (2200/2688)\n",
      "Epoch: 50 | Batch_idx: 30 |  Loss_1: (0.5406) | Acc_1: (80.82%) (3207/3968)\n",
      "Epoch: 50 | Batch_idx: 40 |  Loss_1: (0.5666) | Acc_1: (79.94%) (4195/5248)\n",
      "Epoch: 50 | Batch_idx: 50 |  Loss_1: (0.5925) | Acc_1: (78.86%) (5148/6528)\n",
      "Epoch: 50 | Batch_idx: 60 |  Loss_1: (0.6151) | Acc_1: (78.23%) (6108/7808)\n",
      "Epoch: 50 | Batch_idx: 70 |  Loss_1: (0.6155) | Acc_1: (78.07%) (7095/9088)\n",
      "Epoch: 50 | Batch_idx: 80 |  Loss_1: (0.6266) | Acc_1: (77.62%) (8048/10368)\n",
      "Epoch: 50 | Batch_idx: 90 |  Loss_1: (0.6319) | Acc_1: (77.52%) (9030/11648)\n",
      "Epoch: 50 | Batch_idx: 100 |  Loss_1: (0.6337) | Acc_1: (77.44%) (10012/12928)\n",
      "Epoch: 50 | Batch_idx: 110 |  Loss_1: (0.6387) | Acc_1: (77.29%) (10982/14208)\n",
      "Epoch: 50 | Batch_idx: 120 |  Loss_1: (0.6443) | Acc_1: (77.01%) (11928/15488)\n",
      "Epoch: 50 | Batch_idx: 130 |  Loss_1: (0.6439) | Acc_1: (77.03%) (12917/16768)\n",
      "Epoch: 50 | Batch_idx: 140 |  Loss_1: (0.6455) | Acc_1: (76.97%) (13892/18048)\n",
      "Epoch: 50 | Batch_idx: 150 |  Loss_1: (0.6466) | Acc_1: (76.94%) (14871/19328)\n",
      "Epoch: 50 | Batch_idx: 160 |  Loss_1: (0.6454) | Acc_1: (77.01%) (15871/20608)\n",
      "Epoch: 50 | Batch_idx: 170 |  Loss_1: (0.6425) | Acc_1: (77.13%) (16883/21888)\n",
      "Epoch: 50 | Batch_idx: 180 |  Loss_1: (0.6397) | Acc_1: (77.24%) (17895/23168)\n",
      "Epoch: 50 | Batch_idx: 190 |  Loss_1: (0.6395) | Acc_1: (77.23%) (18880/24448)\n",
      "Epoch: 50 | Batch_idx: 200 |  Loss_1: (0.6397) | Acc_1: (77.19%) (19859/25728)\n",
      "Epoch: 50 | Batch_idx: 210 |  Loss_1: (0.6385) | Acc_1: (77.25%) (20865/27008)\n",
      "Epoch: 50 | Batch_idx: 220 |  Loss_1: (0.6393) | Acc_1: (77.24%) (21849/28288)\n",
      "Epoch: 50 | Batch_idx: 230 |  Loss_1: (0.6418) | Acc_1: (77.09%) (22795/29568)\n",
      "Epoch: 50 | Batch_idx: 240 |  Loss_1: (0.6434) | Acc_1: (77.04%) (23764/30848)\n",
      "Epoch: 50 | Batch_idx: 250 |  Loss_1: (0.6459) | Acc_1: (76.95%) (24724/32128)\n",
      "Epoch: 50 | Batch_idx: 260 |  Loss_1: (0.6467) | Acc_1: (76.93%) (25701/33408)\n",
      "Epoch: 50 | Batch_idx: 270 |  Loss_1: (0.6458) | Acc_1: (76.94%) (26689/34688)\n",
      "Epoch: 50 | Batch_idx: 280 |  Loss_1: (0.6468) | Acc_1: (76.93%) (27669/35968)\n",
      "Epoch: 50 | Batch_idx: 290 |  Loss_1: (0.6441) | Acc_1: (77.02%) (28689/37248)\n",
      "Epoch: 50 | Batch_idx: 300 |  Loss_1: (0.6432) | Acc_1: (77.05%) (29686/38528)\n",
      "Epoch: 50 | Batch_idx: 310 |  Loss_1: (0.6431) | Acc_1: (77.05%) (30673/39808)\n",
      "Epoch: 50 | Batch_idx: 320 |  Loss_1: (0.6428) | Acc_1: (77.07%) (31668/41088)\n",
      "Epoch: 50 | Batch_idx: 330 |  Loss_1: (0.6416) | Acc_1: (77.11%) (32668/42368)\n",
      "Epoch: 50 | Batch_idx: 340 |  Loss_1: (0.6422) | Acc_1: (77.06%) (33634/43648)\n",
      "Epoch: 50 | Batch_idx: 350 |  Loss_1: (0.6427) | Acc_1: (77.05%) (34619/44928)\n",
      "Epoch: 50 | Batch_idx: 360 |  Loss_1: (0.6417) | Acc_1: (77.10%) (35626/46208)\n",
      "Epoch: 50 | Batch_idx: 370 |  Loss_1: (0.6413) | Acc_1: (77.14%) (36632/47488)\n",
      "Epoch: 50 | Batch_idx: 380 |  Loss_1: (0.6421) | Acc_1: (77.11%) (37604/48768)\n",
      "Epoch: 50 | Batch_idx: 390 |  Loss_1: (0.6424) | Acc_1: (77.10%) (38549/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4448) | Acc: (84.73%) (8473/10000)\n",
      "Epoch: 51 | Batch_idx: 0 |  Loss_1: (0.6134) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 51 | Batch_idx: 10 |  Loss_1: (0.5926) | Acc_1: (77.49%) (1091/1408)\n",
      "Epoch: 51 | Batch_idx: 20 |  Loss_1: (0.5949) | Acc_1: (77.90%) (2094/2688)\n",
      "Epoch: 51 | Batch_idx: 30 |  Loss_1: (0.5943) | Acc_1: (77.80%) (3087/3968)\n",
      "Epoch: 51 | Batch_idx: 40 |  Loss_1: (0.5925) | Acc_1: (78.22%) (4105/5248)\n",
      "Epoch: 51 | Batch_idx: 50 |  Loss_1: (0.5877) | Acc_1: (78.58%) (5130/6528)\n",
      "Epoch: 51 | Batch_idx: 60 |  Loss_1: (0.5832) | Acc_1: (78.75%) (6149/7808)\n",
      "Epoch: 51 | Batch_idx: 70 |  Loss_1: (0.5898) | Acc_1: (78.63%) (7146/9088)\n",
      "Epoch: 51 | Batch_idx: 80 |  Loss_1: (0.6031) | Acc_1: (78.24%) (8112/10368)\n",
      "Epoch: 51 | Batch_idx: 90 |  Loss_1: (0.6086) | Acc_1: (78.06%) (9092/11648)\n",
      "Epoch: 51 | Batch_idx: 100 |  Loss_1: (0.6095) | Acc_1: (78.11%) (10098/12928)\n",
      "Epoch: 51 | Batch_idx: 110 |  Loss_1: (0.6086) | Acc_1: (78.16%) (11105/14208)\n",
      "Epoch: 51 | Batch_idx: 120 |  Loss_1: (0.6088) | Acc_1: (78.12%) (12100/15488)\n",
      "Epoch: 51 | Batch_idx: 130 |  Loss_1: (0.6085) | Acc_1: (78.11%) (13098/16768)\n",
      "Epoch: 51 | Batch_idx: 140 |  Loss_1: (0.6099) | Acc_1: (78.12%) (14100/18048)\n",
      "Epoch: 51 | Batch_idx: 150 |  Loss_1: (0.6133) | Acc_1: (78.05%) (15085/19328)\n",
      "Epoch: 51 | Batch_idx: 160 |  Loss_1: (0.6160) | Acc_1: (77.93%) (16059/20608)\n",
      "Epoch: 51 | Batch_idx: 170 |  Loss_1: (0.6167) | Acc_1: (77.93%) (17057/21888)\n",
      "Epoch: 51 | Batch_idx: 180 |  Loss_1: (0.6174) | Acc_1: (77.88%) (18043/23168)\n",
      "Epoch: 51 | Batch_idx: 190 |  Loss_1: (0.6186) | Acc_1: (77.81%) (19024/24448)\n",
      "Epoch: 51 | Batch_idx: 200 |  Loss_1: (0.6176) | Acc_1: (77.86%) (20033/25728)\n",
      "Epoch: 51 | Batch_idx: 210 |  Loss_1: (0.6135) | Acc_1: (78.01%) (21070/27008)\n",
      "Epoch: 51 | Batch_idx: 220 |  Loss_1: (0.6131) | Acc_1: (77.99%) (22061/28288)\n",
      "Epoch: 51 | Batch_idx: 230 |  Loss_1: (0.6149) | Acc_1: (77.96%) (23052/29568)\n",
      "Epoch: 51 | Batch_idx: 240 |  Loss_1: (0.6173) | Acc_1: (77.89%) (24028/30848)\n",
      "Epoch: 51 | Batch_idx: 250 |  Loss_1: (0.6164) | Acc_1: (77.92%) (25033/32128)\n",
      "Epoch: 51 | Batch_idx: 260 |  Loss_1: (0.6153) | Acc_1: (77.98%) (26052/33408)\n",
      "Epoch: 51 | Batch_idx: 270 |  Loss_1: (0.6144) | Acc_1: (77.99%) (27054/34688)\n",
      "Epoch: 51 | Batch_idx: 280 |  Loss_1: (0.6137) | Acc_1: (78.04%) (28070/35968)\n",
      "Epoch: 51 | Batch_idx: 290 |  Loss_1: (0.6140) | Acc_1: (78.03%) (29063/37248)\n",
      "Epoch: 51 | Batch_idx: 300 |  Loss_1: (0.6128) | Acc_1: (78.05%) (30072/38528)\n",
      "Epoch: 51 | Batch_idx: 310 |  Loss_1: (0.6103) | Acc_1: (78.14%) (31104/39808)\n",
      "Epoch: 51 | Batch_idx: 320 |  Loss_1: (0.6097) | Acc_1: (78.16%) (32116/41088)\n",
      "Epoch: 51 | Batch_idx: 330 |  Loss_1: (0.6088) | Acc_1: (78.22%) (33141/42368)\n",
      "Epoch: 51 | Batch_idx: 340 |  Loss_1: (0.6089) | Acc_1: (78.24%) (34149/43648)\n",
      "Epoch: 51 | Batch_idx: 350 |  Loss_1: (0.6105) | Acc_1: (78.21%) (35139/44928)\n",
      "Epoch: 51 | Batch_idx: 360 |  Loss_1: (0.6107) | Acc_1: (78.24%) (36152/46208)\n",
      "Epoch: 51 | Batch_idx: 370 |  Loss_1: (0.6129) | Acc_1: (78.17%) (37121/47488)\n",
      "Epoch: 51 | Batch_idx: 380 |  Loss_1: (0.6214) | Acc_1: (77.95%) (38016/48768)\n",
      "Epoch: 51 | Batch_idx: 390 |  Loss_1: (0.6333) | Acc_1: (77.62%) (38810/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4369) | Acc: (54.36%) (5436/10000)\n",
      "Epoch: 52 | Batch_idx: 0 |  Loss_1: (0.7574) | Acc_1: (71.88%) (92/128)\n",
      "Epoch: 52 | Batch_idx: 10 |  Loss_1: (0.8753) | Acc_1: (67.83%) (955/1408)\n",
      "Epoch: 52 | Batch_idx: 20 |  Loss_1: (0.8391) | Acc_1: (69.83%) (1877/2688)\n",
      "Epoch: 52 | Batch_idx: 30 |  Loss_1: (0.8209) | Acc_1: (70.51%) (2798/3968)\n",
      "Epoch: 52 | Batch_idx: 40 |  Loss_1: (0.7907) | Acc_1: (71.67%) (3761/5248)\n",
      "Epoch: 52 | Batch_idx: 50 |  Loss_1: (0.7777) | Acc_1: (72.30%) (4720/6528)\n",
      "Epoch: 52 | Batch_idx: 60 |  Loss_1: (0.7708) | Acc_1: (72.67%) (5674/7808)\n",
      "Epoch: 52 | Batch_idx: 70 |  Loss_1: (0.7576) | Acc_1: (73.24%) (6656/9088)\n",
      "Epoch: 52 | Batch_idx: 80 |  Loss_1: (0.7502) | Acc_1: (73.59%) (7630/10368)\n",
      "Epoch: 52 | Batch_idx: 90 |  Loss_1: (0.7384) | Acc_1: (73.91%) (8609/11648)\n",
      "Epoch: 52 | Batch_idx: 100 |  Loss_1: (0.7323) | Acc_1: (74.28%) (9603/12928)\n",
      "Epoch: 52 | Batch_idx: 110 |  Loss_1: (0.7303) | Acc_1: (74.42%) (10574/14208)\n",
      "Epoch: 52 | Batch_idx: 120 |  Loss_1: (0.7402) | Acc_1: (74.14%) (11483/15488)\n",
      "Epoch: 52 | Batch_idx: 130 |  Loss_1: (0.7503) | Acc_1: (73.68%) (12354/16768)\n",
      "Epoch: 52 | Batch_idx: 140 |  Loss_1: (0.7509) | Acc_1: (73.54%) (13273/18048)\n",
      "Epoch: 52 | Batch_idx: 150 |  Loss_1: (0.7493) | Acc_1: (73.56%) (14218/19328)\n",
      "Epoch: 52 | Batch_idx: 160 |  Loss_1: (0.7449) | Acc_1: (73.63%) (15173/20608)\n",
      "Epoch: 52 | Batch_idx: 170 |  Loss_1: (0.7368) | Acc_1: (73.92%) (16179/21888)\n",
      "Epoch: 52 | Batch_idx: 180 |  Loss_1: (0.7337) | Acc_1: (74.06%) (17159/23168)\n",
      "Epoch: 52 | Batch_idx: 190 |  Loss_1: (0.7271) | Acc_1: (74.35%) (18177/24448)\n",
      "Epoch: 52 | Batch_idx: 200 |  Loss_1: (0.7251) | Acc_1: (74.44%) (19151/25728)\n",
      "Epoch: 52 | Batch_idx: 210 |  Loss_1: (0.7189) | Acc_1: (74.64%) (20158/27008)\n",
      "Epoch: 52 | Batch_idx: 220 |  Loss_1: (0.7147) | Acc_1: (74.78%) (21155/28288)\n",
      "Epoch: 52 | Batch_idx: 230 |  Loss_1: (0.7111) | Acc_1: (74.91%) (22149/29568)\n",
      "Epoch: 52 | Batch_idx: 240 |  Loss_1: (0.7066) | Acc_1: (75.03%) (23146/30848)\n",
      "Epoch: 52 | Batch_idx: 250 |  Loss_1: (0.7043) | Acc_1: (75.14%) (24141/32128)\n",
      "Epoch: 52 | Batch_idx: 260 |  Loss_1: (0.7035) | Acc_1: (75.13%) (25101/33408)\n",
      "Epoch: 52 | Batch_idx: 270 |  Loss_1: (0.7015) | Acc_1: (75.22%) (26092/34688)\n",
      "Epoch: 52 | Batch_idx: 280 |  Loss_1: (0.6991) | Acc_1: (75.31%) (27089/35968)\n",
      "Epoch: 52 | Batch_idx: 290 |  Loss_1: (0.6971) | Acc_1: (75.34%) (28061/37248)\n",
      "Epoch: 52 | Batch_idx: 300 |  Loss_1: (0.6955) | Acc_1: (75.41%) (29054/38528)\n",
      "Epoch: 52 | Batch_idx: 310 |  Loss_1: (0.6928) | Acc_1: (75.49%) (30051/39808)\n",
      "Epoch: 52 | Batch_idx: 320 |  Loss_1: (0.6904) | Acc_1: (75.56%) (31047/41088)\n",
      "Epoch: 52 | Batch_idx: 330 |  Loss_1: (0.6881) | Acc_1: (75.66%) (32056/42368)\n",
      "Epoch: 52 | Batch_idx: 340 |  Loss_1: (0.6873) | Acc_1: (75.71%) (33047/43648)\n",
      "Epoch: 52 | Batch_idx: 350 |  Loss_1: (0.6842) | Acc_1: (75.81%) (34059/44928)\n",
      "Epoch: 52 | Batch_idx: 360 |  Loss_1: (0.6825) | Acc_1: (75.86%) (35053/46208)\n",
      "Epoch: 52 | Batch_idx: 370 |  Loss_1: (0.6806) | Acc_1: (75.92%) (36051/47488)\n",
      "Epoch: 52 | Batch_idx: 380 |  Loss_1: (0.6795) | Acc_1: (75.95%) (37039/48768)\n",
      "Epoch: 52 | Batch_idx: 390 |  Loss_1: (0.6773) | Acc_1: (76.02%) (38011/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4467) | Acc: (84.84%) (8484/10000)\n",
      "Epoch: 53 | Batch_idx: 0 |  Loss_1: (0.5922) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 53 | Batch_idx: 10 |  Loss_1: (0.5933) | Acc_1: (78.91%) (1111/1408)\n",
      "Epoch: 53 | Batch_idx: 20 |  Loss_1: (0.6065) | Acc_1: (78.31%) (2105/2688)\n",
      "Epoch: 53 | Batch_idx: 30 |  Loss_1: (0.6022) | Acc_1: (78.40%) (3111/3968)\n",
      "Epoch: 53 | Batch_idx: 40 |  Loss_1: (0.6098) | Acc_1: (78.20%) (4104/5248)\n",
      "Epoch: 53 | Batch_idx: 50 |  Loss_1: (0.6124) | Acc_1: (78.17%) (5103/6528)\n",
      "Epoch: 53 | Batch_idx: 60 |  Loss_1: (0.6101) | Acc_1: (78.32%) (6115/7808)\n",
      "Epoch: 53 | Batch_idx: 70 |  Loss_1: (0.6097) | Acc_1: (78.27%) (7113/9088)\n",
      "Epoch: 53 | Batch_idx: 80 |  Loss_1: (0.6070) | Acc_1: (78.33%) (8121/10368)\n",
      "Epoch: 53 | Batch_idx: 90 |  Loss_1: (0.6056) | Acc_1: (78.37%) (9129/11648)\n",
      "Epoch: 53 | Batch_idx: 100 |  Loss_1: (0.6007) | Acc_1: (78.60%) (10162/12928)\n",
      "Epoch: 53 | Batch_idx: 110 |  Loss_1: (0.5997) | Acc_1: (78.70%) (11181/14208)\n",
      "Epoch: 53 | Batch_idx: 120 |  Loss_1: (0.5993) | Acc_1: (78.71%) (12190/15488)\n",
      "Epoch: 53 | Batch_idx: 130 |  Loss_1: (0.5996) | Acc_1: (78.72%) (13199/16768)\n",
      "Epoch: 53 | Batch_idx: 140 |  Loss_1: (0.5999) | Acc_1: (78.86%) (14233/18048)\n",
      "Epoch: 53 | Batch_idx: 150 |  Loss_1: (0.6008) | Acc_1: (78.82%) (15235/19328)\n",
      "Epoch: 53 | Batch_idx: 160 |  Loss_1: (0.5976) | Acc_1: (78.93%) (16266/20608)\n",
      "Epoch: 53 | Batch_idx: 170 |  Loss_1: (0.5983) | Acc_1: (78.95%) (17280/21888)\n",
      "Epoch: 53 | Batch_idx: 180 |  Loss_1: (0.5992) | Acc_1: (78.92%) (18284/23168)\n",
      "Epoch: 53 | Batch_idx: 190 |  Loss_1: (0.6005) | Acc_1: (78.86%) (19279/24448)\n",
      "Epoch: 53 | Batch_idx: 200 |  Loss_1: (0.5997) | Acc_1: (78.86%) (20289/25728)\n",
      "Epoch: 53 | Batch_idx: 210 |  Loss_1: (0.5966) | Acc_1: (78.92%) (21314/27008)\n",
      "Epoch: 53 | Batch_idx: 220 |  Loss_1: (0.5938) | Acc_1: (79.04%) (22359/28288)\n",
      "Epoch: 53 | Batch_idx: 230 |  Loss_1: (0.5937) | Acc_1: (79.03%) (23368/29568)\n",
      "Epoch: 53 | Batch_idx: 240 |  Loss_1: (0.5919) | Acc_1: (79.09%) (24399/30848)\n",
      "Epoch: 53 | Batch_idx: 250 |  Loss_1: (0.5913) | Acc_1: (79.07%) (25405/32128)\n",
      "Epoch: 53 | Batch_idx: 260 |  Loss_1: (0.5920) | Acc_1: (79.03%) (26404/33408)\n",
      "Epoch: 53 | Batch_idx: 270 |  Loss_1: (0.5931) | Acc_1: (79.01%) (27407/34688)\n",
      "Epoch: 53 | Batch_idx: 280 |  Loss_1: (0.5926) | Acc_1: (79.08%) (28443/35968)\n",
      "Epoch: 53 | Batch_idx: 290 |  Loss_1: (0.5941) | Acc_1: (79.03%) (29437/37248)\n",
      "Epoch: 53 | Batch_idx: 300 |  Loss_1: (0.5939) | Acc_1: (79.02%) (30446/38528)\n",
      "Epoch: 53 | Batch_idx: 310 |  Loss_1: (0.5926) | Acc_1: (79.05%) (31470/39808)\n",
      "Epoch: 53 | Batch_idx: 320 |  Loss_1: (0.5926) | Acc_1: (79.09%) (32495/41088)\n",
      "Epoch: 53 | Batch_idx: 330 |  Loss_1: (0.5919) | Acc_1: (79.10%) (33513/42368)\n",
      "Epoch: 53 | Batch_idx: 340 |  Loss_1: (0.5908) | Acc_1: (79.12%) (34534/43648)\n",
      "Epoch: 53 | Batch_idx: 350 |  Loss_1: (0.5898) | Acc_1: (79.15%) (35560/44928)\n",
      "Epoch: 53 | Batch_idx: 360 |  Loss_1: (0.5893) | Acc_1: (79.17%) (36582/46208)\n",
      "Epoch: 53 | Batch_idx: 370 |  Loss_1: (0.5906) | Acc_1: (79.15%) (37586/47488)\n",
      "Epoch: 53 | Batch_idx: 380 |  Loss_1: (0.5897) | Acc_1: (79.21%) (38628/48768)\n",
      "Epoch: 53 | Batch_idx: 390 |  Loss_1: (0.5895) | Acc_1: (79.20%) (39602/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4834) | Acc: (83.35%) (8335/10000)\n",
      "Epoch: 54 | Batch_idx: 0 |  Loss_1: (0.5139) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 54 | Batch_idx: 10 |  Loss_1: (0.5477) | Acc_1: (79.83%) (1124/1408)\n",
      "Epoch: 54 | Batch_idx: 20 |  Loss_1: (0.5443) | Acc_1: (80.02%) (2151/2688)\n",
      "Epoch: 54 | Batch_idx: 30 |  Loss_1: (0.5552) | Acc_1: (79.61%) (3159/3968)\n",
      "Epoch: 54 | Batch_idx: 40 |  Loss_1: (0.5540) | Acc_1: (79.78%) (4187/5248)\n",
      "Epoch: 54 | Batch_idx: 50 |  Loss_1: (0.5613) | Acc_1: (79.64%) (5199/6528)\n",
      "Epoch: 54 | Batch_idx: 60 |  Loss_1: (0.5666) | Acc_1: (79.53%) (6210/7808)\n",
      "Epoch: 54 | Batch_idx: 70 |  Loss_1: (0.5659) | Acc_1: (79.54%) (7229/9088)\n",
      "Epoch: 54 | Batch_idx: 80 |  Loss_1: (0.5683) | Acc_1: (79.60%) (8253/10368)\n",
      "Epoch: 54 | Batch_idx: 90 |  Loss_1: (0.5724) | Acc_1: (79.46%) (9255/11648)\n",
      "Epoch: 54 | Batch_idx: 100 |  Loss_1: (0.5775) | Acc_1: (79.43%) (10269/12928)\n",
      "Epoch: 54 | Batch_idx: 110 |  Loss_1: (0.5792) | Acc_1: (79.35%) (11274/14208)\n",
      "Epoch: 54 | Batch_idx: 120 |  Loss_1: (0.5791) | Acc_1: (79.33%) (12287/15488)\n",
      "Epoch: 54 | Batch_idx: 130 |  Loss_1: (0.5786) | Acc_1: (79.34%) (13303/16768)\n",
      "Epoch: 54 | Batch_idx: 140 |  Loss_1: (0.5798) | Acc_1: (79.34%) (14320/18048)\n",
      "Epoch: 54 | Batch_idx: 150 |  Loss_1: (0.5768) | Acc_1: (79.45%) (15356/19328)\n",
      "Epoch: 54 | Batch_idx: 160 |  Loss_1: (0.5762) | Acc_1: (79.51%) (16386/20608)\n",
      "Epoch: 54 | Batch_idx: 170 |  Loss_1: (0.5768) | Acc_1: (79.55%) (17411/21888)\n",
      "Epoch: 54 | Batch_idx: 180 |  Loss_1: (0.5770) | Acc_1: (79.56%) (18433/23168)\n",
      "Epoch: 54 | Batch_idx: 190 |  Loss_1: (0.5794) | Acc_1: (79.44%) (19422/24448)\n",
      "Epoch: 54 | Batch_idx: 200 |  Loss_1: (0.5793) | Acc_1: (79.45%) (20440/25728)\n",
      "Epoch: 54 | Batch_idx: 210 |  Loss_1: (0.5830) | Acc_1: (79.28%) (21412/27008)\n",
      "Epoch: 54 | Batch_idx: 220 |  Loss_1: (0.5836) | Acc_1: (79.20%) (22403/28288)\n",
      "Epoch: 54 | Batch_idx: 230 |  Loss_1: (0.5868) | Acc_1: (79.05%) (23374/29568)\n",
      "Epoch: 54 | Batch_idx: 240 |  Loss_1: (0.5861) | Acc_1: (79.07%) (24391/30848)\n",
      "Epoch: 54 | Batch_idx: 250 |  Loss_1: (0.5848) | Acc_1: (79.10%) (25414/32128)\n",
      "Epoch: 54 | Batch_idx: 260 |  Loss_1: (0.5841) | Acc_1: (79.17%) (26449/33408)\n",
      "Epoch: 54 | Batch_idx: 270 |  Loss_1: (0.5839) | Acc_1: (79.17%) (27462/34688)\n",
      "Epoch: 54 | Batch_idx: 280 |  Loss_1: (0.5844) | Acc_1: (79.15%) (28467/35968)\n",
      "Epoch: 54 | Batch_idx: 290 |  Loss_1: (0.5848) | Acc_1: (79.17%) (29488/37248)\n",
      "Epoch: 54 | Batch_idx: 300 |  Loss_1: (0.5850) | Acc_1: (79.16%) (30499/38528)\n",
      "Epoch: 54 | Batch_idx: 310 |  Loss_1: (0.5853) | Acc_1: (79.14%) (31504/39808)\n",
      "Epoch: 54 | Batch_idx: 320 |  Loss_1: (0.5866) | Acc_1: (79.07%) (32489/41088)\n",
      "Epoch: 54 | Batch_idx: 330 |  Loss_1: (0.5869) | Acc_1: (79.05%) (33490/42368)\n",
      "Epoch: 54 | Batch_idx: 340 |  Loss_1: (0.5870) | Acc_1: (79.04%) (34501/43648)\n",
      "Epoch: 54 | Batch_idx: 350 |  Loss_1: (0.5856) | Acc_1: (79.08%) (35530/44928)\n",
      "Epoch: 54 | Batch_idx: 360 |  Loss_1: (0.5850) | Acc_1: (79.11%) (36554/46208)\n",
      "Epoch: 54 | Batch_idx: 370 |  Loss_1: (0.5862) | Acc_1: (79.06%) (37546/47488)\n",
      "Epoch: 54 | Batch_idx: 380 |  Loss_1: (0.5859) | Acc_1: (79.06%) (38558/48768)\n",
      "Epoch: 54 | Batch_idx: 390 |  Loss_1: (0.5849) | Acc_1: (79.10%) (39551/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4002) | Acc: (86.88%) (8688/10000)\n",
      "Epoch: 55 | Batch_idx: 0 |  Loss_1: (0.5810) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 55 | Batch_idx: 10 |  Loss_1: (0.5692) | Acc_1: (80.33%) (1131/1408)\n",
      "Epoch: 55 | Batch_idx: 20 |  Loss_1: (0.5569) | Acc_1: (80.80%) (2172/2688)\n",
      "Epoch: 55 | Batch_idx: 30 |  Loss_1: (0.5451) | Acc_1: (80.70%) (3202/3968)\n",
      "Epoch: 55 | Batch_idx: 40 |  Loss_1: (0.5570) | Acc_1: (80.34%) (4216/5248)\n",
      "Epoch: 55 | Batch_idx: 50 |  Loss_1: (0.5597) | Acc_1: (80.33%) (5244/6528)\n",
      "Epoch: 55 | Batch_idx: 60 |  Loss_1: (0.5707) | Acc_1: (79.89%) (6238/7808)\n",
      "Epoch: 55 | Batch_idx: 70 |  Loss_1: (0.5746) | Acc_1: (79.72%) (7245/9088)\n",
      "Epoch: 55 | Batch_idx: 80 |  Loss_1: (0.5796) | Acc_1: (79.34%) (8226/10368)\n",
      "Epoch: 55 | Batch_idx: 90 |  Loss_1: (0.5770) | Acc_1: (79.40%) (9248/11648)\n",
      "Epoch: 55 | Batch_idx: 100 |  Loss_1: (0.5768) | Acc_1: (79.44%) (10270/12928)\n",
      "Epoch: 55 | Batch_idx: 110 |  Loss_1: (0.5752) | Acc_1: (79.59%) (11308/14208)\n",
      "Epoch: 55 | Batch_idx: 120 |  Loss_1: (0.5760) | Acc_1: (79.42%) (12301/15488)\n",
      "Epoch: 55 | Batch_idx: 130 |  Loss_1: (0.5760) | Acc_1: (79.45%) (13322/16768)\n",
      "Epoch: 55 | Batch_idx: 140 |  Loss_1: (0.5768) | Acc_1: (79.39%) (14328/18048)\n",
      "Epoch: 55 | Batch_idx: 150 |  Loss_1: (0.5756) | Acc_1: (79.51%) (15367/19328)\n",
      "Epoch: 55 | Batch_idx: 160 |  Loss_1: (0.5717) | Acc_1: (79.67%) (16419/20608)\n",
      "Epoch: 55 | Batch_idx: 170 |  Loss_1: (0.5701) | Acc_1: (79.66%) (17435/21888)\n",
      "Epoch: 55 | Batch_idx: 180 |  Loss_1: (0.5688) | Acc_1: (79.75%) (18476/23168)\n",
      "Epoch: 55 | Batch_idx: 190 |  Loss_1: (0.5716) | Acc_1: (79.71%) (19488/24448)\n",
      "Epoch: 55 | Batch_idx: 200 |  Loss_1: (0.5741) | Acc_1: (79.68%) (20501/25728)\n",
      "Epoch: 55 | Batch_idx: 210 |  Loss_1: (0.5766) | Acc_1: (79.62%) (21503/27008)\n",
      "Epoch: 55 | Batch_idx: 220 |  Loss_1: (0.5753) | Acc_1: (79.69%) (22542/28288)\n",
      "Epoch: 55 | Batch_idx: 230 |  Loss_1: (0.5774) | Acc_1: (79.60%) (23535/29568)\n",
      "Epoch: 55 | Batch_idx: 240 |  Loss_1: (0.5785) | Acc_1: (79.55%) (24540/30848)\n",
      "Epoch: 55 | Batch_idx: 250 |  Loss_1: (0.5802) | Acc_1: (79.47%) (25531/32128)\n",
      "Epoch: 55 | Batch_idx: 260 |  Loss_1: (0.5808) | Acc_1: (79.44%) (26539/33408)\n",
      "Epoch: 55 | Batch_idx: 270 |  Loss_1: (0.5824) | Acc_1: (79.38%) (27534/34688)\n",
      "Epoch: 55 | Batch_idx: 280 |  Loss_1: (0.5832) | Acc_1: (79.36%) (28543/35968)\n",
      "Epoch: 55 | Batch_idx: 290 |  Loss_1: (0.5831) | Acc_1: (79.37%) (29564/37248)\n",
      "Epoch: 55 | Batch_idx: 300 |  Loss_1: (0.5829) | Acc_1: (79.38%) (30585/38528)\n",
      "Epoch: 55 | Batch_idx: 310 |  Loss_1: (0.5854) | Acc_1: (79.31%) (31571/39808)\n",
      "Epoch: 55 | Batch_idx: 320 |  Loss_1: (0.5856) | Acc_1: (79.29%) (32580/41088)\n",
      "Epoch: 55 | Batch_idx: 330 |  Loss_1: (0.5858) | Acc_1: (79.31%) (33601/42368)\n",
      "Epoch: 55 | Batch_idx: 340 |  Loss_1: (0.5875) | Acc_1: (79.22%) (34580/43648)\n",
      "Epoch: 55 | Batch_idx: 350 |  Loss_1: (0.5884) | Acc_1: (79.18%) (35575/44928)\n",
      "Epoch: 55 | Batch_idx: 360 |  Loss_1: (0.5897) | Acc_1: (79.14%) (36568/46208)\n",
      "Epoch: 55 | Batch_idx: 370 |  Loss_1: (0.5889) | Acc_1: (79.15%) (37587/47488)\n",
      "Epoch: 55 | Batch_idx: 380 |  Loss_1: (0.5889) | Acc_1: (79.12%) (38587/48768)\n",
      "Epoch: 55 | Batch_idx: 390 |  Loss_1: (0.5898) | Acc_1: (79.11%) (39557/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3770) | Acc: (87.49%) (8749/10000)\n",
      "Epoch: 56 | Batch_idx: 0 |  Loss_1: (0.5051) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 56 | Batch_idx: 10 |  Loss_1: (0.5810) | Acc_1: (79.40%) (1118/1408)\n",
      "Epoch: 56 | Batch_idx: 20 |  Loss_1: (0.5425) | Acc_1: (80.92%) (2175/2688)\n",
      "Epoch: 56 | Batch_idx: 30 |  Loss_1: (0.5457) | Acc_1: (80.77%) (3205/3968)\n",
      "Epoch: 56 | Batch_idx: 40 |  Loss_1: (0.5558) | Acc_1: (80.35%) (4217/5248)\n",
      "Epoch: 56 | Batch_idx: 50 |  Loss_1: (0.5504) | Acc_1: (80.51%) (5256/6528)\n",
      "Epoch: 56 | Batch_idx: 60 |  Loss_1: (0.5616) | Acc_1: (79.99%) (6246/7808)\n",
      "Epoch: 56 | Batch_idx: 70 |  Loss_1: (0.5670) | Acc_1: (79.70%) (7243/9088)\n",
      "Epoch: 56 | Batch_idx: 80 |  Loss_1: (0.5664) | Acc_1: (79.68%) (8261/10368)\n",
      "Epoch: 56 | Batch_idx: 90 |  Loss_1: (0.5671) | Acc_1: (79.64%) (9277/11648)\n",
      "Epoch: 56 | Batch_idx: 100 |  Loss_1: (0.5691) | Acc_1: (79.59%) (10290/12928)\n",
      "Epoch: 56 | Batch_idx: 110 |  Loss_1: (0.5688) | Acc_1: (79.62%) (11313/14208)\n",
      "Epoch: 56 | Batch_idx: 120 |  Loss_1: (0.5663) | Acc_1: (79.71%) (12345/15488)\n",
      "Epoch: 56 | Batch_idx: 130 |  Loss_1: (0.5701) | Acc_1: (79.51%) (13332/16768)\n",
      "Epoch: 56 | Batch_idx: 140 |  Loss_1: (0.5692) | Acc_1: (79.57%) (14360/18048)\n",
      "Epoch: 56 | Batch_idx: 150 |  Loss_1: (0.5713) | Acc_1: (79.47%) (15360/19328)\n",
      "Epoch: 56 | Batch_idx: 160 |  Loss_1: (0.5696) | Acc_1: (79.55%) (16393/20608)\n",
      "Epoch: 56 | Batch_idx: 170 |  Loss_1: (0.5697) | Acc_1: (79.56%) (17414/21888)\n",
      "Epoch: 56 | Batch_idx: 180 |  Loss_1: (0.5712) | Acc_1: (79.54%) (18427/23168)\n",
      "Epoch: 56 | Batch_idx: 190 |  Loss_1: (0.5698) | Acc_1: (79.56%) (19452/24448)\n",
      "Epoch: 56 | Batch_idx: 200 |  Loss_1: (0.5663) | Acc_1: (79.70%) (20506/25728)\n",
      "Epoch: 56 | Batch_idx: 210 |  Loss_1: (0.5647) | Acc_1: (79.76%) (21541/27008)\n",
      "Epoch: 56 | Batch_idx: 220 |  Loss_1: (0.5644) | Acc_1: (79.81%) (22576/28288)\n",
      "Epoch: 56 | Batch_idx: 230 |  Loss_1: (0.5655) | Acc_1: (79.84%) (23606/29568)\n",
      "Epoch: 56 | Batch_idx: 240 |  Loss_1: (0.5690) | Acc_1: (79.70%) (24585/30848)\n",
      "Epoch: 56 | Batch_idx: 250 |  Loss_1: (0.5690) | Acc_1: (79.67%) (25597/32128)\n",
      "Epoch: 56 | Batch_idx: 260 |  Loss_1: (0.5691) | Acc_1: (79.64%) (26606/33408)\n",
      "Epoch: 56 | Batch_idx: 270 |  Loss_1: (0.5684) | Acc_1: (79.69%) (27644/34688)\n",
      "Epoch: 56 | Batch_idx: 280 |  Loss_1: (0.5678) | Acc_1: (79.69%) (28662/35968)\n",
      "Epoch: 56 | Batch_idx: 290 |  Loss_1: (0.5681) | Acc_1: (79.69%) (29682/37248)\n",
      "Epoch: 56 | Batch_idx: 300 |  Loss_1: (0.5693) | Acc_1: (79.65%) (30687/38528)\n",
      "Epoch: 56 | Batch_idx: 310 |  Loss_1: (0.5724) | Acc_1: (79.57%) (31676/39808)\n",
      "Epoch: 56 | Batch_idx: 320 |  Loss_1: (0.5729) | Acc_1: (79.51%) (32671/41088)\n",
      "Epoch: 56 | Batch_idx: 330 |  Loss_1: (0.5707) | Acc_1: (79.60%) (33723/42368)\n",
      "Epoch: 56 | Batch_idx: 340 |  Loss_1: (0.5718) | Acc_1: (79.54%) (34718/43648)\n",
      "Epoch: 56 | Batch_idx: 350 |  Loss_1: (0.5718) | Acc_1: (79.56%) (35744/44928)\n",
      "Epoch: 56 | Batch_idx: 360 |  Loss_1: (0.5710) | Acc_1: (79.59%) (36776/46208)\n",
      "Epoch: 56 | Batch_idx: 370 |  Loss_1: (0.5721) | Acc_1: (79.54%) (37770/47488)\n",
      "Epoch: 56 | Batch_idx: 380 |  Loss_1: (0.5736) | Acc_1: (79.48%) (38762/48768)\n",
      "Epoch: 56 | Batch_idx: 390 |  Loss_1: (0.5745) | Acc_1: (79.46%) (39730/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3987) | Acc: (86.52%) (8652/10000)\n",
      "Epoch: 57 | Batch_idx: 0 |  Loss_1: (0.4327) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 57 | Batch_idx: 10 |  Loss_1: (0.5259) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 57 | Batch_idx: 20 |  Loss_1: (0.5356) | Acc_1: (81.06%) (2179/2688)\n",
      "Epoch: 57 | Batch_idx: 30 |  Loss_1: (0.5329) | Acc_1: (80.97%) (3213/3968)\n",
      "Epoch: 57 | Batch_idx: 40 |  Loss_1: (0.5433) | Acc_1: (80.56%) (4228/5248)\n",
      "Epoch: 57 | Batch_idx: 50 |  Loss_1: (0.5526) | Acc_1: (80.35%) (5245/6528)\n",
      "Epoch: 57 | Batch_idx: 60 |  Loss_1: (0.5522) | Acc_1: (80.39%) (6277/7808)\n",
      "Epoch: 57 | Batch_idx: 70 |  Loss_1: (0.5557) | Acc_1: (80.12%) (7281/9088)\n",
      "Epoch: 57 | Batch_idx: 80 |  Loss_1: (0.5568) | Acc_1: (80.09%) (8304/10368)\n",
      "Epoch: 57 | Batch_idx: 90 |  Loss_1: (0.5563) | Acc_1: (80.07%) (9326/11648)\n",
      "Epoch: 57 | Batch_idx: 100 |  Loss_1: (0.5576) | Acc_1: (80.04%) (10348/12928)\n",
      "Epoch: 57 | Batch_idx: 110 |  Loss_1: (0.5549) | Acc_1: (80.21%) (11396/14208)\n",
      "Epoch: 57 | Batch_idx: 120 |  Loss_1: (0.5573) | Acc_1: (80.08%) (12403/15488)\n",
      "Epoch: 57 | Batch_idx: 130 |  Loss_1: (0.5586) | Acc_1: (80.00%) (13415/16768)\n",
      "Epoch: 57 | Batch_idx: 140 |  Loss_1: (0.5618) | Acc_1: (79.89%) (14419/18048)\n",
      "Epoch: 57 | Batch_idx: 150 |  Loss_1: (0.5577) | Acc_1: (80.07%) (15476/19328)\n",
      "Epoch: 57 | Batch_idx: 160 |  Loss_1: (0.5604) | Acc_1: (79.97%) (16480/20608)\n",
      "Epoch: 57 | Batch_idx: 170 |  Loss_1: (0.5612) | Acc_1: (79.94%) (17498/21888)\n",
      "Epoch: 57 | Batch_idx: 180 |  Loss_1: (0.5639) | Acc_1: (79.88%) (18506/23168)\n",
      "Epoch: 57 | Batch_idx: 190 |  Loss_1: (0.5643) | Acc_1: (79.89%) (19531/24448)\n",
      "Epoch: 57 | Batch_idx: 200 |  Loss_1: (0.5643) | Acc_1: (79.87%) (20548/25728)\n",
      "Epoch: 57 | Batch_idx: 210 |  Loss_1: (0.5646) | Acc_1: (79.83%) (21561/27008)\n",
      "Epoch: 57 | Batch_idx: 220 |  Loss_1: (0.5633) | Acc_1: (79.89%) (22599/28288)\n",
      "Epoch: 57 | Batch_idx: 230 |  Loss_1: (0.5645) | Acc_1: (79.87%) (23615/29568)\n",
      "Epoch: 57 | Batch_idx: 240 |  Loss_1: (0.5631) | Acc_1: (79.91%) (24652/30848)\n",
      "Epoch: 57 | Batch_idx: 250 |  Loss_1: (0.5644) | Acc_1: (79.86%) (25658/32128)\n",
      "Epoch: 57 | Batch_idx: 260 |  Loss_1: (0.5646) | Acc_1: (79.85%) (26676/33408)\n",
      "Epoch: 57 | Batch_idx: 270 |  Loss_1: (0.5646) | Acc_1: (79.85%) (27698/34688)\n",
      "Epoch: 57 | Batch_idx: 280 |  Loss_1: (0.5638) | Acc_1: (79.91%) (28741/35968)\n",
      "Epoch: 57 | Batch_idx: 290 |  Loss_1: (0.5650) | Acc_1: (79.89%) (29757/37248)\n",
      "Epoch: 57 | Batch_idx: 300 |  Loss_1: (0.5658) | Acc_1: (79.87%) (30772/38528)\n",
      "Epoch: 57 | Batch_idx: 310 |  Loss_1: (0.5652) | Acc_1: (79.91%) (31812/39808)\n",
      "Epoch: 57 | Batch_idx: 320 |  Loss_1: (0.5649) | Acc_1: (79.90%) (32830/41088)\n",
      "Epoch: 57 | Batch_idx: 330 |  Loss_1: (0.5646) | Acc_1: (79.92%) (33860/42368)\n",
      "Epoch: 57 | Batch_idx: 340 |  Loss_1: (0.5643) | Acc_1: (79.92%) (34884/43648)\n",
      "Epoch: 57 | Batch_idx: 350 |  Loss_1: (0.5649) | Acc_1: (79.89%) (35892/44928)\n",
      "Epoch: 57 | Batch_idx: 360 |  Loss_1: (0.5650) | Acc_1: (79.87%) (36905/46208)\n",
      "Epoch: 57 | Batch_idx: 370 |  Loss_1: (0.5644) | Acc_1: (79.88%) (37933/47488)\n",
      "Epoch: 57 | Batch_idx: 380 |  Loss_1: (0.5641) | Acc_1: (79.87%) (38950/48768)\n",
      "Epoch: 57 | Batch_idx: 390 |  Loss_1: (0.5638) | Acc_1: (79.89%) (39943/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4543) | Acc: (84.94%) (8494/10000)\n",
      "Epoch: 58 | Batch_idx: 0 |  Loss_1: (0.6020) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 58 | Batch_idx: 10 |  Loss_1: (0.5684) | Acc_1: (79.83%) (1124/1408)\n",
      "Epoch: 58 | Batch_idx: 20 |  Loss_1: (0.5790) | Acc_1: (79.39%) (2134/2688)\n",
      "Epoch: 58 | Batch_idx: 30 |  Loss_1: (0.5827) | Acc_1: (79.06%) (3137/3968)\n",
      "Epoch: 58 | Batch_idx: 40 |  Loss_1: (0.5702) | Acc_1: (79.52%) (4173/5248)\n",
      "Epoch: 58 | Batch_idx: 50 |  Loss_1: (0.5588) | Acc_1: (79.83%) (5211/6528)\n",
      "Epoch: 58 | Batch_idx: 60 |  Loss_1: (0.5554) | Acc_1: (79.94%) (6242/7808)\n",
      "Epoch: 58 | Batch_idx: 70 |  Loss_1: (0.5547) | Acc_1: (80.12%) (7281/9088)\n",
      "Epoch: 58 | Batch_idx: 80 |  Loss_1: (0.5611) | Acc_1: (79.87%) (8281/10368)\n",
      "Epoch: 58 | Batch_idx: 90 |  Loss_1: (0.5641) | Acc_1: (79.80%) (9295/11648)\n",
      "Epoch: 58 | Batch_idx: 100 |  Loss_1: (0.5610) | Acc_1: (80.01%) (10344/12928)\n",
      "Epoch: 58 | Batch_idx: 110 |  Loss_1: (0.5602) | Acc_1: (79.95%) (11359/14208)\n",
      "Epoch: 58 | Batch_idx: 120 |  Loss_1: (0.5618) | Acc_1: (79.89%) (12373/15488)\n",
      "Epoch: 58 | Batch_idx: 130 |  Loss_1: (0.5655) | Acc_1: (79.82%) (13385/16768)\n",
      "Epoch: 58 | Batch_idx: 140 |  Loss_1: (0.5643) | Acc_1: (79.77%) (14396/18048)\n",
      "Epoch: 58 | Batch_idx: 150 |  Loss_1: (0.5677) | Acc_1: (79.64%) (15392/19328)\n",
      "Epoch: 58 | Batch_idx: 160 |  Loss_1: (0.5654) | Acc_1: (79.77%) (16439/20608)\n",
      "Epoch: 58 | Batch_idx: 170 |  Loss_1: (0.5662) | Acc_1: (79.71%) (17446/21888)\n",
      "Epoch: 58 | Batch_idx: 180 |  Loss_1: (0.5648) | Acc_1: (79.77%) (18481/23168)\n",
      "Epoch: 58 | Batch_idx: 190 |  Loss_1: (0.5630) | Acc_1: (79.87%) (19527/24448)\n",
      "Epoch: 58 | Batch_idx: 200 |  Loss_1: (0.5646) | Acc_1: (79.87%) (20550/25728)\n",
      "Epoch: 58 | Batch_idx: 210 |  Loss_1: (0.5650) | Acc_1: (79.91%) (21581/27008)\n",
      "Epoch: 58 | Batch_idx: 220 |  Loss_1: (0.5657) | Acc_1: (79.89%) (22599/28288)\n",
      "Epoch: 58 | Batch_idx: 230 |  Loss_1: (0.5644) | Acc_1: (79.92%) (23630/29568)\n",
      "Epoch: 58 | Batch_idx: 240 |  Loss_1: (0.5646) | Acc_1: (79.88%) (24641/30848)\n",
      "Epoch: 58 | Batch_idx: 250 |  Loss_1: (0.5641) | Acc_1: (79.91%) (25675/32128)\n",
      "Epoch: 58 | Batch_idx: 260 |  Loss_1: (0.5626) | Acc_1: (79.97%) (26717/33408)\n",
      "Epoch: 58 | Batch_idx: 270 |  Loss_1: (0.5629) | Acc_1: (80.00%) (27749/34688)\n",
      "Epoch: 58 | Batch_idx: 280 |  Loss_1: (0.5632) | Acc_1: (79.95%) (28755/35968)\n",
      "Epoch: 58 | Batch_idx: 290 |  Loss_1: (0.5626) | Acc_1: (80.00%) (29799/37248)\n",
      "Epoch: 58 | Batch_idx: 300 |  Loss_1: (0.5634) | Acc_1: (79.97%) (30810/38528)\n",
      "Epoch: 58 | Batch_idx: 310 |  Loss_1: (0.5627) | Acc_1: (79.99%) (31842/39808)\n",
      "Epoch: 58 | Batch_idx: 320 |  Loss_1: (0.5612) | Acc_1: (80.07%) (32900/41088)\n",
      "Epoch: 58 | Batch_idx: 330 |  Loss_1: (0.5625) | Acc_1: (80.05%) (33914/42368)\n",
      "Epoch: 58 | Batch_idx: 340 |  Loss_1: (0.5618) | Acc_1: (80.05%) (34941/43648)\n",
      "Epoch: 58 | Batch_idx: 350 |  Loss_1: (0.5624) | Acc_1: (80.06%) (35968/44928)\n",
      "Epoch: 58 | Batch_idx: 360 |  Loss_1: (0.5626) | Acc_1: (80.03%) (36982/46208)\n",
      "Epoch: 58 | Batch_idx: 370 |  Loss_1: (0.5630) | Acc_1: (80.02%) (38002/47488)\n",
      "Epoch: 58 | Batch_idx: 380 |  Loss_1: (0.5633) | Acc_1: (80.01%) (39019/48768)\n",
      "Epoch: 58 | Batch_idx: 390 |  Loss_1: (0.5632) | Acc_1: (79.98%) (39989/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4713) | Acc: (83.74%) (8374/10000)\n",
      "Epoch: 59 | Batch_idx: 0 |  Loss_1: (0.7678) | Acc_1: (75.00%) (96/128)\n",
      "Epoch: 59 | Batch_idx: 10 |  Loss_1: (0.5434) | Acc_1: (79.90%) (1125/1408)\n",
      "Epoch: 59 | Batch_idx: 20 |  Loss_1: (0.5528) | Acc_1: (80.17%) (2155/2688)\n",
      "Epoch: 59 | Batch_idx: 30 |  Loss_1: (0.5627) | Acc_1: (79.79%) (3166/3968)\n",
      "Epoch: 59 | Batch_idx: 40 |  Loss_1: (0.5597) | Acc_1: (79.97%) (4197/5248)\n",
      "Epoch: 59 | Batch_idx: 50 |  Loss_1: (0.5557) | Acc_1: (80.21%) (5236/6528)\n",
      "Epoch: 59 | Batch_idx: 60 |  Loss_1: (0.5556) | Acc_1: (80.11%) (6255/7808)\n",
      "Epoch: 59 | Batch_idx: 70 |  Loss_1: (0.5573) | Acc_1: (80.02%) (7272/9088)\n",
      "Epoch: 59 | Batch_idx: 80 |  Loss_1: (0.5595) | Acc_1: (80.03%) (8297/10368)\n",
      "Epoch: 59 | Batch_idx: 90 |  Loss_1: (0.5606) | Acc_1: (80.03%) (9322/11648)\n",
      "Epoch: 59 | Batch_idx: 100 |  Loss_1: (0.5570) | Acc_1: (80.14%) (10361/12928)\n",
      "Epoch: 59 | Batch_idx: 110 |  Loss_1: (0.5557) | Acc_1: (80.24%) (11400/14208)\n",
      "Epoch: 59 | Batch_idx: 120 |  Loss_1: (0.5578) | Acc_1: (80.14%) (12412/15488)\n",
      "Epoch: 59 | Batch_idx: 130 |  Loss_1: (0.5575) | Acc_1: (80.22%) (13451/16768)\n",
      "Epoch: 59 | Batch_idx: 140 |  Loss_1: (0.5577) | Acc_1: (80.20%) (14475/18048)\n",
      "Epoch: 59 | Batch_idx: 150 |  Loss_1: (0.5568) | Acc_1: (80.15%) (15491/19328)\n",
      "Epoch: 59 | Batch_idx: 160 |  Loss_1: (0.5577) | Acc_1: (80.12%) (16512/20608)\n",
      "Epoch: 59 | Batch_idx: 170 |  Loss_1: (0.5566) | Acc_1: (80.13%) (17539/21888)\n",
      "Epoch: 59 | Batch_idx: 180 |  Loss_1: (0.5582) | Acc_1: (80.07%) (18551/23168)\n",
      "Epoch: 59 | Batch_idx: 190 |  Loss_1: (0.5599) | Acc_1: (80.04%) (19567/24448)\n",
      "Epoch: 59 | Batch_idx: 200 |  Loss_1: (0.5598) | Acc_1: (80.03%) (20589/25728)\n",
      "Epoch: 59 | Batch_idx: 210 |  Loss_1: (0.5599) | Acc_1: (80.06%) (21622/27008)\n",
      "Epoch: 59 | Batch_idx: 220 |  Loss_1: (0.5620) | Acc_1: (80.00%) (22629/28288)\n",
      "Epoch: 59 | Batch_idx: 230 |  Loss_1: (0.5613) | Acc_1: (80.05%) (23668/29568)\n",
      "Epoch: 59 | Batch_idx: 240 |  Loss_1: (0.5611) | Acc_1: (80.03%) (24688/30848)\n",
      "Epoch: 59 | Batch_idx: 250 |  Loss_1: (0.5585) | Acc_1: (80.11%) (25739/32128)\n",
      "Epoch: 59 | Batch_idx: 260 |  Loss_1: (0.5589) | Acc_1: (80.09%) (26756/33408)\n",
      "Epoch: 59 | Batch_idx: 270 |  Loss_1: (0.5594) | Acc_1: (80.09%) (27781/34688)\n",
      "Epoch: 59 | Batch_idx: 280 |  Loss_1: (0.5592) | Acc_1: (80.10%) (28811/35968)\n",
      "Epoch: 59 | Batch_idx: 290 |  Loss_1: (0.5588) | Acc_1: (80.12%) (29843/37248)\n",
      "Epoch: 59 | Batch_idx: 300 |  Loss_1: (0.5588) | Acc_1: (80.12%) (30868/38528)\n",
      "Epoch: 59 | Batch_idx: 310 |  Loss_1: (0.5593) | Acc_1: (80.11%) (31892/39808)\n",
      "Epoch: 59 | Batch_idx: 320 |  Loss_1: (0.5578) | Acc_1: (80.18%) (32944/41088)\n",
      "Epoch: 59 | Batch_idx: 330 |  Loss_1: (0.5588) | Acc_1: (80.14%) (33955/42368)\n",
      "Epoch: 59 | Batch_idx: 340 |  Loss_1: (0.5577) | Acc_1: (80.18%) (34995/43648)\n",
      "Epoch: 59 | Batch_idx: 350 |  Loss_1: (0.5578) | Acc_1: (80.18%) (36024/44928)\n",
      "Epoch: 59 | Batch_idx: 360 |  Loss_1: (0.5567) | Acc_1: (80.23%) (37074/46208)\n",
      "Epoch: 59 | Batch_idx: 370 |  Loss_1: (0.5563) | Acc_1: (80.23%) (38099/47488)\n",
      "Epoch: 59 | Batch_idx: 380 |  Loss_1: (0.5549) | Acc_1: (80.26%) (39141/48768)\n",
      "Epoch: 59 | Batch_idx: 390 |  Loss_1: (0.5552) | Acc_1: (80.23%) (40116/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4976) | Acc: (83.41%) (8341/10000)\n",
      "Epoch: 60 | Batch_idx: 0 |  Loss_1: (0.6112) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 60 | Batch_idx: 10 |  Loss_1: (0.5421) | Acc_1: (80.75%) (1137/1408)\n",
      "Epoch: 60 | Batch_idx: 20 |  Loss_1: (0.5467) | Acc_1: (80.58%) (2166/2688)\n",
      "Epoch: 60 | Batch_idx: 30 |  Loss_1: (0.5466) | Acc_1: (80.62%) (3199/3968)\n",
      "Epoch: 60 | Batch_idx: 40 |  Loss_1: (0.5439) | Acc_1: (80.75%) (4238/5248)\n",
      "Epoch: 60 | Batch_idx: 50 |  Loss_1: (0.5435) | Acc_1: (80.73%) (5270/6528)\n",
      "Epoch: 60 | Batch_idx: 60 |  Loss_1: (0.5415) | Acc_1: (80.69%) (6300/7808)\n",
      "Epoch: 60 | Batch_idx: 70 |  Loss_1: (0.5434) | Acc_1: (80.64%) (7329/9088)\n",
      "Epoch: 60 | Batch_idx: 80 |  Loss_1: (0.5434) | Acc_1: (80.58%) (8355/10368)\n",
      "Epoch: 60 | Batch_idx: 90 |  Loss_1: (0.5381) | Acc_1: (80.71%) (9401/11648)\n",
      "Epoch: 60 | Batch_idx: 100 |  Loss_1: (0.5323) | Acc_1: (80.90%) (10459/12928)\n",
      "Epoch: 60 | Batch_idx: 110 |  Loss_1: (0.5349) | Acc_1: (80.85%) (11487/14208)\n",
      "Epoch: 60 | Batch_idx: 120 |  Loss_1: (0.5328) | Acc_1: (80.91%) (12532/15488)\n",
      "Epoch: 60 | Batch_idx: 130 |  Loss_1: (0.5354) | Acc_1: (80.84%) (13556/16768)\n",
      "Epoch: 60 | Batch_idx: 140 |  Loss_1: (0.5360) | Acc_1: (80.78%) (14579/18048)\n",
      "Epoch: 60 | Batch_idx: 150 |  Loss_1: (0.5365) | Acc_1: (80.71%) (15599/19328)\n",
      "Epoch: 60 | Batch_idx: 160 |  Loss_1: (0.5362) | Acc_1: (80.75%) (16641/20608)\n",
      "Epoch: 60 | Batch_idx: 170 |  Loss_1: (0.5347) | Acc_1: (80.83%) (17693/21888)\n",
      "Epoch: 60 | Batch_idx: 180 |  Loss_1: (0.5352) | Acc_1: (80.81%) (18723/23168)\n",
      "Epoch: 60 | Batch_idx: 190 |  Loss_1: (0.5372) | Acc_1: (80.76%) (19745/24448)\n",
      "Epoch: 60 | Batch_idx: 200 |  Loss_1: (0.5400) | Acc_1: (80.64%) (20746/25728)\n",
      "Epoch: 60 | Batch_idx: 210 |  Loss_1: (0.5421) | Acc_1: (80.54%) (21753/27008)\n",
      "Epoch: 60 | Batch_idx: 220 |  Loss_1: (0.5409) | Acc_1: (80.59%) (22797/28288)\n",
      "Epoch: 60 | Batch_idx: 230 |  Loss_1: (0.5389) | Acc_1: (80.61%) (23835/29568)\n",
      "Epoch: 60 | Batch_idx: 240 |  Loss_1: (0.5381) | Acc_1: (80.65%) (24879/30848)\n",
      "Epoch: 60 | Batch_idx: 250 |  Loss_1: (0.5378) | Acc_1: (80.62%) (25903/32128)\n",
      "Epoch: 60 | Batch_idx: 260 |  Loss_1: (0.5357) | Acc_1: (80.73%) (26969/33408)\n",
      "Epoch: 60 | Batch_idx: 270 |  Loss_1: (0.5368) | Acc_1: (80.65%) (27977/34688)\n",
      "Epoch: 60 | Batch_idx: 280 |  Loss_1: (0.5362) | Acc_1: (80.66%) (29013/35968)\n",
      "Epoch: 60 | Batch_idx: 290 |  Loss_1: (0.5369) | Acc_1: (80.63%) (30034/37248)\n",
      "Epoch: 60 | Batch_idx: 300 |  Loss_1: (0.5394) | Acc_1: (80.56%) (31039/38528)\n",
      "Epoch: 60 | Batch_idx: 310 |  Loss_1: (0.5411) | Acc_1: (80.52%) (32053/39808)\n",
      "Epoch: 60 | Batch_idx: 320 |  Loss_1: (0.5409) | Acc_1: (80.53%) (33088/41088)\n",
      "Epoch: 60 | Batch_idx: 330 |  Loss_1: (0.5417) | Acc_1: (80.50%) (34105/42368)\n",
      "Epoch: 60 | Batch_idx: 340 |  Loss_1: (0.5413) | Acc_1: (80.55%) (35157/43648)\n",
      "Epoch: 60 | Batch_idx: 350 |  Loss_1: (0.5418) | Acc_1: (80.52%) (36177/44928)\n",
      "Epoch: 60 | Batch_idx: 360 |  Loss_1: (0.5417) | Acc_1: (80.54%) (37214/46208)\n",
      "Epoch: 60 | Batch_idx: 370 |  Loss_1: (0.5427) | Acc_1: (80.51%) (38233/47488)\n",
      "Epoch: 60 | Batch_idx: 380 |  Loss_1: (0.5425) | Acc_1: (80.51%) (39264/48768)\n",
      "Epoch: 60 | Batch_idx: 390 |  Loss_1: (0.5428) | Acc_1: (80.49%) (40244/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6940) | Acc: (75.80%) (7580/10000)\n",
      "Epoch: 61 | Batch_idx: 0 |  Loss_1: (0.6277) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 61 | Batch_idx: 10 |  Loss_1: (0.6121) | Acc_1: (79.83%) (1124/1408)\n",
      "Epoch: 61 | Batch_idx: 20 |  Loss_1: (0.5848) | Acc_1: (80.28%) (2158/2688)\n",
      "Epoch: 61 | Batch_idx: 30 |  Loss_1: (0.5716) | Acc_1: (80.22%) (3183/3968)\n",
      "Epoch: 61 | Batch_idx: 40 |  Loss_1: (0.5682) | Acc_1: (80.18%) (4208/5248)\n",
      "Epoch: 61 | Batch_idx: 50 |  Loss_1: (0.5602) | Acc_1: (80.32%) (5243/6528)\n",
      "Epoch: 61 | Batch_idx: 60 |  Loss_1: (0.5593) | Acc_1: (80.30%) (6270/7808)\n",
      "Epoch: 61 | Batch_idx: 70 |  Loss_1: (0.5555) | Acc_1: (80.45%) (7311/9088)\n",
      "Epoch: 61 | Batch_idx: 80 |  Loss_1: (0.5495) | Acc_1: (80.55%) (8351/10368)\n",
      "Epoch: 61 | Batch_idx: 90 |  Loss_1: (0.5493) | Acc_1: (80.59%) (9387/11648)\n",
      "Epoch: 61 | Batch_idx: 100 |  Loss_1: (0.5500) | Acc_1: (80.60%) (10420/12928)\n",
      "Epoch: 61 | Batch_idx: 110 |  Loss_1: (0.5536) | Acc_1: (80.53%) (11442/14208)\n",
      "Epoch: 61 | Batch_idx: 120 |  Loss_1: (0.5513) | Acc_1: (80.58%) (12481/15488)\n",
      "Epoch: 61 | Batch_idx: 130 |  Loss_1: (0.5500) | Acc_1: (80.68%) (13529/16768)\n",
      "Epoch: 61 | Batch_idx: 140 |  Loss_1: (0.5514) | Acc_1: (80.65%) (14556/18048)\n",
      "Epoch: 61 | Batch_idx: 150 |  Loss_1: (0.5511) | Acc_1: (80.65%) (15588/19328)\n",
      "Epoch: 61 | Batch_idx: 160 |  Loss_1: (0.5523) | Acc_1: (80.59%) (16607/20608)\n",
      "Epoch: 61 | Batch_idx: 170 |  Loss_1: (0.5535) | Acc_1: (80.57%) (17636/21888)\n",
      "Epoch: 61 | Batch_idx: 180 |  Loss_1: (0.5527) | Acc_1: (80.56%) (18665/23168)\n",
      "Epoch: 61 | Batch_idx: 190 |  Loss_1: (0.5509) | Acc_1: (80.59%) (19702/24448)\n",
      "Epoch: 61 | Batch_idx: 200 |  Loss_1: (0.5517) | Acc_1: (80.53%) (20720/25728)\n",
      "Epoch: 61 | Batch_idx: 210 |  Loss_1: (0.5509) | Acc_1: (80.57%) (21760/27008)\n",
      "Epoch: 61 | Batch_idx: 220 |  Loss_1: (0.5501) | Acc_1: (80.61%) (22804/28288)\n",
      "Epoch: 61 | Batch_idx: 230 |  Loss_1: (0.5528) | Acc_1: (80.53%) (23811/29568)\n",
      "Epoch: 61 | Batch_idx: 240 |  Loss_1: (0.5535) | Acc_1: (80.50%) (24832/30848)\n",
      "Epoch: 61 | Batch_idx: 250 |  Loss_1: (0.5542) | Acc_1: (80.48%) (25858/32128)\n",
      "Epoch: 61 | Batch_idx: 260 |  Loss_1: (0.5548) | Acc_1: (80.43%) (26871/33408)\n",
      "Epoch: 61 | Batch_idx: 270 |  Loss_1: (0.5562) | Acc_1: (80.38%) (27882/34688)\n",
      "Epoch: 61 | Batch_idx: 280 |  Loss_1: (0.5553) | Acc_1: (80.39%) (28916/35968)\n",
      "Epoch: 61 | Batch_idx: 290 |  Loss_1: (0.5536) | Acc_1: (80.46%) (29970/37248)\n",
      "Epoch: 61 | Batch_idx: 300 |  Loss_1: (0.5530) | Acc_1: (80.46%) (31001/38528)\n",
      "Epoch: 61 | Batch_idx: 310 |  Loss_1: (0.5529) | Acc_1: (80.47%) (32034/39808)\n",
      "Epoch: 61 | Batch_idx: 320 |  Loss_1: (0.5506) | Acc_1: (80.53%) (33088/41088)\n",
      "Epoch: 61 | Batch_idx: 330 |  Loss_1: (0.5493) | Acc_1: (80.57%) (34137/42368)\n",
      "Epoch: 61 | Batch_idx: 340 |  Loss_1: (0.5507) | Acc_1: (80.50%) (35138/43648)\n",
      "Epoch: 61 | Batch_idx: 350 |  Loss_1: (0.5510) | Acc_1: (80.48%) (36159/44928)\n",
      "Epoch: 61 | Batch_idx: 360 |  Loss_1: (0.5507) | Acc_1: (80.48%) (37186/46208)\n",
      "Epoch: 61 | Batch_idx: 370 |  Loss_1: (0.5505) | Acc_1: (80.46%) (38210/47488)\n",
      "Epoch: 61 | Batch_idx: 380 |  Loss_1: (0.5510) | Acc_1: (80.45%) (39232/48768)\n",
      "Epoch: 61 | Batch_idx: 390 |  Loss_1: (0.5510) | Acc_1: (80.43%) (40216/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3835) | Acc: (87.40%) (8740/10000)\n",
      "Epoch: 62 | Batch_idx: 0 |  Loss_1: (0.5564) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 62 | Batch_idx: 10 |  Loss_1: (0.5429) | Acc_1: (80.61%) (1135/1408)\n",
      "Epoch: 62 | Batch_idx: 20 |  Loss_1: (0.5539) | Acc_1: (80.06%) (2152/2688)\n",
      "Epoch: 62 | Batch_idx: 30 |  Loss_1: (0.5558) | Acc_1: (79.86%) (3169/3968)\n",
      "Epoch: 62 | Batch_idx: 40 |  Loss_1: (0.5582) | Acc_1: (79.71%) (4183/5248)\n",
      "Epoch: 62 | Batch_idx: 50 |  Loss_1: (0.5485) | Acc_1: (80.13%) (5231/6528)\n",
      "Epoch: 62 | Batch_idx: 60 |  Loss_1: (0.5469) | Acc_1: (80.10%) (6254/7808)\n",
      "Epoch: 62 | Batch_idx: 70 |  Loss_1: (0.5446) | Acc_1: (80.24%) (7292/9088)\n",
      "Epoch: 62 | Batch_idx: 80 |  Loss_1: (0.5488) | Acc_1: (80.15%) (8310/10368)\n",
      "Epoch: 62 | Batch_idx: 90 |  Loss_1: (0.5536) | Acc_1: (79.91%) (9308/11648)\n",
      "Epoch: 62 | Batch_idx: 100 |  Loss_1: (0.5528) | Acc_1: (80.13%) (10359/12928)\n",
      "Epoch: 62 | Batch_idx: 110 |  Loss_1: (0.5479) | Acc_1: (80.29%) (11407/14208)\n",
      "Epoch: 62 | Batch_idx: 120 |  Loss_1: (0.5438) | Acc_1: (80.39%) (12451/15488)\n",
      "Epoch: 62 | Batch_idx: 130 |  Loss_1: (0.5455) | Acc_1: (80.42%) (13485/16768)\n",
      "Epoch: 62 | Batch_idx: 140 |  Loss_1: (0.5467) | Acc_1: (80.50%) (14528/18048)\n",
      "Epoch: 62 | Batch_idx: 150 |  Loss_1: (0.5470) | Acc_1: (80.54%) (15566/19328)\n",
      "Epoch: 62 | Batch_idx: 160 |  Loss_1: (0.5460) | Acc_1: (80.58%) (16605/20608)\n",
      "Epoch: 62 | Batch_idx: 170 |  Loss_1: (0.5475) | Acc_1: (80.53%) (17626/21888)\n",
      "Epoch: 62 | Batch_idx: 180 |  Loss_1: (0.5482) | Acc_1: (80.50%) (18651/23168)\n",
      "Epoch: 62 | Batch_idx: 190 |  Loss_1: (0.5474) | Acc_1: (80.54%) (19690/24448)\n",
      "Epoch: 62 | Batch_idx: 200 |  Loss_1: (0.5460) | Acc_1: (80.59%) (20733/25728)\n",
      "Epoch: 62 | Batch_idx: 210 |  Loss_1: (0.5433) | Acc_1: (80.68%) (21790/27008)\n",
      "Epoch: 62 | Batch_idx: 220 |  Loss_1: (0.5442) | Acc_1: (80.65%) (22813/28288)\n",
      "Epoch: 62 | Batch_idx: 230 |  Loss_1: (0.5442) | Acc_1: (80.64%) (23843/29568)\n",
      "Epoch: 62 | Batch_idx: 240 |  Loss_1: (0.5451) | Acc_1: (80.62%) (24869/30848)\n",
      "Epoch: 62 | Batch_idx: 250 |  Loss_1: (0.5483) | Acc_1: (80.49%) (25861/32128)\n",
      "Epoch: 62 | Batch_idx: 260 |  Loss_1: (0.5462) | Acc_1: (80.52%) (26900/33408)\n",
      "Epoch: 62 | Batch_idx: 270 |  Loss_1: (0.5455) | Acc_1: (80.55%) (27942/34688)\n",
      "Epoch: 62 | Batch_idx: 280 |  Loss_1: (0.5462) | Acc_1: (80.54%) (28969/35968)\n",
      "Epoch: 62 | Batch_idx: 290 |  Loss_1: (0.5444) | Acc_1: (80.62%) (30028/37248)\n",
      "Epoch: 62 | Batch_idx: 300 |  Loss_1: (0.5445) | Acc_1: (80.59%) (31050/38528)\n",
      "Epoch: 62 | Batch_idx: 310 |  Loss_1: (0.5432) | Acc_1: (80.61%) (32091/39808)\n",
      "Epoch: 62 | Batch_idx: 320 |  Loss_1: (0.5434) | Acc_1: (80.64%) (33134/41088)\n",
      "Epoch: 62 | Batch_idx: 330 |  Loss_1: (0.5419) | Acc_1: (80.65%) (34171/42368)\n",
      "Epoch: 62 | Batch_idx: 340 |  Loss_1: (0.5410) | Acc_1: (80.69%) (35219/43648)\n",
      "Epoch: 62 | Batch_idx: 350 |  Loss_1: (0.5407) | Acc_1: (80.66%) (36240/44928)\n",
      "Epoch: 62 | Batch_idx: 360 |  Loss_1: (0.5403) | Acc_1: (80.68%) (37281/46208)\n",
      "Epoch: 62 | Batch_idx: 370 |  Loss_1: (0.5395) | Acc_1: (80.72%) (38331/47488)\n",
      "Epoch: 62 | Batch_idx: 380 |  Loss_1: (0.5390) | Acc_1: (80.75%) (39382/48768)\n",
      "Epoch: 62 | Batch_idx: 390 |  Loss_1: (0.5383) | Acc_1: (80.76%) (40379/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3816) | Acc: (87.02%) (8702/10000)\n",
      "Epoch: 63 | Batch_idx: 0 |  Loss_1: (0.5223) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 63 | Batch_idx: 10 |  Loss_1: (0.5036) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 63 | Batch_idx: 20 |  Loss_1: (0.5087) | Acc_1: (82.22%) (2210/2688)\n",
      "Epoch: 63 | Batch_idx: 30 |  Loss_1: (0.5131) | Acc_1: (81.65%) (3240/3968)\n",
      "Epoch: 63 | Batch_idx: 40 |  Loss_1: (0.5124) | Acc_1: (81.63%) (4284/5248)\n",
      "Epoch: 63 | Batch_idx: 50 |  Loss_1: (0.5077) | Acc_1: (81.91%) (5347/6528)\n",
      "Epoch: 63 | Batch_idx: 60 |  Loss_1: (0.5090) | Acc_1: (81.83%) (6389/7808)\n",
      "Epoch: 63 | Batch_idx: 70 |  Loss_1: (0.5138) | Acc_1: (81.62%) (7418/9088)\n",
      "Epoch: 63 | Batch_idx: 80 |  Loss_1: (0.5131) | Acc_1: (81.64%) (8464/10368)\n",
      "Epoch: 63 | Batch_idx: 90 |  Loss_1: (0.5179) | Acc_1: (81.36%) (9477/11648)\n",
      "Epoch: 63 | Batch_idx: 100 |  Loss_1: (0.5204) | Acc_1: (81.24%) (10503/12928)\n",
      "Epoch: 63 | Batch_idx: 110 |  Loss_1: (0.5218) | Acc_1: (81.19%) (11536/14208)\n",
      "Epoch: 63 | Batch_idx: 120 |  Loss_1: (0.5209) | Acc_1: (81.25%) (12584/15488)\n",
      "Epoch: 63 | Batch_idx: 130 |  Loss_1: (0.5203) | Acc_1: (81.34%) (13639/16768)\n",
      "Epoch: 63 | Batch_idx: 140 |  Loss_1: (0.5208) | Acc_1: (81.32%) (14677/18048)\n",
      "Epoch: 63 | Batch_idx: 150 |  Loss_1: (0.5229) | Acc_1: (81.19%) (15693/19328)\n",
      "Epoch: 63 | Batch_idx: 160 |  Loss_1: (0.5226) | Acc_1: (81.25%) (16745/20608)\n",
      "Epoch: 63 | Batch_idx: 170 |  Loss_1: (0.5214) | Acc_1: (81.35%) (17805/21888)\n",
      "Epoch: 63 | Batch_idx: 180 |  Loss_1: (0.5209) | Acc_1: (81.34%) (18846/23168)\n",
      "Epoch: 63 | Batch_idx: 190 |  Loss_1: (0.5248) | Acc_1: (81.21%) (19853/24448)\n",
      "Epoch: 63 | Batch_idx: 200 |  Loss_1: (0.5257) | Acc_1: (81.19%) (20889/25728)\n",
      "Epoch: 63 | Batch_idx: 210 |  Loss_1: (0.5260) | Acc_1: (81.18%) (21925/27008)\n",
      "Epoch: 63 | Batch_idx: 220 |  Loss_1: (0.5256) | Acc_1: (81.19%) (22967/28288)\n",
      "Epoch: 63 | Batch_idx: 230 |  Loss_1: (0.5229) | Acc_1: (81.27%) (24031/29568)\n",
      "Epoch: 63 | Batch_idx: 240 |  Loss_1: (0.5244) | Acc_1: (81.19%) (25046/30848)\n",
      "Epoch: 63 | Batch_idx: 250 |  Loss_1: (0.5232) | Acc_1: (81.29%) (26116/32128)\n",
      "Epoch: 63 | Batch_idx: 260 |  Loss_1: (0.5227) | Acc_1: (81.31%) (27165/33408)\n",
      "Epoch: 63 | Batch_idx: 270 |  Loss_1: (0.5231) | Acc_1: (81.31%) (28204/34688)\n",
      "Epoch: 63 | Batch_idx: 280 |  Loss_1: (0.5216) | Acc_1: (81.36%) (29262/35968)\n",
      "Epoch: 63 | Batch_idx: 290 |  Loss_1: (0.5241) | Acc_1: (81.29%) (30278/37248)\n",
      "Epoch: 63 | Batch_idx: 300 |  Loss_1: (0.5256) | Acc_1: (81.20%) (31284/38528)\n",
      "Epoch: 63 | Batch_idx: 310 |  Loss_1: (0.5269) | Acc_1: (81.14%) (32301/39808)\n",
      "Epoch: 63 | Batch_idx: 320 |  Loss_1: (0.5265) | Acc_1: (81.16%) (33349/41088)\n",
      "Epoch: 63 | Batch_idx: 330 |  Loss_1: (0.5259) | Acc_1: (81.18%) (34393/42368)\n",
      "Epoch: 63 | Batch_idx: 340 |  Loss_1: (0.5259) | Acc_1: (81.15%) (35420/43648)\n",
      "Epoch: 63 | Batch_idx: 350 |  Loss_1: (0.5259) | Acc_1: (81.16%) (36464/44928)\n",
      "Epoch: 63 | Batch_idx: 360 |  Loss_1: (0.5260) | Acc_1: (81.16%) (37501/46208)\n",
      "Epoch: 63 | Batch_idx: 370 |  Loss_1: (0.5259) | Acc_1: (81.16%) (38543/47488)\n",
      "Epoch: 63 | Batch_idx: 380 |  Loss_1: (0.5274) | Acc_1: (81.11%) (39557/48768)\n",
      "Epoch: 63 | Batch_idx: 390 |  Loss_1: (0.5276) | Acc_1: (81.10%) (40550/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3555) | Acc: (88.07%) (8807/10000)\n",
      "Epoch: 64 | Batch_idx: 0 |  Loss_1: (0.6468) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 64 | Batch_idx: 10 |  Loss_1: (0.5232) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 64 | Batch_idx: 20 |  Loss_1: (0.5273) | Acc_1: (81.44%) (2189/2688)\n",
      "Epoch: 64 | Batch_idx: 30 |  Loss_1: (0.5239) | Acc_1: (81.40%) (3230/3968)\n",
      "Epoch: 64 | Batch_idx: 40 |  Loss_1: (0.5237) | Acc_1: (81.21%) (4262/5248)\n",
      "Epoch: 64 | Batch_idx: 50 |  Loss_1: (0.5132) | Acc_1: (81.40%) (5314/6528)\n",
      "Epoch: 64 | Batch_idx: 60 |  Loss_1: (0.5195) | Acc_1: (81.07%) (6330/7808)\n",
      "Epoch: 64 | Batch_idx: 70 |  Loss_1: (0.5110) | Acc_1: (81.24%) (7383/9088)\n",
      "Epoch: 64 | Batch_idx: 80 |  Loss_1: (0.5105) | Acc_1: (81.23%) (8422/10368)\n",
      "Epoch: 64 | Batch_idx: 90 |  Loss_1: (0.5137) | Acc_1: (81.10%) (9446/11648)\n",
      "Epoch: 64 | Batch_idx: 100 |  Loss_1: (0.5151) | Acc_1: (81.10%) (10484/12928)\n",
      "Epoch: 64 | Batch_idx: 110 |  Loss_1: (0.5139) | Acc_1: (81.18%) (11534/14208)\n",
      "Epoch: 64 | Batch_idx: 120 |  Loss_1: (0.5119) | Acc_1: (81.22%) (12580/15488)\n",
      "Epoch: 64 | Batch_idx: 130 |  Loss_1: (0.5145) | Acc_1: (81.15%) (13608/16768)\n",
      "Epoch: 64 | Batch_idx: 140 |  Loss_1: (0.5185) | Acc_1: (81.02%) (14622/18048)\n",
      "Epoch: 64 | Batch_idx: 150 |  Loss_1: (0.5183) | Acc_1: (81.05%) (15665/19328)\n",
      "Epoch: 64 | Batch_idx: 160 |  Loss_1: (0.5192) | Acc_1: (81.03%) (16698/20608)\n",
      "Epoch: 64 | Batch_idx: 170 |  Loss_1: (0.5177) | Acc_1: (81.12%) (17755/21888)\n",
      "Epoch: 64 | Batch_idx: 180 |  Loss_1: (0.5152) | Acc_1: (81.16%) (18804/23168)\n",
      "Epoch: 64 | Batch_idx: 190 |  Loss_1: (0.5129) | Acc_1: (81.28%) (19871/24448)\n",
      "Epoch: 64 | Batch_idx: 200 |  Loss_1: (0.5118) | Acc_1: (81.34%) (20926/25728)\n",
      "Epoch: 64 | Batch_idx: 210 |  Loss_1: (0.5116) | Acc_1: (81.33%) (21965/27008)\n",
      "Epoch: 64 | Batch_idx: 220 |  Loss_1: (0.5126) | Acc_1: (81.35%) (23013/28288)\n",
      "Epoch: 64 | Batch_idx: 230 |  Loss_1: (0.5131) | Acc_1: (81.33%) (24047/29568)\n",
      "Epoch: 64 | Batch_idx: 240 |  Loss_1: (0.5130) | Acc_1: (81.36%) (25098/30848)\n",
      "Epoch: 64 | Batch_idx: 250 |  Loss_1: (0.5134) | Acc_1: (81.32%) (26128/32128)\n",
      "Epoch: 64 | Batch_idx: 260 |  Loss_1: (0.5126) | Acc_1: (81.36%) (27180/33408)\n",
      "Epoch: 64 | Batch_idx: 270 |  Loss_1: (0.5135) | Acc_1: (81.30%) (28201/34688)\n",
      "Epoch: 64 | Batch_idx: 280 |  Loss_1: (0.5135) | Acc_1: (81.33%) (29252/35968)\n",
      "Epoch: 64 | Batch_idx: 290 |  Loss_1: (0.5127) | Acc_1: (81.35%) (30302/37248)\n",
      "Epoch: 64 | Batch_idx: 300 |  Loss_1: (0.5124) | Acc_1: (81.44%) (31376/38528)\n",
      "Epoch: 64 | Batch_idx: 310 |  Loss_1: (0.5117) | Acc_1: (81.48%) (32435/39808)\n",
      "Epoch: 64 | Batch_idx: 320 |  Loss_1: (0.5117) | Acc_1: (81.49%) (33481/41088)\n",
      "Epoch: 64 | Batch_idx: 330 |  Loss_1: (0.5118) | Acc_1: (81.50%) (34529/42368)\n",
      "Epoch: 64 | Batch_idx: 340 |  Loss_1: (0.5137) | Acc_1: (81.44%) (35548/43648)\n",
      "Epoch: 64 | Batch_idx: 350 |  Loss_1: (0.5142) | Acc_1: (81.44%) (36589/44928)\n",
      "Epoch: 64 | Batch_idx: 360 |  Loss_1: (0.5152) | Acc_1: (81.42%) (37621/46208)\n",
      "Epoch: 64 | Batch_idx: 370 |  Loss_1: (0.5152) | Acc_1: (81.41%) (38661/47488)\n",
      "Epoch: 64 | Batch_idx: 380 |  Loss_1: (0.5153) | Acc_1: (81.41%) (39704/48768)\n",
      "Epoch: 64 | Batch_idx: 390 |  Loss_1: (0.5156) | Acc_1: (81.42%) (40710/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3844) | Acc: (86.58%) (8658/10000)\n",
      "Epoch: 65 | Batch_idx: 0 |  Loss_1: (0.6397) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 65 | Batch_idx: 10 |  Loss_1: (0.5039) | Acc_1: (80.89%) (1139/1408)\n",
      "Epoch: 65 | Batch_idx: 20 |  Loss_1: (0.4836) | Acc_1: (82.11%) (2207/2688)\n",
      "Epoch: 65 | Batch_idx: 30 |  Loss_1: (0.4922) | Acc_1: (81.85%) (3248/3968)\n",
      "Epoch: 65 | Batch_idx: 40 |  Loss_1: (0.4882) | Acc_1: (82.09%) (4308/5248)\n",
      "Epoch: 65 | Batch_idx: 50 |  Loss_1: (0.4996) | Acc_1: (81.77%) (5338/6528)\n",
      "Epoch: 65 | Batch_idx: 60 |  Loss_1: (0.5042) | Acc_1: (81.74%) (6382/7808)\n",
      "Epoch: 65 | Batch_idx: 70 |  Loss_1: (0.5026) | Acc_1: (81.86%) (7439/9088)\n",
      "Epoch: 65 | Batch_idx: 80 |  Loss_1: (0.5015) | Acc_1: (81.93%) (8495/10368)\n",
      "Epoch: 65 | Batch_idx: 90 |  Loss_1: (0.5035) | Acc_1: (81.78%) (9526/11648)\n",
      "Epoch: 65 | Batch_idx: 100 |  Loss_1: (0.5029) | Acc_1: (81.86%) (10583/12928)\n",
      "Epoch: 65 | Batch_idx: 110 |  Loss_1: (0.5019) | Acc_1: (81.86%) (11631/14208)\n",
      "Epoch: 65 | Batch_idx: 120 |  Loss_1: (0.5030) | Acc_1: (81.81%) (12671/15488)\n",
      "Epoch: 65 | Batch_idx: 130 |  Loss_1: (0.5019) | Acc_1: (81.92%) (13736/16768)\n",
      "Epoch: 65 | Batch_idx: 140 |  Loss_1: (0.5019) | Acc_1: (81.93%) (14786/18048)\n",
      "Epoch: 65 | Batch_idx: 150 |  Loss_1: (0.5016) | Acc_1: (81.94%) (15837/19328)\n",
      "Epoch: 65 | Batch_idx: 160 |  Loss_1: (0.5045) | Acc_1: (81.81%) (16859/20608)\n",
      "Epoch: 65 | Batch_idx: 170 |  Loss_1: (0.5052) | Acc_1: (81.82%) (17908/21888)\n",
      "Epoch: 65 | Batch_idx: 180 |  Loss_1: (0.5057) | Acc_1: (81.76%) (18941/23168)\n",
      "Epoch: 65 | Batch_idx: 190 |  Loss_1: (0.5079) | Acc_1: (81.69%) (19972/24448)\n",
      "Epoch: 65 | Batch_idx: 200 |  Loss_1: (0.5071) | Acc_1: (81.72%) (21026/25728)\n",
      "Epoch: 65 | Batch_idx: 210 |  Loss_1: (0.5076) | Acc_1: (81.71%) (22069/27008)\n",
      "Epoch: 65 | Batch_idx: 220 |  Loss_1: (0.5067) | Acc_1: (81.75%) (23126/28288)\n",
      "Epoch: 65 | Batch_idx: 230 |  Loss_1: (0.5033) | Acc_1: (81.91%) (24218/29568)\n",
      "Epoch: 65 | Batch_idx: 240 |  Loss_1: (0.5049) | Acc_1: (81.86%) (25252/30848)\n",
      "Epoch: 65 | Batch_idx: 250 |  Loss_1: (0.5049) | Acc_1: (81.87%) (26304/32128)\n",
      "Epoch: 65 | Batch_idx: 260 |  Loss_1: (0.5035) | Acc_1: (81.94%) (27373/33408)\n",
      "Epoch: 65 | Batch_idx: 270 |  Loss_1: (0.5053) | Acc_1: (81.91%) (28412/34688)\n",
      "Epoch: 65 | Batch_idx: 280 |  Loss_1: (0.5057) | Acc_1: (81.90%) (29459/35968)\n",
      "Epoch: 65 | Batch_idx: 290 |  Loss_1: (0.5066) | Acc_1: (81.85%) (30489/37248)\n",
      "Epoch: 65 | Batch_idx: 300 |  Loss_1: (0.5071) | Acc_1: (81.87%) (31541/38528)\n",
      "Epoch: 65 | Batch_idx: 310 |  Loss_1: (0.5072) | Acc_1: (81.86%) (32585/39808)\n",
      "Epoch: 65 | Batch_idx: 320 |  Loss_1: (0.5063) | Acc_1: (81.88%) (33644/41088)\n",
      "Epoch: 65 | Batch_idx: 330 |  Loss_1: (0.5071) | Acc_1: (81.88%) (34689/42368)\n",
      "Epoch: 65 | Batch_idx: 340 |  Loss_1: (0.5082) | Acc_1: (81.83%) (35718/43648)\n",
      "Epoch: 65 | Batch_idx: 350 |  Loss_1: (0.5090) | Acc_1: (81.81%) (36755/44928)\n",
      "Epoch: 65 | Batch_idx: 360 |  Loss_1: (0.5090) | Acc_1: (81.80%) (37799/46208)\n",
      "Epoch: 65 | Batch_idx: 370 |  Loss_1: (0.5081) | Acc_1: (81.81%) (38851/47488)\n",
      "Epoch: 65 | Batch_idx: 380 |  Loss_1: (0.5080) | Acc_1: (81.82%) (39902/48768)\n",
      "Epoch: 65 | Batch_idx: 390 |  Loss_1: (0.5079) | Acc_1: (81.85%) (40925/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3618) | Acc: (87.37%) (8737/10000)\n",
      "Epoch: 66 | Batch_idx: 0 |  Loss_1: (0.5479) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 66 | Batch_idx: 10 |  Loss_1: (0.4706) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 66 | Batch_idx: 20 |  Loss_1: (0.4874) | Acc_1: (81.62%) (2194/2688)\n",
      "Epoch: 66 | Batch_idx: 30 |  Loss_1: (0.5041) | Acc_1: (81.02%) (3215/3968)\n",
      "Epoch: 66 | Batch_idx: 40 |  Loss_1: (0.5043) | Acc_1: (81.48%) (4276/5248)\n",
      "Epoch: 66 | Batch_idx: 50 |  Loss_1: (0.4995) | Acc_1: (81.71%) (5334/6528)\n",
      "Epoch: 66 | Batch_idx: 60 |  Loss_1: (0.5041) | Acc_1: (81.60%) (6371/7808)\n",
      "Epoch: 66 | Batch_idx: 70 |  Loss_1: (0.5076) | Acc_1: (81.51%) (7408/9088)\n",
      "Epoch: 66 | Batch_idx: 80 |  Loss_1: (0.5097) | Acc_1: (81.40%) (8440/10368)\n",
      "Epoch: 66 | Batch_idx: 90 |  Loss_1: (0.5110) | Acc_1: (81.29%) (9469/11648)\n",
      "Epoch: 66 | Batch_idx: 100 |  Loss_1: (0.5111) | Acc_1: (81.38%) (10521/12928)\n",
      "Epoch: 66 | Batch_idx: 110 |  Loss_1: (0.5091) | Acc_1: (81.48%) (11577/14208)\n",
      "Epoch: 66 | Batch_idx: 120 |  Loss_1: (0.5128) | Acc_1: (81.28%) (12589/15488)\n",
      "Epoch: 66 | Batch_idx: 130 |  Loss_1: (0.5137) | Acc_1: (81.19%) (13614/16768)\n",
      "Epoch: 66 | Batch_idx: 140 |  Loss_1: (0.5126) | Acc_1: (81.26%) (14666/18048)\n",
      "Epoch: 66 | Batch_idx: 150 |  Loss_1: (0.5093) | Acc_1: (81.41%) (15735/19328)\n",
      "Epoch: 66 | Batch_idx: 160 |  Loss_1: (0.5086) | Acc_1: (81.42%) (16780/20608)\n",
      "Epoch: 66 | Batch_idx: 170 |  Loss_1: (0.5069) | Acc_1: (81.53%) (17845/21888)\n",
      "Epoch: 66 | Batch_idx: 180 |  Loss_1: (0.5075) | Acc_1: (81.55%) (18894/23168)\n",
      "Epoch: 66 | Batch_idx: 190 |  Loss_1: (0.5072) | Acc_1: (81.61%) (19952/24448)\n",
      "Epoch: 66 | Batch_idx: 200 |  Loss_1: (0.5058) | Acc_1: (81.65%) (21007/25728)\n",
      "Epoch: 66 | Batch_idx: 210 |  Loss_1: (0.5078) | Acc_1: (81.64%) (22050/27008)\n",
      "Epoch: 66 | Batch_idx: 220 |  Loss_1: (0.5063) | Acc_1: (81.68%) (23106/28288)\n",
      "Epoch: 66 | Batch_idx: 230 |  Loss_1: (0.5058) | Acc_1: (81.73%) (24166/29568)\n",
      "Epoch: 66 | Batch_idx: 240 |  Loss_1: (0.5063) | Acc_1: (81.74%) (25216/30848)\n",
      "Epoch: 66 | Batch_idx: 250 |  Loss_1: (0.5070) | Acc_1: (81.70%) (26249/32128)\n",
      "Epoch: 66 | Batch_idx: 260 |  Loss_1: (0.5083) | Acc_1: (81.65%) (27276/33408)\n",
      "Epoch: 66 | Batch_idx: 270 |  Loss_1: (0.5071) | Acc_1: (81.69%) (28336/34688)\n",
      "Epoch: 66 | Batch_idx: 280 |  Loss_1: (0.5076) | Acc_1: (81.69%) (29382/35968)\n",
      "Epoch: 66 | Batch_idx: 290 |  Loss_1: (0.5063) | Acc_1: (81.73%) (30443/37248)\n",
      "Epoch: 66 | Batch_idx: 300 |  Loss_1: (0.5064) | Acc_1: (81.76%) (31499/38528)\n",
      "Epoch: 66 | Batch_idx: 310 |  Loss_1: (0.5071) | Acc_1: (81.73%) (32537/39808)\n",
      "Epoch: 66 | Batch_idx: 320 |  Loss_1: (0.5078) | Acc_1: (81.71%) (33573/41088)\n",
      "Epoch: 66 | Batch_idx: 330 |  Loss_1: (0.5076) | Acc_1: (81.71%) (34620/42368)\n",
      "Epoch: 66 | Batch_idx: 340 |  Loss_1: (0.5084) | Acc_1: (81.67%) (35648/43648)\n",
      "Epoch: 66 | Batch_idx: 350 |  Loss_1: (0.5090) | Acc_1: (81.65%) (36682/44928)\n",
      "Epoch: 66 | Batch_idx: 360 |  Loss_1: (0.5088) | Acc_1: (81.66%) (37735/46208)\n",
      "Epoch: 66 | Batch_idx: 370 |  Loss_1: (0.5076) | Acc_1: (81.71%) (38801/47488)\n",
      "Epoch: 66 | Batch_idx: 380 |  Loss_1: (0.5072) | Acc_1: (81.72%) (39853/48768)\n",
      "Epoch: 66 | Batch_idx: 390 |  Loss_1: (0.5067) | Acc_1: (81.73%) (40864/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3365) | Acc: (88.61%) (8861/10000)\n",
      "Epoch: 67 | Batch_idx: 0 |  Loss_1: (0.5337) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 67 | Batch_idx: 10 |  Loss_1: (0.4949) | Acc_1: (82.17%) (1157/1408)\n",
      "Epoch: 67 | Batch_idx: 20 |  Loss_1: (0.4847) | Acc_1: (81.99%) (2204/2688)\n",
      "Epoch: 67 | Batch_idx: 30 |  Loss_1: (0.4939) | Acc_1: (81.65%) (3240/3968)\n",
      "Epoch: 67 | Batch_idx: 40 |  Loss_1: (0.4964) | Acc_1: (81.84%) (4295/5248)\n",
      "Epoch: 67 | Batch_idx: 50 |  Loss_1: (0.4862) | Acc_1: (82.20%) (5366/6528)\n",
      "Epoch: 67 | Batch_idx: 60 |  Loss_1: (0.4817) | Acc_1: (82.42%) (6435/7808)\n",
      "Epoch: 67 | Batch_idx: 70 |  Loss_1: (0.4860) | Acc_1: (82.31%) (7480/9088)\n",
      "Epoch: 67 | Batch_idx: 80 |  Loss_1: (0.4885) | Acc_1: (82.32%) (8535/10368)\n",
      "Epoch: 67 | Batch_idx: 90 |  Loss_1: (0.4854) | Acc_1: (82.42%) (9600/11648)\n",
      "Epoch: 67 | Batch_idx: 100 |  Loss_1: (0.4894) | Acc_1: (82.29%) (10638/12928)\n",
      "Epoch: 67 | Batch_idx: 110 |  Loss_1: (0.4889) | Acc_1: (82.32%) (11696/14208)\n",
      "Epoch: 67 | Batch_idx: 120 |  Loss_1: (0.4882) | Acc_1: (82.32%) (12750/15488)\n",
      "Epoch: 67 | Batch_idx: 130 |  Loss_1: (0.4887) | Acc_1: (82.29%) (13798/16768)\n",
      "Epoch: 67 | Batch_idx: 140 |  Loss_1: (0.4906) | Acc_1: (82.23%) (14840/18048)\n",
      "Epoch: 67 | Batch_idx: 150 |  Loss_1: (0.4895) | Acc_1: (82.28%) (15903/19328)\n",
      "Epoch: 67 | Batch_idx: 160 |  Loss_1: (0.4899) | Acc_1: (82.27%) (16954/20608)\n",
      "Epoch: 67 | Batch_idx: 170 |  Loss_1: (0.4931) | Acc_1: (82.19%) (17989/21888)\n",
      "Epoch: 67 | Batch_idx: 180 |  Loss_1: (0.4943) | Acc_1: (82.13%) (19029/23168)\n",
      "Epoch: 67 | Batch_idx: 190 |  Loss_1: (0.4948) | Acc_1: (82.11%) (20074/24448)\n",
      "Epoch: 67 | Batch_idx: 200 |  Loss_1: (0.4966) | Acc_1: (82.08%) (21118/25728)\n",
      "Epoch: 67 | Batch_idx: 210 |  Loss_1: (0.4959) | Acc_1: (82.11%) (22177/27008)\n",
      "Epoch: 67 | Batch_idx: 220 |  Loss_1: (0.4958) | Acc_1: (82.12%) (23231/28288)\n",
      "Epoch: 67 | Batch_idx: 230 |  Loss_1: (0.4947) | Acc_1: (82.16%) (24293/29568)\n",
      "Epoch: 67 | Batch_idx: 240 |  Loss_1: (0.4945) | Acc_1: (82.15%) (25342/30848)\n",
      "Epoch: 67 | Batch_idx: 250 |  Loss_1: (0.4943) | Acc_1: (82.18%) (26403/32128)\n",
      "Epoch: 67 | Batch_idx: 260 |  Loss_1: (0.4945) | Acc_1: (82.14%) (27440/33408)\n",
      "Epoch: 67 | Batch_idx: 270 |  Loss_1: (0.4956) | Acc_1: (82.06%) (28464/34688)\n",
      "Epoch: 67 | Batch_idx: 280 |  Loss_1: (0.4949) | Acc_1: (82.12%) (29536/35968)\n",
      "Epoch: 67 | Batch_idx: 290 |  Loss_1: (0.4963) | Acc_1: (82.07%) (30568/37248)\n",
      "Epoch: 67 | Batch_idx: 300 |  Loss_1: (0.4952) | Acc_1: (82.09%) (31628/38528)\n",
      "Epoch: 67 | Batch_idx: 310 |  Loss_1: (0.4966) | Acc_1: (82.07%) (32672/39808)\n",
      "Epoch: 67 | Batch_idx: 320 |  Loss_1: (0.4972) | Acc_1: (82.06%) (33715/41088)\n",
      "Epoch: 67 | Batch_idx: 330 |  Loss_1: (0.4958) | Acc_1: (82.11%) (34790/42368)\n",
      "Epoch: 67 | Batch_idx: 340 |  Loss_1: (0.4952) | Acc_1: (82.13%) (35846/43648)\n",
      "Epoch: 67 | Batch_idx: 350 |  Loss_1: (0.4937) | Acc_1: (82.16%) (36914/44928)\n",
      "Epoch: 67 | Batch_idx: 360 |  Loss_1: (0.4962) | Acc_1: (82.08%) (37928/46208)\n",
      "Epoch: 67 | Batch_idx: 370 |  Loss_1: (0.4952) | Acc_1: (82.13%) (39003/47488)\n",
      "Epoch: 67 | Batch_idx: 380 |  Loss_1: (0.4964) | Acc_1: (82.09%) (40036/48768)\n",
      "Epoch: 67 | Batch_idx: 390 |  Loss_1: (0.4978) | Acc_1: (82.06%) (41030/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3696) | Acc: (87.60%) (8760/10000)\n",
      "Epoch: 68 | Batch_idx: 0 |  Loss_1: (0.4344) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 68 | Batch_idx: 10 |  Loss_1: (0.5094) | Acc_1: (81.25%) (1144/1408)\n",
      "Epoch: 68 | Batch_idx: 20 |  Loss_1: (0.4899) | Acc_1: (81.96%) (2203/2688)\n",
      "Epoch: 68 | Batch_idx: 30 |  Loss_1: (0.4830) | Acc_1: (82.51%) (3274/3968)\n",
      "Epoch: 68 | Batch_idx: 40 |  Loss_1: (0.4743) | Acc_1: (82.87%) (4349/5248)\n",
      "Epoch: 68 | Batch_idx: 50 |  Loss_1: (0.4760) | Acc_1: (82.94%) (5414/6528)\n",
      "Epoch: 68 | Batch_idx: 60 |  Loss_1: (0.4745) | Acc_1: (82.85%) (6469/7808)\n",
      "Epoch: 68 | Batch_idx: 70 |  Loss_1: (0.4732) | Acc_1: (82.83%) (7528/9088)\n",
      "Epoch: 68 | Batch_idx: 80 |  Loss_1: (0.4725) | Acc_1: (83.00%) (8605/10368)\n",
      "Epoch: 68 | Batch_idx: 90 |  Loss_1: (0.4740) | Acc_1: (82.93%) (9660/11648)\n",
      "Epoch: 68 | Batch_idx: 100 |  Loss_1: (0.4770) | Acc_1: (82.74%) (10697/12928)\n",
      "Epoch: 68 | Batch_idx: 110 |  Loss_1: (0.4788) | Acc_1: (82.69%) (11748/14208)\n",
      "Epoch: 68 | Batch_idx: 120 |  Loss_1: (0.4775) | Acc_1: (82.79%) (12823/15488)\n",
      "Epoch: 68 | Batch_idx: 130 |  Loss_1: (0.4825) | Acc_1: (82.56%) (13844/16768)\n",
      "Epoch: 68 | Batch_idx: 140 |  Loss_1: (0.4822) | Acc_1: (82.60%) (14907/18048)\n",
      "Epoch: 68 | Batch_idx: 150 |  Loss_1: (0.4806) | Acc_1: (82.64%) (15973/19328)\n",
      "Epoch: 68 | Batch_idx: 160 |  Loss_1: (0.4813) | Acc_1: (82.56%) (17014/20608)\n",
      "Epoch: 68 | Batch_idx: 170 |  Loss_1: (0.4826) | Acc_1: (82.49%) (18055/21888)\n",
      "Epoch: 68 | Batch_idx: 180 |  Loss_1: (0.4809) | Acc_1: (82.51%) (19117/23168)\n",
      "Epoch: 68 | Batch_idx: 190 |  Loss_1: (0.4805) | Acc_1: (82.55%) (20182/24448)\n",
      "Epoch: 68 | Batch_idx: 200 |  Loss_1: (0.4802) | Acc_1: (82.53%) (21233/25728)\n",
      "Epoch: 68 | Batch_idx: 210 |  Loss_1: (0.4814) | Acc_1: (82.50%) (22281/27008)\n",
      "Epoch: 68 | Batch_idx: 220 |  Loss_1: (0.4831) | Acc_1: (82.44%) (23320/28288)\n",
      "Epoch: 68 | Batch_idx: 230 |  Loss_1: (0.4857) | Acc_1: (82.35%) (24348/29568)\n",
      "Epoch: 68 | Batch_idx: 240 |  Loss_1: (0.4865) | Acc_1: (82.35%) (25402/30848)\n",
      "Epoch: 68 | Batch_idx: 250 |  Loss_1: (0.4878) | Acc_1: (82.28%) (26435/32128)\n",
      "Epoch: 68 | Batch_idx: 260 |  Loss_1: (0.4877) | Acc_1: (82.29%) (27491/33408)\n",
      "Epoch: 68 | Batch_idx: 270 |  Loss_1: (0.4880) | Acc_1: (82.31%) (28550/34688)\n",
      "Epoch: 68 | Batch_idx: 280 |  Loss_1: (0.4862) | Acc_1: (82.39%) (29634/35968)\n",
      "Epoch: 68 | Batch_idx: 290 |  Loss_1: (0.4867) | Acc_1: (82.37%) (30683/37248)\n",
      "Epoch: 68 | Batch_idx: 300 |  Loss_1: (0.4874) | Acc_1: (82.37%) (31734/38528)\n",
      "Epoch: 68 | Batch_idx: 310 |  Loss_1: (0.4875) | Acc_1: (82.38%) (32793/39808)\n",
      "Epoch: 68 | Batch_idx: 320 |  Loss_1: (0.4886) | Acc_1: (82.36%) (33840/41088)\n",
      "Epoch: 68 | Batch_idx: 330 |  Loss_1: (0.4884) | Acc_1: (82.39%) (34906/42368)\n",
      "Epoch: 68 | Batch_idx: 340 |  Loss_1: (0.4878) | Acc_1: (82.41%) (35971/43648)\n",
      "Epoch: 68 | Batch_idx: 350 |  Loss_1: (0.4875) | Acc_1: (82.43%) (37032/44928)\n",
      "Epoch: 68 | Batch_idx: 360 |  Loss_1: (0.4877) | Acc_1: (82.41%) (38078/46208)\n",
      "Epoch: 68 | Batch_idx: 370 |  Loss_1: (0.4892) | Acc_1: (82.38%) (39121/47488)\n",
      "Epoch: 68 | Batch_idx: 380 |  Loss_1: (0.4896) | Acc_1: (82.36%) (40163/48768)\n",
      "Epoch: 68 | Batch_idx: 390 |  Loss_1: (0.4908) | Acc_1: (82.31%) (41153/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3528) | Acc: (87.84%) (8784/10000)\n",
      "Epoch: 69 | Batch_idx: 0 |  Loss_1: (0.4316) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 69 | Batch_idx: 10 |  Loss_1: (0.4301) | Acc_1: (84.52%) (1190/1408)\n",
      "Epoch: 69 | Batch_idx: 20 |  Loss_1: (0.4511) | Acc_1: (83.56%) (2246/2688)\n",
      "Epoch: 69 | Batch_idx: 30 |  Loss_1: (0.4723) | Acc_1: (82.84%) (3287/3968)\n",
      "Epoch: 69 | Batch_idx: 40 |  Loss_1: (0.4896) | Acc_1: (82.39%) (4324/5248)\n",
      "Epoch: 69 | Batch_idx: 50 |  Loss_1: (0.4863) | Acc_1: (82.34%) (5375/6528)\n",
      "Epoch: 69 | Batch_idx: 60 |  Loss_1: (0.4843) | Acc_1: (82.12%) (6412/7808)\n",
      "Epoch: 69 | Batch_idx: 70 |  Loss_1: (0.4810) | Acc_1: (82.21%) (7471/9088)\n",
      "Epoch: 69 | Batch_idx: 80 |  Loss_1: (0.4840) | Acc_1: (82.20%) (8522/10368)\n",
      "Epoch: 69 | Batch_idx: 90 |  Loss_1: (0.4846) | Acc_1: (82.19%) (9573/11648)\n",
      "Epoch: 69 | Batch_idx: 100 |  Loss_1: (0.4879) | Acc_1: (81.99%) (10600/12928)\n",
      "Epoch: 69 | Batch_idx: 110 |  Loss_1: (0.4910) | Acc_1: (81.87%) (11632/14208)\n",
      "Epoch: 69 | Batch_idx: 120 |  Loss_1: (0.4897) | Acc_1: (81.97%) (12695/15488)\n",
      "Epoch: 69 | Batch_idx: 130 |  Loss_1: (0.4859) | Acc_1: (82.17%) (13779/16768)\n",
      "Epoch: 69 | Batch_idx: 140 |  Loss_1: (0.4887) | Acc_1: (82.09%) (14815/18048)\n",
      "Epoch: 69 | Batch_idx: 150 |  Loss_1: (0.4923) | Acc_1: (81.96%) (15841/19328)\n",
      "Epoch: 69 | Batch_idx: 160 |  Loss_1: (0.4926) | Acc_1: (81.94%) (16886/20608)\n",
      "Epoch: 69 | Batch_idx: 170 |  Loss_1: (0.4919) | Acc_1: (81.95%) (17937/21888)\n",
      "Epoch: 69 | Batch_idx: 180 |  Loss_1: (0.4896) | Acc_1: (82.11%) (19024/23168)\n",
      "Epoch: 69 | Batch_idx: 190 |  Loss_1: (0.4940) | Acc_1: (81.98%) (20043/24448)\n",
      "Epoch: 69 | Batch_idx: 200 |  Loss_1: (0.4944) | Acc_1: (81.98%) (21092/25728)\n",
      "Epoch: 69 | Batch_idx: 210 |  Loss_1: (0.4967) | Acc_1: (81.92%) (22125/27008)\n",
      "Epoch: 69 | Batch_idx: 220 |  Loss_1: (0.4962) | Acc_1: (81.95%) (23183/28288)\n",
      "Epoch: 69 | Batch_idx: 230 |  Loss_1: (0.4950) | Acc_1: (81.98%) (24239/29568)\n",
      "Epoch: 69 | Batch_idx: 240 |  Loss_1: (0.4961) | Acc_1: (81.99%) (25292/30848)\n",
      "Epoch: 69 | Batch_idx: 250 |  Loss_1: (0.4957) | Acc_1: (82.01%) (26348/32128)\n",
      "Epoch: 69 | Batch_idx: 260 |  Loss_1: (0.4940) | Acc_1: (82.07%) (27418/33408)\n",
      "Epoch: 69 | Batch_idx: 270 |  Loss_1: (0.4923) | Acc_1: (82.17%) (28504/34688)\n",
      "Epoch: 69 | Batch_idx: 280 |  Loss_1: (0.4924) | Acc_1: (82.19%) (29562/35968)\n",
      "Epoch: 69 | Batch_idx: 290 |  Loss_1: (0.4919) | Acc_1: (82.24%) (30632/37248)\n",
      "Epoch: 69 | Batch_idx: 300 |  Loss_1: (0.4929) | Acc_1: (82.22%) (31679/38528)\n",
      "Epoch: 69 | Batch_idx: 310 |  Loss_1: (0.4942) | Acc_1: (82.18%) (32716/39808)\n",
      "Epoch: 69 | Batch_idx: 320 |  Loss_1: (0.4940) | Acc_1: (82.19%) (33772/41088)\n",
      "Epoch: 69 | Batch_idx: 330 |  Loss_1: (0.4953) | Acc_1: (82.16%) (34811/42368)\n",
      "Epoch: 69 | Batch_idx: 340 |  Loss_1: (0.4951) | Acc_1: (82.19%) (35874/43648)\n",
      "Epoch: 69 | Batch_idx: 350 |  Loss_1: (0.4952) | Acc_1: (82.19%) (36927/44928)\n",
      "Epoch: 69 | Batch_idx: 360 |  Loss_1: (0.4944) | Acc_1: (82.22%) (37994/46208)\n",
      "Epoch: 69 | Batch_idx: 370 |  Loss_1: (0.4937) | Acc_1: (82.23%) (39050/47488)\n",
      "Epoch: 69 | Batch_idx: 380 |  Loss_1: (0.4939) | Acc_1: (82.22%) (40095/48768)\n",
      "Epoch: 69 | Batch_idx: 390 |  Loss_1: (0.4941) | Acc_1: (82.23%) (41113/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3739) | Acc: (87.39%) (8739/10000)\n",
      "Epoch: 70 | Batch_idx: 0 |  Loss_1: (0.4832) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 70 | Batch_idx: 10 |  Loss_1: (0.4266) | Acc_1: (85.51%) (1204/1408)\n",
      "Epoch: 70 | Batch_idx: 20 |  Loss_1: (0.4439) | Acc_1: (84.60%) (2274/2688)\n",
      "Epoch: 70 | Batch_idx: 30 |  Loss_1: (0.4500) | Acc_1: (84.32%) (3346/3968)\n",
      "Epoch: 70 | Batch_idx: 40 |  Loss_1: (0.4594) | Acc_1: (83.82%) (4399/5248)\n",
      "Epoch: 70 | Batch_idx: 50 |  Loss_1: (0.4695) | Acc_1: (83.38%) (5443/6528)\n",
      "Epoch: 70 | Batch_idx: 60 |  Loss_1: (0.4721) | Acc_1: (83.24%) (6499/7808)\n",
      "Epoch: 70 | Batch_idx: 70 |  Loss_1: (0.4743) | Acc_1: (83.11%) (7553/9088)\n",
      "Epoch: 70 | Batch_idx: 80 |  Loss_1: (0.4763) | Acc_1: (83.13%) (8619/10368)\n",
      "Epoch: 70 | Batch_idx: 90 |  Loss_1: (0.4786) | Acc_1: (83.08%) (9677/11648)\n",
      "Epoch: 70 | Batch_idx: 100 |  Loss_1: (0.4793) | Acc_1: (83.00%) (10730/12928)\n",
      "Epoch: 70 | Batch_idx: 110 |  Loss_1: (0.4741) | Acc_1: (83.16%) (11815/14208)\n",
      "Epoch: 70 | Batch_idx: 120 |  Loss_1: (0.4745) | Acc_1: (83.04%) (12861/15488)\n",
      "Epoch: 70 | Batch_idx: 130 |  Loss_1: (0.4761) | Acc_1: (83.01%) (13919/16768)\n",
      "Epoch: 70 | Batch_idx: 140 |  Loss_1: (0.4769) | Acc_1: (82.92%) (14966/18048)\n",
      "Epoch: 70 | Batch_idx: 150 |  Loss_1: (0.4790) | Acc_1: (82.88%) (16020/19328)\n",
      "Epoch: 70 | Batch_idx: 160 |  Loss_1: (0.4804) | Acc_1: (82.91%) (17086/20608)\n",
      "Epoch: 70 | Batch_idx: 170 |  Loss_1: (0.4806) | Acc_1: (82.86%) (18137/21888)\n",
      "Epoch: 70 | Batch_idx: 180 |  Loss_1: (0.4783) | Acc_1: (82.89%) (19205/23168)\n",
      "Epoch: 70 | Batch_idx: 190 |  Loss_1: (0.4769) | Acc_1: (82.94%) (20276/24448)\n",
      "Epoch: 70 | Batch_idx: 200 |  Loss_1: (0.4749) | Acc_1: (83.00%) (21355/25728)\n",
      "Epoch: 70 | Batch_idx: 210 |  Loss_1: (0.4753) | Acc_1: (82.97%) (22409/27008)\n",
      "Epoch: 70 | Batch_idx: 220 |  Loss_1: (0.4758) | Acc_1: (82.91%) (23454/28288)\n",
      "Epoch: 70 | Batch_idx: 230 |  Loss_1: (0.4768) | Acc_1: (82.85%) (24496/29568)\n",
      "Epoch: 70 | Batch_idx: 240 |  Loss_1: (0.4779) | Acc_1: (82.85%) (25557/30848)\n",
      "Epoch: 70 | Batch_idx: 250 |  Loss_1: (0.4778) | Acc_1: (82.83%) (26612/32128)\n",
      "Epoch: 70 | Batch_idx: 260 |  Loss_1: (0.4769) | Acc_1: (82.84%) (27675/33408)\n",
      "Epoch: 70 | Batch_idx: 270 |  Loss_1: (0.4762) | Acc_1: (82.87%) (28745/34688)\n",
      "Epoch: 70 | Batch_idx: 280 |  Loss_1: (0.4772) | Acc_1: (82.82%) (29788/35968)\n",
      "Epoch: 70 | Batch_idx: 290 |  Loss_1: (0.4772) | Acc_1: (82.81%) (30846/37248)\n",
      "Epoch: 70 | Batch_idx: 300 |  Loss_1: (0.4761) | Acc_1: (82.88%) (31931/38528)\n",
      "Epoch: 70 | Batch_idx: 310 |  Loss_1: (0.4773) | Acc_1: (82.83%) (32971/39808)\n",
      "Epoch: 70 | Batch_idx: 320 |  Loss_1: (0.4779) | Acc_1: (82.81%) (34023/41088)\n",
      "Epoch: 70 | Batch_idx: 330 |  Loss_1: (0.4776) | Acc_1: (82.81%) (35083/42368)\n",
      "Epoch: 70 | Batch_idx: 340 |  Loss_1: (0.4785) | Acc_1: (82.78%) (36130/43648)\n",
      "Epoch: 70 | Batch_idx: 350 |  Loss_1: (0.4791) | Acc_1: (82.74%) (37175/44928)\n",
      "Epoch: 70 | Batch_idx: 360 |  Loss_1: (0.4798) | Acc_1: (82.72%) (38223/46208)\n",
      "Epoch: 70 | Batch_idx: 370 |  Loss_1: (0.4799) | Acc_1: (82.74%) (39291/47488)\n",
      "Epoch: 70 | Batch_idx: 380 |  Loss_1: (0.4800) | Acc_1: (82.73%) (40348/48768)\n",
      "Epoch: 70 | Batch_idx: 390 |  Loss_1: (0.4800) | Acc_1: (82.74%) (41370/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3062) | Acc: (89.64%) (8964/10000)\n",
      "Epoch: 71 | Batch_idx: 0 |  Loss_1: (0.4039) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 71 | Batch_idx: 10 |  Loss_1: (0.4418) | Acc_1: (84.45%) (1189/1408)\n",
      "Epoch: 71 | Batch_idx: 20 |  Loss_1: (0.4466) | Acc_1: (83.63%) (2248/2688)\n",
      "Epoch: 71 | Batch_idx: 30 |  Loss_1: (0.4399) | Acc_1: (83.95%) (3331/3968)\n",
      "Epoch: 71 | Batch_idx: 40 |  Loss_1: (0.4446) | Acc_1: (83.88%) (4402/5248)\n",
      "Epoch: 71 | Batch_idx: 50 |  Loss_1: (0.4504) | Acc_1: (83.69%) (5463/6528)\n",
      "Epoch: 71 | Batch_idx: 60 |  Loss_1: (0.4537) | Acc_1: (83.48%) (6518/7808)\n",
      "Epoch: 71 | Batch_idx: 70 |  Loss_1: (0.4533) | Acc_1: (83.52%) (7590/9088)\n",
      "Epoch: 71 | Batch_idx: 80 |  Loss_1: (0.4577) | Acc_1: (83.38%) (8645/10368)\n",
      "Epoch: 71 | Batch_idx: 90 |  Loss_1: (0.4597) | Acc_1: (83.32%) (9705/11648)\n",
      "Epoch: 71 | Batch_idx: 100 |  Loss_1: (0.4559) | Acc_1: (83.43%) (10786/12928)\n",
      "Epoch: 71 | Batch_idx: 110 |  Loss_1: (0.4609) | Acc_1: (83.29%) (11834/14208)\n",
      "Epoch: 71 | Batch_idx: 120 |  Loss_1: (0.4644) | Acc_1: (83.16%) (12880/15488)\n",
      "Epoch: 71 | Batch_idx: 130 |  Loss_1: (0.4699) | Acc_1: (82.99%) (13916/16768)\n",
      "Epoch: 71 | Batch_idx: 140 |  Loss_1: (0.4719) | Acc_1: (82.92%) (14966/18048)\n",
      "Epoch: 71 | Batch_idx: 150 |  Loss_1: (0.4718) | Acc_1: (82.91%) (16025/19328)\n",
      "Epoch: 71 | Batch_idx: 160 |  Loss_1: (0.4699) | Acc_1: (83.00%) (17104/20608)\n",
      "Epoch: 71 | Batch_idx: 170 |  Loss_1: (0.4709) | Acc_1: (82.95%) (18157/21888)\n",
      "Epoch: 71 | Batch_idx: 180 |  Loss_1: (0.4688) | Acc_1: (83.03%) (19236/23168)\n",
      "Epoch: 71 | Batch_idx: 190 |  Loss_1: (0.4674) | Acc_1: (83.14%) (20326/24448)\n",
      "Epoch: 71 | Batch_idx: 200 |  Loss_1: (0.4694) | Acc_1: (83.10%) (21381/25728)\n",
      "Epoch: 71 | Batch_idx: 210 |  Loss_1: (0.4686) | Acc_1: (83.11%) (22447/27008)\n",
      "Epoch: 71 | Batch_idx: 220 |  Loss_1: (0.4679) | Acc_1: (83.11%) (23511/28288)\n",
      "Epoch: 71 | Batch_idx: 230 |  Loss_1: (0.4689) | Acc_1: (83.07%) (24561/29568)\n",
      "Epoch: 71 | Batch_idx: 240 |  Loss_1: (0.4681) | Acc_1: (83.13%) (25643/30848)\n",
      "Epoch: 71 | Batch_idx: 250 |  Loss_1: (0.4698) | Acc_1: (83.11%) (26701/32128)\n",
      "Epoch: 71 | Batch_idx: 260 |  Loss_1: (0.4710) | Acc_1: (83.05%) (27745/33408)\n",
      "Epoch: 71 | Batch_idx: 270 |  Loss_1: (0.4698) | Acc_1: (83.03%) (28802/34688)\n",
      "Epoch: 71 | Batch_idx: 280 |  Loss_1: (0.4710) | Acc_1: (82.98%) (29848/35968)\n",
      "Epoch: 71 | Batch_idx: 290 |  Loss_1: (0.4726) | Acc_1: (82.95%) (30897/37248)\n",
      "Epoch: 71 | Batch_idx: 300 |  Loss_1: (0.4720) | Acc_1: (82.98%) (31969/38528)\n",
      "Epoch: 71 | Batch_idx: 310 |  Loss_1: (0.4701) | Acc_1: (83.03%) (33054/39808)\n",
      "Epoch: 71 | Batch_idx: 320 |  Loss_1: (0.4718) | Acc_1: (82.95%) (34081/41088)\n",
      "Epoch: 71 | Batch_idx: 330 |  Loss_1: (0.4727) | Acc_1: (82.94%) (35139/42368)\n",
      "Epoch: 71 | Batch_idx: 340 |  Loss_1: (0.4737) | Acc_1: (82.90%) (36183/43648)\n",
      "Epoch: 71 | Batch_idx: 350 |  Loss_1: (0.4732) | Acc_1: (82.91%) (37249/44928)\n",
      "Epoch: 71 | Batch_idx: 360 |  Loss_1: (0.4738) | Acc_1: (82.91%) (38311/46208)\n",
      "Epoch: 71 | Batch_idx: 370 |  Loss_1: (0.4740) | Acc_1: (82.89%) (39363/47488)\n",
      "Epoch: 71 | Batch_idx: 380 |  Loss_1: (0.4745) | Acc_1: (82.89%) (40424/48768)\n",
      "Epoch: 71 | Batch_idx: 390 |  Loss_1: (0.4747) | Acc_1: (82.90%) (41452/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3318) | Acc: (89.05%) (8905/10000)\n",
      "Epoch: 72 | Batch_idx: 0 |  Loss_1: (0.3403) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 72 | Batch_idx: 10 |  Loss_1: (0.4353) | Acc_1: (84.45%) (1189/1408)\n",
      "Epoch: 72 | Batch_idx: 20 |  Loss_1: (0.4392) | Acc_1: (84.41%) (2269/2688)\n",
      "Epoch: 72 | Batch_idx: 30 |  Loss_1: (0.4380) | Acc_1: (84.17%) (3340/3968)\n",
      "Epoch: 72 | Batch_idx: 40 |  Loss_1: (0.4341) | Acc_1: (84.15%) (4416/5248)\n",
      "Epoch: 72 | Batch_idx: 50 |  Loss_1: (0.4507) | Acc_1: (83.59%) (5457/6528)\n",
      "Epoch: 72 | Batch_idx: 60 |  Loss_1: (0.4547) | Acc_1: (83.45%) (6516/7808)\n",
      "Epoch: 72 | Batch_idx: 70 |  Loss_1: (0.4556) | Acc_1: (83.52%) (7590/9088)\n",
      "Epoch: 72 | Batch_idx: 80 |  Loss_1: (0.4532) | Acc_1: (83.62%) (8670/10368)\n",
      "Epoch: 72 | Batch_idx: 90 |  Loss_1: (0.4532) | Acc_1: (83.53%) (9729/11648)\n",
      "Epoch: 72 | Batch_idx: 100 |  Loss_1: (0.4541) | Acc_1: (83.56%) (10803/12928)\n",
      "Epoch: 72 | Batch_idx: 110 |  Loss_1: (0.4553) | Acc_1: (83.50%) (11864/14208)\n",
      "Epoch: 72 | Batch_idx: 120 |  Loss_1: (0.4550) | Acc_1: (83.54%) (12938/15488)\n",
      "Epoch: 72 | Batch_idx: 130 |  Loss_1: (0.4571) | Acc_1: (83.49%) (14000/16768)\n",
      "Epoch: 72 | Batch_idx: 140 |  Loss_1: (0.4566) | Acc_1: (83.52%) (15073/18048)\n",
      "Epoch: 72 | Batch_idx: 150 |  Loss_1: (0.4553) | Acc_1: (83.56%) (16151/19328)\n",
      "Epoch: 72 | Batch_idx: 160 |  Loss_1: (0.4577) | Acc_1: (83.48%) (17203/20608)\n",
      "Epoch: 72 | Batch_idx: 170 |  Loss_1: (0.4587) | Acc_1: (83.46%) (18268/21888)\n",
      "Epoch: 72 | Batch_idx: 180 |  Loss_1: (0.4554) | Acc_1: (83.61%) (19370/23168)\n",
      "Epoch: 72 | Batch_idx: 190 |  Loss_1: (0.4565) | Acc_1: (83.53%) (20422/24448)\n",
      "Epoch: 72 | Batch_idx: 200 |  Loss_1: (0.4594) | Acc_1: (83.45%) (21470/25728)\n",
      "Epoch: 72 | Batch_idx: 210 |  Loss_1: (0.4599) | Acc_1: (83.43%) (22534/27008)\n",
      "Epoch: 72 | Batch_idx: 220 |  Loss_1: (0.4634) | Acc_1: (83.32%) (23569/28288)\n",
      "Epoch: 72 | Batch_idx: 230 |  Loss_1: (0.4626) | Acc_1: (83.35%) (24645/29568)\n",
      "Epoch: 72 | Batch_idx: 240 |  Loss_1: (0.4619) | Acc_1: (83.36%) (25715/30848)\n",
      "Epoch: 72 | Batch_idx: 250 |  Loss_1: (0.4615) | Acc_1: (83.39%) (26790/32128)\n",
      "Epoch: 72 | Batch_idx: 260 |  Loss_1: (0.4638) | Acc_1: (83.33%) (27840/33408)\n",
      "Epoch: 72 | Batch_idx: 270 |  Loss_1: (0.4639) | Acc_1: (83.36%) (28915/34688)\n",
      "Epoch: 72 | Batch_idx: 280 |  Loss_1: (0.4633) | Acc_1: (83.38%) (29989/35968)\n",
      "Epoch: 72 | Batch_idx: 290 |  Loss_1: (0.4635) | Acc_1: (83.34%) (31041/37248)\n",
      "Epoch: 72 | Batch_idx: 300 |  Loss_1: (0.4637) | Acc_1: (83.32%) (32101/38528)\n",
      "Epoch: 72 | Batch_idx: 310 |  Loss_1: (0.4634) | Acc_1: (83.31%) (33163/39808)\n",
      "Epoch: 72 | Batch_idx: 320 |  Loss_1: (0.4634) | Acc_1: (83.31%) (34231/41088)\n",
      "Epoch: 72 | Batch_idx: 330 |  Loss_1: (0.4632) | Acc_1: (83.29%) (35287/42368)\n",
      "Epoch: 72 | Batch_idx: 340 |  Loss_1: (0.4638) | Acc_1: (83.28%) (36349/43648)\n",
      "Epoch: 72 | Batch_idx: 350 |  Loss_1: (0.4642) | Acc_1: (83.24%) (37398/44928)\n",
      "Epoch: 72 | Batch_idx: 360 |  Loss_1: (0.4643) | Acc_1: (83.25%) (38469/46208)\n",
      "Epoch: 72 | Batch_idx: 370 |  Loss_1: (0.4632) | Acc_1: (83.29%) (39555/47488)\n",
      "Epoch: 72 | Batch_idx: 380 |  Loss_1: (0.4631) | Acc_1: (83.30%) (40626/48768)\n",
      "Epoch: 72 | Batch_idx: 390 |  Loss_1: (0.4634) | Acc_1: (83.31%) (41654/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3119) | Acc: (89.50%) (8950/10000)\n",
      "Epoch: 73 | Batch_idx: 0 |  Loss_1: (0.4114) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 73 | Batch_idx: 10 |  Loss_1: (0.4505) | Acc_1: (83.52%) (1176/1408)\n",
      "Epoch: 73 | Batch_idx: 20 |  Loss_1: (0.4451) | Acc_1: (83.85%) (2254/2688)\n",
      "Epoch: 73 | Batch_idx: 30 |  Loss_1: (0.4426) | Acc_1: (84.00%) (3333/3968)\n",
      "Epoch: 73 | Batch_idx: 40 |  Loss_1: (0.4387) | Acc_1: (83.82%) (4399/5248)\n",
      "Epoch: 73 | Batch_idx: 50 |  Loss_1: (0.4467) | Acc_1: (83.72%) (5465/6528)\n",
      "Epoch: 73 | Batch_idx: 60 |  Loss_1: (0.4524) | Acc_1: (83.71%) (6536/7808)\n",
      "Epoch: 73 | Batch_idx: 70 |  Loss_1: (0.4548) | Acc_1: (83.60%) (7598/9088)\n",
      "Epoch: 73 | Batch_idx: 80 |  Loss_1: (0.4540) | Acc_1: (83.56%) (8663/10368)\n",
      "Epoch: 73 | Batch_idx: 90 |  Loss_1: (0.4525) | Acc_1: (83.59%) (9737/11648)\n",
      "Epoch: 73 | Batch_idx: 100 |  Loss_1: (0.4546) | Acc_1: (83.49%) (10793/12928)\n",
      "Epoch: 73 | Batch_idx: 110 |  Loss_1: (0.4582) | Acc_1: (83.34%) (11841/14208)\n",
      "Epoch: 73 | Batch_idx: 120 |  Loss_1: (0.4592) | Acc_1: (83.34%) (12907/15488)\n",
      "Epoch: 73 | Batch_idx: 130 |  Loss_1: (0.4605) | Acc_1: (83.36%) (13978/16768)\n",
      "Epoch: 73 | Batch_idx: 140 |  Loss_1: (0.4632) | Acc_1: (83.23%) (15021/18048)\n",
      "Epoch: 73 | Batch_idx: 150 |  Loss_1: (0.4622) | Acc_1: (83.29%) (16099/19328)\n",
      "Epoch: 73 | Batch_idx: 160 |  Loss_1: (0.4575) | Acc_1: (83.49%) (17206/20608)\n",
      "Epoch: 73 | Batch_idx: 170 |  Loss_1: (0.4574) | Acc_1: (83.47%) (18271/21888)\n",
      "Epoch: 73 | Batch_idx: 180 |  Loss_1: (0.4588) | Acc_1: (83.48%) (19341/23168)\n",
      "Epoch: 73 | Batch_idx: 190 |  Loss_1: (0.4582) | Acc_1: (83.52%) (20420/24448)\n",
      "Epoch: 73 | Batch_idx: 200 |  Loss_1: (0.4588) | Acc_1: (83.46%) (21472/25728)\n",
      "Epoch: 73 | Batch_idx: 210 |  Loss_1: (0.4577) | Acc_1: (83.52%) (22558/27008)\n",
      "Epoch: 73 | Batch_idx: 220 |  Loss_1: (0.4572) | Acc_1: (83.54%) (23631/28288)\n",
      "Epoch: 73 | Batch_idx: 230 |  Loss_1: (0.4587) | Acc_1: (83.49%) (24687/29568)\n",
      "Epoch: 73 | Batch_idx: 240 |  Loss_1: (0.4584) | Acc_1: (83.46%) (25746/30848)\n",
      "Epoch: 73 | Batch_idx: 250 |  Loss_1: (0.4598) | Acc_1: (83.41%) (26799/32128)\n",
      "Epoch: 73 | Batch_idx: 260 |  Loss_1: (0.4602) | Acc_1: (83.39%) (27860/33408)\n",
      "Epoch: 73 | Batch_idx: 270 |  Loss_1: (0.4596) | Acc_1: (83.42%) (28935/34688)\n",
      "Epoch: 73 | Batch_idx: 280 |  Loss_1: (0.4588) | Acc_1: (83.43%) (30007/35968)\n",
      "Epoch: 73 | Batch_idx: 290 |  Loss_1: (0.4599) | Acc_1: (83.39%) (31061/37248)\n",
      "Epoch: 73 | Batch_idx: 300 |  Loss_1: (0.4603) | Acc_1: (83.40%) (32134/38528)\n",
      "Epoch: 73 | Batch_idx: 310 |  Loss_1: (0.4613) | Acc_1: (83.38%) (33190/39808)\n",
      "Epoch: 73 | Batch_idx: 320 |  Loss_1: (0.4620) | Acc_1: (83.35%) (34245/41088)\n",
      "Epoch: 73 | Batch_idx: 330 |  Loss_1: (0.4631) | Acc_1: (83.30%) (35294/42368)\n",
      "Epoch: 73 | Batch_idx: 340 |  Loss_1: (0.4623) | Acc_1: (83.35%) (36382/43648)\n",
      "Epoch: 73 | Batch_idx: 350 |  Loss_1: (0.4626) | Acc_1: (83.32%) (37434/44928)\n",
      "Epoch: 73 | Batch_idx: 360 |  Loss_1: (0.4635) | Acc_1: (83.30%) (38491/46208)\n",
      "Epoch: 73 | Batch_idx: 370 |  Loss_1: (0.4638) | Acc_1: (83.27%) (39545/47488)\n",
      "Epoch: 73 | Batch_idx: 380 |  Loss_1: (0.4641) | Acc_1: (83.27%) (40608/48768)\n",
      "Epoch: 73 | Batch_idx: 390 |  Loss_1: (0.4637) | Acc_1: (83.30%) (41650/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3343) | Acc: (88.75%) (8875/10000)\n",
      "Epoch: 74 | Batch_idx: 0 |  Loss_1: (0.5253) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 74 | Batch_idx: 10 |  Loss_1: (0.4530) | Acc_1: (83.81%) (1180/1408)\n",
      "Epoch: 74 | Batch_idx: 20 |  Loss_1: (0.4484) | Acc_1: (83.85%) (2254/2688)\n",
      "Epoch: 74 | Batch_idx: 30 |  Loss_1: (0.4524) | Acc_1: (83.80%) (3325/3968)\n",
      "Epoch: 74 | Batch_idx: 40 |  Loss_1: (0.4489) | Acc_1: (83.80%) (4398/5248)\n",
      "Epoch: 74 | Batch_idx: 50 |  Loss_1: (0.4478) | Acc_1: (83.78%) (5469/6528)\n",
      "Epoch: 74 | Batch_idx: 60 |  Loss_1: (0.4442) | Acc_1: (83.86%) (6548/7808)\n",
      "Epoch: 74 | Batch_idx: 70 |  Loss_1: (0.4468) | Acc_1: (83.89%) (7624/9088)\n",
      "Epoch: 74 | Batch_idx: 80 |  Loss_1: (0.4515) | Acc_1: (83.81%) (8689/10368)\n",
      "Epoch: 74 | Batch_idx: 90 |  Loss_1: (0.4521) | Acc_1: (83.80%) (9761/11648)\n",
      "Epoch: 74 | Batch_idx: 100 |  Loss_1: (0.4512) | Acc_1: (83.77%) (10830/12928)\n",
      "Epoch: 74 | Batch_idx: 110 |  Loss_1: (0.4476) | Acc_1: (83.88%) (11917/14208)\n",
      "Epoch: 74 | Batch_idx: 120 |  Loss_1: (0.4455) | Acc_1: (83.97%) (13005/15488)\n",
      "Epoch: 74 | Batch_idx: 130 |  Loss_1: (0.4468) | Acc_1: (84.02%) (14088/16768)\n",
      "Epoch: 74 | Batch_idx: 140 |  Loss_1: (0.4471) | Acc_1: (84.00%) (15161/18048)\n",
      "Epoch: 74 | Batch_idx: 150 |  Loss_1: (0.4500) | Acc_1: (83.83%) (16203/19328)\n",
      "Epoch: 74 | Batch_idx: 160 |  Loss_1: (0.4471) | Acc_1: (83.91%) (17292/20608)\n",
      "Epoch: 74 | Batch_idx: 170 |  Loss_1: (0.4470) | Acc_1: (83.91%) (18366/21888)\n",
      "Epoch: 74 | Batch_idx: 180 |  Loss_1: (0.4500) | Acc_1: (83.83%) (19421/23168)\n",
      "Epoch: 74 | Batch_idx: 190 |  Loss_1: (0.4494) | Acc_1: (83.85%) (20500/24448)\n",
      "Epoch: 74 | Batch_idx: 200 |  Loss_1: (0.4483) | Acc_1: (83.84%) (21571/25728)\n",
      "Epoch: 74 | Batch_idx: 210 |  Loss_1: (0.4500) | Acc_1: (83.78%) (22627/27008)\n",
      "Epoch: 74 | Batch_idx: 220 |  Loss_1: (0.4507) | Acc_1: (83.74%) (23689/28288)\n",
      "Epoch: 74 | Batch_idx: 230 |  Loss_1: (0.4508) | Acc_1: (83.73%) (24757/29568)\n",
      "Epoch: 74 | Batch_idx: 240 |  Loss_1: (0.4518) | Acc_1: (83.72%) (25825/30848)\n",
      "Epoch: 74 | Batch_idx: 250 |  Loss_1: (0.4517) | Acc_1: (83.71%) (26894/32128)\n",
      "Epoch: 74 | Batch_idx: 260 |  Loss_1: (0.4526) | Acc_1: (83.66%) (27949/33408)\n",
      "Epoch: 74 | Batch_idx: 270 |  Loss_1: (0.4520) | Acc_1: (83.69%) (29030/34688)\n",
      "Epoch: 74 | Batch_idx: 280 |  Loss_1: (0.4503) | Acc_1: (83.74%) (30119/35968)\n",
      "Epoch: 74 | Batch_idx: 290 |  Loss_1: (0.4503) | Acc_1: (83.77%) (31203/37248)\n",
      "Epoch: 74 | Batch_idx: 300 |  Loss_1: (0.4516) | Acc_1: (83.73%) (32260/38528)\n",
      "Epoch: 74 | Batch_idx: 310 |  Loss_1: (0.4566) | Acc_1: (83.63%) (33293/39808)\n",
      "Epoch: 74 | Batch_idx: 320 |  Loss_1: (0.4610) | Acc_1: (83.48%) (34302/41088)\n",
      "Epoch: 74 | Batch_idx: 330 |  Loss_1: (0.4644) | Acc_1: (83.38%) (35328/42368)\n",
      "Epoch: 74 | Batch_idx: 340 |  Loss_1: (0.4666) | Acc_1: (83.30%) (36357/43648)\n",
      "Epoch: 74 | Batch_idx: 350 |  Loss_1: (0.4684) | Acc_1: (83.24%) (37397/44928)\n",
      "Epoch: 74 | Batch_idx: 360 |  Loss_1: (0.4691) | Acc_1: (83.18%) (38435/46208)\n",
      "Epoch: 74 | Batch_idx: 370 |  Loss_1: (0.4705) | Acc_1: (83.13%) (39478/47488)\n",
      "Epoch: 74 | Batch_idx: 380 |  Loss_1: (0.4722) | Acc_1: (83.08%) (40518/48768)\n",
      "Epoch: 74 | Batch_idx: 390 |  Loss_1: (0.4734) | Acc_1: (83.03%) (41515/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3640) | Acc: (88.05%) (8805/10000)\n",
      "Epoch: 75 | Batch_idx: 0 |  Loss_1: (0.4969) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 75 | Batch_idx: 10 |  Loss_1: (0.4866) | Acc_1: (82.60%) (1163/1408)\n",
      "Epoch: 75 | Batch_idx: 20 |  Loss_1: (0.4808) | Acc_1: (82.89%) (2228/2688)\n",
      "Epoch: 75 | Batch_idx: 30 |  Loss_1: (0.4743) | Acc_1: (83.04%) (3295/3968)\n",
      "Epoch: 75 | Batch_idx: 40 |  Loss_1: (0.4767) | Acc_1: (83.02%) (4357/5248)\n",
      "Epoch: 75 | Batch_idx: 50 |  Loss_1: (0.4842) | Acc_1: (82.66%) (5396/6528)\n",
      "Epoch: 75 | Batch_idx: 60 |  Loss_1: (0.4823) | Acc_1: (82.71%) (6458/7808)\n",
      "Epoch: 75 | Batch_idx: 70 |  Loss_1: (0.4809) | Acc_1: (82.75%) (7520/9088)\n",
      "Epoch: 75 | Batch_idx: 80 |  Loss_1: (0.4786) | Acc_1: (82.76%) (8581/10368)\n",
      "Epoch: 75 | Batch_idx: 90 |  Loss_1: (0.4779) | Acc_1: (82.67%) (9629/11648)\n",
      "Epoch: 75 | Batch_idx: 100 |  Loss_1: (0.4764) | Acc_1: (82.57%) (10675/12928)\n",
      "Epoch: 75 | Batch_idx: 110 |  Loss_1: (0.4770) | Acc_1: (82.62%) (11738/14208)\n",
      "Epoch: 75 | Batch_idx: 120 |  Loss_1: (0.4752) | Acc_1: (82.68%) (12805/15488)\n",
      "Epoch: 75 | Batch_idx: 130 |  Loss_1: (0.4755) | Acc_1: (82.62%) (13853/16768)\n",
      "Epoch: 75 | Batch_idx: 140 |  Loss_1: (0.4747) | Acc_1: (82.67%) (14921/18048)\n",
      "Epoch: 75 | Batch_idx: 150 |  Loss_1: (0.4743) | Acc_1: (82.72%) (15988/19328)\n",
      "Epoch: 75 | Batch_idx: 160 |  Loss_1: (0.4734) | Acc_1: (82.74%) (17051/20608)\n",
      "Epoch: 75 | Batch_idx: 170 |  Loss_1: (0.4732) | Acc_1: (82.72%) (18106/21888)\n",
      "Epoch: 75 | Batch_idx: 180 |  Loss_1: (0.4750) | Acc_1: (82.68%) (19156/23168)\n",
      "Epoch: 75 | Batch_idx: 190 |  Loss_1: (0.4784) | Acc_1: (82.55%) (20181/24448)\n",
      "Epoch: 75 | Batch_idx: 200 |  Loss_1: (0.4773) | Acc_1: (82.61%) (21255/25728)\n",
      "Epoch: 75 | Batch_idx: 210 |  Loss_1: (0.4758) | Acc_1: (82.67%) (22327/27008)\n",
      "Epoch: 75 | Batch_idx: 220 |  Loss_1: (0.4765) | Acc_1: (82.63%) (23375/28288)\n",
      "Epoch: 75 | Batch_idx: 230 |  Loss_1: (0.4768) | Acc_1: (82.60%) (24423/29568)\n",
      "Epoch: 75 | Batch_idx: 240 |  Loss_1: (0.4771) | Acc_1: (82.59%) (25477/30848)\n",
      "Epoch: 75 | Batch_idx: 250 |  Loss_1: (0.4767) | Acc_1: (82.65%) (26554/32128)\n",
      "Epoch: 75 | Batch_idx: 260 |  Loss_1: (0.4763) | Acc_1: (82.64%) (27607/33408)\n",
      "Epoch: 75 | Batch_idx: 270 |  Loss_1: (0.4746) | Acc_1: (82.69%) (28685/34688)\n",
      "Epoch: 75 | Batch_idx: 280 |  Loss_1: (0.4718) | Acc_1: (82.80%) (29783/35968)\n",
      "Epoch: 75 | Batch_idx: 290 |  Loss_1: (0.4716) | Acc_1: (82.82%) (30850/37248)\n",
      "Epoch: 75 | Batch_idx: 300 |  Loss_1: (0.4698) | Acc_1: (82.90%) (31938/38528)\n",
      "Epoch: 75 | Batch_idx: 310 |  Loss_1: (0.4697) | Acc_1: (82.91%) (33006/39808)\n",
      "Epoch: 75 | Batch_idx: 320 |  Loss_1: (0.4692) | Acc_1: (82.92%) (34072/41088)\n",
      "Epoch: 75 | Batch_idx: 330 |  Loss_1: (0.4684) | Acc_1: (82.94%) (35142/42368)\n",
      "Epoch: 75 | Batch_idx: 340 |  Loss_1: (0.4686) | Acc_1: (82.97%) (36214/43648)\n",
      "Epoch: 75 | Batch_idx: 350 |  Loss_1: (0.4672) | Acc_1: (83.02%) (37299/44928)\n",
      "Epoch: 75 | Batch_idx: 360 |  Loss_1: (0.4662) | Acc_1: (83.04%) (38369/46208)\n",
      "Epoch: 75 | Batch_idx: 370 |  Loss_1: (0.4654) | Acc_1: (83.08%) (39452/47488)\n",
      "Epoch: 75 | Batch_idx: 380 |  Loss_1: (0.4650) | Acc_1: (83.08%) (40516/48768)\n",
      "Epoch: 75 | Batch_idx: 390 |  Loss_1: (0.4646) | Acc_1: (83.09%) (41547/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2889) | Acc: (90.31%) (9031/10000)\n",
      "Epoch: 76 | Batch_idx: 0 |  Loss_1: (0.3651) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 76 | Batch_idx: 10 |  Loss_1: (0.4940) | Acc_1: (82.88%) (1167/1408)\n",
      "Epoch: 76 | Batch_idx: 20 |  Loss_1: (0.4624) | Acc_1: (83.48%) (2244/2688)\n",
      "Epoch: 76 | Batch_idx: 30 |  Loss_1: (0.4601) | Acc_1: (83.59%) (3317/3968)\n",
      "Epoch: 76 | Batch_idx: 40 |  Loss_1: (0.4622) | Acc_1: (83.44%) (4379/5248)\n",
      "Epoch: 76 | Batch_idx: 50 |  Loss_1: (0.4558) | Acc_1: (83.53%) (5453/6528)\n",
      "Epoch: 76 | Batch_idx: 60 |  Loss_1: (0.4507) | Acc_1: (83.79%) (6542/7808)\n",
      "Epoch: 76 | Batch_idx: 70 |  Loss_1: (0.4489) | Acc_1: (83.89%) (7624/9088)\n",
      "Epoch: 76 | Batch_idx: 80 |  Loss_1: (0.4435) | Acc_1: (84.14%) (8724/10368)\n",
      "Epoch: 76 | Batch_idx: 90 |  Loss_1: (0.4388) | Acc_1: (84.30%) (9819/11648)\n",
      "Epoch: 76 | Batch_idx: 100 |  Loss_1: (0.4413) | Acc_1: (84.13%) (10876/12928)\n",
      "Epoch: 76 | Batch_idx: 110 |  Loss_1: (0.4421) | Acc_1: (84.02%) (11937/14208)\n",
      "Epoch: 76 | Batch_idx: 120 |  Loss_1: (0.4434) | Acc_1: (83.97%) (13006/15488)\n",
      "Epoch: 76 | Batch_idx: 130 |  Loss_1: (0.4437) | Acc_1: (83.95%) (14076/16768)\n",
      "Epoch: 76 | Batch_idx: 140 |  Loss_1: (0.4457) | Acc_1: (83.84%) (15132/18048)\n",
      "Epoch: 76 | Batch_idx: 150 |  Loss_1: (0.4477) | Acc_1: (83.79%) (16195/19328)\n",
      "Epoch: 76 | Batch_idx: 160 |  Loss_1: (0.4480) | Acc_1: (83.79%) (17268/20608)\n",
      "Epoch: 76 | Batch_idx: 170 |  Loss_1: (0.4488) | Acc_1: (83.74%) (18328/21888)\n",
      "Epoch: 76 | Batch_idx: 180 |  Loss_1: (0.4498) | Acc_1: (83.68%) (19387/23168)\n",
      "Epoch: 76 | Batch_idx: 190 |  Loss_1: (0.4490) | Acc_1: (83.75%) (20475/24448)\n",
      "Epoch: 76 | Batch_idx: 200 |  Loss_1: (0.4490) | Acc_1: (83.74%) (21544/25728)\n",
      "Epoch: 76 | Batch_idx: 210 |  Loss_1: (0.4498) | Acc_1: (83.73%) (22615/27008)\n",
      "Epoch: 76 | Batch_idx: 220 |  Loss_1: (0.4494) | Acc_1: (83.73%) (23686/28288)\n",
      "Epoch: 76 | Batch_idx: 230 |  Loss_1: (0.4484) | Acc_1: (83.78%) (24773/29568)\n",
      "Epoch: 76 | Batch_idx: 240 |  Loss_1: (0.4492) | Acc_1: (83.78%) (25845/30848)\n",
      "Epoch: 76 | Batch_idx: 250 |  Loss_1: (0.4507) | Acc_1: (83.72%) (26898/32128)\n",
      "Epoch: 76 | Batch_idx: 260 |  Loss_1: (0.4522) | Acc_1: (83.67%) (27951/33408)\n",
      "Epoch: 76 | Batch_idx: 270 |  Loss_1: (0.4530) | Acc_1: (83.65%) (29015/34688)\n",
      "Epoch: 76 | Batch_idx: 280 |  Loss_1: (0.4521) | Acc_1: (83.68%) (30097/35968)\n",
      "Epoch: 76 | Batch_idx: 290 |  Loss_1: (0.4525) | Acc_1: (83.65%) (31158/37248)\n",
      "Epoch: 76 | Batch_idx: 300 |  Loss_1: (0.4528) | Acc_1: (83.65%) (32228/38528)\n",
      "Epoch: 76 | Batch_idx: 310 |  Loss_1: (0.4517) | Acc_1: (83.67%) (33308/39808)\n",
      "Epoch: 76 | Batch_idx: 320 |  Loss_1: (0.4533) | Acc_1: (83.64%) (34364/41088)\n",
      "Epoch: 76 | Batch_idx: 330 |  Loss_1: (0.4540) | Acc_1: (83.61%) (35424/42368)\n",
      "Epoch: 76 | Batch_idx: 340 |  Loss_1: (0.4538) | Acc_1: (83.61%) (36493/43648)\n",
      "Epoch: 76 | Batch_idx: 350 |  Loss_1: (0.4543) | Acc_1: (83.58%) (37550/44928)\n",
      "Epoch: 76 | Batch_idx: 360 |  Loss_1: (0.4536) | Acc_1: (83.60%) (38628/46208)\n",
      "Epoch: 76 | Batch_idx: 370 |  Loss_1: (0.4531) | Acc_1: (83.62%) (39711/47488)\n",
      "Epoch: 76 | Batch_idx: 380 |  Loss_1: (0.4528) | Acc_1: (83.63%) (40787/48768)\n",
      "Epoch: 76 | Batch_idx: 390 |  Loss_1: (0.4524) | Acc_1: (83.64%) (41819/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2925) | Acc: (90.08%) (9008/10000)\n",
      "Epoch: 77 | Batch_idx: 0 |  Loss_1: (0.4567) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 77 | Batch_idx: 10 |  Loss_1: (0.4657) | Acc_1: (82.46%) (1161/1408)\n",
      "Epoch: 77 | Batch_idx: 20 |  Loss_1: (0.4527) | Acc_1: (83.04%) (2232/2688)\n",
      "Epoch: 77 | Batch_idx: 30 |  Loss_1: (0.4476) | Acc_1: (83.39%) (3309/3968)\n",
      "Epoch: 77 | Batch_idx: 40 |  Loss_1: (0.4423) | Acc_1: (83.65%) (4390/5248)\n",
      "Epoch: 77 | Batch_idx: 50 |  Loss_1: (0.4370) | Acc_1: (83.84%) (5473/6528)\n",
      "Epoch: 77 | Batch_idx: 60 |  Loss_1: (0.4367) | Acc_1: (83.99%) (6558/7808)\n",
      "Epoch: 77 | Batch_idx: 70 |  Loss_1: (0.4352) | Acc_1: (84.22%) (7654/9088)\n",
      "Epoch: 77 | Batch_idx: 80 |  Loss_1: (0.4320) | Acc_1: (84.35%) (8745/10368)\n",
      "Epoch: 77 | Batch_idx: 90 |  Loss_1: (0.4263) | Acc_1: (84.59%) (9853/11648)\n",
      "Epoch: 77 | Batch_idx: 100 |  Loss_1: (0.4305) | Acc_1: (84.46%) (10919/12928)\n",
      "Epoch: 77 | Batch_idx: 110 |  Loss_1: (0.4282) | Acc_1: (84.51%) (12007/14208)\n",
      "Epoch: 77 | Batch_idx: 120 |  Loss_1: (0.4295) | Acc_1: (84.52%) (13090/15488)\n",
      "Epoch: 77 | Batch_idx: 130 |  Loss_1: (0.4300) | Acc_1: (84.49%) (14167/16768)\n",
      "Epoch: 77 | Batch_idx: 140 |  Loss_1: (0.4308) | Acc_1: (84.50%) (15250/18048)\n",
      "Epoch: 77 | Batch_idx: 150 |  Loss_1: (0.4315) | Acc_1: (84.43%) (16319/19328)\n",
      "Epoch: 77 | Batch_idx: 160 |  Loss_1: (0.4314) | Acc_1: (84.36%) (17385/20608)\n",
      "Epoch: 77 | Batch_idx: 170 |  Loss_1: (0.4313) | Acc_1: (84.33%) (18459/21888)\n",
      "Epoch: 77 | Batch_idx: 180 |  Loss_1: (0.4317) | Acc_1: (84.32%) (19536/23168)\n",
      "Epoch: 77 | Batch_idx: 190 |  Loss_1: (0.4302) | Acc_1: (84.38%) (20630/24448)\n",
      "Epoch: 77 | Batch_idx: 200 |  Loss_1: (0.4293) | Acc_1: (84.46%) (21731/25728)\n",
      "Epoch: 77 | Batch_idx: 210 |  Loss_1: (0.4288) | Acc_1: (84.50%) (22821/27008)\n",
      "Epoch: 77 | Batch_idx: 220 |  Loss_1: (0.4294) | Acc_1: (84.46%) (23892/28288)\n",
      "Epoch: 77 | Batch_idx: 230 |  Loss_1: (0.4279) | Acc_1: (84.51%) (24989/29568)\n",
      "Epoch: 77 | Batch_idx: 240 |  Loss_1: (0.4267) | Acc_1: (84.53%) (26077/30848)\n",
      "Epoch: 77 | Batch_idx: 250 |  Loss_1: (0.4281) | Acc_1: (84.49%) (27145/32128)\n",
      "Epoch: 77 | Batch_idx: 260 |  Loss_1: (0.4290) | Acc_1: (84.48%) (28222/33408)\n",
      "Epoch: 77 | Batch_idx: 270 |  Loss_1: (0.4307) | Acc_1: (84.44%) (29291/34688)\n",
      "Epoch: 77 | Batch_idx: 280 |  Loss_1: (0.4319) | Acc_1: (84.39%) (30354/35968)\n",
      "Epoch: 77 | Batch_idx: 290 |  Loss_1: (0.4327) | Acc_1: (84.40%) (31439/37248)\n",
      "Epoch: 77 | Batch_idx: 300 |  Loss_1: (0.4343) | Acc_1: (84.35%) (32499/38528)\n",
      "Epoch: 77 | Batch_idx: 310 |  Loss_1: (0.4352) | Acc_1: (84.32%) (33565/39808)\n",
      "Epoch: 77 | Batch_idx: 320 |  Loss_1: (0.4361) | Acc_1: (84.29%) (34634/41088)\n",
      "Epoch: 77 | Batch_idx: 330 |  Loss_1: (0.4362) | Acc_1: (84.29%) (35714/42368)\n",
      "Epoch: 77 | Batch_idx: 340 |  Loss_1: (0.4363) | Acc_1: (84.27%) (36781/43648)\n",
      "Epoch: 77 | Batch_idx: 350 |  Loss_1: (0.4364) | Acc_1: (84.27%) (37860/44928)\n",
      "Epoch: 77 | Batch_idx: 360 |  Loss_1: (0.4366) | Acc_1: (84.26%) (38937/46208)\n",
      "Epoch: 77 | Batch_idx: 370 |  Loss_1: (0.4374) | Acc_1: (84.22%) (39995/47488)\n",
      "Epoch: 77 | Batch_idx: 380 |  Loss_1: (0.4372) | Acc_1: (84.23%) (41078/48768)\n",
      "Epoch: 77 | Batch_idx: 390 |  Loss_1: (0.4371) | Acc_1: (84.24%) (42118/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2987) | Acc: (90.04%) (9004/10000)\n",
      "Epoch: 78 | Batch_idx: 0 |  Loss_1: (0.2484) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 78 | Batch_idx: 10 |  Loss_1: (0.4235) | Acc_1: (84.59%) (1191/1408)\n",
      "Epoch: 78 | Batch_idx: 20 |  Loss_1: (0.4270) | Acc_1: (83.78%) (2252/2688)\n",
      "Epoch: 78 | Batch_idx: 30 |  Loss_1: (0.4251) | Acc_1: (84.05%) (3335/3968)\n",
      "Epoch: 78 | Batch_idx: 40 |  Loss_1: (0.4364) | Acc_1: (83.59%) (4387/5248)\n",
      "Epoch: 78 | Batch_idx: 50 |  Loss_1: (0.4389) | Acc_1: (83.73%) (5466/6528)\n",
      "Epoch: 78 | Batch_idx: 60 |  Loss_1: (0.4407) | Acc_1: (83.63%) (6530/7808)\n",
      "Epoch: 78 | Batch_idx: 70 |  Loss_1: (0.4392) | Acc_1: (83.74%) (7610/9088)\n",
      "Epoch: 78 | Batch_idx: 80 |  Loss_1: (0.4381) | Acc_1: (83.80%) (8688/10368)\n",
      "Epoch: 78 | Batch_idx: 90 |  Loss_1: (0.4358) | Acc_1: (83.89%) (9772/11648)\n",
      "Epoch: 78 | Batch_idx: 100 |  Loss_1: (0.4393) | Acc_1: (83.84%) (10839/12928)\n",
      "Epoch: 78 | Batch_idx: 110 |  Loss_1: (0.4426) | Acc_1: (83.85%) (11913/14208)\n",
      "Epoch: 78 | Batch_idx: 120 |  Loss_1: (0.4417) | Acc_1: (83.86%) (12989/15488)\n",
      "Epoch: 78 | Batch_idx: 130 |  Loss_1: (0.4408) | Acc_1: (83.89%) (14067/16768)\n",
      "Epoch: 78 | Batch_idx: 140 |  Loss_1: (0.4420) | Acc_1: (83.83%) (15129/18048)\n",
      "Epoch: 78 | Batch_idx: 150 |  Loss_1: (0.4385) | Acc_1: (83.97%) (16230/19328)\n",
      "Epoch: 78 | Batch_idx: 160 |  Loss_1: (0.4351) | Acc_1: (84.10%) (17331/20608)\n",
      "Epoch: 78 | Batch_idx: 170 |  Loss_1: (0.4351) | Acc_1: (84.09%) (18405/21888)\n",
      "Epoch: 78 | Batch_idx: 180 |  Loss_1: (0.4335) | Acc_1: (84.16%) (19499/23168)\n",
      "Epoch: 78 | Batch_idx: 190 |  Loss_1: (0.4338) | Acc_1: (84.16%) (20575/24448)\n",
      "Epoch: 78 | Batch_idx: 200 |  Loss_1: (0.4339) | Acc_1: (84.15%) (21650/25728)\n",
      "Epoch: 78 | Batch_idx: 210 |  Loss_1: (0.4343) | Acc_1: (84.12%) (22720/27008)\n",
      "Epoch: 78 | Batch_idx: 220 |  Loss_1: (0.4363) | Acc_1: (84.04%) (23772/28288)\n",
      "Epoch: 78 | Batch_idx: 230 |  Loss_1: (0.4378) | Acc_1: (84.00%) (24837/29568)\n",
      "Epoch: 78 | Batch_idx: 240 |  Loss_1: (0.4404) | Acc_1: (83.91%) (25886/30848)\n",
      "Epoch: 78 | Batch_idx: 250 |  Loss_1: (0.4393) | Acc_1: (83.96%) (26974/32128)\n",
      "Epoch: 78 | Batch_idx: 260 |  Loss_1: (0.4396) | Acc_1: (83.96%) (28050/33408)\n",
      "Epoch: 78 | Batch_idx: 270 |  Loss_1: (0.4408) | Acc_1: (83.93%) (29113/34688)\n",
      "Epoch: 78 | Batch_idx: 280 |  Loss_1: (0.4394) | Acc_1: (83.97%) (30204/35968)\n",
      "Epoch: 78 | Batch_idx: 290 |  Loss_1: (0.4385) | Acc_1: (83.99%) (31285/37248)\n",
      "Epoch: 78 | Batch_idx: 300 |  Loss_1: (0.4396) | Acc_1: (83.98%) (32354/38528)\n",
      "Epoch: 78 | Batch_idx: 310 |  Loss_1: (0.4404) | Acc_1: (83.97%) (33426/39808)\n",
      "Epoch: 78 | Batch_idx: 320 |  Loss_1: (0.4398) | Acc_1: (83.99%) (34511/41088)\n",
      "Epoch: 78 | Batch_idx: 330 |  Loss_1: (0.4423) | Acc_1: (83.93%) (35558/42368)\n",
      "Epoch: 78 | Batch_idx: 340 |  Loss_1: (0.4428) | Acc_1: (83.91%) (36626/43648)\n",
      "Epoch: 78 | Batch_idx: 350 |  Loss_1: (0.4416) | Acc_1: (83.95%) (37715/44928)\n",
      "Epoch: 78 | Batch_idx: 360 |  Loss_1: (0.4426) | Acc_1: (83.90%) (38767/46208)\n",
      "Epoch: 78 | Batch_idx: 370 |  Loss_1: (0.4425) | Acc_1: (83.91%) (39849/47488)\n",
      "Epoch: 78 | Batch_idx: 380 |  Loss_1: (0.4424) | Acc_1: (83.91%) (40920/48768)\n",
      "Epoch: 78 | Batch_idx: 390 |  Loss_1: (0.4421) | Acc_1: (83.92%) (41960/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3140) | Acc: (89.45%) (8945/10000)\n",
      "Epoch: 79 | Batch_idx: 0 |  Loss_1: (0.3753) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 79 | Batch_idx: 10 |  Loss_1: (0.4379) | Acc_1: (84.52%) (1190/1408)\n",
      "Epoch: 79 | Batch_idx: 20 |  Loss_1: (0.4497) | Acc_1: (84.04%) (2259/2688)\n",
      "Epoch: 79 | Batch_idx: 30 |  Loss_1: (0.4466) | Acc_1: (84.12%) (3338/3968)\n",
      "Epoch: 79 | Batch_idx: 40 |  Loss_1: (0.4395) | Acc_1: (84.28%) (4423/5248)\n",
      "Epoch: 79 | Batch_idx: 50 |  Loss_1: (0.4333) | Acc_1: (84.41%) (5510/6528)\n",
      "Epoch: 79 | Batch_idx: 60 |  Loss_1: (0.4315) | Acc_1: (84.35%) (6586/7808)\n",
      "Epoch: 79 | Batch_idx: 70 |  Loss_1: (0.4392) | Acc_1: (84.09%) (7642/9088)\n",
      "Epoch: 79 | Batch_idx: 80 |  Loss_1: (0.4399) | Acc_1: (84.02%) (8711/10368)\n",
      "Epoch: 79 | Batch_idx: 90 |  Loss_1: (0.4451) | Acc_1: (83.77%) (9758/11648)\n",
      "Epoch: 79 | Batch_idx: 100 |  Loss_1: (0.4444) | Acc_1: (83.73%) (10825/12928)\n",
      "Epoch: 79 | Batch_idx: 110 |  Loss_1: (0.4420) | Acc_1: (83.79%) (11905/14208)\n",
      "Epoch: 79 | Batch_idx: 120 |  Loss_1: (0.4423) | Acc_1: (83.74%) (12969/15488)\n",
      "Epoch: 79 | Batch_idx: 130 |  Loss_1: (0.4420) | Acc_1: (83.72%) (14038/16768)\n",
      "Epoch: 79 | Batch_idx: 140 |  Loss_1: (0.4455) | Acc_1: (83.64%) (15095/18048)\n",
      "Epoch: 79 | Batch_idx: 150 |  Loss_1: (0.4453) | Acc_1: (83.65%) (16167/19328)\n",
      "Epoch: 79 | Batch_idx: 160 |  Loss_1: (0.4445) | Acc_1: (83.69%) (17247/20608)\n",
      "Epoch: 79 | Batch_idx: 170 |  Loss_1: (0.4429) | Acc_1: (83.75%) (18332/21888)\n",
      "Epoch: 79 | Batch_idx: 180 |  Loss_1: (0.4431) | Acc_1: (83.73%) (19399/23168)\n",
      "Epoch: 79 | Batch_idx: 190 |  Loss_1: (0.4417) | Acc_1: (83.79%) (20485/24448)\n",
      "Epoch: 79 | Batch_idx: 200 |  Loss_1: (0.4417) | Acc_1: (83.78%) (21554/25728)\n",
      "Epoch: 79 | Batch_idx: 210 |  Loss_1: (0.4408) | Acc_1: (83.78%) (22628/27008)\n",
      "Epoch: 79 | Batch_idx: 220 |  Loss_1: (0.4403) | Acc_1: (83.81%) (23708/28288)\n",
      "Epoch: 79 | Batch_idx: 230 |  Loss_1: (0.4401) | Acc_1: (83.80%) (24778/29568)\n",
      "Epoch: 79 | Batch_idx: 240 |  Loss_1: (0.4422) | Acc_1: (83.73%) (25829/30848)\n",
      "Epoch: 79 | Batch_idx: 250 |  Loss_1: (0.4412) | Acc_1: (83.80%) (26922/32128)\n",
      "Epoch: 79 | Batch_idx: 260 |  Loss_1: (0.4418) | Acc_1: (83.76%) (27982/33408)\n",
      "Epoch: 79 | Batch_idx: 270 |  Loss_1: (0.4423) | Acc_1: (83.74%) (29049/34688)\n",
      "Epoch: 79 | Batch_idx: 280 |  Loss_1: (0.4411) | Acc_1: (83.78%) (30135/35968)\n",
      "Epoch: 79 | Batch_idx: 290 |  Loss_1: (0.4393) | Acc_1: (83.84%) (31227/37248)\n",
      "Epoch: 79 | Batch_idx: 300 |  Loss_1: (0.4393) | Acc_1: (83.86%) (32309/38528)\n",
      "Epoch: 79 | Batch_idx: 310 |  Loss_1: (0.4389) | Acc_1: (83.86%) (33382/39808)\n",
      "Epoch: 79 | Batch_idx: 320 |  Loss_1: (0.4375) | Acc_1: (83.91%) (34476/41088)\n",
      "Epoch: 79 | Batch_idx: 330 |  Loss_1: (0.4372) | Acc_1: (83.92%) (35557/42368)\n",
      "Epoch: 79 | Batch_idx: 340 |  Loss_1: (0.4376) | Acc_1: (83.90%) (36619/43648)\n",
      "Epoch: 79 | Batch_idx: 350 |  Loss_1: (0.4378) | Acc_1: (83.89%) (37691/44928)\n",
      "Epoch: 79 | Batch_idx: 360 |  Loss_1: (0.4381) | Acc_1: (83.91%) (38771/46208)\n",
      "Epoch: 79 | Batch_idx: 370 |  Loss_1: (0.4387) | Acc_1: (83.92%) (39852/47488)\n",
      "Epoch: 79 | Batch_idx: 380 |  Loss_1: (0.4394) | Acc_1: (83.90%) (40914/48768)\n",
      "Epoch: 79 | Batch_idx: 390 |  Loss_1: (0.4400) | Acc_1: (83.87%) (41935/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2908) | Acc: (90.21%) (9021/10000)\n",
      "Epoch: 80 | Batch_idx: 0 |  Loss_1: (0.4733) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 80 | Batch_idx: 10 |  Loss_1: (0.4518) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 80 | Batch_idx: 20 |  Loss_1: (0.4369) | Acc_1: (83.97%) (2257/2688)\n",
      "Epoch: 80 | Batch_idx: 30 |  Loss_1: (0.4436) | Acc_1: (83.74%) (3323/3968)\n",
      "Epoch: 80 | Batch_idx: 40 |  Loss_1: (0.4260) | Acc_1: (84.49%) (4434/5248)\n",
      "Epoch: 80 | Batch_idx: 50 |  Loss_1: (0.4272) | Acc_1: (84.45%) (5513/6528)\n",
      "Epoch: 80 | Batch_idx: 60 |  Loss_1: (0.4230) | Acc_1: (84.67%) (6611/7808)\n",
      "Epoch: 80 | Batch_idx: 70 |  Loss_1: (0.4176) | Acc_1: (84.85%) (7711/9088)\n",
      "Epoch: 80 | Batch_idx: 80 |  Loss_1: (0.4159) | Acc_1: (84.94%) (8807/10368)\n",
      "Epoch: 80 | Batch_idx: 90 |  Loss_1: (0.4142) | Acc_1: (85.03%) (9904/11648)\n",
      "Epoch: 80 | Batch_idx: 100 |  Loss_1: (0.4150) | Acc_1: (85.01%) (10990/12928)\n",
      "Epoch: 80 | Batch_idx: 110 |  Loss_1: (0.4167) | Acc_1: (84.91%) (12064/14208)\n",
      "Epoch: 80 | Batch_idx: 120 |  Loss_1: (0.4145) | Acc_1: (84.98%) (13161/15488)\n",
      "Epoch: 80 | Batch_idx: 130 |  Loss_1: (0.4150) | Acc_1: (84.89%) (14234/16768)\n",
      "Epoch: 80 | Batch_idx: 140 |  Loss_1: (0.4144) | Acc_1: (84.89%) (15321/18048)\n",
      "Epoch: 80 | Batch_idx: 150 |  Loss_1: (0.4171) | Acc_1: (84.80%) (16391/19328)\n",
      "Epoch: 80 | Batch_idx: 160 |  Loss_1: (0.4188) | Acc_1: (84.70%) (17455/20608)\n",
      "Epoch: 80 | Batch_idx: 170 |  Loss_1: (0.4210) | Acc_1: (84.64%) (18526/21888)\n",
      "Epoch: 80 | Batch_idx: 180 |  Loss_1: (0.4211) | Acc_1: (84.61%) (19603/23168)\n",
      "Epoch: 80 | Batch_idx: 190 |  Loss_1: (0.4207) | Acc_1: (84.64%) (20694/24448)\n",
      "Epoch: 80 | Batch_idx: 200 |  Loss_1: (0.4220) | Acc_1: (84.62%) (21772/25728)\n",
      "Epoch: 80 | Batch_idx: 210 |  Loss_1: (0.4233) | Acc_1: (84.58%) (22843/27008)\n",
      "Epoch: 80 | Batch_idx: 220 |  Loss_1: (0.4254) | Acc_1: (84.50%) (23904/28288)\n",
      "Epoch: 80 | Batch_idx: 230 |  Loss_1: (0.4257) | Acc_1: (84.48%) (24978/29568)\n",
      "Epoch: 80 | Batch_idx: 240 |  Loss_1: (0.4248) | Acc_1: (84.52%) (26072/30848)\n",
      "Epoch: 80 | Batch_idx: 250 |  Loss_1: (0.4256) | Acc_1: (84.47%) (27140/32128)\n",
      "Epoch: 80 | Batch_idx: 260 |  Loss_1: (0.4261) | Acc_1: (84.49%) (28225/33408)\n",
      "Epoch: 80 | Batch_idx: 270 |  Loss_1: (0.4247) | Acc_1: (84.55%) (29328/34688)\n",
      "Epoch: 80 | Batch_idx: 280 |  Loss_1: (0.4249) | Acc_1: (84.54%) (30406/35968)\n",
      "Epoch: 80 | Batch_idx: 290 |  Loss_1: (0.4247) | Acc_1: (84.53%) (31484/37248)\n",
      "Epoch: 80 | Batch_idx: 300 |  Loss_1: (0.4258) | Acc_1: (84.49%) (32554/38528)\n",
      "Epoch: 80 | Batch_idx: 310 |  Loss_1: (0.4252) | Acc_1: (84.48%) (33631/39808)\n",
      "Epoch: 80 | Batch_idx: 320 |  Loss_1: (0.4249) | Acc_1: (84.54%) (34734/41088)\n",
      "Epoch: 80 | Batch_idx: 330 |  Loss_1: (0.4256) | Acc_1: (84.52%) (35808/42368)\n",
      "Epoch: 80 | Batch_idx: 340 |  Loss_1: (0.4254) | Acc_1: (84.51%) (36889/43648)\n",
      "Epoch: 80 | Batch_idx: 350 |  Loss_1: (0.4254) | Acc_1: (84.50%) (37965/44928)\n",
      "Epoch: 80 | Batch_idx: 360 |  Loss_1: (0.4254) | Acc_1: (84.50%) (39048/46208)\n",
      "Epoch: 80 | Batch_idx: 370 |  Loss_1: (0.4264) | Acc_1: (84.49%) (40121/47488)\n",
      "Epoch: 80 | Batch_idx: 380 |  Loss_1: (0.4257) | Acc_1: (84.50%) (41209/48768)\n",
      "Epoch: 80 | Batch_idx: 390 |  Loss_1: (0.4253) | Acc_1: (84.52%) (42258/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2953) | Acc: (90.12%) (9012/10000)\n",
      "Epoch: 81 | Batch_idx: 0 |  Loss_1: (0.3632) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 81 | Batch_idx: 10 |  Loss_1: (0.4043) | Acc_1: (85.37%) (1202/1408)\n",
      "Epoch: 81 | Batch_idx: 20 |  Loss_1: (0.4053) | Acc_1: (84.90%) (2282/2688)\n",
      "Epoch: 81 | Batch_idx: 30 |  Loss_1: (0.4099) | Acc_1: (84.63%) (3358/3968)\n",
      "Epoch: 81 | Batch_idx: 40 |  Loss_1: (0.4063) | Acc_1: (84.97%) (4459/5248)\n",
      "Epoch: 81 | Batch_idx: 50 |  Loss_1: (0.4123) | Acc_1: (84.96%) (5546/6528)\n",
      "Epoch: 81 | Batch_idx: 60 |  Loss_1: (0.4141) | Acc_1: (84.86%) (6626/7808)\n",
      "Epoch: 81 | Batch_idx: 70 |  Loss_1: (0.4143) | Acc_1: (84.88%) (7714/9088)\n",
      "Epoch: 81 | Batch_idx: 80 |  Loss_1: (0.4210) | Acc_1: (84.61%) (8772/10368)\n",
      "Epoch: 81 | Batch_idx: 90 |  Loss_1: (0.4235) | Acc_1: (84.56%) (9850/11648)\n",
      "Epoch: 81 | Batch_idx: 100 |  Loss_1: (0.4268) | Acc_1: (84.47%) (10920/12928)\n",
      "Epoch: 81 | Batch_idx: 110 |  Loss_1: (0.4291) | Acc_1: (84.35%) (11985/14208)\n",
      "Epoch: 81 | Batch_idx: 120 |  Loss_1: (0.4282) | Acc_1: (84.39%) (13070/15488)\n",
      "Epoch: 81 | Batch_idx: 130 |  Loss_1: (0.4300) | Acc_1: (84.32%) (14139/16768)\n",
      "Epoch: 81 | Batch_idx: 140 |  Loss_1: (0.4279) | Acc_1: (84.40%) (15232/18048)\n",
      "Epoch: 81 | Batch_idx: 150 |  Loss_1: (0.4242) | Acc_1: (84.49%) (16331/19328)\n",
      "Epoch: 81 | Batch_idx: 160 |  Loss_1: (0.4227) | Acc_1: (84.54%) (17423/20608)\n",
      "Epoch: 81 | Batch_idx: 170 |  Loss_1: (0.4227) | Acc_1: (84.58%) (18512/21888)\n",
      "Epoch: 81 | Batch_idx: 180 |  Loss_1: (0.4226) | Acc_1: (84.60%) (19599/23168)\n",
      "Epoch: 81 | Batch_idx: 190 |  Loss_1: (0.4231) | Acc_1: (84.58%) (20679/24448)\n",
      "Epoch: 81 | Batch_idx: 200 |  Loss_1: (0.4261) | Acc_1: (84.45%) (21727/25728)\n",
      "Epoch: 81 | Batch_idx: 210 |  Loss_1: (0.4242) | Acc_1: (84.54%) (22832/27008)\n",
      "Epoch: 81 | Batch_idx: 220 |  Loss_1: (0.4238) | Acc_1: (84.52%) (23909/28288)\n",
      "Epoch: 81 | Batch_idx: 230 |  Loss_1: (0.4223) | Acc_1: (84.56%) (25004/29568)\n",
      "Epoch: 81 | Batch_idx: 240 |  Loss_1: (0.4213) | Acc_1: (84.61%) (26100/30848)\n",
      "Epoch: 81 | Batch_idx: 250 |  Loss_1: (0.4243) | Acc_1: (84.47%) (27140/32128)\n",
      "Epoch: 81 | Batch_idx: 260 |  Loss_1: (0.4266) | Acc_1: (84.42%) (28202/33408)\n",
      "Epoch: 81 | Batch_idx: 270 |  Loss_1: (0.4244) | Acc_1: (84.50%) (29311/34688)\n",
      "Epoch: 81 | Batch_idx: 280 |  Loss_1: (0.4231) | Acc_1: (84.54%) (30406/35968)\n",
      "Epoch: 81 | Batch_idx: 290 |  Loss_1: (0.4233) | Acc_1: (84.54%) (31490/37248)\n",
      "Epoch: 81 | Batch_idx: 300 |  Loss_1: (0.4222) | Acc_1: (84.58%) (32587/38528)\n",
      "Epoch: 81 | Batch_idx: 310 |  Loss_1: (0.4224) | Acc_1: (84.60%) (33676/39808)\n",
      "Epoch: 81 | Batch_idx: 320 |  Loss_1: (0.4223) | Acc_1: (84.57%) (34748/41088)\n",
      "Epoch: 81 | Batch_idx: 330 |  Loss_1: (0.4228) | Acc_1: (84.56%) (35826/42368)\n",
      "Epoch: 81 | Batch_idx: 340 |  Loss_1: (0.4239) | Acc_1: (84.53%) (36897/43648)\n",
      "Epoch: 81 | Batch_idx: 350 |  Loss_1: (0.4245) | Acc_1: (84.51%) (37967/44928)\n",
      "Epoch: 81 | Batch_idx: 360 |  Loss_1: (0.4247) | Acc_1: (84.50%) (39044/46208)\n",
      "Epoch: 81 | Batch_idx: 370 |  Loss_1: (0.4238) | Acc_1: (84.52%) (40137/47488)\n",
      "Epoch: 81 | Batch_idx: 380 |  Loss_1: (0.4230) | Acc_1: (84.55%) (41233/48768)\n",
      "Epoch: 81 | Batch_idx: 390 |  Loss_1: (0.4222) | Acc_1: (84.55%) (42275/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3134) | Acc: (89.54%) (8954/10000)\n",
      "Epoch: 82 | Batch_idx: 0 |  Loss_1: (0.4237) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 82 | Batch_idx: 10 |  Loss_1: (0.4155) | Acc_1: (84.94%) (1196/1408)\n",
      "Epoch: 82 | Batch_idx: 20 |  Loss_1: (0.3984) | Acc_1: (85.97%) (2311/2688)\n",
      "Epoch: 82 | Batch_idx: 30 |  Loss_1: (0.3980) | Acc_1: (85.96%) (3411/3968)\n",
      "Epoch: 82 | Batch_idx: 40 |  Loss_1: (0.3994) | Acc_1: (85.88%) (4507/5248)\n",
      "Epoch: 82 | Batch_idx: 50 |  Loss_1: (0.3935) | Acc_1: (85.97%) (5612/6528)\n",
      "Epoch: 82 | Batch_idx: 60 |  Loss_1: (0.3999) | Acc_1: (85.72%) (6693/7808)\n",
      "Epoch: 82 | Batch_idx: 70 |  Loss_1: (0.4043) | Acc_1: (85.39%) (7760/9088)\n",
      "Epoch: 82 | Batch_idx: 80 |  Loss_1: (0.4129) | Acc_1: (85.08%) (8821/10368)\n",
      "Epoch: 82 | Batch_idx: 90 |  Loss_1: (0.4147) | Acc_1: (85.00%) (9901/11648)\n",
      "Epoch: 82 | Batch_idx: 100 |  Loss_1: (0.4150) | Acc_1: (84.97%) (10985/12928)\n",
      "Epoch: 82 | Batch_idx: 110 |  Loss_1: (0.4175) | Acc_1: (84.90%) (12062/14208)\n",
      "Epoch: 82 | Batch_idx: 120 |  Loss_1: (0.4157) | Acc_1: (84.89%) (13148/15488)\n",
      "Epoch: 82 | Batch_idx: 130 |  Loss_1: (0.4189) | Acc_1: (84.81%) (14221/16768)\n",
      "Epoch: 82 | Batch_idx: 140 |  Loss_1: (0.4162) | Acc_1: (84.84%) (15312/18048)\n",
      "Epoch: 82 | Batch_idx: 150 |  Loss_1: (0.4164) | Acc_1: (84.78%) (16387/19328)\n",
      "Epoch: 82 | Batch_idx: 160 |  Loss_1: (0.4184) | Acc_1: (84.75%) (17466/20608)\n",
      "Epoch: 82 | Batch_idx: 170 |  Loss_1: (0.4195) | Acc_1: (84.66%) (18531/21888)\n",
      "Epoch: 82 | Batch_idx: 180 |  Loss_1: (0.4190) | Acc_1: (84.69%) (19622/23168)\n",
      "Epoch: 82 | Batch_idx: 190 |  Loss_1: (0.4170) | Acc_1: (84.77%) (20724/24448)\n",
      "Epoch: 82 | Batch_idx: 200 |  Loss_1: (0.4164) | Acc_1: (84.75%) (21804/25728)\n",
      "Epoch: 82 | Batch_idx: 210 |  Loss_1: (0.4167) | Acc_1: (84.78%) (22898/27008)\n",
      "Epoch: 82 | Batch_idx: 220 |  Loss_1: (0.4170) | Acc_1: (84.77%) (23980/28288)\n",
      "Epoch: 82 | Batch_idx: 230 |  Loss_1: (0.4178) | Acc_1: (84.75%) (25059/29568)\n",
      "Epoch: 82 | Batch_idx: 240 |  Loss_1: (0.4174) | Acc_1: (84.77%) (26150/30848)\n",
      "Epoch: 82 | Batch_idx: 250 |  Loss_1: (0.4181) | Acc_1: (84.76%) (27233/32128)\n",
      "Epoch: 82 | Batch_idx: 260 |  Loss_1: (0.4185) | Acc_1: (84.76%) (28315/33408)\n",
      "Epoch: 82 | Batch_idx: 270 |  Loss_1: (0.4197) | Acc_1: (84.74%) (29395/34688)\n",
      "Epoch: 82 | Batch_idx: 280 |  Loss_1: (0.4204) | Acc_1: (84.70%) (30465/35968)\n",
      "Epoch: 82 | Batch_idx: 290 |  Loss_1: (0.4217) | Acc_1: (84.65%) (31529/37248)\n",
      "Epoch: 82 | Batch_idx: 300 |  Loss_1: (0.4211) | Acc_1: (84.67%) (32622/38528)\n",
      "Epoch: 82 | Batch_idx: 310 |  Loss_1: (0.4206) | Acc_1: (84.69%) (33713/39808)\n",
      "Epoch: 82 | Batch_idx: 320 |  Loss_1: (0.4201) | Acc_1: (84.71%) (34805/41088)\n",
      "Epoch: 82 | Batch_idx: 330 |  Loss_1: (0.4208) | Acc_1: (84.69%) (35880/42368)\n",
      "Epoch: 82 | Batch_idx: 340 |  Loss_1: (0.4204) | Acc_1: (84.72%) (36978/43648)\n",
      "Epoch: 82 | Batch_idx: 350 |  Loss_1: (0.4216) | Acc_1: (84.68%) (38046/44928)\n",
      "Epoch: 82 | Batch_idx: 360 |  Loss_1: (0.4226) | Acc_1: (84.65%) (39113/46208)\n",
      "Epoch: 82 | Batch_idx: 370 |  Loss_1: (0.4229) | Acc_1: (84.62%) (40184/47488)\n",
      "Epoch: 82 | Batch_idx: 380 |  Loss_1: (0.4235) | Acc_1: (84.61%) (41261/48768)\n",
      "Epoch: 82 | Batch_idx: 390 |  Loss_1: (0.4238) | Acc_1: (84.61%) (42306/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2940) | Acc: (90.16%) (9016/10000)\n",
      "Epoch: 83 | Batch_idx: 0 |  Loss_1: (0.3240) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 83 | Batch_idx: 10 |  Loss_1: (0.4088) | Acc_1: (85.44%) (1203/1408)\n",
      "Epoch: 83 | Batch_idx: 20 |  Loss_1: (0.3982) | Acc_1: (85.94%) (2310/2688)\n",
      "Epoch: 83 | Batch_idx: 30 |  Loss_1: (0.3963) | Acc_1: (85.84%) (3406/3968)\n",
      "Epoch: 83 | Batch_idx: 40 |  Loss_1: (0.3990) | Acc_1: (85.86%) (4506/5248)\n",
      "Epoch: 83 | Batch_idx: 50 |  Loss_1: (0.4038) | Acc_1: (85.51%) (5582/6528)\n",
      "Epoch: 83 | Batch_idx: 60 |  Loss_1: (0.4042) | Acc_1: (85.49%) (6675/7808)\n",
      "Epoch: 83 | Batch_idx: 70 |  Loss_1: (0.4004) | Acc_1: (85.53%) (7773/9088)\n",
      "Epoch: 83 | Batch_idx: 80 |  Loss_1: (0.4067) | Acc_1: (85.25%) (8839/10368)\n",
      "Epoch: 83 | Batch_idx: 90 |  Loss_1: (0.4048) | Acc_1: (85.30%) (9936/11648)\n",
      "Epoch: 83 | Batch_idx: 100 |  Loss_1: (0.4104) | Acc_1: (85.00%) (10989/12928)\n",
      "Epoch: 83 | Batch_idx: 110 |  Loss_1: (0.4110) | Acc_1: (85.02%) (12080/14208)\n",
      "Epoch: 83 | Batch_idx: 120 |  Loss_1: (0.4073) | Acc_1: (85.12%) (13183/15488)\n",
      "Epoch: 83 | Batch_idx: 130 |  Loss_1: (0.4064) | Acc_1: (85.16%) (14279/16768)\n",
      "Epoch: 83 | Batch_idx: 140 |  Loss_1: (0.4087) | Acc_1: (85.13%) (15365/18048)\n",
      "Epoch: 83 | Batch_idx: 150 |  Loss_1: (0.4101) | Acc_1: (85.08%) (16444/19328)\n",
      "Epoch: 83 | Batch_idx: 160 |  Loss_1: (0.4124) | Acc_1: (84.98%) (17512/20608)\n",
      "Epoch: 83 | Batch_idx: 170 |  Loss_1: (0.4138) | Acc_1: (84.95%) (18593/21888)\n",
      "Epoch: 83 | Batch_idx: 180 |  Loss_1: (0.4128) | Acc_1: (84.98%) (19688/23168)\n",
      "Epoch: 83 | Batch_idx: 190 |  Loss_1: (0.4137) | Acc_1: (84.91%) (20759/24448)\n",
      "Epoch: 83 | Batch_idx: 200 |  Loss_1: (0.4157) | Acc_1: (84.81%) (21820/25728)\n",
      "Epoch: 83 | Batch_idx: 210 |  Loss_1: (0.4164) | Acc_1: (84.81%) (22905/27008)\n",
      "Epoch: 83 | Batch_idx: 220 |  Loss_1: (0.4168) | Acc_1: (84.83%) (23998/28288)\n",
      "Epoch: 83 | Batch_idx: 230 |  Loss_1: (0.4158) | Acc_1: (84.85%) (25088/29568)\n",
      "Epoch: 83 | Batch_idx: 240 |  Loss_1: (0.4146) | Acc_1: (84.90%) (26191/30848)\n",
      "Epoch: 83 | Batch_idx: 250 |  Loss_1: (0.4140) | Acc_1: (84.94%) (27289/32128)\n",
      "Epoch: 83 | Batch_idx: 260 |  Loss_1: (0.4131) | Acc_1: (84.97%) (28387/33408)\n",
      "Epoch: 83 | Batch_idx: 270 |  Loss_1: (0.4116) | Acc_1: (85.04%) (29499/34688)\n",
      "Epoch: 83 | Batch_idx: 280 |  Loss_1: (0.4124) | Acc_1: (85.01%) (30577/35968)\n",
      "Epoch: 83 | Batch_idx: 290 |  Loss_1: (0.4122) | Acc_1: (85.03%) (31671/37248)\n",
      "Epoch: 83 | Batch_idx: 300 |  Loss_1: (0.4128) | Acc_1: (85.03%) (32759/38528)\n",
      "Epoch: 83 | Batch_idx: 310 |  Loss_1: (0.4128) | Acc_1: (85.02%) (33843/39808)\n",
      "Epoch: 83 | Batch_idx: 320 |  Loss_1: (0.4129) | Acc_1: (85.01%) (34930/41088)\n",
      "Epoch: 83 | Batch_idx: 330 |  Loss_1: (0.4121) | Acc_1: (85.03%) (36025/42368)\n",
      "Epoch: 83 | Batch_idx: 340 |  Loss_1: (0.4113) | Acc_1: (85.06%) (37127/43648)\n",
      "Epoch: 83 | Batch_idx: 350 |  Loss_1: (0.4122) | Acc_1: (85.00%) (38190/44928)\n",
      "Epoch: 83 | Batch_idx: 360 |  Loss_1: (0.4125) | Acc_1: (84.99%) (39272/46208)\n",
      "Epoch: 83 | Batch_idx: 370 |  Loss_1: (0.4125) | Acc_1: (84.98%) (40357/47488)\n",
      "Epoch: 83 | Batch_idx: 380 |  Loss_1: (0.4117) | Acc_1: (85.00%) (41451/48768)\n",
      "Epoch: 83 | Batch_idx: 390 |  Loss_1: (0.4116) | Acc_1: (84.99%) (42494/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2774) | Acc: (90.56%) (9056/10000)\n",
      "Epoch: 84 | Batch_idx: 0 |  Loss_1: (0.4781) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 84 | Batch_idx: 10 |  Loss_1: (0.4101) | Acc_1: (85.30%) (1201/1408)\n",
      "Epoch: 84 | Batch_idx: 20 |  Loss_1: (0.3891) | Acc_1: (85.68%) (2303/2688)\n",
      "Epoch: 84 | Batch_idx: 30 |  Loss_1: (0.3929) | Acc_1: (85.61%) (3397/3968)\n",
      "Epoch: 84 | Batch_idx: 40 |  Loss_1: (0.3936) | Acc_1: (85.73%) (4499/5248)\n",
      "Epoch: 84 | Batch_idx: 50 |  Loss_1: (0.3865) | Acc_1: (86.00%) (5614/6528)\n",
      "Epoch: 84 | Batch_idx: 60 |  Loss_1: (0.3813) | Acc_1: (86.16%) (6727/7808)\n",
      "Epoch: 84 | Batch_idx: 70 |  Loss_1: (0.3867) | Acc_1: (85.97%) (7813/9088)\n",
      "Epoch: 84 | Batch_idx: 80 |  Loss_1: (0.3951) | Acc_1: (85.74%) (8890/10368)\n",
      "Epoch: 84 | Batch_idx: 90 |  Loss_1: (0.3995) | Acc_1: (85.51%) (9960/11648)\n",
      "Epoch: 84 | Batch_idx: 100 |  Loss_1: (0.3997) | Acc_1: (85.51%) (11055/12928)\n",
      "Epoch: 84 | Batch_idx: 110 |  Loss_1: (0.4027) | Acc_1: (85.36%) (12128/14208)\n",
      "Epoch: 84 | Batch_idx: 120 |  Loss_1: (0.4030) | Acc_1: (85.40%) (13226/15488)\n",
      "Epoch: 84 | Batch_idx: 130 |  Loss_1: (0.4055) | Acc_1: (85.28%) (14300/16768)\n",
      "Epoch: 84 | Batch_idx: 140 |  Loss_1: (0.4058) | Acc_1: (85.28%) (15392/18048)\n",
      "Epoch: 84 | Batch_idx: 150 |  Loss_1: (0.4069) | Acc_1: (85.21%) (16470/19328)\n",
      "Epoch: 84 | Batch_idx: 160 |  Loss_1: (0.4081) | Acc_1: (85.21%) (17560/20608)\n",
      "Epoch: 84 | Batch_idx: 170 |  Loss_1: (0.4051) | Acc_1: (85.27%) (18664/21888)\n",
      "Epoch: 84 | Batch_idx: 180 |  Loss_1: (0.4026) | Acc_1: (85.37%) (19779/23168)\n",
      "Epoch: 84 | Batch_idx: 190 |  Loss_1: (0.4021) | Acc_1: (85.44%) (20889/24448)\n",
      "Epoch: 84 | Batch_idx: 200 |  Loss_1: (0.4039) | Acc_1: (85.37%) (21965/25728)\n",
      "Epoch: 84 | Batch_idx: 210 |  Loss_1: (0.4049) | Acc_1: (85.32%) (23042/27008)\n",
      "Epoch: 84 | Batch_idx: 220 |  Loss_1: (0.4088) | Acc_1: (85.17%) (24093/28288)\n",
      "Epoch: 84 | Batch_idx: 230 |  Loss_1: (0.4084) | Acc_1: (85.18%) (25187/29568)\n",
      "Epoch: 84 | Batch_idx: 240 |  Loss_1: (0.4088) | Acc_1: (85.18%) (26275/30848)\n",
      "Epoch: 84 | Batch_idx: 250 |  Loss_1: (0.4080) | Acc_1: (85.24%) (27385/32128)\n",
      "Epoch: 84 | Batch_idx: 260 |  Loss_1: (0.4063) | Acc_1: (85.28%) (28489/33408)\n",
      "Epoch: 84 | Batch_idx: 270 |  Loss_1: (0.4069) | Acc_1: (85.29%) (29585/34688)\n",
      "Epoch: 84 | Batch_idx: 280 |  Loss_1: (0.4067) | Acc_1: (85.31%) (30684/35968)\n",
      "Epoch: 84 | Batch_idx: 290 |  Loss_1: (0.4084) | Acc_1: (85.26%) (31759/37248)\n",
      "Epoch: 84 | Batch_idx: 300 |  Loss_1: (0.4096) | Acc_1: (85.23%) (32837/38528)\n",
      "Epoch: 84 | Batch_idx: 310 |  Loss_1: (0.4107) | Acc_1: (85.16%) (33899/39808)\n",
      "Epoch: 84 | Batch_idx: 320 |  Loss_1: (0.4103) | Acc_1: (85.16%) (34991/41088)\n",
      "Epoch: 84 | Batch_idx: 330 |  Loss_1: (0.4100) | Acc_1: (85.16%) (36082/42368)\n",
      "Epoch: 84 | Batch_idx: 340 |  Loss_1: (0.4102) | Acc_1: (85.17%) (37175/43648)\n",
      "Epoch: 84 | Batch_idx: 350 |  Loss_1: (0.4101) | Acc_1: (85.17%) (38264/44928)\n",
      "Epoch: 84 | Batch_idx: 360 |  Loss_1: (0.4094) | Acc_1: (85.20%) (39367/46208)\n",
      "Epoch: 84 | Batch_idx: 370 |  Loss_1: (0.4097) | Acc_1: (85.18%) (40452/47488)\n",
      "Epoch: 84 | Batch_idx: 380 |  Loss_1: (0.4094) | Acc_1: (85.18%) (41540/48768)\n",
      "Epoch: 84 | Batch_idx: 390 |  Loss_1: (0.4089) | Acc_1: (85.19%) (42596/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2798) | Acc: (90.70%) (9070/10000)\n",
      "Epoch: 85 | Batch_idx: 0 |  Loss_1: (0.3477) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 85 | Batch_idx: 10 |  Loss_1: (0.3689) | Acc_1: (86.43%) (1217/1408)\n",
      "Epoch: 85 | Batch_idx: 20 |  Loss_1: (0.3818) | Acc_1: (85.86%) (2308/2688)\n",
      "Epoch: 85 | Batch_idx: 30 |  Loss_1: (0.4132) | Acc_1: (84.50%) (3353/3968)\n",
      "Epoch: 85 | Batch_idx: 40 |  Loss_1: (0.4153) | Acc_1: (84.43%) (4431/5248)\n",
      "Epoch: 85 | Batch_idx: 50 |  Loss_1: (0.4209) | Acc_1: (84.27%) (5501/6528)\n",
      "Epoch: 85 | Batch_idx: 60 |  Loss_1: (0.4241) | Acc_1: (84.29%) (6581/7808)\n",
      "Epoch: 85 | Batch_idx: 70 |  Loss_1: (0.4206) | Acc_1: (84.40%) (7670/9088)\n",
      "Epoch: 85 | Batch_idx: 80 |  Loss_1: (0.4209) | Acc_1: (84.39%) (8750/10368)\n",
      "Epoch: 85 | Batch_idx: 90 |  Loss_1: (0.4246) | Acc_1: (84.29%) (9818/11648)\n",
      "Epoch: 85 | Batch_idx: 100 |  Loss_1: (0.4248) | Acc_1: (84.30%) (10898/12928)\n",
      "Epoch: 85 | Batch_idx: 110 |  Loss_1: (0.4193) | Acc_1: (84.52%) (12009/14208)\n",
      "Epoch: 85 | Batch_idx: 120 |  Loss_1: (0.4178) | Acc_1: (84.68%) (13116/15488)\n",
      "Epoch: 85 | Batch_idx: 130 |  Loss_1: (0.4137) | Acc_1: (84.84%) (14226/16768)\n",
      "Epoch: 85 | Batch_idx: 140 |  Loss_1: (0.4168) | Acc_1: (84.80%) (15305/18048)\n",
      "Epoch: 85 | Batch_idx: 150 |  Loss_1: (0.4163) | Acc_1: (84.81%) (16393/19328)\n",
      "Epoch: 85 | Batch_idx: 160 |  Loss_1: (0.4178) | Acc_1: (84.78%) (17471/20608)\n",
      "Epoch: 85 | Batch_idx: 170 |  Loss_1: (0.4188) | Acc_1: (84.75%) (18550/21888)\n",
      "Epoch: 85 | Batch_idx: 180 |  Loss_1: (0.4184) | Acc_1: (84.73%) (19630/23168)\n",
      "Epoch: 85 | Batch_idx: 190 |  Loss_1: (0.4176) | Acc_1: (84.76%) (20723/24448)\n",
      "Epoch: 85 | Batch_idx: 200 |  Loss_1: (0.4147) | Acc_1: (84.88%) (21838/25728)\n",
      "Epoch: 85 | Batch_idx: 210 |  Loss_1: (0.4140) | Acc_1: (84.88%) (22925/27008)\n",
      "Epoch: 85 | Batch_idx: 220 |  Loss_1: (0.4117) | Acc_1: (84.95%) (24031/28288)\n",
      "Epoch: 85 | Batch_idx: 230 |  Loss_1: (0.4127) | Acc_1: (84.93%) (25113/29568)\n",
      "Epoch: 85 | Batch_idx: 240 |  Loss_1: (0.4142) | Acc_1: (84.89%) (26188/30848)\n",
      "Epoch: 85 | Batch_idx: 250 |  Loss_1: (0.4150) | Acc_1: (84.86%) (27265/32128)\n",
      "Epoch: 85 | Batch_idx: 260 |  Loss_1: (0.4161) | Acc_1: (84.84%) (28343/33408)\n",
      "Epoch: 85 | Batch_idx: 270 |  Loss_1: (0.4157) | Acc_1: (84.84%) (29431/34688)\n",
      "Epoch: 85 | Batch_idx: 280 |  Loss_1: (0.4157) | Acc_1: (84.85%) (30518/35968)\n",
      "Epoch: 85 | Batch_idx: 290 |  Loss_1: (0.4153) | Acc_1: (84.89%) (31618/37248)\n",
      "Epoch: 85 | Batch_idx: 300 |  Loss_1: (0.4156) | Acc_1: (84.91%) (32714/38528)\n",
      "Epoch: 85 | Batch_idx: 310 |  Loss_1: (0.4152) | Acc_1: (84.93%) (33809/39808)\n",
      "Epoch: 85 | Batch_idx: 320 |  Loss_1: (0.4160) | Acc_1: (84.90%) (34885/41088)\n",
      "Epoch: 85 | Batch_idx: 330 |  Loss_1: (0.4166) | Acc_1: (84.86%) (35952/42368)\n",
      "Epoch: 85 | Batch_idx: 340 |  Loss_1: (0.4163) | Acc_1: (84.87%) (37043/43648)\n",
      "Epoch: 85 | Batch_idx: 350 |  Loss_1: (0.4175) | Acc_1: (84.82%) (38110/44928)\n",
      "Epoch: 85 | Batch_idx: 360 |  Loss_1: (0.4177) | Acc_1: (84.82%) (39195/46208)\n",
      "Epoch: 85 | Batch_idx: 370 |  Loss_1: (0.4180) | Acc_1: (84.80%) (40270/47488)\n",
      "Epoch: 85 | Batch_idx: 380 |  Loss_1: (0.4188) | Acc_1: (84.76%) (41338/48768)\n",
      "Epoch: 85 | Batch_idx: 390 |  Loss_1: (0.4177) | Acc_1: (84.79%) (42393/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2791) | Acc: (90.58%) (9058/10000)\n",
      "Epoch: 86 | Batch_idx: 0 |  Loss_1: (0.4755) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 86 | Batch_idx: 10 |  Loss_1: (0.4133) | Acc_1: (85.30%) (1201/1408)\n",
      "Epoch: 86 | Batch_idx: 20 |  Loss_1: (0.4162) | Acc_1: (84.97%) (2284/2688)\n",
      "Epoch: 86 | Batch_idx: 30 |  Loss_1: (0.4063) | Acc_1: (85.16%) (3379/3968)\n",
      "Epoch: 86 | Batch_idx: 40 |  Loss_1: (0.3972) | Acc_1: (85.58%) (4491/5248)\n",
      "Epoch: 86 | Batch_idx: 50 |  Loss_1: (0.3936) | Acc_1: (85.54%) (5584/6528)\n",
      "Epoch: 86 | Batch_idx: 60 |  Loss_1: (0.3949) | Acc_1: (85.49%) (6675/7808)\n",
      "Epoch: 86 | Batch_idx: 70 |  Loss_1: (0.3950) | Acc_1: (85.50%) (7770/9088)\n",
      "Epoch: 86 | Batch_idx: 80 |  Loss_1: (0.3998) | Acc_1: (85.38%) (8852/10368)\n",
      "Epoch: 86 | Batch_idx: 90 |  Loss_1: (0.3956) | Acc_1: (85.52%) (9961/11648)\n",
      "Epoch: 86 | Batch_idx: 100 |  Loss_1: (0.3991) | Acc_1: (85.40%) (11040/12928)\n",
      "Epoch: 86 | Batch_idx: 110 |  Loss_1: (0.4014) | Acc_1: (85.38%) (12131/14208)\n",
      "Epoch: 86 | Batch_idx: 120 |  Loss_1: (0.3975) | Acc_1: (85.56%) (13251/15488)\n",
      "Epoch: 86 | Batch_idx: 130 |  Loss_1: (0.3985) | Acc_1: (85.53%) (14342/16768)\n",
      "Epoch: 86 | Batch_idx: 140 |  Loss_1: (0.4002) | Acc_1: (85.49%) (15429/18048)\n",
      "Epoch: 86 | Batch_idx: 150 |  Loss_1: (0.4016) | Acc_1: (85.41%) (16509/19328)\n",
      "Epoch: 86 | Batch_idx: 160 |  Loss_1: (0.3996) | Acc_1: (85.44%) (17607/20608)\n",
      "Epoch: 86 | Batch_idx: 170 |  Loss_1: (0.4010) | Acc_1: (85.38%) (18688/21888)\n",
      "Epoch: 86 | Batch_idx: 180 |  Loss_1: (0.4017) | Acc_1: (85.39%) (19784/23168)\n",
      "Epoch: 86 | Batch_idx: 190 |  Loss_1: (0.4035) | Acc_1: (85.33%) (20862/24448)\n",
      "Epoch: 86 | Batch_idx: 200 |  Loss_1: (0.4032) | Acc_1: (85.32%) (21951/25728)\n",
      "Epoch: 86 | Batch_idx: 210 |  Loss_1: (0.4033) | Acc_1: (85.31%) (23040/27008)\n",
      "Epoch: 86 | Batch_idx: 220 |  Loss_1: (0.4039) | Acc_1: (85.33%) (24138/28288)\n",
      "Epoch: 86 | Batch_idx: 230 |  Loss_1: (0.4030) | Acc_1: (85.34%) (25233/29568)\n",
      "Epoch: 86 | Batch_idx: 240 |  Loss_1: (0.4035) | Acc_1: (85.33%) (26322/30848)\n",
      "Epoch: 86 | Batch_idx: 250 |  Loss_1: (0.4033) | Acc_1: (85.33%) (27414/32128)\n",
      "Epoch: 86 | Batch_idx: 260 |  Loss_1: (0.4031) | Acc_1: (85.33%) (28506/33408)\n",
      "Epoch: 86 | Batch_idx: 270 |  Loss_1: (0.4027) | Acc_1: (85.33%) (29601/34688)\n",
      "Epoch: 86 | Batch_idx: 280 |  Loss_1: (0.4021) | Acc_1: (85.38%) (30710/35968)\n",
      "Epoch: 86 | Batch_idx: 290 |  Loss_1: (0.4022) | Acc_1: (85.37%) (31800/37248)\n",
      "Epoch: 86 | Batch_idx: 300 |  Loss_1: (0.4042) | Acc_1: (85.31%) (32869/38528)\n",
      "Epoch: 86 | Batch_idx: 310 |  Loss_1: (0.4040) | Acc_1: (85.32%) (33966/39808)\n",
      "Epoch: 86 | Batch_idx: 320 |  Loss_1: (0.4041) | Acc_1: (85.31%) (35053/41088)\n",
      "Epoch: 86 | Batch_idx: 330 |  Loss_1: (0.4042) | Acc_1: (85.31%) (36144/42368)\n",
      "Epoch: 86 | Batch_idx: 340 |  Loss_1: (0.4038) | Acc_1: (85.30%) (37231/43648)\n",
      "Epoch: 86 | Batch_idx: 350 |  Loss_1: (0.4050) | Acc_1: (85.28%) (38316/44928)\n",
      "Epoch: 86 | Batch_idx: 360 |  Loss_1: (0.4051) | Acc_1: (85.28%) (39404/46208)\n",
      "Epoch: 86 | Batch_idx: 370 |  Loss_1: (0.4072) | Acc_1: (85.22%) (40468/47488)\n",
      "Epoch: 86 | Batch_idx: 380 |  Loss_1: (0.4069) | Acc_1: (85.22%) (41562/48768)\n",
      "Epoch: 86 | Batch_idx: 390 |  Loss_1: (0.4075) | Acc_1: (85.20%) (42598/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2752) | Acc: (90.77%) (9077/10000)\n",
      "Epoch: 87 | Batch_idx: 0 |  Loss_1: (0.3537) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 87 | Batch_idx: 10 |  Loss_1: (0.4055) | Acc_1: (84.30%) (1187/1408)\n",
      "Epoch: 87 | Batch_idx: 20 |  Loss_1: (0.3920) | Acc_1: (85.53%) (2299/2688)\n",
      "Epoch: 87 | Batch_idx: 30 |  Loss_1: (0.4146) | Acc_1: (84.93%) (3370/3968)\n",
      "Epoch: 87 | Batch_idx: 40 |  Loss_1: (0.3993) | Acc_1: (85.59%) (4492/5248)\n",
      "Epoch: 87 | Batch_idx: 50 |  Loss_1: (0.3942) | Acc_1: (85.68%) (5593/6528)\n",
      "Epoch: 87 | Batch_idx: 60 |  Loss_1: (0.3942) | Acc_1: (85.67%) (6689/7808)\n",
      "Epoch: 87 | Batch_idx: 70 |  Loss_1: (0.3954) | Acc_1: (85.66%) (7785/9088)\n",
      "Epoch: 87 | Batch_idx: 80 |  Loss_1: (0.3927) | Acc_1: (85.76%) (8892/10368)\n",
      "Epoch: 87 | Batch_idx: 90 |  Loss_1: (0.3901) | Acc_1: (85.91%) (10007/11648)\n",
      "Epoch: 87 | Batch_idx: 100 |  Loss_1: (0.3909) | Acc_1: (85.91%) (11106/12928)\n",
      "Epoch: 87 | Batch_idx: 110 |  Loss_1: (0.3891) | Acc_1: (85.99%) (12218/14208)\n",
      "Epoch: 87 | Batch_idx: 120 |  Loss_1: (0.3886) | Acc_1: (85.94%) (13310/15488)\n",
      "Epoch: 87 | Batch_idx: 130 |  Loss_1: (0.3899) | Acc_1: (85.84%) (14394/16768)\n",
      "Epoch: 87 | Batch_idx: 140 |  Loss_1: (0.3905) | Acc_1: (85.85%) (15495/18048)\n",
      "Epoch: 87 | Batch_idx: 150 |  Loss_1: (0.3897) | Acc_1: (85.87%) (16596/19328)\n",
      "Epoch: 87 | Batch_idx: 160 |  Loss_1: (0.3900) | Acc_1: (85.86%) (17693/20608)\n",
      "Epoch: 87 | Batch_idx: 170 |  Loss_1: (0.3904) | Acc_1: (85.81%) (18781/21888)\n",
      "Epoch: 87 | Batch_idx: 180 |  Loss_1: (0.3886) | Acc_1: (85.84%) (19887/23168)\n",
      "Epoch: 87 | Batch_idx: 190 |  Loss_1: (0.3884) | Acc_1: (85.81%) (20979/24448)\n",
      "Epoch: 87 | Batch_idx: 200 |  Loss_1: (0.3890) | Acc_1: (85.79%) (22073/25728)\n",
      "Epoch: 87 | Batch_idx: 210 |  Loss_1: (0.3879) | Acc_1: (85.84%) (23183/27008)\n",
      "Epoch: 87 | Batch_idx: 220 |  Loss_1: (0.3869) | Acc_1: (85.87%) (24290/28288)\n",
      "Epoch: 87 | Batch_idx: 230 |  Loss_1: (0.3861) | Acc_1: (85.90%) (25398/29568)\n",
      "Epoch: 87 | Batch_idx: 240 |  Loss_1: (0.3874) | Acc_1: (85.83%) (26478/30848)\n",
      "Epoch: 87 | Batch_idx: 250 |  Loss_1: (0.3864) | Acc_1: (85.88%) (27593/32128)\n",
      "Epoch: 87 | Batch_idx: 260 |  Loss_1: (0.3860) | Acc_1: (85.86%) (28684/33408)\n",
      "Epoch: 87 | Batch_idx: 270 |  Loss_1: (0.3869) | Acc_1: (85.85%) (29781/34688)\n",
      "Epoch: 87 | Batch_idx: 280 |  Loss_1: (0.3869) | Acc_1: (85.88%) (30888/35968)\n",
      "Epoch: 87 | Batch_idx: 290 |  Loss_1: (0.3893) | Acc_1: (85.79%) (31955/37248)\n",
      "Epoch: 87 | Batch_idx: 300 |  Loss_1: (0.3874) | Acc_1: (85.82%) (33065/38528)\n",
      "Epoch: 87 | Batch_idx: 310 |  Loss_1: (0.3888) | Acc_1: (85.75%) (34134/39808)\n",
      "Epoch: 87 | Batch_idx: 320 |  Loss_1: (0.3887) | Acc_1: (85.75%) (35235/41088)\n",
      "Epoch: 87 | Batch_idx: 330 |  Loss_1: (0.3889) | Acc_1: (85.76%) (36333/42368)\n",
      "Epoch: 87 | Batch_idx: 340 |  Loss_1: (0.3895) | Acc_1: (85.73%) (37421/43648)\n",
      "Epoch: 87 | Batch_idx: 350 |  Loss_1: (0.3891) | Acc_1: (85.74%) (38520/44928)\n",
      "Epoch: 87 | Batch_idx: 360 |  Loss_1: (0.3906) | Acc_1: (85.68%) (39592/46208)\n",
      "Epoch: 87 | Batch_idx: 370 |  Loss_1: (0.3932) | Acc_1: (85.57%) (40637/47488)\n",
      "Epoch: 87 | Batch_idx: 380 |  Loss_1: (0.3938) | Acc_1: (85.57%) (41729/48768)\n",
      "Epoch: 87 | Batch_idx: 390 |  Loss_1: (0.3944) | Acc_1: (85.52%) (42761/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3102) | Acc: (89.75%) (8975/10000)\n",
      "Epoch: 88 | Batch_idx: 0 |  Loss_1: (0.4111) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 88 | Batch_idx: 10 |  Loss_1: (0.4109) | Acc_1: (84.73%) (1193/1408)\n",
      "Epoch: 88 | Batch_idx: 20 |  Loss_1: (0.3979) | Acc_1: (85.23%) (2291/2688)\n",
      "Epoch: 88 | Batch_idx: 30 |  Loss_1: (0.4023) | Acc_1: (85.23%) (3382/3968)\n",
      "Epoch: 88 | Batch_idx: 40 |  Loss_1: (0.3983) | Acc_1: (85.23%) (4473/5248)\n",
      "Epoch: 88 | Batch_idx: 50 |  Loss_1: (0.4034) | Acc_1: (84.93%) (5544/6528)\n",
      "Epoch: 88 | Batch_idx: 60 |  Loss_1: (0.4068) | Acc_1: (84.80%) (6621/7808)\n",
      "Epoch: 88 | Batch_idx: 70 |  Loss_1: (0.4019) | Acc_1: (84.99%) (7724/9088)\n",
      "Epoch: 88 | Batch_idx: 80 |  Loss_1: (0.4054) | Acc_1: (84.85%) (8797/10368)\n",
      "Epoch: 88 | Batch_idx: 90 |  Loss_1: (0.4055) | Acc_1: (84.91%) (9890/11648)\n",
      "Epoch: 88 | Batch_idx: 100 |  Loss_1: (0.4060) | Acc_1: (84.93%) (10980/12928)\n",
      "Epoch: 88 | Batch_idx: 110 |  Loss_1: (0.4096) | Acc_1: (84.89%) (12061/14208)\n",
      "Epoch: 88 | Batch_idx: 120 |  Loss_1: (0.4095) | Acc_1: (84.84%) (13140/15488)\n",
      "Epoch: 88 | Batch_idx: 130 |  Loss_1: (0.4086) | Acc_1: (84.91%) (14238/16768)\n",
      "Epoch: 88 | Batch_idx: 140 |  Loss_1: (0.4072) | Acc_1: (84.99%) (15339/18048)\n",
      "Epoch: 88 | Batch_idx: 150 |  Loss_1: (0.4064) | Acc_1: (84.98%) (16424/19328)\n",
      "Epoch: 88 | Batch_idx: 160 |  Loss_1: (0.4056) | Acc_1: (84.97%) (17511/20608)\n",
      "Epoch: 88 | Batch_idx: 170 |  Loss_1: (0.4031) | Acc_1: (85.08%) (18622/21888)\n",
      "Epoch: 88 | Batch_idx: 180 |  Loss_1: (0.4022) | Acc_1: (85.13%) (19722/23168)\n",
      "Epoch: 88 | Batch_idx: 190 |  Loss_1: (0.4040) | Acc_1: (85.08%) (20801/24448)\n",
      "Epoch: 88 | Batch_idx: 200 |  Loss_1: (0.4029) | Acc_1: (85.15%) (21908/25728)\n",
      "Epoch: 88 | Batch_idx: 210 |  Loss_1: (0.4026) | Acc_1: (85.16%) (23000/27008)\n",
      "Epoch: 88 | Batch_idx: 220 |  Loss_1: (0.4010) | Acc_1: (85.27%) (24120/28288)\n",
      "Epoch: 88 | Batch_idx: 230 |  Loss_1: (0.3994) | Acc_1: (85.32%) (25227/29568)\n",
      "Epoch: 88 | Batch_idx: 240 |  Loss_1: (0.3990) | Acc_1: (85.32%) (26321/30848)\n",
      "Epoch: 88 | Batch_idx: 250 |  Loss_1: (0.3979) | Acc_1: (85.37%) (27427/32128)\n",
      "Epoch: 88 | Batch_idx: 260 |  Loss_1: (0.3965) | Acc_1: (85.43%) (28542/33408)\n",
      "Epoch: 88 | Batch_idx: 270 |  Loss_1: (0.3962) | Acc_1: (85.47%) (29648/34688)\n",
      "Epoch: 88 | Batch_idx: 280 |  Loss_1: (0.3972) | Acc_1: (85.43%) (30728/35968)\n",
      "Epoch: 88 | Batch_idx: 290 |  Loss_1: (0.3974) | Acc_1: (85.44%) (31824/37248)\n",
      "Epoch: 88 | Batch_idx: 300 |  Loss_1: (0.3975) | Acc_1: (85.43%) (32916/38528)\n",
      "Epoch: 88 | Batch_idx: 310 |  Loss_1: (0.3976) | Acc_1: (85.43%) (34007/39808)\n",
      "Epoch: 88 | Batch_idx: 320 |  Loss_1: (0.3979) | Acc_1: (85.43%) (35102/41088)\n",
      "Epoch: 88 | Batch_idx: 330 |  Loss_1: (0.3996) | Acc_1: (85.36%) (36164/42368)\n",
      "Epoch: 88 | Batch_idx: 340 |  Loss_1: (0.4004) | Acc_1: (85.31%) (37237/43648)\n",
      "Epoch: 88 | Batch_idx: 350 |  Loss_1: (0.4016) | Acc_1: (85.27%) (38311/44928)\n",
      "Epoch: 88 | Batch_idx: 360 |  Loss_1: (0.4036) | Acc_1: (85.19%) (39366/46208)\n",
      "Epoch: 88 | Batch_idx: 370 |  Loss_1: (0.4035) | Acc_1: (85.20%) (40462/47488)\n",
      "Epoch: 88 | Batch_idx: 380 |  Loss_1: (0.4034) | Acc_1: (85.20%) (41552/48768)\n",
      "Epoch: 88 | Batch_idx: 390 |  Loss_1: (0.4038) | Acc_1: (85.18%) (42591/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2963) | Acc: (90.05%) (9005/10000)\n",
      "Epoch: 89 | Batch_idx: 0 |  Loss_1: (0.4000) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 89 | Batch_idx: 10 |  Loss_1: (0.3799) | Acc_1: (86.29%) (1215/1408)\n",
      "Epoch: 89 | Batch_idx: 20 |  Loss_1: (0.3749) | Acc_1: (86.20%) (2317/2688)\n",
      "Epoch: 89 | Batch_idx: 30 |  Loss_1: (0.3731) | Acc_1: (86.19%) (3420/3968)\n",
      "Epoch: 89 | Batch_idx: 40 |  Loss_1: (0.3767) | Acc_1: (86.05%) (4516/5248)\n",
      "Epoch: 89 | Batch_idx: 50 |  Loss_1: (0.3839) | Acc_1: (85.98%) (5613/6528)\n",
      "Epoch: 89 | Batch_idx: 60 |  Loss_1: (0.3820) | Acc_1: (86.08%) (6721/7808)\n",
      "Epoch: 89 | Batch_idx: 70 |  Loss_1: (0.3822) | Acc_1: (86.03%) (7818/9088)\n",
      "Epoch: 89 | Batch_idx: 80 |  Loss_1: (0.3739) | Acc_1: (86.33%) (8951/10368)\n",
      "Epoch: 89 | Batch_idx: 90 |  Loss_1: (0.3726) | Acc_1: (86.44%) (10069/11648)\n",
      "Epoch: 89 | Batch_idx: 100 |  Loss_1: (0.3756) | Acc_1: (86.30%) (11157/12928)\n",
      "Epoch: 89 | Batch_idx: 110 |  Loss_1: (0.3770) | Acc_1: (86.19%) (12246/14208)\n",
      "Epoch: 89 | Batch_idx: 120 |  Loss_1: (0.3770) | Acc_1: (86.18%) (13348/15488)\n",
      "Epoch: 89 | Batch_idx: 130 |  Loss_1: (0.3739) | Acc_1: (86.24%) (14460/16768)\n",
      "Epoch: 89 | Batch_idx: 140 |  Loss_1: (0.3713) | Acc_1: (86.33%) (15580/18048)\n",
      "Epoch: 89 | Batch_idx: 150 |  Loss_1: (0.3744) | Acc_1: (86.21%) (16663/19328)\n",
      "Epoch: 89 | Batch_idx: 160 |  Loss_1: (0.3760) | Acc_1: (86.12%) (17748/20608)\n",
      "Epoch: 89 | Batch_idx: 170 |  Loss_1: (0.3795) | Acc_1: (86.04%) (18833/21888)\n",
      "Epoch: 89 | Batch_idx: 180 |  Loss_1: (0.3825) | Acc_1: (85.92%) (19907/23168)\n",
      "Epoch: 89 | Batch_idx: 190 |  Loss_1: (0.3840) | Acc_1: (85.87%) (20994/24448)\n",
      "Epoch: 89 | Batch_idx: 200 |  Loss_1: (0.3850) | Acc_1: (85.86%) (22091/25728)\n",
      "Epoch: 89 | Batch_idx: 210 |  Loss_1: (0.3854) | Acc_1: (85.86%) (23188/27008)\n",
      "Epoch: 89 | Batch_idx: 220 |  Loss_1: (0.3829) | Acc_1: (85.98%) (24321/28288)\n",
      "Epoch: 89 | Batch_idx: 230 |  Loss_1: (0.3826) | Acc_1: (85.97%) (25420/29568)\n",
      "Epoch: 89 | Batch_idx: 240 |  Loss_1: (0.3831) | Acc_1: (85.97%) (26521/30848)\n",
      "Epoch: 89 | Batch_idx: 250 |  Loss_1: (0.3842) | Acc_1: (85.91%) (27602/32128)\n",
      "Epoch: 89 | Batch_idx: 260 |  Loss_1: (0.3844) | Acc_1: (85.88%) (28692/33408)\n",
      "Epoch: 89 | Batch_idx: 270 |  Loss_1: (0.3861) | Acc_1: (85.85%) (29778/34688)\n",
      "Epoch: 89 | Batch_idx: 280 |  Loss_1: (0.3875) | Acc_1: (85.80%) (30861/35968)\n",
      "Epoch: 89 | Batch_idx: 290 |  Loss_1: (0.3874) | Acc_1: (85.81%) (31964/37248)\n",
      "Epoch: 89 | Batch_idx: 300 |  Loss_1: (0.3870) | Acc_1: (85.84%) (33071/38528)\n",
      "Epoch: 89 | Batch_idx: 310 |  Loss_1: (0.3874) | Acc_1: (85.83%) (34168/39808)\n",
      "Epoch: 89 | Batch_idx: 320 |  Loss_1: (0.3866) | Acc_1: (85.86%) (35277/41088)\n",
      "Epoch: 89 | Batch_idx: 330 |  Loss_1: (0.3876) | Acc_1: (85.82%) (36362/42368)\n",
      "Epoch: 89 | Batch_idx: 340 |  Loss_1: (0.3884) | Acc_1: (85.80%) (37448/43648)\n",
      "Epoch: 89 | Batch_idx: 350 |  Loss_1: (0.3890) | Acc_1: (85.77%) (38535/44928)\n",
      "Epoch: 89 | Batch_idx: 360 |  Loss_1: (0.3889) | Acc_1: (85.80%) (39645/46208)\n",
      "Epoch: 89 | Batch_idx: 370 |  Loss_1: (0.3906) | Acc_1: (85.74%) (40714/47488)\n",
      "Epoch: 89 | Batch_idx: 380 |  Loss_1: (0.3909) | Acc_1: (85.71%) (41800/48768)\n",
      "Epoch: 89 | Batch_idx: 390 |  Loss_1: (0.3909) | Acc_1: (85.71%) (42857/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2782) | Acc: (90.81%) (9081/10000)\n",
      "Epoch: 90 | Batch_idx: 0 |  Loss_1: (0.3154) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 90 | Batch_idx: 10 |  Loss_1: (0.3307) | Acc_1: (88.35%) (1244/1408)\n",
      "Epoch: 90 | Batch_idx: 20 |  Loss_1: (0.3339) | Acc_1: (88.02%) (2366/2688)\n",
      "Epoch: 90 | Batch_idx: 30 |  Loss_1: (0.3385) | Acc_1: (87.95%) (3490/3968)\n",
      "Epoch: 90 | Batch_idx: 40 |  Loss_1: (0.3497) | Acc_1: (87.63%) (4599/5248)\n",
      "Epoch: 90 | Batch_idx: 50 |  Loss_1: (0.3622) | Acc_1: (87.12%) (5687/6528)\n",
      "Epoch: 90 | Batch_idx: 60 |  Loss_1: (0.3653) | Acc_1: (86.67%) (6767/7808)\n",
      "Epoch: 90 | Batch_idx: 70 |  Loss_1: (0.3707) | Acc_1: (86.51%) (7862/9088)\n",
      "Epoch: 90 | Batch_idx: 80 |  Loss_1: (0.3731) | Acc_1: (86.38%) (8956/10368)\n",
      "Epoch: 90 | Batch_idx: 90 |  Loss_1: (0.3713) | Acc_1: (86.48%) (10073/11648)\n",
      "Epoch: 90 | Batch_idx: 100 |  Loss_1: (0.3760) | Acc_1: (86.35%) (11163/12928)\n",
      "Epoch: 90 | Batch_idx: 110 |  Loss_1: (0.3780) | Acc_1: (86.30%) (12262/14208)\n",
      "Epoch: 90 | Batch_idx: 120 |  Loss_1: (0.3787) | Acc_1: (86.29%) (13364/15488)\n",
      "Epoch: 90 | Batch_idx: 130 |  Loss_1: (0.3811) | Acc_1: (86.21%) (14456/16768)\n",
      "Epoch: 90 | Batch_idx: 140 |  Loss_1: (0.3808) | Acc_1: (86.24%) (15565/18048)\n",
      "Epoch: 90 | Batch_idx: 150 |  Loss_1: (0.3799) | Acc_1: (86.23%) (16667/19328)\n",
      "Epoch: 90 | Batch_idx: 160 |  Loss_1: (0.3800) | Acc_1: (86.23%) (17770/20608)\n",
      "Epoch: 90 | Batch_idx: 170 |  Loss_1: (0.3816) | Acc_1: (86.15%) (18857/21888)\n",
      "Epoch: 90 | Batch_idx: 180 |  Loss_1: (0.3822) | Acc_1: (86.13%) (19955/23168)\n",
      "Epoch: 90 | Batch_idx: 190 |  Loss_1: (0.3838) | Acc_1: (86.07%) (21043/24448)\n",
      "Epoch: 90 | Batch_idx: 200 |  Loss_1: (0.3865) | Acc_1: (85.98%) (22120/25728)\n",
      "Epoch: 90 | Batch_idx: 210 |  Loss_1: (0.3878) | Acc_1: (85.92%) (23206/27008)\n",
      "Epoch: 90 | Batch_idx: 220 |  Loss_1: (0.3888) | Acc_1: (85.90%) (24299/28288)\n",
      "Epoch: 90 | Batch_idx: 230 |  Loss_1: (0.3897) | Acc_1: (85.89%) (25395/29568)\n",
      "Epoch: 90 | Batch_idx: 240 |  Loss_1: (0.3901) | Acc_1: (85.83%) (26478/30848)\n",
      "Epoch: 90 | Batch_idx: 250 |  Loss_1: (0.3912) | Acc_1: (85.77%) (27556/32128)\n",
      "Epoch: 90 | Batch_idx: 260 |  Loss_1: (0.3910) | Acc_1: (85.75%) (28647/33408)\n",
      "Epoch: 90 | Batch_idx: 270 |  Loss_1: (0.3912) | Acc_1: (85.75%) (29745/34688)\n",
      "Epoch: 90 | Batch_idx: 280 |  Loss_1: (0.3908) | Acc_1: (85.74%) (30838/35968)\n",
      "Epoch: 90 | Batch_idx: 290 |  Loss_1: (0.3900) | Acc_1: (85.77%) (31947/37248)\n",
      "Epoch: 90 | Batch_idx: 300 |  Loss_1: (0.3910) | Acc_1: (85.75%) (33036/38528)\n",
      "Epoch: 90 | Batch_idx: 310 |  Loss_1: (0.3904) | Acc_1: (85.77%) (34142/39808)\n",
      "Epoch: 90 | Batch_idx: 320 |  Loss_1: (0.3915) | Acc_1: (85.74%) (35228/41088)\n",
      "Epoch: 90 | Batch_idx: 330 |  Loss_1: (0.3912) | Acc_1: (85.73%) (36323/42368)\n",
      "Epoch: 90 | Batch_idx: 340 |  Loss_1: (0.3900) | Acc_1: (85.75%) (37429/43648)\n",
      "Epoch: 90 | Batch_idx: 350 |  Loss_1: (0.3898) | Acc_1: (85.76%) (38532/44928)\n",
      "Epoch: 90 | Batch_idx: 360 |  Loss_1: (0.3895) | Acc_1: (85.79%) (39644/46208)\n",
      "Epoch: 90 | Batch_idx: 370 |  Loss_1: (0.3894) | Acc_1: (85.80%) (40745/47488)\n",
      "Epoch: 90 | Batch_idx: 380 |  Loss_1: (0.3898) | Acc_1: (85.79%) (41837/48768)\n",
      "Epoch: 90 | Batch_idx: 390 |  Loss_1: (0.3904) | Acc_1: (85.76%) (42879/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2996) | Acc: (90.24%) (9024/10000)\n",
      "Epoch: 91 | Batch_idx: 0 |  Loss_1: (0.3781) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 91 | Batch_idx: 10 |  Loss_1: (0.3764) | Acc_1: (86.15%) (1213/1408)\n",
      "Epoch: 91 | Batch_idx: 20 |  Loss_1: (0.3616) | Acc_1: (86.64%) (2329/2688)\n",
      "Epoch: 91 | Batch_idx: 30 |  Loss_1: (0.3666) | Acc_1: (86.16%) (3419/3968)\n",
      "Epoch: 91 | Batch_idx: 40 |  Loss_1: (0.3799) | Acc_1: (85.58%) (4491/5248)\n",
      "Epoch: 91 | Batch_idx: 50 |  Loss_1: (0.3851) | Acc_1: (85.42%) (5576/6528)\n",
      "Epoch: 91 | Batch_idx: 60 |  Loss_1: (0.3787) | Acc_1: (85.78%) (6698/7808)\n",
      "Epoch: 91 | Batch_idx: 70 |  Loss_1: (0.3842) | Acc_1: (85.57%) (7777/9088)\n",
      "Epoch: 91 | Batch_idx: 80 |  Loss_1: (0.3842) | Acc_1: (85.62%) (8877/10368)\n",
      "Epoch: 91 | Batch_idx: 90 |  Loss_1: (0.3806) | Acc_1: (85.75%) (9988/11648)\n",
      "Epoch: 91 | Batch_idx: 100 |  Loss_1: (0.3794) | Acc_1: (85.78%) (11090/12928)\n",
      "Epoch: 91 | Batch_idx: 110 |  Loss_1: (0.3792) | Acc_1: (85.73%) (12180/14208)\n",
      "Epoch: 91 | Batch_idx: 120 |  Loss_1: (0.3786) | Acc_1: (85.77%) (13284/15488)\n",
      "Epoch: 91 | Batch_idx: 130 |  Loss_1: (0.3772) | Acc_1: (85.90%) (14404/16768)\n",
      "Epoch: 91 | Batch_idx: 140 |  Loss_1: (0.3778) | Acc_1: (85.88%) (15500/18048)\n",
      "Epoch: 91 | Batch_idx: 150 |  Loss_1: (0.3789) | Acc_1: (85.92%) (16607/19328)\n",
      "Epoch: 91 | Batch_idx: 160 |  Loss_1: (0.3806) | Acc_1: (85.90%) (17703/20608)\n",
      "Epoch: 91 | Batch_idx: 170 |  Loss_1: (0.3832) | Acc_1: (85.84%) (18788/21888)\n",
      "Epoch: 91 | Batch_idx: 180 |  Loss_1: (0.3831) | Acc_1: (85.83%) (19886/23168)\n",
      "Epoch: 91 | Batch_idx: 190 |  Loss_1: (0.3823) | Acc_1: (85.81%) (20978/24448)\n",
      "Epoch: 91 | Batch_idx: 200 |  Loss_1: (0.3818) | Acc_1: (85.84%) (22085/25728)\n",
      "Epoch: 91 | Batch_idx: 210 |  Loss_1: (0.3811) | Acc_1: (85.84%) (23185/27008)\n",
      "Epoch: 91 | Batch_idx: 220 |  Loss_1: (0.3805) | Acc_1: (85.86%) (24288/28288)\n",
      "Epoch: 91 | Batch_idx: 230 |  Loss_1: (0.3817) | Acc_1: (85.88%) (25394/29568)\n",
      "Epoch: 91 | Batch_idx: 240 |  Loss_1: (0.3816) | Acc_1: (85.93%) (26509/30848)\n",
      "Epoch: 91 | Batch_idx: 250 |  Loss_1: (0.3841) | Acc_1: (85.85%) (27582/32128)\n",
      "Epoch: 91 | Batch_idx: 260 |  Loss_1: (0.3826) | Acc_1: (85.91%) (28702/33408)\n",
      "Epoch: 91 | Batch_idx: 270 |  Loss_1: (0.3827) | Acc_1: (85.90%) (29797/34688)\n",
      "Epoch: 91 | Batch_idx: 280 |  Loss_1: (0.3839) | Acc_1: (85.88%) (30890/35968)\n",
      "Epoch: 91 | Batch_idx: 290 |  Loss_1: (0.3839) | Acc_1: (85.89%) (31992/37248)\n",
      "Epoch: 91 | Batch_idx: 300 |  Loss_1: (0.3846) | Acc_1: (85.87%) (33083/38528)\n",
      "Epoch: 91 | Batch_idx: 310 |  Loss_1: (0.3848) | Acc_1: (85.86%) (34181/39808)\n",
      "Epoch: 91 | Batch_idx: 320 |  Loss_1: (0.3834) | Acc_1: (85.91%) (35299/41088)\n",
      "Epoch: 91 | Batch_idx: 330 |  Loss_1: (0.3835) | Acc_1: (85.91%) (36400/42368)\n",
      "Epoch: 91 | Batch_idx: 340 |  Loss_1: (0.3859) | Acc_1: (85.82%) (37460/43648)\n",
      "Epoch: 91 | Batch_idx: 350 |  Loss_1: (0.3852) | Acc_1: (85.82%) (38559/44928)\n",
      "Epoch: 91 | Batch_idx: 360 |  Loss_1: (0.3849) | Acc_1: (85.83%) (39659/46208)\n",
      "Epoch: 91 | Batch_idx: 370 |  Loss_1: (0.3857) | Acc_1: (85.82%) (40752/47488)\n",
      "Epoch: 91 | Batch_idx: 380 |  Loss_1: (0.3853) | Acc_1: (85.83%) (41857/48768)\n",
      "Epoch: 91 | Batch_idx: 390 |  Loss_1: (0.3862) | Acc_1: (85.78%) (42892/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3151) | Acc: (89.28%) (8928/10000)\n",
      "Epoch: 92 | Batch_idx: 0 |  Loss_1: (0.2600) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 92 | Batch_idx: 10 |  Loss_1: (0.3384) | Acc_1: (86.93%) (1224/1408)\n",
      "Epoch: 92 | Batch_idx: 20 |  Loss_1: (0.3511) | Acc_1: (86.72%) (2331/2688)\n",
      "Epoch: 92 | Batch_idx: 30 |  Loss_1: (0.3661) | Acc_1: (86.34%) (3426/3968)\n",
      "Epoch: 92 | Batch_idx: 40 |  Loss_1: (0.3599) | Acc_1: (86.60%) (4545/5248)\n",
      "Epoch: 92 | Batch_idx: 50 |  Loss_1: (0.3587) | Acc_1: (86.61%) (5654/6528)\n",
      "Epoch: 92 | Batch_idx: 60 |  Loss_1: (0.3625) | Acc_1: (86.42%) (6748/7808)\n",
      "Epoch: 92 | Batch_idx: 70 |  Loss_1: (0.3579) | Acc_1: (86.64%) (7874/9088)\n",
      "Epoch: 92 | Batch_idx: 80 |  Loss_1: (0.3616) | Acc_1: (86.61%) (8980/10368)\n",
      "Epoch: 92 | Batch_idx: 90 |  Loss_1: (0.3640) | Acc_1: (86.50%) (10075/11648)\n",
      "Epoch: 92 | Batch_idx: 100 |  Loss_1: (0.3665) | Acc_1: (86.42%) (11173/12928)\n",
      "Epoch: 92 | Batch_idx: 110 |  Loss_1: (0.3661) | Acc_1: (86.51%) (12291/14208)\n",
      "Epoch: 92 | Batch_idx: 120 |  Loss_1: (0.3700) | Acc_1: (86.40%) (13382/15488)\n",
      "Epoch: 92 | Batch_idx: 130 |  Loss_1: (0.3704) | Acc_1: (86.43%) (14493/16768)\n",
      "Epoch: 92 | Batch_idx: 140 |  Loss_1: (0.3705) | Acc_1: (86.41%) (15595/18048)\n",
      "Epoch: 92 | Batch_idx: 150 |  Loss_1: (0.3696) | Acc_1: (86.43%) (16705/19328)\n",
      "Epoch: 92 | Batch_idx: 160 |  Loss_1: (0.3708) | Acc_1: (86.40%) (17806/20608)\n",
      "Epoch: 92 | Batch_idx: 170 |  Loss_1: (0.3719) | Acc_1: (86.30%) (18890/21888)\n",
      "Epoch: 92 | Batch_idx: 180 |  Loss_1: (0.3730) | Acc_1: (86.25%) (19982/23168)\n",
      "Epoch: 92 | Batch_idx: 190 |  Loss_1: (0.3742) | Acc_1: (86.23%) (21082/24448)\n",
      "Epoch: 92 | Batch_idx: 200 |  Loss_1: (0.3746) | Acc_1: (86.24%) (22188/25728)\n",
      "Epoch: 92 | Batch_idx: 210 |  Loss_1: (0.3735) | Acc_1: (86.27%) (23301/27008)\n",
      "Epoch: 92 | Batch_idx: 220 |  Loss_1: (0.3725) | Acc_1: (86.31%) (24416/28288)\n",
      "Epoch: 92 | Batch_idx: 230 |  Loss_1: (0.3719) | Acc_1: (86.35%) (25532/29568)\n",
      "Epoch: 92 | Batch_idx: 240 |  Loss_1: (0.3728) | Acc_1: (86.30%) (26622/30848)\n",
      "Epoch: 92 | Batch_idx: 250 |  Loss_1: (0.3729) | Acc_1: (86.29%) (27724/32128)\n",
      "Epoch: 92 | Batch_idx: 260 |  Loss_1: (0.3728) | Acc_1: (86.28%) (28826/33408)\n",
      "Epoch: 92 | Batch_idx: 270 |  Loss_1: (0.3741) | Acc_1: (86.22%) (29907/34688)\n",
      "Epoch: 92 | Batch_idx: 280 |  Loss_1: (0.3733) | Acc_1: (86.23%) (31014/35968)\n",
      "Epoch: 92 | Batch_idx: 290 |  Loss_1: (0.3738) | Acc_1: (86.22%) (32115/37248)\n",
      "Epoch: 92 | Batch_idx: 300 |  Loss_1: (0.3738) | Acc_1: (86.21%) (33215/38528)\n",
      "Epoch: 92 | Batch_idx: 310 |  Loss_1: (0.3724) | Acc_1: (86.28%) (34346/39808)\n",
      "Epoch: 92 | Batch_idx: 320 |  Loss_1: (0.3725) | Acc_1: (86.29%) (35455/41088)\n",
      "Epoch: 92 | Batch_idx: 330 |  Loss_1: (0.3728) | Acc_1: (86.28%) (36556/42368)\n",
      "Epoch: 92 | Batch_idx: 340 |  Loss_1: (0.3725) | Acc_1: (86.28%) (37658/43648)\n",
      "Epoch: 92 | Batch_idx: 350 |  Loss_1: (0.3719) | Acc_1: (86.29%) (38770/44928)\n",
      "Epoch: 92 | Batch_idx: 360 |  Loss_1: (0.3724) | Acc_1: (86.28%) (39868/46208)\n",
      "Epoch: 92 | Batch_idx: 370 |  Loss_1: (0.3732) | Acc_1: (86.26%) (40962/47488)\n",
      "Epoch: 92 | Batch_idx: 380 |  Loss_1: (0.3732) | Acc_1: (86.24%) (42058/48768)\n",
      "Epoch: 92 | Batch_idx: 390 |  Loss_1: (0.3744) | Acc_1: (86.21%) (43106/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2974) | Acc: (90.23%) (9023/10000)\n",
      "Epoch: 93 | Batch_idx: 0 |  Loss_1: (0.3286) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 93 | Batch_idx: 10 |  Loss_1: (0.3837) | Acc_1: (85.51%) (1204/1408)\n",
      "Epoch: 93 | Batch_idx: 20 |  Loss_1: (0.3781) | Acc_1: (85.90%) (2309/2688)\n",
      "Epoch: 93 | Batch_idx: 30 |  Loss_1: (0.3676) | Acc_1: (86.27%) (3423/3968)\n",
      "Epoch: 93 | Batch_idx: 40 |  Loss_1: (0.3700) | Acc_1: (86.28%) (4528/5248)\n",
      "Epoch: 93 | Batch_idx: 50 |  Loss_1: (0.3663) | Acc_1: (86.52%) (5648/6528)\n",
      "Epoch: 93 | Batch_idx: 60 |  Loss_1: (0.3625) | Acc_1: (86.58%) (6760/7808)\n",
      "Epoch: 93 | Batch_idx: 70 |  Loss_1: (0.3607) | Acc_1: (86.70%) (7879/9088)\n",
      "Epoch: 93 | Batch_idx: 80 |  Loss_1: (0.3624) | Acc_1: (86.66%) (8985/10368)\n",
      "Epoch: 93 | Batch_idx: 90 |  Loss_1: (0.3670) | Acc_1: (86.57%) (10084/11648)\n",
      "Epoch: 93 | Batch_idx: 100 |  Loss_1: (0.3694) | Acc_1: (86.44%) (11175/12928)\n",
      "Epoch: 93 | Batch_idx: 110 |  Loss_1: (0.3683) | Acc_1: (86.44%) (12282/14208)\n",
      "Epoch: 93 | Batch_idx: 120 |  Loss_1: (0.3653) | Acc_1: (86.56%) (13406/15488)\n",
      "Epoch: 93 | Batch_idx: 130 |  Loss_1: (0.3669) | Acc_1: (86.56%) (14514/16768)\n",
      "Epoch: 93 | Batch_idx: 140 |  Loss_1: (0.3757) | Acc_1: (86.21%) (15559/18048)\n",
      "Epoch: 93 | Batch_idx: 150 |  Loss_1: (0.3860) | Acc_1: (85.97%) (16616/19328)\n",
      "Epoch: 93 | Batch_idx: 160 |  Loss_1: (0.3950) | Acc_1: (85.79%) (17680/20608)\n",
      "Epoch: 93 | Batch_idx: 170 |  Loss_1: (0.4002) | Acc_1: (85.62%) (18740/21888)\n",
      "Epoch: 93 | Batch_idx: 180 |  Loss_1: (0.4025) | Acc_1: (85.56%) (19822/23168)\n",
      "Epoch: 93 | Batch_idx: 190 |  Loss_1: (0.4072) | Acc_1: (85.37%) (20872/24448)\n",
      "Epoch: 93 | Batch_idx: 200 |  Loss_1: (0.4080) | Acc_1: (85.32%) (21950/25728)\n",
      "Epoch: 93 | Batch_idx: 210 |  Loss_1: (0.4086) | Acc_1: (85.27%) (23030/27008)\n",
      "Epoch: 93 | Batch_idx: 220 |  Loss_1: (0.4097) | Acc_1: (85.22%) (24107/28288)\n",
      "Epoch: 93 | Batch_idx: 230 |  Loss_1: (0.4081) | Acc_1: (85.28%) (25215/29568)\n",
      "Epoch: 93 | Batch_idx: 240 |  Loss_1: (0.4079) | Acc_1: (85.27%) (26304/30848)\n",
      "Epoch: 93 | Batch_idx: 250 |  Loss_1: (0.4064) | Acc_1: (85.31%) (27409/32128)\n",
      "Epoch: 93 | Batch_idx: 260 |  Loss_1: (0.4040) | Acc_1: (85.40%) (28531/33408)\n",
      "Epoch: 93 | Batch_idx: 270 |  Loss_1: (0.4018) | Acc_1: (85.47%) (29649/34688)\n",
      "Epoch: 93 | Batch_idx: 280 |  Loss_1: (0.4021) | Acc_1: (85.46%) (30737/35968)\n",
      "Epoch: 93 | Batch_idx: 290 |  Loss_1: (0.4013) | Acc_1: (85.49%) (31844/37248)\n",
      "Epoch: 93 | Batch_idx: 300 |  Loss_1: (0.4006) | Acc_1: (85.48%) (32933/38528)\n",
      "Epoch: 93 | Batch_idx: 310 |  Loss_1: (0.4019) | Acc_1: (85.43%) (34006/39808)\n",
      "Epoch: 93 | Batch_idx: 320 |  Loss_1: (0.4016) | Acc_1: (85.44%) (35104/41088)\n",
      "Epoch: 93 | Batch_idx: 330 |  Loss_1: (0.4009) | Acc_1: (85.45%) (36204/42368)\n",
      "Epoch: 93 | Batch_idx: 340 |  Loss_1: (0.3998) | Acc_1: (85.46%) (37302/43648)\n",
      "Epoch: 93 | Batch_idx: 350 |  Loss_1: (0.3980) | Acc_1: (85.52%) (38423/44928)\n",
      "Epoch: 93 | Batch_idx: 360 |  Loss_1: (0.3969) | Acc_1: (85.54%) (39528/46208)\n",
      "Epoch: 93 | Batch_idx: 370 |  Loss_1: (0.3966) | Acc_1: (85.56%) (40633/47488)\n",
      "Epoch: 93 | Batch_idx: 380 |  Loss_1: (0.3961) | Acc_1: (85.57%) (41733/48768)\n",
      "Epoch: 93 | Batch_idx: 390 |  Loss_1: (0.3959) | Acc_1: (85.56%) (42782/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2763) | Acc: (91.02%) (9102/10000)\n",
      "Epoch: 94 | Batch_idx: 0 |  Loss_1: (0.5198) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 94 | Batch_idx: 10 |  Loss_1: (0.3472) | Acc_1: (87.14%) (1227/1408)\n",
      "Epoch: 94 | Batch_idx: 20 |  Loss_1: (0.3833) | Acc_1: (86.16%) (2316/2688)\n",
      "Epoch: 94 | Batch_idx: 30 |  Loss_1: (0.3844) | Acc_1: (86.09%) (3416/3968)\n",
      "Epoch: 94 | Batch_idx: 40 |  Loss_1: (0.3786) | Acc_1: (86.32%) (4530/5248)\n",
      "Epoch: 94 | Batch_idx: 50 |  Loss_1: (0.3826) | Acc_1: (86.04%) (5617/6528)\n",
      "Epoch: 94 | Batch_idx: 60 |  Loss_1: (0.3873) | Acc_1: (85.82%) (6701/7808)\n",
      "Epoch: 94 | Batch_idx: 70 |  Loss_1: (0.3816) | Acc_1: (86.05%) (7820/9088)\n",
      "Epoch: 94 | Batch_idx: 80 |  Loss_1: (0.3823) | Acc_1: (86.04%) (8921/10368)\n",
      "Epoch: 94 | Batch_idx: 90 |  Loss_1: (0.3864) | Acc_1: (85.88%) (10003/11648)\n",
      "Epoch: 94 | Batch_idx: 100 |  Loss_1: (0.3843) | Acc_1: (85.95%) (11111/12928)\n",
      "Epoch: 94 | Batch_idx: 110 |  Loss_1: (0.3804) | Acc_1: (86.06%) (12228/14208)\n",
      "Epoch: 94 | Batch_idx: 120 |  Loss_1: (0.3794) | Acc_1: (86.16%) (13345/15488)\n",
      "Epoch: 94 | Batch_idx: 130 |  Loss_1: (0.3782) | Acc_1: (86.17%) (14449/16768)\n",
      "Epoch: 94 | Batch_idx: 140 |  Loss_1: (0.3812) | Acc_1: (86.08%) (15536/18048)\n",
      "Epoch: 94 | Batch_idx: 150 |  Loss_1: (0.3817) | Acc_1: (86.04%) (16629/19328)\n",
      "Epoch: 94 | Batch_idx: 160 |  Loss_1: (0.3818) | Acc_1: (86.04%) (17731/20608)\n",
      "Epoch: 94 | Batch_idx: 170 |  Loss_1: (0.3812) | Acc_1: (86.08%) (18841/21888)\n",
      "Epoch: 94 | Batch_idx: 180 |  Loss_1: (0.3808) | Acc_1: (86.07%) (19941/23168)\n",
      "Epoch: 94 | Batch_idx: 190 |  Loss_1: (0.3802) | Acc_1: (86.16%) (21064/24448)\n",
      "Epoch: 94 | Batch_idx: 200 |  Loss_1: (0.3777) | Acc_1: (86.23%) (22186/25728)\n",
      "Epoch: 94 | Batch_idx: 210 |  Loss_1: (0.3775) | Acc_1: (86.22%) (23285/27008)\n",
      "Epoch: 94 | Batch_idx: 220 |  Loss_1: (0.3788) | Acc_1: (86.17%) (24377/28288)\n",
      "Epoch: 94 | Batch_idx: 230 |  Loss_1: (0.3794) | Acc_1: (86.15%) (25474/29568)\n",
      "Epoch: 94 | Batch_idx: 240 |  Loss_1: (0.3798) | Acc_1: (86.13%) (26568/30848)\n",
      "Epoch: 94 | Batch_idx: 250 |  Loss_1: (0.3798) | Acc_1: (86.11%) (27667/32128)\n",
      "Epoch: 94 | Batch_idx: 260 |  Loss_1: (0.3800) | Acc_1: (86.13%) (28775/33408)\n",
      "Epoch: 94 | Batch_idx: 270 |  Loss_1: (0.3800) | Acc_1: (86.14%) (29881/34688)\n",
      "Epoch: 94 | Batch_idx: 280 |  Loss_1: (0.3782) | Acc_1: (86.18%) (30996/35968)\n",
      "Epoch: 94 | Batch_idx: 290 |  Loss_1: (0.3779) | Acc_1: (86.17%) (32095/37248)\n",
      "Epoch: 94 | Batch_idx: 300 |  Loss_1: (0.3778) | Acc_1: (86.15%) (33191/38528)\n",
      "Epoch: 94 | Batch_idx: 310 |  Loss_1: (0.3772) | Acc_1: (86.17%) (34301/39808)\n",
      "Epoch: 94 | Batch_idx: 320 |  Loss_1: (0.3764) | Acc_1: (86.21%) (35421/41088)\n",
      "Epoch: 94 | Batch_idx: 330 |  Loss_1: (0.3765) | Acc_1: (86.21%) (36525/42368)\n",
      "Epoch: 94 | Batch_idx: 340 |  Loss_1: (0.3751) | Acc_1: (86.24%) (37643/43648)\n",
      "Epoch: 94 | Batch_idx: 350 |  Loss_1: (0.3736) | Acc_1: (86.31%) (38777/44928)\n",
      "Epoch: 94 | Batch_idx: 360 |  Loss_1: (0.3730) | Acc_1: (86.32%) (39885/46208)\n",
      "Epoch: 94 | Batch_idx: 370 |  Loss_1: (0.3726) | Acc_1: (86.33%) (40997/47488)\n",
      "Epoch: 94 | Batch_idx: 380 |  Loss_1: (0.3728) | Acc_1: (86.33%) (42101/48768)\n",
      "Epoch: 94 | Batch_idx: 390 |  Loss_1: (0.3733) | Acc_1: (86.28%) (43140/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2889) | Acc: (90.53%) (9053/10000)\n",
      "Epoch: 95 | Batch_idx: 0 |  Loss_1: (0.3729) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 95 | Batch_idx: 10 |  Loss_1: (0.3820) | Acc_1: (86.15%) (1213/1408)\n",
      "Epoch: 95 | Batch_idx: 20 |  Loss_1: (0.3675) | Acc_1: (86.35%) (2321/2688)\n",
      "Epoch: 95 | Batch_idx: 30 |  Loss_1: (0.3645) | Acc_1: (86.52%) (3433/3968)\n",
      "Epoch: 95 | Batch_idx: 40 |  Loss_1: (0.3632) | Acc_1: (86.62%) (4546/5248)\n",
      "Epoch: 95 | Batch_idx: 50 |  Loss_1: (0.3627) | Acc_1: (86.57%) (5651/6528)\n",
      "Epoch: 95 | Batch_idx: 60 |  Loss_1: (0.3620) | Acc_1: (86.67%) (6767/7808)\n",
      "Epoch: 95 | Batch_idx: 70 |  Loss_1: (0.3655) | Acc_1: (86.53%) (7864/9088)\n",
      "Epoch: 95 | Batch_idx: 80 |  Loss_1: (0.3682) | Acc_1: (86.40%) (8958/10368)\n",
      "Epoch: 95 | Batch_idx: 90 |  Loss_1: (0.3658) | Acc_1: (86.50%) (10076/11648)\n",
      "Epoch: 95 | Batch_idx: 100 |  Loss_1: (0.3635) | Acc_1: (86.60%) (11196/12928)\n",
      "Epoch: 95 | Batch_idx: 110 |  Loss_1: (0.3639) | Acc_1: (86.56%) (12298/14208)\n",
      "Epoch: 95 | Batch_idx: 120 |  Loss_1: (0.3590) | Acc_1: (86.73%) (13432/15488)\n",
      "Epoch: 95 | Batch_idx: 130 |  Loss_1: (0.3580) | Acc_1: (86.71%) (14540/16768)\n",
      "Epoch: 95 | Batch_idx: 140 |  Loss_1: (0.3568) | Acc_1: (86.72%) (15651/18048)\n",
      "Epoch: 95 | Batch_idx: 150 |  Loss_1: (0.3578) | Acc_1: (86.66%) (16749/19328)\n",
      "Epoch: 95 | Batch_idx: 160 |  Loss_1: (0.3586) | Acc_1: (86.61%) (17848/20608)\n",
      "Epoch: 95 | Batch_idx: 170 |  Loss_1: (0.3611) | Acc_1: (86.50%) (18933/21888)\n",
      "Epoch: 95 | Batch_idx: 180 |  Loss_1: (0.3622) | Acc_1: (86.46%) (20032/23168)\n",
      "Epoch: 95 | Batch_idx: 190 |  Loss_1: (0.3636) | Acc_1: (86.44%) (21134/24448)\n",
      "Epoch: 95 | Batch_idx: 200 |  Loss_1: (0.3633) | Acc_1: (86.45%) (22241/25728)\n",
      "Epoch: 95 | Batch_idx: 210 |  Loss_1: (0.3645) | Acc_1: (86.39%) (23332/27008)\n",
      "Epoch: 95 | Batch_idx: 220 |  Loss_1: (0.3651) | Acc_1: (86.39%) (24437/28288)\n",
      "Epoch: 95 | Batch_idx: 230 |  Loss_1: (0.3662) | Acc_1: (86.38%) (25542/29568)\n",
      "Epoch: 95 | Batch_idx: 240 |  Loss_1: (0.3666) | Acc_1: (86.38%) (26645/30848)\n",
      "Epoch: 95 | Batch_idx: 250 |  Loss_1: (0.3660) | Acc_1: (86.40%) (27760/32128)\n",
      "Epoch: 95 | Batch_idx: 260 |  Loss_1: (0.3659) | Acc_1: (86.41%) (28867/33408)\n",
      "Epoch: 95 | Batch_idx: 270 |  Loss_1: (0.3664) | Acc_1: (86.41%) (29975/34688)\n",
      "Epoch: 95 | Batch_idx: 280 |  Loss_1: (0.3663) | Acc_1: (86.42%) (31082/35968)\n",
      "Epoch: 95 | Batch_idx: 290 |  Loss_1: (0.3663) | Acc_1: (86.43%) (32193/37248)\n",
      "Epoch: 95 | Batch_idx: 300 |  Loss_1: (0.3665) | Acc_1: (86.42%) (33295/38528)\n",
      "Epoch: 95 | Batch_idx: 310 |  Loss_1: (0.3663) | Acc_1: (86.41%) (34400/39808)\n",
      "Epoch: 95 | Batch_idx: 320 |  Loss_1: (0.3656) | Acc_1: (86.46%) (35526/41088)\n",
      "Epoch: 95 | Batch_idx: 330 |  Loss_1: (0.3653) | Acc_1: (86.48%) (36639/42368)\n",
      "Epoch: 95 | Batch_idx: 340 |  Loss_1: (0.3667) | Acc_1: (86.46%) (37736/43648)\n",
      "Epoch: 95 | Batch_idx: 350 |  Loss_1: (0.3667) | Acc_1: (86.44%) (38837/44928)\n",
      "Epoch: 95 | Batch_idx: 360 |  Loss_1: (0.3661) | Acc_1: (86.46%) (39950/46208)\n",
      "Epoch: 95 | Batch_idx: 370 |  Loss_1: (0.3670) | Acc_1: (86.42%) (41041/47488)\n",
      "Epoch: 95 | Batch_idx: 380 |  Loss_1: (0.3672) | Acc_1: (86.41%) (42142/48768)\n",
      "Epoch: 95 | Batch_idx: 390 |  Loss_1: (0.3681) | Acc_1: (86.38%) (43192/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2862) | Acc: (90.45%) (9045/10000)\n",
      "Epoch: 96 | Batch_idx: 0 |  Loss_1: (0.2626) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 96 | Batch_idx: 10 |  Loss_1: (0.3711) | Acc_1: (86.65%) (1220/1408)\n",
      "Epoch: 96 | Batch_idx: 20 |  Loss_1: (0.3471) | Acc_1: (87.13%) (2342/2688)\n",
      "Epoch: 96 | Batch_idx: 30 |  Loss_1: (0.3548) | Acc_1: (86.77%) (3443/3968)\n",
      "Epoch: 96 | Batch_idx: 40 |  Loss_1: (0.3441) | Acc_1: (87.00%) (4566/5248)\n",
      "Epoch: 96 | Batch_idx: 50 |  Loss_1: (0.3522) | Acc_1: (86.64%) (5656/6528)\n",
      "Epoch: 96 | Batch_idx: 60 |  Loss_1: (0.3547) | Acc_1: (86.59%) (6761/7808)\n",
      "Epoch: 96 | Batch_idx: 70 |  Loss_1: (0.3636) | Acc_1: (86.22%) (7836/9088)\n",
      "Epoch: 96 | Batch_idx: 80 |  Loss_1: (0.3632) | Acc_1: (86.28%) (8945/10368)\n",
      "Epoch: 96 | Batch_idx: 90 |  Loss_1: (0.3617) | Acc_1: (86.32%) (10055/11648)\n",
      "Epoch: 96 | Batch_idx: 100 |  Loss_1: (0.3621) | Acc_1: (86.30%) (11157/12928)\n",
      "Epoch: 96 | Batch_idx: 110 |  Loss_1: (0.3605) | Acc_1: (86.36%) (12270/14208)\n",
      "Epoch: 96 | Batch_idx: 120 |  Loss_1: (0.3609) | Acc_1: (86.31%) (13368/15488)\n",
      "Epoch: 96 | Batch_idx: 130 |  Loss_1: (0.3625) | Acc_1: (86.35%) (14479/16768)\n",
      "Epoch: 96 | Batch_idx: 140 |  Loss_1: (0.3638) | Acc_1: (86.28%) (15571/18048)\n",
      "Epoch: 96 | Batch_idx: 150 |  Loss_1: (0.3657) | Acc_1: (86.25%) (16670/19328)\n",
      "Epoch: 96 | Batch_idx: 160 |  Loss_1: (0.3646) | Acc_1: (86.31%) (17787/20608)\n",
      "Epoch: 96 | Batch_idx: 170 |  Loss_1: (0.3659) | Acc_1: (86.22%) (18872/21888)\n",
      "Epoch: 96 | Batch_idx: 180 |  Loss_1: (0.3654) | Acc_1: (86.23%) (19978/23168)\n",
      "Epoch: 96 | Batch_idx: 190 |  Loss_1: (0.3652) | Acc_1: (86.26%) (21090/24448)\n",
      "Epoch: 96 | Batch_idx: 200 |  Loss_1: (0.3653) | Acc_1: (86.30%) (22204/25728)\n",
      "Epoch: 96 | Batch_idx: 210 |  Loss_1: (0.3642) | Acc_1: (86.36%) (23323/27008)\n",
      "Epoch: 96 | Batch_idx: 220 |  Loss_1: (0.3647) | Acc_1: (86.34%) (24423/28288)\n",
      "Epoch: 96 | Batch_idx: 230 |  Loss_1: (0.3650) | Acc_1: (86.35%) (25531/29568)\n",
      "Epoch: 96 | Batch_idx: 240 |  Loss_1: (0.3669) | Acc_1: (86.25%) (26605/30848)\n",
      "Epoch: 96 | Batch_idx: 250 |  Loss_1: (0.3676) | Acc_1: (86.22%) (27701/32128)\n",
      "Epoch: 96 | Batch_idx: 260 |  Loss_1: (0.3668) | Acc_1: (86.23%) (28809/33408)\n",
      "Epoch: 96 | Batch_idx: 270 |  Loss_1: (0.3672) | Acc_1: (86.25%) (29918/34688)\n",
      "Epoch: 96 | Batch_idx: 280 |  Loss_1: (0.3668) | Acc_1: (86.29%) (31036/35968)\n",
      "Epoch: 96 | Batch_idx: 290 |  Loss_1: (0.3671) | Acc_1: (86.29%) (32141/37248)\n",
      "Epoch: 96 | Batch_idx: 300 |  Loss_1: (0.3671) | Acc_1: (86.31%) (33253/38528)\n",
      "Epoch: 96 | Batch_idx: 310 |  Loss_1: (0.3681) | Acc_1: (86.30%) (34354/39808)\n",
      "Epoch: 96 | Batch_idx: 320 |  Loss_1: (0.3680) | Acc_1: (86.29%) (35455/41088)\n",
      "Epoch: 96 | Batch_idx: 330 |  Loss_1: (0.3665) | Acc_1: (86.36%) (36588/42368)\n",
      "Epoch: 96 | Batch_idx: 340 |  Loss_1: (0.3665) | Acc_1: (86.36%) (37696/43648)\n",
      "Epoch: 96 | Batch_idx: 350 |  Loss_1: (0.3668) | Acc_1: (86.36%) (38799/44928)\n",
      "Epoch: 96 | Batch_idx: 360 |  Loss_1: (0.3666) | Acc_1: (86.36%) (39907/46208)\n",
      "Epoch: 96 | Batch_idx: 370 |  Loss_1: (0.3684) | Acc_1: (86.31%) (40988/47488)\n",
      "Epoch: 96 | Batch_idx: 380 |  Loss_1: (0.3686) | Acc_1: (86.30%) (42087/48768)\n",
      "Epoch: 96 | Batch_idx: 390 |  Loss_1: (0.3686) | Acc_1: (86.30%) (43152/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2984) | Acc: (90.35%) (9035/10000)\n",
      "Epoch: 97 | Batch_idx: 0 |  Loss_1: (0.4257) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 97 | Batch_idx: 10 |  Loss_1: (0.3880) | Acc_1: (84.59%) (1191/1408)\n",
      "Epoch: 97 | Batch_idx: 20 |  Loss_1: (0.3712) | Acc_1: (85.79%) (2306/2688)\n",
      "Epoch: 97 | Batch_idx: 30 |  Loss_1: (0.3541) | Acc_1: (86.64%) (3438/3968)\n",
      "Epoch: 97 | Batch_idx: 40 |  Loss_1: (0.3576) | Acc_1: (86.53%) (4541/5248)\n",
      "Epoch: 97 | Batch_idx: 50 |  Loss_1: (0.3619) | Acc_1: (86.41%) (5641/6528)\n",
      "Epoch: 97 | Batch_idx: 60 |  Loss_1: (0.3680) | Acc_1: (86.27%) (6736/7808)\n",
      "Epoch: 97 | Batch_idx: 70 |  Loss_1: (0.3651) | Acc_1: (86.44%) (7856/9088)\n",
      "Epoch: 97 | Batch_idx: 80 |  Loss_1: (0.3663) | Acc_1: (86.38%) (8956/10368)\n",
      "Epoch: 97 | Batch_idx: 90 |  Loss_1: (0.3666) | Acc_1: (86.37%) (10060/11648)\n",
      "Epoch: 97 | Batch_idx: 100 |  Loss_1: (0.3696) | Acc_1: (86.24%) (11149/12928)\n",
      "Epoch: 97 | Batch_idx: 110 |  Loss_1: (0.3678) | Acc_1: (86.31%) (12263/14208)\n",
      "Epoch: 97 | Batch_idx: 120 |  Loss_1: (0.3647) | Acc_1: (86.42%) (13385/15488)\n",
      "Epoch: 97 | Batch_idx: 130 |  Loss_1: (0.3644) | Acc_1: (86.47%) (14500/16768)\n",
      "Epoch: 97 | Batch_idx: 140 |  Loss_1: (0.3635) | Acc_1: (86.48%) (15608/18048)\n",
      "Epoch: 97 | Batch_idx: 150 |  Loss_1: (0.3660) | Acc_1: (86.48%) (16715/19328)\n",
      "Epoch: 97 | Batch_idx: 160 |  Loss_1: (0.3646) | Acc_1: (86.52%) (17830/20608)\n",
      "Epoch: 97 | Batch_idx: 170 |  Loss_1: (0.3642) | Acc_1: (86.54%) (18941/21888)\n",
      "Epoch: 97 | Batch_idx: 180 |  Loss_1: (0.3644) | Acc_1: (86.49%) (20038/23168)\n",
      "Epoch: 97 | Batch_idx: 190 |  Loss_1: (0.3662) | Acc_1: (86.46%) (21138/24448)\n",
      "Epoch: 97 | Batch_idx: 200 |  Loss_1: (0.3647) | Acc_1: (86.49%) (22252/25728)\n",
      "Epoch: 97 | Batch_idx: 210 |  Loss_1: (0.3648) | Acc_1: (86.46%) (23350/27008)\n",
      "Epoch: 97 | Batch_idx: 220 |  Loss_1: (0.3642) | Acc_1: (86.49%) (24465/28288)\n",
      "Epoch: 97 | Batch_idx: 230 |  Loss_1: (0.3638) | Acc_1: (86.49%) (25572/29568)\n",
      "Epoch: 97 | Batch_idx: 240 |  Loss_1: (0.3635) | Acc_1: (86.50%) (26682/30848)\n",
      "Epoch: 97 | Batch_idx: 250 |  Loss_1: (0.3630) | Acc_1: (86.51%) (27795/32128)\n",
      "Epoch: 97 | Batch_idx: 260 |  Loss_1: (0.3629) | Acc_1: (86.55%) (28913/33408)\n",
      "Epoch: 97 | Batch_idx: 270 |  Loss_1: (0.3605) | Acc_1: (86.66%) (30059/34688)\n",
      "Epoch: 97 | Batch_idx: 280 |  Loss_1: (0.3610) | Acc_1: (86.67%) (31172/35968)\n",
      "Epoch: 97 | Batch_idx: 290 |  Loss_1: (0.3622) | Acc_1: (86.61%) (32262/37248)\n",
      "Epoch: 97 | Batch_idx: 300 |  Loss_1: (0.3631) | Acc_1: (86.59%) (33361/38528)\n",
      "Epoch: 97 | Batch_idx: 310 |  Loss_1: (0.3624) | Acc_1: (86.61%) (34477/39808)\n",
      "Epoch: 97 | Batch_idx: 320 |  Loss_1: (0.3613) | Acc_1: (86.63%) (35594/41088)\n",
      "Epoch: 97 | Batch_idx: 330 |  Loss_1: (0.3599) | Acc_1: (86.67%) (36721/42368)\n",
      "Epoch: 97 | Batch_idx: 340 |  Loss_1: (0.3605) | Acc_1: (86.67%) (37830/43648)\n",
      "Epoch: 97 | Batch_idx: 350 |  Loss_1: (0.3624) | Acc_1: (86.62%) (38916/44928)\n",
      "Epoch: 97 | Batch_idx: 360 |  Loss_1: (0.3627) | Acc_1: (86.60%) (40015/46208)\n",
      "Epoch: 97 | Batch_idx: 370 |  Loss_1: (0.3632) | Acc_1: (86.59%) (41121/47488)\n",
      "Epoch: 97 | Batch_idx: 380 |  Loss_1: (0.3637) | Acc_1: (86.58%) (42223/48768)\n",
      "Epoch: 97 | Batch_idx: 390 |  Loss_1: (0.3636) | Acc_1: (86.58%) (43292/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2692) | Acc: (90.91%) (9091/10000)\n",
      "Epoch: 98 | Batch_idx: 0 |  Loss_1: (0.4654) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 98 | Batch_idx: 10 |  Loss_1: (0.3869) | Acc_1: (85.80%) (1208/1408)\n",
      "Epoch: 98 | Batch_idx: 20 |  Loss_1: (0.3785) | Acc_1: (86.05%) (2313/2688)\n",
      "Epoch: 98 | Batch_idx: 30 |  Loss_1: (0.3725) | Acc_1: (86.49%) (3432/3968)\n",
      "Epoch: 98 | Batch_idx: 40 |  Loss_1: (0.3685) | Acc_1: (86.55%) (4542/5248)\n",
      "Epoch: 98 | Batch_idx: 50 |  Loss_1: (0.3612) | Acc_1: (86.84%) (5669/6528)\n",
      "Epoch: 98 | Batch_idx: 60 |  Loss_1: (0.3597) | Acc_1: (86.87%) (6783/7808)\n",
      "Epoch: 98 | Batch_idx: 70 |  Loss_1: (0.3585) | Acc_1: (86.89%) (7897/9088)\n",
      "Epoch: 98 | Batch_idx: 80 |  Loss_1: (0.3526) | Acc_1: (87.09%) (9030/10368)\n",
      "Epoch: 98 | Batch_idx: 90 |  Loss_1: (0.3520) | Acc_1: (87.16%) (10152/11648)\n",
      "Epoch: 98 | Batch_idx: 100 |  Loss_1: (0.3536) | Acc_1: (87.10%) (11260/12928)\n",
      "Epoch: 98 | Batch_idx: 110 |  Loss_1: (0.3538) | Acc_1: (87.08%) (12373/14208)\n",
      "Epoch: 98 | Batch_idx: 120 |  Loss_1: (0.3583) | Acc_1: (86.93%) (13464/15488)\n",
      "Epoch: 98 | Batch_idx: 130 |  Loss_1: (0.3576) | Acc_1: (86.91%) (14573/16768)\n",
      "Epoch: 98 | Batch_idx: 140 |  Loss_1: (0.3605) | Acc_1: (86.91%) (15686/18048)\n",
      "Epoch: 98 | Batch_idx: 150 |  Loss_1: (0.3600) | Acc_1: (86.89%) (16795/19328)\n",
      "Epoch: 98 | Batch_idx: 160 |  Loss_1: (0.3590) | Acc_1: (86.93%) (17914/20608)\n",
      "Epoch: 98 | Batch_idx: 170 |  Loss_1: (0.3621) | Acc_1: (86.78%) (18994/21888)\n",
      "Epoch: 98 | Batch_idx: 180 |  Loss_1: (0.3602) | Acc_1: (86.82%) (20115/23168)\n",
      "Epoch: 98 | Batch_idx: 190 |  Loss_1: (0.3605) | Acc_1: (86.83%) (21227/24448)\n",
      "Epoch: 98 | Batch_idx: 200 |  Loss_1: (0.3616) | Acc_1: (86.78%) (22327/25728)\n",
      "Epoch: 98 | Batch_idx: 210 |  Loss_1: (0.3627) | Acc_1: (86.74%) (23428/27008)\n",
      "Epoch: 98 | Batch_idx: 220 |  Loss_1: (0.3616) | Acc_1: (86.76%) (24544/28288)\n",
      "Epoch: 98 | Batch_idx: 230 |  Loss_1: (0.3606) | Acc_1: (86.78%) (25659/29568)\n",
      "Epoch: 98 | Batch_idx: 240 |  Loss_1: (0.3600) | Acc_1: (86.80%) (26777/30848)\n",
      "Epoch: 98 | Batch_idx: 250 |  Loss_1: (0.3621) | Acc_1: (86.71%) (27857/32128)\n",
      "Epoch: 98 | Batch_idx: 260 |  Loss_1: (0.3649) | Acc_1: (86.61%) (28933/33408)\n",
      "Epoch: 98 | Batch_idx: 270 |  Loss_1: (0.3638) | Acc_1: (86.66%) (30059/34688)\n",
      "Epoch: 98 | Batch_idx: 280 |  Loss_1: (0.3629) | Acc_1: (86.69%) (31180/35968)\n",
      "Epoch: 98 | Batch_idx: 290 |  Loss_1: (0.3628) | Acc_1: (86.68%) (32287/37248)\n",
      "Epoch: 98 | Batch_idx: 300 |  Loss_1: (0.3624) | Acc_1: (86.72%) (33411/38528)\n",
      "Epoch: 98 | Batch_idx: 310 |  Loss_1: (0.3627) | Acc_1: (86.69%) (34510/39808)\n",
      "Epoch: 98 | Batch_idx: 320 |  Loss_1: (0.3628) | Acc_1: (86.72%) (35632/41088)\n",
      "Epoch: 98 | Batch_idx: 330 |  Loss_1: (0.3635) | Acc_1: (86.70%) (36731/42368)\n",
      "Epoch: 98 | Batch_idx: 340 |  Loss_1: (0.3632) | Acc_1: (86.71%) (37845/43648)\n",
      "Epoch: 98 | Batch_idx: 350 |  Loss_1: (0.3636) | Acc_1: (86.67%) (38937/44928)\n",
      "Epoch: 98 | Batch_idx: 360 |  Loss_1: (0.3632) | Acc_1: (86.67%) (40048/46208)\n",
      "Epoch: 98 | Batch_idx: 370 |  Loss_1: (0.3636) | Acc_1: (86.66%) (41151/47488)\n",
      "Epoch: 98 | Batch_idx: 380 |  Loss_1: (0.3634) | Acc_1: (86.67%) (42268/48768)\n",
      "Epoch: 98 | Batch_idx: 390 |  Loss_1: (0.3638) | Acc_1: (86.66%) (43328/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3076) | Acc: (90.32%) (9032/10000)\n",
      "Epoch: 99 | Batch_idx: 0 |  Loss_1: (0.4189) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 99 | Batch_idx: 10 |  Loss_1: (0.3340) | Acc_1: (88.21%) (1242/1408)\n",
      "Epoch: 99 | Batch_idx: 20 |  Loss_1: (0.3448) | Acc_1: (87.43%) (2350/2688)\n",
      "Epoch: 99 | Batch_idx: 30 |  Loss_1: (0.3585) | Acc_1: (86.87%) (3447/3968)\n",
      "Epoch: 99 | Batch_idx: 40 |  Loss_1: (0.3553) | Acc_1: (86.87%) (4559/5248)\n",
      "Epoch: 99 | Batch_idx: 50 |  Loss_1: (0.3516) | Acc_1: (87.12%) (5687/6528)\n",
      "Epoch: 99 | Batch_idx: 60 |  Loss_1: (0.3515) | Acc_1: (87.12%) (6802/7808)\n",
      "Epoch: 99 | Batch_idx: 70 |  Loss_1: (0.3527) | Acc_1: (87.08%) (7914/9088)\n",
      "Epoch: 99 | Batch_idx: 80 |  Loss_1: (0.3532) | Acc_1: (87.17%) (9038/10368)\n",
      "Epoch: 99 | Batch_idx: 90 |  Loss_1: (0.3507) | Acc_1: (87.22%) (10159/11648)\n",
      "Epoch: 99 | Batch_idx: 100 |  Loss_1: (0.3518) | Acc_1: (87.18%) (11271/12928)\n",
      "Epoch: 99 | Batch_idx: 110 |  Loss_1: (0.3522) | Acc_1: (87.11%) (12377/14208)\n",
      "Epoch: 99 | Batch_idx: 120 |  Loss_1: (0.3528) | Acc_1: (87.11%) (13491/15488)\n",
      "Epoch: 99 | Batch_idx: 130 |  Loss_1: (0.3537) | Acc_1: (87.05%) (14597/16768)\n",
      "Epoch: 99 | Batch_idx: 140 |  Loss_1: (0.3511) | Acc_1: (87.12%) (15724/18048)\n",
      "Epoch: 99 | Batch_idx: 150 |  Loss_1: (0.3535) | Acc_1: (87.00%) (16816/19328)\n",
      "Epoch: 99 | Batch_idx: 160 |  Loss_1: (0.3539) | Acc_1: (86.97%) (17922/20608)\n",
      "Epoch: 99 | Batch_idx: 170 |  Loss_1: (0.3550) | Acc_1: (86.92%) (19026/21888)\n",
      "Epoch: 99 | Batch_idx: 180 |  Loss_1: (0.3560) | Acc_1: (86.91%) (20135/23168)\n",
      "Epoch: 99 | Batch_idx: 190 |  Loss_1: (0.3572) | Acc_1: (86.88%) (21240/24448)\n",
      "Epoch: 99 | Batch_idx: 200 |  Loss_1: (0.3555) | Acc_1: (86.94%) (22368/25728)\n",
      "Epoch: 99 | Batch_idx: 210 |  Loss_1: (0.3557) | Acc_1: (86.97%) (23488/27008)\n",
      "Epoch: 99 | Batch_idx: 220 |  Loss_1: (0.3558) | Acc_1: (86.97%) (24601/28288)\n",
      "Epoch: 99 | Batch_idx: 230 |  Loss_1: (0.3557) | Acc_1: (86.99%) (25720/29568)\n",
      "Epoch: 99 | Batch_idx: 240 |  Loss_1: (0.3568) | Acc_1: (86.94%) (26820/30848)\n",
      "Epoch: 99 | Batch_idx: 250 |  Loss_1: (0.3574) | Acc_1: (86.92%) (27925/32128)\n",
      "Epoch: 99 | Batch_idx: 260 |  Loss_1: (0.3581) | Acc_1: (86.89%) (29027/33408)\n",
      "Epoch: 99 | Batch_idx: 270 |  Loss_1: (0.3568) | Acc_1: (86.91%) (30149/34688)\n",
      "Epoch: 99 | Batch_idx: 280 |  Loss_1: (0.3567) | Acc_1: (86.90%) (31257/35968)\n",
      "Epoch: 99 | Batch_idx: 290 |  Loss_1: (0.3563) | Acc_1: (86.91%) (32372/37248)\n",
      "Epoch: 99 | Batch_idx: 300 |  Loss_1: (0.3568) | Acc_1: (86.92%) (33488/38528)\n",
      "Epoch: 99 | Batch_idx: 310 |  Loss_1: (0.3566) | Acc_1: (86.94%) (34611/39808)\n",
      "Epoch: 99 | Batch_idx: 320 |  Loss_1: (0.3576) | Acc_1: (86.89%) (35703/41088)\n",
      "Epoch: 99 | Batch_idx: 330 |  Loss_1: (0.3562) | Acc_1: (86.95%) (36838/42368)\n",
      "Epoch: 99 | Batch_idx: 340 |  Loss_1: (0.3557) | Acc_1: (86.95%) (37950/43648)\n",
      "Epoch: 99 | Batch_idx: 350 |  Loss_1: (0.3555) | Acc_1: (86.96%) (39068/44928)\n",
      "Epoch: 99 | Batch_idx: 360 |  Loss_1: (0.3553) | Acc_1: (86.96%) (40183/46208)\n",
      "Epoch: 99 | Batch_idx: 370 |  Loss_1: (0.3569) | Acc_1: (86.93%) (41280/47488)\n",
      "Epoch: 99 | Batch_idx: 380 |  Loss_1: (0.3577) | Acc_1: (86.90%) (42379/48768)\n",
      "Epoch: 99 | Batch_idx: 390 |  Loss_1: (0.3577) | Acc_1: (86.91%) (43455/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2629) | Acc: (91.35%) (9135/10000)\n",
      "Epoch: 100 | Batch_idx: 0 |  Loss_1: (0.3828) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 100 | Batch_idx: 10 |  Loss_1: (0.3487) | Acc_1: (86.65%) (1220/1408)\n",
      "Epoch: 100 | Batch_idx: 20 |  Loss_1: (0.3358) | Acc_1: (86.98%) (2338/2688)\n",
      "Epoch: 100 | Batch_idx: 30 |  Loss_1: (0.3519) | Acc_1: (86.64%) (3438/3968)\n",
      "Epoch: 100 | Batch_idx: 40 |  Loss_1: (0.3585) | Acc_1: (86.49%) (4539/5248)\n",
      "Epoch: 100 | Batch_idx: 50 |  Loss_1: (0.3693) | Acc_1: (86.26%) (5631/6528)\n",
      "Epoch: 100 | Batch_idx: 60 |  Loss_1: (0.3687) | Acc_1: (86.23%) (6733/7808)\n",
      "Epoch: 100 | Batch_idx: 70 |  Loss_1: (0.3649) | Acc_1: (86.42%) (7854/9088)\n",
      "Epoch: 100 | Batch_idx: 80 |  Loss_1: (0.3616) | Acc_1: (86.57%) (8976/10368)\n",
      "Epoch: 100 | Batch_idx: 90 |  Loss_1: (0.3641) | Acc_1: (86.59%) (10086/11648)\n",
      "Epoch: 100 | Batch_idx: 100 |  Loss_1: (0.3637) | Acc_1: (86.56%) (11191/12928)\n",
      "Epoch: 100 | Batch_idx: 110 |  Loss_1: (0.3608) | Acc_1: (86.65%) (12311/14208)\n",
      "Epoch: 100 | Batch_idx: 120 |  Loss_1: (0.3594) | Acc_1: (86.67%) (13424/15488)\n",
      "Epoch: 100 | Batch_idx: 130 |  Loss_1: (0.3603) | Acc_1: (86.68%) (14535/16768)\n",
      "Epoch: 100 | Batch_idx: 140 |  Loss_1: (0.3603) | Acc_1: (86.71%) (15649/18048)\n",
      "Epoch: 100 | Batch_idx: 150 |  Loss_1: (0.3573) | Acc_1: (86.83%) (16783/19328)\n",
      "Epoch: 100 | Batch_idx: 160 |  Loss_1: (0.3560) | Acc_1: (86.88%) (17905/20608)\n",
      "Epoch: 100 | Batch_idx: 170 |  Loss_1: (0.3577) | Acc_1: (86.81%) (19001/21888)\n",
      "Epoch: 100 | Batch_idx: 180 |  Loss_1: (0.3578) | Acc_1: (86.82%) (20114/23168)\n",
      "Epoch: 100 | Batch_idx: 190 |  Loss_1: (0.3548) | Acc_1: (86.90%) (21246/24448)\n",
      "Epoch: 100 | Batch_idx: 200 |  Loss_1: (0.3542) | Acc_1: (86.93%) (22366/25728)\n",
      "Epoch: 100 | Batch_idx: 210 |  Loss_1: (0.3537) | Acc_1: (86.94%) (23480/27008)\n",
      "Epoch: 100 | Batch_idx: 220 |  Loss_1: (0.3504) | Acc_1: (87.03%) (24620/28288)\n",
      "Epoch: 100 | Batch_idx: 230 |  Loss_1: (0.3514) | Acc_1: (87.04%) (25735/29568)\n",
      "Epoch: 100 | Batch_idx: 240 |  Loss_1: (0.3529) | Acc_1: (86.94%) (26819/30848)\n",
      "Epoch: 100 | Batch_idx: 250 |  Loss_1: (0.3538) | Acc_1: (86.91%) (27923/32128)\n",
      "Epoch: 100 | Batch_idx: 260 |  Loss_1: (0.3560) | Acc_1: (86.82%) (29005/33408)\n",
      "Epoch: 100 | Batch_idx: 270 |  Loss_1: (0.3564) | Acc_1: (86.79%) (30105/34688)\n",
      "Epoch: 100 | Batch_idx: 280 |  Loss_1: (0.3556) | Acc_1: (86.80%) (31222/35968)\n",
      "Epoch: 100 | Batch_idx: 290 |  Loss_1: (0.3548) | Acc_1: (86.81%) (32336/37248)\n",
      "Epoch: 100 | Batch_idx: 300 |  Loss_1: (0.3548) | Acc_1: (86.82%) (33449/38528)\n",
      "Epoch: 100 | Batch_idx: 310 |  Loss_1: (0.3548) | Acc_1: (86.82%) (34561/39808)\n",
      "Epoch: 100 | Batch_idx: 320 |  Loss_1: (0.3555) | Acc_1: (86.80%) (35666/41088)\n",
      "Epoch: 100 | Batch_idx: 330 |  Loss_1: (0.3549) | Acc_1: (86.84%) (36792/42368)\n",
      "Epoch: 100 | Batch_idx: 340 |  Loss_1: (0.3549) | Acc_1: (86.82%) (37897/43648)\n",
      "Epoch: 100 | Batch_idx: 350 |  Loss_1: (0.3546) | Acc_1: (86.83%) (39013/44928)\n",
      "Epoch: 100 | Batch_idx: 360 |  Loss_1: (0.3545) | Acc_1: (86.85%) (40132/46208)\n",
      "Epoch: 100 | Batch_idx: 370 |  Loss_1: (0.3543) | Acc_1: (86.84%) (41240/47488)\n",
      "Epoch: 100 | Batch_idx: 380 |  Loss_1: (0.3532) | Acc_1: (86.89%) (42375/48768)\n",
      "Epoch: 100 | Batch_idx: 390 |  Loss_1: (0.3542) | Acc_1: (86.85%) (43425/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2765) | Acc: (91.28%) (9128/10000)\n",
      "Epoch: 101 | Batch_idx: 0 |  Loss_1: (0.4351) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 101 | Batch_idx: 10 |  Loss_1: (0.3465) | Acc_1: (87.43%) (1231/1408)\n",
      "Epoch: 101 | Batch_idx: 20 |  Loss_1: (0.3460) | Acc_1: (86.68%) (2330/2688)\n",
      "Epoch: 101 | Batch_idx: 30 |  Loss_1: (0.3434) | Acc_1: (86.92%) (3449/3968)\n",
      "Epoch: 101 | Batch_idx: 40 |  Loss_1: (0.3445) | Acc_1: (86.83%) (4557/5248)\n",
      "Epoch: 101 | Batch_idx: 50 |  Loss_1: (0.3377) | Acc_1: (87.10%) (5686/6528)\n",
      "Epoch: 101 | Batch_idx: 60 |  Loss_1: (0.3326) | Acc_1: (87.27%) (6814/7808)\n",
      "Epoch: 101 | Batch_idx: 70 |  Loss_1: (0.3342) | Acc_1: (87.26%) (7930/9088)\n",
      "Epoch: 101 | Batch_idx: 80 |  Loss_1: (0.3388) | Acc_1: (87.07%) (9027/10368)\n",
      "Epoch: 101 | Batch_idx: 90 |  Loss_1: (0.3374) | Acc_1: (87.17%) (10153/11648)\n",
      "Epoch: 101 | Batch_idx: 100 |  Loss_1: (0.3406) | Acc_1: (87.08%) (11258/12928)\n",
      "Epoch: 101 | Batch_idx: 110 |  Loss_1: (0.3409) | Acc_1: (87.06%) (12369/14208)\n",
      "Epoch: 101 | Batch_idx: 120 |  Loss_1: (0.3426) | Acc_1: (87.01%) (13476/15488)\n",
      "Epoch: 101 | Batch_idx: 130 |  Loss_1: (0.3418) | Acc_1: (87.08%) (14601/16768)\n",
      "Epoch: 101 | Batch_idx: 140 |  Loss_1: (0.3439) | Acc_1: (87.04%) (15709/18048)\n",
      "Epoch: 101 | Batch_idx: 150 |  Loss_1: (0.3462) | Acc_1: (86.95%) (16806/19328)\n",
      "Epoch: 101 | Batch_idx: 160 |  Loss_1: (0.3436) | Acc_1: (87.06%) (17942/20608)\n",
      "Epoch: 101 | Batch_idx: 170 |  Loss_1: (0.3456) | Acc_1: (86.97%) (19037/21888)\n",
      "Epoch: 101 | Batch_idx: 180 |  Loss_1: (0.3461) | Acc_1: (86.99%) (20155/23168)\n",
      "Epoch: 101 | Batch_idx: 190 |  Loss_1: (0.3459) | Acc_1: (86.96%) (21261/24448)\n",
      "Epoch: 101 | Batch_idx: 200 |  Loss_1: (0.3460) | Acc_1: (86.97%) (22376/25728)\n",
      "Epoch: 101 | Batch_idx: 210 |  Loss_1: (0.3463) | Acc_1: (86.99%) (23495/27008)\n",
      "Epoch: 101 | Batch_idx: 220 |  Loss_1: (0.3472) | Acc_1: (86.97%) (24601/28288)\n",
      "Epoch: 101 | Batch_idx: 230 |  Loss_1: (0.3472) | Acc_1: (87.00%) (25724/29568)\n",
      "Epoch: 101 | Batch_idx: 240 |  Loss_1: (0.3485) | Acc_1: (86.97%) (26827/30848)\n",
      "Epoch: 101 | Batch_idx: 250 |  Loss_1: (0.3504) | Acc_1: (86.91%) (27924/32128)\n",
      "Epoch: 101 | Batch_idx: 260 |  Loss_1: (0.3511) | Acc_1: (86.91%) (29034/33408)\n",
      "Epoch: 101 | Batch_idx: 270 |  Loss_1: (0.3491) | Acc_1: (86.97%) (30168/34688)\n",
      "Epoch: 101 | Batch_idx: 280 |  Loss_1: (0.3493) | Acc_1: (86.95%) (31273/35968)\n",
      "Epoch: 101 | Batch_idx: 290 |  Loss_1: (0.3499) | Acc_1: (86.90%) (32368/37248)\n",
      "Epoch: 101 | Batch_idx: 300 |  Loss_1: (0.3488) | Acc_1: (86.95%) (33499/38528)\n",
      "Epoch: 101 | Batch_idx: 310 |  Loss_1: (0.3464) | Acc_1: (87.05%) (34653/39808)\n",
      "Epoch: 101 | Batch_idx: 320 |  Loss_1: (0.3469) | Acc_1: (87.04%) (35763/41088)\n",
      "Epoch: 101 | Batch_idx: 330 |  Loss_1: (0.3465) | Acc_1: (87.06%) (36886/42368)\n",
      "Epoch: 101 | Batch_idx: 340 |  Loss_1: (0.3465) | Acc_1: (87.06%) (38000/43648)\n",
      "Epoch: 101 | Batch_idx: 350 |  Loss_1: (0.3471) | Acc_1: (87.07%) (39117/44928)\n",
      "Epoch: 101 | Batch_idx: 360 |  Loss_1: (0.3465) | Acc_1: (87.09%) (40243/46208)\n",
      "Epoch: 101 | Batch_idx: 370 |  Loss_1: (0.3459) | Acc_1: (87.13%) (41376/47488)\n",
      "Epoch: 101 | Batch_idx: 380 |  Loss_1: (0.3464) | Acc_1: (87.12%) (42487/48768)\n",
      "Epoch: 101 | Batch_idx: 390 |  Loss_1: (0.3466) | Acc_1: (87.10%) (43552/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2809) | Acc: (91.37%) (9137/10000)\n",
      "Epoch: 102 | Batch_idx: 0 |  Loss_1: (0.3853) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 102 | Batch_idx: 10 |  Loss_1: (0.3680) | Acc_1: (87.22%) (1228/1408)\n",
      "Epoch: 102 | Batch_idx: 20 |  Loss_1: (0.3338) | Acc_1: (88.36%) (2375/2688)\n",
      "Epoch: 102 | Batch_idx: 30 |  Loss_1: (0.3384) | Acc_1: (88.05%) (3494/3968)\n",
      "Epoch: 102 | Batch_idx: 40 |  Loss_1: (0.3411) | Acc_1: (87.98%) (4617/5248)\n",
      "Epoch: 102 | Batch_idx: 50 |  Loss_1: (0.3414) | Acc_1: (87.71%) (5726/6528)\n",
      "Epoch: 102 | Batch_idx: 60 |  Loss_1: (0.3429) | Acc_1: (87.65%) (6844/7808)\n",
      "Epoch: 102 | Batch_idx: 70 |  Loss_1: (0.3435) | Acc_1: (87.52%) (7954/9088)\n",
      "Epoch: 102 | Batch_idx: 80 |  Loss_1: (0.3413) | Acc_1: (87.45%) (9067/10368)\n",
      "Epoch: 102 | Batch_idx: 90 |  Loss_1: (0.3402) | Acc_1: (87.44%) (10185/11648)\n",
      "Epoch: 102 | Batch_idx: 100 |  Loss_1: (0.3410) | Acc_1: (87.34%) (11291/12928)\n",
      "Epoch: 102 | Batch_idx: 110 |  Loss_1: (0.3415) | Acc_1: (87.33%) (12408/14208)\n",
      "Epoch: 102 | Batch_idx: 120 |  Loss_1: (0.3391) | Acc_1: (87.40%) (13536/15488)\n",
      "Epoch: 102 | Batch_idx: 130 |  Loss_1: (0.3447) | Acc_1: (87.20%) (14621/16768)\n",
      "Epoch: 102 | Batch_idx: 140 |  Loss_1: (0.3445) | Acc_1: (87.18%) (15734/18048)\n",
      "Epoch: 102 | Batch_idx: 150 |  Loss_1: (0.3439) | Acc_1: (87.20%) (16854/19328)\n",
      "Epoch: 102 | Batch_idx: 160 |  Loss_1: (0.3435) | Acc_1: (87.22%) (17974/20608)\n",
      "Epoch: 102 | Batch_idx: 170 |  Loss_1: (0.3438) | Acc_1: (87.23%) (19092/21888)\n",
      "Epoch: 102 | Batch_idx: 180 |  Loss_1: (0.3456) | Acc_1: (87.16%) (20194/23168)\n",
      "Epoch: 102 | Batch_idx: 190 |  Loss_1: (0.3444) | Acc_1: (87.20%) (21319/24448)\n",
      "Epoch: 102 | Batch_idx: 200 |  Loss_1: (0.3435) | Acc_1: (87.22%) (22439/25728)\n",
      "Epoch: 102 | Batch_idx: 210 |  Loss_1: (0.3431) | Acc_1: (87.24%) (23562/27008)\n",
      "Epoch: 102 | Batch_idx: 220 |  Loss_1: (0.3430) | Acc_1: (87.26%) (24683/28288)\n",
      "Epoch: 102 | Batch_idx: 230 |  Loss_1: (0.3431) | Acc_1: (87.25%) (25799/29568)\n",
      "Epoch: 102 | Batch_idx: 240 |  Loss_1: (0.3441) | Acc_1: (87.21%) (26904/30848)\n",
      "Epoch: 102 | Batch_idx: 250 |  Loss_1: (0.3450) | Acc_1: (87.19%) (28012/32128)\n",
      "Epoch: 102 | Batch_idx: 260 |  Loss_1: (0.3454) | Acc_1: (87.16%) (29120/33408)\n",
      "Epoch: 102 | Batch_idx: 270 |  Loss_1: (0.3467) | Acc_1: (87.14%) (30226/34688)\n",
      "Epoch: 102 | Batch_idx: 280 |  Loss_1: (0.3473) | Acc_1: (87.09%) (31326/35968)\n",
      "Epoch: 102 | Batch_idx: 290 |  Loss_1: (0.3459) | Acc_1: (87.14%) (32459/37248)\n",
      "Epoch: 102 | Batch_idx: 300 |  Loss_1: (0.3458) | Acc_1: (87.15%) (33578/38528)\n",
      "Epoch: 102 | Batch_idx: 310 |  Loss_1: (0.3465) | Acc_1: (87.11%) (34678/39808)\n",
      "Epoch: 102 | Batch_idx: 320 |  Loss_1: (0.3469) | Acc_1: (87.09%) (35783/41088)\n",
      "Epoch: 102 | Batch_idx: 330 |  Loss_1: (0.3469) | Acc_1: (87.10%) (36901/42368)\n",
      "Epoch: 102 | Batch_idx: 340 |  Loss_1: (0.3464) | Acc_1: (87.11%) (38020/43648)\n",
      "Epoch: 102 | Batch_idx: 350 |  Loss_1: (0.3471) | Acc_1: (87.10%) (39132/44928)\n",
      "Epoch: 102 | Batch_idx: 360 |  Loss_1: (0.3472) | Acc_1: (87.08%) (40237/46208)\n",
      "Epoch: 102 | Batch_idx: 370 |  Loss_1: (0.3473) | Acc_1: (87.08%) (41352/47488)\n",
      "Epoch: 102 | Batch_idx: 380 |  Loss_1: (0.3466) | Acc_1: (87.11%) (42481/48768)\n",
      "Epoch: 102 | Batch_idx: 390 |  Loss_1: (0.3470) | Acc_1: (87.11%) (43555/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2578) | Acc: (91.39%) (9139/10000)\n",
      "Epoch: 103 | Batch_idx: 0 |  Loss_1: (0.3951) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 103 | Batch_idx: 10 |  Loss_1: (0.3379) | Acc_1: (86.86%) (1223/1408)\n",
      "Epoch: 103 | Batch_idx: 20 |  Loss_1: (0.3585) | Acc_1: (86.61%) (2328/2688)\n",
      "Epoch: 103 | Batch_idx: 30 |  Loss_1: (0.3494) | Acc_1: (86.72%) (3441/3968)\n",
      "Epoch: 103 | Batch_idx: 40 |  Loss_1: (0.3459) | Acc_1: (86.99%) (4565/5248)\n",
      "Epoch: 103 | Batch_idx: 50 |  Loss_1: (0.3380) | Acc_1: (87.33%) (5701/6528)\n",
      "Epoch: 103 | Batch_idx: 60 |  Loss_1: (0.3356) | Acc_1: (87.47%) (6830/7808)\n",
      "Epoch: 103 | Batch_idx: 70 |  Loss_1: (0.3359) | Acc_1: (87.35%) (7938/9088)\n",
      "Epoch: 103 | Batch_idx: 80 |  Loss_1: (0.3381) | Acc_1: (87.23%) (9044/10368)\n",
      "Epoch: 103 | Batch_idx: 90 |  Loss_1: (0.3385) | Acc_1: (87.33%) (10172/11648)\n",
      "Epoch: 103 | Batch_idx: 100 |  Loss_1: (0.3402) | Acc_1: (87.32%) (11289/12928)\n",
      "Epoch: 103 | Batch_idx: 110 |  Loss_1: (0.3392) | Acc_1: (87.35%) (12411/14208)\n",
      "Epoch: 103 | Batch_idx: 120 |  Loss_1: (0.3350) | Acc_1: (87.49%) (13550/15488)\n",
      "Epoch: 103 | Batch_idx: 130 |  Loss_1: (0.3320) | Acc_1: (87.54%) (14679/16768)\n",
      "Epoch: 103 | Batch_idx: 140 |  Loss_1: (0.3321) | Acc_1: (87.55%) (15801/18048)\n",
      "Epoch: 103 | Batch_idx: 150 |  Loss_1: (0.3334) | Acc_1: (87.49%) (16910/19328)\n",
      "Epoch: 103 | Batch_idx: 160 |  Loss_1: (0.3355) | Acc_1: (87.42%) (18016/20608)\n",
      "Epoch: 103 | Batch_idx: 170 |  Loss_1: (0.3356) | Acc_1: (87.44%) (19138/21888)\n",
      "Epoch: 103 | Batch_idx: 180 |  Loss_1: (0.3384) | Acc_1: (87.37%) (20243/23168)\n",
      "Epoch: 103 | Batch_idx: 190 |  Loss_1: (0.3401) | Acc_1: (87.32%) (21347/24448)\n",
      "Epoch: 103 | Batch_idx: 200 |  Loss_1: (0.3421) | Acc_1: (87.24%) (22445/25728)\n",
      "Epoch: 103 | Batch_idx: 210 |  Loss_1: (0.3422) | Acc_1: (87.22%) (23557/27008)\n",
      "Epoch: 103 | Batch_idx: 220 |  Loss_1: (0.3440) | Acc_1: (87.14%) (24651/28288)\n",
      "Epoch: 103 | Batch_idx: 230 |  Loss_1: (0.3443) | Acc_1: (87.17%) (25774/29568)\n",
      "Epoch: 103 | Batch_idx: 240 |  Loss_1: (0.3442) | Acc_1: (87.22%) (26905/30848)\n",
      "Epoch: 103 | Batch_idx: 250 |  Loss_1: (0.3451) | Acc_1: (87.17%) (28005/32128)\n",
      "Epoch: 103 | Batch_idx: 260 |  Loss_1: (0.3461) | Acc_1: (87.16%) (29118/33408)\n",
      "Epoch: 103 | Batch_idx: 270 |  Loss_1: (0.3468) | Acc_1: (87.15%) (30230/34688)\n",
      "Epoch: 103 | Batch_idx: 280 |  Loss_1: (0.3475) | Acc_1: (87.12%) (31337/35968)\n",
      "Epoch: 103 | Batch_idx: 290 |  Loss_1: (0.3469) | Acc_1: (87.14%) (32458/37248)\n",
      "Epoch: 103 | Batch_idx: 300 |  Loss_1: (0.3471) | Acc_1: (87.13%) (33569/38528)\n",
      "Epoch: 103 | Batch_idx: 310 |  Loss_1: (0.3483) | Acc_1: (87.09%) (34668/39808)\n",
      "Epoch: 103 | Batch_idx: 320 |  Loss_1: (0.3475) | Acc_1: (87.11%) (35793/41088)\n",
      "Epoch: 103 | Batch_idx: 330 |  Loss_1: (0.3478) | Acc_1: (87.10%) (36903/42368)\n",
      "Epoch: 103 | Batch_idx: 340 |  Loss_1: (0.3479) | Acc_1: (87.11%) (38022/43648)\n",
      "Epoch: 103 | Batch_idx: 350 |  Loss_1: (0.3483) | Acc_1: (87.09%) (39128/44928)\n",
      "Epoch: 103 | Batch_idx: 360 |  Loss_1: (0.3486) | Acc_1: (87.08%) (40236/46208)\n",
      "Epoch: 103 | Batch_idx: 370 |  Loss_1: (0.3484) | Acc_1: (87.07%) (41350/47488)\n",
      "Epoch: 103 | Batch_idx: 380 |  Loss_1: (0.3475) | Acc_1: (87.11%) (42482/48768)\n",
      "Epoch: 103 | Batch_idx: 390 |  Loss_1: (0.3475) | Acc_1: (87.12%) (43559/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2654) | Acc: (91.38%) (9138/10000)\n",
      "Epoch: 104 | Batch_idx: 0 |  Loss_1: (0.4245) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 104 | Batch_idx: 10 |  Loss_1: (0.3421) | Acc_1: (87.43%) (1231/1408)\n",
      "Epoch: 104 | Batch_idx: 20 |  Loss_1: (0.3363) | Acc_1: (87.39%) (2349/2688)\n",
      "Epoch: 104 | Batch_idx: 30 |  Loss_1: (0.3316) | Acc_1: (87.65%) (3478/3968)\n",
      "Epoch: 104 | Batch_idx: 40 |  Loss_1: (0.3343) | Acc_1: (87.48%) (4591/5248)\n",
      "Epoch: 104 | Batch_idx: 50 |  Loss_1: (0.3354) | Acc_1: (87.55%) (5715/6528)\n",
      "Epoch: 104 | Batch_idx: 60 |  Loss_1: (0.3337) | Acc_1: (87.64%) (6843/7808)\n",
      "Epoch: 104 | Batch_idx: 70 |  Loss_1: (0.3378) | Acc_1: (87.43%) (7946/9088)\n",
      "Epoch: 104 | Batch_idx: 80 |  Loss_1: (0.3354) | Acc_1: (87.46%) (9068/10368)\n",
      "Epoch: 104 | Batch_idx: 90 |  Loss_1: (0.3361) | Acc_1: (87.46%) (10187/11648)\n",
      "Epoch: 104 | Batch_idx: 100 |  Loss_1: (0.3377) | Acc_1: (87.36%) (11294/12928)\n",
      "Epoch: 104 | Batch_idx: 110 |  Loss_1: (0.3401) | Acc_1: (87.35%) (12410/14208)\n",
      "Epoch: 104 | Batch_idx: 120 |  Loss_1: (0.3408) | Acc_1: (87.33%) (13525/15488)\n",
      "Epoch: 104 | Batch_idx: 130 |  Loss_1: (0.3398) | Acc_1: (87.38%) (14652/16768)\n",
      "Epoch: 104 | Batch_idx: 140 |  Loss_1: (0.3390) | Acc_1: (87.39%) (15772/18048)\n",
      "Epoch: 104 | Batch_idx: 150 |  Loss_1: (0.3395) | Acc_1: (87.37%) (16887/19328)\n",
      "Epoch: 104 | Batch_idx: 160 |  Loss_1: (0.3381) | Acc_1: (87.40%) (18011/20608)\n",
      "Epoch: 104 | Batch_idx: 170 |  Loss_1: (0.3387) | Acc_1: (87.38%) (19126/21888)\n",
      "Epoch: 104 | Batch_idx: 180 |  Loss_1: (0.3377) | Acc_1: (87.46%) (20263/23168)\n",
      "Epoch: 104 | Batch_idx: 190 |  Loss_1: (0.3363) | Acc_1: (87.54%) (21401/24448)\n",
      "Epoch: 104 | Batch_idx: 200 |  Loss_1: (0.3387) | Acc_1: (87.44%) (22496/25728)\n",
      "Epoch: 104 | Batch_idx: 210 |  Loss_1: (0.3399) | Acc_1: (87.38%) (23599/27008)\n",
      "Epoch: 104 | Batch_idx: 220 |  Loss_1: (0.3411) | Acc_1: (87.29%) (24694/28288)\n",
      "Epoch: 104 | Batch_idx: 230 |  Loss_1: (0.3413) | Acc_1: (87.27%) (25803/29568)\n",
      "Epoch: 104 | Batch_idx: 240 |  Loss_1: (0.3418) | Acc_1: (87.26%) (26918/30848)\n",
      "Epoch: 104 | Batch_idx: 250 |  Loss_1: (0.3421) | Acc_1: (87.27%) (28037/32128)\n",
      "Epoch: 104 | Batch_idx: 260 |  Loss_1: (0.3419) | Acc_1: (87.30%) (29165/33408)\n",
      "Epoch: 104 | Batch_idx: 270 |  Loss_1: (0.3418) | Acc_1: (87.29%) (30279/34688)\n",
      "Epoch: 104 | Batch_idx: 280 |  Loss_1: (0.3419) | Acc_1: (87.27%) (31391/35968)\n",
      "Epoch: 104 | Batch_idx: 290 |  Loss_1: (0.3407) | Acc_1: (87.33%) (32527/37248)\n",
      "Epoch: 104 | Batch_idx: 300 |  Loss_1: (0.3404) | Acc_1: (87.32%) (33641/38528)\n",
      "Epoch: 104 | Batch_idx: 310 |  Loss_1: (0.3411) | Acc_1: (87.31%) (34758/39808)\n",
      "Epoch: 104 | Batch_idx: 320 |  Loss_1: (0.3411) | Acc_1: (87.31%) (35875/41088)\n",
      "Epoch: 104 | Batch_idx: 330 |  Loss_1: (0.3406) | Acc_1: (87.35%) (37007/42368)\n",
      "Epoch: 104 | Batch_idx: 340 |  Loss_1: (0.3408) | Acc_1: (87.34%) (38120/43648)\n",
      "Epoch: 104 | Batch_idx: 350 |  Loss_1: (0.3407) | Acc_1: (87.35%) (39244/44928)\n",
      "Epoch: 104 | Batch_idx: 360 |  Loss_1: (0.3409) | Acc_1: (87.34%) (40359/46208)\n",
      "Epoch: 104 | Batch_idx: 370 |  Loss_1: (0.3418) | Acc_1: (87.31%) (41460/47488)\n",
      "Epoch: 104 | Batch_idx: 380 |  Loss_1: (0.3415) | Acc_1: (87.31%) (42579/48768)\n",
      "Epoch: 104 | Batch_idx: 390 |  Loss_1: (0.3410) | Acc_1: (87.36%) (43678/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2680) | Acc: (91.54%) (9154/10000)\n",
      "Epoch: 105 | Batch_idx: 0 |  Loss_1: (0.3697) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 105 | Batch_idx: 10 |  Loss_1: (0.3053) | Acc_1: (88.35%) (1244/1408)\n",
      "Epoch: 105 | Batch_idx: 20 |  Loss_1: (0.3201) | Acc_1: (87.91%) (2363/2688)\n",
      "Epoch: 105 | Batch_idx: 30 |  Loss_1: (0.3092) | Acc_1: (88.46%) (3510/3968)\n",
      "Epoch: 105 | Batch_idx: 40 |  Loss_1: (0.3159) | Acc_1: (88.22%) (4630/5248)\n",
      "Epoch: 105 | Batch_idx: 50 |  Loss_1: (0.3145) | Acc_1: (88.33%) (5766/6528)\n",
      "Epoch: 105 | Batch_idx: 60 |  Loss_1: (0.3209) | Acc_1: (88.06%) (6876/7808)\n",
      "Epoch: 105 | Batch_idx: 70 |  Loss_1: (0.3195) | Acc_1: (88.08%) (8005/9088)\n",
      "Epoch: 105 | Batch_idx: 80 |  Loss_1: (0.3210) | Acc_1: (88.01%) (9125/10368)\n",
      "Epoch: 105 | Batch_idx: 90 |  Loss_1: (0.3243) | Acc_1: (87.96%) (10246/11648)\n",
      "Epoch: 105 | Batch_idx: 100 |  Loss_1: (0.3252) | Acc_1: (87.90%) (11364/12928)\n",
      "Epoch: 105 | Batch_idx: 110 |  Loss_1: (0.3270) | Acc_1: (87.86%) (12483/14208)\n",
      "Epoch: 105 | Batch_idx: 120 |  Loss_1: (0.3283) | Acc_1: (87.87%) (13610/15488)\n",
      "Epoch: 105 | Batch_idx: 130 |  Loss_1: (0.3282) | Acc_1: (87.84%) (14729/16768)\n",
      "Epoch: 105 | Batch_idx: 140 |  Loss_1: (0.3282) | Acc_1: (87.88%) (15861/18048)\n",
      "Epoch: 105 | Batch_idx: 150 |  Loss_1: (0.3315) | Acc_1: (87.75%) (16960/19328)\n",
      "Epoch: 105 | Batch_idx: 160 |  Loss_1: (0.3314) | Acc_1: (87.74%) (18082/20608)\n",
      "Epoch: 105 | Batch_idx: 170 |  Loss_1: (0.3332) | Acc_1: (87.64%) (19183/21888)\n",
      "Epoch: 105 | Batch_idx: 180 |  Loss_1: (0.3338) | Acc_1: (87.62%) (20299/23168)\n",
      "Epoch: 105 | Batch_idx: 190 |  Loss_1: (0.3336) | Acc_1: (87.60%) (21416/24448)\n",
      "Epoch: 105 | Batch_idx: 200 |  Loss_1: (0.3321) | Acc_1: (87.67%) (22555/25728)\n",
      "Epoch: 105 | Batch_idx: 210 |  Loss_1: (0.3299) | Acc_1: (87.73%) (23694/27008)\n",
      "Epoch: 105 | Batch_idx: 220 |  Loss_1: (0.3290) | Acc_1: (87.76%) (24826/28288)\n",
      "Epoch: 105 | Batch_idx: 230 |  Loss_1: (0.3308) | Acc_1: (87.71%) (25934/29568)\n",
      "Epoch: 105 | Batch_idx: 240 |  Loss_1: (0.3322) | Acc_1: (87.64%) (27034/30848)\n",
      "Epoch: 105 | Batch_idx: 250 |  Loss_1: (0.3328) | Acc_1: (87.61%) (28147/32128)\n",
      "Epoch: 105 | Batch_idx: 260 |  Loss_1: (0.3337) | Acc_1: (87.54%) (29246/33408)\n",
      "Epoch: 105 | Batch_idx: 270 |  Loss_1: (0.3336) | Acc_1: (87.54%) (30367/34688)\n",
      "Epoch: 105 | Batch_idx: 280 |  Loss_1: (0.3339) | Acc_1: (87.53%) (31483/35968)\n",
      "Epoch: 105 | Batch_idx: 290 |  Loss_1: (0.3335) | Acc_1: (87.56%) (32614/37248)\n",
      "Epoch: 105 | Batch_idx: 300 |  Loss_1: (0.3344) | Acc_1: (87.53%) (33724/38528)\n",
      "Epoch: 105 | Batch_idx: 310 |  Loss_1: (0.3343) | Acc_1: (87.54%) (34846/39808)\n",
      "Epoch: 105 | Batch_idx: 320 |  Loss_1: (0.3351) | Acc_1: (87.53%) (35965/41088)\n",
      "Epoch: 105 | Batch_idx: 330 |  Loss_1: (0.3364) | Acc_1: (87.48%) (37063/42368)\n",
      "Epoch: 105 | Batch_idx: 340 |  Loss_1: (0.3368) | Acc_1: (87.46%) (38176/43648)\n",
      "Epoch: 105 | Batch_idx: 350 |  Loss_1: (0.3368) | Acc_1: (87.46%) (39295/44928)\n",
      "Epoch: 105 | Batch_idx: 360 |  Loss_1: (0.3376) | Acc_1: (87.42%) (40397/46208)\n",
      "Epoch: 105 | Batch_idx: 370 |  Loss_1: (0.3387) | Acc_1: (87.39%) (41501/47488)\n",
      "Epoch: 105 | Batch_idx: 380 |  Loss_1: (0.3383) | Acc_1: (87.41%) (42630/48768)\n",
      "Epoch: 105 | Batch_idx: 390 |  Loss_1: (0.3391) | Acc_1: (87.39%) (43695/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3111) | Acc: (90.62%) (9062/10000)\n",
      "Epoch: 106 | Batch_idx: 0 |  Loss_1: (0.2074) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 106 | Batch_idx: 10 |  Loss_1: (0.3319) | Acc_1: (87.43%) (1231/1408)\n",
      "Epoch: 106 | Batch_idx: 20 |  Loss_1: (0.3295) | Acc_1: (87.95%) (2364/2688)\n",
      "Epoch: 106 | Batch_idx: 30 |  Loss_1: (0.3352) | Acc_1: (87.60%) (3476/3968)\n",
      "Epoch: 106 | Batch_idx: 40 |  Loss_1: (0.3296) | Acc_1: (87.88%) (4612/5248)\n",
      "Epoch: 106 | Batch_idx: 50 |  Loss_1: (0.3285) | Acc_1: (87.99%) (5744/6528)\n",
      "Epoch: 106 | Batch_idx: 60 |  Loss_1: (0.3294) | Acc_1: (87.85%) (6859/7808)\n",
      "Epoch: 106 | Batch_idx: 70 |  Loss_1: (0.3300) | Acc_1: (87.93%) (7991/9088)\n",
      "Epoch: 106 | Batch_idx: 80 |  Loss_1: (0.3319) | Acc_1: (87.78%) (9101/10368)\n",
      "Epoch: 106 | Batch_idx: 90 |  Loss_1: (0.3303) | Acc_1: (87.87%) (10235/11648)\n",
      "Epoch: 106 | Batch_idx: 100 |  Loss_1: (0.3325) | Acc_1: (87.69%) (11337/12928)\n",
      "Epoch: 106 | Batch_idx: 110 |  Loss_1: (0.3334) | Acc_1: (87.68%) (12458/14208)\n",
      "Epoch: 106 | Batch_idx: 120 |  Loss_1: (0.3333) | Acc_1: (87.70%) (13583/15488)\n",
      "Epoch: 106 | Batch_idx: 130 |  Loss_1: (0.3346) | Acc_1: (87.63%) (14694/16768)\n",
      "Epoch: 106 | Batch_idx: 140 |  Loss_1: (0.3351) | Acc_1: (87.56%) (15802/18048)\n",
      "Epoch: 106 | Batch_idx: 150 |  Loss_1: (0.3356) | Acc_1: (87.51%) (16914/19328)\n",
      "Epoch: 106 | Batch_idx: 160 |  Loss_1: (0.3373) | Acc_1: (87.41%) (18014/20608)\n",
      "Epoch: 106 | Batch_idx: 170 |  Loss_1: (0.3369) | Acc_1: (87.42%) (19134/21888)\n",
      "Epoch: 106 | Batch_idx: 180 |  Loss_1: (0.3369) | Acc_1: (87.41%) (20250/23168)\n",
      "Epoch: 106 | Batch_idx: 190 |  Loss_1: (0.3363) | Acc_1: (87.44%) (21378/24448)\n",
      "Epoch: 106 | Batch_idx: 200 |  Loss_1: (0.3350) | Acc_1: (87.51%) (22514/25728)\n",
      "Epoch: 106 | Batch_idx: 210 |  Loss_1: (0.3339) | Acc_1: (87.52%) (23638/27008)\n",
      "Epoch: 106 | Batch_idx: 220 |  Loss_1: (0.3347) | Acc_1: (87.51%) (24756/28288)\n",
      "Epoch: 106 | Batch_idx: 230 |  Loss_1: (0.3354) | Acc_1: (87.48%) (25867/29568)\n",
      "Epoch: 106 | Batch_idx: 240 |  Loss_1: (0.3365) | Acc_1: (87.46%) (26981/30848)\n",
      "Epoch: 106 | Batch_idx: 250 |  Loss_1: (0.3374) | Acc_1: (87.41%) (28083/32128)\n",
      "Epoch: 106 | Batch_idx: 260 |  Loss_1: (0.3379) | Acc_1: (87.39%) (29196/33408)\n",
      "Epoch: 106 | Batch_idx: 270 |  Loss_1: (0.3389) | Acc_1: (87.36%) (30303/34688)\n",
      "Epoch: 106 | Batch_idx: 280 |  Loss_1: (0.3391) | Acc_1: (87.35%) (31418/35968)\n",
      "Epoch: 106 | Batch_idx: 290 |  Loss_1: (0.3394) | Acc_1: (87.36%) (32540/37248)\n",
      "Epoch: 106 | Batch_idx: 300 |  Loss_1: (0.3402) | Acc_1: (87.34%) (33651/38528)\n",
      "Epoch: 106 | Batch_idx: 310 |  Loss_1: (0.3405) | Acc_1: (87.34%) (34769/39808)\n",
      "Epoch: 106 | Batch_idx: 320 |  Loss_1: (0.3414) | Acc_1: (87.32%) (35877/41088)\n",
      "Epoch: 106 | Batch_idx: 330 |  Loss_1: (0.3415) | Acc_1: (87.32%) (36996/42368)\n",
      "Epoch: 106 | Batch_idx: 340 |  Loss_1: (0.3416) | Acc_1: (87.32%) (38115/43648)\n",
      "Epoch: 106 | Batch_idx: 350 |  Loss_1: (0.3411) | Acc_1: (87.36%) (39250/44928)\n",
      "Epoch: 106 | Batch_idx: 360 |  Loss_1: (0.3424) | Acc_1: (87.30%) (40341/46208)\n",
      "Epoch: 106 | Batch_idx: 370 |  Loss_1: (0.3432) | Acc_1: (87.27%) (41442/47488)\n",
      "Epoch: 106 | Batch_idx: 380 |  Loss_1: (0.3436) | Acc_1: (87.25%) (42551/48768)\n",
      "Epoch: 106 | Batch_idx: 390 |  Loss_1: (0.3427) | Acc_1: (87.27%) (43637/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2914) | Acc: (90.95%) (9095/10000)\n",
      "Epoch: 107 | Batch_idx: 0 |  Loss_1: (0.4763) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 107 | Batch_idx: 10 |  Loss_1: (0.3400) | Acc_1: (86.58%) (1219/1408)\n",
      "Epoch: 107 | Batch_idx: 20 |  Loss_1: (0.3313) | Acc_1: (87.20%) (2344/2688)\n",
      "Epoch: 107 | Batch_idx: 30 |  Loss_1: (0.3263) | Acc_1: (87.45%) (3470/3968)\n",
      "Epoch: 107 | Batch_idx: 40 |  Loss_1: (0.3183) | Acc_1: (87.50%) (4592/5248)\n",
      "Epoch: 107 | Batch_idx: 50 |  Loss_1: (0.3126) | Acc_1: (87.62%) (5720/6528)\n",
      "Epoch: 107 | Batch_idx: 60 |  Loss_1: (0.3163) | Acc_1: (87.47%) (6830/7808)\n",
      "Epoch: 107 | Batch_idx: 70 |  Loss_1: (0.3141) | Acc_1: (87.62%) (7963/9088)\n",
      "Epoch: 107 | Batch_idx: 80 |  Loss_1: (0.3165) | Acc_1: (87.62%) (9084/10368)\n",
      "Epoch: 107 | Batch_idx: 90 |  Loss_1: (0.3219) | Acc_1: (87.56%) (10199/11648)\n",
      "Epoch: 107 | Batch_idx: 100 |  Loss_1: (0.3209) | Acc_1: (87.65%) (11332/12928)\n",
      "Epoch: 107 | Batch_idx: 110 |  Loss_1: (0.3206) | Acc_1: (87.70%) (12460/14208)\n",
      "Epoch: 107 | Batch_idx: 120 |  Loss_1: (0.3218) | Acc_1: (87.67%) (13578/15488)\n",
      "Epoch: 107 | Batch_idx: 130 |  Loss_1: (0.3243) | Acc_1: (87.58%) (14686/16768)\n",
      "Epoch: 107 | Batch_idx: 140 |  Loss_1: (0.3261) | Acc_1: (87.57%) (15804/18048)\n",
      "Epoch: 107 | Batch_idx: 150 |  Loss_1: (0.3264) | Acc_1: (87.59%) (16930/19328)\n",
      "Epoch: 107 | Batch_idx: 160 |  Loss_1: (0.3274) | Acc_1: (87.55%) (18042/20608)\n",
      "Epoch: 107 | Batch_idx: 170 |  Loss_1: (0.3286) | Acc_1: (87.50%) (19153/21888)\n",
      "Epoch: 107 | Batch_idx: 180 |  Loss_1: (0.3301) | Acc_1: (87.45%) (20260/23168)\n",
      "Epoch: 107 | Batch_idx: 190 |  Loss_1: (0.3306) | Acc_1: (87.45%) (21379/24448)\n",
      "Epoch: 107 | Batch_idx: 200 |  Loss_1: (0.3305) | Acc_1: (87.46%) (22501/25728)\n",
      "Epoch: 107 | Batch_idx: 210 |  Loss_1: (0.3297) | Acc_1: (87.49%) (23628/27008)\n",
      "Epoch: 107 | Batch_idx: 220 |  Loss_1: (0.3292) | Acc_1: (87.49%) (24749/28288)\n",
      "Epoch: 107 | Batch_idx: 230 |  Loss_1: (0.3289) | Acc_1: (87.53%) (25880/29568)\n",
      "Epoch: 107 | Batch_idx: 240 |  Loss_1: (0.3308) | Acc_1: (87.46%) (26979/30848)\n",
      "Epoch: 107 | Batch_idx: 250 |  Loss_1: (0.3306) | Acc_1: (87.49%) (28110/32128)\n",
      "Epoch: 107 | Batch_idx: 260 |  Loss_1: (0.3286) | Acc_1: (87.57%) (29254/33408)\n",
      "Epoch: 107 | Batch_idx: 270 |  Loss_1: (0.3283) | Acc_1: (87.56%) (30374/34688)\n",
      "Epoch: 107 | Batch_idx: 280 |  Loss_1: (0.3281) | Acc_1: (87.54%) (31488/35968)\n",
      "Epoch: 107 | Batch_idx: 290 |  Loss_1: (0.3287) | Acc_1: (87.55%) (32610/37248)\n",
      "Epoch: 107 | Batch_idx: 300 |  Loss_1: (0.3295) | Acc_1: (87.52%) (33720/38528)\n",
      "Epoch: 107 | Batch_idx: 310 |  Loss_1: (0.3287) | Acc_1: (87.57%) (34858/39808)\n",
      "Epoch: 107 | Batch_idx: 320 |  Loss_1: (0.3294) | Acc_1: (87.51%) (35958/41088)\n",
      "Epoch: 107 | Batch_idx: 330 |  Loss_1: (0.3293) | Acc_1: (87.54%) (37089/42368)\n",
      "Epoch: 107 | Batch_idx: 340 |  Loss_1: (0.3283) | Acc_1: (87.58%) (38227/43648)\n",
      "Epoch: 107 | Batch_idx: 350 |  Loss_1: (0.3283) | Acc_1: (87.60%) (39357/44928)\n",
      "Epoch: 107 | Batch_idx: 360 |  Loss_1: (0.3296) | Acc_1: (87.55%) (40455/46208)\n",
      "Epoch: 107 | Batch_idx: 370 |  Loss_1: (0.3299) | Acc_1: (87.54%) (41570/47488)\n",
      "Epoch: 107 | Batch_idx: 380 |  Loss_1: (0.3302) | Acc_1: (87.53%) (42688/48768)\n",
      "Epoch: 107 | Batch_idx: 390 |  Loss_1: (0.3301) | Acc_1: (87.54%) (43769/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2936) | Acc: (90.84%) (9084/10000)\n",
      "Epoch: 108 | Batch_idx: 0 |  Loss_1: (0.4332) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 108 | Batch_idx: 10 |  Loss_1: (0.3455) | Acc_1: (87.36%) (1230/1408)\n",
      "Epoch: 108 | Batch_idx: 20 |  Loss_1: (0.3383) | Acc_1: (87.46%) (2351/2688)\n",
      "Epoch: 108 | Batch_idx: 30 |  Loss_1: (0.3281) | Acc_1: (87.95%) (3490/3968)\n",
      "Epoch: 108 | Batch_idx: 40 |  Loss_1: (0.3388) | Acc_1: (87.50%) (4592/5248)\n",
      "Epoch: 108 | Batch_idx: 50 |  Loss_1: (0.3326) | Acc_1: (87.84%) (5734/6528)\n",
      "Epoch: 108 | Batch_idx: 60 |  Loss_1: (0.3340) | Acc_1: (87.74%) (6851/7808)\n",
      "Epoch: 108 | Batch_idx: 70 |  Loss_1: (0.3402) | Acc_1: (87.56%) (7957/9088)\n",
      "Epoch: 108 | Batch_idx: 80 |  Loss_1: (0.3413) | Acc_1: (87.51%) (9073/10368)\n",
      "Epoch: 108 | Batch_idx: 90 |  Loss_1: (0.3466) | Acc_1: (87.32%) (10171/11648)\n",
      "Epoch: 108 | Batch_idx: 100 |  Loss_1: (0.3477) | Acc_1: (87.26%) (11281/12928)\n",
      "Epoch: 108 | Batch_idx: 110 |  Loss_1: (0.3503) | Acc_1: (87.13%) (12379/14208)\n",
      "Epoch: 108 | Batch_idx: 120 |  Loss_1: (0.3527) | Acc_1: (86.97%) (13470/15488)\n",
      "Epoch: 108 | Batch_idx: 130 |  Loss_1: (0.3553) | Acc_1: (86.81%) (14557/16768)\n",
      "Epoch: 108 | Batch_idx: 140 |  Loss_1: (0.3560) | Acc_1: (86.81%) (15668/18048)\n",
      "Epoch: 108 | Batch_idx: 150 |  Loss_1: (0.3537) | Acc_1: (86.86%) (16788/19328)\n",
      "Epoch: 108 | Batch_idx: 160 |  Loss_1: (0.3523) | Acc_1: (86.89%) (17907/20608)\n",
      "Epoch: 108 | Batch_idx: 170 |  Loss_1: (0.3526) | Acc_1: (86.95%) (19032/21888)\n",
      "Epoch: 108 | Batch_idx: 180 |  Loss_1: (0.3542) | Acc_1: (86.91%) (20135/23168)\n",
      "Epoch: 108 | Batch_idx: 190 |  Loss_1: (0.3527) | Acc_1: (86.96%) (21261/24448)\n",
      "Epoch: 108 | Batch_idx: 200 |  Loss_1: (0.3512) | Acc_1: (87.03%) (22391/25728)\n",
      "Epoch: 108 | Batch_idx: 210 |  Loss_1: (0.3511) | Acc_1: (87.00%) (23496/27008)\n",
      "Epoch: 108 | Batch_idx: 220 |  Loss_1: (0.3494) | Acc_1: (87.03%) (24620/28288)\n",
      "Epoch: 108 | Batch_idx: 230 |  Loss_1: (0.3482) | Acc_1: (87.07%) (25745/29568)\n",
      "Epoch: 108 | Batch_idx: 240 |  Loss_1: (0.3477) | Acc_1: (87.09%) (26865/30848)\n",
      "Epoch: 108 | Batch_idx: 250 |  Loss_1: (0.3462) | Acc_1: (87.14%) (27997/32128)\n",
      "Epoch: 108 | Batch_idx: 260 |  Loss_1: (0.3453) | Acc_1: (87.17%) (29121/33408)\n",
      "Epoch: 108 | Batch_idx: 270 |  Loss_1: (0.3443) | Acc_1: (87.19%) (30245/34688)\n",
      "Epoch: 108 | Batch_idx: 280 |  Loss_1: (0.3440) | Acc_1: (87.22%) (31372/35968)\n",
      "Epoch: 108 | Batch_idx: 290 |  Loss_1: (0.3435) | Acc_1: (87.27%) (32505/37248)\n",
      "Epoch: 108 | Batch_idx: 300 |  Loss_1: (0.3434) | Acc_1: (87.28%) (33629/38528)\n",
      "Epoch: 108 | Batch_idx: 310 |  Loss_1: (0.3432) | Acc_1: (87.32%) (34759/39808)\n",
      "Epoch: 108 | Batch_idx: 320 |  Loss_1: (0.3428) | Acc_1: (87.33%) (35884/41088)\n",
      "Epoch: 108 | Batch_idx: 330 |  Loss_1: (0.3431) | Acc_1: (87.31%) (36993/42368)\n",
      "Epoch: 108 | Batch_idx: 340 |  Loss_1: (0.3435) | Acc_1: (87.31%) (38111/43648)\n",
      "Epoch: 108 | Batch_idx: 350 |  Loss_1: (0.3437) | Acc_1: (87.31%) (39228/44928)\n",
      "Epoch: 108 | Batch_idx: 360 |  Loss_1: (0.3425) | Acc_1: (87.37%) (40371/46208)\n",
      "Epoch: 108 | Batch_idx: 370 |  Loss_1: (0.3414) | Acc_1: (87.38%) (41496/47488)\n",
      "Epoch: 108 | Batch_idx: 380 |  Loss_1: (0.3411) | Acc_1: (87.39%) (42617/48768)\n",
      "Epoch: 108 | Batch_idx: 390 |  Loss_1: (0.3407) | Acc_1: (87.39%) (43696/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2923) | Acc: (91.39%) (9139/10000)\n",
      "Epoch: 109 | Batch_idx: 0 |  Loss_1: (0.3959) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 109 | Batch_idx: 10 |  Loss_1: (0.3509) | Acc_1: (86.79%) (1222/1408)\n",
      "Epoch: 109 | Batch_idx: 20 |  Loss_1: (0.3665) | Acc_1: (86.38%) (2322/2688)\n",
      "Epoch: 109 | Batch_idx: 30 |  Loss_1: (0.3554) | Acc_1: (86.67%) (3439/3968)\n",
      "Epoch: 109 | Batch_idx: 40 |  Loss_1: (0.3452) | Acc_1: (87.06%) (4569/5248)\n",
      "Epoch: 109 | Batch_idx: 50 |  Loss_1: (0.3429) | Acc_1: (87.24%) (5695/6528)\n",
      "Epoch: 109 | Batch_idx: 60 |  Loss_1: (0.3433) | Acc_1: (87.13%) (6803/7808)\n",
      "Epoch: 109 | Batch_idx: 70 |  Loss_1: (0.3368) | Acc_1: (87.44%) (7947/9088)\n",
      "Epoch: 109 | Batch_idx: 80 |  Loss_1: (0.3371) | Acc_1: (87.36%) (9058/10368)\n",
      "Epoch: 109 | Batch_idx: 90 |  Loss_1: (0.3351) | Acc_1: (87.45%) (10186/11648)\n",
      "Epoch: 109 | Batch_idx: 100 |  Loss_1: (0.3339) | Acc_1: (87.53%) (11316/12928)\n",
      "Epoch: 109 | Batch_idx: 110 |  Loss_1: (0.3353) | Acc_1: (87.46%) (12427/14208)\n",
      "Epoch: 109 | Batch_idx: 120 |  Loss_1: (0.3351) | Acc_1: (87.52%) (13555/15488)\n",
      "Epoch: 109 | Batch_idx: 130 |  Loss_1: (0.3358) | Acc_1: (87.49%) (14670/16768)\n",
      "Epoch: 109 | Batch_idx: 140 |  Loss_1: (0.3316) | Acc_1: (87.63%) (15815/18048)\n",
      "Epoch: 109 | Batch_idx: 150 |  Loss_1: (0.3312) | Acc_1: (87.62%) (16936/19328)\n",
      "Epoch: 109 | Batch_idx: 160 |  Loss_1: (0.3328) | Acc_1: (87.52%) (18036/20608)\n",
      "Epoch: 109 | Batch_idx: 170 |  Loss_1: (0.3343) | Acc_1: (87.53%) (19158/21888)\n",
      "Epoch: 109 | Batch_idx: 180 |  Loss_1: (0.3339) | Acc_1: (87.51%) (20275/23168)\n",
      "Epoch: 109 | Batch_idx: 190 |  Loss_1: (0.3327) | Acc_1: (87.54%) (21402/24448)\n",
      "Epoch: 109 | Batch_idx: 200 |  Loss_1: (0.3339) | Acc_1: (87.45%) (22499/25728)\n",
      "Epoch: 109 | Batch_idx: 210 |  Loss_1: (0.3340) | Acc_1: (87.51%) (23634/27008)\n",
      "Epoch: 109 | Batch_idx: 220 |  Loss_1: (0.3334) | Acc_1: (87.55%) (24765/28288)\n",
      "Epoch: 109 | Batch_idx: 230 |  Loss_1: (0.3323) | Acc_1: (87.56%) (25891/29568)\n",
      "Epoch: 109 | Batch_idx: 240 |  Loss_1: (0.3344) | Acc_1: (87.49%) (26988/30848)\n",
      "Epoch: 109 | Batch_idx: 250 |  Loss_1: (0.3349) | Acc_1: (87.48%) (28105/32128)\n",
      "Epoch: 109 | Batch_idx: 260 |  Loss_1: (0.3351) | Acc_1: (87.48%) (29226/33408)\n",
      "Epoch: 109 | Batch_idx: 270 |  Loss_1: (0.3351) | Acc_1: (87.51%) (30354/34688)\n",
      "Epoch: 109 | Batch_idx: 280 |  Loss_1: (0.3349) | Acc_1: (87.51%) (31475/35968)\n",
      "Epoch: 109 | Batch_idx: 290 |  Loss_1: (0.3347) | Acc_1: (87.51%) (32594/37248)\n",
      "Epoch: 109 | Batch_idx: 300 |  Loss_1: (0.3338) | Acc_1: (87.54%) (33728/38528)\n",
      "Epoch: 109 | Batch_idx: 310 |  Loss_1: (0.3323) | Acc_1: (87.59%) (34866/39808)\n",
      "Epoch: 109 | Batch_idx: 320 |  Loss_1: (0.3328) | Acc_1: (87.55%) (35973/41088)\n",
      "Epoch: 109 | Batch_idx: 330 |  Loss_1: (0.3329) | Acc_1: (87.54%) (37090/42368)\n",
      "Epoch: 109 | Batch_idx: 340 |  Loss_1: (0.3321) | Acc_1: (87.58%) (38227/43648)\n",
      "Epoch: 109 | Batch_idx: 350 |  Loss_1: (0.3315) | Acc_1: (87.60%) (39358/44928)\n",
      "Epoch: 109 | Batch_idx: 360 |  Loss_1: (0.3323) | Acc_1: (87.58%) (40468/46208)\n",
      "Epoch: 109 | Batch_idx: 370 |  Loss_1: (0.3321) | Acc_1: (87.58%) (41592/47488)\n",
      "Epoch: 109 | Batch_idx: 380 |  Loss_1: (0.3328) | Acc_1: (87.57%) (42705/48768)\n",
      "Epoch: 109 | Batch_idx: 390 |  Loss_1: (0.3331) | Acc_1: (87.57%) (43786/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3207) | Acc: (90.41%) (9041/10000)\n",
      "Epoch: 110 | Batch_idx: 0 |  Loss_1: (0.3700) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 110 | Batch_idx: 10 |  Loss_1: (0.3537) | Acc_1: (87.50%) (1232/1408)\n",
      "Epoch: 110 | Batch_idx: 20 |  Loss_1: (0.3305) | Acc_1: (88.06%) (2367/2688)\n",
      "Epoch: 110 | Batch_idx: 30 |  Loss_1: (0.3282) | Acc_1: (87.78%) (3483/3968)\n",
      "Epoch: 110 | Batch_idx: 40 |  Loss_1: (0.3268) | Acc_1: (87.88%) (4612/5248)\n",
      "Epoch: 110 | Batch_idx: 50 |  Loss_1: (0.3211) | Acc_1: (88.16%) (5755/6528)\n",
      "Epoch: 110 | Batch_idx: 60 |  Loss_1: (0.3249) | Acc_1: (88.04%) (6874/7808)\n",
      "Epoch: 110 | Batch_idx: 70 |  Loss_1: (0.3191) | Acc_1: (88.15%) (8011/9088)\n",
      "Epoch: 110 | Batch_idx: 80 |  Loss_1: (0.3199) | Acc_1: (88.16%) (9140/10368)\n",
      "Epoch: 110 | Batch_idx: 90 |  Loss_1: (0.3172) | Acc_1: (88.30%) (10285/11648)\n",
      "Epoch: 110 | Batch_idx: 100 |  Loss_1: (0.3181) | Acc_1: (88.23%) (11406/12928)\n",
      "Epoch: 110 | Batch_idx: 110 |  Loss_1: (0.3183) | Acc_1: (88.23%) (12536/14208)\n",
      "Epoch: 110 | Batch_idx: 120 |  Loss_1: (0.3170) | Acc_1: (88.27%) (13671/15488)\n",
      "Epoch: 110 | Batch_idx: 130 |  Loss_1: (0.3173) | Acc_1: (88.25%) (14797/16768)\n",
      "Epoch: 110 | Batch_idx: 140 |  Loss_1: (0.3157) | Acc_1: (88.26%) (15929/18048)\n",
      "Epoch: 110 | Batch_idx: 150 |  Loss_1: (0.3173) | Acc_1: (88.15%) (17038/19328)\n",
      "Epoch: 110 | Batch_idx: 160 |  Loss_1: (0.3193) | Acc_1: (88.09%) (18154/20608)\n",
      "Epoch: 110 | Batch_idx: 170 |  Loss_1: (0.3197) | Acc_1: (88.08%) (19280/21888)\n",
      "Epoch: 110 | Batch_idx: 180 |  Loss_1: (0.3196) | Acc_1: (88.10%) (20411/23168)\n",
      "Epoch: 110 | Batch_idx: 190 |  Loss_1: (0.3200) | Acc_1: (88.09%) (21537/24448)\n",
      "Epoch: 110 | Batch_idx: 200 |  Loss_1: (0.3183) | Acc_1: (88.13%) (22674/25728)\n",
      "Epoch: 110 | Batch_idx: 210 |  Loss_1: (0.3189) | Acc_1: (88.07%) (23787/27008)\n",
      "Epoch: 110 | Batch_idx: 220 |  Loss_1: (0.3206) | Acc_1: (88.03%) (24902/28288)\n",
      "Epoch: 110 | Batch_idx: 230 |  Loss_1: (0.3214) | Acc_1: (87.97%) (26010/29568)\n",
      "Epoch: 110 | Batch_idx: 240 |  Loss_1: (0.3222) | Acc_1: (87.96%) (27133/30848)\n",
      "Epoch: 110 | Batch_idx: 250 |  Loss_1: (0.3225) | Acc_1: (87.98%) (28265/32128)\n",
      "Epoch: 110 | Batch_idx: 260 |  Loss_1: (0.3236) | Acc_1: (87.96%) (29387/33408)\n",
      "Epoch: 110 | Batch_idx: 270 |  Loss_1: (0.3232) | Acc_1: (87.98%) (30518/34688)\n",
      "Epoch: 110 | Batch_idx: 280 |  Loss_1: (0.3229) | Acc_1: (87.99%) (31649/35968)\n",
      "Epoch: 110 | Batch_idx: 290 |  Loss_1: (0.3249) | Acc_1: (87.94%) (32756/37248)\n",
      "Epoch: 110 | Batch_idx: 300 |  Loss_1: (0.3256) | Acc_1: (87.93%) (33878/38528)\n",
      "Epoch: 110 | Batch_idx: 310 |  Loss_1: (0.3259) | Acc_1: (87.92%) (35001/39808)\n",
      "Epoch: 110 | Batch_idx: 320 |  Loss_1: (0.3267) | Acc_1: (87.88%) (36109/41088)\n",
      "Epoch: 110 | Batch_idx: 330 |  Loss_1: (0.3269) | Acc_1: (87.87%) (37229/42368)\n",
      "Epoch: 110 | Batch_idx: 340 |  Loss_1: (0.3271) | Acc_1: (87.88%) (38356/43648)\n",
      "Epoch: 110 | Batch_idx: 350 |  Loss_1: (0.3267) | Acc_1: (87.89%) (39487/44928)\n",
      "Epoch: 110 | Batch_idx: 360 |  Loss_1: (0.3289) | Acc_1: (87.81%) (40574/46208)\n",
      "Epoch: 110 | Batch_idx: 370 |  Loss_1: (0.3296) | Acc_1: (87.78%) (41687/47488)\n",
      "Epoch: 110 | Batch_idx: 380 |  Loss_1: (0.3304) | Acc_1: (87.74%) (42790/48768)\n",
      "Epoch: 110 | Batch_idx: 390 |  Loss_1: (0.3302) | Acc_1: (87.73%) (43867/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2879) | Acc: (91.08%) (9108/10000)\n",
      "Epoch: 111 | Batch_idx: 0 |  Loss_1: (0.2802) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 111 | Batch_idx: 10 |  Loss_1: (0.3001) | Acc_1: (89.56%) (1261/1408)\n",
      "Epoch: 111 | Batch_idx: 20 |  Loss_1: (0.3214) | Acc_1: (88.43%) (2377/2688)\n",
      "Epoch: 111 | Batch_idx: 30 |  Loss_1: (0.3183) | Acc_1: (88.21%) (3500/3968)\n",
      "Epoch: 111 | Batch_idx: 40 |  Loss_1: (0.3147) | Acc_1: (88.62%) (4651/5248)\n",
      "Epoch: 111 | Batch_idx: 50 |  Loss_1: (0.3221) | Acc_1: (88.39%) (5770/6528)\n",
      "Epoch: 111 | Batch_idx: 60 |  Loss_1: (0.3233) | Acc_1: (88.32%) (6896/7808)\n",
      "Epoch: 111 | Batch_idx: 70 |  Loss_1: (0.3250) | Acc_1: (88.19%) (8015/9088)\n",
      "Epoch: 111 | Batch_idx: 80 |  Loss_1: (0.3263) | Acc_1: (88.17%) (9141/10368)\n",
      "Epoch: 111 | Batch_idx: 90 |  Loss_1: (0.3266) | Acc_1: (88.02%) (10253/11648)\n",
      "Epoch: 111 | Batch_idx: 100 |  Loss_1: (0.3256) | Acc_1: (88.07%) (11386/12928)\n",
      "Epoch: 111 | Batch_idx: 110 |  Loss_1: (0.3218) | Acc_1: (88.20%) (12532/14208)\n",
      "Epoch: 111 | Batch_idx: 120 |  Loss_1: (0.3200) | Acc_1: (88.26%) (13670/15488)\n",
      "Epoch: 111 | Batch_idx: 130 |  Loss_1: (0.3210) | Acc_1: (88.17%) (14784/16768)\n",
      "Epoch: 111 | Batch_idx: 140 |  Loss_1: (0.3225) | Acc_1: (88.14%) (15908/18048)\n",
      "Epoch: 111 | Batch_idx: 150 |  Loss_1: (0.3204) | Acc_1: (88.18%) (17043/19328)\n",
      "Epoch: 111 | Batch_idx: 160 |  Loss_1: (0.3209) | Acc_1: (88.16%) (18167/20608)\n",
      "Epoch: 111 | Batch_idx: 170 |  Loss_1: (0.3216) | Acc_1: (88.15%) (19294/21888)\n",
      "Epoch: 111 | Batch_idx: 180 |  Loss_1: (0.3236) | Acc_1: (88.07%) (20404/23168)\n",
      "Epoch: 111 | Batch_idx: 190 |  Loss_1: (0.3252) | Acc_1: (88.00%) (21514/24448)\n",
      "Epoch: 111 | Batch_idx: 200 |  Loss_1: (0.3257) | Acc_1: (87.96%) (22631/25728)\n",
      "Epoch: 111 | Batch_idx: 210 |  Loss_1: (0.3265) | Acc_1: (87.93%) (23748/27008)\n",
      "Epoch: 111 | Batch_idx: 220 |  Loss_1: (0.3286) | Acc_1: (87.84%) (24848/28288)\n",
      "Epoch: 111 | Batch_idx: 230 |  Loss_1: (0.3289) | Acc_1: (87.82%) (25966/29568)\n",
      "Epoch: 111 | Batch_idx: 240 |  Loss_1: (0.3290) | Acc_1: (87.79%) (27082/30848)\n",
      "Epoch: 111 | Batch_idx: 250 |  Loss_1: (0.3282) | Acc_1: (87.80%) (28209/32128)\n",
      "Epoch: 111 | Batch_idx: 260 |  Loss_1: (0.3279) | Acc_1: (87.83%) (29343/33408)\n",
      "Epoch: 111 | Batch_idx: 270 |  Loss_1: (0.3284) | Acc_1: (87.83%) (30468/34688)\n",
      "Epoch: 111 | Batch_idx: 280 |  Loss_1: (0.3290) | Acc_1: (87.81%) (31583/35968)\n",
      "Epoch: 111 | Batch_idx: 290 |  Loss_1: (0.3287) | Acc_1: (87.81%) (32708/37248)\n",
      "Epoch: 111 | Batch_idx: 300 |  Loss_1: (0.3291) | Acc_1: (87.80%) (33827/38528)\n",
      "Epoch: 111 | Batch_idx: 310 |  Loss_1: (0.3290) | Acc_1: (87.80%) (34953/39808)\n",
      "Epoch: 111 | Batch_idx: 320 |  Loss_1: (0.3290) | Acc_1: (87.80%) (36075/41088)\n",
      "Epoch: 111 | Batch_idx: 330 |  Loss_1: (0.3303) | Acc_1: (87.75%) (37176/42368)\n",
      "Epoch: 111 | Batch_idx: 340 |  Loss_1: (0.3294) | Acc_1: (87.77%) (38308/43648)\n",
      "Epoch: 111 | Batch_idx: 350 |  Loss_1: (0.3286) | Acc_1: (87.79%) (39442/44928)\n",
      "Epoch: 111 | Batch_idx: 360 |  Loss_1: (0.3286) | Acc_1: (87.79%) (40568/46208)\n",
      "Epoch: 111 | Batch_idx: 370 |  Loss_1: (0.3279) | Acc_1: (87.82%) (41703/47488)\n",
      "Epoch: 111 | Batch_idx: 380 |  Loss_1: (0.3277) | Acc_1: (87.84%) (42840/48768)\n",
      "Epoch: 111 | Batch_idx: 390 |  Loss_1: (0.3279) | Acc_1: (87.83%) (43914/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2676) | Acc: (91.90%) (9190/10000)\n",
      "Epoch: 112 | Batch_idx: 0 |  Loss_1: (0.3230) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 112 | Batch_idx: 10 |  Loss_1: (0.3070) | Acc_1: (88.35%) (1244/1408)\n",
      "Epoch: 112 | Batch_idx: 20 |  Loss_1: (0.3165) | Acc_1: (88.06%) (2367/2688)\n",
      "Epoch: 112 | Batch_idx: 30 |  Loss_1: (0.3086) | Acc_1: (88.33%) (3505/3968)\n",
      "Epoch: 112 | Batch_idx: 40 |  Loss_1: (0.3124) | Acc_1: (88.30%) (4634/5248)\n",
      "Epoch: 112 | Batch_idx: 50 |  Loss_1: (0.3109) | Acc_1: (88.36%) (5768/6528)\n",
      "Epoch: 112 | Batch_idx: 60 |  Loss_1: (0.3160) | Acc_1: (88.20%) (6887/7808)\n",
      "Epoch: 112 | Batch_idx: 70 |  Loss_1: (0.3120) | Acc_1: (88.38%) (8032/9088)\n",
      "Epoch: 112 | Batch_idx: 80 |  Loss_1: (0.3193) | Acc_1: (88.12%) (9136/10368)\n",
      "Epoch: 112 | Batch_idx: 90 |  Loss_1: (0.3156) | Acc_1: (88.24%) (10278/11648)\n",
      "Epoch: 112 | Batch_idx: 100 |  Loss_1: (0.3147) | Acc_1: (88.30%) (11415/12928)\n",
      "Epoch: 112 | Batch_idx: 110 |  Loss_1: (0.3144) | Acc_1: (88.29%) (12544/14208)\n",
      "Epoch: 112 | Batch_idx: 120 |  Loss_1: (0.3152) | Acc_1: (88.22%) (13664/15488)\n",
      "Epoch: 112 | Batch_idx: 130 |  Loss_1: (0.3134) | Acc_1: (88.31%) (14807/16768)\n",
      "Epoch: 112 | Batch_idx: 140 |  Loss_1: (0.3145) | Acc_1: (88.24%) (15925/18048)\n",
      "Epoch: 112 | Batch_idx: 150 |  Loss_1: (0.3152) | Acc_1: (88.17%) (17042/19328)\n",
      "Epoch: 112 | Batch_idx: 160 |  Loss_1: (0.3184) | Acc_1: (88.08%) (18151/20608)\n",
      "Epoch: 112 | Batch_idx: 170 |  Loss_1: (0.3183) | Acc_1: (88.09%) (19282/21888)\n",
      "Epoch: 112 | Batch_idx: 180 |  Loss_1: (0.3200) | Acc_1: (88.02%) (20393/23168)\n",
      "Epoch: 112 | Batch_idx: 190 |  Loss_1: (0.3193) | Acc_1: (88.02%) (21518/24448)\n",
      "Epoch: 112 | Batch_idx: 200 |  Loss_1: (0.3205) | Acc_1: (87.97%) (22632/25728)\n",
      "Epoch: 112 | Batch_idx: 210 |  Loss_1: (0.3209) | Acc_1: (87.92%) (23746/27008)\n",
      "Epoch: 112 | Batch_idx: 220 |  Loss_1: (0.3205) | Acc_1: (87.95%) (24878/28288)\n",
      "Epoch: 112 | Batch_idx: 230 |  Loss_1: (0.3199) | Acc_1: (87.96%) (26008/29568)\n",
      "Epoch: 112 | Batch_idx: 240 |  Loss_1: (0.3203) | Acc_1: (87.95%) (27131/30848)\n",
      "Epoch: 112 | Batch_idx: 250 |  Loss_1: (0.3211) | Acc_1: (87.93%) (28249/32128)\n",
      "Epoch: 112 | Batch_idx: 260 |  Loss_1: (0.3204) | Acc_1: (87.94%) (29380/33408)\n",
      "Epoch: 112 | Batch_idx: 270 |  Loss_1: (0.3206) | Acc_1: (87.93%) (30502/34688)\n",
      "Epoch: 112 | Batch_idx: 280 |  Loss_1: (0.3221) | Acc_1: (87.88%) (31610/35968)\n",
      "Epoch: 112 | Batch_idx: 290 |  Loss_1: (0.3212) | Acc_1: (87.94%) (32756/37248)\n",
      "Epoch: 112 | Batch_idx: 300 |  Loss_1: (0.3212) | Acc_1: (87.93%) (33877/38528)\n",
      "Epoch: 112 | Batch_idx: 310 |  Loss_1: (0.3211) | Acc_1: (87.95%) (35011/39808)\n",
      "Epoch: 112 | Batch_idx: 320 |  Loss_1: (0.3202) | Acc_1: (87.97%) (36145/41088)\n",
      "Epoch: 112 | Batch_idx: 330 |  Loss_1: (0.3207) | Acc_1: (87.95%) (37264/42368)\n",
      "Epoch: 112 | Batch_idx: 340 |  Loss_1: (0.3198) | Acc_1: (87.96%) (38394/43648)\n",
      "Epoch: 112 | Batch_idx: 350 |  Loss_1: (0.3188) | Acc_1: (88.00%) (39537/44928)\n",
      "Epoch: 112 | Batch_idx: 360 |  Loss_1: (0.3204) | Acc_1: (87.93%) (40630/46208)\n",
      "Epoch: 112 | Batch_idx: 370 |  Loss_1: (0.3199) | Acc_1: (87.93%) (41756/47488)\n",
      "Epoch: 112 | Batch_idx: 380 |  Loss_1: (0.3196) | Acc_1: (87.94%) (42889/48768)\n",
      "Epoch: 112 | Batch_idx: 390 |  Loss_1: (0.3189) | Acc_1: (87.98%) (43990/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2617) | Acc: (91.75%) (9175/10000)\n",
      "Epoch: 113 | Batch_idx: 0 |  Loss_1: (0.2936) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 113 | Batch_idx: 10 |  Loss_1: (0.3352) | Acc_1: (86.93%) (1224/1408)\n",
      "Epoch: 113 | Batch_idx: 20 |  Loss_1: (0.3081) | Acc_1: (87.98%) (2365/2688)\n",
      "Epoch: 113 | Batch_idx: 30 |  Loss_1: (0.2993) | Acc_1: (88.46%) (3510/3968)\n",
      "Epoch: 113 | Batch_idx: 40 |  Loss_1: (0.3069) | Acc_1: (88.26%) (4632/5248)\n",
      "Epoch: 113 | Batch_idx: 50 |  Loss_1: (0.3104) | Acc_1: (88.19%) (5757/6528)\n",
      "Epoch: 113 | Batch_idx: 60 |  Loss_1: (0.3132) | Acc_1: (88.06%) (6876/7808)\n",
      "Epoch: 113 | Batch_idx: 70 |  Loss_1: (0.3132) | Acc_1: (88.09%) (8006/9088)\n",
      "Epoch: 113 | Batch_idx: 80 |  Loss_1: (0.3130) | Acc_1: (88.10%) (9134/10368)\n",
      "Epoch: 113 | Batch_idx: 90 |  Loss_1: (0.3164) | Acc_1: (87.88%) (10236/11648)\n",
      "Epoch: 113 | Batch_idx: 100 |  Loss_1: (0.3216) | Acc_1: (87.76%) (11345/12928)\n",
      "Epoch: 113 | Batch_idx: 110 |  Loss_1: (0.3183) | Acc_1: (87.89%) (12488/14208)\n",
      "Epoch: 113 | Batch_idx: 120 |  Loss_1: (0.3202) | Acc_1: (87.82%) (13602/15488)\n",
      "Epoch: 113 | Batch_idx: 130 |  Loss_1: (0.3185) | Acc_1: (87.92%) (14742/16768)\n",
      "Epoch: 113 | Batch_idx: 140 |  Loss_1: (0.3159) | Acc_1: (88.05%) (15892/18048)\n",
      "Epoch: 113 | Batch_idx: 150 |  Loss_1: (0.3162) | Acc_1: (88.03%) (17014/19328)\n",
      "Epoch: 113 | Batch_idx: 160 |  Loss_1: (0.3164) | Acc_1: (88.00%) (18135/20608)\n",
      "Epoch: 113 | Batch_idx: 170 |  Loss_1: (0.3154) | Acc_1: (88.03%) (19267/21888)\n",
      "Epoch: 113 | Batch_idx: 180 |  Loss_1: (0.3151) | Acc_1: (88.04%) (20397/23168)\n",
      "Epoch: 113 | Batch_idx: 190 |  Loss_1: (0.3146) | Acc_1: (88.08%) (21535/24448)\n",
      "Epoch: 113 | Batch_idx: 200 |  Loss_1: (0.3135) | Acc_1: (88.16%) (22682/25728)\n",
      "Epoch: 113 | Batch_idx: 210 |  Loss_1: (0.3136) | Acc_1: (88.16%) (23811/27008)\n",
      "Epoch: 113 | Batch_idx: 220 |  Loss_1: (0.3135) | Acc_1: (88.15%) (24935/28288)\n",
      "Epoch: 113 | Batch_idx: 230 |  Loss_1: (0.3154) | Acc_1: (88.06%) (26039/29568)\n",
      "Epoch: 113 | Batch_idx: 240 |  Loss_1: (0.3152) | Acc_1: (88.07%) (27167/30848)\n",
      "Epoch: 113 | Batch_idx: 250 |  Loss_1: (0.3175) | Acc_1: (87.99%) (28270/32128)\n",
      "Epoch: 113 | Batch_idx: 260 |  Loss_1: (0.3165) | Acc_1: (88.04%) (29414/33408)\n",
      "Epoch: 113 | Batch_idx: 270 |  Loss_1: (0.3160) | Acc_1: (88.07%) (30549/34688)\n",
      "Epoch: 113 | Batch_idx: 280 |  Loss_1: (0.3160) | Acc_1: (88.06%) (31675/35968)\n",
      "Epoch: 113 | Batch_idx: 290 |  Loss_1: (0.3165) | Acc_1: (88.06%) (32800/37248)\n",
      "Epoch: 113 | Batch_idx: 300 |  Loss_1: (0.3161) | Acc_1: (88.08%) (33936/38528)\n",
      "Epoch: 113 | Batch_idx: 310 |  Loss_1: (0.3163) | Acc_1: (88.09%) (35065/39808)\n",
      "Epoch: 113 | Batch_idx: 320 |  Loss_1: (0.3170) | Acc_1: (88.06%) (36183/41088)\n",
      "Epoch: 113 | Batch_idx: 330 |  Loss_1: (0.3165) | Acc_1: (88.10%) (37325/42368)\n",
      "Epoch: 113 | Batch_idx: 340 |  Loss_1: (0.3174) | Acc_1: (88.09%) (38449/43648)\n",
      "Epoch: 113 | Batch_idx: 350 |  Loss_1: (0.3174) | Acc_1: (88.09%) (39577/44928)\n",
      "Epoch: 113 | Batch_idx: 360 |  Loss_1: (0.3175) | Acc_1: (88.10%) (40708/46208)\n",
      "Epoch: 113 | Batch_idx: 370 |  Loss_1: (0.3179) | Acc_1: (88.08%) (41827/47488)\n",
      "Epoch: 113 | Batch_idx: 380 |  Loss_1: (0.3192) | Acc_1: (88.04%) (42935/48768)\n",
      "Epoch: 113 | Batch_idx: 390 |  Loss_1: (0.3201) | Acc_1: (88.01%) (44004/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2567) | Acc: (92.26%) (9226/10000)\n",
      "Epoch: 114 | Batch_idx: 0 |  Loss_1: (0.4355) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 114 | Batch_idx: 10 |  Loss_1: (0.3370) | Acc_1: (87.07%) (1226/1408)\n",
      "Epoch: 114 | Batch_idx: 20 |  Loss_1: (0.3122) | Acc_1: (88.06%) (2367/2688)\n",
      "Epoch: 114 | Batch_idx: 30 |  Loss_1: (0.3126) | Acc_1: (88.23%) (3501/3968)\n",
      "Epoch: 114 | Batch_idx: 40 |  Loss_1: (0.3131) | Acc_1: (88.28%) (4633/5248)\n",
      "Epoch: 114 | Batch_idx: 50 |  Loss_1: (0.3082) | Acc_1: (88.63%) (5786/6528)\n",
      "Epoch: 114 | Batch_idx: 60 |  Loss_1: (0.3063) | Acc_1: (88.76%) (6930/7808)\n",
      "Epoch: 114 | Batch_idx: 70 |  Loss_1: (0.3039) | Acc_1: (88.79%) (8069/9088)\n",
      "Epoch: 114 | Batch_idx: 80 |  Loss_1: (0.3040) | Acc_1: (88.74%) (9201/10368)\n",
      "Epoch: 114 | Batch_idx: 90 |  Loss_1: (0.3060) | Acc_1: (88.68%) (10330/11648)\n",
      "Epoch: 114 | Batch_idx: 100 |  Loss_1: (0.3127) | Acc_1: (88.41%) (11429/12928)\n",
      "Epoch: 114 | Batch_idx: 110 |  Loss_1: (0.3119) | Acc_1: (88.37%) (12556/14208)\n",
      "Epoch: 114 | Batch_idx: 120 |  Loss_1: (0.3134) | Acc_1: (88.31%) (13678/15488)\n",
      "Epoch: 114 | Batch_idx: 130 |  Loss_1: (0.3154) | Acc_1: (88.25%) (14797/16768)\n",
      "Epoch: 114 | Batch_idx: 140 |  Loss_1: (0.3156) | Acc_1: (88.24%) (15925/18048)\n",
      "Epoch: 114 | Batch_idx: 150 |  Loss_1: (0.3159) | Acc_1: (88.25%) (17056/19328)\n",
      "Epoch: 114 | Batch_idx: 160 |  Loss_1: (0.3174) | Acc_1: (88.20%) (18176/20608)\n",
      "Epoch: 114 | Batch_idx: 170 |  Loss_1: (0.3170) | Acc_1: (88.20%) (19305/21888)\n",
      "Epoch: 114 | Batch_idx: 180 |  Loss_1: (0.3189) | Acc_1: (88.11%) (20413/23168)\n",
      "Epoch: 114 | Batch_idx: 190 |  Loss_1: (0.3212) | Acc_1: (87.99%) (21513/24448)\n",
      "Epoch: 114 | Batch_idx: 200 |  Loss_1: (0.3207) | Acc_1: (88.03%) (22648/25728)\n",
      "Epoch: 114 | Batch_idx: 210 |  Loss_1: (0.3218) | Acc_1: (87.97%) (23760/27008)\n",
      "Epoch: 114 | Batch_idx: 220 |  Loss_1: (0.3218) | Acc_1: (87.97%) (24886/28288)\n",
      "Epoch: 114 | Batch_idx: 230 |  Loss_1: (0.3212) | Acc_1: (87.97%) (26012/29568)\n",
      "Epoch: 114 | Batch_idx: 240 |  Loss_1: (0.3220) | Acc_1: (87.95%) (27132/30848)\n",
      "Epoch: 114 | Batch_idx: 250 |  Loss_1: (0.3206) | Acc_1: (88.00%) (28272/32128)\n",
      "Epoch: 114 | Batch_idx: 260 |  Loss_1: (0.3218) | Acc_1: (87.98%) (29392/33408)\n",
      "Epoch: 114 | Batch_idx: 270 |  Loss_1: (0.3227) | Acc_1: (87.98%) (30518/34688)\n",
      "Epoch: 114 | Batch_idx: 280 |  Loss_1: (0.3223) | Acc_1: (87.98%) (31646/35968)\n",
      "Epoch: 114 | Batch_idx: 290 |  Loss_1: (0.3233) | Acc_1: (87.95%) (32761/37248)\n",
      "Epoch: 114 | Batch_idx: 300 |  Loss_1: (0.3236) | Acc_1: (87.95%) (33885/38528)\n",
      "Epoch: 114 | Batch_idx: 310 |  Loss_1: (0.3240) | Acc_1: (87.92%) (34998/39808)\n",
      "Epoch: 114 | Batch_idx: 320 |  Loss_1: (0.3246) | Acc_1: (87.91%) (36122/41088)\n",
      "Epoch: 114 | Batch_idx: 330 |  Loss_1: (0.3253) | Acc_1: (87.89%) (37238/42368)\n",
      "Epoch: 114 | Batch_idx: 340 |  Loss_1: (0.3246) | Acc_1: (87.91%) (38373/43648)\n",
      "Epoch: 114 | Batch_idx: 350 |  Loss_1: (0.3241) | Acc_1: (87.91%) (39497/44928)\n",
      "Epoch: 114 | Batch_idx: 360 |  Loss_1: (0.3240) | Acc_1: (87.90%) (40619/46208)\n",
      "Epoch: 114 | Batch_idx: 370 |  Loss_1: (0.3236) | Acc_1: (87.93%) (41758/47488)\n",
      "Epoch: 114 | Batch_idx: 380 |  Loss_1: (0.3237) | Acc_1: (87.92%) (42875/48768)\n",
      "Epoch: 114 | Batch_idx: 390 |  Loss_1: (0.3224) | Acc_1: (87.96%) (43978/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2566) | Acc: (92.10%) (9210/10000)\n",
      "Epoch: 115 | Batch_idx: 0 |  Loss_1: (0.2472) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 115 | Batch_idx: 10 |  Loss_1: (0.3109) | Acc_1: (89.20%) (1256/1408)\n",
      "Epoch: 115 | Batch_idx: 20 |  Loss_1: (0.3181) | Acc_1: (88.47%) (2378/2688)\n",
      "Epoch: 115 | Batch_idx: 30 |  Loss_1: (0.3182) | Acc_1: (88.28%) (3503/3968)\n",
      "Epoch: 115 | Batch_idx: 40 |  Loss_1: (0.3235) | Acc_1: (87.98%) (4617/5248)\n",
      "Epoch: 115 | Batch_idx: 50 |  Loss_1: (0.3193) | Acc_1: (88.16%) (5755/6528)\n",
      "Epoch: 115 | Batch_idx: 60 |  Loss_1: (0.3195) | Acc_1: (88.13%) (6881/7808)\n",
      "Epoch: 115 | Batch_idx: 70 |  Loss_1: (0.3180) | Acc_1: (88.12%) (8008/9088)\n",
      "Epoch: 115 | Batch_idx: 80 |  Loss_1: (0.3174) | Acc_1: (88.10%) (9134/10368)\n",
      "Epoch: 115 | Batch_idx: 90 |  Loss_1: (0.3170) | Acc_1: (88.02%) (10253/11648)\n",
      "Epoch: 115 | Batch_idx: 100 |  Loss_1: (0.3175) | Acc_1: (88.04%) (11382/12928)\n",
      "Epoch: 115 | Batch_idx: 110 |  Loss_1: (0.3188) | Acc_1: (88.01%) (12504/14208)\n",
      "Epoch: 115 | Batch_idx: 120 |  Loss_1: (0.3201) | Acc_1: (87.94%) (13620/15488)\n",
      "Epoch: 115 | Batch_idx: 130 |  Loss_1: (0.3176) | Acc_1: (88.05%) (14764/16768)\n",
      "Epoch: 115 | Batch_idx: 140 |  Loss_1: (0.3157) | Acc_1: (88.14%) (15908/18048)\n",
      "Epoch: 115 | Batch_idx: 150 |  Loss_1: (0.3151) | Acc_1: (88.18%) (17044/19328)\n",
      "Epoch: 115 | Batch_idx: 160 |  Loss_1: (0.3161) | Acc_1: (88.14%) (18163/20608)\n",
      "Epoch: 115 | Batch_idx: 170 |  Loss_1: (0.3178) | Acc_1: (88.06%) (19274/21888)\n",
      "Epoch: 115 | Batch_idx: 180 |  Loss_1: (0.3171) | Acc_1: (88.09%) (20409/23168)\n",
      "Epoch: 115 | Batch_idx: 190 |  Loss_1: (0.3159) | Acc_1: (88.14%) (21548/24448)\n",
      "Epoch: 115 | Batch_idx: 200 |  Loss_1: (0.3166) | Acc_1: (88.11%) (22669/25728)\n",
      "Epoch: 115 | Batch_idx: 210 |  Loss_1: (0.3181) | Acc_1: (88.05%) (23781/27008)\n",
      "Epoch: 115 | Batch_idx: 220 |  Loss_1: (0.3173) | Acc_1: (88.07%) (24914/28288)\n",
      "Epoch: 115 | Batch_idx: 230 |  Loss_1: (0.3169) | Acc_1: (88.08%) (26044/29568)\n",
      "Epoch: 115 | Batch_idx: 240 |  Loss_1: (0.3160) | Acc_1: (88.14%) (27189/30848)\n",
      "Epoch: 115 | Batch_idx: 250 |  Loss_1: (0.3154) | Acc_1: (88.15%) (28321/32128)\n",
      "Epoch: 115 | Batch_idx: 260 |  Loss_1: (0.3151) | Acc_1: (88.15%) (29449/33408)\n",
      "Epoch: 115 | Batch_idx: 270 |  Loss_1: (0.3146) | Acc_1: (88.17%) (30585/34688)\n",
      "Epoch: 115 | Batch_idx: 280 |  Loss_1: (0.3139) | Acc_1: (88.19%) (31720/35968)\n",
      "Epoch: 115 | Batch_idx: 290 |  Loss_1: (0.3139) | Acc_1: (88.18%) (32846/37248)\n",
      "Epoch: 115 | Batch_idx: 300 |  Loss_1: (0.3141) | Acc_1: (88.20%) (33980/38528)\n",
      "Epoch: 115 | Batch_idx: 310 |  Loss_1: (0.3147) | Acc_1: (88.19%) (35105/39808)\n",
      "Epoch: 115 | Batch_idx: 320 |  Loss_1: (0.3148) | Acc_1: (88.18%) (36231/41088)\n",
      "Epoch: 115 | Batch_idx: 330 |  Loss_1: (0.3152) | Acc_1: (88.18%) (37358/42368)\n",
      "Epoch: 115 | Batch_idx: 340 |  Loss_1: (0.3152) | Acc_1: (88.19%) (38495/43648)\n",
      "Epoch: 115 | Batch_idx: 350 |  Loss_1: (0.3161) | Acc_1: (88.17%) (39613/44928)\n",
      "Epoch: 115 | Batch_idx: 360 |  Loss_1: (0.3171) | Acc_1: (88.15%) (40733/46208)\n",
      "Epoch: 115 | Batch_idx: 370 |  Loss_1: (0.3173) | Acc_1: (88.13%) (41850/47488)\n",
      "Epoch: 115 | Batch_idx: 380 |  Loss_1: (0.3161) | Acc_1: (88.19%) (43009/48768)\n",
      "Epoch: 115 | Batch_idx: 390 |  Loss_1: (0.3167) | Acc_1: (88.15%) (44076/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2533) | Acc: (92.36%) (9236/10000)\n",
      "Epoch: 116 | Batch_idx: 0 |  Loss_1: (0.4656) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 116 | Batch_idx: 10 |  Loss_1: (0.3231) | Acc_1: (88.57%) (1247/1408)\n",
      "Epoch: 116 | Batch_idx: 20 |  Loss_1: (0.3247) | Acc_1: (88.36%) (2375/2688)\n",
      "Epoch: 116 | Batch_idx: 30 |  Loss_1: (0.3138) | Acc_1: (88.53%) (3513/3968)\n",
      "Epoch: 116 | Batch_idx: 40 |  Loss_1: (0.3104) | Acc_1: (88.70%) (4655/5248)\n",
      "Epoch: 116 | Batch_idx: 50 |  Loss_1: (0.3128) | Acc_1: (88.71%) (5791/6528)\n",
      "Epoch: 116 | Batch_idx: 60 |  Loss_1: (0.3084) | Acc_1: (88.77%) (6931/7808)\n",
      "Epoch: 116 | Batch_idx: 70 |  Loss_1: (0.3069) | Acc_1: (88.73%) (8064/9088)\n",
      "Epoch: 116 | Batch_idx: 80 |  Loss_1: (0.3026) | Acc_1: (88.85%) (9212/10368)\n",
      "Epoch: 116 | Batch_idx: 90 |  Loss_1: (0.3028) | Acc_1: (88.87%) (10352/11648)\n",
      "Epoch: 116 | Batch_idx: 100 |  Loss_1: (0.3011) | Acc_1: (89.01%) (11507/12928)\n",
      "Epoch: 116 | Batch_idx: 110 |  Loss_1: (0.3041) | Acc_1: (88.91%) (12632/14208)\n",
      "Epoch: 116 | Batch_idx: 120 |  Loss_1: (0.3040) | Acc_1: (88.89%) (13768/15488)\n",
      "Epoch: 116 | Batch_idx: 130 |  Loss_1: (0.3076) | Acc_1: (88.76%) (14883/16768)\n",
      "Epoch: 116 | Batch_idx: 140 |  Loss_1: (0.3105) | Acc_1: (88.68%) (16005/18048)\n",
      "Epoch: 116 | Batch_idx: 150 |  Loss_1: (0.3113) | Acc_1: (88.63%) (17130/19328)\n",
      "Epoch: 116 | Batch_idx: 160 |  Loss_1: (0.3144) | Acc_1: (88.53%) (18244/20608)\n",
      "Epoch: 116 | Batch_idx: 170 |  Loss_1: (0.3144) | Acc_1: (88.50%) (19370/21888)\n",
      "Epoch: 116 | Batch_idx: 180 |  Loss_1: (0.3174) | Acc_1: (88.37%) (20474/23168)\n",
      "Epoch: 116 | Batch_idx: 190 |  Loss_1: (0.3175) | Acc_1: (88.33%) (21594/24448)\n",
      "Epoch: 116 | Batch_idx: 200 |  Loss_1: (0.3164) | Acc_1: (88.37%) (22737/25728)\n",
      "Epoch: 116 | Batch_idx: 210 |  Loss_1: (0.3187) | Acc_1: (88.27%) (23840/27008)\n",
      "Epoch: 116 | Batch_idx: 220 |  Loss_1: (0.3201) | Acc_1: (88.25%) (24964/28288)\n",
      "Epoch: 116 | Batch_idx: 230 |  Loss_1: (0.3189) | Acc_1: (88.29%) (26105/29568)\n",
      "Epoch: 116 | Batch_idx: 240 |  Loss_1: (0.3176) | Acc_1: (88.34%) (27251/30848)\n",
      "Epoch: 116 | Batch_idx: 250 |  Loss_1: (0.3192) | Acc_1: (88.30%) (28368/32128)\n",
      "Epoch: 116 | Batch_idx: 260 |  Loss_1: (0.3186) | Acc_1: (88.30%) (29499/33408)\n",
      "Epoch: 116 | Batch_idx: 270 |  Loss_1: (0.3184) | Acc_1: (88.31%) (30632/34688)\n",
      "Epoch: 116 | Batch_idx: 280 |  Loss_1: (0.3175) | Acc_1: (88.35%) (31777/35968)\n",
      "Epoch: 116 | Batch_idx: 290 |  Loss_1: (0.3166) | Acc_1: (88.37%) (32915/37248)\n",
      "Epoch: 116 | Batch_idx: 300 |  Loss_1: (0.3166) | Acc_1: (88.36%) (34044/38528)\n",
      "Epoch: 116 | Batch_idx: 310 |  Loss_1: (0.3177) | Acc_1: (88.34%) (35166/39808)\n",
      "Epoch: 116 | Batch_idx: 320 |  Loss_1: (0.3177) | Acc_1: (88.33%) (36295/41088)\n",
      "Epoch: 116 | Batch_idx: 330 |  Loss_1: (0.3163) | Acc_1: (88.39%) (37451/42368)\n",
      "Epoch: 116 | Batch_idx: 340 |  Loss_1: (0.3158) | Acc_1: (88.41%) (38588/43648)\n",
      "Epoch: 116 | Batch_idx: 350 |  Loss_1: (0.3150) | Acc_1: (88.42%) (39725/44928)\n",
      "Epoch: 116 | Batch_idx: 360 |  Loss_1: (0.3147) | Acc_1: (88.42%) (40857/46208)\n",
      "Epoch: 116 | Batch_idx: 370 |  Loss_1: (0.3154) | Acc_1: (88.38%) (41972/47488)\n",
      "Epoch: 116 | Batch_idx: 380 |  Loss_1: (0.3154) | Acc_1: (88.39%) (43105/48768)\n",
      "Epoch: 116 | Batch_idx: 390 |  Loss_1: (0.3172) | Acc_1: (88.32%) (44160/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2605) | Acc: (92.18%) (9218/10000)\n",
      "Epoch: 117 | Batch_idx: 0 |  Loss_1: (0.3967) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 117 | Batch_idx: 10 |  Loss_1: (0.3008) | Acc_1: (88.85%) (1251/1408)\n",
      "Epoch: 117 | Batch_idx: 20 |  Loss_1: (0.3199) | Acc_1: (87.95%) (2364/2688)\n",
      "Epoch: 117 | Batch_idx: 30 |  Loss_1: (0.3140) | Acc_1: (88.31%) (3504/3968)\n",
      "Epoch: 117 | Batch_idx: 40 |  Loss_1: (0.3129) | Acc_1: (88.22%) (4630/5248)\n",
      "Epoch: 117 | Batch_idx: 50 |  Loss_1: (0.3156) | Acc_1: (88.07%) (5749/6528)\n",
      "Epoch: 117 | Batch_idx: 60 |  Loss_1: (0.3107) | Acc_1: (88.18%) (6885/7808)\n",
      "Epoch: 117 | Batch_idx: 70 |  Loss_1: (0.3169) | Acc_1: (87.91%) (7989/9088)\n",
      "Epoch: 117 | Batch_idx: 80 |  Loss_1: (0.3158) | Acc_1: (87.98%) (9122/10368)\n",
      "Epoch: 117 | Batch_idx: 90 |  Loss_1: (0.3200) | Acc_1: (87.80%) (10227/11648)\n",
      "Epoch: 117 | Batch_idx: 100 |  Loss_1: (0.3217) | Acc_1: (87.86%) (11359/12928)\n",
      "Epoch: 117 | Batch_idx: 110 |  Loss_1: (0.3176) | Acc_1: (88.00%) (12503/14208)\n",
      "Epoch: 117 | Batch_idx: 120 |  Loss_1: (0.3138) | Acc_1: (88.16%) (13654/15488)\n",
      "Epoch: 117 | Batch_idx: 130 |  Loss_1: (0.3137) | Acc_1: (88.17%) (14784/16768)\n",
      "Epoch: 117 | Batch_idx: 140 |  Loss_1: (0.3136) | Acc_1: (88.20%) (15918/18048)\n",
      "Epoch: 117 | Batch_idx: 150 |  Loss_1: (0.3119) | Acc_1: (88.25%) (17056/19328)\n",
      "Epoch: 117 | Batch_idx: 160 |  Loss_1: (0.3106) | Acc_1: (88.28%) (18193/20608)\n",
      "Epoch: 117 | Batch_idx: 170 |  Loss_1: (0.3110) | Acc_1: (88.30%) (19328/21888)\n",
      "Epoch: 117 | Batch_idx: 180 |  Loss_1: (0.3101) | Acc_1: (88.35%) (20469/23168)\n",
      "Epoch: 117 | Batch_idx: 190 |  Loss_1: (0.3078) | Acc_1: (88.43%) (21619/24448)\n",
      "Epoch: 117 | Batch_idx: 200 |  Loss_1: (0.3078) | Acc_1: (88.42%) (22749/25728)\n",
      "Epoch: 117 | Batch_idx: 210 |  Loss_1: (0.3081) | Acc_1: (88.41%) (23878/27008)\n",
      "Epoch: 117 | Batch_idx: 220 |  Loss_1: (0.3096) | Acc_1: (88.33%) (24987/28288)\n",
      "Epoch: 117 | Batch_idx: 230 |  Loss_1: (0.3112) | Acc_1: (88.32%) (26113/29568)\n",
      "Epoch: 117 | Batch_idx: 240 |  Loss_1: (0.3114) | Acc_1: (88.31%) (27242/30848)\n",
      "Epoch: 117 | Batch_idx: 250 |  Loss_1: (0.3118) | Acc_1: (88.32%) (28374/32128)\n",
      "Epoch: 117 | Batch_idx: 260 |  Loss_1: (0.3106) | Acc_1: (88.35%) (29516/33408)\n",
      "Epoch: 117 | Batch_idx: 270 |  Loss_1: (0.3108) | Acc_1: (88.38%) (30656/34688)\n",
      "Epoch: 117 | Batch_idx: 280 |  Loss_1: (0.3106) | Acc_1: (88.37%) (31784/35968)\n",
      "Epoch: 117 | Batch_idx: 290 |  Loss_1: (0.3114) | Acc_1: (88.34%) (32905/37248)\n",
      "Epoch: 117 | Batch_idx: 300 |  Loss_1: (0.3103) | Acc_1: (88.37%) (34048/38528)\n",
      "Epoch: 117 | Batch_idx: 310 |  Loss_1: (0.3105) | Acc_1: (88.38%) (35184/39808)\n",
      "Epoch: 117 | Batch_idx: 320 |  Loss_1: (0.3106) | Acc_1: (88.39%) (36317/41088)\n",
      "Epoch: 117 | Batch_idx: 330 |  Loss_1: (0.3100) | Acc_1: (88.41%) (37459/42368)\n",
      "Epoch: 117 | Batch_idx: 340 |  Loss_1: (0.3101) | Acc_1: (88.40%) (38584/43648)\n",
      "Epoch: 117 | Batch_idx: 350 |  Loss_1: (0.3095) | Acc_1: (88.42%) (39727/44928)\n",
      "Epoch: 117 | Batch_idx: 360 |  Loss_1: (0.3103) | Acc_1: (88.39%) (40842/46208)\n",
      "Epoch: 117 | Batch_idx: 370 |  Loss_1: (0.3115) | Acc_1: (88.34%) (41949/47488)\n",
      "Epoch: 117 | Batch_idx: 380 |  Loss_1: (0.3112) | Acc_1: (88.34%) (43081/48768)\n",
      "Epoch: 117 | Batch_idx: 390 |  Loss_1: (0.3120) | Acc_1: (88.28%) (44142/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2718) | Acc: (92.05%) (9205/10000)\n",
      "Epoch: 118 | Batch_idx: 0 |  Loss_1: (0.3164) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 118 | Batch_idx: 10 |  Loss_1: (0.2779) | Acc_1: (89.13%) (1255/1408)\n",
      "Epoch: 118 | Batch_idx: 20 |  Loss_1: (0.2886) | Acc_1: (89.36%) (2402/2688)\n",
      "Epoch: 118 | Batch_idx: 30 |  Loss_1: (0.2979) | Acc_1: (88.66%) (3518/3968)\n",
      "Epoch: 118 | Batch_idx: 40 |  Loss_1: (0.2994) | Acc_1: (88.70%) (4655/5248)\n",
      "Epoch: 118 | Batch_idx: 50 |  Loss_1: (0.2979) | Acc_1: (88.79%) (5796/6528)\n",
      "Epoch: 118 | Batch_idx: 60 |  Loss_1: (0.2940) | Acc_1: (88.87%) (6939/7808)\n",
      "Epoch: 118 | Batch_idx: 70 |  Loss_1: (0.2992) | Acc_1: (88.61%) (8053/9088)\n",
      "Epoch: 118 | Batch_idx: 80 |  Loss_1: (0.3055) | Acc_1: (88.31%) (9156/10368)\n",
      "Epoch: 118 | Batch_idx: 90 |  Loss_1: (0.3047) | Acc_1: (88.33%) (10289/11648)\n",
      "Epoch: 118 | Batch_idx: 100 |  Loss_1: (0.3041) | Acc_1: (88.39%) (11427/12928)\n",
      "Epoch: 118 | Batch_idx: 110 |  Loss_1: (0.3050) | Acc_1: (88.31%) (12547/14208)\n",
      "Epoch: 118 | Batch_idx: 120 |  Loss_1: (0.3061) | Acc_1: (88.28%) (13673/15488)\n",
      "Epoch: 118 | Batch_idx: 130 |  Loss_1: (0.3076) | Acc_1: (88.20%) (14790/16768)\n",
      "Epoch: 118 | Batch_idx: 140 |  Loss_1: (0.3079) | Acc_1: (88.20%) (15918/18048)\n",
      "Epoch: 118 | Batch_idx: 150 |  Loss_1: (0.3093) | Acc_1: (88.14%) (17036/19328)\n",
      "Epoch: 118 | Batch_idx: 160 |  Loss_1: (0.3079) | Acc_1: (88.17%) (18171/20608)\n",
      "Epoch: 118 | Batch_idx: 170 |  Loss_1: (0.3088) | Acc_1: (88.17%) (19298/21888)\n",
      "Epoch: 118 | Batch_idx: 180 |  Loss_1: (0.3089) | Acc_1: (88.18%) (20429/23168)\n",
      "Epoch: 118 | Batch_idx: 190 |  Loss_1: (0.3099) | Acc_1: (88.10%) (21539/24448)\n",
      "Epoch: 118 | Batch_idx: 200 |  Loss_1: (0.3102) | Acc_1: (88.12%) (22671/25728)\n",
      "Epoch: 118 | Batch_idx: 210 |  Loss_1: (0.3103) | Acc_1: (88.15%) (23808/27008)\n",
      "Epoch: 118 | Batch_idx: 220 |  Loss_1: (0.3096) | Acc_1: (88.21%) (24952/28288)\n",
      "Epoch: 118 | Batch_idx: 230 |  Loss_1: (0.3090) | Acc_1: (88.22%) (26084/29568)\n",
      "Epoch: 118 | Batch_idx: 240 |  Loss_1: (0.3105) | Acc_1: (88.17%) (27198/30848)\n",
      "Epoch: 118 | Batch_idx: 250 |  Loss_1: (0.3103) | Acc_1: (88.17%) (28326/32128)\n",
      "Epoch: 118 | Batch_idx: 260 |  Loss_1: (0.3106) | Acc_1: (88.15%) (29450/33408)\n",
      "Epoch: 118 | Batch_idx: 270 |  Loss_1: (0.3104) | Acc_1: (88.17%) (30584/34688)\n",
      "Epoch: 118 | Batch_idx: 280 |  Loss_1: (0.3096) | Acc_1: (88.21%) (31727/35968)\n",
      "Epoch: 118 | Batch_idx: 290 |  Loss_1: (0.3105) | Acc_1: (88.17%) (32842/37248)\n",
      "Epoch: 118 | Batch_idx: 300 |  Loss_1: (0.3100) | Acc_1: (88.21%) (33984/38528)\n",
      "Epoch: 118 | Batch_idx: 310 |  Loss_1: (0.3101) | Acc_1: (88.20%) (35112/39808)\n",
      "Epoch: 118 | Batch_idx: 320 |  Loss_1: (0.3103) | Acc_1: (88.20%) (36239/41088)\n",
      "Epoch: 118 | Batch_idx: 330 |  Loss_1: (0.3103) | Acc_1: (88.22%) (37379/42368)\n",
      "Epoch: 118 | Batch_idx: 340 |  Loss_1: (0.3099) | Acc_1: (88.25%) (38519/43648)\n",
      "Epoch: 118 | Batch_idx: 350 |  Loss_1: (0.3100) | Acc_1: (88.25%) (39651/44928)\n",
      "Epoch: 118 | Batch_idx: 360 |  Loss_1: (0.3103) | Acc_1: (88.25%) (40779/46208)\n",
      "Epoch: 118 | Batch_idx: 370 |  Loss_1: (0.3113) | Acc_1: (88.21%) (41888/47488)\n",
      "Epoch: 118 | Batch_idx: 380 |  Loss_1: (0.3120) | Acc_1: (88.20%) (43012/48768)\n",
      "Epoch: 118 | Batch_idx: 390 |  Loss_1: (0.3115) | Acc_1: (88.23%) (44116/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2632) | Acc: (91.79%) (9179/10000)\n",
      "Epoch: 119 | Batch_idx: 0 |  Loss_1: (0.2772) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 119 | Batch_idx: 10 |  Loss_1: (0.2656) | Acc_1: (89.77%) (1264/1408)\n",
      "Epoch: 119 | Batch_idx: 20 |  Loss_1: (0.2748) | Acc_1: (89.25%) (2399/2688)\n",
      "Epoch: 119 | Batch_idx: 30 |  Loss_1: (0.2793) | Acc_1: (89.01%) (3532/3968)\n",
      "Epoch: 119 | Batch_idx: 40 |  Loss_1: (0.2804) | Acc_1: (88.93%) (4667/5248)\n",
      "Epoch: 119 | Batch_idx: 50 |  Loss_1: (0.2850) | Acc_1: (88.80%) (5797/6528)\n",
      "Epoch: 119 | Batch_idx: 60 |  Loss_1: (0.2897) | Acc_1: (88.65%) (6922/7808)\n",
      "Epoch: 119 | Batch_idx: 70 |  Loss_1: (0.2940) | Acc_1: (88.47%) (8040/9088)\n",
      "Epoch: 119 | Batch_idx: 80 |  Loss_1: (0.2962) | Acc_1: (88.37%) (9162/10368)\n",
      "Epoch: 119 | Batch_idx: 90 |  Loss_1: (0.2955) | Acc_1: (88.47%) (10305/11648)\n",
      "Epoch: 119 | Batch_idx: 100 |  Loss_1: (0.2962) | Acc_1: (88.53%) (11445/12928)\n",
      "Epoch: 119 | Batch_idx: 110 |  Loss_1: (0.2941) | Acc_1: (88.61%) (12590/14208)\n",
      "Epoch: 119 | Batch_idx: 120 |  Loss_1: (0.2990) | Acc_1: (88.46%) (13700/15488)\n",
      "Epoch: 119 | Batch_idx: 130 |  Loss_1: (0.3016) | Acc_1: (88.41%) (14824/16768)\n",
      "Epoch: 119 | Batch_idx: 140 |  Loss_1: (0.3033) | Acc_1: (88.36%) (15948/18048)\n",
      "Epoch: 119 | Batch_idx: 150 |  Loss_1: (0.3043) | Acc_1: (88.35%) (17077/19328)\n",
      "Epoch: 119 | Batch_idx: 160 |  Loss_1: (0.3076) | Acc_1: (88.30%) (18196/20608)\n",
      "Epoch: 119 | Batch_idx: 170 |  Loss_1: (0.3075) | Acc_1: (88.32%) (19332/21888)\n",
      "Epoch: 119 | Batch_idx: 180 |  Loss_1: (0.3083) | Acc_1: (88.28%) (20453/23168)\n",
      "Epoch: 119 | Batch_idx: 190 |  Loss_1: (0.3105) | Acc_1: (88.24%) (21573/24448)\n",
      "Epoch: 119 | Batch_idx: 200 |  Loss_1: (0.3107) | Acc_1: (88.21%) (22695/25728)\n",
      "Epoch: 119 | Batch_idx: 210 |  Loss_1: (0.3085) | Acc_1: (88.29%) (23846/27008)\n",
      "Epoch: 119 | Batch_idx: 220 |  Loss_1: (0.3090) | Acc_1: (88.28%) (24973/28288)\n",
      "Epoch: 119 | Batch_idx: 230 |  Loss_1: (0.3092) | Acc_1: (88.27%) (26101/29568)\n",
      "Epoch: 119 | Batch_idx: 240 |  Loss_1: (0.3093) | Acc_1: (88.31%) (27242/30848)\n",
      "Epoch: 119 | Batch_idx: 250 |  Loss_1: (0.3078) | Acc_1: (88.34%) (28383/32128)\n",
      "Epoch: 119 | Batch_idx: 260 |  Loss_1: (0.3079) | Acc_1: (88.34%) (29513/33408)\n",
      "Epoch: 119 | Batch_idx: 270 |  Loss_1: (0.3061) | Acc_1: (88.40%) (30664/34688)\n",
      "Epoch: 119 | Batch_idx: 280 |  Loss_1: (0.3066) | Acc_1: (88.38%) (31788/35968)\n",
      "Epoch: 119 | Batch_idx: 290 |  Loss_1: (0.3058) | Acc_1: (88.41%) (32932/37248)\n",
      "Epoch: 119 | Batch_idx: 300 |  Loss_1: (0.3052) | Acc_1: (88.44%) (34074/38528)\n",
      "Epoch: 119 | Batch_idx: 310 |  Loss_1: (0.3048) | Acc_1: (88.46%) (35216/39808)\n",
      "Epoch: 119 | Batch_idx: 320 |  Loss_1: (0.3040) | Acc_1: (88.49%) (36360/41088)\n",
      "Epoch: 119 | Batch_idx: 330 |  Loss_1: (0.3060) | Acc_1: (88.44%) (37470/42368)\n",
      "Epoch: 119 | Batch_idx: 340 |  Loss_1: (0.3061) | Acc_1: (88.42%) (38594/43648)\n",
      "Epoch: 119 | Batch_idx: 350 |  Loss_1: (0.3057) | Acc_1: (88.45%) (39741/44928)\n",
      "Epoch: 119 | Batch_idx: 360 |  Loss_1: (0.3068) | Acc_1: (88.42%) (40855/46208)\n",
      "Epoch: 119 | Batch_idx: 370 |  Loss_1: (0.3065) | Acc_1: (88.44%) (41997/47488)\n",
      "Epoch: 119 | Batch_idx: 380 |  Loss_1: (0.3069) | Acc_1: (88.45%) (43136/48768)\n",
      "Epoch: 119 | Batch_idx: 390 |  Loss_1: (0.3066) | Acc_1: (88.46%) (44229/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2881) | Acc: (91.83%) (9183/10000)\n",
      "Epoch: 120 | Batch_idx: 0 |  Loss_1: (0.4216) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 120 | Batch_idx: 10 |  Loss_1: (0.2973) | Acc_1: (88.49%) (1246/1408)\n",
      "Epoch: 120 | Batch_idx: 20 |  Loss_1: (0.2931) | Acc_1: (88.69%) (2384/2688)\n",
      "Epoch: 120 | Batch_idx: 30 |  Loss_1: (0.2886) | Acc_1: (88.89%) (3527/3968)\n",
      "Epoch: 120 | Batch_idx: 40 |  Loss_1: (0.2829) | Acc_1: (89.14%) (4678/5248)\n",
      "Epoch: 120 | Batch_idx: 50 |  Loss_1: (0.2834) | Acc_1: (89.14%) (5819/6528)\n",
      "Epoch: 120 | Batch_idx: 60 |  Loss_1: (0.2830) | Acc_1: (89.22%) (6966/7808)\n",
      "Epoch: 120 | Batch_idx: 70 |  Loss_1: (0.2832) | Acc_1: (89.25%) (8111/9088)\n",
      "Epoch: 120 | Batch_idx: 80 |  Loss_1: (0.2850) | Acc_1: (89.27%) (9256/10368)\n",
      "Epoch: 120 | Batch_idx: 90 |  Loss_1: (0.2861) | Acc_1: (89.19%) (10389/11648)\n",
      "Epoch: 120 | Batch_idx: 100 |  Loss_1: (0.2927) | Acc_1: (88.94%) (11498/12928)\n",
      "Epoch: 120 | Batch_idx: 110 |  Loss_1: (0.2950) | Acc_1: (88.84%) (12622/14208)\n",
      "Epoch: 120 | Batch_idx: 120 |  Loss_1: (0.2946) | Acc_1: (88.91%) (13770/15488)\n",
      "Epoch: 120 | Batch_idx: 130 |  Loss_1: (0.2929) | Acc_1: (88.94%) (14913/16768)\n",
      "Epoch: 120 | Batch_idx: 140 |  Loss_1: (0.2927) | Acc_1: (88.90%) (16044/18048)\n",
      "Epoch: 120 | Batch_idx: 150 |  Loss_1: (0.2919) | Acc_1: (88.93%) (17188/19328)\n",
      "Epoch: 120 | Batch_idx: 160 |  Loss_1: (0.2910) | Acc_1: (88.98%) (18336/20608)\n",
      "Epoch: 120 | Batch_idx: 170 |  Loss_1: (0.2898) | Acc_1: (89.03%) (19487/21888)\n",
      "Epoch: 120 | Batch_idx: 180 |  Loss_1: (0.2876) | Acc_1: (89.13%) (20650/23168)\n",
      "Epoch: 120 | Batch_idx: 190 |  Loss_1: (0.2891) | Acc_1: (89.07%) (21776/24448)\n",
      "Epoch: 120 | Batch_idx: 200 |  Loss_1: (0.2890) | Acc_1: (89.12%) (22929/25728)\n",
      "Epoch: 120 | Batch_idx: 210 |  Loss_1: (0.2893) | Acc_1: (89.11%) (24067/27008)\n",
      "Epoch: 120 | Batch_idx: 220 |  Loss_1: (0.2901) | Acc_1: (89.07%) (25197/28288)\n",
      "Epoch: 120 | Batch_idx: 230 |  Loss_1: (0.2908) | Acc_1: (89.05%) (26331/29568)\n",
      "Epoch: 120 | Batch_idx: 240 |  Loss_1: (0.2923) | Acc_1: (89.00%) (27454/30848)\n",
      "Epoch: 120 | Batch_idx: 250 |  Loss_1: (0.2921) | Acc_1: (88.98%) (28587/32128)\n",
      "Epoch: 120 | Batch_idx: 260 |  Loss_1: (0.2931) | Acc_1: (88.98%) (29725/33408)\n",
      "Epoch: 120 | Batch_idx: 270 |  Loss_1: (0.2926) | Acc_1: (88.98%) (30864/34688)\n",
      "Epoch: 120 | Batch_idx: 280 |  Loss_1: (0.2920) | Acc_1: (89.00%) (32013/35968)\n",
      "Epoch: 120 | Batch_idx: 290 |  Loss_1: (0.2928) | Acc_1: (88.97%) (33139/37248)\n",
      "Epoch: 120 | Batch_idx: 300 |  Loss_1: (0.2927) | Acc_1: (88.99%) (34286/38528)\n",
      "Epoch: 120 | Batch_idx: 310 |  Loss_1: (0.2930) | Acc_1: (88.99%) (35425/39808)\n",
      "Epoch: 120 | Batch_idx: 320 |  Loss_1: (0.2937) | Acc_1: (88.97%) (36555/41088)\n",
      "Epoch: 120 | Batch_idx: 330 |  Loss_1: (0.2948) | Acc_1: (88.94%) (37680/42368)\n",
      "Epoch: 120 | Batch_idx: 340 |  Loss_1: (0.2947) | Acc_1: (88.94%) (38819/43648)\n",
      "Epoch: 120 | Batch_idx: 350 |  Loss_1: (0.2956) | Acc_1: (88.89%) (39936/44928)\n",
      "Epoch: 120 | Batch_idx: 360 |  Loss_1: (0.2949) | Acc_1: (88.92%) (41088/46208)\n",
      "Epoch: 120 | Batch_idx: 370 |  Loss_1: (0.2957) | Acc_1: (88.89%) (42210/47488)\n",
      "Epoch: 120 | Batch_idx: 380 |  Loss_1: (0.2959) | Acc_1: (88.87%) (43339/48768)\n",
      "Epoch: 120 | Batch_idx: 390 |  Loss_1: (0.2959) | Acc_1: (88.87%) (44434/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2789) | Acc: (91.79%) (9179/10000)\n",
      "Epoch: 121 | Batch_idx: 0 |  Loss_1: (0.2752) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 121 | Batch_idx: 10 |  Loss_1: (0.3079) | Acc_1: (88.00%) (1239/1408)\n",
      "Epoch: 121 | Batch_idx: 20 |  Loss_1: (0.3171) | Acc_1: (88.02%) (2366/2688)\n",
      "Epoch: 121 | Batch_idx: 30 |  Loss_1: (0.2968) | Acc_1: (88.99%) (3531/3968)\n",
      "Epoch: 121 | Batch_idx: 40 |  Loss_1: (0.2922) | Acc_1: (89.08%) (4675/5248)\n",
      "Epoch: 121 | Batch_idx: 50 |  Loss_1: (0.2982) | Acc_1: (88.86%) (5801/6528)\n",
      "Epoch: 121 | Batch_idx: 60 |  Loss_1: (0.2942) | Acc_1: (88.99%) (6948/7808)\n",
      "Epoch: 121 | Batch_idx: 70 |  Loss_1: (0.2940) | Acc_1: (88.91%) (8080/9088)\n",
      "Epoch: 121 | Batch_idx: 80 |  Loss_1: (0.2946) | Acc_1: (88.88%) (9215/10368)\n",
      "Epoch: 121 | Batch_idx: 90 |  Loss_1: (0.2964) | Acc_1: (88.83%) (10347/11648)\n",
      "Epoch: 121 | Batch_idx: 100 |  Loss_1: (0.2956) | Acc_1: (88.90%) (11493/12928)\n",
      "Epoch: 121 | Batch_idx: 110 |  Loss_1: (0.2946) | Acc_1: (88.93%) (12635/14208)\n",
      "Epoch: 121 | Batch_idx: 120 |  Loss_1: (0.2963) | Acc_1: (88.82%) (13756/15488)\n",
      "Epoch: 121 | Batch_idx: 130 |  Loss_1: (0.3000) | Acc_1: (88.71%) (14875/16768)\n",
      "Epoch: 121 | Batch_idx: 140 |  Loss_1: (0.3026) | Acc_1: (88.61%) (15992/18048)\n",
      "Epoch: 121 | Batch_idx: 150 |  Loss_1: (0.3023) | Acc_1: (88.61%) (17126/19328)\n",
      "Epoch: 121 | Batch_idx: 160 |  Loss_1: (0.3034) | Acc_1: (88.55%) (18249/20608)\n",
      "Epoch: 121 | Batch_idx: 170 |  Loss_1: (0.3043) | Acc_1: (88.57%) (19386/21888)\n",
      "Epoch: 121 | Batch_idx: 180 |  Loss_1: (0.3031) | Acc_1: (88.62%) (20531/23168)\n",
      "Epoch: 121 | Batch_idx: 190 |  Loss_1: (0.3017) | Acc_1: (88.65%) (21674/24448)\n",
      "Epoch: 121 | Batch_idx: 200 |  Loss_1: (0.3020) | Acc_1: (88.65%) (22807/25728)\n",
      "Epoch: 121 | Batch_idx: 210 |  Loss_1: (0.3028) | Acc_1: (88.63%) (23938/27008)\n",
      "Epoch: 121 | Batch_idx: 220 |  Loss_1: (0.3009) | Acc_1: (88.70%) (25091/28288)\n",
      "Epoch: 121 | Batch_idx: 230 |  Loss_1: (0.3010) | Acc_1: (88.74%) (26240/29568)\n",
      "Epoch: 121 | Batch_idx: 240 |  Loss_1: (0.3022) | Acc_1: (88.68%) (27355/30848)\n",
      "Epoch: 121 | Batch_idx: 250 |  Loss_1: (0.3024) | Acc_1: (88.67%) (28489/32128)\n",
      "Epoch: 121 | Batch_idx: 260 |  Loss_1: (0.3031) | Acc_1: (88.63%) (29611/33408)\n",
      "Epoch: 121 | Batch_idx: 270 |  Loss_1: (0.3038) | Acc_1: (88.61%) (30738/34688)\n",
      "Epoch: 121 | Batch_idx: 280 |  Loss_1: (0.3046) | Acc_1: (88.58%) (31862/35968)\n",
      "Epoch: 121 | Batch_idx: 290 |  Loss_1: (0.3052) | Acc_1: (88.55%) (32983/37248)\n",
      "Epoch: 121 | Batch_idx: 300 |  Loss_1: (0.3061) | Acc_1: (88.51%) (34102/38528)\n",
      "Epoch: 121 | Batch_idx: 310 |  Loss_1: (0.3054) | Acc_1: (88.52%) (35239/39808)\n",
      "Epoch: 121 | Batch_idx: 320 |  Loss_1: (0.3048) | Acc_1: (88.55%) (36382/41088)\n",
      "Epoch: 121 | Batch_idx: 330 |  Loss_1: (0.3055) | Acc_1: (88.51%) (37498/42368)\n",
      "Epoch: 121 | Batch_idx: 340 |  Loss_1: (0.3061) | Acc_1: (88.49%) (38623/43648)\n",
      "Epoch: 121 | Batch_idx: 350 |  Loss_1: (0.3063) | Acc_1: (88.48%) (39753/44928)\n",
      "Epoch: 121 | Batch_idx: 360 |  Loss_1: (0.3070) | Acc_1: (88.45%) (40873/46208)\n",
      "Epoch: 121 | Batch_idx: 370 |  Loss_1: (0.3070) | Acc_1: (88.46%) (42009/47488)\n",
      "Epoch: 121 | Batch_idx: 380 |  Loss_1: (0.3072) | Acc_1: (88.46%) (43139/48768)\n",
      "Epoch: 121 | Batch_idx: 390 |  Loss_1: (0.3079) | Acc_1: (88.45%) (44225/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2648) | Acc: (91.76%) (9176/10000)\n",
      "Epoch: 122 | Batch_idx: 0 |  Loss_1: (0.1458) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 122 | Batch_idx: 10 |  Loss_1: (0.2975) | Acc_1: (88.64%) (1248/1408)\n",
      "Epoch: 122 | Batch_idx: 20 |  Loss_1: (0.2994) | Acc_1: (88.80%) (2387/2688)\n",
      "Epoch: 122 | Batch_idx: 30 |  Loss_1: (0.3020) | Acc_1: (88.68%) (3519/3968)\n",
      "Epoch: 122 | Batch_idx: 40 |  Loss_1: (0.2937) | Acc_1: (88.95%) (4668/5248)\n",
      "Epoch: 122 | Batch_idx: 50 |  Loss_1: (0.2955) | Acc_1: (88.91%) (5804/6528)\n",
      "Epoch: 122 | Batch_idx: 60 |  Loss_1: (0.2963) | Acc_1: (88.91%) (6942/7808)\n",
      "Epoch: 122 | Batch_idx: 70 |  Loss_1: (0.2944) | Acc_1: (88.92%) (8081/9088)\n",
      "Epoch: 122 | Batch_idx: 80 |  Loss_1: (0.2960) | Acc_1: (88.83%) (9210/10368)\n",
      "Epoch: 122 | Batch_idx: 90 |  Loss_1: (0.2939) | Acc_1: (88.99%) (10366/11648)\n",
      "Epoch: 122 | Batch_idx: 100 |  Loss_1: (0.2931) | Acc_1: (89.04%) (11511/12928)\n",
      "Epoch: 122 | Batch_idx: 110 |  Loss_1: (0.2918) | Acc_1: (89.13%) (12664/14208)\n",
      "Epoch: 122 | Batch_idx: 120 |  Loss_1: (0.2938) | Acc_1: (89.09%) (13798/15488)\n",
      "Epoch: 122 | Batch_idx: 130 |  Loss_1: (0.2946) | Acc_1: (89.04%) (14931/16768)\n",
      "Epoch: 122 | Batch_idx: 140 |  Loss_1: (0.2945) | Acc_1: (89.06%) (16073/18048)\n",
      "Epoch: 122 | Batch_idx: 150 |  Loss_1: (0.2940) | Acc_1: (89.05%) (17211/19328)\n",
      "Epoch: 122 | Batch_idx: 160 |  Loss_1: (0.2927) | Acc_1: (89.10%) (18362/20608)\n",
      "Epoch: 122 | Batch_idx: 170 |  Loss_1: (0.2938) | Acc_1: (89.07%) (19495/21888)\n",
      "Epoch: 122 | Batch_idx: 180 |  Loss_1: (0.2941) | Acc_1: (89.08%) (20639/23168)\n",
      "Epoch: 122 | Batch_idx: 190 |  Loss_1: (0.2948) | Acc_1: (89.05%) (21770/24448)\n",
      "Epoch: 122 | Batch_idx: 200 |  Loss_1: (0.2952) | Acc_1: (89.04%) (22907/25728)\n",
      "Epoch: 122 | Batch_idx: 210 |  Loss_1: (0.2951) | Acc_1: (89.05%) (24050/27008)\n",
      "Epoch: 122 | Batch_idx: 220 |  Loss_1: (0.2963) | Acc_1: (88.97%) (25167/28288)\n",
      "Epoch: 122 | Batch_idx: 230 |  Loss_1: (0.2955) | Acc_1: (88.99%) (26312/29568)\n",
      "Epoch: 122 | Batch_idx: 240 |  Loss_1: (0.2951) | Acc_1: (89.03%) (27463/30848)\n",
      "Epoch: 122 | Batch_idx: 250 |  Loss_1: (0.2962) | Acc_1: (88.97%) (28584/32128)\n",
      "Epoch: 122 | Batch_idx: 260 |  Loss_1: (0.2965) | Acc_1: (88.93%) (29710/33408)\n",
      "Epoch: 122 | Batch_idx: 270 |  Loss_1: (0.2973) | Acc_1: (88.92%) (30844/34688)\n",
      "Epoch: 122 | Batch_idx: 280 |  Loss_1: (0.2963) | Acc_1: (88.95%) (31994/35968)\n",
      "Epoch: 122 | Batch_idx: 290 |  Loss_1: (0.2970) | Acc_1: (88.92%) (33121/37248)\n",
      "Epoch: 122 | Batch_idx: 300 |  Loss_1: (0.2971) | Acc_1: (88.90%) (34251/38528)\n",
      "Epoch: 122 | Batch_idx: 310 |  Loss_1: (0.2974) | Acc_1: (88.88%) (35383/39808)\n",
      "Epoch: 122 | Batch_idx: 320 |  Loss_1: (0.2974) | Acc_1: (88.89%) (36523/41088)\n",
      "Epoch: 122 | Batch_idx: 330 |  Loss_1: (0.2986) | Acc_1: (88.85%) (37644/42368)\n",
      "Epoch: 122 | Batch_idx: 340 |  Loss_1: (0.2986) | Acc_1: (88.85%) (38780/43648)\n",
      "Epoch: 122 | Batch_idx: 350 |  Loss_1: (0.2993) | Acc_1: (88.83%) (39909/44928)\n",
      "Epoch: 122 | Batch_idx: 360 |  Loss_1: (0.2986) | Acc_1: (88.86%) (41060/46208)\n",
      "Epoch: 122 | Batch_idx: 370 |  Loss_1: (0.2995) | Acc_1: (88.84%) (42188/47488)\n",
      "Epoch: 122 | Batch_idx: 380 |  Loss_1: (0.3001) | Acc_1: (88.81%) (43313/48768)\n",
      "Epoch: 122 | Batch_idx: 390 |  Loss_1: (0.3009) | Acc_1: (88.78%) (44390/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2484) | Acc: (91.98%) (9198/10000)\n",
      "Epoch: 123 | Batch_idx: 0 |  Loss_1: (0.1949) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 123 | Batch_idx: 10 |  Loss_1: (0.2841) | Acc_1: (89.28%) (1257/1408)\n",
      "Epoch: 123 | Batch_idx: 20 |  Loss_1: (0.2924) | Acc_1: (88.95%) (2391/2688)\n",
      "Epoch: 123 | Batch_idx: 30 |  Loss_1: (0.2808) | Acc_1: (89.29%) (3543/3968)\n",
      "Epoch: 123 | Batch_idx: 40 |  Loss_1: (0.2875) | Acc_1: (89.10%) (4676/5248)\n",
      "Epoch: 123 | Batch_idx: 50 |  Loss_1: (0.2868) | Acc_1: (89.25%) (5826/6528)\n",
      "Epoch: 123 | Batch_idx: 60 |  Loss_1: (0.2893) | Acc_1: (89.08%) (6955/7808)\n",
      "Epoch: 123 | Batch_idx: 70 |  Loss_1: (0.3016) | Acc_1: (88.71%) (8062/9088)\n",
      "Epoch: 123 | Batch_idx: 80 |  Loss_1: (0.2981) | Acc_1: (88.79%) (9206/10368)\n",
      "Epoch: 123 | Batch_idx: 90 |  Loss_1: (0.3019) | Acc_1: (88.60%) (10320/11648)\n",
      "Epoch: 123 | Batch_idx: 100 |  Loss_1: (0.3000) | Acc_1: (88.68%) (11465/12928)\n",
      "Epoch: 123 | Batch_idx: 110 |  Loss_1: (0.2990) | Acc_1: (88.70%) (12603/14208)\n",
      "Epoch: 123 | Batch_idx: 120 |  Loss_1: (0.2986) | Acc_1: (88.71%) (13740/15488)\n",
      "Epoch: 123 | Batch_idx: 130 |  Loss_1: (0.3008) | Acc_1: (88.66%) (14866/16768)\n",
      "Epoch: 123 | Batch_idx: 140 |  Loss_1: (0.3013) | Acc_1: (88.60%) (15990/18048)\n",
      "Epoch: 123 | Batch_idx: 150 |  Loss_1: (0.3008) | Acc_1: (88.59%) (17123/19328)\n",
      "Epoch: 123 | Batch_idx: 160 |  Loss_1: (0.2985) | Acc_1: (88.68%) (18276/20608)\n",
      "Epoch: 123 | Batch_idx: 170 |  Loss_1: (0.2983) | Acc_1: (88.67%) (19408/21888)\n",
      "Epoch: 123 | Batch_idx: 180 |  Loss_1: (0.2993) | Acc_1: (88.63%) (20534/23168)\n",
      "Epoch: 123 | Batch_idx: 190 |  Loss_1: (0.2996) | Acc_1: (88.62%) (21667/24448)\n",
      "Epoch: 123 | Batch_idx: 200 |  Loss_1: (0.3008) | Acc_1: (88.57%) (22788/25728)\n",
      "Epoch: 123 | Batch_idx: 210 |  Loss_1: (0.3006) | Acc_1: (88.59%) (23926/27008)\n",
      "Epoch: 123 | Batch_idx: 220 |  Loss_1: (0.3022) | Acc_1: (88.51%) (25037/28288)\n",
      "Epoch: 123 | Batch_idx: 230 |  Loss_1: (0.2998) | Acc_1: (88.58%) (26192/29568)\n",
      "Epoch: 123 | Batch_idx: 240 |  Loss_1: (0.3008) | Acc_1: (88.54%) (27312/30848)\n",
      "Epoch: 123 | Batch_idx: 250 |  Loss_1: (0.3017) | Acc_1: (88.52%) (28440/32128)\n",
      "Epoch: 123 | Batch_idx: 260 |  Loss_1: (0.3019) | Acc_1: (88.52%) (29574/33408)\n",
      "Epoch: 123 | Batch_idx: 270 |  Loss_1: (0.3015) | Acc_1: (88.54%) (30712/34688)\n",
      "Epoch: 123 | Batch_idx: 280 |  Loss_1: (0.3011) | Acc_1: (88.54%) (31845/35968)\n",
      "Epoch: 123 | Batch_idx: 290 |  Loss_1: (0.3010) | Acc_1: (88.54%) (32980/37248)\n",
      "Epoch: 123 | Batch_idx: 300 |  Loss_1: (0.3016) | Acc_1: (88.54%) (34112/38528)\n",
      "Epoch: 123 | Batch_idx: 310 |  Loss_1: (0.3022) | Acc_1: (88.52%) (35239/39808)\n",
      "Epoch: 123 | Batch_idx: 320 |  Loss_1: (0.3017) | Acc_1: (88.54%) (36379/41088)\n",
      "Epoch: 123 | Batch_idx: 330 |  Loss_1: (0.3020) | Acc_1: (88.53%) (37507/42368)\n",
      "Epoch: 123 | Batch_idx: 340 |  Loss_1: (0.3024) | Acc_1: (88.50%) (38630/43648)\n",
      "Epoch: 123 | Batch_idx: 350 |  Loss_1: (0.3020) | Acc_1: (88.51%) (39766/44928)\n",
      "Epoch: 123 | Batch_idx: 360 |  Loss_1: (0.3018) | Acc_1: (88.51%) (40900/46208)\n",
      "Epoch: 123 | Batch_idx: 370 |  Loss_1: (0.3021) | Acc_1: (88.51%) (42032/47488)\n",
      "Epoch: 123 | Batch_idx: 380 |  Loss_1: (0.3015) | Acc_1: (88.55%) (43184/48768)\n",
      "Epoch: 123 | Batch_idx: 390 |  Loss_1: (0.3011) | Acc_1: (88.58%) (44292/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3323) | Acc: (90.80%) (9080/10000)\n",
      "Epoch: 124 | Batch_idx: 0 |  Loss_1: (0.1748) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 124 | Batch_idx: 10 |  Loss_1: (0.2409) | Acc_1: (90.77%) (1278/1408)\n",
      "Epoch: 124 | Batch_idx: 20 |  Loss_1: (0.2605) | Acc_1: (89.96%) (2418/2688)\n",
      "Epoch: 124 | Batch_idx: 30 |  Loss_1: (0.2644) | Acc_1: (89.92%) (3568/3968)\n",
      "Epoch: 124 | Batch_idx: 40 |  Loss_1: (0.2701) | Acc_1: (89.79%) (4712/5248)\n",
      "Epoch: 124 | Batch_idx: 50 |  Loss_1: (0.2802) | Acc_1: (89.43%) (5838/6528)\n",
      "Epoch: 124 | Batch_idx: 60 |  Loss_1: (0.2842) | Acc_1: (89.25%) (6969/7808)\n",
      "Epoch: 124 | Batch_idx: 70 |  Loss_1: (0.2869) | Acc_1: (89.12%) (8099/9088)\n",
      "Epoch: 124 | Batch_idx: 80 |  Loss_1: (0.2853) | Acc_1: (89.19%) (9247/10368)\n",
      "Epoch: 124 | Batch_idx: 90 |  Loss_1: (0.2887) | Acc_1: (89.04%) (10371/11648)\n",
      "Epoch: 124 | Batch_idx: 100 |  Loss_1: (0.2855) | Acc_1: (89.10%) (11519/12928)\n",
      "Epoch: 124 | Batch_idx: 110 |  Loss_1: (0.2889) | Acc_1: (88.95%) (12638/14208)\n",
      "Epoch: 124 | Batch_idx: 120 |  Loss_1: (0.2906) | Acc_1: (88.89%) (13767/15488)\n",
      "Epoch: 124 | Batch_idx: 130 |  Loss_1: (0.2929) | Acc_1: (88.90%) (14907/16768)\n",
      "Epoch: 124 | Batch_idx: 140 |  Loss_1: (0.2932) | Acc_1: (88.92%) (16049/18048)\n",
      "Epoch: 124 | Batch_idx: 150 |  Loss_1: (0.2935) | Acc_1: (88.88%) (17178/19328)\n",
      "Epoch: 124 | Batch_idx: 160 |  Loss_1: (0.2914) | Acc_1: (88.93%) (18327/20608)\n",
      "Epoch: 124 | Batch_idx: 170 |  Loss_1: (0.2899) | Acc_1: (89.00%) (19480/21888)\n",
      "Epoch: 124 | Batch_idx: 180 |  Loss_1: (0.2905) | Acc_1: (88.97%) (20613/23168)\n",
      "Epoch: 124 | Batch_idx: 190 |  Loss_1: (0.2923) | Acc_1: (88.89%) (21733/24448)\n",
      "Epoch: 124 | Batch_idx: 200 |  Loss_1: (0.2921) | Acc_1: (88.93%) (22879/25728)\n",
      "Epoch: 124 | Batch_idx: 210 |  Loss_1: (0.2914) | Acc_1: (88.96%) (24026/27008)\n",
      "Epoch: 124 | Batch_idx: 220 |  Loss_1: (0.2913) | Acc_1: (88.93%) (25157/28288)\n",
      "Epoch: 124 | Batch_idx: 230 |  Loss_1: (0.2905) | Acc_1: (88.99%) (26312/29568)\n",
      "Epoch: 124 | Batch_idx: 240 |  Loss_1: (0.2892) | Acc_1: (89.02%) (27462/30848)\n",
      "Epoch: 124 | Batch_idx: 250 |  Loss_1: (0.2905) | Acc_1: (88.95%) (28578/32128)\n",
      "Epoch: 124 | Batch_idx: 260 |  Loss_1: (0.2905) | Acc_1: (88.98%) (29726/33408)\n",
      "Epoch: 124 | Batch_idx: 270 |  Loss_1: (0.2918) | Acc_1: (88.93%) (30849/34688)\n",
      "Epoch: 124 | Batch_idx: 280 |  Loss_1: (0.2920) | Acc_1: (88.93%) (31987/35968)\n",
      "Epoch: 124 | Batch_idx: 290 |  Loss_1: (0.2933) | Acc_1: (88.89%) (33110/37248)\n",
      "Epoch: 124 | Batch_idx: 300 |  Loss_1: (0.2937) | Acc_1: (88.88%) (34242/38528)\n",
      "Epoch: 124 | Batch_idx: 310 |  Loss_1: (0.2941) | Acc_1: (88.87%) (35377/39808)\n",
      "Epoch: 124 | Batch_idx: 320 |  Loss_1: (0.2942) | Acc_1: (88.87%) (36513/41088)\n",
      "Epoch: 124 | Batch_idx: 330 |  Loss_1: (0.2937) | Acc_1: (88.89%) (37660/42368)\n",
      "Epoch: 124 | Batch_idx: 340 |  Loss_1: (0.2934) | Acc_1: (88.89%) (38799/43648)\n",
      "Epoch: 124 | Batch_idx: 350 |  Loss_1: (0.2933) | Acc_1: (88.90%) (39943/44928)\n",
      "Epoch: 124 | Batch_idx: 360 |  Loss_1: (0.2925) | Acc_1: (88.94%) (41096/46208)\n",
      "Epoch: 124 | Batch_idx: 370 |  Loss_1: (0.2916) | Acc_1: (88.97%) (42252/47488)\n",
      "Epoch: 124 | Batch_idx: 380 |  Loss_1: (0.2908) | Acc_1: (89.00%) (43402/48768)\n",
      "Epoch: 124 | Batch_idx: 390 |  Loss_1: (0.2919) | Acc_1: (88.96%) (44481/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2834) | Acc: (91.86%) (9186/10000)\n",
      "Epoch: 125 | Batch_idx: 0 |  Loss_1: (0.2891) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 125 | Batch_idx: 10 |  Loss_1: (0.2937) | Acc_1: (88.64%) (1248/1408)\n",
      "Epoch: 125 | Batch_idx: 20 |  Loss_1: (0.2979) | Acc_1: (88.32%) (2374/2688)\n",
      "Epoch: 125 | Batch_idx: 30 |  Loss_1: (0.3028) | Acc_1: (88.23%) (3501/3968)\n",
      "Epoch: 125 | Batch_idx: 40 |  Loss_1: (0.3033) | Acc_1: (88.32%) (4635/5248)\n",
      "Epoch: 125 | Batch_idx: 50 |  Loss_1: (0.3030) | Acc_1: (88.48%) (5776/6528)\n",
      "Epoch: 125 | Batch_idx: 60 |  Loss_1: (0.3005) | Acc_1: (88.61%) (6919/7808)\n",
      "Epoch: 125 | Batch_idx: 70 |  Loss_1: (0.2964) | Acc_1: (88.84%) (8074/9088)\n",
      "Epoch: 125 | Batch_idx: 80 |  Loss_1: (0.2945) | Acc_1: (88.91%) (9218/10368)\n",
      "Epoch: 125 | Batch_idx: 90 |  Loss_1: (0.2979) | Acc_1: (88.76%) (10339/11648)\n",
      "Epoch: 125 | Batch_idx: 100 |  Loss_1: (0.2987) | Acc_1: (88.73%) (11471/12928)\n",
      "Epoch: 125 | Batch_idx: 110 |  Loss_1: (0.2992) | Acc_1: (88.64%) (12594/14208)\n",
      "Epoch: 125 | Batch_idx: 120 |  Loss_1: (0.2980) | Acc_1: (88.73%) (13743/15488)\n",
      "Epoch: 125 | Batch_idx: 130 |  Loss_1: (0.2985) | Acc_1: (88.69%) (14872/16768)\n",
      "Epoch: 125 | Batch_idx: 140 |  Loss_1: (0.2966) | Acc_1: (88.75%) (16017/18048)\n",
      "Epoch: 125 | Batch_idx: 150 |  Loss_1: (0.2961) | Acc_1: (88.74%) (17151/19328)\n",
      "Epoch: 125 | Batch_idx: 160 |  Loss_1: (0.2942) | Acc_1: (88.82%) (18304/20608)\n",
      "Epoch: 125 | Batch_idx: 170 |  Loss_1: (0.2928) | Acc_1: (88.89%) (19457/21888)\n",
      "Epoch: 125 | Batch_idx: 180 |  Loss_1: (0.2949) | Acc_1: (88.84%) (20582/23168)\n",
      "Epoch: 125 | Batch_idx: 190 |  Loss_1: (0.2964) | Acc_1: (88.81%) (21712/24448)\n",
      "Epoch: 125 | Batch_idx: 200 |  Loss_1: (0.2966) | Acc_1: (88.80%) (22847/25728)\n",
      "Epoch: 125 | Batch_idx: 210 |  Loss_1: (0.2947) | Acc_1: (88.88%) (24005/27008)\n",
      "Epoch: 125 | Batch_idx: 220 |  Loss_1: (0.2952) | Acc_1: (88.86%) (25136/28288)\n",
      "Epoch: 125 | Batch_idx: 230 |  Loss_1: (0.2962) | Acc_1: (88.82%) (26263/29568)\n",
      "Epoch: 125 | Batch_idx: 240 |  Loss_1: (0.2965) | Acc_1: (88.81%) (27396/30848)\n",
      "Epoch: 125 | Batch_idx: 250 |  Loss_1: (0.2958) | Acc_1: (88.83%) (28539/32128)\n",
      "Epoch: 125 | Batch_idx: 260 |  Loss_1: (0.2960) | Acc_1: (88.82%) (29673/33408)\n",
      "Epoch: 125 | Batch_idx: 270 |  Loss_1: (0.2951) | Acc_1: (88.85%) (30819/34688)\n",
      "Epoch: 125 | Batch_idx: 280 |  Loss_1: (0.2952) | Acc_1: (88.86%) (31960/35968)\n",
      "Epoch: 125 | Batch_idx: 290 |  Loss_1: (0.2951) | Acc_1: (88.86%) (33099/37248)\n",
      "Epoch: 125 | Batch_idx: 300 |  Loss_1: (0.2958) | Acc_1: (88.82%) (34221/38528)\n",
      "Epoch: 125 | Batch_idx: 310 |  Loss_1: (0.2960) | Acc_1: (88.81%) (35353/39808)\n",
      "Epoch: 125 | Batch_idx: 320 |  Loss_1: (0.2968) | Acc_1: (88.76%) (36471/41088)\n",
      "Epoch: 125 | Batch_idx: 330 |  Loss_1: (0.2969) | Acc_1: (88.77%) (37609/42368)\n",
      "Epoch: 125 | Batch_idx: 340 |  Loss_1: (0.2977) | Acc_1: (88.74%) (38734/43648)\n",
      "Epoch: 125 | Batch_idx: 350 |  Loss_1: (0.2980) | Acc_1: (88.74%) (39871/44928)\n",
      "Epoch: 125 | Batch_idx: 360 |  Loss_1: (0.2978) | Acc_1: (88.75%) (41010/46208)\n",
      "Epoch: 125 | Batch_idx: 370 |  Loss_1: (0.2980) | Acc_1: (88.74%) (42141/47488)\n",
      "Epoch: 125 | Batch_idx: 380 |  Loss_1: (0.2981) | Acc_1: (88.74%) (43278/48768)\n",
      "Epoch: 125 | Batch_idx: 390 |  Loss_1: (0.2973) | Acc_1: (88.77%) (44384/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2707) | Acc: (92.03%) (9203/10000)\n",
      "Epoch: 126 | Batch_idx: 0 |  Loss_1: (0.3359) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 126 | Batch_idx: 10 |  Loss_1: (0.2804) | Acc_1: (89.35%) (1258/1408)\n",
      "Epoch: 126 | Batch_idx: 20 |  Loss_1: (0.2830) | Acc_1: (89.47%) (2405/2688)\n",
      "Epoch: 126 | Batch_idx: 30 |  Loss_1: (0.3040) | Acc_1: (88.86%) (3526/3968)\n",
      "Epoch: 126 | Batch_idx: 40 |  Loss_1: (0.2998) | Acc_1: (88.89%) (4665/5248)\n",
      "Epoch: 126 | Batch_idx: 50 |  Loss_1: (0.2887) | Acc_1: (89.26%) (5827/6528)\n",
      "Epoch: 126 | Batch_idx: 60 |  Loss_1: (0.2874) | Acc_1: (89.23%) (6967/7808)\n",
      "Epoch: 126 | Batch_idx: 70 |  Loss_1: (0.2798) | Acc_1: (89.54%) (8137/9088)\n",
      "Epoch: 126 | Batch_idx: 80 |  Loss_1: (0.2822) | Acc_1: (89.38%) (9267/10368)\n",
      "Epoch: 126 | Batch_idx: 90 |  Loss_1: (0.2828) | Acc_1: (89.43%) (10417/11648)\n",
      "Epoch: 126 | Batch_idx: 100 |  Loss_1: (0.2832) | Acc_1: (89.46%) (11565/12928)\n",
      "Epoch: 126 | Batch_idx: 110 |  Loss_1: (0.2837) | Acc_1: (89.40%) (12702/14208)\n",
      "Epoch: 126 | Batch_idx: 120 |  Loss_1: (0.2849) | Acc_1: (89.36%) (13840/15488)\n",
      "Epoch: 126 | Batch_idx: 130 |  Loss_1: (0.2834) | Acc_1: (89.43%) (14996/16768)\n",
      "Epoch: 126 | Batch_idx: 140 |  Loss_1: (0.2818) | Acc_1: (89.47%) (16147/18048)\n",
      "Epoch: 126 | Batch_idx: 150 |  Loss_1: (0.2821) | Acc_1: (89.43%) (17285/19328)\n",
      "Epoch: 126 | Batch_idx: 160 |  Loss_1: (0.2842) | Acc_1: (89.37%) (18417/20608)\n",
      "Epoch: 126 | Batch_idx: 170 |  Loss_1: (0.2870) | Acc_1: (89.27%) (19540/21888)\n",
      "Epoch: 126 | Batch_idx: 180 |  Loss_1: (0.2885) | Acc_1: (89.27%) (20681/23168)\n",
      "Epoch: 126 | Batch_idx: 190 |  Loss_1: (0.2890) | Acc_1: (89.24%) (21818/24448)\n",
      "Epoch: 126 | Batch_idx: 200 |  Loss_1: (0.2913) | Acc_1: (89.14%) (22935/25728)\n",
      "Epoch: 126 | Batch_idx: 210 |  Loss_1: (0.2899) | Acc_1: (89.18%) (24086/27008)\n",
      "Epoch: 126 | Batch_idx: 220 |  Loss_1: (0.2901) | Acc_1: (89.15%) (25220/28288)\n",
      "Epoch: 126 | Batch_idx: 230 |  Loss_1: (0.2903) | Acc_1: (89.14%) (26356/29568)\n",
      "Epoch: 126 | Batch_idx: 240 |  Loss_1: (0.2907) | Acc_1: (89.10%) (27485/30848)\n",
      "Epoch: 126 | Batch_idx: 250 |  Loss_1: (0.2918) | Acc_1: (89.06%) (28612/32128)\n",
      "Epoch: 126 | Batch_idx: 260 |  Loss_1: (0.2924) | Acc_1: (89.02%) (29741/33408)\n",
      "Epoch: 126 | Batch_idx: 270 |  Loss_1: (0.2923) | Acc_1: (89.03%) (30881/34688)\n",
      "Epoch: 126 | Batch_idx: 280 |  Loss_1: (0.2927) | Acc_1: (88.99%) (32009/35968)\n",
      "Epoch: 126 | Batch_idx: 290 |  Loss_1: (0.2936) | Acc_1: (88.94%) (33127/37248)\n",
      "Epoch: 126 | Batch_idx: 300 |  Loss_1: (0.2940) | Acc_1: (88.93%) (34262/38528)\n",
      "Epoch: 126 | Batch_idx: 310 |  Loss_1: (0.2941) | Acc_1: (88.94%) (35404/39808)\n",
      "Epoch: 126 | Batch_idx: 320 |  Loss_1: (0.2948) | Acc_1: (88.92%) (36535/41088)\n",
      "Epoch: 126 | Batch_idx: 330 |  Loss_1: (0.2958) | Acc_1: (88.89%) (37659/42368)\n",
      "Epoch: 126 | Batch_idx: 340 |  Loss_1: (0.2961) | Acc_1: (88.88%) (38795/43648)\n",
      "Epoch: 126 | Batch_idx: 350 |  Loss_1: (0.2958) | Acc_1: (88.90%) (39940/44928)\n",
      "Epoch: 126 | Batch_idx: 360 |  Loss_1: (0.2946) | Acc_1: (88.95%) (41103/46208)\n",
      "Epoch: 126 | Batch_idx: 370 |  Loss_1: (0.2939) | Acc_1: (88.99%) (42258/47488)\n",
      "Epoch: 126 | Batch_idx: 380 |  Loss_1: (0.2942) | Acc_1: (88.97%) (43390/48768)\n",
      "Epoch: 126 | Batch_idx: 390 |  Loss_1: (0.2939) | Acc_1: (88.98%) (44492/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2648) | Acc: (92.61%) (9261/10000)\n",
      "Epoch: 127 | Batch_idx: 0 |  Loss_1: (0.3511) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 127 | Batch_idx: 10 |  Loss_1: (0.3084) | Acc_1: (88.28%) (1243/1408)\n",
      "Epoch: 127 | Batch_idx: 20 |  Loss_1: (0.3129) | Acc_1: (87.98%) (2365/2688)\n",
      "Epoch: 127 | Batch_idx: 30 |  Loss_1: (0.3000) | Acc_1: (88.43%) (3509/3968)\n",
      "Epoch: 127 | Batch_idx: 40 |  Loss_1: (0.2959) | Acc_1: (88.83%) (4662/5248)\n",
      "Epoch: 127 | Batch_idx: 50 |  Loss_1: (0.2938) | Acc_1: (88.89%) (5803/6528)\n",
      "Epoch: 127 | Batch_idx: 60 |  Loss_1: (0.2971) | Acc_1: (88.82%) (6935/7808)\n",
      "Epoch: 127 | Batch_idx: 70 |  Loss_1: (0.2951) | Acc_1: (88.89%) (8078/9088)\n",
      "Epoch: 127 | Batch_idx: 80 |  Loss_1: (0.2964) | Acc_1: (88.86%) (9213/10368)\n",
      "Epoch: 127 | Batch_idx: 90 |  Loss_1: (0.2986) | Acc_1: (88.78%) (10341/11648)\n",
      "Epoch: 127 | Batch_idx: 100 |  Loss_1: (0.3003) | Acc_1: (88.71%) (11468/12928)\n",
      "Epoch: 127 | Batch_idx: 110 |  Loss_1: (0.2969) | Acc_1: (88.84%) (12622/14208)\n",
      "Epoch: 127 | Batch_idx: 120 |  Loss_1: (0.2970) | Acc_1: (88.84%) (13760/15488)\n",
      "Epoch: 127 | Batch_idx: 130 |  Loss_1: (0.2992) | Acc_1: (88.82%) (14893/16768)\n",
      "Epoch: 127 | Batch_idx: 140 |  Loss_1: (0.2986) | Acc_1: (88.84%) (16033/18048)\n",
      "Epoch: 127 | Batch_idx: 150 |  Loss_1: (0.2975) | Acc_1: (88.88%) (17178/19328)\n",
      "Epoch: 127 | Batch_idx: 160 |  Loss_1: (0.2969) | Acc_1: (88.90%) (18321/20608)\n",
      "Epoch: 127 | Batch_idx: 170 |  Loss_1: (0.2974) | Acc_1: (88.87%) (19451/21888)\n",
      "Epoch: 127 | Batch_idx: 180 |  Loss_1: (0.2975) | Acc_1: (88.87%) (20589/23168)\n",
      "Epoch: 127 | Batch_idx: 190 |  Loss_1: (0.2963) | Acc_1: (88.89%) (21731/24448)\n",
      "Epoch: 127 | Batch_idx: 200 |  Loss_1: (0.2953) | Acc_1: (88.92%) (22878/25728)\n",
      "Epoch: 127 | Batch_idx: 210 |  Loss_1: (0.2939) | Acc_1: (88.96%) (24027/27008)\n",
      "Epoch: 127 | Batch_idx: 220 |  Loss_1: (0.2933) | Acc_1: (88.99%) (25173/28288)\n",
      "Epoch: 127 | Batch_idx: 230 |  Loss_1: (0.2939) | Acc_1: (88.99%) (26314/29568)\n",
      "Epoch: 127 | Batch_idx: 240 |  Loss_1: (0.2953) | Acc_1: (88.96%) (27443/30848)\n",
      "Epoch: 127 | Batch_idx: 250 |  Loss_1: (0.2952) | Acc_1: (88.94%) (28575/32128)\n",
      "Epoch: 127 | Batch_idx: 260 |  Loss_1: (0.2955) | Acc_1: (88.94%) (29712/33408)\n",
      "Epoch: 127 | Batch_idx: 270 |  Loss_1: (0.2960) | Acc_1: (88.92%) (30843/34688)\n",
      "Epoch: 127 | Batch_idx: 280 |  Loss_1: (0.2967) | Acc_1: (88.91%) (31980/35968)\n",
      "Epoch: 127 | Batch_idx: 290 |  Loss_1: (0.2979) | Acc_1: (88.85%) (33096/37248)\n",
      "Epoch: 127 | Batch_idx: 300 |  Loss_1: (0.2983) | Acc_1: (88.83%) (34226/38528)\n",
      "Epoch: 127 | Batch_idx: 310 |  Loss_1: (0.2984) | Acc_1: (88.84%) (35365/39808)\n",
      "Epoch: 127 | Batch_idx: 320 |  Loss_1: (0.2985) | Acc_1: (88.84%) (36501/41088)\n",
      "Epoch: 127 | Batch_idx: 330 |  Loss_1: (0.2989) | Acc_1: (88.81%) (37627/42368)\n",
      "Epoch: 127 | Batch_idx: 340 |  Loss_1: (0.2980) | Acc_1: (88.84%) (38779/43648)\n",
      "Epoch: 127 | Batch_idx: 350 |  Loss_1: (0.2991) | Acc_1: (88.81%) (39902/44928)\n",
      "Epoch: 127 | Batch_idx: 360 |  Loss_1: (0.2992) | Acc_1: (88.78%) (41024/46208)\n",
      "Epoch: 127 | Batch_idx: 370 |  Loss_1: (0.3003) | Acc_1: (88.74%) (42142/47488)\n",
      "Epoch: 127 | Batch_idx: 380 |  Loss_1: (0.3018) | Acc_1: (88.70%) (43258/48768)\n",
      "Epoch: 127 | Batch_idx: 390 |  Loss_1: (0.3016) | Acc_1: (88.71%) (44354/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2653) | Acc: (92.07%) (9207/10000)\n",
      "Epoch: 128 | Batch_idx: 0 |  Loss_1: (0.2666) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 128 | Batch_idx: 10 |  Loss_1: (0.2877) | Acc_1: (89.49%) (1260/1408)\n",
      "Epoch: 128 | Batch_idx: 20 |  Loss_1: (0.2960) | Acc_1: (89.10%) (2395/2688)\n",
      "Epoch: 128 | Batch_idx: 30 |  Loss_1: (0.2914) | Acc_1: (89.36%) (3546/3968)\n",
      "Epoch: 128 | Batch_idx: 40 |  Loss_1: (0.2987) | Acc_1: (89.04%) (4673/5248)\n",
      "Epoch: 128 | Batch_idx: 50 |  Loss_1: (0.3018) | Acc_1: (88.69%) (5790/6528)\n",
      "Epoch: 128 | Batch_idx: 60 |  Loss_1: (0.2993) | Acc_1: (88.79%) (6933/7808)\n",
      "Epoch: 128 | Batch_idx: 70 |  Loss_1: (0.3008) | Acc_1: (88.75%) (8066/9088)\n",
      "Epoch: 128 | Batch_idx: 80 |  Loss_1: (0.3022) | Acc_1: (88.75%) (9202/10368)\n",
      "Epoch: 128 | Batch_idx: 90 |  Loss_1: (0.3031) | Acc_1: (88.63%) (10324/11648)\n",
      "Epoch: 128 | Batch_idx: 100 |  Loss_1: (0.3031) | Acc_1: (88.64%) (11460/12928)\n",
      "Epoch: 128 | Batch_idx: 110 |  Loss_1: (0.3005) | Acc_1: (88.71%) (12604/14208)\n",
      "Epoch: 128 | Batch_idx: 120 |  Loss_1: (0.2983) | Acc_1: (88.80%) (13753/15488)\n",
      "Epoch: 128 | Batch_idx: 130 |  Loss_1: (0.2976) | Acc_1: (88.87%) (14902/16768)\n",
      "Epoch: 128 | Batch_idx: 140 |  Loss_1: (0.2968) | Acc_1: (88.93%) (16050/18048)\n",
      "Epoch: 128 | Batch_idx: 150 |  Loss_1: (0.2954) | Acc_1: (88.98%) (17198/19328)\n",
      "Epoch: 128 | Batch_idx: 160 |  Loss_1: (0.2969) | Acc_1: (88.93%) (18326/20608)\n",
      "Epoch: 128 | Batch_idx: 170 |  Loss_1: (0.2977) | Acc_1: (88.86%) (19450/21888)\n",
      "Epoch: 128 | Batch_idx: 180 |  Loss_1: (0.2961) | Acc_1: (88.91%) (20598/23168)\n",
      "Epoch: 128 | Batch_idx: 190 |  Loss_1: (0.2977) | Acc_1: (88.84%) (21719/24448)\n",
      "Epoch: 128 | Batch_idx: 200 |  Loss_1: (0.2975) | Acc_1: (88.83%) (22853/25728)\n",
      "Epoch: 128 | Batch_idx: 210 |  Loss_1: (0.2979) | Acc_1: (88.78%) (23979/27008)\n",
      "Epoch: 128 | Batch_idx: 220 |  Loss_1: (0.2985) | Acc_1: (88.77%) (25111/28288)\n",
      "Epoch: 128 | Batch_idx: 230 |  Loss_1: (0.2980) | Acc_1: (88.79%) (26253/29568)\n",
      "Epoch: 128 | Batch_idx: 240 |  Loss_1: (0.2979) | Acc_1: (88.82%) (27400/30848)\n",
      "Epoch: 128 | Batch_idx: 250 |  Loss_1: (0.2985) | Acc_1: (88.78%) (28523/32128)\n",
      "Epoch: 128 | Batch_idx: 260 |  Loss_1: (0.2990) | Acc_1: (88.74%) (29647/33408)\n",
      "Epoch: 128 | Batch_idx: 270 |  Loss_1: (0.3002) | Acc_1: (88.70%) (30767/34688)\n",
      "Epoch: 128 | Batch_idx: 280 |  Loss_1: (0.3001) | Acc_1: (88.70%) (31902/35968)\n",
      "Epoch: 128 | Batch_idx: 290 |  Loss_1: (0.3000) | Acc_1: (88.71%) (33041/37248)\n",
      "Epoch: 128 | Batch_idx: 300 |  Loss_1: (0.3002) | Acc_1: (88.68%) (34167/38528)\n",
      "Epoch: 128 | Batch_idx: 310 |  Loss_1: (0.2993) | Acc_1: (88.73%) (35322/39808)\n",
      "Epoch: 128 | Batch_idx: 320 |  Loss_1: (0.2989) | Acc_1: (88.75%) (36466/41088)\n",
      "Epoch: 128 | Batch_idx: 330 |  Loss_1: (0.2979) | Acc_1: (88.79%) (37617/42368)\n",
      "Epoch: 128 | Batch_idx: 340 |  Loss_1: (0.2977) | Acc_1: (88.76%) (38743/43648)\n",
      "Epoch: 128 | Batch_idx: 350 |  Loss_1: (0.2976) | Acc_1: (88.77%) (39883/44928)\n",
      "Epoch: 128 | Batch_idx: 360 |  Loss_1: (0.2978) | Acc_1: (88.76%) (41016/46208)\n",
      "Epoch: 128 | Batch_idx: 370 |  Loss_1: (0.2981) | Acc_1: (88.74%) (42140/47488)\n",
      "Epoch: 128 | Batch_idx: 380 |  Loss_1: (0.2976) | Acc_1: (88.77%) (43293/48768)\n",
      "Epoch: 128 | Batch_idx: 390 |  Loss_1: (0.2968) | Acc_1: (88.79%) (44397/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2861) | Acc: (91.62%) (9162/10000)\n",
      "Epoch: 129 | Batch_idx: 0 |  Loss_1: (0.2233) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 129 | Batch_idx: 10 |  Loss_1: (0.3037) | Acc_1: (88.07%) (1240/1408)\n",
      "Epoch: 129 | Batch_idx: 20 |  Loss_1: (0.2975) | Acc_1: (88.50%) (2379/2688)\n",
      "Epoch: 129 | Batch_idx: 30 |  Loss_1: (0.2889) | Acc_1: (89.01%) (3532/3968)\n",
      "Epoch: 129 | Batch_idx: 40 |  Loss_1: (0.2830) | Acc_1: (89.10%) (4676/5248)\n",
      "Epoch: 129 | Batch_idx: 50 |  Loss_1: (0.2924) | Acc_1: (88.73%) (5792/6528)\n",
      "Epoch: 129 | Batch_idx: 60 |  Loss_1: (0.2912) | Acc_1: (88.73%) (6928/7808)\n",
      "Epoch: 129 | Batch_idx: 70 |  Loss_1: (0.2938) | Acc_1: (88.66%) (8057/9088)\n",
      "Epoch: 129 | Batch_idx: 80 |  Loss_1: (0.2948) | Acc_1: (88.70%) (9196/10368)\n",
      "Epoch: 129 | Batch_idx: 90 |  Loss_1: (0.2951) | Acc_1: (88.69%) (10331/11648)\n",
      "Epoch: 129 | Batch_idx: 100 |  Loss_1: (0.2940) | Acc_1: (88.71%) (11468/12928)\n",
      "Epoch: 129 | Batch_idx: 110 |  Loss_1: (0.2959) | Acc_1: (88.64%) (12594/14208)\n",
      "Epoch: 129 | Batch_idx: 120 |  Loss_1: (0.2936) | Acc_1: (88.77%) (13748/15488)\n",
      "Epoch: 129 | Batch_idx: 130 |  Loss_1: (0.2914) | Acc_1: (88.84%) (14897/16768)\n",
      "Epoch: 129 | Batch_idx: 140 |  Loss_1: (0.2912) | Acc_1: (88.86%) (16037/18048)\n",
      "Epoch: 129 | Batch_idx: 150 |  Loss_1: (0.2921) | Acc_1: (88.82%) (17168/19328)\n",
      "Epoch: 129 | Batch_idx: 160 |  Loss_1: (0.2922) | Acc_1: (88.84%) (18308/20608)\n",
      "Epoch: 129 | Batch_idx: 170 |  Loss_1: (0.2930) | Acc_1: (88.80%) (19437/21888)\n",
      "Epoch: 129 | Batch_idx: 180 |  Loss_1: (0.2938) | Acc_1: (88.78%) (20569/23168)\n",
      "Epoch: 129 | Batch_idx: 190 |  Loss_1: (0.2931) | Acc_1: (88.83%) (21718/24448)\n",
      "Epoch: 129 | Batch_idx: 200 |  Loss_1: (0.2934) | Acc_1: (88.83%) (22853/25728)\n",
      "Epoch: 129 | Batch_idx: 210 |  Loss_1: (0.2944) | Acc_1: (88.78%) (23979/27008)\n",
      "Epoch: 129 | Batch_idx: 220 |  Loss_1: (0.2942) | Acc_1: (88.78%) (25114/28288)\n",
      "Epoch: 129 | Batch_idx: 230 |  Loss_1: (0.2938) | Acc_1: (88.79%) (26252/29568)\n",
      "Epoch: 129 | Batch_idx: 240 |  Loss_1: (0.2915) | Acc_1: (88.86%) (27413/30848)\n",
      "Epoch: 129 | Batch_idx: 250 |  Loss_1: (0.2914) | Acc_1: (88.84%) (28544/32128)\n",
      "Epoch: 129 | Batch_idx: 260 |  Loss_1: (0.2908) | Acc_1: (88.88%) (29692/33408)\n",
      "Epoch: 129 | Batch_idx: 270 |  Loss_1: (0.2910) | Acc_1: (88.89%) (30835/34688)\n",
      "Epoch: 129 | Batch_idx: 280 |  Loss_1: (0.2902) | Acc_1: (88.94%) (31989/35968)\n",
      "Epoch: 129 | Batch_idx: 290 |  Loss_1: (0.2908) | Acc_1: (88.89%) (33108/37248)\n",
      "Epoch: 129 | Batch_idx: 300 |  Loss_1: (0.2911) | Acc_1: (88.88%) (34242/38528)\n",
      "Epoch: 129 | Batch_idx: 310 |  Loss_1: (0.2910) | Acc_1: (88.88%) (35382/39808)\n",
      "Epoch: 129 | Batch_idx: 320 |  Loss_1: (0.2908) | Acc_1: (88.91%) (36533/41088)\n",
      "Epoch: 129 | Batch_idx: 330 |  Loss_1: (0.2899) | Acc_1: (88.96%) (37691/42368)\n",
      "Epoch: 129 | Batch_idx: 340 |  Loss_1: (0.2902) | Acc_1: (88.95%) (38824/43648)\n",
      "Epoch: 129 | Batch_idx: 350 |  Loss_1: (0.2890) | Acc_1: (89.00%) (39984/44928)\n",
      "Epoch: 129 | Batch_idx: 360 |  Loss_1: (0.2896) | Acc_1: (88.96%) (41107/46208)\n",
      "Epoch: 129 | Batch_idx: 370 |  Loss_1: (0.2897) | Acc_1: (88.96%) (42244/47488)\n",
      "Epoch: 129 | Batch_idx: 380 |  Loss_1: (0.2900) | Acc_1: (88.96%) (43383/48768)\n",
      "Epoch: 129 | Batch_idx: 390 |  Loss_1: (0.2902) | Acc_1: (88.95%) (44475/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2883) | Acc: (92.20%) (9220/10000)\n",
      "Epoch: 130 | Batch_idx: 0 |  Loss_1: (0.3003) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 130 | Batch_idx: 10 |  Loss_1: (0.2952) | Acc_1: (88.57%) (1247/1408)\n",
      "Epoch: 130 | Batch_idx: 20 |  Loss_1: (0.2897) | Acc_1: (88.54%) (2380/2688)\n",
      "Epoch: 130 | Batch_idx: 30 |  Loss_1: (0.2915) | Acc_1: (88.51%) (3512/3968)\n",
      "Epoch: 130 | Batch_idx: 40 |  Loss_1: (0.2776) | Acc_1: (89.08%) (4675/5248)\n",
      "Epoch: 130 | Batch_idx: 50 |  Loss_1: (0.2798) | Acc_1: (89.06%) (5814/6528)\n",
      "Epoch: 130 | Batch_idx: 60 |  Loss_1: (0.2722) | Acc_1: (89.41%) (6981/7808)\n",
      "Epoch: 130 | Batch_idx: 70 |  Loss_1: (0.2732) | Acc_1: (89.36%) (8121/9088)\n",
      "Epoch: 130 | Batch_idx: 80 |  Loss_1: (0.2764) | Acc_1: (89.28%) (9257/10368)\n",
      "Epoch: 130 | Batch_idx: 90 |  Loss_1: (0.2768) | Acc_1: (89.33%) (10405/11648)\n",
      "Epoch: 130 | Batch_idx: 100 |  Loss_1: (0.2763) | Acc_1: (89.36%) (11552/12928)\n",
      "Epoch: 130 | Batch_idx: 110 |  Loss_1: (0.2775) | Acc_1: (89.34%) (12693/14208)\n",
      "Epoch: 130 | Batch_idx: 120 |  Loss_1: (0.2780) | Acc_1: (89.31%) (13833/15488)\n",
      "Epoch: 130 | Batch_idx: 130 |  Loss_1: (0.2780) | Acc_1: (89.35%) (14982/16768)\n",
      "Epoch: 130 | Batch_idx: 140 |  Loss_1: (0.2779) | Acc_1: (89.37%) (16129/18048)\n",
      "Epoch: 130 | Batch_idx: 150 |  Loss_1: (0.2754) | Acc_1: (89.50%) (17299/19328)\n",
      "Epoch: 130 | Batch_idx: 160 |  Loss_1: (0.2737) | Acc_1: (89.57%) (18459/20608)\n",
      "Epoch: 130 | Batch_idx: 170 |  Loss_1: (0.2721) | Acc_1: (89.65%) (19623/21888)\n",
      "Epoch: 130 | Batch_idx: 180 |  Loss_1: (0.2729) | Acc_1: (89.64%) (20767/23168)\n",
      "Epoch: 130 | Batch_idx: 190 |  Loss_1: (0.2737) | Acc_1: (89.58%) (21900/24448)\n",
      "Epoch: 130 | Batch_idx: 200 |  Loss_1: (0.2769) | Acc_1: (89.50%) (23026/25728)\n",
      "Epoch: 130 | Batch_idx: 210 |  Loss_1: (0.2779) | Acc_1: (89.47%) (24164/27008)\n",
      "Epoch: 130 | Batch_idx: 220 |  Loss_1: (0.2792) | Acc_1: (89.40%) (25290/28288)\n",
      "Epoch: 130 | Batch_idx: 230 |  Loss_1: (0.2793) | Acc_1: (89.38%) (26427/29568)\n",
      "Epoch: 130 | Batch_idx: 240 |  Loss_1: (0.2786) | Acc_1: (89.39%) (27576/30848)\n",
      "Epoch: 130 | Batch_idx: 250 |  Loss_1: (0.2800) | Acc_1: (89.35%) (28705/32128)\n",
      "Epoch: 130 | Batch_idx: 260 |  Loss_1: (0.2807) | Acc_1: (89.32%) (29840/33408)\n",
      "Epoch: 130 | Batch_idx: 270 |  Loss_1: (0.2826) | Acc_1: (89.23%) (30953/34688)\n",
      "Epoch: 130 | Batch_idx: 280 |  Loss_1: (0.2817) | Acc_1: (89.26%) (32105/35968)\n",
      "Epoch: 130 | Batch_idx: 290 |  Loss_1: (0.2825) | Acc_1: (89.23%) (33238/37248)\n",
      "Epoch: 130 | Batch_idx: 300 |  Loss_1: (0.2830) | Acc_1: (89.22%) (34374/38528)\n",
      "Epoch: 130 | Batch_idx: 310 |  Loss_1: (0.2840) | Acc_1: (89.18%) (35500/39808)\n",
      "Epoch: 130 | Batch_idx: 320 |  Loss_1: (0.2837) | Acc_1: (89.21%) (36655/41088)\n",
      "Epoch: 130 | Batch_idx: 330 |  Loss_1: (0.2838) | Acc_1: (89.22%) (37800/42368)\n",
      "Epoch: 130 | Batch_idx: 340 |  Loss_1: (0.2836) | Acc_1: (89.22%) (38943/43648)\n",
      "Epoch: 130 | Batch_idx: 350 |  Loss_1: (0.2833) | Acc_1: (89.23%) (40090/44928)\n",
      "Epoch: 130 | Batch_idx: 360 |  Loss_1: (0.2835) | Acc_1: (89.21%) (41222/46208)\n",
      "Epoch: 130 | Batch_idx: 370 |  Loss_1: (0.2832) | Acc_1: (89.21%) (42366/47488)\n",
      "Epoch: 130 | Batch_idx: 380 |  Loss_1: (0.2836) | Acc_1: (89.22%) (43510/48768)\n",
      "Epoch: 130 | Batch_idx: 390 |  Loss_1: (0.2843) | Acc_1: (89.19%) (44594/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2847) | Acc: (92.07%) (9207/10000)\n",
      "Epoch: 131 | Batch_idx: 0 |  Loss_1: (0.3186) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 131 | Batch_idx: 10 |  Loss_1: (0.2858) | Acc_1: (88.85%) (1251/1408)\n",
      "Epoch: 131 | Batch_idx: 20 |  Loss_1: (0.2918) | Acc_1: (88.62%) (2382/2688)\n",
      "Epoch: 131 | Batch_idx: 30 |  Loss_1: (0.2848) | Acc_1: (89.09%) (3535/3968)\n",
      "Epoch: 131 | Batch_idx: 40 |  Loss_1: (0.2762) | Acc_1: (89.60%) (4702/5248)\n",
      "Epoch: 131 | Batch_idx: 50 |  Loss_1: (0.2735) | Acc_1: (89.57%) (5847/6528)\n",
      "Epoch: 131 | Batch_idx: 60 |  Loss_1: (0.2766) | Acc_1: (89.45%) (6984/7808)\n",
      "Epoch: 131 | Batch_idx: 70 |  Loss_1: (0.2776) | Acc_1: (89.35%) (8120/9088)\n",
      "Epoch: 131 | Batch_idx: 80 |  Loss_1: (0.2774) | Acc_1: (89.39%) (9268/10368)\n",
      "Epoch: 131 | Batch_idx: 90 |  Loss_1: (0.2786) | Acc_1: (89.35%) (10408/11648)\n",
      "Epoch: 131 | Batch_idx: 100 |  Loss_1: (0.2792) | Acc_1: (89.33%) (11549/12928)\n",
      "Epoch: 131 | Batch_idx: 110 |  Loss_1: (0.2808) | Acc_1: (89.27%) (12683/14208)\n",
      "Epoch: 131 | Batch_idx: 120 |  Loss_1: (0.2791) | Acc_1: (89.33%) (13835/15488)\n",
      "Epoch: 131 | Batch_idx: 130 |  Loss_1: (0.2802) | Acc_1: (89.26%) (14967/16768)\n",
      "Epoch: 131 | Batch_idx: 140 |  Loss_1: (0.2793) | Acc_1: (89.29%) (16115/18048)\n",
      "Epoch: 131 | Batch_idx: 150 |  Loss_1: (0.2810) | Acc_1: (89.27%) (17255/19328)\n",
      "Epoch: 131 | Batch_idx: 160 |  Loss_1: (0.2805) | Acc_1: (89.33%) (18409/20608)\n",
      "Epoch: 131 | Batch_idx: 170 |  Loss_1: (0.2811) | Acc_1: (89.31%) (19548/21888)\n",
      "Epoch: 131 | Batch_idx: 180 |  Loss_1: (0.2807) | Acc_1: (89.35%) (20700/23168)\n",
      "Epoch: 131 | Batch_idx: 190 |  Loss_1: (0.2802) | Acc_1: (89.37%) (21848/24448)\n",
      "Epoch: 131 | Batch_idx: 200 |  Loss_1: (0.2813) | Acc_1: (89.32%) (22980/25728)\n",
      "Epoch: 131 | Batch_idx: 210 |  Loss_1: (0.2825) | Acc_1: (89.27%) (24110/27008)\n",
      "Epoch: 131 | Batch_idx: 220 |  Loss_1: (0.2817) | Acc_1: (89.30%) (25261/28288)\n",
      "Epoch: 131 | Batch_idx: 230 |  Loss_1: (0.2823) | Acc_1: (89.30%) (26403/29568)\n",
      "Epoch: 131 | Batch_idx: 240 |  Loss_1: (0.2819) | Acc_1: (89.28%) (27542/30848)\n",
      "Epoch: 131 | Batch_idx: 250 |  Loss_1: (0.2807) | Acc_1: (89.33%) (28699/32128)\n",
      "Epoch: 131 | Batch_idx: 260 |  Loss_1: (0.2797) | Acc_1: (89.36%) (29853/33408)\n",
      "Epoch: 131 | Batch_idx: 270 |  Loss_1: (0.2805) | Acc_1: (89.32%) (30985/34688)\n",
      "Epoch: 131 | Batch_idx: 280 |  Loss_1: (0.2814) | Acc_1: (89.29%) (32115/35968)\n",
      "Epoch: 131 | Batch_idx: 290 |  Loss_1: (0.2815) | Acc_1: (89.27%) (33251/37248)\n",
      "Epoch: 131 | Batch_idx: 300 |  Loss_1: (0.2811) | Acc_1: (89.28%) (34396/38528)\n",
      "Epoch: 131 | Batch_idx: 310 |  Loss_1: (0.2805) | Acc_1: (89.31%) (35551/39808)\n",
      "Epoch: 131 | Batch_idx: 320 |  Loss_1: (0.2810) | Acc_1: (89.32%) (36700/41088)\n",
      "Epoch: 131 | Batch_idx: 330 |  Loss_1: (0.2802) | Acc_1: (89.37%) (37863/42368)\n",
      "Epoch: 131 | Batch_idx: 340 |  Loss_1: (0.2802) | Acc_1: (89.37%) (39010/43648)\n",
      "Epoch: 131 | Batch_idx: 350 |  Loss_1: (0.2800) | Acc_1: (89.38%) (40156/44928)\n",
      "Epoch: 131 | Batch_idx: 360 |  Loss_1: (0.2810) | Acc_1: (89.33%) (41279/46208)\n",
      "Epoch: 131 | Batch_idx: 370 |  Loss_1: (0.2806) | Acc_1: (89.33%) (42421/47488)\n",
      "Epoch: 131 | Batch_idx: 380 |  Loss_1: (0.2812) | Acc_1: (89.31%) (43556/48768)\n",
      "Epoch: 131 | Batch_idx: 390 |  Loss_1: (0.2817) | Acc_1: (89.30%) (44648/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2783) | Acc: (92.39%) (9239/10000)\n",
      "Epoch: 132 | Batch_idx: 0 |  Loss_1: (0.1901) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 132 | Batch_idx: 10 |  Loss_1: (0.3042) | Acc_1: (89.06%) (1254/1408)\n",
      "Epoch: 132 | Batch_idx: 20 |  Loss_1: (0.3080) | Acc_1: (88.50%) (2379/2688)\n",
      "Epoch: 132 | Batch_idx: 30 |  Loss_1: (0.2973) | Acc_1: (88.86%) (3526/3968)\n",
      "Epoch: 132 | Batch_idx: 40 |  Loss_1: (0.2908) | Acc_1: (89.14%) (4678/5248)\n",
      "Epoch: 132 | Batch_idx: 50 |  Loss_1: (0.2886) | Acc_1: (89.11%) (5817/6528)\n",
      "Epoch: 132 | Batch_idx: 60 |  Loss_1: (0.2871) | Acc_1: (89.20%) (6965/7808)\n",
      "Epoch: 132 | Batch_idx: 70 |  Loss_1: (0.2872) | Acc_1: (89.19%) (8106/9088)\n",
      "Epoch: 132 | Batch_idx: 80 |  Loss_1: (0.2851) | Acc_1: (89.31%) (9260/10368)\n",
      "Epoch: 132 | Batch_idx: 90 |  Loss_1: (0.2818) | Acc_1: (89.48%) (10423/11648)\n",
      "Epoch: 132 | Batch_idx: 100 |  Loss_1: (0.2822) | Acc_1: (89.45%) (11564/12928)\n",
      "Epoch: 132 | Batch_idx: 110 |  Loss_1: (0.2805) | Acc_1: (89.56%) (12724/14208)\n",
      "Epoch: 132 | Batch_idx: 120 |  Loss_1: (0.2776) | Acc_1: (89.66%) (13886/15488)\n",
      "Epoch: 132 | Batch_idx: 130 |  Loss_1: (0.2782) | Acc_1: (89.63%) (15029/16768)\n",
      "Epoch: 132 | Batch_idx: 140 |  Loss_1: (0.2769) | Acc_1: (89.68%) (16185/18048)\n",
      "Epoch: 132 | Batch_idx: 150 |  Loss_1: (0.2775) | Acc_1: (89.62%) (17321/19328)\n",
      "Epoch: 132 | Batch_idx: 160 |  Loss_1: (0.2786) | Acc_1: (89.57%) (18459/20608)\n",
      "Epoch: 132 | Batch_idx: 170 |  Loss_1: (0.2790) | Acc_1: (89.53%) (19597/21888)\n",
      "Epoch: 132 | Batch_idx: 180 |  Loss_1: (0.2777) | Acc_1: (89.60%) (20759/23168)\n",
      "Epoch: 132 | Batch_idx: 190 |  Loss_1: (0.2798) | Acc_1: (89.50%) (21882/24448)\n",
      "Epoch: 132 | Batch_idx: 200 |  Loss_1: (0.2811) | Acc_1: (89.46%) (23015/25728)\n",
      "Epoch: 132 | Batch_idx: 210 |  Loss_1: (0.2816) | Acc_1: (89.41%) (24147/27008)\n",
      "Epoch: 132 | Batch_idx: 220 |  Loss_1: (0.2826) | Acc_1: (89.37%) (25281/28288)\n",
      "Epoch: 132 | Batch_idx: 230 |  Loss_1: (0.2836) | Acc_1: (89.33%) (26412/29568)\n",
      "Epoch: 132 | Batch_idx: 240 |  Loss_1: (0.2819) | Acc_1: (89.36%) (27566/30848)\n",
      "Epoch: 132 | Batch_idx: 250 |  Loss_1: (0.2830) | Acc_1: (89.31%) (28694/32128)\n",
      "Epoch: 132 | Batch_idx: 260 |  Loss_1: (0.2845) | Acc_1: (89.27%) (29822/33408)\n",
      "Epoch: 132 | Batch_idx: 270 |  Loss_1: (0.2847) | Acc_1: (89.24%) (30954/34688)\n",
      "Epoch: 132 | Batch_idx: 280 |  Loss_1: (0.2863) | Acc_1: (89.20%) (32085/35968)\n",
      "Epoch: 132 | Batch_idx: 290 |  Loss_1: (0.2864) | Acc_1: (89.19%) (33222/37248)\n",
      "Epoch: 132 | Batch_idx: 300 |  Loss_1: (0.2870) | Acc_1: (89.17%) (34357/38528)\n",
      "Epoch: 132 | Batch_idx: 310 |  Loss_1: (0.2872) | Acc_1: (89.16%) (35491/39808)\n",
      "Epoch: 132 | Batch_idx: 320 |  Loss_1: (0.2869) | Acc_1: (89.16%) (36636/41088)\n",
      "Epoch: 132 | Batch_idx: 330 |  Loss_1: (0.2864) | Acc_1: (89.20%) (37791/42368)\n",
      "Epoch: 132 | Batch_idx: 340 |  Loss_1: (0.2854) | Acc_1: (89.23%) (38947/43648)\n",
      "Epoch: 132 | Batch_idx: 350 |  Loss_1: (0.2846) | Acc_1: (89.25%) (40100/44928)\n",
      "Epoch: 132 | Batch_idx: 360 |  Loss_1: (0.2849) | Acc_1: (89.23%) (41231/46208)\n",
      "Epoch: 132 | Batch_idx: 370 |  Loss_1: (0.2833) | Acc_1: (89.30%) (42405/47488)\n",
      "Epoch: 132 | Batch_idx: 380 |  Loss_1: (0.2830) | Acc_1: (89.30%) (43549/48768)\n",
      "Epoch: 132 | Batch_idx: 390 |  Loss_1: (0.2833) | Acc_1: (89.27%) (44635/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2873) | Acc: (92.06%) (9206/10000)\n",
      "Epoch: 133 | Batch_idx: 0 |  Loss_1: (0.2619) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 133 | Batch_idx: 10 |  Loss_1: (0.3047) | Acc_1: (88.71%) (1249/1408)\n",
      "Epoch: 133 | Batch_idx: 20 |  Loss_1: (0.2975) | Acc_1: (88.84%) (2388/2688)\n",
      "Epoch: 133 | Batch_idx: 30 |  Loss_1: (0.2873) | Acc_1: (89.11%) (3536/3968)\n",
      "Epoch: 133 | Batch_idx: 40 |  Loss_1: (0.2987) | Acc_1: (88.64%) (4652/5248)\n",
      "Epoch: 133 | Batch_idx: 50 |  Loss_1: (0.3016) | Acc_1: (88.50%) (5777/6528)\n",
      "Epoch: 133 | Batch_idx: 60 |  Loss_1: (0.2964) | Acc_1: (88.65%) (6922/7808)\n",
      "Epoch: 133 | Batch_idx: 70 |  Loss_1: (0.2915) | Acc_1: (88.77%) (8067/9088)\n",
      "Epoch: 133 | Batch_idx: 80 |  Loss_1: (0.2862) | Acc_1: (88.98%) (9225/10368)\n",
      "Epoch: 133 | Batch_idx: 90 |  Loss_1: (0.2835) | Acc_1: (89.11%) (10379/11648)\n",
      "Epoch: 133 | Batch_idx: 100 |  Loss_1: (0.2839) | Acc_1: (89.09%) (11517/12928)\n",
      "Epoch: 133 | Batch_idx: 110 |  Loss_1: (0.2823) | Acc_1: (89.13%) (12663/14208)\n",
      "Epoch: 133 | Batch_idx: 120 |  Loss_1: (0.2833) | Acc_1: (89.11%) (13801/15488)\n",
      "Epoch: 133 | Batch_idx: 130 |  Loss_1: (0.2837) | Acc_1: (89.09%) (14938/16768)\n",
      "Epoch: 133 | Batch_idx: 140 |  Loss_1: (0.2863) | Acc_1: (89.02%) (16067/18048)\n",
      "Epoch: 133 | Batch_idx: 150 |  Loss_1: (0.2873) | Acc_1: (89.01%) (17204/19328)\n",
      "Epoch: 133 | Batch_idx: 160 |  Loss_1: (0.2873) | Acc_1: (89.05%) (18351/20608)\n",
      "Epoch: 133 | Batch_idx: 170 |  Loss_1: (0.2879) | Acc_1: (89.03%) (19486/21888)\n",
      "Epoch: 133 | Batch_idx: 180 |  Loss_1: (0.2877) | Acc_1: (89.07%) (20636/23168)\n",
      "Epoch: 133 | Batch_idx: 190 |  Loss_1: (0.2861) | Acc_1: (89.12%) (21789/24448)\n",
      "Epoch: 133 | Batch_idx: 200 |  Loss_1: (0.2859) | Acc_1: (89.13%) (22932/25728)\n",
      "Epoch: 133 | Batch_idx: 210 |  Loss_1: (0.2875) | Acc_1: (89.07%) (24057/27008)\n",
      "Epoch: 133 | Batch_idx: 220 |  Loss_1: (0.2874) | Acc_1: (89.07%) (25196/28288)\n",
      "Epoch: 133 | Batch_idx: 230 |  Loss_1: (0.2867) | Acc_1: (89.09%) (26342/29568)\n",
      "Epoch: 133 | Batch_idx: 240 |  Loss_1: (0.2879) | Acc_1: (89.04%) (27468/30848)\n",
      "Epoch: 133 | Batch_idx: 250 |  Loss_1: (0.2883) | Acc_1: (89.02%) (28600/32128)\n",
      "Epoch: 133 | Batch_idx: 260 |  Loss_1: (0.2878) | Acc_1: (89.07%) (29755/33408)\n",
      "Epoch: 133 | Batch_idx: 270 |  Loss_1: (0.2876) | Acc_1: (89.09%) (30903/34688)\n",
      "Epoch: 133 | Batch_idx: 280 |  Loss_1: (0.2865) | Acc_1: (89.15%) (32067/35968)\n",
      "Epoch: 133 | Batch_idx: 290 |  Loss_1: (0.2866) | Acc_1: (89.18%) (33216/37248)\n",
      "Epoch: 133 | Batch_idx: 300 |  Loss_1: (0.2861) | Acc_1: (89.19%) (34364/38528)\n",
      "Epoch: 133 | Batch_idx: 310 |  Loss_1: (0.2860) | Acc_1: (89.20%) (35508/39808)\n",
      "Epoch: 133 | Batch_idx: 320 |  Loss_1: (0.2863) | Acc_1: (89.19%) (36646/41088)\n",
      "Epoch: 133 | Batch_idx: 330 |  Loss_1: (0.2878) | Acc_1: (89.13%) (37762/42368)\n",
      "Epoch: 133 | Batch_idx: 340 |  Loss_1: (0.2878) | Acc_1: (89.14%) (38909/43648)\n",
      "Epoch: 133 | Batch_idx: 350 |  Loss_1: (0.2874) | Acc_1: (89.15%) (40052/44928)\n",
      "Epoch: 133 | Batch_idx: 360 |  Loss_1: (0.2870) | Acc_1: (89.15%) (41196/46208)\n",
      "Epoch: 133 | Batch_idx: 370 |  Loss_1: (0.2868) | Acc_1: (89.17%) (42346/47488)\n",
      "Epoch: 133 | Batch_idx: 380 |  Loss_1: (0.2866) | Acc_1: (89.17%) (43486/48768)\n",
      "Epoch: 133 | Batch_idx: 390 |  Loss_1: (0.2877) | Acc_1: (89.15%) (44577/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2795) | Acc: (92.24%) (9224/10000)\n",
      "Epoch: 134 | Batch_idx: 0 |  Loss_1: (0.3633) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 134 | Batch_idx: 10 |  Loss_1: (0.2893) | Acc_1: (88.57%) (1247/1408)\n",
      "Epoch: 134 | Batch_idx: 20 |  Loss_1: (0.2884) | Acc_1: (88.99%) (2392/2688)\n",
      "Epoch: 134 | Batch_idx: 30 |  Loss_1: (0.2934) | Acc_1: (89.16%) (3538/3968)\n",
      "Epoch: 134 | Batch_idx: 40 |  Loss_1: (0.2879) | Acc_1: (89.20%) (4681/5248)\n",
      "Epoch: 134 | Batch_idx: 50 |  Loss_1: (0.2916) | Acc_1: (89.08%) (5815/6528)\n",
      "Epoch: 134 | Batch_idx: 60 |  Loss_1: (0.2889) | Acc_1: (89.15%) (6961/7808)\n",
      "Epoch: 134 | Batch_idx: 70 |  Loss_1: (0.2829) | Acc_1: (89.30%) (8116/9088)\n",
      "Epoch: 134 | Batch_idx: 80 |  Loss_1: (0.2819) | Acc_1: (89.28%) (9257/10368)\n",
      "Epoch: 134 | Batch_idx: 90 |  Loss_1: (0.2807) | Acc_1: (89.33%) (10405/11648)\n",
      "Epoch: 134 | Batch_idx: 100 |  Loss_1: (0.2809) | Acc_1: (89.35%) (11551/12928)\n",
      "Epoch: 134 | Batch_idx: 110 |  Loss_1: (0.2783) | Acc_1: (89.46%) (12710/14208)\n",
      "Epoch: 134 | Batch_idx: 120 |  Loss_1: (0.2787) | Acc_1: (89.46%) (13856/15488)\n",
      "Epoch: 134 | Batch_idx: 130 |  Loss_1: (0.2778) | Acc_1: (89.48%) (15004/16768)\n",
      "Epoch: 134 | Batch_idx: 140 |  Loss_1: (0.2754) | Acc_1: (89.62%) (16174/18048)\n",
      "Epoch: 134 | Batch_idx: 150 |  Loss_1: (0.2747) | Acc_1: (89.63%) (17324/19328)\n",
      "Epoch: 134 | Batch_idx: 160 |  Loss_1: (0.2770) | Acc_1: (89.55%) (18455/20608)\n",
      "Epoch: 134 | Batch_idx: 170 |  Loss_1: (0.2758) | Acc_1: (89.60%) (19612/21888)\n",
      "Epoch: 134 | Batch_idx: 180 |  Loss_1: (0.2781) | Acc_1: (89.49%) (20734/23168)\n",
      "Epoch: 134 | Batch_idx: 190 |  Loss_1: (0.2774) | Acc_1: (89.51%) (21884/24448)\n",
      "Epoch: 134 | Batch_idx: 200 |  Loss_1: (0.2758) | Acc_1: (89.59%) (23049/25728)\n",
      "Epoch: 134 | Batch_idx: 210 |  Loss_1: (0.2758) | Acc_1: (89.61%) (24203/27008)\n",
      "Epoch: 134 | Batch_idx: 220 |  Loss_1: (0.2755) | Acc_1: (89.62%) (25353/28288)\n",
      "Epoch: 134 | Batch_idx: 230 |  Loss_1: (0.2744) | Acc_1: (89.64%) (26505/29568)\n",
      "Epoch: 134 | Batch_idx: 240 |  Loss_1: (0.2736) | Acc_1: (89.68%) (27663/30848)\n",
      "Epoch: 134 | Batch_idx: 250 |  Loss_1: (0.2753) | Acc_1: (89.60%) (28786/32128)\n",
      "Epoch: 134 | Batch_idx: 260 |  Loss_1: (0.2753) | Acc_1: (89.58%) (29927/33408)\n",
      "Epoch: 134 | Batch_idx: 270 |  Loss_1: (0.2753) | Acc_1: (89.59%) (31077/34688)\n",
      "Epoch: 134 | Batch_idx: 280 |  Loss_1: (0.2764) | Acc_1: (89.54%) (32205/35968)\n",
      "Epoch: 134 | Batch_idx: 290 |  Loss_1: (0.2778) | Acc_1: (89.48%) (33328/37248)\n",
      "Epoch: 134 | Batch_idx: 300 |  Loss_1: (0.2776) | Acc_1: (89.48%) (34476/38528)\n",
      "Epoch: 134 | Batch_idx: 310 |  Loss_1: (0.2780) | Acc_1: (89.46%) (35614/39808)\n",
      "Epoch: 134 | Batch_idx: 320 |  Loss_1: (0.2764) | Acc_1: (89.52%) (36781/41088)\n",
      "Epoch: 134 | Batch_idx: 330 |  Loss_1: (0.2781) | Acc_1: (89.48%) (37911/42368)\n",
      "Epoch: 134 | Batch_idx: 340 |  Loss_1: (0.2778) | Acc_1: (89.48%) (39056/43648)\n",
      "Epoch: 134 | Batch_idx: 350 |  Loss_1: (0.2773) | Acc_1: (89.49%) (40207/44928)\n",
      "Epoch: 134 | Batch_idx: 360 |  Loss_1: (0.2781) | Acc_1: (89.45%) (41333/46208)\n",
      "Epoch: 134 | Batch_idx: 370 |  Loss_1: (0.2776) | Acc_1: (89.46%) (42484/47488)\n",
      "Epoch: 134 | Batch_idx: 380 |  Loss_1: (0.2768) | Acc_1: (89.50%) (43647/48768)\n",
      "Epoch: 134 | Batch_idx: 390 |  Loss_1: (0.2758) | Acc_1: (89.54%) (44769/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2827) | Acc: (92.44%) (9244/10000)\n",
      "Epoch: 135 | Batch_idx: 0 |  Loss_1: (0.3381) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 135 | Batch_idx: 10 |  Loss_1: (0.2778) | Acc_1: (88.85%) (1251/1408)\n",
      "Epoch: 135 | Batch_idx: 20 |  Loss_1: (0.2879) | Acc_1: (88.76%) (2386/2688)\n",
      "Epoch: 135 | Batch_idx: 30 |  Loss_1: (0.2855) | Acc_1: (89.11%) (3536/3968)\n",
      "Epoch: 135 | Batch_idx: 40 |  Loss_1: (0.2822) | Acc_1: (89.23%) (4683/5248)\n",
      "Epoch: 135 | Batch_idx: 50 |  Loss_1: (0.2768) | Acc_1: (89.48%) (5841/6528)\n",
      "Epoch: 135 | Batch_idx: 60 |  Loss_1: (0.2814) | Acc_1: (89.31%) (6973/7808)\n",
      "Epoch: 135 | Batch_idx: 70 |  Loss_1: (0.2777) | Acc_1: (89.40%) (8125/9088)\n",
      "Epoch: 135 | Batch_idx: 80 |  Loss_1: (0.2797) | Acc_1: (89.35%) (9264/10368)\n",
      "Epoch: 135 | Batch_idx: 90 |  Loss_1: (0.2783) | Acc_1: (89.42%) (10416/11648)\n",
      "Epoch: 135 | Batch_idx: 100 |  Loss_1: (0.2786) | Acc_1: (89.43%) (11561/12928)\n",
      "Epoch: 135 | Batch_idx: 110 |  Loss_1: (0.2789) | Acc_1: (89.41%) (12704/14208)\n",
      "Epoch: 135 | Batch_idx: 120 |  Loss_1: (0.2777) | Acc_1: (89.43%) (13851/15488)\n",
      "Epoch: 135 | Batch_idx: 130 |  Loss_1: (0.2766) | Acc_1: (89.47%) (15003/16768)\n",
      "Epoch: 135 | Batch_idx: 140 |  Loss_1: (0.2753) | Acc_1: (89.52%) (16156/18048)\n",
      "Epoch: 135 | Batch_idx: 150 |  Loss_1: (0.2752) | Acc_1: (89.50%) (17298/19328)\n",
      "Epoch: 135 | Batch_idx: 160 |  Loss_1: (0.2755) | Acc_1: (89.51%) (18446/20608)\n",
      "Epoch: 135 | Batch_idx: 170 |  Loss_1: (0.2781) | Acc_1: (89.44%) (19577/21888)\n",
      "Epoch: 135 | Batch_idx: 180 |  Loss_1: (0.2772) | Acc_1: (89.45%) (20723/23168)\n",
      "Epoch: 135 | Batch_idx: 190 |  Loss_1: (0.2782) | Acc_1: (89.38%) (21851/24448)\n",
      "Epoch: 135 | Batch_idx: 200 |  Loss_1: (0.2788) | Acc_1: (89.33%) (22983/25728)\n",
      "Epoch: 135 | Batch_idx: 210 |  Loss_1: (0.2797) | Acc_1: (89.27%) (24111/27008)\n",
      "Epoch: 135 | Batch_idx: 220 |  Loss_1: (0.2802) | Acc_1: (89.26%) (25250/28288)\n",
      "Epoch: 135 | Batch_idx: 230 |  Loss_1: (0.2809) | Acc_1: (89.25%) (26389/29568)\n",
      "Epoch: 135 | Batch_idx: 240 |  Loss_1: (0.2815) | Acc_1: (89.24%) (27529/30848)\n",
      "Epoch: 135 | Batch_idx: 250 |  Loss_1: (0.2805) | Acc_1: (89.30%) (28691/32128)\n",
      "Epoch: 135 | Batch_idx: 260 |  Loss_1: (0.2811) | Acc_1: (89.28%) (29826/33408)\n",
      "Epoch: 135 | Batch_idx: 270 |  Loss_1: (0.2815) | Acc_1: (89.27%) (30965/34688)\n",
      "Epoch: 135 | Batch_idx: 280 |  Loss_1: (0.2811) | Acc_1: (89.28%) (32111/35968)\n",
      "Epoch: 135 | Batch_idx: 290 |  Loss_1: (0.2817) | Acc_1: (89.25%) (33245/37248)\n",
      "Epoch: 135 | Batch_idx: 300 |  Loss_1: (0.2805) | Acc_1: (89.31%) (34411/38528)\n",
      "Epoch: 135 | Batch_idx: 310 |  Loss_1: (0.2805) | Acc_1: (89.32%) (35558/39808)\n",
      "Epoch: 135 | Batch_idx: 320 |  Loss_1: (0.2802) | Acc_1: (89.32%) (36698/41088)\n",
      "Epoch: 135 | Batch_idx: 330 |  Loss_1: (0.2807) | Acc_1: (89.29%) (37830/42368)\n",
      "Epoch: 135 | Batch_idx: 340 |  Loss_1: (0.2801) | Acc_1: (89.31%) (38983/43648)\n",
      "Epoch: 135 | Batch_idx: 350 |  Loss_1: (0.2798) | Acc_1: (89.33%) (40133/44928)\n",
      "Epoch: 135 | Batch_idx: 360 |  Loss_1: (0.2802) | Acc_1: (89.32%) (41272/46208)\n",
      "Epoch: 135 | Batch_idx: 370 |  Loss_1: (0.2798) | Acc_1: (89.32%) (42417/47488)\n",
      "Epoch: 135 | Batch_idx: 380 |  Loss_1: (0.2795) | Acc_1: (89.34%) (43569/48768)\n",
      "Epoch: 135 | Batch_idx: 390 |  Loss_1: (0.2788) | Acc_1: (89.36%) (44681/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2663) | Acc: (92.56%) (9256/10000)\n",
      "Epoch: 136 | Batch_idx: 0 |  Loss_1: (0.2512) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 136 | Batch_idx: 10 |  Loss_1: (0.2910) | Acc_1: (88.99%) (1253/1408)\n",
      "Epoch: 136 | Batch_idx: 20 |  Loss_1: (0.2707) | Acc_1: (89.88%) (2416/2688)\n",
      "Epoch: 136 | Batch_idx: 30 |  Loss_1: (0.2796) | Acc_1: (89.64%) (3557/3968)\n",
      "Epoch: 136 | Batch_idx: 40 |  Loss_1: (0.2758) | Acc_1: (89.79%) (4712/5248)\n",
      "Epoch: 136 | Batch_idx: 50 |  Loss_1: (0.2642) | Acc_1: (90.17%) (5886/6528)\n",
      "Epoch: 136 | Batch_idx: 60 |  Loss_1: (0.2674) | Acc_1: (90.07%) (7033/7808)\n",
      "Epoch: 136 | Batch_idx: 70 |  Loss_1: (0.2663) | Acc_1: (90.06%) (8185/9088)\n",
      "Epoch: 136 | Batch_idx: 80 |  Loss_1: (0.2703) | Acc_1: (89.88%) (9319/10368)\n",
      "Epoch: 136 | Batch_idx: 90 |  Loss_1: (0.2696) | Acc_1: (89.93%) (10475/11648)\n",
      "Epoch: 136 | Batch_idx: 100 |  Loss_1: (0.2715) | Acc_1: (89.91%) (11624/12928)\n",
      "Epoch: 136 | Batch_idx: 110 |  Loss_1: (0.2694) | Acc_1: (90.01%) (12788/14208)\n",
      "Epoch: 136 | Batch_idx: 120 |  Loss_1: (0.2699) | Acc_1: (89.98%) (13936/15488)\n",
      "Epoch: 136 | Batch_idx: 130 |  Loss_1: (0.2681) | Acc_1: (90.06%) (15101/16768)\n",
      "Epoch: 136 | Batch_idx: 140 |  Loss_1: (0.2670) | Acc_1: (90.12%) (16265/18048)\n",
      "Epoch: 136 | Batch_idx: 150 |  Loss_1: (0.2665) | Acc_1: (90.14%) (17423/19328)\n",
      "Epoch: 136 | Batch_idx: 160 |  Loss_1: (0.2680) | Acc_1: (90.07%) (18561/20608)\n",
      "Epoch: 136 | Batch_idx: 170 |  Loss_1: (0.2701) | Acc_1: (89.97%) (19692/21888)\n",
      "Epoch: 136 | Batch_idx: 180 |  Loss_1: (0.2713) | Acc_1: (89.88%) (20823/23168)\n",
      "Epoch: 136 | Batch_idx: 190 |  Loss_1: (0.2730) | Acc_1: (89.78%) (21950/24448)\n",
      "Epoch: 136 | Batch_idx: 200 |  Loss_1: (0.2744) | Acc_1: (89.72%) (23083/25728)\n",
      "Epoch: 136 | Batch_idx: 210 |  Loss_1: (0.2743) | Acc_1: (89.70%) (24227/27008)\n",
      "Epoch: 136 | Batch_idx: 220 |  Loss_1: (0.2745) | Acc_1: (89.73%) (25382/28288)\n",
      "Epoch: 136 | Batch_idx: 230 |  Loss_1: (0.2745) | Acc_1: (89.73%) (26530/29568)\n",
      "Epoch: 136 | Batch_idx: 240 |  Loss_1: (0.2738) | Acc_1: (89.75%) (27687/30848)\n",
      "Epoch: 136 | Batch_idx: 250 |  Loss_1: (0.2739) | Acc_1: (89.74%) (28833/32128)\n",
      "Epoch: 136 | Batch_idx: 260 |  Loss_1: (0.2742) | Acc_1: (89.75%) (29985/33408)\n",
      "Epoch: 136 | Batch_idx: 270 |  Loss_1: (0.2743) | Acc_1: (89.75%) (31133/34688)\n",
      "Epoch: 136 | Batch_idx: 280 |  Loss_1: (0.2743) | Acc_1: (89.75%) (32280/35968)\n",
      "Epoch: 136 | Batch_idx: 290 |  Loss_1: (0.2742) | Acc_1: (89.73%) (33422/37248)\n",
      "Epoch: 136 | Batch_idx: 300 |  Loss_1: (0.2755) | Acc_1: (89.65%) (34541/38528)\n",
      "Epoch: 136 | Batch_idx: 310 |  Loss_1: (0.2748) | Acc_1: (89.68%) (35700/39808)\n",
      "Epoch: 136 | Batch_idx: 320 |  Loss_1: (0.2756) | Acc_1: (89.65%) (36837/41088)\n",
      "Epoch: 136 | Batch_idx: 330 |  Loss_1: (0.2759) | Acc_1: (89.65%) (37981/42368)\n",
      "Epoch: 136 | Batch_idx: 340 |  Loss_1: (0.2768) | Acc_1: (89.60%) (39110/43648)\n",
      "Epoch: 136 | Batch_idx: 350 |  Loss_1: (0.2765) | Acc_1: (89.62%) (40265/44928)\n",
      "Epoch: 136 | Batch_idx: 360 |  Loss_1: (0.2774) | Acc_1: (89.58%) (41395/46208)\n",
      "Epoch: 136 | Batch_idx: 370 |  Loss_1: (0.2788) | Acc_1: (89.52%) (42509/47488)\n",
      "Epoch: 136 | Batch_idx: 380 |  Loss_1: (0.2795) | Acc_1: (89.49%) (43643/48768)\n",
      "Epoch: 136 | Batch_idx: 390 |  Loss_1: (0.2800) | Acc_1: (89.48%) (44738/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2635) | Acc: (92.59%) (9259/10000)\n",
      "Epoch: 137 | Batch_idx: 0 |  Loss_1: (0.1793) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 137 | Batch_idx: 10 |  Loss_1: (0.2395) | Acc_1: (91.05%) (1282/1408)\n",
      "Epoch: 137 | Batch_idx: 20 |  Loss_1: (0.2481) | Acc_1: (90.62%) (2436/2688)\n",
      "Epoch: 137 | Batch_idx: 30 |  Loss_1: (0.2525) | Acc_1: (90.55%) (3593/3968)\n",
      "Epoch: 137 | Batch_idx: 40 |  Loss_1: (0.2527) | Acc_1: (90.62%) (4756/5248)\n",
      "Epoch: 137 | Batch_idx: 50 |  Loss_1: (0.2571) | Acc_1: (90.52%) (5909/6528)\n",
      "Epoch: 137 | Batch_idx: 60 |  Loss_1: (0.2516) | Acc_1: (90.60%) (7074/7808)\n",
      "Epoch: 137 | Batch_idx: 70 |  Loss_1: (0.2554) | Acc_1: (90.50%) (8225/9088)\n",
      "Epoch: 137 | Batch_idx: 80 |  Loss_1: (0.2584) | Acc_1: (90.33%) (9365/10368)\n",
      "Epoch: 137 | Batch_idx: 90 |  Loss_1: (0.2575) | Acc_1: (90.37%) (10526/11648)\n",
      "Epoch: 137 | Batch_idx: 100 |  Loss_1: (0.2591) | Acc_1: (90.32%) (11677/12928)\n",
      "Epoch: 137 | Batch_idx: 110 |  Loss_1: (0.2590) | Acc_1: (90.29%) (12829/14208)\n",
      "Epoch: 137 | Batch_idx: 120 |  Loss_1: (0.2635) | Acc_1: (90.09%) (13953/15488)\n",
      "Epoch: 137 | Batch_idx: 130 |  Loss_1: (0.2659) | Acc_1: (90.00%) (15091/16768)\n",
      "Epoch: 137 | Batch_idx: 140 |  Loss_1: (0.2670) | Acc_1: (89.98%) (16239/18048)\n",
      "Epoch: 137 | Batch_idx: 150 |  Loss_1: (0.2662) | Acc_1: (90.03%) (17401/19328)\n",
      "Epoch: 137 | Batch_idx: 160 |  Loss_1: (0.2676) | Acc_1: (89.98%) (18543/20608)\n",
      "Epoch: 137 | Batch_idx: 170 |  Loss_1: (0.2679) | Acc_1: (89.99%) (19696/21888)\n",
      "Epoch: 137 | Batch_idx: 180 |  Loss_1: (0.2704) | Acc_1: (89.88%) (20824/23168)\n",
      "Epoch: 137 | Batch_idx: 190 |  Loss_1: (0.2693) | Acc_1: (89.92%) (21984/24448)\n",
      "Epoch: 137 | Batch_idx: 200 |  Loss_1: (0.2689) | Acc_1: (89.92%) (23135/25728)\n",
      "Epoch: 137 | Batch_idx: 210 |  Loss_1: (0.2685) | Acc_1: (89.93%) (24287/27008)\n",
      "Epoch: 137 | Batch_idx: 220 |  Loss_1: (0.2681) | Acc_1: (89.93%) (25440/28288)\n",
      "Epoch: 137 | Batch_idx: 230 |  Loss_1: (0.2681) | Acc_1: (89.92%) (26589/29568)\n",
      "Epoch: 137 | Batch_idx: 240 |  Loss_1: (0.2695) | Acc_1: (89.87%) (27724/30848)\n",
      "Epoch: 137 | Batch_idx: 250 |  Loss_1: (0.2693) | Acc_1: (89.88%) (28876/32128)\n",
      "Epoch: 137 | Batch_idx: 260 |  Loss_1: (0.2693) | Acc_1: (89.89%) (30032/33408)\n",
      "Epoch: 137 | Batch_idx: 270 |  Loss_1: (0.2695) | Acc_1: (89.88%) (31179/34688)\n",
      "Epoch: 137 | Batch_idx: 280 |  Loss_1: (0.2703) | Acc_1: (89.85%) (32318/35968)\n",
      "Epoch: 137 | Batch_idx: 290 |  Loss_1: (0.2704) | Acc_1: (89.85%) (33468/37248)\n",
      "Epoch: 137 | Batch_idx: 300 |  Loss_1: (0.2708) | Acc_1: (89.82%) (34607/38528)\n",
      "Epoch: 137 | Batch_idx: 310 |  Loss_1: (0.2712) | Acc_1: (89.81%) (35751/39808)\n",
      "Epoch: 137 | Batch_idx: 320 |  Loss_1: (0.2723) | Acc_1: (89.75%) (36878/41088)\n",
      "Epoch: 137 | Batch_idx: 330 |  Loss_1: (0.2727) | Acc_1: (89.74%) (38022/42368)\n",
      "Epoch: 137 | Batch_idx: 340 |  Loss_1: (0.2723) | Acc_1: (89.74%) (39170/43648)\n",
      "Epoch: 137 | Batch_idx: 350 |  Loss_1: (0.2735) | Acc_1: (89.70%) (40301/44928)\n",
      "Epoch: 137 | Batch_idx: 360 |  Loss_1: (0.2725) | Acc_1: (89.73%) (41463/46208)\n",
      "Epoch: 137 | Batch_idx: 370 |  Loss_1: (0.2718) | Acc_1: (89.75%) (42622/47488)\n",
      "Epoch: 137 | Batch_idx: 380 |  Loss_1: (0.2717) | Acc_1: (89.76%) (43774/48768)\n",
      "Epoch: 137 | Batch_idx: 390 |  Loss_1: (0.2718) | Acc_1: (89.76%) (44880/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2979) | Acc: (92.67%) (9267/10000)\n",
      "Epoch: 138 | Batch_idx: 0 |  Loss_1: (0.3095) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 138 | Batch_idx: 10 |  Loss_1: (0.2603) | Acc_1: (89.84%) (1265/1408)\n",
      "Epoch: 138 | Batch_idx: 20 |  Loss_1: (0.2582) | Acc_1: (89.96%) (2418/2688)\n",
      "Epoch: 138 | Batch_idx: 30 |  Loss_1: (0.2589) | Acc_1: (89.89%) (3567/3968)\n",
      "Epoch: 138 | Batch_idx: 40 |  Loss_1: (0.2585) | Acc_1: (89.94%) (4720/5248)\n",
      "Epoch: 138 | Batch_idx: 50 |  Loss_1: (0.2642) | Acc_1: (89.78%) (5861/6528)\n",
      "Epoch: 138 | Batch_idx: 60 |  Loss_1: (0.2698) | Acc_1: (89.54%) (6991/7808)\n",
      "Epoch: 138 | Batch_idx: 70 |  Loss_1: (0.2682) | Acc_1: (89.65%) (8147/9088)\n",
      "Epoch: 138 | Batch_idx: 80 |  Loss_1: (0.2700) | Acc_1: (89.65%) (9295/10368)\n",
      "Epoch: 138 | Batch_idx: 90 |  Loss_1: (0.2703) | Acc_1: (89.61%) (10438/11648)\n",
      "Epoch: 138 | Batch_idx: 100 |  Loss_1: (0.2725) | Acc_1: (89.50%) (11570/12928)\n",
      "Epoch: 138 | Batch_idx: 110 |  Loss_1: (0.2728) | Acc_1: (89.49%) (12715/14208)\n",
      "Epoch: 138 | Batch_idx: 120 |  Loss_1: (0.2733) | Acc_1: (89.51%) (13863/15488)\n",
      "Epoch: 138 | Batch_idx: 130 |  Loss_1: (0.2728) | Acc_1: (89.58%) (15020/16768)\n",
      "Epoch: 138 | Batch_idx: 140 |  Loss_1: (0.2721) | Acc_1: (89.61%) (16172/18048)\n",
      "Epoch: 138 | Batch_idx: 150 |  Loss_1: (0.2725) | Acc_1: (89.55%) (17308/19328)\n",
      "Epoch: 138 | Batch_idx: 160 |  Loss_1: (0.2746) | Acc_1: (89.46%) (18436/20608)\n",
      "Epoch: 138 | Batch_idx: 170 |  Loss_1: (0.2766) | Acc_1: (89.39%) (19566/21888)\n",
      "Epoch: 138 | Batch_idx: 180 |  Loss_1: (0.2772) | Acc_1: (89.35%) (20701/23168)\n",
      "Epoch: 138 | Batch_idx: 190 |  Loss_1: (0.2769) | Acc_1: (89.34%) (21843/24448)\n",
      "Epoch: 138 | Batch_idx: 200 |  Loss_1: (0.2763) | Acc_1: (89.39%) (22998/25728)\n",
      "Epoch: 138 | Batch_idx: 210 |  Loss_1: (0.2764) | Acc_1: (89.39%) (24142/27008)\n",
      "Epoch: 138 | Batch_idx: 220 |  Loss_1: (0.2776) | Acc_1: (89.34%) (25273/28288)\n",
      "Epoch: 138 | Batch_idx: 230 |  Loss_1: (0.2788) | Acc_1: (89.31%) (26408/29568)\n",
      "Epoch: 138 | Batch_idx: 240 |  Loss_1: (0.2764) | Acc_1: (89.40%) (27578/30848)\n",
      "Epoch: 138 | Batch_idx: 250 |  Loss_1: (0.2775) | Acc_1: (89.36%) (28710/32128)\n",
      "Epoch: 138 | Batch_idx: 260 |  Loss_1: (0.2773) | Acc_1: (89.37%) (29858/33408)\n",
      "Epoch: 138 | Batch_idx: 270 |  Loss_1: (0.2770) | Acc_1: (89.39%) (31008/34688)\n",
      "Epoch: 138 | Batch_idx: 280 |  Loss_1: (0.2766) | Acc_1: (89.41%) (32160/35968)\n",
      "Epoch: 138 | Batch_idx: 290 |  Loss_1: (0.2775) | Acc_1: (89.40%) (33298/37248)\n",
      "Epoch: 138 | Batch_idx: 300 |  Loss_1: (0.2765) | Acc_1: (89.43%) (34455/38528)\n",
      "Epoch: 138 | Batch_idx: 310 |  Loss_1: (0.2773) | Acc_1: (89.42%) (35596/39808)\n",
      "Epoch: 138 | Batch_idx: 320 |  Loss_1: (0.2766) | Acc_1: (89.44%) (36751/41088)\n",
      "Epoch: 138 | Batch_idx: 330 |  Loss_1: (0.2760) | Acc_1: (89.45%) (37899/42368)\n",
      "Epoch: 138 | Batch_idx: 340 |  Loss_1: (0.2760) | Acc_1: (89.46%) (39047/43648)\n",
      "Epoch: 138 | Batch_idx: 350 |  Loss_1: (0.2761) | Acc_1: (89.46%) (40192/44928)\n",
      "Epoch: 138 | Batch_idx: 360 |  Loss_1: (0.2755) | Acc_1: (89.48%) (41347/46208)\n",
      "Epoch: 138 | Batch_idx: 370 |  Loss_1: (0.2754) | Acc_1: (89.49%) (42497/47488)\n",
      "Epoch: 138 | Batch_idx: 380 |  Loss_1: (0.2756) | Acc_1: (89.51%) (43651/48768)\n",
      "Epoch: 138 | Batch_idx: 390 |  Loss_1: (0.2757) | Acc_1: (89.50%) (44752/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2653) | Acc: (92.61%) (9261/10000)\n",
      "Epoch: 139 | Batch_idx: 0 |  Loss_1: (0.2989) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 139 | Batch_idx: 10 |  Loss_1: (0.2534) | Acc_1: (90.48%) (1274/1408)\n",
      "Epoch: 139 | Batch_idx: 20 |  Loss_1: (0.2403) | Acc_1: (90.92%) (2444/2688)\n",
      "Epoch: 139 | Batch_idx: 30 |  Loss_1: (0.2471) | Acc_1: (90.50%) (3591/3968)\n",
      "Epoch: 139 | Batch_idx: 40 |  Loss_1: (0.2505) | Acc_1: (90.43%) (4746/5248)\n",
      "Epoch: 139 | Batch_idx: 50 |  Loss_1: (0.2637) | Acc_1: (89.89%) (5868/6528)\n",
      "Epoch: 139 | Batch_idx: 60 |  Loss_1: (0.2661) | Acc_1: (89.72%) (7005/7808)\n",
      "Epoch: 139 | Batch_idx: 70 |  Loss_1: (0.2613) | Acc_1: (89.84%) (8165/9088)\n",
      "Epoch: 139 | Batch_idx: 80 |  Loss_1: (0.2597) | Acc_1: (89.94%) (9325/10368)\n",
      "Epoch: 139 | Batch_idx: 90 |  Loss_1: (0.2620) | Acc_1: (89.87%) (10468/11648)\n",
      "Epoch: 139 | Batch_idx: 100 |  Loss_1: (0.2627) | Acc_1: (89.81%) (11610/12928)\n",
      "Epoch: 139 | Batch_idx: 110 |  Loss_1: (0.2631) | Acc_1: (89.81%) (12760/14208)\n",
      "Epoch: 139 | Batch_idx: 120 |  Loss_1: (0.2624) | Acc_1: (89.84%) (13915/15488)\n",
      "Epoch: 139 | Batch_idx: 130 |  Loss_1: (0.2615) | Acc_1: (89.89%) (15072/16768)\n",
      "Epoch: 139 | Batch_idx: 140 |  Loss_1: (0.2625) | Acc_1: (89.87%) (16219/18048)\n",
      "Epoch: 139 | Batch_idx: 150 |  Loss_1: (0.2608) | Acc_1: (89.94%) (17384/19328)\n",
      "Epoch: 139 | Batch_idx: 160 |  Loss_1: (0.2603) | Acc_1: (89.99%) (18545/20608)\n",
      "Epoch: 139 | Batch_idx: 170 |  Loss_1: (0.2609) | Acc_1: (89.98%) (19695/21888)\n",
      "Epoch: 139 | Batch_idx: 180 |  Loss_1: (0.2606) | Acc_1: (90.02%) (20856/23168)\n",
      "Epoch: 139 | Batch_idx: 190 |  Loss_1: (0.2644) | Acc_1: (89.91%) (21980/24448)\n",
      "Epoch: 139 | Batch_idx: 200 |  Loss_1: (0.2649) | Acc_1: (89.87%) (23123/25728)\n",
      "Epoch: 139 | Batch_idx: 210 |  Loss_1: (0.2656) | Acc_1: (89.85%) (24266/27008)\n",
      "Epoch: 139 | Batch_idx: 220 |  Loss_1: (0.2677) | Acc_1: (89.76%) (25390/28288)\n",
      "Epoch: 139 | Batch_idx: 230 |  Loss_1: (0.2688) | Acc_1: (89.73%) (26532/29568)\n",
      "Epoch: 139 | Batch_idx: 240 |  Loss_1: (0.2684) | Acc_1: (89.77%) (27692/30848)\n",
      "Epoch: 139 | Batch_idx: 250 |  Loss_1: (0.2689) | Acc_1: (89.73%) (28829/32128)\n",
      "Epoch: 139 | Batch_idx: 260 |  Loss_1: (0.2687) | Acc_1: (89.73%) (29977/33408)\n",
      "Epoch: 139 | Batch_idx: 270 |  Loss_1: (0.2679) | Acc_1: (89.76%) (31137/34688)\n",
      "Epoch: 139 | Batch_idx: 280 |  Loss_1: (0.2685) | Acc_1: (89.75%) (32280/35968)\n",
      "Epoch: 139 | Batch_idx: 290 |  Loss_1: (0.2688) | Acc_1: (89.73%) (33423/37248)\n",
      "Epoch: 139 | Batch_idx: 300 |  Loss_1: (0.2692) | Acc_1: (89.72%) (34566/38528)\n",
      "Epoch: 139 | Batch_idx: 310 |  Loss_1: (0.2698) | Acc_1: (89.69%) (35702/39808)\n",
      "Epoch: 139 | Batch_idx: 320 |  Loss_1: (0.2689) | Acc_1: (89.72%) (36864/41088)\n",
      "Epoch: 139 | Batch_idx: 330 |  Loss_1: (0.2693) | Acc_1: (89.70%) (38005/42368)\n",
      "Epoch: 139 | Batch_idx: 340 |  Loss_1: (0.2690) | Acc_1: (89.71%) (39158/43648)\n",
      "Epoch: 139 | Batch_idx: 350 |  Loss_1: (0.2689) | Acc_1: (89.72%) (40308/44928)\n",
      "Epoch: 139 | Batch_idx: 360 |  Loss_1: (0.2681) | Acc_1: (89.74%) (41465/46208)\n",
      "Epoch: 139 | Batch_idx: 370 |  Loss_1: (0.2682) | Acc_1: (89.74%) (42615/47488)\n",
      "Epoch: 139 | Batch_idx: 380 |  Loss_1: (0.2681) | Acc_1: (89.75%) (43768/48768)\n",
      "Epoch: 139 | Batch_idx: 390 |  Loss_1: (0.2681) | Acc_1: (89.76%) (44879/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3010) | Acc: (92.34%) (9234/10000)\n",
      "Epoch: 140 | Batch_idx: 0 |  Loss_1: (0.3441) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 140 | Batch_idx: 10 |  Loss_1: (0.2772) | Acc_1: (89.13%) (1255/1408)\n",
      "Epoch: 140 | Batch_idx: 20 |  Loss_1: (0.2767) | Acc_1: (89.10%) (2395/2688)\n",
      "Epoch: 140 | Batch_idx: 30 |  Loss_1: (0.2744) | Acc_1: (89.34%) (3545/3968)\n",
      "Epoch: 140 | Batch_idx: 40 |  Loss_1: (0.2739) | Acc_1: (89.42%) (4693/5248)\n",
      "Epoch: 140 | Batch_idx: 50 |  Loss_1: (0.2698) | Acc_1: (89.60%) (5849/6528)\n",
      "Epoch: 140 | Batch_idx: 60 |  Loss_1: (0.2690) | Acc_1: (89.65%) (7000/7808)\n",
      "Epoch: 140 | Batch_idx: 70 |  Loss_1: (0.2666) | Acc_1: (89.80%) (8161/9088)\n",
      "Epoch: 140 | Batch_idx: 80 |  Loss_1: (0.2677) | Acc_1: (89.73%) (9303/10368)\n",
      "Epoch: 140 | Batch_idx: 90 |  Loss_1: (0.2662) | Acc_1: (89.75%) (10454/11648)\n",
      "Epoch: 140 | Batch_idx: 100 |  Loss_1: (0.2672) | Acc_1: (89.69%) (11595/12928)\n",
      "Epoch: 140 | Batch_idx: 110 |  Loss_1: (0.2706) | Acc_1: (89.57%) (12726/14208)\n",
      "Epoch: 140 | Batch_idx: 120 |  Loss_1: (0.2724) | Acc_1: (89.54%) (13868/15488)\n",
      "Epoch: 140 | Batch_idx: 130 |  Loss_1: (0.2704) | Acc_1: (89.65%) (15032/16768)\n",
      "Epoch: 140 | Batch_idx: 140 |  Loss_1: (0.2690) | Acc_1: (89.73%) (16195/18048)\n",
      "Epoch: 140 | Batch_idx: 150 |  Loss_1: (0.2676) | Acc_1: (89.76%) (17349/19328)\n",
      "Epoch: 140 | Batch_idx: 160 |  Loss_1: (0.2687) | Acc_1: (89.70%) (18486/20608)\n",
      "Epoch: 140 | Batch_idx: 170 |  Loss_1: (0.2699) | Acc_1: (89.69%) (19631/21888)\n",
      "Epoch: 140 | Batch_idx: 180 |  Loss_1: (0.2694) | Acc_1: (89.72%) (20786/23168)\n",
      "Epoch: 140 | Batch_idx: 190 |  Loss_1: (0.2691) | Acc_1: (89.73%) (21936/24448)\n",
      "Epoch: 140 | Batch_idx: 200 |  Loss_1: (0.2702) | Acc_1: (89.69%) (23076/25728)\n",
      "Epoch: 140 | Batch_idx: 210 |  Loss_1: (0.2700) | Acc_1: (89.71%) (24229/27008)\n",
      "Epoch: 140 | Batch_idx: 220 |  Loss_1: (0.2717) | Acc_1: (89.66%) (25362/28288)\n",
      "Epoch: 140 | Batch_idx: 230 |  Loss_1: (0.2708) | Acc_1: (89.68%) (26518/29568)\n",
      "Epoch: 140 | Batch_idx: 240 |  Loss_1: (0.2708) | Acc_1: (89.66%) (27658/30848)\n",
      "Epoch: 140 | Batch_idx: 250 |  Loss_1: (0.2694) | Acc_1: (89.73%) (28828/32128)\n",
      "Epoch: 140 | Batch_idx: 260 |  Loss_1: (0.2698) | Acc_1: (89.70%) (29967/33408)\n",
      "Epoch: 140 | Batch_idx: 270 |  Loss_1: (0.2694) | Acc_1: (89.71%) (31117/34688)\n",
      "Epoch: 140 | Batch_idx: 280 |  Loss_1: (0.2698) | Acc_1: (89.69%) (32259/35968)\n",
      "Epoch: 140 | Batch_idx: 290 |  Loss_1: (0.2715) | Acc_1: (89.62%) (33382/37248)\n",
      "Epoch: 140 | Batch_idx: 300 |  Loss_1: (0.2706) | Acc_1: (89.66%) (34543/38528)\n",
      "Epoch: 140 | Batch_idx: 310 |  Loss_1: (0.2700) | Acc_1: (89.70%) (35706/39808)\n",
      "Epoch: 140 | Batch_idx: 320 |  Loss_1: (0.2691) | Acc_1: (89.73%) (36868/41088)\n",
      "Epoch: 140 | Batch_idx: 330 |  Loss_1: (0.2703) | Acc_1: (89.69%) (38000/42368)\n",
      "Epoch: 140 | Batch_idx: 340 |  Loss_1: (0.2699) | Acc_1: (89.72%) (39159/43648)\n",
      "Epoch: 140 | Batch_idx: 350 |  Loss_1: (0.2702) | Acc_1: (89.69%) (40298/44928)\n",
      "Epoch: 140 | Batch_idx: 360 |  Loss_1: (0.2688) | Acc_1: (89.76%) (41476/46208)\n",
      "Epoch: 140 | Batch_idx: 370 |  Loss_1: (0.2691) | Acc_1: (89.78%) (42635/47488)\n",
      "Epoch: 140 | Batch_idx: 380 |  Loss_1: (0.2690) | Acc_1: (89.79%) (43788/48768)\n",
      "Epoch: 140 | Batch_idx: 390 |  Loss_1: (0.2695) | Acc_1: (89.77%) (44883/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2801) | Acc: (92.34%) (9234/10000)\n",
      "Epoch: 141 | Batch_idx: 0 |  Loss_1: (0.1879) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 141 | Batch_idx: 10 |  Loss_1: (0.2814) | Acc_1: (89.42%) (1259/1408)\n",
      "Epoch: 141 | Batch_idx: 20 |  Loss_1: (0.2566) | Acc_1: (90.40%) (2430/2688)\n",
      "Epoch: 141 | Batch_idx: 30 |  Loss_1: (0.2661) | Acc_1: (90.22%) (3580/3968)\n",
      "Epoch: 141 | Batch_idx: 40 |  Loss_1: (0.2646) | Acc_1: (90.11%) (4729/5248)\n",
      "Epoch: 141 | Batch_idx: 50 |  Loss_1: (0.2663) | Acc_1: (90.06%) (5879/6528)\n",
      "Epoch: 141 | Batch_idx: 60 |  Loss_1: (0.2667) | Acc_1: (89.92%) (7021/7808)\n",
      "Epoch: 141 | Batch_idx: 70 |  Loss_1: (0.2662) | Acc_1: (89.91%) (8171/9088)\n",
      "Epoch: 141 | Batch_idx: 80 |  Loss_1: (0.2666) | Acc_1: (89.89%) (9320/10368)\n",
      "Epoch: 141 | Batch_idx: 90 |  Loss_1: (0.2673) | Acc_1: (89.87%) (10468/11648)\n",
      "Epoch: 141 | Batch_idx: 100 |  Loss_1: (0.2654) | Acc_1: (89.91%) (11623/12928)\n",
      "Epoch: 141 | Batch_idx: 110 |  Loss_1: (0.2632) | Acc_1: (89.99%) (12786/14208)\n",
      "Epoch: 141 | Batch_idx: 120 |  Loss_1: (0.2643) | Acc_1: (89.95%) (13932/15488)\n",
      "Epoch: 141 | Batch_idx: 130 |  Loss_1: (0.2668) | Acc_1: (89.81%) (15059/16768)\n",
      "Epoch: 141 | Batch_idx: 140 |  Loss_1: (0.2704) | Acc_1: (89.68%) (16185/18048)\n",
      "Epoch: 141 | Batch_idx: 150 |  Loss_1: (0.2704) | Acc_1: (89.66%) (17329/19328)\n",
      "Epoch: 141 | Batch_idx: 160 |  Loss_1: (0.2699) | Acc_1: (89.65%) (18476/20608)\n",
      "Epoch: 141 | Batch_idx: 170 |  Loss_1: (0.2699) | Acc_1: (89.65%) (19622/21888)\n",
      "Epoch: 141 | Batch_idx: 180 |  Loss_1: (0.2699) | Acc_1: (89.66%) (20773/23168)\n",
      "Epoch: 141 | Batch_idx: 190 |  Loss_1: (0.2684) | Acc_1: (89.75%) (21942/24448)\n",
      "Epoch: 141 | Batch_idx: 200 |  Loss_1: (0.2687) | Acc_1: (89.73%) (23085/25728)\n",
      "Epoch: 141 | Batch_idx: 210 |  Loss_1: (0.2698) | Acc_1: (89.67%) (24219/27008)\n",
      "Epoch: 141 | Batch_idx: 220 |  Loss_1: (0.2704) | Acc_1: (89.65%) (25360/28288)\n",
      "Epoch: 141 | Batch_idx: 230 |  Loss_1: (0.2701) | Acc_1: (89.63%) (26502/29568)\n",
      "Epoch: 141 | Batch_idx: 240 |  Loss_1: (0.2695) | Acc_1: (89.67%) (27660/30848)\n",
      "Epoch: 141 | Batch_idx: 250 |  Loss_1: (0.2711) | Acc_1: (89.64%) (28801/32128)\n",
      "Epoch: 141 | Batch_idx: 260 |  Loss_1: (0.2724) | Acc_1: (89.57%) (29925/33408)\n",
      "Epoch: 141 | Batch_idx: 270 |  Loss_1: (0.2725) | Acc_1: (89.58%) (31075/34688)\n",
      "Epoch: 141 | Batch_idx: 280 |  Loss_1: (0.2722) | Acc_1: (89.59%) (32223/35968)\n",
      "Epoch: 141 | Batch_idx: 290 |  Loss_1: (0.2721) | Acc_1: (89.60%) (33373/37248)\n",
      "Epoch: 141 | Batch_idx: 300 |  Loss_1: (0.2715) | Acc_1: (89.62%) (34527/38528)\n",
      "Epoch: 141 | Batch_idx: 310 |  Loss_1: (0.2715) | Acc_1: (89.62%) (35674/39808)\n",
      "Epoch: 141 | Batch_idx: 320 |  Loss_1: (0.2714) | Acc_1: (89.62%) (36822/41088)\n",
      "Epoch: 141 | Batch_idx: 330 |  Loss_1: (0.2711) | Acc_1: (89.62%) (37970/42368)\n",
      "Epoch: 141 | Batch_idx: 340 |  Loss_1: (0.2708) | Acc_1: (89.62%) (39117/43648)\n",
      "Epoch: 141 | Batch_idx: 350 |  Loss_1: (0.2710) | Acc_1: (89.60%) (40256/44928)\n",
      "Epoch: 141 | Batch_idx: 360 |  Loss_1: (0.2703) | Acc_1: (89.64%) (41420/46208)\n",
      "Epoch: 141 | Batch_idx: 370 |  Loss_1: (0.2700) | Acc_1: (89.66%) (42576/47488)\n",
      "Epoch: 141 | Batch_idx: 380 |  Loss_1: (0.2709) | Acc_1: (89.63%) (43713/48768)\n",
      "Epoch: 141 | Batch_idx: 390 |  Loss_1: (0.2722) | Acc_1: (89.58%) (44791/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2957) | Acc: (92.66%) (9266/10000)\n",
      "Epoch: 142 | Batch_idx: 0 |  Loss_1: (0.2879) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 142 | Batch_idx: 10 |  Loss_1: (0.2717) | Acc_1: (89.91%) (1266/1408)\n",
      "Epoch: 142 | Batch_idx: 20 |  Loss_1: (0.2673) | Acc_1: (89.99%) (2419/2688)\n",
      "Epoch: 142 | Batch_idx: 30 |  Loss_1: (0.2564) | Acc_1: (90.37%) (3586/3968)\n",
      "Epoch: 142 | Batch_idx: 40 |  Loss_1: (0.2608) | Acc_1: (90.17%) (4732/5248)\n",
      "Epoch: 142 | Batch_idx: 50 |  Loss_1: (0.2626) | Acc_1: (90.07%) (5880/6528)\n",
      "Epoch: 142 | Batch_idx: 60 |  Loss_1: (0.2625) | Acc_1: (90.01%) (7028/7808)\n",
      "Epoch: 142 | Batch_idx: 70 |  Loss_1: (0.2624) | Acc_1: (90.02%) (8181/9088)\n",
      "Epoch: 142 | Batch_idx: 80 |  Loss_1: (0.2650) | Acc_1: (89.88%) (9319/10368)\n",
      "Epoch: 142 | Batch_idx: 90 |  Loss_1: (0.2666) | Acc_1: (89.83%) (10463/11648)\n",
      "Epoch: 142 | Batch_idx: 100 |  Loss_1: (0.2698) | Acc_1: (89.72%) (11599/12928)\n",
      "Epoch: 142 | Batch_idx: 110 |  Loss_1: (0.2713) | Acc_1: (89.66%) (12739/14208)\n",
      "Epoch: 142 | Batch_idx: 120 |  Loss_1: (0.2711) | Acc_1: (89.67%) (13888/15488)\n",
      "Epoch: 142 | Batch_idx: 130 |  Loss_1: (0.2721) | Acc_1: (89.61%) (15026/16768)\n",
      "Epoch: 142 | Batch_idx: 140 |  Loss_1: (0.2754) | Acc_1: (89.52%) (16157/18048)\n",
      "Epoch: 142 | Batch_idx: 150 |  Loss_1: (0.2765) | Acc_1: (89.51%) (17301/19328)\n",
      "Epoch: 142 | Batch_idx: 160 |  Loss_1: (0.2738) | Acc_1: (89.63%) (18470/20608)\n",
      "Epoch: 142 | Batch_idx: 170 |  Loss_1: (0.2740) | Acc_1: (89.58%) (19608/21888)\n",
      "Epoch: 142 | Batch_idx: 180 |  Loss_1: (0.2741) | Acc_1: (89.60%) (20759/23168)\n",
      "Epoch: 142 | Batch_idx: 190 |  Loss_1: (0.2749) | Acc_1: (89.57%) (21899/24448)\n",
      "Epoch: 142 | Batch_idx: 200 |  Loss_1: (0.2757) | Acc_1: (89.55%) (23040/25728)\n",
      "Epoch: 142 | Batch_idx: 210 |  Loss_1: (0.2753) | Acc_1: (89.56%) (24188/27008)\n",
      "Epoch: 142 | Batch_idx: 220 |  Loss_1: (0.2757) | Acc_1: (89.54%) (25330/28288)\n",
      "Epoch: 142 | Batch_idx: 230 |  Loss_1: (0.2761) | Acc_1: (89.54%) (26474/29568)\n",
      "Epoch: 142 | Batch_idx: 240 |  Loss_1: (0.2763) | Acc_1: (89.54%) (27622/30848)\n",
      "Epoch: 142 | Batch_idx: 250 |  Loss_1: (0.2770) | Acc_1: (89.51%) (28758/32128)\n",
      "Epoch: 142 | Batch_idx: 260 |  Loss_1: (0.2776) | Acc_1: (89.45%) (29885/33408)\n",
      "Epoch: 142 | Batch_idx: 270 |  Loss_1: (0.2772) | Acc_1: (89.45%) (31027/34688)\n",
      "Epoch: 142 | Batch_idx: 280 |  Loss_1: (0.2765) | Acc_1: (89.49%) (32188/35968)\n",
      "Epoch: 142 | Batch_idx: 290 |  Loss_1: (0.2763) | Acc_1: (89.51%) (33341/37248)\n",
      "Epoch: 142 | Batch_idx: 300 |  Loss_1: (0.2764) | Acc_1: (89.52%) (34491/38528)\n",
      "Epoch: 142 | Batch_idx: 310 |  Loss_1: (0.2756) | Acc_1: (89.56%) (35652/39808)\n",
      "Epoch: 142 | Batch_idx: 320 |  Loss_1: (0.2760) | Acc_1: (89.54%) (36791/41088)\n",
      "Epoch: 142 | Batch_idx: 330 |  Loss_1: (0.2751) | Acc_1: (89.57%) (37949/42368)\n",
      "Epoch: 142 | Batch_idx: 340 |  Loss_1: (0.2748) | Acc_1: (89.59%) (39106/43648)\n",
      "Epoch: 142 | Batch_idx: 350 |  Loss_1: (0.2744) | Acc_1: (89.61%) (40261/44928)\n",
      "Epoch: 142 | Batch_idx: 360 |  Loss_1: (0.2735) | Acc_1: (89.64%) (41421/46208)\n",
      "Epoch: 142 | Batch_idx: 370 |  Loss_1: (0.2725) | Acc_1: (89.68%) (42585/47488)\n",
      "Epoch: 142 | Batch_idx: 380 |  Loss_1: (0.2730) | Acc_1: (89.64%) (43717/48768)\n",
      "Epoch: 142 | Batch_idx: 390 |  Loss_1: (0.2731) | Acc_1: (89.65%) (44824/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2722) | Acc: (92.70%) (9270/10000)\n",
      "Epoch: 143 | Batch_idx: 0 |  Loss_1: (0.2451) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 143 | Batch_idx: 10 |  Loss_1: (0.2489) | Acc_1: (90.41%) (1273/1408)\n",
      "Epoch: 143 | Batch_idx: 20 |  Loss_1: (0.2576) | Acc_1: (89.81%) (2414/2688)\n",
      "Epoch: 143 | Batch_idx: 30 |  Loss_1: (0.2589) | Acc_1: (89.72%) (3560/3968)\n",
      "Epoch: 143 | Batch_idx: 40 |  Loss_1: (0.2654) | Acc_1: (89.63%) (4704/5248)\n",
      "Epoch: 143 | Batch_idx: 50 |  Loss_1: (0.2623) | Acc_1: (89.77%) (5860/6528)\n",
      "Epoch: 143 | Batch_idx: 60 |  Loss_1: (0.2663) | Acc_1: (89.59%) (6995/7808)\n",
      "Epoch: 143 | Batch_idx: 70 |  Loss_1: (0.2706) | Acc_1: (89.46%) (8130/9088)\n",
      "Epoch: 143 | Batch_idx: 80 |  Loss_1: (0.2734) | Acc_1: (89.36%) (9265/10368)\n",
      "Epoch: 143 | Batch_idx: 90 |  Loss_1: (0.2722) | Acc_1: (89.43%) (10417/11648)\n",
      "Epoch: 143 | Batch_idx: 100 |  Loss_1: (0.2706) | Acc_1: (89.50%) (11571/12928)\n",
      "Epoch: 143 | Batch_idx: 110 |  Loss_1: (0.2711) | Acc_1: (89.56%) (12724/14208)\n",
      "Epoch: 143 | Batch_idx: 120 |  Loss_1: (0.2693) | Acc_1: (89.67%) (13888/15488)\n",
      "Epoch: 143 | Batch_idx: 130 |  Loss_1: (0.2684) | Acc_1: (89.72%) (15044/16768)\n",
      "Epoch: 143 | Batch_idx: 140 |  Loss_1: (0.2676) | Acc_1: (89.79%) (16205/18048)\n",
      "Epoch: 143 | Batch_idx: 150 |  Loss_1: (0.2671) | Acc_1: (89.81%) (17358/19328)\n",
      "Epoch: 143 | Batch_idx: 160 |  Loss_1: (0.2670) | Acc_1: (89.84%) (18514/20608)\n",
      "Epoch: 143 | Batch_idx: 170 |  Loss_1: (0.2654) | Acc_1: (89.92%) (19681/21888)\n",
      "Epoch: 143 | Batch_idx: 180 |  Loss_1: (0.2659) | Acc_1: (89.91%) (20831/23168)\n",
      "Epoch: 143 | Batch_idx: 190 |  Loss_1: (0.2661) | Acc_1: (89.89%) (21976/24448)\n",
      "Epoch: 143 | Batch_idx: 200 |  Loss_1: (0.2660) | Acc_1: (89.90%) (23130/25728)\n",
      "Epoch: 143 | Batch_idx: 210 |  Loss_1: (0.2658) | Acc_1: (89.92%) (24286/27008)\n",
      "Epoch: 143 | Batch_idx: 220 |  Loss_1: (0.2635) | Acc_1: (90.01%) (25461/28288)\n",
      "Epoch: 143 | Batch_idx: 230 |  Loss_1: (0.2643) | Acc_1: (89.99%) (26607/29568)\n",
      "Epoch: 143 | Batch_idx: 240 |  Loss_1: (0.2644) | Acc_1: (89.97%) (27753/30848)\n",
      "Epoch: 143 | Batch_idx: 250 |  Loss_1: (0.2662) | Acc_1: (89.92%) (28889/32128)\n",
      "Epoch: 143 | Batch_idx: 260 |  Loss_1: (0.2660) | Acc_1: (89.91%) (30038/33408)\n",
      "Epoch: 143 | Batch_idx: 270 |  Loss_1: (0.2668) | Acc_1: (89.88%) (31176/34688)\n",
      "Epoch: 143 | Batch_idx: 280 |  Loss_1: (0.2674) | Acc_1: (89.85%) (32317/35968)\n",
      "Epoch: 143 | Batch_idx: 290 |  Loss_1: (0.2672) | Acc_1: (89.82%) (33458/37248)\n",
      "Epoch: 143 | Batch_idx: 300 |  Loss_1: (0.2661) | Acc_1: (89.89%) (34634/38528)\n",
      "Epoch: 143 | Batch_idx: 310 |  Loss_1: (0.2656) | Acc_1: (89.91%) (35791/39808)\n",
      "Epoch: 143 | Batch_idx: 320 |  Loss_1: (0.2659) | Acc_1: (89.89%) (36932/41088)\n",
      "Epoch: 143 | Batch_idx: 330 |  Loss_1: (0.2664) | Acc_1: (89.88%) (38080/42368)\n",
      "Epoch: 143 | Batch_idx: 340 |  Loss_1: (0.2665) | Acc_1: (89.86%) (39224/43648)\n",
      "Epoch: 143 | Batch_idx: 350 |  Loss_1: (0.2665) | Acc_1: (89.89%) (40385/44928)\n",
      "Epoch: 143 | Batch_idx: 360 |  Loss_1: (0.2666) | Acc_1: (89.88%) (41530/46208)\n",
      "Epoch: 143 | Batch_idx: 370 |  Loss_1: (0.2661) | Acc_1: (89.90%) (42694/47488)\n",
      "Epoch: 143 | Batch_idx: 380 |  Loss_1: (0.2666) | Acc_1: (89.87%) (43826/48768)\n",
      "Epoch: 143 | Batch_idx: 390 |  Loss_1: (0.2668) | Acc_1: (89.85%) (44924/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2719) | Acc: (92.91%) (9291/10000)\n",
      "Epoch: 144 | Batch_idx: 0 |  Loss_1: (0.2312) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 144 | Batch_idx: 10 |  Loss_1: (0.2376) | Acc_1: (90.84%) (1279/1408)\n",
      "Epoch: 144 | Batch_idx: 20 |  Loss_1: (0.2583) | Acc_1: (90.14%) (2423/2688)\n",
      "Epoch: 144 | Batch_idx: 30 |  Loss_1: (0.2462) | Acc_1: (90.60%) (3595/3968)\n",
      "Epoch: 144 | Batch_idx: 40 |  Loss_1: (0.2457) | Acc_1: (90.64%) (4757/5248)\n",
      "Epoch: 144 | Batch_idx: 50 |  Loss_1: (0.2523) | Acc_1: (90.47%) (5906/6528)\n",
      "Epoch: 144 | Batch_idx: 60 |  Loss_1: (0.2554) | Acc_1: (90.33%) (7053/7808)\n",
      "Epoch: 144 | Batch_idx: 70 |  Loss_1: (0.2518) | Acc_1: (90.46%) (8221/9088)\n",
      "Epoch: 144 | Batch_idx: 80 |  Loss_1: (0.2537) | Acc_1: (90.44%) (9377/10368)\n",
      "Epoch: 144 | Batch_idx: 90 |  Loss_1: (0.2530) | Acc_1: (90.44%) (10534/11648)\n",
      "Epoch: 144 | Batch_idx: 100 |  Loss_1: (0.2566) | Acc_1: (90.25%) (11668/12928)\n",
      "Epoch: 144 | Batch_idx: 110 |  Loss_1: (0.2612) | Acc_1: (90.08%) (12799/14208)\n",
      "Epoch: 144 | Batch_idx: 120 |  Loss_1: (0.2622) | Acc_1: (90.04%) (13946/15488)\n",
      "Epoch: 144 | Batch_idx: 130 |  Loss_1: (0.2641) | Acc_1: (89.96%) (15085/16768)\n",
      "Epoch: 144 | Batch_idx: 140 |  Loss_1: (0.2633) | Acc_1: (89.99%) (16241/18048)\n",
      "Epoch: 144 | Batch_idx: 150 |  Loss_1: (0.2661) | Acc_1: (89.93%) (17381/19328)\n",
      "Epoch: 144 | Batch_idx: 160 |  Loss_1: (0.2651) | Acc_1: (89.92%) (18530/20608)\n",
      "Epoch: 144 | Batch_idx: 170 |  Loss_1: (0.2636) | Acc_1: (89.97%) (19692/21888)\n",
      "Epoch: 144 | Batch_idx: 180 |  Loss_1: (0.2619) | Acc_1: (90.03%) (20858/23168)\n",
      "Epoch: 144 | Batch_idx: 190 |  Loss_1: (0.2629) | Acc_1: (89.97%) (21995/24448)\n",
      "Epoch: 144 | Batch_idx: 200 |  Loss_1: (0.2639) | Acc_1: (89.95%) (23143/25728)\n",
      "Epoch: 144 | Batch_idx: 210 |  Loss_1: (0.2649) | Acc_1: (89.93%) (24288/27008)\n",
      "Epoch: 144 | Batch_idx: 220 |  Loss_1: (0.2649) | Acc_1: (89.94%) (25442/28288)\n",
      "Epoch: 144 | Batch_idx: 230 |  Loss_1: (0.2637) | Acc_1: (89.97%) (26603/29568)\n",
      "Epoch: 144 | Batch_idx: 240 |  Loss_1: (0.2636) | Acc_1: (89.98%) (27757/30848)\n",
      "Epoch: 144 | Batch_idx: 250 |  Loss_1: (0.2642) | Acc_1: (89.95%) (28898/32128)\n",
      "Epoch: 144 | Batch_idx: 260 |  Loss_1: (0.2643) | Acc_1: (89.94%) (30047/33408)\n",
      "Epoch: 144 | Batch_idx: 270 |  Loss_1: (0.2631) | Acc_1: (89.99%) (31215/34688)\n",
      "Epoch: 144 | Batch_idx: 280 |  Loss_1: (0.2635) | Acc_1: (89.99%) (32366/35968)\n",
      "Epoch: 144 | Batch_idx: 290 |  Loss_1: (0.2639) | Acc_1: (89.98%) (33517/37248)\n",
      "Epoch: 144 | Batch_idx: 300 |  Loss_1: (0.2635) | Acc_1: (90.00%) (34674/38528)\n",
      "Epoch: 144 | Batch_idx: 310 |  Loss_1: (0.2632) | Acc_1: (90.00%) (35828/39808)\n",
      "Epoch: 144 | Batch_idx: 320 |  Loss_1: (0.2633) | Acc_1: (90.00%) (36978/41088)\n",
      "Epoch: 144 | Batch_idx: 330 |  Loss_1: (0.2642) | Acc_1: (89.96%) (38114/42368)\n",
      "Epoch: 144 | Batch_idx: 340 |  Loss_1: (0.2637) | Acc_1: (89.98%) (39275/43648)\n",
      "Epoch: 144 | Batch_idx: 350 |  Loss_1: (0.2634) | Acc_1: (89.98%) (40424/44928)\n",
      "Epoch: 144 | Batch_idx: 360 |  Loss_1: (0.2628) | Acc_1: (90.00%) (41589/46208)\n",
      "Epoch: 144 | Batch_idx: 370 |  Loss_1: (0.2632) | Acc_1: (89.98%) (42731/47488)\n",
      "Epoch: 144 | Batch_idx: 380 |  Loss_1: (0.2628) | Acc_1: (90.01%) (43894/48768)\n",
      "Epoch: 144 | Batch_idx: 390 |  Loss_1: (0.2621) | Acc_1: (90.01%) (45005/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2884) | Acc: (92.64%) (9264/10000)\n",
      "Epoch: 145 | Batch_idx: 0 |  Loss_1: (0.1727) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 145 | Batch_idx: 10 |  Loss_1: (0.2472) | Acc_1: (90.98%) (1281/1408)\n",
      "Epoch: 145 | Batch_idx: 20 |  Loss_1: (0.2468) | Acc_1: (90.81%) (2441/2688)\n",
      "Epoch: 145 | Batch_idx: 30 |  Loss_1: (0.2582) | Acc_1: (90.25%) (3581/3968)\n",
      "Epoch: 145 | Batch_idx: 40 |  Loss_1: (0.2569) | Acc_1: (90.24%) (4736/5248)\n",
      "Epoch: 145 | Batch_idx: 50 |  Loss_1: (0.2482) | Acc_1: (90.72%) (5922/6528)\n",
      "Epoch: 145 | Batch_idx: 60 |  Loss_1: (0.2434) | Acc_1: (90.88%) (7096/7808)\n",
      "Epoch: 145 | Batch_idx: 70 |  Loss_1: (0.2468) | Acc_1: (90.65%) (8238/9088)\n",
      "Epoch: 145 | Batch_idx: 80 |  Loss_1: (0.2469) | Acc_1: (90.62%) (9396/10368)\n",
      "Epoch: 145 | Batch_idx: 90 |  Loss_1: (0.2462) | Acc_1: (90.63%) (10557/11648)\n",
      "Epoch: 145 | Batch_idx: 100 |  Loss_1: (0.2510) | Acc_1: (90.42%) (11689/12928)\n",
      "Epoch: 145 | Batch_idx: 110 |  Loss_1: (0.2526) | Acc_1: (90.36%) (12839/14208)\n",
      "Epoch: 145 | Batch_idx: 120 |  Loss_1: (0.2537) | Acc_1: (90.38%) (13998/15488)\n",
      "Epoch: 145 | Batch_idx: 130 |  Loss_1: (0.2557) | Acc_1: (90.31%) (15143/16768)\n",
      "Epoch: 145 | Batch_idx: 140 |  Loss_1: (0.2531) | Acc_1: (90.41%) (16317/18048)\n",
      "Epoch: 145 | Batch_idx: 150 |  Loss_1: (0.2556) | Acc_1: (90.30%) (17453/19328)\n",
      "Epoch: 145 | Batch_idx: 160 |  Loss_1: (0.2577) | Acc_1: (90.22%) (18592/20608)\n",
      "Epoch: 145 | Batch_idx: 170 |  Loss_1: (0.2598) | Acc_1: (90.18%) (19738/21888)\n",
      "Epoch: 145 | Batch_idx: 180 |  Loss_1: (0.2604) | Acc_1: (90.12%) (20880/23168)\n",
      "Epoch: 145 | Batch_idx: 190 |  Loss_1: (0.2606) | Acc_1: (90.10%) (22028/24448)\n",
      "Epoch: 145 | Batch_idx: 200 |  Loss_1: (0.2596) | Acc_1: (90.14%) (23192/25728)\n",
      "Epoch: 145 | Batch_idx: 210 |  Loss_1: (0.2609) | Acc_1: (90.08%) (24328/27008)\n",
      "Epoch: 145 | Batch_idx: 220 |  Loss_1: (0.2610) | Acc_1: (90.07%) (25478/28288)\n",
      "Epoch: 145 | Batch_idx: 230 |  Loss_1: (0.2616) | Acc_1: (90.07%) (26631/29568)\n",
      "Epoch: 145 | Batch_idx: 240 |  Loss_1: (0.2628) | Acc_1: (90.03%) (27772/30848)\n",
      "Epoch: 145 | Batch_idx: 250 |  Loss_1: (0.2629) | Acc_1: (90.01%) (28920/32128)\n",
      "Epoch: 145 | Batch_idx: 260 |  Loss_1: (0.2641) | Acc_1: (89.96%) (30055/33408)\n",
      "Epoch: 145 | Batch_idx: 270 |  Loss_1: (0.2622) | Acc_1: (90.05%) (31236/34688)\n",
      "Epoch: 145 | Batch_idx: 280 |  Loss_1: (0.2625) | Acc_1: (90.05%) (32390/35968)\n",
      "Epoch: 145 | Batch_idx: 290 |  Loss_1: (0.2624) | Acc_1: (90.06%) (33545/37248)\n",
      "Epoch: 145 | Batch_idx: 300 |  Loss_1: (0.2626) | Acc_1: (90.04%) (34692/38528)\n",
      "Epoch: 145 | Batch_idx: 310 |  Loss_1: (0.2627) | Acc_1: (90.03%) (35841/39808)\n",
      "Epoch: 145 | Batch_idx: 320 |  Loss_1: (0.2629) | Acc_1: (90.02%) (36988/41088)\n",
      "Epoch: 145 | Batch_idx: 330 |  Loss_1: (0.2632) | Acc_1: (90.00%) (38132/42368)\n",
      "Epoch: 145 | Batch_idx: 340 |  Loss_1: (0.2638) | Acc_1: (89.99%) (39278/43648)\n",
      "Epoch: 145 | Batch_idx: 350 |  Loss_1: (0.2637) | Acc_1: (89.98%) (40426/44928)\n",
      "Epoch: 145 | Batch_idx: 360 |  Loss_1: (0.2642) | Acc_1: (89.96%) (41571/46208)\n",
      "Epoch: 145 | Batch_idx: 370 |  Loss_1: (0.2645) | Acc_1: (89.95%) (42717/47488)\n",
      "Epoch: 145 | Batch_idx: 380 |  Loss_1: (0.2644) | Acc_1: (89.95%) (43868/48768)\n",
      "Epoch: 145 | Batch_idx: 390 |  Loss_1: (0.2643) | Acc_1: (89.96%) (44980/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3119) | Acc: (92.55%) (9255/10000)\n",
      "Epoch: 146 | Batch_idx: 0 |  Loss_1: (0.3606) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 146 | Batch_idx: 10 |  Loss_1: (0.2688) | Acc_1: (89.70%) (1263/1408)\n",
      "Epoch: 146 | Batch_idx: 20 |  Loss_1: (0.2700) | Acc_1: (89.47%) (2405/2688)\n",
      "Epoch: 146 | Batch_idx: 30 |  Loss_1: (0.2698) | Acc_1: (89.49%) (3551/3968)\n",
      "Epoch: 146 | Batch_idx: 40 |  Loss_1: (0.2624) | Acc_1: (89.67%) (4706/5248)\n",
      "Epoch: 146 | Batch_idx: 50 |  Loss_1: (0.2689) | Acc_1: (89.52%) (5844/6528)\n",
      "Epoch: 146 | Batch_idx: 60 |  Loss_1: (0.2682) | Acc_1: (89.68%) (7002/7808)\n",
      "Epoch: 146 | Batch_idx: 70 |  Loss_1: (0.2692) | Acc_1: (89.67%) (8149/9088)\n",
      "Epoch: 146 | Batch_idx: 80 |  Loss_1: (0.2718) | Acc_1: (89.55%) (9285/10368)\n",
      "Epoch: 146 | Batch_idx: 90 |  Loss_1: (0.2747) | Acc_1: (89.55%) (10431/11648)\n",
      "Epoch: 146 | Batch_idx: 100 |  Loss_1: (0.2735) | Acc_1: (89.57%) (11580/12928)\n",
      "Epoch: 146 | Batch_idx: 110 |  Loss_1: (0.2754) | Acc_1: (89.48%) (12713/14208)\n",
      "Epoch: 146 | Batch_idx: 120 |  Loss_1: (0.2747) | Acc_1: (89.53%) (13866/15488)\n",
      "Epoch: 146 | Batch_idx: 130 |  Loss_1: (0.2717) | Acc_1: (89.66%) (15034/16768)\n",
      "Epoch: 146 | Batch_idx: 140 |  Loss_1: (0.2716) | Acc_1: (89.60%) (16171/18048)\n",
      "Epoch: 146 | Batch_idx: 150 |  Loss_1: (0.2704) | Acc_1: (89.64%) (17326/19328)\n",
      "Epoch: 146 | Batch_idx: 160 |  Loss_1: (0.2709) | Acc_1: (89.63%) (18471/20608)\n",
      "Epoch: 146 | Batch_idx: 170 |  Loss_1: (0.2694) | Acc_1: (89.68%) (19629/21888)\n",
      "Epoch: 146 | Batch_idx: 180 |  Loss_1: (0.2715) | Acc_1: (89.57%) (20752/23168)\n",
      "Epoch: 146 | Batch_idx: 190 |  Loss_1: (0.2726) | Acc_1: (89.55%) (21894/24448)\n",
      "Epoch: 146 | Batch_idx: 200 |  Loss_1: (0.2723) | Acc_1: (89.58%) (23046/25728)\n",
      "Epoch: 146 | Batch_idx: 210 |  Loss_1: (0.2731) | Acc_1: (89.54%) (24182/27008)\n",
      "Epoch: 146 | Batch_idx: 220 |  Loss_1: (0.2736) | Acc_1: (89.51%) (25321/28288)\n",
      "Epoch: 146 | Batch_idx: 230 |  Loss_1: (0.2736) | Acc_1: (89.53%) (26471/29568)\n",
      "Epoch: 146 | Batch_idx: 240 |  Loss_1: (0.2739) | Acc_1: (89.51%) (27612/30848)\n",
      "Epoch: 146 | Batch_idx: 250 |  Loss_1: (0.2744) | Acc_1: (89.46%) (28741/32128)\n",
      "Epoch: 146 | Batch_idx: 260 |  Loss_1: (0.2757) | Acc_1: (89.43%) (29878/33408)\n",
      "Epoch: 146 | Batch_idx: 270 |  Loss_1: (0.2766) | Acc_1: (89.41%) (31013/34688)\n",
      "Epoch: 146 | Batch_idx: 280 |  Loss_1: (0.2759) | Acc_1: (89.44%) (32169/35968)\n",
      "Epoch: 146 | Batch_idx: 290 |  Loss_1: (0.2756) | Acc_1: (89.46%) (33323/37248)\n",
      "Epoch: 146 | Batch_idx: 300 |  Loss_1: (0.2749) | Acc_1: (89.49%) (34480/38528)\n",
      "Epoch: 146 | Batch_idx: 310 |  Loss_1: (0.2746) | Acc_1: (89.51%) (35633/39808)\n",
      "Epoch: 146 | Batch_idx: 320 |  Loss_1: (0.2734) | Acc_1: (89.55%) (36794/41088)\n",
      "Epoch: 146 | Batch_idx: 330 |  Loss_1: (0.2728) | Acc_1: (89.57%) (37950/42368)\n",
      "Epoch: 146 | Batch_idx: 340 |  Loss_1: (0.2730) | Acc_1: (89.55%) (39088/43648)\n",
      "Epoch: 146 | Batch_idx: 350 |  Loss_1: (0.2724) | Acc_1: (89.57%) (40243/44928)\n",
      "Epoch: 146 | Batch_idx: 360 |  Loss_1: (0.2721) | Acc_1: (89.58%) (41393/46208)\n",
      "Epoch: 146 | Batch_idx: 370 |  Loss_1: (0.2724) | Acc_1: (89.56%) (42531/47488)\n",
      "Epoch: 146 | Batch_idx: 380 |  Loss_1: (0.2724) | Acc_1: (89.56%) (43676/48768)\n",
      "Epoch: 146 | Batch_idx: 390 |  Loss_1: (0.2734) | Acc_1: (89.54%) (44770/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2867) | Acc: (92.83%) (9283/10000)\n",
      "Epoch: 147 | Batch_idx: 0 |  Loss_1: (0.3040) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 147 | Batch_idx: 10 |  Loss_1: (0.2698) | Acc_1: (89.91%) (1266/1408)\n",
      "Epoch: 147 | Batch_idx: 20 |  Loss_1: (0.2871) | Acc_1: (89.21%) (2398/2688)\n",
      "Epoch: 147 | Batch_idx: 30 |  Loss_1: (0.2787) | Acc_1: (89.19%) (3539/3968)\n",
      "Epoch: 147 | Batch_idx: 40 |  Loss_1: (0.2679) | Acc_1: (89.65%) (4705/5248)\n",
      "Epoch: 147 | Batch_idx: 50 |  Loss_1: (0.2674) | Acc_1: (89.64%) (5852/6528)\n",
      "Epoch: 147 | Batch_idx: 60 |  Loss_1: (0.2634) | Acc_1: (89.83%) (7014/7808)\n",
      "Epoch: 147 | Batch_idx: 70 |  Loss_1: (0.2615) | Acc_1: (89.94%) (8174/9088)\n",
      "Epoch: 147 | Batch_idx: 80 |  Loss_1: (0.2637) | Acc_1: (89.84%) (9315/10368)\n",
      "Epoch: 147 | Batch_idx: 90 |  Loss_1: (0.2636) | Acc_1: (89.81%) (10461/11648)\n",
      "Epoch: 147 | Batch_idx: 100 |  Loss_1: (0.2653) | Acc_1: (89.78%) (11607/12928)\n",
      "Epoch: 147 | Batch_idx: 110 |  Loss_1: (0.2655) | Acc_1: (89.82%) (12762/14208)\n",
      "Epoch: 147 | Batch_idx: 120 |  Loss_1: (0.2646) | Acc_1: (89.90%) (13924/15488)\n",
      "Epoch: 147 | Batch_idx: 130 |  Loss_1: (0.2648) | Acc_1: (89.91%) (15076/16768)\n",
      "Epoch: 147 | Batch_idx: 140 |  Loss_1: (0.2626) | Acc_1: (90.02%) (16246/18048)\n",
      "Epoch: 147 | Batch_idx: 150 |  Loss_1: (0.2623) | Acc_1: (90.08%) (17411/19328)\n",
      "Epoch: 147 | Batch_idx: 160 |  Loss_1: (0.2623) | Acc_1: (90.10%) (18568/20608)\n",
      "Epoch: 147 | Batch_idx: 170 |  Loss_1: (0.2638) | Acc_1: (90.01%) (19701/21888)\n",
      "Epoch: 147 | Batch_idx: 180 |  Loss_1: (0.2639) | Acc_1: (89.98%) (20847/23168)\n",
      "Epoch: 147 | Batch_idx: 190 |  Loss_1: (0.2650) | Acc_1: (89.91%) (21980/24448)\n",
      "Epoch: 147 | Batch_idx: 200 |  Loss_1: (0.2661) | Acc_1: (89.84%) (23115/25728)\n",
      "Epoch: 147 | Batch_idx: 210 |  Loss_1: (0.2642) | Acc_1: (89.90%) (24281/27008)\n",
      "Epoch: 147 | Batch_idx: 220 |  Loss_1: (0.2636) | Acc_1: (89.91%) (25435/28288)\n",
      "Epoch: 147 | Batch_idx: 230 |  Loss_1: (0.2644) | Acc_1: (89.90%) (26583/29568)\n",
      "Epoch: 147 | Batch_idx: 240 |  Loss_1: (0.2633) | Acc_1: (89.94%) (27745/30848)\n",
      "Epoch: 147 | Batch_idx: 250 |  Loss_1: (0.2630) | Acc_1: (89.95%) (28898/32128)\n",
      "Epoch: 147 | Batch_idx: 260 |  Loss_1: (0.2625) | Acc_1: (89.95%) (30052/33408)\n",
      "Epoch: 147 | Batch_idx: 270 |  Loss_1: (0.2625) | Acc_1: (89.95%) (31202/34688)\n",
      "Epoch: 147 | Batch_idx: 280 |  Loss_1: (0.2636) | Acc_1: (89.93%) (32347/35968)\n",
      "Epoch: 147 | Batch_idx: 290 |  Loss_1: (0.2657) | Acc_1: (89.88%) (33478/37248)\n",
      "Epoch: 147 | Batch_idx: 300 |  Loss_1: (0.2659) | Acc_1: (89.87%) (34626/38528)\n",
      "Epoch: 147 | Batch_idx: 310 |  Loss_1: (0.2672) | Acc_1: (89.82%) (35757/39808)\n",
      "Epoch: 147 | Batch_idx: 320 |  Loss_1: (0.2660) | Acc_1: (89.87%) (36924/41088)\n",
      "Epoch: 147 | Batch_idx: 330 |  Loss_1: (0.2654) | Acc_1: (89.88%) (38080/42368)\n",
      "Epoch: 147 | Batch_idx: 340 |  Loss_1: (0.2654) | Acc_1: (89.88%) (39230/43648)\n",
      "Epoch: 147 | Batch_idx: 350 |  Loss_1: (0.2652) | Acc_1: (89.89%) (40385/44928)\n",
      "Epoch: 147 | Batch_idx: 360 |  Loss_1: (0.2649) | Acc_1: (89.91%) (41546/46208)\n",
      "Epoch: 147 | Batch_idx: 370 |  Loss_1: (0.2648) | Acc_1: (89.90%) (42694/47488)\n",
      "Epoch: 147 | Batch_idx: 380 |  Loss_1: (0.2652) | Acc_1: (89.88%) (43834/48768)\n",
      "Epoch: 147 | Batch_idx: 390 |  Loss_1: (0.2645) | Acc_1: (89.90%) (44948/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2960) | Acc: (92.67%) (9267/10000)\n",
      "Epoch: 148 | Batch_idx: 0 |  Loss_1: (0.3579) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 148 | Batch_idx: 10 |  Loss_1: (0.2637) | Acc_1: (89.70%) (1263/1408)\n",
      "Epoch: 148 | Batch_idx: 20 |  Loss_1: (0.2366) | Acc_1: (90.85%) (2442/2688)\n",
      "Epoch: 148 | Batch_idx: 30 |  Loss_1: (0.2407) | Acc_1: (90.75%) (3601/3968)\n",
      "Epoch: 148 | Batch_idx: 40 |  Loss_1: (0.2436) | Acc_1: (90.62%) (4756/5248)\n",
      "Epoch: 148 | Batch_idx: 50 |  Loss_1: (0.2501) | Acc_1: (90.29%) (5894/6528)\n",
      "Epoch: 148 | Batch_idx: 60 |  Loss_1: (0.2537) | Acc_1: (90.19%) (7042/7808)\n",
      "Epoch: 148 | Batch_idx: 70 |  Loss_1: (0.2540) | Acc_1: (90.22%) (8199/9088)\n",
      "Epoch: 148 | Batch_idx: 80 |  Loss_1: (0.2531) | Acc_1: (90.21%) (9353/10368)\n",
      "Epoch: 148 | Batch_idx: 90 |  Loss_1: (0.2542) | Acc_1: (90.14%) (10500/11648)\n",
      "Epoch: 148 | Batch_idx: 100 |  Loss_1: (0.2568) | Acc_1: (90.08%) (11646/12928)\n",
      "Epoch: 148 | Batch_idx: 110 |  Loss_1: (0.2578) | Acc_1: (90.08%) (12799/14208)\n",
      "Epoch: 148 | Batch_idx: 120 |  Loss_1: (0.2569) | Acc_1: (90.09%) (13953/15488)\n",
      "Epoch: 148 | Batch_idx: 130 |  Loss_1: (0.2560) | Acc_1: (90.13%) (15113/16768)\n",
      "Epoch: 148 | Batch_idx: 140 |  Loss_1: (0.2554) | Acc_1: (90.17%) (16273/18048)\n",
      "Epoch: 148 | Batch_idx: 150 |  Loss_1: (0.2556) | Acc_1: (90.16%) (17427/19328)\n",
      "Epoch: 148 | Batch_idx: 160 |  Loss_1: (0.2566) | Acc_1: (90.12%) (18572/20608)\n",
      "Epoch: 148 | Batch_idx: 170 |  Loss_1: (0.2556) | Acc_1: (90.13%) (19727/21888)\n",
      "Epoch: 148 | Batch_idx: 180 |  Loss_1: (0.2545) | Acc_1: (90.15%) (20885/23168)\n",
      "Epoch: 148 | Batch_idx: 190 |  Loss_1: (0.2582) | Acc_1: (90.01%) (22005/24448)\n",
      "Epoch: 148 | Batch_idx: 200 |  Loss_1: (0.2596) | Acc_1: (89.96%) (23146/25728)\n",
      "Epoch: 148 | Batch_idx: 210 |  Loss_1: (0.2592) | Acc_1: (89.98%) (24303/27008)\n",
      "Epoch: 148 | Batch_idx: 220 |  Loss_1: (0.2610) | Acc_1: (89.93%) (25439/28288)\n",
      "Epoch: 148 | Batch_idx: 230 |  Loss_1: (0.2605) | Acc_1: (89.95%) (26597/29568)\n",
      "Epoch: 148 | Batch_idx: 240 |  Loss_1: (0.2607) | Acc_1: (89.94%) (27745/30848)\n",
      "Epoch: 148 | Batch_idx: 250 |  Loss_1: (0.2614) | Acc_1: (89.93%) (28893/32128)\n",
      "Epoch: 148 | Batch_idx: 260 |  Loss_1: (0.2598) | Acc_1: (89.98%) (30062/33408)\n",
      "Epoch: 148 | Batch_idx: 270 |  Loss_1: (0.2609) | Acc_1: (89.94%) (31200/34688)\n",
      "Epoch: 148 | Batch_idx: 280 |  Loss_1: (0.2628) | Acc_1: (89.89%) (32332/35968)\n",
      "Epoch: 148 | Batch_idx: 290 |  Loss_1: (0.2629) | Acc_1: (89.90%) (33485/37248)\n",
      "Epoch: 148 | Batch_idx: 300 |  Loss_1: (0.2641) | Acc_1: (89.85%) (34617/38528)\n",
      "Epoch: 148 | Batch_idx: 310 |  Loss_1: (0.2645) | Acc_1: (89.84%) (35764/39808)\n",
      "Epoch: 148 | Batch_idx: 320 |  Loss_1: (0.2650) | Acc_1: (89.81%) (36903/41088)\n",
      "Epoch: 148 | Batch_idx: 330 |  Loss_1: (0.2647) | Acc_1: (89.82%) (38055/42368)\n",
      "Epoch: 148 | Batch_idx: 340 |  Loss_1: (0.2647) | Acc_1: (89.82%) (39204/43648)\n",
      "Epoch: 148 | Batch_idx: 350 |  Loss_1: (0.2649) | Acc_1: (89.81%) (40351/44928)\n",
      "Epoch: 148 | Batch_idx: 360 |  Loss_1: (0.2658) | Acc_1: (89.80%) (41493/46208)\n",
      "Epoch: 148 | Batch_idx: 370 |  Loss_1: (0.2651) | Acc_1: (89.82%) (42654/47488)\n",
      "Epoch: 148 | Batch_idx: 380 |  Loss_1: (0.2646) | Acc_1: (89.85%) (43817/48768)\n",
      "Epoch: 148 | Batch_idx: 390 |  Loss_1: (0.2648) | Acc_1: (89.86%) (44928/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2805) | Acc: (93.05%) (9305/10000)\n",
      "Epoch: 149 | Batch_idx: 0 |  Loss_1: (0.1484) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 149 | Batch_idx: 10 |  Loss_1: (0.2509) | Acc_1: (90.27%) (1271/1408)\n",
      "Epoch: 149 | Batch_idx: 20 |  Loss_1: (0.2510) | Acc_1: (90.33%) (2428/2688)\n",
      "Epoch: 149 | Batch_idx: 30 |  Loss_1: (0.2560) | Acc_1: (90.20%) (3579/3968)\n",
      "Epoch: 149 | Batch_idx: 40 |  Loss_1: (0.2561) | Acc_1: (90.30%) (4739/5248)\n",
      "Epoch: 149 | Batch_idx: 50 |  Loss_1: (0.2549) | Acc_1: (90.44%) (5904/6528)\n",
      "Epoch: 149 | Batch_idx: 60 |  Loss_1: (0.2571) | Acc_1: (90.30%) (7051/7808)\n",
      "Epoch: 149 | Batch_idx: 70 |  Loss_1: (0.2537) | Acc_1: (90.45%) (8220/9088)\n",
      "Epoch: 149 | Batch_idx: 80 |  Loss_1: (0.2483) | Acc_1: (90.59%) (9392/10368)\n",
      "Epoch: 149 | Batch_idx: 90 |  Loss_1: (0.2471) | Acc_1: (90.57%) (10550/11648)\n",
      "Epoch: 149 | Batch_idx: 100 |  Loss_1: (0.2465) | Acc_1: (90.59%) (11711/12928)\n",
      "Epoch: 149 | Batch_idx: 110 |  Loss_1: (0.2519) | Acc_1: (90.34%) (12836/14208)\n",
      "Epoch: 149 | Batch_idx: 120 |  Loss_1: (0.2542) | Acc_1: (90.27%) (13981/15488)\n",
      "Epoch: 149 | Batch_idx: 130 |  Loss_1: (0.2538) | Acc_1: (90.27%) (15137/16768)\n",
      "Epoch: 149 | Batch_idx: 140 |  Loss_1: (0.2565) | Acc_1: (90.18%) (16276/18048)\n",
      "Epoch: 149 | Batch_idx: 150 |  Loss_1: (0.2559) | Acc_1: (90.21%) (17435/19328)\n",
      "Epoch: 149 | Batch_idx: 160 |  Loss_1: (0.2557) | Acc_1: (90.23%) (18594/20608)\n",
      "Epoch: 149 | Batch_idx: 170 |  Loss_1: (0.2547) | Acc_1: (90.27%) (19759/21888)\n",
      "Epoch: 149 | Batch_idx: 180 |  Loss_1: (0.2539) | Acc_1: (90.31%) (20924/23168)\n",
      "Epoch: 149 | Batch_idx: 190 |  Loss_1: (0.2547) | Acc_1: (90.27%) (22070/24448)\n",
      "Epoch: 149 | Batch_idx: 200 |  Loss_1: (0.2557) | Acc_1: (90.25%) (23220/25728)\n",
      "Epoch: 149 | Batch_idx: 210 |  Loss_1: (0.2536) | Acc_1: (90.35%) (24401/27008)\n",
      "Epoch: 149 | Batch_idx: 220 |  Loss_1: (0.2544) | Acc_1: (90.31%) (25547/28288)\n",
      "Epoch: 149 | Batch_idx: 230 |  Loss_1: (0.2525) | Acc_1: (90.38%) (26723/29568)\n",
      "Epoch: 149 | Batch_idx: 240 |  Loss_1: (0.2547) | Acc_1: (90.29%) (27854/30848)\n",
      "Epoch: 149 | Batch_idx: 250 |  Loss_1: (0.2555) | Acc_1: (90.28%) (29006/32128)\n",
      "Epoch: 149 | Batch_idx: 260 |  Loss_1: (0.2543) | Acc_1: (90.31%) (30171/33408)\n",
      "Epoch: 149 | Batch_idx: 270 |  Loss_1: (0.2548) | Acc_1: (90.29%) (31319/34688)\n",
      "Epoch: 149 | Batch_idx: 280 |  Loss_1: (0.2546) | Acc_1: (90.30%) (32479/35968)\n",
      "Epoch: 149 | Batch_idx: 290 |  Loss_1: (0.2557) | Acc_1: (90.27%) (33623/37248)\n",
      "Epoch: 149 | Batch_idx: 300 |  Loss_1: (0.2562) | Acc_1: (90.25%) (34771/38528)\n",
      "Epoch: 149 | Batch_idx: 310 |  Loss_1: (0.2571) | Acc_1: (90.22%) (35914/39808)\n",
      "Epoch: 149 | Batch_idx: 320 |  Loss_1: (0.2577) | Acc_1: (90.18%) (37055/41088)\n",
      "Epoch: 149 | Batch_idx: 330 |  Loss_1: (0.2582) | Acc_1: (90.17%) (38203/42368)\n",
      "Epoch: 149 | Batch_idx: 340 |  Loss_1: (0.2587) | Acc_1: (90.15%) (39349/43648)\n",
      "Epoch: 149 | Batch_idx: 350 |  Loss_1: (0.2582) | Acc_1: (90.17%) (40511/44928)\n",
      "Epoch: 149 | Batch_idx: 360 |  Loss_1: (0.2583) | Acc_1: (90.16%) (41660/46208)\n",
      "Epoch: 149 | Batch_idx: 370 |  Loss_1: (0.2586) | Acc_1: (90.15%) (42812/47488)\n",
      "Epoch: 149 | Batch_idx: 380 |  Loss_1: (0.2590) | Acc_1: (90.13%) (43954/48768)\n",
      "Epoch: 149 | Batch_idx: 390 |  Loss_1: (0.2598) | Acc_1: (90.09%) (45044/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2822) | Acc: (93.13%) (9313/10000)\n",
      "Epoch: 150 | Batch_idx: 0 |  Loss_1: (0.2241) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 150 | Batch_idx: 10 |  Loss_1: (0.2533) | Acc_1: (90.27%) (1271/1408)\n",
      "Epoch: 150 | Batch_idx: 20 |  Loss_1: (0.2651) | Acc_1: (89.88%) (2416/2688)\n",
      "Epoch: 150 | Batch_idx: 30 |  Loss_1: (0.2639) | Acc_1: (89.99%) (3571/3968)\n",
      "Epoch: 150 | Batch_idx: 40 |  Loss_1: (0.2634) | Acc_1: (90.02%) (4724/5248)\n",
      "Epoch: 150 | Batch_idx: 50 |  Loss_1: (0.2698) | Acc_1: (89.69%) (5855/6528)\n",
      "Epoch: 150 | Batch_idx: 60 |  Loss_1: (0.2686) | Acc_1: (89.74%) (7007/7808)\n",
      "Epoch: 150 | Batch_idx: 70 |  Loss_1: (0.2658) | Acc_1: (89.81%) (8162/9088)\n",
      "Epoch: 150 | Batch_idx: 80 |  Loss_1: (0.2660) | Acc_1: (89.72%) (9302/10368)\n",
      "Epoch: 150 | Batch_idx: 90 |  Loss_1: (0.2693) | Acc_1: (89.59%) (10436/11648)\n",
      "Epoch: 150 | Batch_idx: 100 |  Loss_1: (0.2710) | Acc_1: (89.56%) (11578/12928)\n",
      "Epoch: 150 | Batch_idx: 110 |  Loss_1: (0.2693) | Acc_1: (89.66%) (12739/14208)\n",
      "Epoch: 150 | Batch_idx: 120 |  Loss_1: (0.2718) | Acc_1: (89.54%) (13868/15488)\n",
      "Epoch: 150 | Batch_idx: 130 |  Loss_1: (0.2697) | Acc_1: (89.61%) (15026/16768)\n",
      "Epoch: 150 | Batch_idx: 140 |  Loss_1: (0.2674) | Acc_1: (89.71%) (16191/18048)\n",
      "Epoch: 150 | Batch_idx: 150 |  Loss_1: (0.2665) | Acc_1: (89.72%) (17342/19328)\n",
      "Epoch: 150 | Batch_idx: 160 |  Loss_1: (0.2648) | Acc_1: (89.80%) (18505/20608)\n",
      "Epoch: 150 | Batch_idx: 170 |  Loss_1: (0.2641) | Acc_1: (89.82%) (19659/21888)\n",
      "Epoch: 150 | Batch_idx: 180 |  Loss_1: (0.2653) | Acc_1: (89.75%) (20794/23168)\n",
      "Epoch: 150 | Batch_idx: 190 |  Loss_1: (0.2647) | Acc_1: (89.79%) (21952/24448)\n",
      "Epoch: 150 | Batch_idx: 200 |  Loss_1: (0.2644) | Acc_1: (89.83%) (23112/25728)\n",
      "Epoch: 150 | Batch_idx: 210 |  Loss_1: (0.2644) | Acc_1: (89.81%) (24256/27008)\n",
      "Epoch: 150 | Batch_idx: 220 |  Loss_1: (0.2641) | Acc_1: (89.84%) (25414/28288)\n",
      "Epoch: 150 | Batch_idx: 230 |  Loss_1: (0.2652) | Acc_1: (89.83%) (26561/29568)\n",
      "Epoch: 150 | Batch_idx: 240 |  Loss_1: (0.2641) | Acc_1: (89.85%) (27717/30848)\n",
      "Epoch: 150 | Batch_idx: 250 |  Loss_1: (0.2636) | Acc_1: (89.87%) (28873/32128)\n",
      "Epoch: 150 | Batch_idx: 260 |  Loss_1: (0.2642) | Acc_1: (89.86%) (30022/33408)\n",
      "Epoch: 150 | Batch_idx: 270 |  Loss_1: (0.2645) | Acc_1: (89.85%) (31167/34688)\n",
      "Epoch: 150 | Batch_idx: 280 |  Loss_1: (0.2640) | Acc_1: (89.86%) (32322/35968)\n",
      "Epoch: 150 | Batch_idx: 290 |  Loss_1: (0.2635) | Acc_1: (89.88%) (33480/37248)\n",
      "Epoch: 150 | Batch_idx: 300 |  Loss_1: (0.2634) | Acc_1: (89.89%) (34633/38528)\n",
      "Epoch: 150 | Batch_idx: 310 |  Loss_1: (0.2631) | Acc_1: (89.89%) (35782/39808)\n",
      "Epoch: 150 | Batch_idx: 320 |  Loss_1: (0.2624) | Acc_1: (89.92%) (36946/41088)\n",
      "Epoch: 150 | Batch_idx: 330 |  Loss_1: (0.2605) | Acc_1: (89.99%) (38125/42368)\n",
      "Epoch: 150 | Batch_idx: 340 |  Loss_1: (0.2604) | Acc_1: (89.99%) (39280/43648)\n",
      "Epoch: 150 | Batch_idx: 350 |  Loss_1: (0.2598) | Acc_1: (90.00%) (40436/44928)\n",
      "Epoch: 150 | Batch_idx: 360 |  Loss_1: (0.2595) | Acc_1: (90.01%) (41594/46208)\n",
      "Epoch: 150 | Batch_idx: 370 |  Loss_1: (0.2588) | Acc_1: (90.04%) (42760/47488)\n",
      "Epoch: 150 | Batch_idx: 380 |  Loss_1: (0.2593) | Acc_1: (90.01%) (43898/48768)\n",
      "Epoch: 150 | Batch_idx: 390 |  Loss_1: (0.2584) | Acc_1: (90.05%) (45026/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2929) | Acc: (92.80%) (9280/10000)\n",
      "Epoch: 151 | Batch_idx: 0 |  Loss_1: (0.1748) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 151 | Batch_idx: 10 |  Loss_1: (0.2954) | Acc_1: (88.14%) (1241/1408)\n",
      "Epoch: 151 | Batch_idx: 20 |  Loss_1: (0.2813) | Acc_1: (88.95%) (2391/2688)\n",
      "Epoch: 151 | Batch_idx: 30 |  Loss_1: (0.2781) | Acc_1: (89.26%) (3542/3968)\n",
      "Epoch: 151 | Batch_idx: 40 |  Loss_1: (0.2719) | Acc_1: (89.42%) (4693/5248)\n",
      "Epoch: 151 | Batch_idx: 50 |  Loss_1: (0.2727) | Acc_1: (89.45%) (5839/6528)\n",
      "Epoch: 151 | Batch_idx: 60 |  Loss_1: (0.2681) | Acc_1: (89.61%) (6997/7808)\n",
      "Epoch: 151 | Batch_idx: 70 |  Loss_1: (0.2638) | Acc_1: (89.80%) (8161/9088)\n",
      "Epoch: 151 | Batch_idx: 80 |  Loss_1: (0.2639) | Acc_1: (89.83%) (9314/10368)\n",
      "Epoch: 151 | Batch_idx: 90 |  Loss_1: (0.2675) | Acc_1: (89.67%) (10445/11648)\n",
      "Epoch: 151 | Batch_idx: 100 |  Loss_1: (0.2660) | Acc_1: (89.75%) (11603/12928)\n",
      "Epoch: 151 | Batch_idx: 110 |  Loss_1: (0.2649) | Acc_1: (89.80%) (12759/14208)\n",
      "Epoch: 151 | Batch_idx: 120 |  Loss_1: (0.2626) | Acc_1: (89.89%) (13922/15488)\n",
      "Epoch: 151 | Batch_idx: 130 |  Loss_1: (0.2619) | Acc_1: (89.92%) (15078/16768)\n",
      "Epoch: 151 | Batch_idx: 140 |  Loss_1: (0.2606) | Acc_1: (89.98%) (16239/18048)\n",
      "Epoch: 151 | Batch_idx: 150 |  Loss_1: (0.2608) | Acc_1: (89.95%) (17386/19328)\n",
      "Epoch: 151 | Batch_idx: 160 |  Loss_1: (0.2600) | Acc_1: (90.02%) (18551/20608)\n",
      "Epoch: 151 | Batch_idx: 170 |  Loss_1: (0.2601) | Acc_1: (90.02%) (19703/21888)\n",
      "Epoch: 151 | Batch_idx: 180 |  Loss_1: (0.2605) | Acc_1: (90.00%) (20851/23168)\n",
      "Epoch: 151 | Batch_idx: 190 |  Loss_1: (0.2615) | Acc_1: (89.95%) (21990/24448)\n",
      "Epoch: 151 | Batch_idx: 200 |  Loss_1: (0.2627) | Acc_1: (89.88%) (23125/25728)\n",
      "Epoch: 151 | Batch_idx: 210 |  Loss_1: (0.2614) | Acc_1: (89.92%) (24285/27008)\n",
      "Epoch: 151 | Batch_idx: 220 |  Loss_1: (0.2610) | Acc_1: (89.91%) (25433/28288)\n",
      "Epoch: 151 | Batch_idx: 230 |  Loss_1: (0.2620) | Acc_1: (89.87%) (26572/29568)\n",
      "Epoch: 151 | Batch_idx: 240 |  Loss_1: (0.2609) | Acc_1: (89.92%) (27740/30848)\n",
      "Epoch: 151 | Batch_idx: 250 |  Loss_1: (0.2613) | Acc_1: (89.92%) (28890/32128)\n",
      "Epoch: 151 | Batch_idx: 260 |  Loss_1: (0.2623) | Acc_1: (89.87%) (30025/33408)\n",
      "Epoch: 151 | Batch_idx: 270 |  Loss_1: (0.2629) | Acc_1: (89.84%) (31165/34688)\n",
      "Epoch: 151 | Batch_idx: 280 |  Loss_1: (0.2627) | Acc_1: (89.86%) (32321/35968)\n",
      "Epoch: 151 | Batch_idx: 290 |  Loss_1: (0.2615) | Acc_1: (89.91%) (33488/37248)\n",
      "Epoch: 151 | Batch_idx: 300 |  Loss_1: (0.2610) | Acc_1: (89.92%) (34645/38528)\n",
      "Epoch: 151 | Batch_idx: 310 |  Loss_1: (0.2602) | Acc_1: (89.96%) (35810/39808)\n",
      "Epoch: 151 | Batch_idx: 320 |  Loss_1: (0.2601) | Acc_1: (89.97%) (36966/41088)\n",
      "Epoch: 151 | Batch_idx: 330 |  Loss_1: (0.2601) | Acc_1: (89.97%) (38119/42368)\n",
      "Epoch: 151 | Batch_idx: 340 |  Loss_1: (0.2604) | Acc_1: (89.96%) (39264/43648)\n",
      "Epoch: 151 | Batch_idx: 350 |  Loss_1: (0.2598) | Acc_1: (89.99%) (40432/44928)\n",
      "Epoch: 151 | Batch_idx: 360 |  Loss_1: (0.2600) | Acc_1: (89.99%) (41582/46208)\n",
      "Epoch: 151 | Batch_idx: 370 |  Loss_1: (0.2605) | Acc_1: (89.97%) (42723/47488)\n",
      "Epoch: 151 | Batch_idx: 380 |  Loss_1: (0.2605) | Acc_1: (89.96%) (43874/48768)\n",
      "Epoch: 151 | Batch_idx: 390 |  Loss_1: (0.2606) | Acc_1: (89.95%) (44977/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3205) | Acc: (92.51%) (9251/10000)\n",
      "Epoch: 152 | Batch_idx: 0 |  Loss_1: (0.2246) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 152 | Batch_idx: 10 |  Loss_1: (0.2500) | Acc_1: (90.34%) (1272/1408)\n",
      "Epoch: 152 | Batch_idx: 20 |  Loss_1: (0.2551) | Acc_1: (90.14%) (2423/2688)\n",
      "Epoch: 152 | Batch_idx: 30 |  Loss_1: (0.2437) | Acc_1: (90.55%) (3593/3968)\n",
      "Epoch: 152 | Batch_idx: 40 |  Loss_1: (0.2426) | Acc_1: (90.70%) (4760/5248)\n",
      "Epoch: 152 | Batch_idx: 50 |  Loss_1: (0.2437) | Acc_1: (90.73%) (5923/6528)\n",
      "Epoch: 152 | Batch_idx: 60 |  Loss_1: (0.2504) | Acc_1: (90.43%) (7061/7808)\n",
      "Epoch: 152 | Batch_idx: 70 |  Loss_1: (0.2514) | Acc_1: (90.44%) (8219/9088)\n",
      "Epoch: 152 | Batch_idx: 80 |  Loss_1: (0.2491) | Acc_1: (90.55%) (9388/10368)\n",
      "Epoch: 152 | Batch_idx: 90 |  Loss_1: (0.2509) | Acc_1: (90.44%) (10534/11648)\n",
      "Epoch: 152 | Batch_idx: 100 |  Loss_1: (0.2517) | Acc_1: (90.42%) (11689/12928)\n",
      "Epoch: 152 | Batch_idx: 110 |  Loss_1: (0.2498) | Acc_1: (90.48%) (12855/14208)\n",
      "Epoch: 152 | Batch_idx: 120 |  Loss_1: (0.2502) | Acc_1: (90.47%) (14012/15488)\n",
      "Epoch: 152 | Batch_idx: 130 |  Loss_1: (0.2490) | Acc_1: (90.46%) (15169/16768)\n",
      "Epoch: 152 | Batch_idx: 140 |  Loss_1: (0.2504) | Acc_1: (90.38%) (16312/18048)\n",
      "Epoch: 152 | Batch_idx: 150 |  Loss_1: (0.2520) | Acc_1: (90.29%) (17452/19328)\n",
      "Epoch: 152 | Batch_idx: 160 |  Loss_1: (0.2522) | Acc_1: (90.26%) (18600/20608)\n",
      "Epoch: 152 | Batch_idx: 170 |  Loss_1: (0.2519) | Acc_1: (90.27%) (19758/21888)\n",
      "Epoch: 152 | Batch_idx: 180 |  Loss_1: (0.2528) | Acc_1: (90.22%) (20903/23168)\n",
      "Epoch: 152 | Batch_idx: 190 |  Loss_1: (0.2533) | Acc_1: (90.20%) (22053/24448)\n",
      "Epoch: 152 | Batch_idx: 200 |  Loss_1: (0.2543) | Acc_1: (90.21%) (23209/25728)\n",
      "Epoch: 152 | Batch_idx: 210 |  Loss_1: (0.2553) | Acc_1: (90.18%) (24356/27008)\n",
      "Epoch: 152 | Batch_idx: 220 |  Loss_1: (0.2563) | Acc_1: (90.13%) (25496/28288)\n",
      "Epoch: 152 | Batch_idx: 230 |  Loss_1: (0.2564) | Acc_1: (90.13%) (26650/29568)\n",
      "Epoch: 152 | Batch_idx: 240 |  Loss_1: (0.2564) | Acc_1: (90.15%) (27808/30848)\n",
      "Epoch: 152 | Batch_idx: 250 |  Loss_1: (0.2566) | Acc_1: (90.14%) (28961/32128)\n",
      "Epoch: 152 | Batch_idx: 260 |  Loss_1: (0.2571) | Acc_1: (90.12%) (30108/33408)\n",
      "Epoch: 152 | Batch_idx: 270 |  Loss_1: (0.2579) | Acc_1: (90.09%) (31250/34688)\n",
      "Epoch: 152 | Batch_idx: 280 |  Loss_1: (0.2573) | Acc_1: (90.11%) (32412/35968)\n",
      "Epoch: 152 | Batch_idx: 290 |  Loss_1: (0.2565) | Acc_1: (90.14%) (33576/37248)\n",
      "Epoch: 152 | Batch_idx: 300 |  Loss_1: (0.2573) | Acc_1: (90.11%) (34716/38528)\n",
      "Epoch: 152 | Batch_idx: 310 |  Loss_1: (0.2582) | Acc_1: (90.06%) (35853/39808)\n",
      "Epoch: 152 | Batch_idx: 320 |  Loss_1: (0.2604) | Acc_1: (89.98%) (36973/41088)\n",
      "Epoch: 152 | Batch_idx: 330 |  Loss_1: (0.2597) | Acc_1: (90.00%) (38132/42368)\n",
      "Epoch: 152 | Batch_idx: 340 |  Loss_1: (0.2599) | Acc_1: (90.00%) (39285/43648)\n",
      "Epoch: 152 | Batch_idx: 350 |  Loss_1: (0.2595) | Acc_1: (90.03%) (40448/44928)\n",
      "Epoch: 152 | Batch_idx: 360 |  Loss_1: (0.2582) | Acc_1: (90.08%) (41624/46208)\n",
      "Epoch: 152 | Batch_idx: 370 |  Loss_1: (0.2580) | Acc_1: (90.09%) (42784/47488)\n",
      "Epoch: 152 | Batch_idx: 380 |  Loss_1: (0.2576) | Acc_1: (90.09%) (43936/48768)\n",
      "Epoch: 152 | Batch_idx: 390 |  Loss_1: (0.2577) | Acc_1: (90.10%) (45048/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3054) | Acc: (92.64%) (9264/10000)\n",
      "Epoch: 153 | Batch_idx: 0 |  Loss_1: (0.2219) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 153 | Batch_idx: 10 |  Loss_1: (0.2617) | Acc_1: (90.20%) (1270/1408)\n",
      "Epoch: 153 | Batch_idx: 20 |  Loss_1: (0.2492) | Acc_1: (90.81%) (2441/2688)\n",
      "Epoch: 153 | Batch_idx: 30 |  Loss_1: (0.2393) | Acc_1: (91.15%) (3617/3968)\n",
      "Epoch: 153 | Batch_idx: 40 |  Loss_1: (0.2452) | Acc_1: (90.83%) (4767/5248)\n",
      "Epoch: 153 | Batch_idx: 50 |  Loss_1: (0.2493) | Acc_1: (90.64%) (5917/6528)\n",
      "Epoch: 153 | Batch_idx: 60 |  Loss_1: (0.2472) | Acc_1: (90.61%) (7075/7808)\n",
      "Epoch: 153 | Batch_idx: 70 |  Loss_1: (0.2491) | Acc_1: (90.51%) (8226/9088)\n",
      "Epoch: 153 | Batch_idx: 80 |  Loss_1: (0.2503) | Acc_1: (90.50%) (9383/10368)\n",
      "Epoch: 153 | Batch_idx: 90 |  Loss_1: (0.2492) | Acc_1: (90.50%) (10541/11648)\n",
      "Epoch: 153 | Batch_idx: 100 |  Loss_1: (0.2505) | Acc_1: (90.39%) (11685/12928)\n",
      "Epoch: 153 | Batch_idx: 110 |  Loss_1: (0.2498) | Acc_1: (90.41%) (12846/14208)\n",
      "Epoch: 153 | Batch_idx: 120 |  Loss_1: (0.2490) | Acc_1: (90.41%) (14003/15488)\n",
      "Epoch: 153 | Batch_idx: 130 |  Loss_1: (0.2493) | Acc_1: (90.40%) (15158/16768)\n",
      "Epoch: 153 | Batch_idx: 140 |  Loss_1: (0.2520) | Acc_1: (90.27%) (16292/18048)\n",
      "Epoch: 153 | Batch_idx: 150 |  Loss_1: (0.2531) | Acc_1: (90.21%) (17436/19328)\n",
      "Epoch: 153 | Batch_idx: 160 |  Loss_1: (0.2545) | Acc_1: (90.16%) (18580/20608)\n",
      "Epoch: 153 | Batch_idx: 170 |  Loss_1: (0.2561) | Acc_1: (90.17%) (19736/21888)\n",
      "Epoch: 153 | Batch_idx: 180 |  Loss_1: (0.2571) | Acc_1: (90.13%) (20882/23168)\n",
      "Epoch: 153 | Batch_idx: 190 |  Loss_1: (0.2570) | Acc_1: (90.18%) (22046/24448)\n",
      "Epoch: 153 | Batch_idx: 200 |  Loss_1: (0.2581) | Acc_1: (90.14%) (23191/25728)\n",
      "Epoch: 153 | Batch_idx: 210 |  Loss_1: (0.2588) | Acc_1: (90.11%) (24336/27008)\n",
      "Epoch: 153 | Batch_idx: 220 |  Loss_1: (0.2582) | Acc_1: (90.13%) (25495/28288)\n",
      "Epoch: 153 | Batch_idx: 230 |  Loss_1: (0.2595) | Acc_1: (90.07%) (26632/29568)\n",
      "Epoch: 153 | Batch_idx: 240 |  Loss_1: (0.2589) | Acc_1: (90.09%) (27790/30848)\n",
      "Epoch: 153 | Batch_idx: 250 |  Loss_1: (0.2594) | Acc_1: (90.07%) (28937/32128)\n",
      "Epoch: 153 | Batch_idx: 260 |  Loss_1: (0.2611) | Acc_1: (90.02%) (30073/33408)\n",
      "Epoch: 153 | Batch_idx: 270 |  Loss_1: (0.2606) | Acc_1: (90.04%) (31232/34688)\n",
      "Epoch: 153 | Batch_idx: 280 |  Loss_1: (0.2609) | Acc_1: (90.02%) (32378/35968)\n",
      "Epoch: 153 | Batch_idx: 290 |  Loss_1: (0.2612) | Acc_1: (89.98%) (33516/37248)\n",
      "Epoch: 153 | Batch_idx: 300 |  Loss_1: (0.2606) | Acc_1: (89.98%) (34669/38528)\n",
      "Epoch: 153 | Batch_idx: 310 |  Loss_1: (0.2593) | Acc_1: (90.03%) (35841/39808)\n",
      "Epoch: 153 | Batch_idx: 320 |  Loss_1: (0.2587) | Acc_1: (90.06%) (37002/41088)\n",
      "Epoch: 153 | Batch_idx: 330 |  Loss_1: (0.2581) | Acc_1: (90.08%) (38164/42368)\n",
      "Epoch: 153 | Batch_idx: 340 |  Loss_1: (0.2577) | Acc_1: (90.08%) (39317/43648)\n",
      "Epoch: 153 | Batch_idx: 350 |  Loss_1: (0.2567) | Acc_1: (90.11%) (40483/44928)\n",
      "Epoch: 153 | Batch_idx: 360 |  Loss_1: (0.2562) | Acc_1: (90.11%) (41639/46208)\n",
      "Epoch: 153 | Batch_idx: 370 |  Loss_1: (0.2556) | Acc_1: (90.13%) (42801/47488)\n",
      "Epoch: 153 | Batch_idx: 380 |  Loss_1: (0.2557) | Acc_1: (90.13%) (43956/48768)\n",
      "Epoch: 153 | Batch_idx: 390 |  Loss_1: (0.2557) | Acc_1: (90.12%) (45061/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3467) | Acc: (92.28%) (9228/10000)\n",
      "Epoch: 154 | Batch_idx: 0 |  Loss_1: (0.2400) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 154 | Batch_idx: 10 |  Loss_1: (0.2290) | Acc_1: (91.12%) (1283/1408)\n",
      "Epoch: 154 | Batch_idx: 20 |  Loss_1: (0.2436) | Acc_1: (90.59%) (2435/2688)\n",
      "Epoch: 154 | Batch_idx: 30 |  Loss_1: (0.2563) | Acc_1: (90.22%) (3580/3968)\n",
      "Epoch: 154 | Batch_idx: 40 |  Loss_1: (0.2566) | Acc_1: (90.21%) (4734/5248)\n",
      "Epoch: 154 | Batch_idx: 50 |  Loss_1: (0.2560) | Acc_1: (90.30%) (5895/6528)\n",
      "Epoch: 154 | Batch_idx: 60 |  Loss_1: (0.2535) | Acc_1: (90.34%) (7054/7808)\n",
      "Epoch: 154 | Batch_idx: 70 |  Loss_1: (0.2527) | Acc_1: (90.40%) (8216/9088)\n",
      "Epoch: 154 | Batch_idx: 80 |  Loss_1: (0.2500) | Acc_1: (90.44%) (9377/10368)\n",
      "Epoch: 154 | Batch_idx: 90 |  Loss_1: (0.2517) | Acc_1: (90.42%) (10532/11648)\n",
      "Epoch: 154 | Batch_idx: 100 |  Loss_1: (0.2522) | Acc_1: (90.39%) (11686/12928)\n",
      "Epoch: 154 | Batch_idx: 110 |  Loss_1: (0.2532) | Acc_1: (90.36%) (12838/14208)\n",
      "Epoch: 154 | Batch_idx: 120 |  Loss_1: (0.2524) | Acc_1: (90.39%) (13999/15488)\n",
      "Epoch: 154 | Batch_idx: 130 |  Loss_1: (0.2534) | Acc_1: (90.35%) (15150/16768)\n",
      "Epoch: 154 | Batch_idx: 140 |  Loss_1: (0.2532) | Acc_1: (90.33%) (16302/18048)\n",
      "Epoch: 154 | Batch_idx: 150 |  Loss_1: (0.2522) | Acc_1: (90.36%) (17464/19328)\n",
      "Epoch: 154 | Batch_idx: 160 |  Loss_1: (0.2529) | Acc_1: (90.35%) (18619/20608)\n",
      "Epoch: 154 | Batch_idx: 170 |  Loss_1: (0.2530) | Acc_1: (90.33%) (19771/21888)\n",
      "Epoch: 154 | Batch_idx: 180 |  Loss_1: (0.2527) | Acc_1: (90.37%) (20936/23168)\n",
      "Epoch: 154 | Batch_idx: 190 |  Loss_1: (0.2549) | Acc_1: (90.24%) (22063/24448)\n",
      "Epoch: 154 | Batch_idx: 200 |  Loss_1: (0.2561) | Acc_1: (90.19%) (23203/25728)\n",
      "Epoch: 154 | Batch_idx: 210 |  Loss_1: (0.2565) | Acc_1: (90.18%) (24355/27008)\n",
      "Epoch: 154 | Batch_idx: 220 |  Loss_1: (0.2553) | Acc_1: (90.21%) (25518/28288)\n",
      "Epoch: 154 | Batch_idx: 230 |  Loss_1: (0.2572) | Acc_1: (90.11%) (26645/29568)\n",
      "Epoch: 154 | Batch_idx: 240 |  Loss_1: (0.2573) | Acc_1: (90.11%) (27798/30848)\n",
      "Epoch: 154 | Batch_idx: 250 |  Loss_1: (0.2558) | Acc_1: (90.18%) (28974/32128)\n",
      "Epoch: 154 | Batch_idx: 260 |  Loss_1: (0.2576) | Acc_1: (90.14%) (30113/33408)\n",
      "Epoch: 154 | Batch_idx: 270 |  Loss_1: (0.2564) | Acc_1: (90.19%) (31285/34688)\n",
      "Epoch: 154 | Batch_idx: 280 |  Loss_1: (0.2573) | Acc_1: (90.16%) (32427/35968)\n",
      "Epoch: 154 | Batch_idx: 290 |  Loss_1: (0.2565) | Acc_1: (90.19%) (33593/37248)\n",
      "Epoch: 154 | Batch_idx: 300 |  Loss_1: (0.2572) | Acc_1: (90.15%) (34732/38528)\n",
      "Epoch: 154 | Batch_idx: 310 |  Loss_1: (0.2562) | Acc_1: (90.18%) (35897/39808)\n",
      "Epoch: 154 | Batch_idx: 320 |  Loss_1: (0.2569) | Acc_1: (90.16%) (37043/41088)\n",
      "Epoch: 154 | Batch_idx: 330 |  Loss_1: (0.2566) | Acc_1: (90.15%) (38195/42368)\n",
      "Epoch: 154 | Batch_idx: 340 |  Loss_1: (0.2555) | Acc_1: (90.19%) (39365/43648)\n",
      "Epoch: 154 | Batch_idx: 350 |  Loss_1: (0.2551) | Acc_1: (90.20%) (40524/44928)\n",
      "Epoch: 154 | Batch_idx: 360 |  Loss_1: (0.2555) | Acc_1: (90.18%) (41670/46208)\n",
      "Epoch: 154 | Batch_idx: 370 |  Loss_1: (0.2548) | Acc_1: (90.20%) (42833/47488)\n",
      "Epoch: 154 | Batch_idx: 380 |  Loss_1: (0.2539) | Acc_1: (90.23%) (44001/48768)\n",
      "Epoch: 154 | Batch_idx: 390 |  Loss_1: (0.2532) | Acc_1: (90.25%) (45127/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2968) | Acc: (92.76%) (9276/10000)\n",
      "Epoch: 155 | Batch_idx: 0 |  Loss_1: (0.2407) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 155 | Batch_idx: 10 |  Loss_1: (0.2467) | Acc_1: (90.34%) (1272/1408)\n",
      "Epoch: 155 | Batch_idx: 20 |  Loss_1: (0.2555) | Acc_1: (89.99%) (2419/2688)\n",
      "Epoch: 155 | Batch_idx: 30 |  Loss_1: (0.2573) | Acc_1: (90.12%) (3576/3968)\n",
      "Epoch: 155 | Batch_idx: 40 |  Loss_1: (0.2562) | Acc_1: (90.11%) (4729/5248)\n",
      "Epoch: 155 | Batch_idx: 50 |  Loss_1: (0.2534) | Acc_1: (90.23%) (5890/6528)\n",
      "Epoch: 155 | Batch_idx: 60 |  Loss_1: (0.2559) | Acc_1: (90.05%) (7031/7808)\n",
      "Epoch: 155 | Batch_idx: 70 |  Loss_1: (0.2594) | Acc_1: (89.95%) (8175/9088)\n",
      "Epoch: 155 | Batch_idx: 80 |  Loss_1: (0.2576) | Acc_1: (90.08%) (9339/10368)\n",
      "Epoch: 155 | Batch_idx: 90 |  Loss_1: (0.2558) | Acc_1: (90.18%) (10504/11648)\n",
      "Epoch: 155 | Batch_idx: 100 |  Loss_1: (0.2554) | Acc_1: (90.19%) (11660/12928)\n",
      "Epoch: 155 | Batch_idx: 110 |  Loss_1: (0.2581) | Acc_1: (90.12%) (12804/14208)\n",
      "Epoch: 155 | Batch_idx: 120 |  Loss_1: (0.2593) | Acc_1: (90.05%) (13947/15488)\n",
      "Epoch: 155 | Batch_idx: 130 |  Loss_1: (0.2568) | Acc_1: (90.17%) (15119/16768)\n",
      "Epoch: 155 | Batch_idx: 140 |  Loss_1: (0.2575) | Acc_1: (90.15%) (16270/18048)\n",
      "Epoch: 155 | Batch_idx: 150 |  Loss_1: (0.2571) | Acc_1: (90.12%) (17419/19328)\n",
      "Epoch: 155 | Batch_idx: 160 |  Loss_1: (0.2578) | Acc_1: (90.09%) (18566/20608)\n",
      "Epoch: 155 | Batch_idx: 170 |  Loss_1: (0.2556) | Acc_1: (90.18%) (19739/21888)\n",
      "Epoch: 155 | Batch_idx: 180 |  Loss_1: (0.2539) | Acc_1: (90.24%) (20907/23168)\n",
      "Epoch: 155 | Batch_idx: 190 |  Loss_1: (0.2534) | Acc_1: (90.25%) (22065/24448)\n",
      "Epoch: 155 | Batch_idx: 200 |  Loss_1: (0.2529) | Acc_1: (90.26%) (23222/25728)\n",
      "Epoch: 155 | Batch_idx: 210 |  Loss_1: (0.2538) | Acc_1: (90.23%) (24369/27008)\n",
      "Epoch: 155 | Batch_idx: 220 |  Loss_1: (0.2552) | Acc_1: (90.18%) (25510/28288)\n",
      "Epoch: 155 | Batch_idx: 230 |  Loss_1: (0.2556) | Acc_1: (90.15%) (26657/29568)\n",
      "Epoch: 155 | Batch_idx: 240 |  Loss_1: (0.2552) | Acc_1: (90.17%) (27817/30848)\n",
      "Epoch: 155 | Batch_idx: 250 |  Loss_1: (0.2569) | Acc_1: (90.12%) (28955/32128)\n",
      "Epoch: 155 | Batch_idx: 260 |  Loss_1: (0.2577) | Acc_1: (90.10%) (30100/33408)\n",
      "Epoch: 155 | Batch_idx: 270 |  Loss_1: (0.2587) | Acc_1: (90.06%) (31240/34688)\n",
      "Epoch: 155 | Batch_idx: 280 |  Loss_1: (0.2585) | Acc_1: (90.09%) (32404/35968)\n",
      "Epoch: 155 | Batch_idx: 290 |  Loss_1: (0.2585) | Acc_1: (90.09%) (33558/37248)\n",
      "Epoch: 155 | Batch_idx: 300 |  Loss_1: (0.2577) | Acc_1: (90.13%) (34724/38528)\n",
      "Epoch: 155 | Batch_idx: 310 |  Loss_1: (0.2573) | Acc_1: (90.16%) (35889/39808)\n",
      "Epoch: 155 | Batch_idx: 320 |  Loss_1: (0.2577) | Acc_1: (90.14%) (37036/41088)\n",
      "Epoch: 155 | Batch_idx: 330 |  Loss_1: (0.2575) | Acc_1: (90.15%) (38196/42368)\n",
      "Epoch: 155 | Batch_idx: 340 |  Loss_1: (0.2566) | Acc_1: (90.20%) (39370/43648)\n",
      "Epoch: 155 | Batch_idx: 350 |  Loss_1: (0.2562) | Acc_1: (90.21%) (40528/44928)\n",
      "Epoch: 155 | Batch_idx: 360 |  Loss_1: (0.2563) | Acc_1: (90.19%) (41674/46208)\n",
      "Epoch: 155 | Batch_idx: 370 |  Loss_1: (0.2566) | Acc_1: (90.18%) (42824/47488)\n",
      "Epoch: 155 | Batch_idx: 380 |  Loss_1: (0.2568) | Acc_1: (90.17%) (43972/48768)\n",
      "Epoch: 155 | Batch_idx: 390 |  Loss_1: (0.2567) | Acc_1: (90.18%) (45091/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3021) | Acc: (92.91%) (9291/10000)\n",
      "Epoch: 156 | Batch_idx: 0 |  Loss_1: (0.2954) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 156 | Batch_idx: 10 |  Loss_1: (0.2452) | Acc_1: (90.55%) (1275/1408)\n",
      "Epoch: 156 | Batch_idx: 20 |  Loss_1: (0.2623) | Acc_1: (89.96%) (2418/2688)\n",
      "Epoch: 156 | Batch_idx: 30 |  Loss_1: (0.2630) | Acc_1: (89.84%) (3565/3968)\n",
      "Epoch: 156 | Batch_idx: 40 |  Loss_1: (0.2671) | Acc_1: (89.81%) (4713/5248)\n",
      "Epoch: 156 | Batch_idx: 50 |  Loss_1: (0.2681) | Acc_1: (89.86%) (5866/6528)\n",
      "Epoch: 156 | Batch_idx: 60 |  Loss_1: (0.2670) | Acc_1: (89.84%) (7015/7808)\n",
      "Epoch: 156 | Batch_idx: 70 |  Loss_1: (0.2690) | Acc_1: (89.77%) (8158/9088)\n",
      "Epoch: 156 | Batch_idx: 80 |  Loss_1: (0.2663) | Acc_1: (89.81%) (9312/10368)\n",
      "Epoch: 156 | Batch_idx: 90 |  Loss_1: (0.2653) | Acc_1: (89.83%) (10463/11648)\n",
      "Epoch: 156 | Batch_idx: 100 |  Loss_1: (0.2640) | Acc_1: (89.84%) (11614/12928)\n",
      "Epoch: 156 | Batch_idx: 110 |  Loss_1: (0.2653) | Acc_1: (89.85%) (12766/14208)\n",
      "Epoch: 156 | Batch_idx: 120 |  Loss_1: (0.2657) | Acc_1: (89.85%) (13916/15488)\n",
      "Epoch: 156 | Batch_idx: 130 |  Loss_1: (0.2617) | Acc_1: (90.01%) (15093/16768)\n",
      "Epoch: 156 | Batch_idx: 140 |  Loss_1: (0.2621) | Acc_1: (90.02%) (16247/18048)\n",
      "Epoch: 156 | Batch_idx: 150 |  Loss_1: (0.2609) | Acc_1: (90.08%) (17410/19328)\n",
      "Epoch: 156 | Batch_idx: 160 |  Loss_1: (0.2600) | Acc_1: (90.13%) (18573/20608)\n",
      "Epoch: 156 | Batch_idx: 170 |  Loss_1: (0.2617) | Acc_1: (90.05%) (19710/21888)\n",
      "Epoch: 156 | Batch_idx: 180 |  Loss_1: (0.2605) | Acc_1: (90.08%) (20869/23168)\n",
      "Epoch: 156 | Batch_idx: 190 |  Loss_1: (0.2609) | Acc_1: (90.06%) (22018/24448)\n",
      "Epoch: 156 | Batch_idx: 200 |  Loss_1: (0.2604) | Acc_1: (90.07%) (23173/25728)\n",
      "Epoch: 156 | Batch_idx: 210 |  Loss_1: (0.2605) | Acc_1: (90.05%) (24322/27008)\n",
      "Epoch: 156 | Batch_idx: 220 |  Loss_1: (0.2595) | Acc_1: (90.09%) (25484/28288)\n",
      "Epoch: 156 | Batch_idx: 230 |  Loss_1: (0.2596) | Acc_1: (90.08%) (26636/29568)\n",
      "Epoch: 156 | Batch_idx: 240 |  Loss_1: (0.2581) | Acc_1: (90.14%) (27807/30848)\n",
      "Epoch: 156 | Batch_idx: 250 |  Loss_1: (0.2566) | Acc_1: (90.18%) (28973/32128)\n",
      "Epoch: 156 | Batch_idx: 260 |  Loss_1: (0.2553) | Acc_1: (90.24%) (30149/33408)\n",
      "Epoch: 156 | Batch_idx: 270 |  Loss_1: (0.2549) | Acc_1: (90.25%) (31307/34688)\n",
      "Epoch: 156 | Batch_idx: 280 |  Loss_1: (0.2560) | Acc_1: (90.21%) (32445/35968)\n",
      "Epoch: 156 | Batch_idx: 290 |  Loss_1: (0.2571) | Acc_1: (90.16%) (33584/37248)\n",
      "Epoch: 156 | Batch_idx: 300 |  Loss_1: (0.2576) | Acc_1: (90.13%) (34727/38528)\n",
      "Epoch: 156 | Batch_idx: 310 |  Loss_1: (0.2572) | Acc_1: (90.16%) (35889/39808)\n",
      "Epoch: 156 | Batch_idx: 320 |  Loss_1: (0.2571) | Acc_1: (90.15%) (37041/41088)\n",
      "Epoch: 156 | Batch_idx: 330 |  Loss_1: (0.2569) | Acc_1: (90.15%) (38196/42368)\n",
      "Epoch: 156 | Batch_idx: 340 |  Loss_1: (0.2574) | Acc_1: (90.13%) (39342/43648)\n",
      "Epoch: 156 | Batch_idx: 350 |  Loss_1: (0.2566) | Acc_1: (90.17%) (40513/44928)\n",
      "Epoch: 156 | Batch_idx: 360 |  Loss_1: (0.2560) | Acc_1: (90.21%) (41684/46208)\n",
      "Epoch: 156 | Batch_idx: 370 |  Loss_1: (0.2558) | Acc_1: (90.22%) (42844/47488)\n",
      "Epoch: 156 | Batch_idx: 380 |  Loss_1: (0.2558) | Acc_1: (90.22%) (43997/48768)\n",
      "Epoch: 156 | Batch_idx: 390 |  Loss_1: (0.2556) | Acc_1: (90.23%) (45114/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3007) | Acc: (93.11%) (9311/10000)\n",
      "Epoch: 157 | Batch_idx: 0 |  Loss_1: (0.2707) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 157 | Batch_idx: 10 |  Loss_1: (0.2335) | Acc_1: (91.12%) (1283/1408)\n",
      "Epoch: 157 | Batch_idx: 20 |  Loss_1: (0.2256) | Acc_1: (91.33%) (2455/2688)\n",
      "Epoch: 157 | Batch_idx: 30 |  Loss_1: (0.2450) | Acc_1: (90.45%) (3589/3968)\n",
      "Epoch: 157 | Batch_idx: 40 |  Loss_1: (0.2552) | Acc_1: (90.13%) (4730/5248)\n",
      "Epoch: 157 | Batch_idx: 50 |  Loss_1: (0.2522) | Acc_1: (90.12%) (5883/6528)\n",
      "Epoch: 157 | Batch_idx: 60 |  Loss_1: (0.2515) | Acc_1: (90.19%) (7042/7808)\n",
      "Epoch: 157 | Batch_idx: 70 |  Loss_1: (0.2600) | Acc_1: (89.85%) (8166/9088)\n",
      "Epoch: 157 | Batch_idx: 80 |  Loss_1: (0.2649) | Acc_1: (89.66%) (9296/10368)\n",
      "Epoch: 157 | Batch_idx: 90 |  Loss_1: (0.2641) | Acc_1: (89.71%) (10449/11648)\n",
      "Epoch: 157 | Batch_idx: 100 |  Loss_1: (0.2636) | Acc_1: (89.67%) (11593/12928)\n",
      "Epoch: 157 | Batch_idx: 110 |  Loss_1: (0.2638) | Acc_1: (89.67%) (12740/14208)\n",
      "Epoch: 157 | Batch_idx: 120 |  Loss_1: (0.2619) | Acc_1: (89.73%) (13897/15488)\n",
      "Epoch: 157 | Batch_idx: 130 |  Loss_1: (0.2621) | Acc_1: (89.76%) (15051/16768)\n",
      "Epoch: 157 | Batch_idx: 140 |  Loss_1: (0.2625) | Acc_1: (89.76%) (16199/18048)\n",
      "Epoch: 157 | Batch_idx: 150 |  Loss_1: (0.2652) | Acc_1: (89.69%) (17335/19328)\n",
      "Epoch: 157 | Batch_idx: 160 |  Loss_1: (0.2655) | Acc_1: (89.68%) (18482/20608)\n",
      "Epoch: 157 | Batch_idx: 170 |  Loss_1: (0.2626) | Acc_1: (89.79%) (19654/21888)\n",
      "Epoch: 157 | Batch_idx: 180 |  Loss_1: (0.2619) | Acc_1: (89.80%) (20805/23168)\n",
      "Epoch: 157 | Batch_idx: 190 |  Loss_1: (0.2603) | Acc_1: (89.88%) (21974/24448)\n",
      "Epoch: 157 | Batch_idx: 200 |  Loss_1: (0.2610) | Acc_1: (89.87%) (23122/25728)\n",
      "Epoch: 157 | Batch_idx: 210 |  Loss_1: (0.2609) | Acc_1: (89.84%) (24265/27008)\n",
      "Epoch: 157 | Batch_idx: 220 |  Loss_1: (0.2602) | Acc_1: (89.87%) (25423/28288)\n",
      "Epoch: 157 | Batch_idx: 230 |  Loss_1: (0.2607) | Acc_1: (89.85%) (26566/29568)\n",
      "Epoch: 157 | Batch_idx: 240 |  Loss_1: (0.2603) | Acc_1: (89.87%) (27722/30848)\n",
      "Epoch: 157 | Batch_idx: 250 |  Loss_1: (0.2585) | Acc_1: (89.93%) (28894/32128)\n",
      "Epoch: 157 | Batch_idx: 260 |  Loss_1: (0.2584) | Acc_1: (89.95%) (30051/33408)\n",
      "Epoch: 157 | Batch_idx: 270 |  Loss_1: (0.2589) | Acc_1: (89.95%) (31201/34688)\n",
      "Epoch: 157 | Batch_idx: 280 |  Loss_1: (0.2591) | Acc_1: (89.94%) (32350/35968)\n",
      "Epoch: 157 | Batch_idx: 290 |  Loss_1: (0.2593) | Acc_1: (89.93%) (33496/37248)\n",
      "Epoch: 157 | Batch_idx: 300 |  Loss_1: (0.2606) | Acc_1: (89.88%) (34629/38528)\n",
      "Epoch: 157 | Batch_idx: 310 |  Loss_1: (0.2604) | Acc_1: (89.89%) (35785/39808)\n",
      "Epoch: 157 | Batch_idx: 320 |  Loss_1: (0.2603) | Acc_1: (89.91%) (36941/41088)\n",
      "Epoch: 157 | Batch_idx: 330 |  Loss_1: (0.2596) | Acc_1: (89.96%) (38113/42368)\n",
      "Epoch: 157 | Batch_idx: 340 |  Loss_1: (0.2602) | Acc_1: (89.94%) (39256/43648)\n",
      "Epoch: 157 | Batch_idx: 350 |  Loss_1: (0.2600) | Acc_1: (89.94%) (40408/44928)\n",
      "Epoch: 157 | Batch_idx: 360 |  Loss_1: (0.2592) | Acc_1: (89.96%) (41570/46208)\n",
      "Epoch: 157 | Batch_idx: 370 |  Loss_1: (0.2581) | Acc_1: (90.00%) (42741/47488)\n",
      "Epoch: 157 | Batch_idx: 380 |  Loss_1: (0.2583) | Acc_1: (90.00%) (43891/48768)\n",
      "Epoch: 157 | Batch_idx: 390 |  Loss_1: (0.2594) | Acc_1: (89.97%) (44987/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3088) | Acc: (92.81%) (9281/10000)\n",
      "Epoch: 158 | Batch_idx: 0 |  Loss_1: (0.1893) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 158 | Batch_idx: 10 |  Loss_1: (0.2188) | Acc_1: (91.48%) (1288/1408)\n",
      "Epoch: 158 | Batch_idx: 20 |  Loss_1: (0.2272) | Acc_1: (91.22%) (2452/2688)\n",
      "Epoch: 158 | Batch_idx: 30 |  Loss_1: (0.2373) | Acc_1: (90.88%) (3606/3968)\n",
      "Epoch: 158 | Batch_idx: 40 |  Loss_1: (0.2417) | Acc_1: (90.59%) (4754/5248)\n",
      "Epoch: 158 | Batch_idx: 50 |  Loss_1: (0.2485) | Acc_1: (90.26%) (5892/6528)\n",
      "Epoch: 158 | Batch_idx: 60 |  Loss_1: (0.2462) | Acc_1: (90.28%) (7049/7808)\n",
      "Epoch: 158 | Batch_idx: 70 |  Loss_1: (0.2462) | Acc_1: (90.31%) (8207/9088)\n",
      "Epoch: 158 | Batch_idx: 80 |  Loss_1: (0.2452) | Acc_1: (90.38%) (9371/10368)\n",
      "Epoch: 158 | Batch_idx: 90 |  Loss_1: (0.2443) | Acc_1: (90.40%) (10530/11648)\n",
      "Epoch: 158 | Batch_idx: 100 |  Loss_1: (0.2445) | Acc_1: (90.42%) (11689/12928)\n",
      "Epoch: 158 | Batch_idx: 110 |  Loss_1: (0.2467) | Acc_1: (90.34%) (12836/14208)\n",
      "Epoch: 158 | Batch_idx: 120 |  Loss_1: (0.2460) | Acc_1: (90.37%) (13997/15488)\n",
      "Epoch: 158 | Batch_idx: 130 |  Loss_1: (0.2472) | Acc_1: (90.37%) (15153/16768)\n",
      "Epoch: 158 | Batch_idx: 140 |  Loss_1: (0.2481) | Acc_1: (90.35%) (16307/18048)\n",
      "Epoch: 158 | Batch_idx: 150 |  Loss_1: (0.2488) | Acc_1: (90.33%) (17459/19328)\n",
      "Epoch: 158 | Batch_idx: 160 |  Loss_1: (0.2453) | Acc_1: (90.50%) (18651/20608)\n",
      "Epoch: 158 | Batch_idx: 170 |  Loss_1: (0.2452) | Acc_1: (90.54%) (19818/21888)\n",
      "Epoch: 158 | Batch_idx: 180 |  Loss_1: (0.2454) | Acc_1: (90.56%) (20980/23168)\n",
      "Epoch: 158 | Batch_idx: 190 |  Loss_1: (0.2469) | Acc_1: (90.50%) (22125/24448)\n",
      "Epoch: 158 | Batch_idx: 200 |  Loss_1: (0.2470) | Acc_1: (90.53%) (23292/25728)\n",
      "Epoch: 158 | Batch_idx: 210 |  Loss_1: (0.2470) | Acc_1: (90.54%) (24454/27008)\n",
      "Epoch: 158 | Batch_idx: 220 |  Loss_1: (0.2467) | Acc_1: (90.55%) (25616/28288)\n",
      "Epoch: 158 | Batch_idx: 230 |  Loss_1: (0.2463) | Acc_1: (90.59%) (26785/29568)\n",
      "Epoch: 158 | Batch_idx: 240 |  Loss_1: (0.2470) | Acc_1: (90.58%) (27943/30848)\n",
      "Epoch: 158 | Batch_idx: 250 |  Loss_1: (0.2470) | Acc_1: (90.58%) (29100/32128)\n",
      "Epoch: 158 | Batch_idx: 260 |  Loss_1: (0.2465) | Acc_1: (90.60%) (30269/33408)\n",
      "Epoch: 158 | Batch_idx: 270 |  Loss_1: (0.2468) | Acc_1: (90.60%) (31426/34688)\n",
      "Epoch: 158 | Batch_idx: 280 |  Loss_1: (0.2471) | Acc_1: (90.61%) (32589/35968)\n",
      "Epoch: 158 | Batch_idx: 290 |  Loss_1: (0.2482) | Acc_1: (90.58%) (33738/37248)\n",
      "Epoch: 158 | Batch_idx: 300 |  Loss_1: (0.2485) | Acc_1: (90.54%) (34885/38528)\n",
      "Epoch: 158 | Batch_idx: 310 |  Loss_1: (0.2489) | Acc_1: (90.53%) (36037/39808)\n",
      "Epoch: 158 | Batch_idx: 320 |  Loss_1: (0.2494) | Acc_1: (90.52%) (37192/41088)\n",
      "Epoch: 158 | Batch_idx: 330 |  Loss_1: (0.2494) | Acc_1: (90.52%) (38350/42368)\n",
      "Epoch: 158 | Batch_idx: 340 |  Loss_1: (0.2496) | Acc_1: (90.51%) (39505/43648)\n",
      "Epoch: 158 | Batch_idx: 350 |  Loss_1: (0.2498) | Acc_1: (90.51%) (40665/44928)\n",
      "Epoch: 158 | Batch_idx: 360 |  Loss_1: (0.2495) | Acc_1: (90.51%) (41823/46208)\n",
      "Epoch: 158 | Batch_idx: 370 |  Loss_1: (0.2496) | Acc_1: (90.51%) (42983/47488)\n",
      "Epoch: 158 | Batch_idx: 380 |  Loss_1: (0.2504) | Acc_1: (90.48%) (44126/48768)\n",
      "Epoch: 158 | Batch_idx: 390 |  Loss_1: (0.2509) | Acc_1: (90.45%) (45226/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3091) | Acc: (92.64%) (9264/10000)\n",
      "Epoch: 159 | Batch_idx: 0 |  Loss_1: (0.1984) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 159 | Batch_idx: 10 |  Loss_1: (0.2267) | Acc_1: (90.91%) (1280/1408)\n",
      "Epoch: 159 | Batch_idx: 20 |  Loss_1: (0.2338) | Acc_1: (90.77%) (2440/2688)\n",
      "Epoch: 159 | Batch_idx: 30 |  Loss_1: (0.2299) | Acc_1: (90.95%) (3609/3968)\n",
      "Epoch: 159 | Batch_idx: 40 |  Loss_1: (0.2321) | Acc_1: (90.99%) (4775/5248)\n",
      "Epoch: 159 | Batch_idx: 50 |  Loss_1: (0.2363) | Acc_1: (90.84%) (5930/6528)\n",
      "Epoch: 159 | Batch_idx: 60 |  Loss_1: (0.2389) | Acc_1: (90.70%) (7082/7808)\n",
      "Epoch: 159 | Batch_idx: 70 |  Loss_1: (0.2421) | Acc_1: (90.64%) (8237/9088)\n",
      "Epoch: 159 | Batch_idx: 80 |  Loss_1: (0.2437) | Acc_1: (90.62%) (9395/10368)\n",
      "Epoch: 159 | Batch_idx: 90 |  Loss_1: (0.2455) | Acc_1: (90.53%) (10545/11648)\n",
      "Epoch: 159 | Batch_idx: 100 |  Loss_1: (0.2453) | Acc_1: (90.57%) (11709/12928)\n",
      "Epoch: 159 | Batch_idx: 110 |  Loss_1: (0.2439) | Acc_1: (90.62%) (12876/14208)\n",
      "Epoch: 159 | Batch_idx: 120 |  Loss_1: (0.2460) | Acc_1: (90.54%) (14023/15488)\n",
      "Epoch: 159 | Batch_idx: 130 |  Loss_1: (0.2437) | Acc_1: (90.62%) (15195/16768)\n",
      "Epoch: 159 | Batch_idx: 140 |  Loss_1: (0.2432) | Acc_1: (90.65%) (16360/18048)\n",
      "Epoch: 159 | Batch_idx: 150 |  Loss_1: (0.2410) | Acc_1: (90.75%) (17541/19328)\n",
      "Epoch: 159 | Batch_idx: 160 |  Loss_1: (0.2411) | Acc_1: (90.74%) (18699/20608)\n",
      "Epoch: 159 | Batch_idx: 170 |  Loss_1: (0.2411) | Acc_1: (90.77%) (19867/21888)\n",
      "Epoch: 159 | Batch_idx: 180 |  Loss_1: (0.2438) | Acc_1: (90.67%) (21006/23168)\n",
      "Epoch: 159 | Batch_idx: 190 |  Loss_1: (0.2433) | Acc_1: (90.67%) (22168/24448)\n",
      "Epoch: 159 | Batch_idx: 200 |  Loss_1: (0.2428) | Acc_1: (90.68%) (23329/25728)\n",
      "Epoch: 159 | Batch_idx: 210 |  Loss_1: (0.2423) | Acc_1: (90.67%) (24489/27008)\n",
      "Epoch: 159 | Batch_idx: 220 |  Loss_1: (0.2419) | Acc_1: (90.68%) (25652/28288)\n",
      "Epoch: 159 | Batch_idx: 230 |  Loss_1: (0.2421) | Acc_1: (90.70%) (26818/29568)\n",
      "Epoch: 159 | Batch_idx: 240 |  Loss_1: (0.2416) | Acc_1: (90.71%) (27983/30848)\n",
      "Epoch: 159 | Batch_idx: 250 |  Loss_1: (0.2423) | Acc_1: (90.69%) (29136/32128)\n",
      "Epoch: 159 | Batch_idx: 260 |  Loss_1: (0.2423) | Acc_1: (90.68%) (30294/33408)\n",
      "Epoch: 159 | Batch_idx: 270 |  Loss_1: (0.2423) | Acc_1: (90.69%) (31458/34688)\n",
      "Epoch: 159 | Batch_idx: 280 |  Loss_1: (0.2426) | Acc_1: (90.71%) (32625/35968)\n",
      "Epoch: 159 | Batch_idx: 290 |  Loss_1: (0.2432) | Acc_1: (90.68%) (33776/37248)\n",
      "Epoch: 159 | Batch_idx: 300 |  Loss_1: (0.2435) | Acc_1: (90.65%) (34926/38528)\n",
      "Epoch: 159 | Batch_idx: 310 |  Loss_1: (0.2437) | Acc_1: (90.65%) (36084/39808)\n",
      "Epoch: 159 | Batch_idx: 320 |  Loss_1: (0.2431) | Acc_1: (90.67%) (37255/41088)\n",
      "Epoch: 159 | Batch_idx: 330 |  Loss_1: (0.2434) | Acc_1: (90.66%) (38411/42368)\n",
      "Epoch: 159 | Batch_idx: 340 |  Loss_1: (0.2434) | Acc_1: (90.68%) (39578/43648)\n",
      "Epoch: 159 | Batch_idx: 350 |  Loss_1: (0.2432) | Acc_1: (90.69%) (40746/44928)\n",
      "Epoch: 159 | Batch_idx: 360 |  Loss_1: (0.2435) | Acc_1: (90.68%) (41901/46208)\n",
      "Epoch: 159 | Batch_idx: 370 |  Loss_1: (0.2440) | Acc_1: (90.66%) (43052/47488)\n",
      "Epoch: 159 | Batch_idx: 380 |  Loss_1: (0.2455) | Acc_1: (90.60%) (44182/48768)\n",
      "Epoch: 159 | Batch_idx: 390 |  Loss_1: (0.2455) | Acc_1: (90.58%) (45292/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3167) | Acc: (92.85%) (9285/10000)\n",
      "Epoch: 160 | Batch_idx: 0 |  Loss_1: (0.3593) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 160 | Batch_idx: 10 |  Loss_1: (0.2585) | Acc_1: (89.70%) (1263/1408)\n",
      "Epoch: 160 | Batch_idx: 20 |  Loss_1: (0.2584) | Acc_1: (90.07%) (2421/2688)\n",
      "Epoch: 160 | Batch_idx: 30 |  Loss_1: (0.2575) | Acc_1: (90.15%) (3577/3968)\n",
      "Epoch: 160 | Batch_idx: 40 |  Loss_1: (0.2538) | Acc_1: (90.36%) (4742/5248)\n",
      "Epoch: 160 | Batch_idx: 50 |  Loss_1: (0.2498) | Acc_1: (90.46%) (5905/6528)\n",
      "Epoch: 160 | Batch_idx: 60 |  Loss_1: (0.2518) | Acc_1: (90.32%) (7052/7808)\n",
      "Epoch: 160 | Batch_idx: 70 |  Loss_1: (0.2508) | Acc_1: (90.40%) (8216/9088)\n",
      "Epoch: 160 | Batch_idx: 80 |  Loss_1: (0.2479) | Acc_1: (90.48%) (9381/10368)\n",
      "Epoch: 160 | Batch_idx: 90 |  Loss_1: (0.2458) | Acc_1: (90.53%) (10545/11648)\n",
      "Epoch: 160 | Batch_idx: 100 |  Loss_1: (0.2441) | Acc_1: (90.59%) (11711/12928)\n",
      "Epoch: 160 | Batch_idx: 110 |  Loss_1: (0.2440) | Acc_1: (90.55%) (12865/14208)\n",
      "Epoch: 160 | Batch_idx: 120 |  Loss_1: (0.2462) | Acc_1: (90.47%) (14012/15488)\n",
      "Epoch: 160 | Batch_idx: 130 |  Loss_1: (0.2474) | Acc_1: (90.43%) (15163/16768)\n",
      "Epoch: 160 | Batch_idx: 140 |  Loss_1: (0.2496) | Acc_1: (90.34%) (16305/18048)\n",
      "Epoch: 160 | Batch_idx: 150 |  Loss_1: (0.2471) | Acc_1: (90.46%) (17484/19328)\n",
      "Epoch: 160 | Batch_idx: 160 |  Loss_1: (0.2477) | Acc_1: (90.46%) (18642/20608)\n",
      "Epoch: 160 | Batch_idx: 170 |  Loss_1: (0.2485) | Acc_1: (90.40%) (19786/21888)\n",
      "Epoch: 160 | Batch_idx: 180 |  Loss_1: (0.2483) | Acc_1: (90.40%) (20943/23168)\n",
      "Epoch: 160 | Batch_idx: 190 |  Loss_1: (0.2500) | Acc_1: (90.32%) (22082/24448)\n",
      "Epoch: 160 | Batch_idx: 200 |  Loss_1: (0.2494) | Acc_1: (90.36%) (23249/25728)\n",
      "Epoch: 160 | Batch_idx: 210 |  Loss_1: (0.2496) | Acc_1: (90.35%) (24402/27008)\n",
      "Epoch: 160 | Batch_idx: 220 |  Loss_1: (0.2500) | Acc_1: (90.36%) (25560/28288)\n",
      "Epoch: 160 | Batch_idx: 230 |  Loss_1: (0.2508) | Acc_1: (90.32%) (26707/29568)\n",
      "Epoch: 160 | Batch_idx: 240 |  Loss_1: (0.2526) | Acc_1: (90.27%) (27846/30848)\n",
      "Epoch: 160 | Batch_idx: 250 |  Loss_1: (0.2524) | Acc_1: (90.28%) (29005/32128)\n",
      "Epoch: 160 | Batch_idx: 260 |  Loss_1: (0.2527) | Acc_1: (90.25%) (30152/33408)\n",
      "Epoch: 160 | Batch_idx: 270 |  Loss_1: (0.2540) | Acc_1: (90.21%) (31292/34688)\n",
      "Epoch: 160 | Batch_idx: 280 |  Loss_1: (0.2528) | Acc_1: (90.26%) (32463/35968)\n",
      "Epoch: 160 | Batch_idx: 290 |  Loss_1: (0.2524) | Acc_1: (90.28%) (33626/37248)\n",
      "Epoch: 160 | Batch_idx: 300 |  Loss_1: (0.2519) | Acc_1: (90.30%) (34791/38528)\n",
      "Epoch: 160 | Batch_idx: 310 |  Loss_1: (0.2512) | Acc_1: (90.34%) (35963/39808)\n",
      "Epoch: 160 | Batch_idx: 320 |  Loss_1: (0.2514) | Acc_1: (90.32%) (37112/41088)\n",
      "Epoch: 160 | Batch_idx: 330 |  Loss_1: (0.2524) | Acc_1: (90.29%) (38252/42368)\n",
      "Epoch: 160 | Batch_idx: 340 |  Loss_1: (0.2522) | Acc_1: (90.29%) (39409/43648)\n",
      "Epoch: 160 | Batch_idx: 350 |  Loss_1: (0.2527) | Acc_1: (90.26%) (40550/44928)\n",
      "Epoch: 160 | Batch_idx: 360 |  Loss_1: (0.2529) | Acc_1: (90.26%) (41706/46208)\n",
      "Epoch: 160 | Batch_idx: 370 |  Loss_1: (0.2522) | Acc_1: (90.29%) (42878/47488)\n",
      "Epoch: 160 | Batch_idx: 380 |  Loss_1: (0.2528) | Acc_1: (90.26%) (44020/48768)\n",
      "Epoch: 160 | Batch_idx: 390 |  Loss_1: (0.2526) | Acc_1: (90.26%) (45132/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3110) | Acc: (92.65%) (9265/10000)\n",
      "Epoch: 161 | Batch_idx: 0 |  Loss_1: (0.2619) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 161 | Batch_idx: 10 |  Loss_1: (0.2357) | Acc_1: (90.77%) (1278/1408)\n",
      "Epoch: 161 | Batch_idx: 20 |  Loss_1: (0.2517) | Acc_1: (89.96%) (2418/2688)\n",
      "Epoch: 161 | Batch_idx: 30 |  Loss_1: (0.2586) | Acc_1: (89.74%) (3561/3968)\n",
      "Epoch: 161 | Batch_idx: 40 |  Loss_1: (0.2607) | Acc_1: (89.73%) (4709/5248)\n",
      "Epoch: 161 | Batch_idx: 50 |  Loss_1: (0.2561) | Acc_1: (89.94%) (5871/6528)\n",
      "Epoch: 161 | Batch_idx: 60 |  Loss_1: (0.2531) | Acc_1: (90.07%) (7033/7808)\n",
      "Epoch: 161 | Batch_idx: 70 |  Loss_1: (0.2556) | Acc_1: (90.07%) (8186/9088)\n",
      "Epoch: 161 | Batch_idx: 80 |  Loss_1: (0.2538) | Acc_1: (90.19%) (9351/10368)\n",
      "Epoch: 161 | Batch_idx: 90 |  Loss_1: (0.2520) | Acc_1: (90.28%) (10516/11648)\n",
      "Epoch: 161 | Batch_idx: 100 |  Loss_1: (0.2497) | Acc_1: (90.41%) (11688/12928)\n",
      "Epoch: 161 | Batch_idx: 110 |  Loss_1: (0.2474) | Acc_1: (90.53%) (12862/14208)\n",
      "Epoch: 161 | Batch_idx: 120 |  Loss_1: (0.2479) | Acc_1: (90.50%) (14016/15488)\n",
      "Epoch: 161 | Batch_idx: 130 |  Loss_1: (0.2471) | Acc_1: (90.53%) (15180/16768)\n",
      "Epoch: 161 | Batch_idx: 140 |  Loss_1: (0.2444) | Acc_1: (90.65%) (16360/18048)\n",
      "Epoch: 161 | Batch_idx: 150 |  Loss_1: (0.2448) | Acc_1: (90.65%) (17521/19328)\n",
      "Epoch: 161 | Batch_idx: 160 |  Loss_1: (0.2449) | Acc_1: (90.61%) (18673/20608)\n",
      "Epoch: 161 | Batch_idx: 170 |  Loss_1: (0.2479) | Acc_1: (90.52%) (19814/21888)\n",
      "Epoch: 161 | Batch_idx: 180 |  Loss_1: (0.2496) | Acc_1: (90.45%) (20956/23168)\n",
      "Epoch: 161 | Batch_idx: 190 |  Loss_1: (0.2489) | Acc_1: (90.47%) (22118/24448)\n",
      "Epoch: 161 | Batch_idx: 200 |  Loss_1: (0.2503) | Acc_1: (90.42%) (23264/25728)\n",
      "Epoch: 161 | Batch_idx: 210 |  Loss_1: (0.2499) | Acc_1: (90.45%) (24429/27008)\n",
      "Epoch: 161 | Batch_idx: 220 |  Loss_1: (0.2507) | Acc_1: (90.41%) (25575/28288)\n",
      "Epoch: 161 | Batch_idx: 230 |  Loss_1: (0.2512) | Acc_1: (90.38%) (26723/29568)\n",
      "Epoch: 161 | Batch_idx: 240 |  Loss_1: (0.2526) | Acc_1: (90.35%) (27871/30848)\n",
      "Epoch: 161 | Batch_idx: 250 |  Loss_1: (0.2527) | Acc_1: (90.34%) (29025/32128)\n",
      "Epoch: 161 | Batch_idx: 260 |  Loss_1: (0.2521) | Acc_1: (90.37%) (30192/33408)\n",
      "Epoch: 161 | Batch_idx: 270 |  Loss_1: (0.2513) | Acc_1: (90.38%) (31352/34688)\n",
      "Epoch: 161 | Batch_idx: 280 |  Loss_1: (0.2519) | Acc_1: (90.36%) (32499/35968)\n",
      "Epoch: 161 | Batch_idx: 290 |  Loss_1: (0.2514) | Acc_1: (90.38%) (33666/37248)\n",
      "Epoch: 161 | Batch_idx: 300 |  Loss_1: (0.2511) | Acc_1: (90.39%) (34825/38528)\n",
      "Epoch: 161 | Batch_idx: 310 |  Loss_1: (0.2506) | Acc_1: (90.41%) (35991/39808)\n",
      "Epoch: 161 | Batch_idx: 320 |  Loss_1: (0.2502) | Acc_1: (90.44%) (37160/41088)\n",
      "Epoch: 161 | Batch_idx: 330 |  Loss_1: (0.2490) | Acc_1: (90.48%) (38336/42368)\n",
      "Epoch: 161 | Batch_idx: 340 |  Loss_1: (0.2486) | Acc_1: (90.50%) (39502/43648)\n",
      "Epoch: 161 | Batch_idx: 350 |  Loss_1: (0.2499) | Acc_1: (90.44%) (40633/44928)\n",
      "Epoch: 161 | Batch_idx: 360 |  Loss_1: (0.2500) | Acc_1: (90.44%) (41790/46208)\n",
      "Epoch: 161 | Batch_idx: 370 |  Loss_1: (0.2504) | Acc_1: (90.42%) (42940/47488)\n",
      "Epoch: 161 | Batch_idx: 380 |  Loss_1: (0.2504) | Acc_1: (90.43%) (44100/48768)\n",
      "Epoch: 161 | Batch_idx: 390 |  Loss_1: (0.2513) | Acc_1: (90.39%) (45196/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2990) | Acc: (93.07%) (9307/10000)\n",
      "Epoch: 162 | Batch_idx: 0 |  Loss_1: (0.2936) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 162 | Batch_idx: 10 |  Loss_1: (0.2475) | Acc_1: (90.62%) (1276/1408)\n",
      "Epoch: 162 | Batch_idx: 20 |  Loss_1: (0.2479) | Acc_1: (90.70%) (2438/2688)\n",
      "Epoch: 162 | Batch_idx: 30 |  Loss_1: (0.2553) | Acc_1: (90.65%) (3597/3968)\n",
      "Epoch: 162 | Batch_idx: 40 |  Loss_1: (0.2653) | Acc_1: (90.22%) (4735/5248)\n",
      "Epoch: 162 | Batch_idx: 50 |  Loss_1: (0.2665) | Acc_1: (90.13%) (5884/6528)\n",
      "Epoch: 162 | Batch_idx: 60 |  Loss_1: (0.2672) | Acc_1: (90.07%) (7033/7808)\n",
      "Epoch: 162 | Batch_idx: 70 |  Loss_1: (0.2661) | Acc_1: (90.10%) (8188/9088)\n",
      "Epoch: 162 | Batch_idx: 80 |  Loss_1: (0.2602) | Acc_1: (90.30%) (9362/10368)\n",
      "Epoch: 162 | Batch_idx: 90 |  Loss_1: (0.2588) | Acc_1: (90.30%) (10518/11648)\n",
      "Epoch: 162 | Batch_idx: 100 |  Loss_1: (0.2575) | Acc_1: (90.35%) (11681/12928)\n",
      "Epoch: 162 | Batch_idx: 110 |  Loss_1: (0.2529) | Acc_1: (90.47%) (12854/14208)\n",
      "Epoch: 162 | Batch_idx: 120 |  Loss_1: (0.2533) | Acc_1: (90.43%) (14006/15488)\n",
      "Epoch: 162 | Batch_idx: 130 |  Loss_1: (0.2517) | Acc_1: (90.48%) (15172/16768)\n",
      "Epoch: 162 | Batch_idx: 140 |  Loss_1: (0.2526) | Acc_1: (90.45%) (16324/18048)\n",
      "Epoch: 162 | Batch_idx: 150 |  Loss_1: (0.2510) | Acc_1: (90.51%) (17494/19328)\n",
      "Epoch: 162 | Batch_idx: 160 |  Loss_1: (0.2519) | Acc_1: (90.51%) (18653/20608)\n",
      "Epoch: 162 | Batch_idx: 170 |  Loss_1: (0.2512) | Acc_1: (90.52%) (19813/21888)\n",
      "Epoch: 162 | Batch_idx: 180 |  Loss_1: (0.2487) | Acc_1: (90.60%) (20990/23168)\n",
      "Epoch: 162 | Batch_idx: 190 |  Loss_1: (0.2469) | Acc_1: (90.65%) (22162/24448)\n",
      "Epoch: 162 | Batch_idx: 200 |  Loss_1: (0.2479) | Acc_1: (90.60%) (23310/25728)\n",
      "Epoch: 162 | Batch_idx: 210 |  Loss_1: (0.2462) | Acc_1: (90.68%) (24492/27008)\n",
      "Epoch: 162 | Batch_idx: 220 |  Loss_1: (0.2457) | Acc_1: (90.72%) (25662/28288)\n",
      "Epoch: 162 | Batch_idx: 230 |  Loss_1: (0.2454) | Acc_1: (90.73%) (26826/29568)\n",
      "Epoch: 162 | Batch_idx: 240 |  Loss_1: (0.2463) | Acc_1: (90.68%) (27973/30848)\n",
      "Epoch: 162 | Batch_idx: 250 |  Loss_1: (0.2449) | Acc_1: (90.73%) (29151/32128)\n",
      "Epoch: 162 | Batch_idx: 260 |  Loss_1: (0.2447) | Acc_1: (90.74%) (30315/33408)\n",
      "Epoch: 162 | Batch_idx: 270 |  Loss_1: (0.2456) | Acc_1: (90.71%) (31466/34688)\n",
      "Epoch: 162 | Batch_idx: 280 |  Loss_1: (0.2453) | Acc_1: (90.71%) (32627/35968)\n",
      "Epoch: 162 | Batch_idx: 290 |  Loss_1: (0.2468) | Acc_1: (90.67%) (33772/37248)\n",
      "Epoch: 162 | Batch_idx: 300 |  Loss_1: (0.2464) | Acc_1: (90.68%) (34937/38528)\n",
      "Epoch: 162 | Batch_idx: 310 |  Loss_1: (0.2461) | Acc_1: (90.68%) (36097/39808)\n",
      "Epoch: 162 | Batch_idx: 320 |  Loss_1: (0.2460) | Acc_1: (90.68%) (37258/41088)\n",
      "Epoch: 162 | Batch_idx: 330 |  Loss_1: (0.2468) | Acc_1: (90.65%) (38405/42368)\n",
      "Epoch: 162 | Batch_idx: 340 |  Loss_1: (0.2471) | Acc_1: (90.61%) (39550/43648)\n",
      "Epoch: 162 | Batch_idx: 350 |  Loss_1: (0.2470) | Acc_1: (90.61%) (40711/44928)\n",
      "Epoch: 162 | Batch_idx: 360 |  Loss_1: (0.2469) | Acc_1: (90.62%) (41875/46208)\n",
      "Epoch: 162 | Batch_idx: 370 |  Loss_1: (0.2470) | Acc_1: (90.62%) (43036/47488)\n",
      "Epoch: 162 | Batch_idx: 380 |  Loss_1: (0.2471) | Acc_1: (90.64%) (44201/48768)\n",
      "Epoch: 162 | Batch_idx: 390 |  Loss_1: (0.2475) | Acc_1: (90.60%) (45302/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2908) | Acc: (92.91%) (9291/10000)\n",
      "Epoch: 163 | Batch_idx: 0 |  Loss_1: (0.2186) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 163 | Batch_idx: 10 |  Loss_1: (0.1996) | Acc_1: (92.33%) (1300/1408)\n",
      "Epoch: 163 | Batch_idx: 20 |  Loss_1: (0.2133) | Acc_1: (92.00%) (2473/2688)\n",
      "Epoch: 163 | Batch_idx: 30 |  Loss_1: (0.2228) | Acc_1: (91.58%) (3634/3968)\n",
      "Epoch: 163 | Batch_idx: 40 |  Loss_1: (0.2267) | Acc_1: (91.37%) (4795/5248)\n",
      "Epoch: 163 | Batch_idx: 50 |  Loss_1: (0.2314) | Acc_1: (91.15%) (5950/6528)\n",
      "Epoch: 163 | Batch_idx: 60 |  Loss_1: (0.2391) | Acc_1: (90.84%) (7093/7808)\n",
      "Epoch: 163 | Batch_idx: 70 |  Loss_1: (0.2454) | Acc_1: (90.58%) (8232/9088)\n",
      "Epoch: 163 | Batch_idx: 80 |  Loss_1: (0.2484) | Acc_1: (90.49%) (9382/10368)\n",
      "Epoch: 163 | Batch_idx: 90 |  Loss_1: (0.2494) | Acc_1: (90.42%) (10532/11648)\n",
      "Epoch: 163 | Batch_idx: 100 |  Loss_1: (0.2519) | Acc_1: (90.35%) (11680/12928)\n",
      "Epoch: 163 | Batch_idx: 110 |  Loss_1: (0.2543) | Acc_1: (90.22%) (12819/14208)\n",
      "Epoch: 163 | Batch_idx: 120 |  Loss_1: (0.2527) | Acc_1: (90.27%) (13981/15488)\n",
      "Epoch: 163 | Batch_idx: 130 |  Loss_1: (0.2479) | Acc_1: (90.46%) (15169/16768)\n",
      "Epoch: 163 | Batch_idx: 140 |  Loss_1: (0.2479) | Acc_1: (90.50%) (16334/18048)\n",
      "Epoch: 163 | Batch_idx: 150 |  Loss_1: (0.2474) | Acc_1: (90.51%) (17493/19328)\n",
      "Epoch: 163 | Batch_idx: 160 |  Loss_1: (0.2489) | Acc_1: (90.44%) (18638/20608)\n",
      "Epoch: 163 | Batch_idx: 170 |  Loss_1: (0.2512) | Acc_1: (90.34%) (19774/21888)\n",
      "Epoch: 163 | Batch_idx: 180 |  Loss_1: (0.2495) | Acc_1: (90.44%) (20952/23168)\n",
      "Epoch: 163 | Batch_idx: 190 |  Loss_1: (0.2495) | Acc_1: (90.41%) (22103/24448)\n",
      "Epoch: 163 | Batch_idx: 200 |  Loss_1: (0.2502) | Acc_1: (90.40%) (23258/25728)\n",
      "Epoch: 163 | Batch_idx: 210 |  Loss_1: (0.2503) | Acc_1: (90.39%) (24412/27008)\n",
      "Epoch: 163 | Batch_idx: 220 |  Loss_1: (0.2498) | Acc_1: (90.41%) (25576/28288)\n",
      "Epoch: 163 | Batch_idx: 230 |  Loss_1: (0.2506) | Acc_1: (90.36%) (26718/29568)\n",
      "Epoch: 163 | Batch_idx: 240 |  Loss_1: (0.2509) | Acc_1: (90.35%) (27871/30848)\n",
      "Epoch: 163 | Batch_idx: 250 |  Loss_1: (0.2507) | Acc_1: (90.37%) (29033/32128)\n",
      "Epoch: 163 | Batch_idx: 260 |  Loss_1: (0.2517) | Acc_1: (90.33%) (30179/33408)\n",
      "Epoch: 163 | Batch_idx: 270 |  Loss_1: (0.2520) | Acc_1: (90.31%) (31326/34688)\n",
      "Epoch: 163 | Batch_idx: 280 |  Loss_1: (0.2529) | Acc_1: (90.27%) (32469/35968)\n",
      "Epoch: 163 | Batch_idx: 290 |  Loss_1: (0.2527) | Acc_1: (90.28%) (33628/37248)\n",
      "Epoch: 163 | Batch_idx: 300 |  Loss_1: (0.2525) | Acc_1: (90.28%) (34785/38528)\n",
      "Epoch: 163 | Batch_idx: 310 |  Loss_1: (0.2523) | Acc_1: (90.30%) (35946/39808)\n",
      "Epoch: 163 | Batch_idx: 320 |  Loss_1: (0.2525) | Acc_1: (90.31%) (37105/41088)\n",
      "Epoch: 163 | Batch_idx: 330 |  Loss_1: (0.2524) | Acc_1: (90.33%) (38270/42368)\n",
      "Epoch: 163 | Batch_idx: 340 |  Loss_1: (0.2515) | Acc_1: (90.36%) (39440/43648)\n",
      "Epoch: 163 | Batch_idx: 350 |  Loss_1: (0.2515) | Acc_1: (90.35%) (40592/44928)\n",
      "Epoch: 163 | Batch_idx: 360 |  Loss_1: (0.2519) | Acc_1: (90.33%) (41739/46208)\n",
      "Epoch: 163 | Batch_idx: 370 |  Loss_1: (0.2521) | Acc_1: (90.31%) (42887/47488)\n",
      "Epoch: 163 | Batch_idx: 380 |  Loss_1: (0.2535) | Acc_1: (90.26%) (44018/48768)\n",
      "Epoch: 163 | Batch_idx: 390 |  Loss_1: (0.2532) | Acc_1: (90.28%) (45138/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2841) | Acc: (93.31%) (9331/10000)\n",
      "Epoch: 164 | Batch_idx: 0 |  Loss_1: (0.2608) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 164 | Batch_idx: 10 |  Loss_1: (0.2176) | Acc_1: (92.12%) (1297/1408)\n",
      "Epoch: 164 | Batch_idx: 20 |  Loss_1: (0.2172) | Acc_1: (91.59%) (2462/2688)\n",
      "Epoch: 164 | Batch_idx: 30 |  Loss_1: (0.2393) | Acc_1: (90.80%) (3603/3968)\n",
      "Epoch: 164 | Batch_idx: 40 |  Loss_1: (0.2475) | Acc_1: (90.53%) (4751/5248)\n",
      "Epoch: 164 | Batch_idx: 50 |  Loss_1: (0.2473) | Acc_1: (90.46%) (5905/6528)\n",
      "Epoch: 164 | Batch_idx: 60 |  Loss_1: (0.2466) | Acc_1: (90.54%) (7069/7808)\n",
      "Epoch: 164 | Batch_idx: 70 |  Loss_1: (0.2451) | Acc_1: (90.58%) (8232/9088)\n",
      "Epoch: 164 | Batch_idx: 80 |  Loss_1: (0.2429) | Acc_1: (90.68%) (9402/10368)\n",
      "Epoch: 164 | Batch_idx: 90 |  Loss_1: (0.2467) | Acc_1: (90.53%) (10545/11648)\n",
      "Epoch: 164 | Batch_idx: 100 |  Loss_1: (0.2443) | Acc_1: (90.62%) (11715/12928)\n",
      "Epoch: 164 | Batch_idx: 110 |  Loss_1: (0.2426) | Acc_1: (90.65%) (12880/14208)\n",
      "Epoch: 164 | Batch_idx: 120 |  Loss_1: (0.2423) | Acc_1: (90.66%) (14042/15488)\n",
      "Epoch: 164 | Batch_idx: 130 |  Loss_1: (0.2419) | Acc_1: (90.64%) (15198/16768)\n",
      "Epoch: 164 | Batch_idx: 140 |  Loss_1: (0.2405) | Acc_1: (90.71%) (16371/18048)\n",
      "Epoch: 164 | Batch_idx: 150 |  Loss_1: (0.2401) | Acc_1: (90.74%) (17539/19328)\n",
      "Epoch: 164 | Batch_idx: 160 |  Loss_1: (0.2419) | Acc_1: (90.69%) (18689/20608)\n",
      "Epoch: 164 | Batch_idx: 170 |  Loss_1: (0.2421) | Acc_1: (90.70%) (19852/21888)\n",
      "Epoch: 164 | Batch_idx: 180 |  Loss_1: (0.2436) | Acc_1: (90.62%) (20994/23168)\n",
      "Epoch: 164 | Batch_idx: 190 |  Loss_1: (0.2421) | Acc_1: (90.67%) (22168/24448)\n",
      "Epoch: 164 | Batch_idx: 200 |  Loss_1: (0.2425) | Acc_1: (90.65%) (23323/25728)\n",
      "Epoch: 164 | Batch_idx: 210 |  Loss_1: (0.2418) | Acc_1: (90.67%) (24487/27008)\n",
      "Epoch: 164 | Batch_idx: 220 |  Loss_1: (0.2427) | Acc_1: (90.62%) (25636/28288)\n",
      "Epoch: 164 | Batch_idx: 230 |  Loss_1: (0.2437) | Acc_1: (90.57%) (26779/29568)\n",
      "Epoch: 164 | Batch_idx: 240 |  Loss_1: (0.2436) | Acc_1: (90.59%) (27946/30848)\n",
      "Epoch: 164 | Batch_idx: 250 |  Loss_1: (0.2435) | Acc_1: (90.61%) (29110/32128)\n",
      "Epoch: 164 | Batch_idx: 260 |  Loss_1: (0.2440) | Acc_1: (90.58%) (30260/33408)\n",
      "Epoch: 164 | Batch_idx: 270 |  Loss_1: (0.2441) | Acc_1: (90.58%) (31419/34688)\n",
      "Epoch: 164 | Batch_idx: 280 |  Loss_1: (0.2441) | Acc_1: (90.56%) (32574/35968)\n",
      "Epoch: 164 | Batch_idx: 290 |  Loss_1: (0.2448) | Acc_1: (90.53%) (33722/37248)\n",
      "Epoch: 164 | Batch_idx: 300 |  Loss_1: (0.2454) | Acc_1: (90.52%) (34877/38528)\n",
      "Epoch: 164 | Batch_idx: 310 |  Loss_1: (0.2456) | Acc_1: (90.53%) (36037/39808)\n",
      "Epoch: 164 | Batch_idx: 320 |  Loss_1: (0.2451) | Acc_1: (90.54%) (37200/41088)\n",
      "Epoch: 164 | Batch_idx: 330 |  Loss_1: (0.2450) | Acc_1: (90.54%) (38358/42368)\n",
      "Epoch: 164 | Batch_idx: 340 |  Loss_1: (0.2451) | Acc_1: (90.53%) (39516/43648)\n",
      "Epoch: 164 | Batch_idx: 350 |  Loss_1: (0.2465) | Acc_1: (90.48%) (40652/44928)\n",
      "Epoch: 164 | Batch_idx: 360 |  Loss_1: (0.2464) | Acc_1: (90.47%) (41804/46208)\n",
      "Epoch: 164 | Batch_idx: 370 |  Loss_1: (0.2459) | Acc_1: (90.48%) (42967/47488)\n",
      "Epoch: 164 | Batch_idx: 380 |  Loss_1: (0.2460) | Acc_1: (90.49%) (44131/48768)\n",
      "Epoch: 164 | Batch_idx: 390 |  Loss_1: (0.2464) | Acc_1: (90.48%) (45239/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2944) | Acc: (93.24%) (9324/10000)\n",
      "Epoch: 165 | Batch_idx: 0 |  Loss_1: (0.1856) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 165 | Batch_idx: 10 |  Loss_1: (0.2337) | Acc_1: (91.05%) (1282/1408)\n",
      "Epoch: 165 | Batch_idx: 20 |  Loss_1: (0.2501) | Acc_1: (90.29%) (2427/2688)\n",
      "Epoch: 165 | Batch_idx: 30 |  Loss_1: (0.2602) | Acc_1: (89.97%) (3570/3968)\n",
      "Epoch: 165 | Batch_idx: 40 |  Loss_1: (0.2538) | Acc_1: (90.22%) (4735/5248)\n",
      "Epoch: 165 | Batch_idx: 50 |  Loss_1: (0.2433) | Acc_1: (90.61%) (5915/6528)\n",
      "Epoch: 165 | Batch_idx: 60 |  Loss_1: (0.2443) | Acc_1: (90.64%) (7077/7808)\n",
      "Epoch: 165 | Batch_idx: 70 |  Loss_1: (0.2413) | Acc_1: (90.75%) (8247/9088)\n",
      "Epoch: 165 | Batch_idx: 80 |  Loss_1: (0.2424) | Acc_1: (90.66%) (9400/10368)\n",
      "Epoch: 165 | Batch_idx: 90 |  Loss_1: (0.2425) | Acc_1: (90.68%) (10562/11648)\n",
      "Epoch: 165 | Batch_idx: 100 |  Loss_1: (0.2401) | Acc_1: (90.74%) (11731/12928)\n",
      "Epoch: 165 | Batch_idx: 110 |  Loss_1: (0.2429) | Acc_1: (90.58%) (12870/14208)\n",
      "Epoch: 165 | Batch_idx: 120 |  Loss_1: (0.2439) | Acc_1: (90.52%) (14019/15488)\n",
      "Epoch: 165 | Batch_idx: 130 |  Loss_1: (0.2446) | Acc_1: (90.49%) (15174/16768)\n",
      "Epoch: 165 | Batch_idx: 140 |  Loss_1: (0.2442) | Acc_1: (90.54%) (16340/18048)\n",
      "Epoch: 165 | Batch_idx: 150 |  Loss_1: (0.2468) | Acc_1: (90.44%) (17480/19328)\n",
      "Epoch: 165 | Batch_idx: 160 |  Loss_1: (0.2463) | Acc_1: (90.48%) (18647/20608)\n",
      "Epoch: 165 | Batch_idx: 170 |  Loss_1: (0.2477) | Acc_1: (90.43%) (19793/21888)\n",
      "Epoch: 165 | Batch_idx: 180 |  Loss_1: (0.2478) | Acc_1: (90.40%) (20945/23168)\n",
      "Epoch: 165 | Batch_idx: 190 |  Loss_1: (0.2463) | Acc_1: (90.45%) (22114/24448)\n",
      "Epoch: 165 | Batch_idx: 200 |  Loss_1: (0.2477) | Acc_1: (90.40%) (23258/25728)\n",
      "Epoch: 165 | Batch_idx: 210 |  Loss_1: (0.2468) | Acc_1: (90.45%) (24429/27008)\n",
      "Epoch: 165 | Batch_idx: 220 |  Loss_1: (0.2471) | Acc_1: (90.47%) (25591/28288)\n",
      "Epoch: 165 | Batch_idx: 230 |  Loss_1: (0.2477) | Acc_1: (90.45%) (26744/29568)\n",
      "Epoch: 165 | Batch_idx: 240 |  Loss_1: (0.2469) | Acc_1: (90.48%) (27912/30848)\n",
      "Epoch: 165 | Batch_idx: 250 |  Loss_1: (0.2472) | Acc_1: (90.47%) (29067/32128)\n",
      "Epoch: 165 | Batch_idx: 260 |  Loss_1: (0.2460) | Acc_1: (90.54%) (30246/33408)\n",
      "Epoch: 165 | Batch_idx: 270 |  Loss_1: (0.2453) | Acc_1: (90.58%) (31421/34688)\n",
      "Epoch: 165 | Batch_idx: 280 |  Loss_1: (0.2445) | Acc_1: (90.61%) (32590/35968)\n",
      "Epoch: 165 | Batch_idx: 290 |  Loss_1: (0.2445) | Acc_1: (90.61%) (33749/37248)\n",
      "Epoch: 165 | Batch_idx: 300 |  Loss_1: (0.2459) | Acc_1: (90.54%) (34883/38528)\n",
      "Epoch: 165 | Batch_idx: 310 |  Loss_1: (0.2465) | Acc_1: (90.52%) (36033/39808)\n",
      "Epoch: 165 | Batch_idx: 320 |  Loss_1: (0.2470) | Acc_1: (90.49%) (37181/41088)\n",
      "Epoch: 165 | Batch_idx: 330 |  Loss_1: (0.2470) | Acc_1: (90.49%) (38338/42368)\n",
      "Epoch: 165 | Batch_idx: 340 |  Loss_1: (0.2475) | Acc_1: (90.47%) (39489/43648)\n",
      "Epoch: 165 | Batch_idx: 350 |  Loss_1: (0.2475) | Acc_1: (90.47%) (40647/44928)\n",
      "Epoch: 165 | Batch_idx: 360 |  Loss_1: (0.2477) | Acc_1: (90.47%) (41806/46208)\n",
      "Epoch: 165 | Batch_idx: 370 |  Loss_1: (0.2470) | Acc_1: (90.51%) (42981/47488)\n",
      "Epoch: 165 | Batch_idx: 380 |  Loss_1: (0.2466) | Acc_1: (90.52%) (44146/48768)\n",
      "Epoch: 165 | Batch_idx: 390 |  Loss_1: (0.2470) | Acc_1: (90.51%) (45255/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3009) | Acc: (93.05%) (9305/10000)\n",
      "Epoch: 166 | Batch_idx: 0 |  Loss_1: (0.2003) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 166 | Batch_idx: 10 |  Loss_1: (0.2421) | Acc_1: (90.70%) (1277/1408)\n",
      "Epoch: 166 | Batch_idx: 20 |  Loss_1: (0.2422) | Acc_1: (90.74%) (2439/2688)\n",
      "Epoch: 166 | Batch_idx: 30 |  Loss_1: (0.2385) | Acc_1: (90.85%) (3605/3968)\n",
      "Epoch: 166 | Batch_idx: 40 |  Loss_1: (0.2338) | Acc_1: (91.08%) (4780/5248)\n",
      "Epoch: 166 | Batch_idx: 50 |  Loss_1: (0.2373) | Acc_1: (90.90%) (5934/6528)\n",
      "Epoch: 166 | Batch_idx: 60 |  Loss_1: (0.2378) | Acc_1: (90.86%) (7094/7808)\n",
      "Epoch: 166 | Batch_idx: 70 |  Loss_1: (0.2392) | Acc_1: (90.81%) (8253/9088)\n",
      "Epoch: 166 | Batch_idx: 80 |  Loss_1: (0.2423) | Acc_1: (90.73%) (9407/10368)\n",
      "Epoch: 166 | Batch_idx: 90 |  Loss_1: (0.2392) | Acc_1: (90.82%) (10579/11648)\n",
      "Epoch: 166 | Batch_idx: 100 |  Loss_1: (0.2415) | Acc_1: (90.76%) (11734/12928)\n",
      "Epoch: 166 | Batch_idx: 110 |  Loss_1: (0.2415) | Acc_1: (90.78%) (12898/14208)\n",
      "Epoch: 166 | Batch_idx: 120 |  Loss_1: (0.2405) | Acc_1: (90.79%) (14061/15488)\n",
      "Epoch: 166 | Batch_idx: 130 |  Loss_1: (0.2403) | Acc_1: (90.79%) (15223/16768)\n",
      "Epoch: 166 | Batch_idx: 140 |  Loss_1: (0.2399) | Acc_1: (90.80%) (16388/18048)\n",
      "Epoch: 166 | Batch_idx: 150 |  Loss_1: (0.2404) | Acc_1: (90.78%) (17546/19328)\n",
      "Epoch: 166 | Batch_idx: 160 |  Loss_1: (0.2422) | Acc_1: (90.72%) (18695/20608)\n",
      "Epoch: 166 | Batch_idx: 170 |  Loss_1: (0.2432) | Acc_1: (90.68%) (19847/21888)\n",
      "Epoch: 166 | Batch_idx: 180 |  Loss_1: (0.2433) | Acc_1: (90.69%) (21010/23168)\n",
      "Epoch: 166 | Batch_idx: 190 |  Loss_1: (0.2432) | Acc_1: (90.69%) (22173/24448)\n",
      "Epoch: 166 | Batch_idx: 200 |  Loss_1: (0.2440) | Acc_1: (90.65%) (23323/25728)\n",
      "Epoch: 166 | Batch_idx: 210 |  Loss_1: (0.2449) | Acc_1: (90.60%) (24470/27008)\n",
      "Epoch: 166 | Batch_idx: 220 |  Loss_1: (0.2449) | Acc_1: (90.62%) (25634/28288)\n",
      "Epoch: 166 | Batch_idx: 230 |  Loss_1: (0.2438) | Acc_1: (90.68%) (26811/29568)\n",
      "Epoch: 166 | Batch_idx: 240 |  Loss_1: (0.2442) | Acc_1: (90.66%) (27968/30848)\n",
      "Epoch: 166 | Batch_idx: 250 |  Loss_1: (0.2438) | Acc_1: (90.66%) (29127/32128)\n",
      "Epoch: 166 | Batch_idx: 260 |  Loss_1: (0.2447) | Acc_1: (90.63%) (30279/33408)\n",
      "Epoch: 166 | Batch_idx: 270 |  Loss_1: (0.2446) | Acc_1: (90.64%) (31440/34688)\n",
      "Epoch: 166 | Batch_idx: 280 |  Loss_1: (0.2450) | Acc_1: (90.63%) (32597/35968)\n",
      "Epoch: 166 | Batch_idx: 290 |  Loss_1: (0.2446) | Acc_1: (90.64%) (33761/37248)\n",
      "Epoch: 166 | Batch_idx: 300 |  Loss_1: (0.2434) | Acc_1: (90.71%) (34947/38528)\n",
      "Epoch: 166 | Batch_idx: 310 |  Loss_1: (0.2424) | Acc_1: (90.74%) (36122/39808)\n",
      "Epoch: 166 | Batch_idx: 320 |  Loss_1: (0.2431) | Acc_1: (90.70%) (37268/41088)\n",
      "Epoch: 166 | Batch_idx: 330 |  Loss_1: (0.2429) | Acc_1: (90.71%) (38432/42368)\n",
      "Epoch: 166 | Batch_idx: 340 |  Loss_1: (0.2434) | Acc_1: (90.68%) (39580/43648)\n",
      "Epoch: 166 | Batch_idx: 350 |  Loss_1: (0.2437) | Acc_1: (90.67%) (40734/44928)\n",
      "Epoch: 166 | Batch_idx: 360 |  Loss_1: (0.2437) | Acc_1: (90.68%) (41902/46208)\n",
      "Epoch: 166 | Batch_idx: 370 |  Loss_1: (0.2438) | Acc_1: (90.68%) (43064/47488)\n",
      "Epoch: 166 | Batch_idx: 380 |  Loss_1: (0.2439) | Acc_1: (90.66%) (44214/48768)\n",
      "Epoch: 166 | Batch_idx: 390 |  Loss_1: (0.2434) | Acc_1: (90.67%) (45335/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3181) | Acc: (93.00%) (9300/10000)\n",
      "Epoch: 167 | Batch_idx: 0 |  Loss_1: (0.2960) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 167 | Batch_idx: 10 |  Loss_1: (0.2268) | Acc_1: (91.90%) (1294/1408)\n",
      "Epoch: 167 | Batch_idx: 20 |  Loss_1: (0.2444) | Acc_1: (91.26%) (2453/2688)\n",
      "Epoch: 167 | Batch_idx: 30 |  Loss_1: (0.2304) | Acc_1: (91.51%) (3631/3968)\n",
      "Epoch: 167 | Batch_idx: 40 |  Loss_1: (0.2349) | Acc_1: (91.35%) (4794/5248)\n",
      "Epoch: 167 | Batch_idx: 50 |  Loss_1: (0.2279) | Acc_1: (91.54%) (5976/6528)\n",
      "Epoch: 167 | Batch_idx: 60 |  Loss_1: (0.2324) | Acc_1: (91.34%) (7132/7808)\n",
      "Epoch: 167 | Batch_idx: 70 |  Loss_1: (0.2313) | Acc_1: (91.40%) (8306/9088)\n",
      "Epoch: 167 | Batch_idx: 80 |  Loss_1: (0.2350) | Acc_1: (91.14%) (9449/10368)\n",
      "Epoch: 167 | Batch_idx: 90 |  Loss_1: (0.2390) | Acc_1: (91.00%) (10600/11648)\n",
      "Epoch: 167 | Batch_idx: 100 |  Loss_1: (0.2382) | Acc_1: (91.00%) (11765/12928)\n",
      "Epoch: 167 | Batch_idx: 110 |  Loss_1: (0.2412) | Acc_1: (90.86%) (12910/14208)\n",
      "Epoch: 167 | Batch_idx: 120 |  Loss_1: (0.2425) | Acc_1: (90.82%) (14066/15488)\n",
      "Epoch: 167 | Batch_idx: 130 |  Loss_1: (0.2437) | Acc_1: (90.77%) (15220/16768)\n",
      "Epoch: 167 | Batch_idx: 140 |  Loss_1: (0.2432) | Acc_1: (90.80%) (16388/18048)\n",
      "Epoch: 167 | Batch_idx: 150 |  Loss_1: (0.2454) | Acc_1: (90.72%) (17535/19328)\n",
      "Epoch: 167 | Batch_idx: 160 |  Loss_1: (0.2469) | Acc_1: (90.64%) (18679/20608)\n",
      "Epoch: 167 | Batch_idx: 170 |  Loss_1: (0.2469) | Acc_1: (90.62%) (19836/21888)\n",
      "Epoch: 167 | Batch_idx: 180 |  Loss_1: (0.2447) | Acc_1: (90.68%) (21009/23168)\n",
      "Epoch: 167 | Batch_idx: 190 |  Loss_1: (0.2445) | Acc_1: (90.68%) (22170/24448)\n",
      "Epoch: 167 | Batch_idx: 200 |  Loss_1: (0.2456) | Acc_1: (90.62%) (23315/25728)\n",
      "Epoch: 167 | Batch_idx: 210 |  Loss_1: (0.2466) | Acc_1: (90.58%) (24465/27008)\n",
      "Epoch: 167 | Batch_idx: 220 |  Loss_1: (0.2479) | Acc_1: (90.52%) (25606/28288)\n",
      "Epoch: 167 | Batch_idx: 230 |  Loss_1: (0.2471) | Acc_1: (90.55%) (26773/29568)\n",
      "Epoch: 167 | Batch_idx: 240 |  Loss_1: (0.2468) | Acc_1: (90.56%) (27937/30848)\n",
      "Epoch: 167 | Batch_idx: 250 |  Loss_1: (0.2474) | Acc_1: (90.52%) (29081/32128)\n",
      "Epoch: 167 | Batch_idx: 260 |  Loss_1: (0.2479) | Acc_1: (90.50%) (30235/33408)\n",
      "Epoch: 167 | Batch_idx: 270 |  Loss_1: (0.2474) | Acc_1: (90.55%) (31411/34688)\n",
      "Epoch: 167 | Batch_idx: 280 |  Loss_1: (0.2479) | Acc_1: (90.54%) (32566/35968)\n",
      "Epoch: 167 | Batch_idx: 290 |  Loss_1: (0.2478) | Acc_1: (90.53%) (33719/37248)\n",
      "Epoch: 167 | Batch_idx: 300 |  Loss_1: (0.2493) | Acc_1: (90.47%) (34855/38528)\n",
      "Epoch: 167 | Batch_idx: 310 |  Loss_1: (0.2495) | Acc_1: (90.45%) (36006/39808)\n",
      "Epoch: 167 | Batch_idx: 320 |  Loss_1: (0.2490) | Acc_1: (90.47%) (37171/41088)\n",
      "Epoch: 167 | Batch_idx: 330 |  Loss_1: (0.2493) | Acc_1: (90.45%) (38322/42368)\n",
      "Epoch: 167 | Batch_idx: 340 |  Loss_1: (0.2490) | Acc_1: (90.46%) (39486/43648)\n",
      "Epoch: 167 | Batch_idx: 350 |  Loss_1: (0.2481) | Acc_1: (90.51%) (40665/44928)\n",
      "Epoch: 167 | Batch_idx: 360 |  Loss_1: (0.2491) | Acc_1: (90.48%) (41808/46208)\n",
      "Epoch: 167 | Batch_idx: 370 |  Loss_1: (0.2478) | Acc_1: (90.53%) (42993/47488)\n",
      "Epoch: 167 | Batch_idx: 380 |  Loss_1: (0.2476) | Acc_1: (90.53%) (44151/48768)\n",
      "Epoch: 167 | Batch_idx: 390 |  Loss_1: (0.2479) | Acc_1: (90.53%) (45266/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3182) | Acc: (92.93%) (9293/10000)\n",
      "Epoch: 168 | Batch_idx: 0 |  Loss_1: (0.3350) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 168 | Batch_idx: 10 |  Loss_1: (0.2332) | Acc_1: (91.48%) (1288/1408)\n",
      "Epoch: 168 | Batch_idx: 20 |  Loss_1: (0.2476) | Acc_1: (90.70%) (2438/2688)\n",
      "Epoch: 168 | Batch_idx: 30 |  Loss_1: (0.2399) | Acc_1: (90.98%) (3610/3968)\n",
      "Epoch: 168 | Batch_idx: 40 |  Loss_1: (0.2446) | Acc_1: (90.83%) (4767/5248)\n",
      "Epoch: 168 | Batch_idx: 50 |  Loss_1: (0.2408) | Acc_1: (90.99%) (5940/6528)\n",
      "Epoch: 168 | Batch_idx: 60 |  Loss_1: (0.2435) | Acc_1: (90.87%) (7095/7808)\n",
      "Epoch: 168 | Batch_idx: 70 |  Loss_1: (0.2493) | Acc_1: (90.59%) (8233/9088)\n",
      "Epoch: 168 | Batch_idx: 80 |  Loss_1: (0.2501) | Acc_1: (90.50%) (9383/10368)\n",
      "Epoch: 168 | Batch_idx: 90 |  Loss_1: (0.2484) | Acc_1: (90.56%) (10548/11648)\n",
      "Epoch: 168 | Batch_idx: 100 |  Loss_1: (0.2469) | Acc_1: (90.59%) (11711/12928)\n",
      "Epoch: 168 | Batch_idx: 110 |  Loss_1: (0.2480) | Acc_1: (90.48%) (12856/14208)\n",
      "Epoch: 168 | Batch_idx: 120 |  Loss_1: (0.2490) | Acc_1: (90.44%) (14007/15488)\n",
      "Epoch: 168 | Batch_idx: 130 |  Loss_1: (0.2493) | Acc_1: (90.43%) (15163/16768)\n",
      "Epoch: 168 | Batch_idx: 140 |  Loss_1: (0.2484) | Acc_1: (90.49%) (16331/18048)\n",
      "Epoch: 168 | Batch_idx: 150 |  Loss_1: (0.2493) | Acc_1: (90.44%) (17481/19328)\n",
      "Epoch: 168 | Batch_idx: 160 |  Loss_1: (0.2480) | Acc_1: (90.49%) (18649/20608)\n",
      "Epoch: 168 | Batch_idx: 170 |  Loss_1: (0.2482) | Acc_1: (90.49%) (19806/21888)\n",
      "Epoch: 168 | Batch_idx: 180 |  Loss_1: (0.2472) | Acc_1: (90.51%) (20970/23168)\n",
      "Epoch: 168 | Batch_idx: 190 |  Loss_1: (0.2462) | Acc_1: (90.55%) (22138/24448)\n",
      "Epoch: 168 | Batch_idx: 200 |  Loss_1: (0.2468) | Acc_1: (90.52%) (23288/25728)\n",
      "Epoch: 168 | Batch_idx: 210 |  Loss_1: (0.2456) | Acc_1: (90.57%) (24461/27008)\n",
      "Epoch: 168 | Batch_idx: 220 |  Loss_1: (0.2461) | Acc_1: (90.55%) (25614/28288)\n",
      "Epoch: 168 | Batch_idx: 230 |  Loss_1: (0.2446) | Acc_1: (90.58%) (26784/29568)\n",
      "Epoch: 168 | Batch_idx: 240 |  Loss_1: (0.2447) | Acc_1: (90.57%) (27940/30848)\n",
      "Epoch: 168 | Batch_idx: 250 |  Loss_1: (0.2444) | Acc_1: (90.58%) (29103/32128)\n",
      "Epoch: 168 | Batch_idx: 260 |  Loss_1: (0.2448) | Acc_1: (90.57%) (30257/33408)\n",
      "Epoch: 168 | Batch_idx: 270 |  Loss_1: (0.2450) | Acc_1: (90.55%) (31410/34688)\n",
      "Epoch: 168 | Batch_idx: 280 |  Loss_1: (0.2447) | Acc_1: (90.57%) (32576/35968)\n",
      "Epoch: 168 | Batch_idx: 290 |  Loss_1: (0.2445) | Acc_1: (90.58%) (33740/37248)\n",
      "Epoch: 168 | Batch_idx: 300 |  Loss_1: (0.2449) | Acc_1: (90.57%) (34894/38528)\n",
      "Epoch: 168 | Batch_idx: 310 |  Loss_1: (0.2451) | Acc_1: (90.56%) (36050/39808)\n",
      "Epoch: 168 | Batch_idx: 320 |  Loss_1: (0.2453) | Acc_1: (90.56%) (37209/41088)\n",
      "Epoch: 168 | Batch_idx: 330 |  Loss_1: (0.2458) | Acc_1: (90.53%) (38354/42368)\n",
      "Epoch: 168 | Batch_idx: 340 |  Loss_1: (0.2448) | Acc_1: (90.57%) (39534/43648)\n",
      "Epoch: 168 | Batch_idx: 350 |  Loss_1: (0.2449) | Acc_1: (90.59%) (40699/44928)\n",
      "Epoch: 168 | Batch_idx: 360 |  Loss_1: (0.2441) | Acc_1: (90.61%) (41870/46208)\n",
      "Epoch: 168 | Batch_idx: 370 |  Loss_1: (0.2445) | Acc_1: (90.60%) (43025/47488)\n",
      "Epoch: 168 | Batch_idx: 380 |  Loss_1: (0.2448) | Acc_1: (90.59%) (44178/48768)\n",
      "Epoch: 168 | Batch_idx: 390 |  Loss_1: (0.2444) | Acc_1: (90.60%) (45302/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3061) | Acc: (92.87%) (9287/10000)\n",
      "Epoch: 169 | Batch_idx: 0 |  Loss_1: (0.1563) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 169 | Batch_idx: 10 |  Loss_1: (0.2494) | Acc_1: (90.84%) (1279/1408)\n",
      "Epoch: 169 | Batch_idx: 20 |  Loss_1: (0.2286) | Acc_1: (91.67%) (2464/2688)\n",
      "Epoch: 169 | Batch_idx: 30 |  Loss_1: (0.2430) | Acc_1: (90.88%) (3606/3968)\n",
      "Epoch: 169 | Batch_idx: 40 |  Loss_1: (0.2380) | Acc_1: (91.03%) (4777/5248)\n",
      "Epoch: 169 | Batch_idx: 50 |  Loss_1: (0.2402) | Acc_1: (91.01%) (5941/6528)\n",
      "Epoch: 169 | Batch_idx: 60 |  Loss_1: (0.2390) | Acc_1: (90.92%) (7099/7808)\n",
      "Epoch: 169 | Batch_idx: 70 |  Loss_1: (0.2373) | Acc_1: (90.91%) (8262/9088)\n",
      "Epoch: 169 | Batch_idx: 80 |  Loss_1: (0.2366) | Acc_1: (90.95%) (9430/10368)\n",
      "Epoch: 169 | Batch_idx: 90 |  Loss_1: (0.2386) | Acc_1: (90.86%) (10583/11648)\n",
      "Epoch: 169 | Batch_idx: 100 |  Loss_1: (0.2361) | Acc_1: (90.93%) (11756/12928)\n",
      "Epoch: 169 | Batch_idx: 110 |  Loss_1: (0.2364) | Acc_1: (90.87%) (12911/14208)\n",
      "Epoch: 169 | Batch_idx: 120 |  Loss_1: (0.2367) | Acc_1: (90.84%) (14069/15488)\n",
      "Epoch: 169 | Batch_idx: 130 |  Loss_1: (0.2377) | Acc_1: (90.81%) (15227/16768)\n",
      "Epoch: 169 | Batch_idx: 140 |  Loss_1: (0.2375) | Acc_1: (90.82%) (16392/18048)\n",
      "Epoch: 169 | Batch_idx: 150 |  Loss_1: (0.2389) | Acc_1: (90.80%) (17549/19328)\n",
      "Epoch: 169 | Batch_idx: 160 |  Loss_1: (0.2378) | Acc_1: (90.87%) (18726/20608)\n",
      "Epoch: 169 | Batch_idx: 170 |  Loss_1: (0.2382) | Acc_1: (90.84%) (19882/21888)\n",
      "Epoch: 169 | Batch_idx: 180 |  Loss_1: (0.2379) | Acc_1: (90.84%) (21045/23168)\n",
      "Epoch: 169 | Batch_idx: 190 |  Loss_1: (0.2378) | Acc_1: (90.83%) (22207/24448)\n",
      "Epoch: 169 | Batch_idx: 200 |  Loss_1: (0.2392) | Acc_1: (90.78%) (23356/25728)\n",
      "Epoch: 169 | Batch_idx: 210 |  Loss_1: (0.2387) | Acc_1: (90.78%) (24518/27008)\n",
      "Epoch: 169 | Batch_idx: 220 |  Loss_1: (0.2390) | Acc_1: (90.77%) (25678/28288)\n",
      "Epoch: 169 | Batch_idx: 230 |  Loss_1: (0.2398) | Acc_1: (90.74%) (26829/29568)\n",
      "Epoch: 169 | Batch_idx: 240 |  Loss_1: (0.2381) | Acc_1: (90.79%) (28007/30848)\n",
      "Epoch: 169 | Batch_idx: 250 |  Loss_1: (0.2391) | Acc_1: (90.76%) (29159/32128)\n",
      "Epoch: 169 | Batch_idx: 260 |  Loss_1: (0.2388) | Acc_1: (90.77%) (30326/33408)\n",
      "Epoch: 169 | Batch_idx: 270 |  Loss_1: (0.2394) | Acc_1: (90.75%) (31480/34688)\n",
      "Epoch: 169 | Batch_idx: 280 |  Loss_1: (0.2407) | Acc_1: (90.73%) (32633/35968)\n",
      "Epoch: 169 | Batch_idx: 290 |  Loss_1: (0.2405) | Acc_1: (90.74%) (33797/37248)\n",
      "Epoch: 169 | Batch_idx: 300 |  Loss_1: (0.2414) | Acc_1: (90.70%) (34945/38528)\n",
      "Epoch: 169 | Batch_idx: 310 |  Loss_1: (0.2413) | Acc_1: (90.72%) (36112/39808)\n",
      "Epoch: 169 | Batch_idx: 320 |  Loss_1: (0.2405) | Acc_1: (90.74%) (37282/41088)\n",
      "Epoch: 169 | Batch_idx: 330 |  Loss_1: (0.2404) | Acc_1: (90.75%) (38451/42368)\n",
      "Epoch: 169 | Batch_idx: 340 |  Loss_1: (0.2407) | Acc_1: (90.75%) (39609/43648)\n",
      "Epoch: 169 | Batch_idx: 350 |  Loss_1: (0.2417) | Acc_1: (90.70%) (40749/44928)\n",
      "Epoch: 169 | Batch_idx: 360 |  Loss_1: (0.2412) | Acc_1: (90.72%) (41920/46208)\n",
      "Epoch: 169 | Batch_idx: 370 |  Loss_1: (0.2422) | Acc_1: (90.68%) (43064/47488)\n",
      "Epoch: 169 | Batch_idx: 380 |  Loss_1: (0.2434) | Acc_1: (90.64%) (44203/48768)\n",
      "Epoch: 169 | Batch_idx: 390 |  Loss_1: (0.2437) | Acc_1: (90.62%) (45311/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3345) | Acc: (92.78%) (9278/10000)\n",
      "Epoch: 170 | Batch_idx: 0 |  Loss_1: (0.1486) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 170 | Batch_idx: 10 |  Loss_1: (0.2106) | Acc_1: (91.97%) (1295/1408)\n",
      "Epoch: 170 | Batch_idx: 20 |  Loss_1: (0.2243) | Acc_1: (91.33%) (2455/2688)\n",
      "Epoch: 170 | Batch_idx: 30 |  Loss_1: (0.2252) | Acc_1: (91.20%) (3619/3968)\n",
      "Epoch: 170 | Batch_idx: 40 |  Loss_1: (0.2307) | Acc_1: (90.91%) (4771/5248)\n",
      "Epoch: 170 | Batch_idx: 50 |  Loss_1: (0.2290) | Acc_1: (90.98%) (5939/6528)\n",
      "Epoch: 170 | Batch_idx: 60 |  Loss_1: (0.2276) | Acc_1: (91.11%) (7114/7808)\n",
      "Epoch: 170 | Batch_idx: 70 |  Loss_1: (0.2284) | Acc_1: (91.04%) (8274/9088)\n",
      "Epoch: 170 | Batch_idx: 80 |  Loss_1: (0.2309) | Acc_1: (91.05%) (9440/10368)\n",
      "Epoch: 170 | Batch_idx: 90 |  Loss_1: (0.2337) | Acc_1: (91.01%) (10601/11648)\n",
      "Epoch: 170 | Batch_idx: 100 |  Loss_1: (0.2326) | Acc_1: (91.03%) (11769/12928)\n",
      "Epoch: 170 | Batch_idx: 110 |  Loss_1: (0.2378) | Acc_1: (90.88%) (12912/14208)\n",
      "Epoch: 170 | Batch_idx: 120 |  Loss_1: (0.2380) | Acc_1: (90.89%) (14077/15488)\n",
      "Epoch: 170 | Batch_idx: 130 |  Loss_1: (0.2398) | Acc_1: (90.79%) (15224/16768)\n",
      "Epoch: 170 | Batch_idx: 140 |  Loss_1: (0.2412) | Acc_1: (90.74%) (16377/18048)\n",
      "Epoch: 170 | Batch_idx: 150 |  Loss_1: (0.2423) | Acc_1: (90.67%) (17524/19328)\n",
      "Epoch: 170 | Batch_idx: 160 |  Loss_1: (0.2427) | Acc_1: (90.66%) (18683/20608)\n",
      "Epoch: 170 | Batch_idx: 170 |  Loss_1: (0.2431) | Acc_1: (90.64%) (19840/21888)\n",
      "Epoch: 170 | Batch_idx: 180 |  Loss_1: (0.2451) | Acc_1: (90.59%) (20989/23168)\n",
      "Epoch: 170 | Batch_idx: 190 |  Loss_1: (0.2455) | Acc_1: (90.57%) (22142/24448)\n",
      "Epoch: 170 | Batch_idx: 200 |  Loss_1: (0.2452) | Acc_1: (90.61%) (23311/25728)\n",
      "Epoch: 170 | Batch_idx: 210 |  Loss_1: (0.2451) | Acc_1: (90.60%) (24469/27008)\n",
      "Epoch: 170 | Batch_idx: 220 |  Loss_1: (0.2457) | Acc_1: (90.55%) (25614/28288)\n",
      "Epoch: 170 | Batch_idx: 230 |  Loss_1: (0.2457) | Acc_1: (90.55%) (26774/29568)\n",
      "Epoch: 170 | Batch_idx: 240 |  Loss_1: (0.2450) | Acc_1: (90.56%) (27936/30848)\n",
      "Epoch: 170 | Batch_idx: 250 |  Loss_1: (0.2451) | Acc_1: (90.56%) (29096/32128)\n",
      "Epoch: 170 | Batch_idx: 260 |  Loss_1: (0.2444) | Acc_1: (90.59%) (30264/33408)\n",
      "Epoch: 170 | Batch_idx: 270 |  Loss_1: (0.2449) | Acc_1: (90.57%) (31416/34688)\n",
      "Epoch: 170 | Batch_idx: 280 |  Loss_1: (0.2452) | Acc_1: (90.57%) (32576/35968)\n",
      "Epoch: 170 | Batch_idx: 290 |  Loss_1: (0.2448) | Acc_1: (90.57%) (33736/37248)\n",
      "Epoch: 170 | Batch_idx: 300 |  Loss_1: (0.2441) | Acc_1: (90.60%) (34907/38528)\n",
      "Epoch: 170 | Batch_idx: 310 |  Loss_1: (0.2441) | Acc_1: (90.60%) (36068/39808)\n",
      "Epoch: 170 | Batch_idx: 320 |  Loss_1: (0.2449) | Acc_1: (90.57%) (37214/41088)\n",
      "Epoch: 170 | Batch_idx: 330 |  Loss_1: (0.2459) | Acc_1: (90.52%) (38352/42368)\n",
      "Epoch: 170 | Batch_idx: 340 |  Loss_1: (0.2462) | Acc_1: (90.51%) (39504/43648)\n",
      "Epoch: 170 | Batch_idx: 350 |  Loss_1: (0.2460) | Acc_1: (90.52%) (40668/44928)\n",
      "Epoch: 170 | Batch_idx: 360 |  Loss_1: (0.2460) | Acc_1: (90.52%) (41827/46208)\n",
      "Epoch: 170 | Batch_idx: 370 |  Loss_1: (0.2461) | Acc_1: (90.53%) (42990/47488)\n",
      "Epoch: 170 | Batch_idx: 380 |  Loss_1: (0.2456) | Acc_1: (90.55%) (44158/48768)\n",
      "Epoch: 170 | Batch_idx: 390 |  Loss_1: (0.2457) | Acc_1: (90.55%) (45276/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2969) | Acc: (93.26%) (9326/10000)\n",
      "Epoch: 171 | Batch_idx: 0 |  Loss_1: (0.2750) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 171 | Batch_idx: 10 |  Loss_1: (0.2226) | Acc_1: (91.41%) (1287/1408)\n",
      "Epoch: 171 | Batch_idx: 20 |  Loss_1: (0.2275) | Acc_1: (91.48%) (2459/2688)\n",
      "Epoch: 171 | Batch_idx: 30 |  Loss_1: (0.2297) | Acc_1: (91.28%) (3622/3968)\n",
      "Epoch: 171 | Batch_idx: 40 |  Loss_1: (0.2298) | Acc_1: (91.16%) (4784/5248)\n",
      "Epoch: 171 | Batch_idx: 50 |  Loss_1: (0.2281) | Acc_1: (91.24%) (5956/6528)\n",
      "Epoch: 171 | Batch_idx: 60 |  Loss_1: (0.2314) | Acc_1: (91.05%) (7109/7808)\n",
      "Epoch: 171 | Batch_idx: 70 |  Loss_1: (0.2257) | Acc_1: (91.30%) (8297/9088)\n",
      "Epoch: 171 | Batch_idx: 80 |  Loss_1: (0.2283) | Acc_1: (91.18%) (9454/10368)\n",
      "Epoch: 171 | Batch_idx: 90 |  Loss_1: (0.2301) | Acc_1: (91.12%) (10614/11648)\n",
      "Epoch: 171 | Batch_idx: 100 |  Loss_1: (0.2273) | Acc_1: (91.26%) (11798/12928)\n",
      "Epoch: 171 | Batch_idx: 110 |  Loss_1: (0.2286) | Acc_1: (91.17%) (12953/14208)\n",
      "Epoch: 171 | Batch_idx: 120 |  Loss_1: (0.2304) | Acc_1: (91.11%) (14111/15488)\n",
      "Epoch: 171 | Batch_idx: 130 |  Loss_1: (0.2317) | Acc_1: (91.07%) (15270/16768)\n",
      "Epoch: 171 | Batch_idx: 140 |  Loss_1: (0.2313) | Acc_1: (91.08%) (16438/18048)\n",
      "Epoch: 171 | Batch_idx: 150 |  Loss_1: (0.2306) | Acc_1: (91.14%) (17615/19328)\n",
      "Epoch: 171 | Batch_idx: 160 |  Loss_1: (0.2313) | Acc_1: (91.12%) (18777/20608)\n",
      "Epoch: 171 | Batch_idx: 170 |  Loss_1: (0.2340) | Acc_1: (91.02%) (19922/21888)\n",
      "Epoch: 171 | Batch_idx: 180 |  Loss_1: (0.2352) | Acc_1: (90.98%) (21078/23168)\n",
      "Epoch: 171 | Batch_idx: 190 |  Loss_1: (0.2361) | Acc_1: (90.93%) (22230/24448)\n",
      "Epoch: 171 | Batch_idx: 200 |  Loss_1: (0.2375) | Acc_1: (90.85%) (23375/25728)\n",
      "Epoch: 171 | Batch_idx: 210 |  Loss_1: (0.2397) | Acc_1: (90.81%) (24526/27008)\n",
      "Epoch: 171 | Batch_idx: 220 |  Loss_1: (0.2390) | Acc_1: (90.82%) (25691/28288)\n",
      "Epoch: 171 | Batch_idx: 230 |  Loss_1: (0.2403) | Acc_1: (90.76%) (26835/29568)\n",
      "Epoch: 171 | Batch_idx: 240 |  Loss_1: (0.2404) | Acc_1: (90.75%) (27996/30848)\n",
      "Epoch: 171 | Batch_idx: 250 |  Loss_1: (0.2410) | Acc_1: (90.74%) (29152/32128)\n",
      "Epoch: 171 | Batch_idx: 260 |  Loss_1: (0.2406) | Acc_1: (90.74%) (30316/33408)\n",
      "Epoch: 171 | Batch_idx: 270 |  Loss_1: (0.2409) | Acc_1: (90.73%) (31474/34688)\n",
      "Epoch: 171 | Batch_idx: 280 |  Loss_1: (0.2405) | Acc_1: (90.74%) (32639/35968)\n",
      "Epoch: 171 | Batch_idx: 290 |  Loss_1: (0.2400) | Acc_1: (90.76%) (33808/37248)\n",
      "Epoch: 171 | Batch_idx: 300 |  Loss_1: (0.2403) | Acc_1: (90.74%) (34962/38528)\n",
      "Epoch: 171 | Batch_idx: 310 |  Loss_1: (0.2410) | Acc_1: (90.70%) (36107/39808)\n",
      "Epoch: 171 | Batch_idx: 320 |  Loss_1: (0.2408) | Acc_1: (90.69%) (37263/41088)\n",
      "Epoch: 171 | Batch_idx: 330 |  Loss_1: (0.2404) | Acc_1: (90.71%) (38432/42368)\n",
      "Epoch: 171 | Batch_idx: 340 |  Loss_1: (0.2410) | Acc_1: (90.69%) (39584/43648)\n",
      "Epoch: 171 | Batch_idx: 350 |  Loss_1: (0.2418) | Acc_1: (90.65%) (40726/44928)\n",
      "Epoch: 171 | Batch_idx: 360 |  Loss_1: (0.2417) | Acc_1: (90.65%) (41886/46208)\n",
      "Epoch: 171 | Batch_idx: 370 |  Loss_1: (0.2425) | Acc_1: (90.61%) (43027/47488)\n",
      "Epoch: 171 | Batch_idx: 380 |  Loss_1: (0.2428) | Acc_1: (90.60%) (44184/48768)\n",
      "Epoch: 171 | Batch_idx: 390 |  Loss_1: (0.2435) | Acc_1: (90.58%) (45289/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3159) | Acc: (93.16%) (9316/10000)\n",
      "Epoch: 172 | Batch_idx: 0 |  Loss_1: (0.1890) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 172 | Batch_idx: 10 |  Loss_1: (0.2269) | Acc_1: (91.76%) (1292/1408)\n",
      "Epoch: 172 | Batch_idx: 20 |  Loss_1: (0.2413) | Acc_1: (90.92%) (2444/2688)\n",
      "Epoch: 172 | Batch_idx: 30 |  Loss_1: (0.2436) | Acc_1: (90.95%) (3609/3968)\n",
      "Epoch: 172 | Batch_idx: 40 |  Loss_1: (0.2428) | Acc_1: (90.95%) (4773/5248)\n",
      "Epoch: 172 | Batch_idx: 50 |  Loss_1: (0.2414) | Acc_1: (90.93%) (5936/6528)\n",
      "Epoch: 172 | Batch_idx: 60 |  Loss_1: (0.2403) | Acc_1: (90.91%) (7098/7808)\n",
      "Epoch: 172 | Batch_idx: 70 |  Loss_1: (0.2417) | Acc_1: (90.87%) (8258/9088)\n",
      "Epoch: 172 | Batch_idx: 80 |  Loss_1: (0.2431) | Acc_1: (90.78%) (9412/10368)\n",
      "Epoch: 172 | Batch_idx: 90 |  Loss_1: (0.2456) | Acc_1: (90.64%) (10558/11648)\n",
      "Epoch: 172 | Batch_idx: 100 |  Loss_1: (0.2449) | Acc_1: (90.69%) (11725/12928)\n",
      "Epoch: 172 | Batch_idx: 110 |  Loss_1: (0.2450) | Acc_1: (90.67%) (12882/14208)\n",
      "Epoch: 172 | Batch_idx: 120 |  Loss_1: (0.2459) | Acc_1: (90.61%) (14034/15488)\n",
      "Epoch: 172 | Batch_idx: 130 |  Loss_1: (0.2458) | Acc_1: (90.62%) (15195/16768)\n",
      "Epoch: 172 | Batch_idx: 140 |  Loss_1: (0.2451) | Acc_1: (90.64%) (16359/18048)\n",
      "Epoch: 172 | Batch_idx: 150 |  Loss_1: (0.2436) | Acc_1: (90.68%) (17527/19328)\n",
      "Epoch: 172 | Batch_idx: 160 |  Loss_1: (0.2423) | Acc_1: (90.72%) (18696/20608)\n",
      "Epoch: 172 | Batch_idx: 170 |  Loss_1: (0.2430) | Acc_1: (90.76%) (19865/21888)\n",
      "Epoch: 172 | Batch_idx: 180 |  Loss_1: (0.2418) | Acc_1: (90.81%) (21040/23168)\n",
      "Epoch: 172 | Batch_idx: 190 |  Loss_1: (0.2409) | Acc_1: (90.84%) (22208/24448)\n",
      "Epoch: 172 | Batch_idx: 200 |  Loss_1: (0.2417) | Acc_1: (90.82%) (23365/25728)\n",
      "Epoch: 172 | Batch_idx: 210 |  Loss_1: (0.2410) | Acc_1: (90.83%) (24531/27008)\n",
      "Epoch: 172 | Batch_idx: 220 |  Loss_1: (0.2422) | Acc_1: (90.79%) (25683/28288)\n",
      "Epoch: 172 | Batch_idx: 230 |  Loss_1: (0.2446) | Acc_1: (90.69%) (26816/29568)\n",
      "Epoch: 172 | Batch_idx: 240 |  Loss_1: (0.2451) | Acc_1: (90.66%) (27967/30848)\n",
      "Epoch: 172 | Batch_idx: 250 |  Loss_1: (0.2452) | Acc_1: (90.63%) (29119/32128)\n",
      "Epoch: 172 | Batch_idx: 260 |  Loss_1: (0.2457) | Acc_1: (90.63%) (30279/33408)\n",
      "Epoch: 172 | Batch_idx: 270 |  Loss_1: (0.2451) | Acc_1: (90.67%) (31453/34688)\n",
      "Epoch: 172 | Batch_idx: 280 |  Loss_1: (0.2460) | Acc_1: (90.64%) (32600/35968)\n",
      "Epoch: 172 | Batch_idx: 290 |  Loss_1: (0.2463) | Acc_1: (90.61%) (33750/37248)\n",
      "Epoch: 172 | Batch_idx: 300 |  Loss_1: (0.2464) | Acc_1: (90.60%) (34906/38528)\n",
      "Epoch: 172 | Batch_idx: 310 |  Loss_1: (0.2456) | Acc_1: (90.64%) (36080/39808)\n",
      "Epoch: 172 | Batch_idx: 320 |  Loss_1: (0.2445) | Acc_1: (90.67%) (37253/41088)\n",
      "Epoch: 172 | Batch_idx: 330 |  Loss_1: (0.2447) | Acc_1: (90.65%) (38405/42368)\n",
      "Epoch: 172 | Batch_idx: 340 |  Loss_1: (0.2455) | Acc_1: (90.62%) (39554/43648)\n",
      "Epoch: 172 | Batch_idx: 350 |  Loss_1: (0.2457) | Acc_1: (90.60%) (40703/44928)\n",
      "Epoch: 172 | Batch_idx: 360 |  Loss_1: (0.2459) | Acc_1: (90.58%) (41854/46208)\n",
      "Epoch: 172 | Batch_idx: 370 |  Loss_1: (0.2459) | Acc_1: (90.58%) (43013/47488)\n",
      "Epoch: 172 | Batch_idx: 380 |  Loss_1: (0.2465) | Acc_1: (90.56%) (44165/48768)\n",
      "Epoch: 172 | Batch_idx: 390 |  Loss_1: (0.2468) | Acc_1: (90.56%) (45278/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2926) | Acc: (92.94%) (9294/10000)\n",
      "Epoch: 173 | Batch_idx: 0 |  Loss_1: (0.2174) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 173 | Batch_idx: 10 |  Loss_1: (0.2335) | Acc_1: (91.26%) (1285/1408)\n",
      "Epoch: 173 | Batch_idx: 20 |  Loss_1: (0.2371) | Acc_1: (91.22%) (2452/2688)\n",
      "Epoch: 173 | Batch_idx: 30 |  Loss_1: (0.2472) | Acc_1: (90.80%) (3603/3968)\n",
      "Epoch: 173 | Batch_idx: 40 |  Loss_1: (0.2449) | Acc_1: (90.91%) (4771/5248)\n",
      "Epoch: 173 | Batch_idx: 50 |  Loss_1: (0.2368) | Acc_1: (91.18%) (5952/6528)\n",
      "Epoch: 173 | Batch_idx: 60 |  Loss_1: (0.2377) | Acc_1: (91.07%) (7111/7808)\n",
      "Epoch: 173 | Batch_idx: 70 |  Loss_1: (0.2359) | Acc_1: (91.14%) (8283/9088)\n",
      "Epoch: 173 | Batch_idx: 80 |  Loss_1: (0.2354) | Acc_1: (91.17%) (9453/10368)\n",
      "Epoch: 173 | Batch_idx: 90 |  Loss_1: (0.2346) | Acc_1: (91.12%) (10614/11648)\n",
      "Epoch: 173 | Batch_idx: 100 |  Loss_1: (0.2339) | Acc_1: (91.12%) (11780/12928)\n",
      "Epoch: 173 | Batch_idx: 110 |  Loss_1: (0.2344) | Acc_1: (91.14%) (12949/14208)\n",
      "Epoch: 173 | Batch_idx: 120 |  Loss_1: (0.2376) | Acc_1: (91.02%) (14097/15488)\n",
      "Epoch: 173 | Batch_idx: 130 |  Loss_1: (0.2366) | Acc_1: (91.06%) (15269/16768)\n",
      "Epoch: 173 | Batch_idx: 140 |  Loss_1: (0.2405) | Acc_1: (90.89%) (16404/18048)\n",
      "Epoch: 173 | Batch_idx: 150 |  Loss_1: (0.2408) | Acc_1: (90.89%) (17567/19328)\n",
      "Epoch: 173 | Batch_idx: 160 |  Loss_1: (0.2405) | Acc_1: (90.89%) (18730/20608)\n",
      "Epoch: 173 | Batch_idx: 170 |  Loss_1: (0.2405) | Acc_1: (90.89%) (19895/21888)\n",
      "Epoch: 173 | Batch_idx: 180 |  Loss_1: (0.2414) | Acc_1: (90.85%) (21049/23168)\n",
      "Epoch: 173 | Batch_idx: 190 |  Loss_1: (0.2429) | Acc_1: (90.79%) (22196/24448)\n",
      "Epoch: 173 | Batch_idx: 200 |  Loss_1: (0.2438) | Acc_1: (90.75%) (23349/25728)\n",
      "Epoch: 173 | Batch_idx: 210 |  Loss_1: (0.2445) | Acc_1: (90.72%) (24502/27008)\n",
      "Epoch: 173 | Batch_idx: 220 |  Loss_1: (0.2436) | Acc_1: (90.75%) (25671/28288)\n",
      "Epoch: 173 | Batch_idx: 230 |  Loss_1: (0.2434) | Acc_1: (90.77%) (26839/29568)\n",
      "Epoch: 173 | Batch_idx: 240 |  Loss_1: (0.2441) | Acc_1: (90.74%) (27990/30848)\n",
      "Epoch: 173 | Batch_idx: 250 |  Loss_1: (0.2440) | Acc_1: (90.72%) (29146/32128)\n",
      "Epoch: 173 | Batch_idx: 260 |  Loss_1: (0.2441) | Acc_1: (90.69%) (30299/33408)\n",
      "Epoch: 173 | Batch_idx: 270 |  Loss_1: (0.2436) | Acc_1: (90.71%) (31467/34688)\n",
      "Epoch: 173 | Batch_idx: 280 |  Loss_1: (0.2435) | Acc_1: (90.69%) (32619/35968)\n",
      "Epoch: 173 | Batch_idx: 290 |  Loss_1: (0.2439) | Acc_1: (90.69%) (33780/37248)\n",
      "Epoch: 173 | Batch_idx: 300 |  Loss_1: (0.2446) | Acc_1: (90.65%) (34927/38528)\n",
      "Epoch: 173 | Batch_idx: 310 |  Loss_1: (0.2441) | Acc_1: (90.66%) (36090/39808)\n",
      "Epoch: 173 | Batch_idx: 320 |  Loss_1: (0.2438) | Acc_1: (90.67%) (37256/41088)\n",
      "Epoch: 173 | Batch_idx: 330 |  Loss_1: (0.2438) | Acc_1: (90.66%) (38411/42368)\n",
      "Epoch: 173 | Batch_idx: 340 |  Loss_1: (0.2437) | Acc_1: (90.67%) (39576/43648)\n",
      "Epoch: 173 | Batch_idx: 350 |  Loss_1: (0.2449) | Acc_1: (90.63%) (40717/44928)\n",
      "Epoch: 173 | Batch_idx: 360 |  Loss_1: (0.2447) | Acc_1: (90.62%) (41875/46208)\n",
      "Epoch: 173 | Batch_idx: 370 |  Loss_1: (0.2445) | Acc_1: (90.63%) (43038/47488)\n",
      "Epoch: 173 | Batch_idx: 380 |  Loss_1: (0.2447) | Acc_1: (90.61%) (44187/48768)\n",
      "Epoch: 173 | Batch_idx: 390 |  Loss_1: (0.2448) | Acc_1: (90.60%) (45302/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3163) | Acc: (93.08%) (9308/10000)\n",
      "Epoch: 174 | Batch_idx: 0 |  Loss_1: (0.2027) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 174 | Batch_idx: 10 |  Loss_1: (0.2419) | Acc_1: (90.77%) (1278/1408)\n",
      "Epoch: 174 | Batch_idx: 20 |  Loss_1: (0.2501) | Acc_1: (90.14%) (2423/2688)\n",
      "Epoch: 174 | Batch_idx: 30 |  Loss_1: (0.2570) | Acc_1: (89.99%) (3571/3968)\n",
      "Epoch: 174 | Batch_idx: 40 |  Loss_1: (0.2503) | Acc_1: (90.42%) (4745/5248)\n",
      "Epoch: 174 | Batch_idx: 50 |  Loss_1: (0.2415) | Acc_1: (90.73%) (5923/6528)\n",
      "Epoch: 174 | Batch_idx: 60 |  Loss_1: (0.2427) | Acc_1: (90.75%) (7086/7808)\n",
      "Epoch: 174 | Batch_idx: 70 |  Loss_1: (0.2442) | Acc_1: (90.67%) (8240/9088)\n",
      "Epoch: 174 | Batch_idx: 80 |  Loss_1: (0.2475) | Acc_1: (90.56%) (9389/10368)\n",
      "Epoch: 174 | Batch_idx: 90 |  Loss_1: (0.2498) | Acc_1: (90.45%) (10536/11648)\n",
      "Epoch: 174 | Batch_idx: 100 |  Loss_1: (0.2488) | Acc_1: (90.52%) (11702/12928)\n",
      "Epoch: 174 | Batch_idx: 110 |  Loss_1: (0.2490) | Acc_1: (90.49%) (12857/14208)\n",
      "Epoch: 174 | Batch_idx: 120 |  Loss_1: (0.2476) | Acc_1: (90.52%) (14020/15488)\n",
      "Epoch: 174 | Batch_idx: 130 |  Loss_1: (0.2502) | Acc_1: (90.47%) (15170/16768)\n",
      "Epoch: 174 | Batch_idx: 140 |  Loss_1: (0.2496) | Acc_1: (90.47%) (16328/18048)\n",
      "Epoch: 174 | Batch_idx: 150 |  Loss_1: (0.2500) | Acc_1: (90.43%) (17478/19328)\n",
      "Epoch: 174 | Batch_idx: 160 |  Loss_1: (0.2481) | Acc_1: (90.46%) (18642/20608)\n",
      "Epoch: 174 | Batch_idx: 170 |  Loss_1: (0.2473) | Acc_1: (90.47%) (19803/21888)\n",
      "Epoch: 174 | Batch_idx: 180 |  Loss_1: (0.2476) | Acc_1: (90.45%) (20955/23168)\n",
      "Epoch: 174 | Batch_idx: 190 |  Loss_1: (0.2481) | Acc_1: (90.44%) (22110/24448)\n",
      "Epoch: 174 | Batch_idx: 200 |  Loss_1: (0.2467) | Acc_1: (90.53%) (23291/25728)\n",
      "Epoch: 174 | Batch_idx: 210 |  Loss_1: (0.2459) | Acc_1: (90.56%) (24459/27008)\n",
      "Epoch: 174 | Batch_idx: 220 |  Loss_1: (0.2460) | Acc_1: (90.54%) (25612/28288)\n",
      "Epoch: 174 | Batch_idx: 230 |  Loss_1: (0.2449) | Acc_1: (90.58%) (26783/29568)\n",
      "Epoch: 174 | Batch_idx: 240 |  Loss_1: (0.2437) | Acc_1: (90.62%) (27954/30848)\n",
      "Epoch: 174 | Batch_idx: 250 |  Loss_1: (0.2436) | Acc_1: (90.61%) (29111/32128)\n",
      "Epoch: 174 | Batch_idx: 260 |  Loss_1: (0.2435) | Acc_1: (90.59%) (30264/33408)\n",
      "Epoch: 174 | Batch_idx: 270 |  Loss_1: (0.2439) | Acc_1: (90.58%) (31420/34688)\n",
      "Epoch: 174 | Batch_idx: 280 |  Loss_1: (0.2433) | Acc_1: (90.59%) (32584/35968)\n",
      "Epoch: 174 | Batch_idx: 290 |  Loss_1: (0.2429) | Acc_1: (90.61%) (33749/37248)\n",
      "Epoch: 174 | Batch_idx: 300 |  Loss_1: (0.2430) | Acc_1: (90.59%) (34903/38528)\n",
      "Epoch: 174 | Batch_idx: 310 |  Loss_1: (0.2431) | Acc_1: (90.58%) (36059/39808)\n",
      "Epoch: 174 | Batch_idx: 320 |  Loss_1: (0.2434) | Acc_1: (90.59%) (37221/41088)\n",
      "Epoch: 174 | Batch_idx: 330 |  Loss_1: (0.2438) | Acc_1: (90.59%) (38382/42368)\n",
      "Epoch: 174 | Batch_idx: 340 |  Loss_1: (0.2448) | Acc_1: (90.55%) (39525/43648)\n",
      "Epoch: 174 | Batch_idx: 350 |  Loss_1: (0.2443) | Acc_1: (90.59%) (40700/44928)\n",
      "Epoch: 174 | Batch_idx: 360 |  Loss_1: (0.2440) | Acc_1: (90.59%) (41860/46208)\n",
      "Epoch: 174 | Batch_idx: 370 |  Loss_1: (0.2443) | Acc_1: (90.57%) (43011/47488)\n",
      "Epoch: 174 | Batch_idx: 380 |  Loss_1: (0.2441) | Acc_1: (90.59%) (44179/48768)\n",
      "Epoch: 174 | Batch_idx: 390 |  Loss_1: (0.2439) | Acc_1: (90.61%) (45303/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3075) | Acc: (93.19%) (9319/10000)\n",
      "Epoch: 175 | Batch_idx: 0 |  Loss_1: (0.1991) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 175 | Batch_idx: 10 |  Loss_1: (0.2729) | Acc_1: (89.13%) (1255/1408)\n",
      "Epoch: 175 | Batch_idx: 20 |  Loss_1: (0.2616) | Acc_1: (89.81%) (2414/2688)\n",
      "Epoch: 175 | Batch_idx: 30 |  Loss_1: (0.2593) | Acc_1: (89.87%) (3566/3968)\n",
      "Epoch: 175 | Batch_idx: 40 |  Loss_1: (0.2527) | Acc_1: (90.02%) (4724/5248)\n",
      "Epoch: 175 | Batch_idx: 50 |  Loss_1: (0.2483) | Acc_1: (90.30%) (5895/6528)\n",
      "Epoch: 175 | Batch_idx: 60 |  Loss_1: (0.2497) | Acc_1: (90.27%) (7048/7808)\n",
      "Epoch: 175 | Batch_idx: 70 |  Loss_1: (0.2547) | Acc_1: (90.11%) (8189/9088)\n",
      "Epoch: 175 | Batch_idx: 80 |  Loss_1: (0.2522) | Acc_1: (90.21%) (9353/10368)\n",
      "Epoch: 175 | Batch_idx: 90 |  Loss_1: (0.2489) | Acc_1: (90.32%) (10520/11648)\n",
      "Epoch: 175 | Batch_idx: 100 |  Loss_1: (0.2525) | Acc_1: (90.14%) (11653/12928)\n",
      "Epoch: 175 | Batch_idx: 110 |  Loss_1: (0.2521) | Acc_1: (90.10%) (12801/14208)\n",
      "Epoch: 175 | Batch_idx: 120 |  Loss_1: (0.2520) | Acc_1: (90.10%) (13954/15488)\n",
      "Epoch: 175 | Batch_idx: 130 |  Loss_1: (0.2545) | Acc_1: (90.02%) (15094/16768)\n",
      "Epoch: 175 | Batch_idx: 140 |  Loss_1: (0.2538) | Acc_1: (90.04%) (16251/18048)\n",
      "Epoch: 175 | Batch_idx: 150 |  Loss_1: (0.2534) | Acc_1: (90.06%) (17407/19328)\n",
      "Epoch: 175 | Batch_idx: 160 |  Loss_1: (0.2523) | Acc_1: (90.14%) (18577/20608)\n",
      "Epoch: 175 | Batch_idx: 170 |  Loss_1: (0.2527) | Acc_1: (90.13%) (19728/21888)\n",
      "Epoch: 175 | Batch_idx: 180 |  Loss_1: (0.2534) | Acc_1: (90.12%) (20878/23168)\n",
      "Epoch: 175 | Batch_idx: 190 |  Loss_1: (0.2538) | Acc_1: (90.14%) (22038/24448)\n",
      "Epoch: 175 | Batch_idx: 200 |  Loss_1: (0.2535) | Acc_1: (90.14%) (23192/25728)\n",
      "Epoch: 175 | Batch_idx: 210 |  Loss_1: (0.2517) | Acc_1: (90.22%) (24366/27008)\n",
      "Epoch: 175 | Batch_idx: 220 |  Loss_1: (0.2519) | Acc_1: (90.21%) (25518/28288)\n",
      "Epoch: 175 | Batch_idx: 230 |  Loss_1: (0.2500) | Acc_1: (90.27%) (26690/29568)\n",
      "Epoch: 175 | Batch_idx: 240 |  Loss_1: (0.2494) | Acc_1: (90.30%) (27857/30848)\n",
      "Epoch: 175 | Batch_idx: 250 |  Loss_1: (0.2500) | Acc_1: (90.28%) (29006/32128)\n",
      "Epoch: 175 | Batch_idx: 260 |  Loss_1: (0.2503) | Acc_1: (90.28%) (30160/33408)\n",
      "Epoch: 175 | Batch_idx: 270 |  Loss_1: (0.2499) | Acc_1: (90.30%) (31322/34688)\n",
      "Epoch: 175 | Batch_idx: 280 |  Loss_1: (0.2502) | Acc_1: (90.27%) (32469/35968)\n",
      "Epoch: 175 | Batch_idx: 290 |  Loss_1: (0.2507) | Acc_1: (90.27%) (33623/37248)\n",
      "Epoch: 175 | Batch_idx: 300 |  Loss_1: (0.2501) | Acc_1: (90.28%) (34783/38528)\n",
      "Epoch: 175 | Batch_idx: 310 |  Loss_1: (0.2506) | Acc_1: (90.26%) (35930/39808)\n",
      "Epoch: 175 | Batch_idx: 320 |  Loss_1: (0.2503) | Acc_1: (90.26%) (37087/41088)\n",
      "Epoch: 175 | Batch_idx: 330 |  Loss_1: (0.2503) | Acc_1: (90.27%) (38245/42368)\n",
      "Epoch: 175 | Batch_idx: 340 |  Loss_1: (0.2499) | Acc_1: (90.28%) (39405/43648)\n",
      "Epoch: 175 | Batch_idx: 350 |  Loss_1: (0.2500) | Acc_1: (90.27%) (40558/44928)\n",
      "Epoch: 175 | Batch_idx: 360 |  Loss_1: (0.2508) | Acc_1: (90.23%) (41694/46208)\n",
      "Epoch: 175 | Batch_idx: 370 |  Loss_1: (0.2500) | Acc_1: (90.26%) (42864/47488)\n",
      "Epoch: 175 | Batch_idx: 380 |  Loss_1: (0.2493) | Acc_1: (90.30%) (44036/48768)\n",
      "Epoch: 175 | Batch_idx: 390 |  Loss_1: (0.2486) | Acc_1: (90.33%) (45165/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3177) | Acc: (93.22%) (9322/10000)\n",
      "Epoch: 176 | Batch_idx: 0 |  Loss_1: (0.2256) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 176 | Batch_idx: 10 |  Loss_1: (0.2251) | Acc_1: (91.05%) (1282/1408)\n",
      "Epoch: 176 | Batch_idx: 20 |  Loss_1: (0.2335) | Acc_1: (90.74%) (2439/2688)\n",
      "Epoch: 176 | Batch_idx: 30 |  Loss_1: (0.2441) | Acc_1: (90.50%) (3591/3968)\n",
      "Epoch: 176 | Batch_idx: 40 |  Loss_1: (0.2423) | Acc_1: (90.66%) (4758/5248)\n",
      "Epoch: 176 | Batch_idx: 50 |  Loss_1: (0.2489) | Acc_1: (90.49%) (5907/6528)\n",
      "Epoch: 176 | Batch_idx: 60 |  Loss_1: (0.2509) | Acc_1: (90.43%) (7061/7808)\n",
      "Epoch: 176 | Batch_idx: 70 |  Loss_1: (0.2547) | Acc_1: (90.32%) (8208/9088)\n",
      "Epoch: 176 | Batch_idx: 80 |  Loss_1: (0.2553) | Acc_1: (90.33%) (9365/10368)\n",
      "Epoch: 176 | Batch_idx: 90 |  Loss_1: (0.2581) | Acc_1: (90.16%) (10502/11648)\n",
      "Epoch: 176 | Batch_idx: 100 |  Loss_1: (0.2551) | Acc_1: (90.22%) (11664/12928)\n",
      "Epoch: 176 | Batch_idx: 110 |  Loss_1: (0.2514) | Acc_1: (90.36%) (12839/14208)\n",
      "Epoch: 176 | Batch_idx: 120 |  Loss_1: (0.2526) | Acc_1: (90.37%) (13997/15488)\n",
      "Epoch: 176 | Batch_idx: 130 |  Loss_1: (0.2508) | Acc_1: (90.44%) (15165/16768)\n",
      "Epoch: 176 | Batch_idx: 140 |  Loss_1: (0.2533) | Acc_1: (90.32%) (16301/18048)\n",
      "Epoch: 176 | Batch_idx: 150 |  Loss_1: (0.2519) | Acc_1: (90.38%) (17468/19328)\n",
      "Epoch: 176 | Batch_idx: 160 |  Loss_1: (0.2497) | Acc_1: (90.46%) (18643/20608)\n",
      "Epoch: 176 | Batch_idx: 170 |  Loss_1: (0.2490) | Acc_1: (90.48%) (19804/21888)\n",
      "Epoch: 176 | Batch_idx: 180 |  Loss_1: (0.2474) | Acc_1: (90.53%) (20973/23168)\n",
      "Epoch: 176 | Batch_idx: 190 |  Loss_1: (0.2475) | Acc_1: (90.53%) (22133/24448)\n",
      "Epoch: 176 | Batch_idx: 200 |  Loss_1: (0.2478) | Acc_1: (90.52%) (23290/25728)\n",
      "Epoch: 176 | Batch_idx: 210 |  Loss_1: (0.2469) | Acc_1: (90.57%) (24462/27008)\n",
      "Epoch: 176 | Batch_idx: 220 |  Loss_1: (0.2461) | Acc_1: (90.61%) (25631/28288)\n",
      "Epoch: 176 | Batch_idx: 230 |  Loss_1: (0.2457) | Acc_1: (90.61%) (26793/29568)\n",
      "Epoch: 176 | Batch_idx: 240 |  Loss_1: (0.2469) | Acc_1: (90.57%) (27939/30848)\n",
      "Epoch: 176 | Batch_idx: 250 |  Loss_1: (0.2469) | Acc_1: (90.58%) (29101/32128)\n",
      "Epoch: 176 | Batch_idx: 260 |  Loss_1: (0.2457) | Acc_1: (90.62%) (30275/33408)\n",
      "Epoch: 176 | Batch_idx: 270 |  Loss_1: (0.2464) | Acc_1: (90.59%) (31425/34688)\n",
      "Epoch: 176 | Batch_idx: 280 |  Loss_1: (0.2457) | Acc_1: (90.62%) (32593/35968)\n",
      "Epoch: 176 | Batch_idx: 290 |  Loss_1: (0.2454) | Acc_1: (90.62%) (33754/37248)\n",
      "Epoch: 176 | Batch_idx: 300 |  Loss_1: (0.2454) | Acc_1: (90.63%) (34919/38528)\n",
      "Epoch: 176 | Batch_idx: 310 |  Loss_1: (0.2463) | Acc_1: (90.60%) (36065/39808)\n",
      "Epoch: 176 | Batch_idx: 320 |  Loss_1: (0.2445) | Acc_1: (90.66%) (37250/41088)\n",
      "Epoch: 176 | Batch_idx: 330 |  Loss_1: (0.2441) | Acc_1: (90.67%) (38416/42368)\n",
      "Epoch: 176 | Batch_idx: 340 |  Loss_1: (0.2434) | Acc_1: (90.70%) (39590/43648)\n",
      "Epoch: 176 | Batch_idx: 350 |  Loss_1: (0.2429) | Acc_1: (90.72%) (40759/44928)\n",
      "Epoch: 176 | Batch_idx: 360 |  Loss_1: (0.2430) | Acc_1: (90.72%) (41921/46208)\n",
      "Epoch: 176 | Batch_idx: 370 |  Loss_1: (0.2421) | Acc_1: (90.76%) (43098/47488)\n",
      "Epoch: 176 | Batch_idx: 380 |  Loss_1: (0.2418) | Acc_1: (90.77%) (44267/48768)\n",
      "Epoch: 176 | Batch_idx: 390 |  Loss_1: (0.2420) | Acc_1: (90.76%) (45382/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3203) | Acc: (93.28%) (9328/10000)\n",
      "Epoch: 177 | Batch_idx: 0 |  Loss_1: (0.2801) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 177 | Batch_idx: 10 |  Loss_1: (0.2388) | Acc_1: (91.12%) (1283/1408)\n",
      "Epoch: 177 | Batch_idx: 20 |  Loss_1: (0.2603) | Acc_1: (90.22%) (2425/2688)\n",
      "Epoch: 177 | Batch_idx: 30 |  Loss_1: (0.2594) | Acc_1: (90.12%) (3576/3968)\n",
      "Epoch: 177 | Batch_idx: 40 |  Loss_1: (0.2521) | Acc_1: (90.36%) (4742/5248)\n",
      "Epoch: 177 | Batch_idx: 50 |  Loss_1: (0.2565) | Acc_1: (90.07%) (5880/6528)\n",
      "Epoch: 177 | Batch_idx: 60 |  Loss_1: (0.2562) | Acc_1: (90.11%) (7036/7808)\n",
      "Epoch: 177 | Batch_idx: 70 |  Loss_1: (0.2511) | Acc_1: (90.29%) (8206/9088)\n",
      "Epoch: 177 | Batch_idx: 80 |  Loss_1: (0.2519) | Acc_1: (90.34%) (9366/10368)\n",
      "Epoch: 177 | Batch_idx: 90 |  Loss_1: (0.2513) | Acc_1: (90.36%) (10525/11648)\n",
      "Epoch: 177 | Batch_idx: 100 |  Loss_1: (0.2485) | Acc_1: (90.42%) (11690/12928)\n",
      "Epoch: 177 | Batch_idx: 110 |  Loss_1: (0.2453) | Acc_1: (90.54%) (12864/14208)\n",
      "Epoch: 177 | Batch_idx: 120 |  Loss_1: (0.2459) | Acc_1: (90.55%) (14024/15488)\n",
      "Epoch: 177 | Batch_idx: 130 |  Loss_1: (0.2477) | Acc_1: (90.46%) (15169/16768)\n",
      "Epoch: 177 | Batch_idx: 140 |  Loss_1: (0.2467) | Acc_1: (90.51%) (16336/18048)\n",
      "Epoch: 177 | Batch_idx: 150 |  Loss_1: (0.2453) | Acc_1: (90.55%) (17501/19328)\n",
      "Epoch: 177 | Batch_idx: 160 |  Loss_1: (0.2478) | Acc_1: (90.46%) (18642/20608)\n",
      "Epoch: 177 | Batch_idx: 170 |  Loss_1: (0.2466) | Acc_1: (90.50%) (19808/21888)\n",
      "Epoch: 177 | Batch_idx: 180 |  Loss_1: (0.2460) | Acc_1: (90.52%) (20971/23168)\n",
      "Epoch: 177 | Batch_idx: 190 |  Loss_1: (0.2464) | Acc_1: (90.50%) (22126/24448)\n",
      "Epoch: 177 | Batch_idx: 200 |  Loss_1: (0.2441) | Acc_1: (90.59%) (23306/25728)\n",
      "Epoch: 177 | Batch_idx: 210 |  Loss_1: (0.2450) | Acc_1: (90.54%) (24454/27008)\n",
      "Epoch: 177 | Batch_idx: 220 |  Loss_1: (0.2433) | Acc_1: (90.63%) (25637/28288)\n",
      "Epoch: 177 | Batch_idx: 230 |  Loss_1: (0.2434) | Acc_1: (90.64%) (26799/29568)\n",
      "Epoch: 177 | Batch_idx: 240 |  Loss_1: (0.2432) | Acc_1: (90.63%) (27957/30848)\n",
      "Epoch: 177 | Batch_idx: 250 |  Loss_1: (0.2445) | Acc_1: (90.55%) (29093/32128)\n",
      "Epoch: 177 | Batch_idx: 260 |  Loss_1: (0.2451) | Acc_1: (90.53%) (30245/33408)\n",
      "Epoch: 177 | Batch_idx: 270 |  Loss_1: (0.2453) | Acc_1: (90.54%) (31406/34688)\n",
      "Epoch: 177 | Batch_idx: 280 |  Loss_1: (0.2450) | Acc_1: (90.55%) (32568/35968)\n",
      "Epoch: 177 | Batch_idx: 290 |  Loss_1: (0.2457) | Acc_1: (90.51%) (33715/37248)\n",
      "Epoch: 177 | Batch_idx: 300 |  Loss_1: (0.2459) | Acc_1: (90.52%) (34874/38528)\n",
      "Epoch: 177 | Batch_idx: 310 |  Loss_1: (0.2460) | Acc_1: (90.53%) (36037/39808)\n",
      "Epoch: 177 | Batch_idx: 320 |  Loss_1: (0.2465) | Acc_1: (90.50%) (37184/41088)\n",
      "Epoch: 177 | Batch_idx: 330 |  Loss_1: (0.2452) | Acc_1: (90.55%) (38363/42368)\n",
      "Epoch: 177 | Batch_idx: 340 |  Loss_1: (0.2448) | Acc_1: (90.55%) (39524/43648)\n",
      "Epoch: 177 | Batch_idx: 350 |  Loss_1: (0.2443) | Acc_1: (90.56%) (40686/44928)\n",
      "Epoch: 177 | Batch_idx: 360 |  Loss_1: (0.2439) | Acc_1: (90.58%) (41853/46208)\n",
      "Epoch: 177 | Batch_idx: 370 |  Loss_1: (0.2434) | Acc_1: (90.59%) (43019/47488)\n",
      "Epoch: 177 | Batch_idx: 380 |  Loss_1: (0.2437) | Acc_1: (90.56%) (44166/48768)\n",
      "Epoch: 177 | Batch_idx: 390 |  Loss_1: (0.2442) | Acc_1: (90.54%) (45268/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3115) | Acc: (93.01%) (9301/10000)\n",
      "Epoch: 178 | Batch_idx: 0 |  Loss_1: (0.2301) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 178 | Batch_idx: 10 |  Loss_1: (0.2043) | Acc_1: (91.90%) (1294/1408)\n",
      "Epoch: 178 | Batch_idx: 20 |  Loss_1: (0.2213) | Acc_1: (91.82%) (2468/2688)\n",
      "Epoch: 178 | Batch_idx: 30 |  Loss_1: (0.2165) | Acc_1: (91.83%) (3644/3968)\n",
      "Epoch: 178 | Batch_idx: 40 |  Loss_1: (0.2208) | Acc_1: (91.65%) (4810/5248)\n",
      "Epoch: 178 | Batch_idx: 50 |  Loss_1: (0.2257) | Acc_1: (91.50%) (5973/6528)\n",
      "Epoch: 178 | Batch_idx: 60 |  Loss_1: (0.2324) | Acc_1: (91.27%) (7126/7808)\n",
      "Epoch: 178 | Batch_idx: 70 |  Loss_1: (0.2378) | Acc_1: (91.09%) (8278/9088)\n",
      "Epoch: 178 | Batch_idx: 80 |  Loss_1: (0.2364) | Acc_1: (91.09%) (9444/10368)\n",
      "Epoch: 178 | Batch_idx: 90 |  Loss_1: (0.2429) | Acc_1: (90.78%) (10574/11648)\n",
      "Epoch: 178 | Batch_idx: 100 |  Loss_1: (0.2450) | Acc_1: (90.66%) (11720/12928)\n",
      "Epoch: 178 | Batch_idx: 110 |  Loss_1: (0.2459) | Acc_1: (90.61%) (12874/14208)\n",
      "Epoch: 178 | Batch_idx: 120 |  Loss_1: (0.2424) | Acc_1: (90.72%) (14050/15488)\n",
      "Epoch: 178 | Batch_idx: 130 |  Loss_1: (0.2408) | Acc_1: (90.77%) (15220/16768)\n",
      "Epoch: 178 | Batch_idx: 140 |  Loss_1: (0.2429) | Acc_1: (90.69%) (16367/18048)\n",
      "Epoch: 178 | Batch_idx: 150 |  Loss_1: (0.2429) | Acc_1: (90.66%) (17522/19328)\n",
      "Epoch: 178 | Batch_idx: 160 |  Loss_1: (0.2405) | Acc_1: (90.77%) (18705/20608)\n",
      "Epoch: 178 | Batch_idx: 170 |  Loss_1: (0.2393) | Acc_1: (90.84%) (19883/21888)\n",
      "Epoch: 178 | Batch_idx: 180 |  Loss_1: (0.2392) | Acc_1: (90.80%) (21036/23168)\n",
      "Epoch: 178 | Batch_idx: 190 |  Loss_1: (0.2396) | Acc_1: (90.78%) (22193/24448)\n",
      "Epoch: 178 | Batch_idx: 200 |  Loss_1: (0.2397) | Acc_1: (90.78%) (23356/25728)\n",
      "Epoch: 178 | Batch_idx: 210 |  Loss_1: (0.2392) | Acc_1: (90.80%) (24522/27008)\n",
      "Epoch: 178 | Batch_idx: 220 |  Loss_1: (0.2396) | Acc_1: (90.76%) (25675/28288)\n",
      "Epoch: 178 | Batch_idx: 230 |  Loss_1: (0.2398) | Acc_1: (90.75%) (26834/29568)\n",
      "Epoch: 178 | Batch_idx: 240 |  Loss_1: (0.2402) | Acc_1: (90.74%) (27992/30848)\n",
      "Epoch: 178 | Batch_idx: 250 |  Loss_1: (0.2395) | Acc_1: (90.77%) (29162/32128)\n",
      "Epoch: 178 | Batch_idx: 260 |  Loss_1: (0.2404) | Acc_1: (90.74%) (30314/33408)\n",
      "Epoch: 178 | Batch_idx: 270 |  Loss_1: (0.2408) | Acc_1: (90.73%) (31473/34688)\n",
      "Epoch: 178 | Batch_idx: 280 |  Loss_1: (0.2407) | Acc_1: (90.72%) (32631/35968)\n",
      "Epoch: 178 | Batch_idx: 290 |  Loss_1: (0.2410) | Acc_1: (90.72%) (33790/37248)\n",
      "Epoch: 178 | Batch_idx: 300 |  Loss_1: (0.2414) | Acc_1: (90.70%) (34945/38528)\n",
      "Epoch: 178 | Batch_idx: 310 |  Loss_1: (0.2416) | Acc_1: (90.70%) (36105/39808)\n",
      "Epoch: 178 | Batch_idx: 320 |  Loss_1: (0.2419) | Acc_1: (90.68%) (37260/41088)\n",
      "Epoch: 178 | Batch_idx: 330 |  Loss_1: (0.2410) | Acc_1: (90.71%) (38434/42368)\n",
      "Epoch: 178 | Batch_idx: 340 |  Loss_1: (0.2411) | Acc_1: (90.72%) (39597/43648)\n",
      "Epoch: 178 | Batch_idx: 350 |  Loss_1: (0.2413) | Acc_1: (90.72%) (40759/44928)\n",
      "Epoch: 178 | Batch_idx: 360 |  Loss_1: (0.2417) | Acc_1: (90.70%) (41910/46208)\n",
      "Epoch: 178 | Batch_idx: 370 |  Loss_1: (0.2417) | Acc_1: (90.71%) (43075/47488)\n",
      "Epoch: 178 | Batch_idx: 380 |  Loss_1: (0.2418) | Acc_1: (90.69%) (44228/48768)\n",
      "Epoch: 178 | Batch_idx: 390 |  Loss_1: (0.2413) | Acc_1: (90.70%) (45352/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3180) | Acc: (93.30%) (9330/10000)\n",
      "Epoch: 179 | Batch_idx: 0 |  Loss_1: (0.2242) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 179 | Batch_idx: 10 |  Loss_1: (0.2287) | Acc_1: (91.34%) (1286/1408)\n",
      "Epoch: 179 | Batch_idx: 20 |  Loss_1: (0.2212) | Acc_1: (91.44%) (2458/2688)\n",
      "Epoch: 179 | Batch_idx: 30 |  Loss_1: (0.2257) | Acc_1: (91.41%) (3627/3968)\n",
      "Epoch: 179 | Batch_idx: 40 |  Loss_1: (0.2293) | Acc_1: (91.18%) (4785/5248)\n",
      "Epoch: 179 | Batch_idx: 50 |  Loss_1: (0.2404) | Acc_1: (90.73%) (5923/6528)\n",
      "Epoch: 179 | Batch_idx: 60 |  Loss_1: (0.2393) | Acc_1: (90.74%) (7085/7808)\n",
      "Epoch: 179 | Batch_idx: 70 |  Loss_1: (0.2426) | Acc_1: (90.58%) (8232/9088)\n",
      "Epoch: 179 | Batch_idx: 80 |  Loss_1: (0.2442) | Acc_1: (90.55%) (9388/10368)\n",
      "Epoch: 179 | Batch_idx: 90 |  Loss_1: (0.2405) | Acc_1: (90.69%) (10564/11648)\n",
      "Epoch: 179 | Batch_idx: 100 |  Loss_1: (0.2413) | Acc_1: (90.65%) (11719/12928)\n",
      "Epoch: 179 | Batch_idx: 110 |  Loss_1: (0.2421) | Acc_1: (90.70%) (12886/14208)\n",
      "Epoch: 179 | Batch_idx: 120 |  Loss_1: (0.2445) | Acc_1: (90.59%) (14031/15488)\n",
      "Epoch: 179 | Batch_idx: 130 |  Loss_1: (0.2448) | Acc_1: (90.61%) (15193/16768)\n",
      "Epoch: 179 | Batch_idx: 140 |  Loss_1: (0.2441) | Acc_1: (90.65%) (16361/18048)\n",
      "Epoch: 179 | Batch_idx: 150 |  Loss_1: (0.2433) | Acc_1: (90.71%) (17532/19328)\n",
      "Epoch: 179 | Batch_idx: 160 |  Loss_1: (0.2420) | Acc_1: (90.75%) (18702/20608)\n",
      "Epoch: 179 | Batch_idx: 170 |  Loss_1: (0.2433) | Acc_1: (90.70%) (19853/21888)\n",
      "Epoch: 179 | Batch_idx: 180 |  Loss_1: (0.2435) | Acc_1: (90.69%) (21011/23168)\n",
      "Epoch: 179 | Batch_idx: 190 |  Loss_1: (0.2437) | Acc_1: (90.65%) (22162/24448)\n",
      "Epoch: 179 | Batch_idx: 200 |  Loss_1: (0.2428) | Acc_1: (90.66%) (23324/25728)\n",
      "Epoch: 179 | Batch_idx: 210 |  Loss_1: (0.2416) | Acc_1: (90.72%) (24501/27008)\n",
      "Epoch: 179 | Batch_idx: 220 |  Loss_1: (0.2412) | Acc_1: (90.72%) (25662/28288)\n",
      "Epoch: 179 | Batch_idx: 230 |  Loss_1: (0.2403) | Acc_1: (90.76%) (26837/29568)\n",
      "Epoch: 179 | Batch_idx: 240 |  Loss_1: (0.2402) | Acc_1: (90.78%) (28005/30848)\n",
      "Epoch: 179 | Batch_idx: 250 |  Loss_1: (0.2413) | Acc_1: (90.75%) (29157/32128)\n",
      "Epoch: 179 | Batch_idx: 260 |  Loss_1: (0.2408) | Acc_1: (90.77%) (30325/33408)\n",
      "Epoch: 179 | Batch_idx: 270 |  Loss_1: (0.2405) | Acc_1: (90.80%) (31496/34688)\n",
      "Epoch: 179 | Batch_idx: 280 |  Loss_1: (0.2402) | Acc_1: (90.81%) (32664/35968)\n",
      "Epoch: 179 | Batch_idx: 290 |  Loss_1: (0.2388) | Acc_1: (90.85%) (33841/37248)\n",
      "Epoch: 179 | Batch_idx: 300 |  Loss_1: (0.2392) | Acc_1: (90.82%) (34993/38528)\n",
      "Epoch: 179 | Batch_idx: 310 |  Loss_1: (0.2386) | Acc_1: (90.84%) (36160/39808)\n",
      "Epoch: 179 | Batch_idx: 320 |  Loss_1: (0.2396) | Acc_1: (90.78%) (37301/41088)\n",
      "Epoch: 179 | Batch_idx: 330 |  Loss_1: (0.2389) | Acc_1: (90.83%) (38481/42368)\n",
      "Epoch: 179 | Batch_idx: 340 |  Loss_1: (0.2390) | Acc_1: (90.81%) (39635/43648)\n",
      "Epoch: 179 | Batch_idx: 350 |  Loss_1: (0.2393) | Acc_1: (90.80%) (40793/44928)\n",
      "Epoch: 179 | Batch_idx: 360 |  Loss_1: (0.2386) | Acc_1: (90.82%) (41965/46208)\n",
      "Epoch: 179 | Batch_idx: 370 |  Loss_1: (0.2390) | Acc_1: (90.79%) (43113/47488)\n",
      "Epoch: 179 | Batch_idx: 380 |  Loss_1: (0.2397) | Acc_1: (90.76%) (44264/48768)\n",
      "Epoch: 179 | Batch_idx: 390 |  Loss_1: (0.2393) | Acc_1: (90.79%) (45395/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3240) | Acc: (93.22%) (9322/10000)\n",
      "Epoch: 180 | Batch_idx: 0 |  Loss_1: (0.1112) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 180 | Batch_idx: 10 |  Loss_1: (0.2387) | Acc_1: (90.84%) (1279/1408)\n",
      "Epoch: 180 | Batch_idx: 20 |  Loss_1: (0.2253) | Acc_1: (91.22%) (2452/2688)\n",
      "Epoch: 180 | Batch_idx: 30 |  Loss_1: (0.2237) | Acc_1: (91.28%) (3622/3968)\n",
      "Epoch: 180 | Batch_idx: 40 |  Loss_1: (0.2224) | Acc_1: (91.41%) (4797/5248)\n",
      "Epoch: 180 | Batch_idx: 50 |  Loss_1: (0.2264) | Acc_1: (91.33%) (5962/6528)\n",
      "Epoch: 180 | Batch_idx: 60 |  Loss_1: (0.2302) | Acc_1: (91.21%) (7122/7808)\n",
      "Epoch: 180 | Batch_idx: 70 |  Loss_1: (0.2341) | Acc_1: (91.00%) (8270/9088)\n",
      "Epoch: 180 | Batch_idx: 80 |  Loss_1: (0.2363) | Acc_1: (90.94%) (9429/10368)\n",
      "Epoch: 180 | Batch_idx: 90 |  Loss_1: (0.2359) | Acc_1: (90.93%) (10592/11648)\n",
      "Epoch: 180 | Batch_idx: 100 |  Loss_1: (0.2343) | Acc_1: (90.97%) (11761/12928)\n",
      "Epoch: 180 | Batch_idx: 110 |  Loss_1: (0.2346) | Acc_1: (90.98%) (12927/14208)\n",
      "Epoch: 180 | Batch_idx: 120 |  Loss_1: (0.2330) | Acc_1: (91.04%) (14100/15488)\n",
      "Epoch: 180 | Batch_idx: 130 |  Loss_1: (0.2340) | Acc_1: (90.96%) (15253/16768)\n",
      "Epoch: 180 | Batch_idx: 140 |  Loss_1: (0.2325) | Acc_1: (91.04%) (16430/18048)\n",
      "Epoch: 180 | Batch_idx: 150 |  Loss_1: (0.2322) | Acc_1: (91.01%) (17590/19328)\n",
      "Epoch: 180 | Batch_idx: 160 |  Loss_1: (0.2336) | Acc_1: (90.95%) (18744/20608)\n",
      "Epoch: 180 | Batch_idx: 170 |  Loss_1: (0.2338) | Acc_1: (90.95%) (19908/21888)\n",
      "Epoch: 180 | Batch_idx: 180 |  Loss_1: (0.2348) | Acc_1: (90.94%) (21068/23168)\n",
      "Epoch: 180 | Batch_idx: 190 |  Loss_1: (0.2358) | Acc_1: (90.92%) (22228/24448)\n",
      "Epoch: 180 | Batch_idx: 200 |  Loss_1: (0.2348) | Acc_1: (90.97%) (23405/25728)\n",
      "Epoch: 180 | Batch_idx: 210 |  Loss_1: (0.2363) | Acc_1: (90.93%) (24559/27008)\n",
      "Epoch: 180 | Batch_idx: 220 |  Loss_1: (0.2350) | Acc_1: (90.97%) (25733/28288)\n",
      "Epoch: 180 | Batch_idx: 230 |  Loss_1: (0.2355) | Acc_1: (90.94%) (26890/29568)\n",
      "Epoch: 180 | Batch_idx: 240 |  Loss_1: (0.2353) | Acc_1: (90.97%) (28063/30848)\n",
      "Epoch: 180 | Batch_idx: 250 |  Loss_1: (0.2362) | Acc_1: (90.95%) (29221/32128)\n",
      "Epoch: 180 | Batch_idx: 260 |  Loss_1: (0.2365) | Acc_1: (90.95%) (30385/33408)\n",
      "Epoch: 180 | Batch_idx: 270 |  Loss_1: (0.2382) | Acc_1: (90.87%) (31520/34688)\n",
      "Epoch: 180 | Batch_idx: 280 |  Loss_1: (0.2396) | Acc_1: (90.82%) (32666/35968)\n",
      "Epoch: 180 | Batch_idx: 290 |  Loss_1: (0.2402) | Acc_1: (90.78%) (33815/37248)\n",
      "Epoch: 180 | Batch_idx: 300 |  Loss_1: (0.2405) | Acc_1: (90.77%) (34973/38528)\n",
      "Epoch: 180 | Batch_idx: 310 |  Loss_1: (0.2406) | Acc_1: (90.76%) (36129/39808)\n",
      "Epoch: 180 | Batch_idx: 320 |  Loss_1: (0.2409) | Acc_1: (90.74%) (37283/41088)\n",
      "Epoch: 180 | Batch_idx: 330 |  Loss_1: (0.2416) | Acc_1: (90.71%) (38432/42368)\n",
      "Epoch: 180 | Batch_idx: 340 |  Loss_1: (0.2418) | Acc_1: (90.70%) (39590/43648)\n",
      "Epoch: 180 | Batch_idx: 350 |  Loss_1: (0.2419) | Acc_1: (90.71%) (40754/44928)\n",
      "Epoch: 180 | Batch_idx: 360 |  Loss_1: (0.2415) | Acc_1: (90.73%) (41924/46208)\n",
      "Epoch: 180 | Batch_idx: 370 |  Loss_1: (0.2408) | Acc_1: (90.77%) (43107/47488)\n",
      "Epoch: 180 | Batch_idx: 380 |  Loss_1: (0.2396) | Acc_1: (90.81%) (44287/48768)\n",
      "Epoch: 180 | Batch_idx: 390 |  Loss_1: (0.2399) | Acc_1: (90.81%) (45405/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3275) | Acc: (92.90%) (9290/10000)\n",
      "Epoch: 181 | Batch_idx: 0 |  Loss_1: (0.1785) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 181 | Batch_idx: 10 |  Loss_1: (0.2338) | Acc_1: (90.84%) (1279/1408)\n",
      "Epoch: 181 | Batch_idx: 20 |  Loss_1: (0.2250) | Acc_1: (91.18%) (2451/2688)\n",
      "Epoch: 181 | Batch_idx: 30 |  Loss_1: (0.2364) | Acc_1: (90.88%) (3606/3968)\n",
      "Epoch: 181 | Batch_idx: 40 |  Loss_1: (0.2420) | Acc_1: (90.57%) (4753/5248)\n",
      "Epoch: 181 | Batch_idx: 50 |  Loss_1: (0.2444) | Acc_1: (90.49%) (5907/6528)\n",
      "Epoch: 181 | Batch_idx: 60 |  Loss_1: (0.2446) | Acc_1: (90.43%) (7061/7808)\n",
      "Epoch: 181 | Batch_idx: 70 |  Loss_1: (0.2450) | Acc_1: (90.48%) (8223/9088)\n",
      "Epoch: 181 | Batch_idx: 80 |  Loss_1: (0.2444) | Acc_1: (90.52%) (9385/10368)\n",
      "Epoch: 181 | Batch_idx: 90 |  Loss_1: (0.2448) | Acc_1: (90.51%) (10543/11648)\n",
      "Epoch: 181 | Batch_idx: 100 |  Loss_1: (0.2408) | Acc_1: (90.60%) (11713/12928)\n",
      "Epoch: 181 | Batch_idx: 110 |  Loss_1: (0.2437) | Acc_1: (90.51%) (12860/14208)\n",
      "Epoch: 181 | Batch_idx: 120 |  Loss_1: (0.2431) | Acc_1: (90.53%) (14021/15488)\n",
      "Epoch: 181 | Batch_idx: 130 |  Loss_1: (0.2428) | Acc_1: (90.51%) (15177/16768)\n",
      "Epoch: 181 | Batch_idx: 140 |  Loss_1: (0.2457) | Acc_1: (90.42%) (16319/18048)\n",
      "Epoch: 181 | Batch_idx: 150 |  Loss_1: (0.2451) | Acc_1: (90.44%) (17481/19328)\n",
      "Epoch: 181 | Batch_idx: 160 |  Loss_1: (0.2443) | Acc_1: (90.49%) (18648/20608)\n",
      "Epoch: 181 | Batch_idx: 170 |  Loss_1: (0.2438) | Acc_1: (90.53%) (19816/21888)\n",
      "Epoch: 181 | Batch_idx: 180 |  Loss_1: (0.2458) | Acc_1: (90.47%) (20961/23168)\n",
      "Epoch: 181 | Batch_idx: 190 |  Loss_1: (0.2454) | Acc_1: (90.49%) (22122/24448)\n",
      "Epoch: 181 | Batch_idx: 200 |  Loss_1: (0.2445) | Acc_1: (90.52%) (23289/25728)\n",
      "Epoch: 181 | Batch_idx: 210 |  Loss_1: (0.2440) | Acc_1: (90.55%) (24456/27008)\n",
      "Epoch: 181 | Batch_idx: 220 |  Loss_1: (0.2442) | Acc_1: (90.53%) (25609/28288)\n",
      "Epoch: 181 | Batch_idx: 230 |  Loss_1: (0.2436) | Acc_1: (90.54%) (26771/29568)\n",
      "Epoch: 181 | Batch_idx: 240 |  Loss_1: (0.2438) | Acc_1: (90.56%) (27935/30848)\n",
      "Epoch: 181 | Batch_idx: 250 |  Loss_1: (0.2438) | Acc_1: (90.56%) (29096/32128)\n",
      "Epoch: 181 | Batch_idx: 260 |  Loss_1: (0.2460) | Acc_1: (90.47%) (30224/33408)\n",
      "Epoch: 181 | Batch_idx: 270 |  Loss_1: (0.2462) | Acc_1: (90.47%) (31382/34688)\n",
      "Epoch: 181 | Batch_idx: 280 |  Loss_1: (0.2458) | Acc_1: (90.48%) (32543/35968)\n",
      "Epoch: 181 | Batch_idx: 290 |  Loss_1: (0.2462) | Acc_1: (90.47%) (33700/37248)\n",
      "Epoch: 181 | Batch_idx: 300 |  Loss_1: (0.2464) | Acc_1: (90.48%) (34860/38528)\n",
      "Epoch: 181 | Batch_idx: 310 |  Loss_1: (0.2473) | Acc_1: (90.45%) (36008/39808)\n",
      "Epoch: 181 | Batch_idx: 320 |  Loss_1: (0.2474) | Acc_1: (90.45%) (37164/41088)\n",
      "Epoch: 181 | Batch_idx: 330 |  Loss_1: (0.2470) | Acc_1: (90.47%) (38332/42368)\n",
      "Epoch: 181 | Batch_idx: 340 |  Loss_1: (0.2468) | Acc_1: (90.48%) (39492/43648)\n",
      "Epoch: 181 | Batch_idx: 350 |  Loss_1: (0.2464) | Acc_1: (90.50%) (40660/44928)\n",
      "Epoch: 181 | Batch_idx: 360 |  Loss_1: (0.2457) | Acc_1: (90.51%) (41825/46208)\n",
      "Epoch: 181 | Batch_idx: 370 |  Loss_1: (0.2466) | Acc_1: (90.48%) (42967/47488)\n",
      "Epoch: 181 | Batch_idx: 380 |  Loss_1: (0.2464) | Acc_1: (90.49%) (44129/48768)\n",
      "Epoch: 181 | Batch_idx: 390 |  Loss_1: (0.2466) | Acc_1: (90.48%) (45239/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3282) | Acc: (93.01%) (9301/10000)\n",
      "Epoch: 182 | Batch_idx: 0 |  Loss_1: (0.2719) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 182 | Batch_idx: 10 |  Loss_1: (0.2185) | Acc_1: (91.19%) (1284/1408)\n",
      "Epoch: 182 | Batch_idx: 20 |  Loss_1: (0.2336) | Acc_1: (90.77%) (2440/2688)\n",
      "Epoch: 182 | Batch_idx: 30 |  Loss_1: (0.2405) | Acc_1: (90.52%) (3592/3968)\n",
      "Epoch: 182 | Batch_idx: 40 |  Loss_1: (0.2388) | Acc_1: (90.57%) (4753/5248)\n",
      "Epoch: 182 | Batch_idx: 50 |  Loss_1: (0.2419) | Acc_1: (90.49%) (5907/6528)\n",
      "Epoch: 182 | Batch_idx: 60 |  Loss_1: (0.2396) | Acc_1: (90.59%) (7073/7808)\n",
      "Epoch: 182 | Batch_idx: 70 |  Loss_1: (0.2389) | Acc_1: (90.58%) (8232/9088)\n",
      "Epoch: 182 | Batch_idx: 80 |  Loss_1: (0.2385) | Acc_1: (90.66%) (9400/10368)\n",
      "Epoch: 182 | Batch_idx: 90 |  Loss_1: (0.2363) | Acc_1: (90.74%) (10569/11648)\n",
      "Epoch: 182 | Batch_idx: 100 |  Loss_1: (0.2335) | Acc_1: (90.85%) (11745/12928)\n",
      "Epoch: 182 | Batch_idx: 110 |  Loss_1: (0.2327) | Acc_1: (90.85%) (12908/14208)\n",
      "Epoch: 182 | Batch_idx: 120 |  Loss_1: (0.2351) | Acc_1: (90.74%) (14054/15488)\n",
      "Epoch: 182 | Batch_idx: 130 |  Loss_1: (0.2343) | Acc_1: (90.80%) (15226/16768)\n",
      "Epoch: 182 | Batch_idx: 140 |  Loss_1: (0.2332) | Acc_1: (90.83%) (16393/18048)\n",
      "Epoch: 182 | Batch_idx: 150 |  Loss_1: (0.2328) | Acc_1: (90.85%) (17560/19328)\n",
      "Epoch: 182 | Batch_idx: 160 |  Loss_1: (0.2332) | Acc_1: (90.87%) (18726/20608)\n",
      "Epoch: 182 | Batch_idx: 170 |  Loss_1: (0.2336) | Acc_1: (90.84%) (19883/21888)\n",
      "Epoch: 182 | Batch_idx: 180 |  Loss_1: (0.2356) | Acc_1: (90.75%) (21024/23168)\n",
      "Epoch: 182 | Batch_idx: 190 |  Loss_1: (0.2363) | Acc_1: (90.72%) (22179/24448)\n",
      "Epoch: 182 | Batch_idx: 200 |  Loss_1: (0.2367) | Acc_1: (90.72%) (23341/25728)\n",
      "Epoch: 182 | Batch_idx: 210 |  Loss_1: (0.2352) | Acc_1: (90.79%) (24520/27008)\n",
      "Epoch: 182 | Batch_idx: 220 |  Loss_1: (0.2356) | Acc_1: (90.79%) (25682/28288)\n",
      "Epoch: 182 | Batch_idx: 230 |  Loss_1: (0.2347) | Acc_1: (90.84%) (26859/29568)\n",
      "Epoch: 182 | Batch_idx: 240 |  Loss_1: (0.2344) | Acc_1: (90.84%) (28022/30848)\n",
      "Epoch: 182 | Batch_idx: 250 |  Loss_1: (0.2352) | Acc_1: (90.80%) (29172/32128)\n",
      "Epoch: 182 | Batch_idx: 260 |  Loss_1: (0.2350) | Acc_1: (90.82%) (30340/33408)\n",
      "Epoch: 182 | Batch_idx: 270 |  Loss_1: (0.2353) | Acc_1: (90.81%) (31499/34688)\n",
      "Epoch: 182 | Batch_idx: 280 |  Loss_1: (0.2359) | Acc_1: (90.82%) (32667/35968)\n",
      "Epoch: 182 | Batch_idx: 290 |  Loss_1: (0.2365) | Acc_1: (90.80%) (33822/37248)\n",
      "Epoch: 182 | Batch_idx: 300 |  Loss_1: (0.2358) | Acc_1: (90.82%) (34992/38528)\n",
      "Epoch: 182 | Batch_idx: 310 |  Loss_1: (0.2365) | Acc_1: (90.80%) (36147/39808)\n",
      "Epoch: 182 | Batch_idx: 320 |  Loss_1: (0.2370) | Acc_1: (90.77%) (37294/41088)\n",
      "Epoch: 182 | Batch_idx: 330 |  Loss_1: (0.2378) | Acc_1: (90.74%) (38444/42368)\n",
      "Epoch: 182 | Batch_idx: 340 |  Loss_1: (0.2378) | Acc_1: (90.73%) (39601/43648)\n",
      "Epoch: 182 | Batch_idx: 350 |  Loss_1: (0.2377) | Acc_1: (90.73%) (40765/44928)\n",
      "Epoch: 182 | Batch_idx: 360 |  Loss_1: (0.2375) | Acc_1: (90.75%) (41935/46208)\n",
      "Epoch: 182 | Batch_idx: 370 |  Loss_1: (0.2372) | Acc_1: (90.77%) (43104/47488)\n",
      "Epoch: 182 | Batch_idx: 380 |  Loss_1: (0.2380) | Acc_1: (90.74%) (44254/48768)\n",
      "Epoch: 182 | Batch_idx: 390 |  Loss_1: (0.2384) | Acc_1: (90.73%) (45363/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3172) | Acc: (93.27%) (9327/10000)\n",
      "Epoch: 183 | Batch_idx: 0 |  Loss_1: (0.3656) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 183 | Batch_idx: 10 |  Loss_1: (0.2789) | Acc_1: (89.28%) (1257/1408)\n",
      "Epoch: 183 | Batch_idx: 20 |  Loss_1: (0.2494) | Acc_1: (90.48%) (2432/2688)\n",
      "Epoch: 183 | Batch_idx: 30 |  Loss_1: (0.2494) | Acc_1: (90.37%) (3586/3968)\n",
      "Epoch: 183 | Batch_idx: 40 |  Loss_1: (0.2380) | Acc_1: (90.78%) (4764/5248)\n",
      "Epoch: 183 | Batch_idx: 50 |  Loss_1: (0.2309) | Acc_1: (91.04%) (5943/6528)\n",
      "Epoch: 183 | Batch_idx: 60 |  Loss_1: (0.2335) | Acc_1: (91.00%) (7105/7808)\n",
      "Epoch: 183 | Batch_idx: 70 |  Loss_1: (0.2334) | Acc_1: (91.00%) (8270/9088)\n",
      "Epoch: 183 | Batch_idx: 80 |  Loss_1: (0.2321) | Acc_1: (91.09%) (9444/10368)\n",
      "Epoch: 183 | Batch_idx: 90 |  Loss_1: (0.2344) | Acc_1: (91.01%) (10601/11648)\n",
      "Epoch: 183 | Batch_idx: 100 |  Loss_1: (0.2336) | Acc_1: (91.04%) (11770/12928)\n",
      "Epoch: 183 | Batch_idx: 110 |  Loss_1: (0.2331) | Acc_1: (91.04%) (12935/14208)\n",
      "Epoch: 183 | Batch_idx: 120 |  Loss_1: (0.2326) | Acc_1: (91.03%) (14099/15488)\n",
      "Epoch: 183 | Batch_idx: 130 |  Loss_1: (0.2347) | Acc_1: (90.99%) (15257/16768)\n",
      "Epoch: 183 | Batch_idx: 140 |  Loss_1: (0.2360) | Acc_1: (90.92%) (16409/18048)\n",
      "Epoch: 183 | Batch_idx: 150 |  Loss_1: (0.2372) | Acc_1: (90.88%) (17565/19328)\n",
      "Epoch: 183 | Batch_idx: 160 |  Loss_1: (0.2376) | Acc_1: (90.84%) (18720/20608)\n",
      "Epoch: 183 | Batch_idx: 170 |  Loss_1: (0.2373) | Acc_1: (90.86%) (19888/21888)\n",
      "Epoch: 183 | Batch_idx: 180 |  Loss_1: (0.2382) | Acc_1: (90.85%) (21049/23168)\n",
      "Epoch: 183 | Batch_idx: 190 |  Loss_1: (0.2403) | Acc_1: (90.77%) (22191/24448)\n",
      "Epoch: 183 | Batch_idx: 200 |  Loss_1: (0.2412) | Acc_1: (90.73%) (23342/25728)\n",
      "Epoch: 183 | Batch_idx: 210 |  Loss_1: (0.2416) | Acc_1: (90.70%) (24495/27008)\n",
      "Epoch: 183 | Batch_idx: 220 |  Loss_1: (0.2403) | Acc_1: (90.75%) (25671/28288)\n",
      "Epoch: 183 | Batch_idx: 230 |  Loss_1: (0.2395) | Acc_1: (90.80%) (26848/29568)\n",
      "Epoch: 183 | Batch_idx: 240 |  Loss_1: (0.2393) | Acc_1: (90.82%) (28017/30848)\n",
      "Epoch: 183 | Batch_idx: 250 |  Loss_1: (0.2403) | Acc_1: (90.78%) (29166/32128)\n",
      "Epoch: 183 | Batch_idx: 260 |  Loss_1: (0.2405) | Acc_1: (90.76%) (30322/33408)\n",
      "Epoch: 183 | Batch_idx: 270 |  Loss_1: (0.2398) | Acc_1: (90.77%) (31488/34688)\n",
      "Epoch: 183 | Batch_idx: 280 |  Loss_1: (0.2401) | Acc_1: (90.75%) (32640/35968)\n",
      "Epoch: 183 | Batch_idx: 290 |  Loss_1: (0.2410) | Acc_1: (90.73%) (33794/37248)\n",
      "Epoch: 183 | Batch_idx: 300 |  Loss_1: (0.2417) | Acc_1: (90.70%) (34946/38528)\n",
      "Epoch: 183 | Batch_idx: 310 |  Loss_1: (0.2415) | Acc_1: (90.72%) (36113/39808)\n",
      "Epoch: 183 | Batch_idx: 320 |  Loss_1: (0.2408) | Acc_1: (90.74%) (37283/41088)\n",
      "Epoch: 183 | Batch_idx: 330 |  Loss_1: (0.2400) | Acc_1: (90.77%) (38458/42368)\n",
      "Epoch: 183 | Batch_idx: 340 |  Loss_1: (0.2402) | Acc_1: (90.77%) (39618/43648)\n",
      "Epoch: 183 | Batch_idx: 350 |  Loss_1: (0.2396) | Acc_1: (90.79%) (40790/44928)\n",
      "Epoch: 183 | Batch_idx: 360 |  Loss_1: (0.2390) | Acc_1: (90.81%) (41961/46208)\n",
      "Epoch: 183 | Batch_idx: 370 |  Loss_1: (0.2392) | Acc_1: (90.80%) (43121/47488)\n",
      "Epoch: 183 | Batch_idx: 380 |  Loss_1: (0.2398) | Acc_1: (90.78%) (44274/48768)\n",
      "Epoch: 183 | Batch_idx: 390 |  Loss_1: (0.2407) | Acc_1: (90.75%) (45375/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3277) | Acc: (93.10%) (9310/10000)\n",
      "Epoch: 184 | Batch_idx: 0 |  Loss_1: (0.2720) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 184 | Batch_idx: 10 |  Loss_1: (0.2544) | Acc_1: (89.99%) (1267/1408)\n",
      "Epoch: 184 | Batch_idx: 20 |  Loss_1: (0.2504) | Acc_1: (90.10%) (2422/2688)\n",
      "Epoch: 184 | Batch_idx: 30 |  Loss_1: (0.2457) | Acc_1: (90.32%) (3584/3968)\n",
      "Epoch: 184 | Batch_idx: 40 |  Loss_1: (0.2454) | Acc_1: (90.40%) (4744/5248)\n",
      "Epoch: 184 | Batch_idx: 50 |  Loss_1: (0.2377) | Acc_1: (90.67%) (5919/6528)\n",
      "Epoch: 184 | Batch_idx: 60 |  Loss_1: (0.2429) | Acc_1: (90.55%) (7070/7808)\n",
      "Epoch: 184 | Batch_idx: 70 |  Loss_1: (0.2379) | Acc_1: (90.70%) (8243/9088)\n",
      "Epoch: 184 | Batch_idx: 80 |  Loss_1: (0.2395) | Acc_1: (90.68%) (9402/10368)\n",
      "Epoch: 184 | Batch_idx: 90 |  Loss_1: (0.2380) | Acc_1: (90.82%) (10579/11648)\n",
      "Epoch: 184 | Batch_idx: 100 |  Loss_1: (0.2347) | Acc_1: (90.99%) (11763/12928)\n",
      "Epoch: 184 | Batch_idx: 110 |  Loss_1: (0.2353) | Acc_1: (90.96%) (12923/14208)\n",
      "Epoch: 184 | Batch_idx: 120 |  Loss_1: (0.2378) | Acc_1: (90.84%) (14070/15488)\n",
      "Epoch: 184 | Batch_idx: 130 |  Loss_1: (0.2387) | Acc_1: (90.85%) (15233/16768)\n",
      "Epoch: 184 | Batch_idx: 140 |  Loss_1: (0.2388) | Acc_1: (90.86%) (16398/18048)\n",
      "Epoch: 184 | Batch_idx: 150 |  Loss_1: (0.2391) | Acc_1: (90.87%) (17563/19328)\n",
      "Epoch: 184 | Batch_idx: 160 |  Loss_1: (0.2380) | Acc_1: (90.91%) (18735/20608)\n",
      "Epoch: 184 | Batch_idx: 170 |  Loss_1: (0.2369) | Acc_1: (90.96%) (19910/21888)\n",
      "Epoch: 184 | Batch_idx: 180 |  Loss_1: (0.2353) | Acc_1: (91.02%) (21088/23168)\n",
      "Epoch: 184 | Batch_idx: 190 |  Loss_1: (0.2352) | Acc_1: (91.03%) (22254/24448)\n",
      "Epoch: 184 | Batch_idx: 200 |  Loss_1: (0.2355) | Acc_1: (91.00%) (23412/25728)\n",
      "Epoch: 184 | Batch_idx: 210 |  Loss_1: (0.2347) | Acc_1: (91.04%) (24588/27008)\n",
      "Epoch: 184 | Batch_idx: 220 |  Loss_1: (0.2357) | Acc_1: (90.99%) (25739/28288)\n",
      "Epoch: 184 | Batch_idx: 230 |  Loss_1: (0.2360) | Acc_1: (90.98%) (26901/29568)\n",
      "Epoch: 184 | Batch_idx: 240 |  Loss_1: (0.2365) | Acc_1: (90.97%) (28061/30848)\n",
      "Epoch: 184 | Batch_idx: 250 |  Loss_1: (0.2376) | Acc_1: (90.91%) (29209/32128)\n",
      "Epoch: 184 | Batch_idx: 260 |  Loss_1: (0.2367) | Acc_1: (90.95%) (30383/33408)\n",
      "Epoch: 184 | Batch_idx: 270 |  Loss_1: (0.2371) | Acc_1: (90.94%) (31545/34688)\n",
      "Epoch: 184 | Batch_idx: 280 |  Loss_1: (0.2370) | Acc_1: (90.93%) (32707/35968)\n",
      "Epoch: 184 | Batch_idx: 290 |  Loss_1: (0.2377) | Acc_1: (90.90%) (33858/37248)\n",
      "Epoch: 184 | Batch_idx: 300 |  Loss_1: (0.2397) | Acc_1: (90.82%) (34992/38528)\n",
      "Epoch: 184 | Batch_idx: 310 |  Loss_1: (0.2405) | Acc_1: (90.78%) (36138/39808)\n",
      "Epoch: 184 | Batch_idx: 320 |  Loss_1: (0.2404) | Acc_1: (90.78%) (37300/41088)\n",
      "Epoch: 184 | Batch_idx: 330 |  Loss_1: (0.2400) | Acc_1: (90.78%) (38463/42368)\n",
      "Epoch: 184 | Batch_idx: 340 |  Loss_1: (0.2407) | Acc_1: (90.74%) (39607/43648)\n",
      "Epoch: 184 | Batch_idx: 350 |  Loss_1: (0.2412) | Acc_1: (90.73%) (40761/44928)\n",
      "Epoch: 184 | Batch_idx: 360 |  Loss_1: (0.2408) | Acc_1: (90.74%) (41928/46208)\n",
      "Epoch: 184 | Batch_idx: 370 |  Loss_1: (0.2408) | Acc_1: (90.73%) (43085/47488)\n",
      "Epoch: 184 | Batch_idx: 380 |  Loss_1: (0.2414) | Acc_1: (90.70%) (44235/48768)\n",
      "Epoch: 184 | Batch_idx: 390 |  Loss_1: (0.2415) | Acc_1: (90.71%) (45355/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3084) | Acc: (93.26%) (9326/10000)\n",
      "Epoch: 185 | Batch_idx: 0 |  Loss_1: (0.1677) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 185 | Batch_idx: 10 |  Loss_1: (0.2283) | Acc_1: (91.76%) (1292/1408)\n",
      "Epoch: 185 | Batch_idx: 20 |  Loss_1: (0.2343) | Acc_1: (91.41%) (2457/2688)\n",
      "Epoch: 185 | Batch_idx: 30 |  Loss_1: (0.2351) | Acc_1: (91.26%) (3621/3968)\n",
      "Epoch: 185 | Batch_idx: 40 |  Loss_1: (0.2395) | Acc_1: (90.87%) (4769/5248)\n",
      "Epoch: 185 | Batch_idx: 50 |  Loss_1: (0.2466) | Acc_1: (90.64%) (5917/6528)\n",
      "Epoch: 185 | Batch_idx: 60 |  Loss_1: (0.2431) | Acc_1: (90.80%) (7090/7808)\n",
      "Epoch: 185 | Batch_idx: 70 |  Loss_1: (0.2414) | Acc_1: (90.82%) (8254/9088)\n",
      "Epoch: 185 | Batch_idx: 80 |  Loss_1: (0.2422) | Acc_1: (90.79%) (9413/10368)\n",
      "Epoch: 185 | Batch_idx: 90 |  Loss_1: (0.2428) | Acc_1: (90.72%) (10567/11648)\n",
      "Epoch: 185 | Batch_idx: 100 |  Loss_1: (0.2410) | Acc_1: (90.76%) (11733/12928)\n",
      "Epoch: 185 | Batch_idx: 110 |  Loss_1: (0.2407) | Acc_1: (90.75%) (12894/14208)\n",
      "Epoch: 185 | Batch_idx: 120 |  Loss_1: (0.2388) | Acc_1: (90.80%) (14063/15488)\n",
      "Epoch: 185 | Batch_idx: 130 |  Loss_1: (0.2385) | Acc_1: (90.82%) (15228/16768)\n",
      "Epoch: 185 | Batch_idx: 140 |  Loss_1: (0.2375) | Acc_1: (90.84%) (16394/18048)\n",
      "Epoch: 185 | Batch_idx: 150 |  Loss_1: (0.2364) | Acc_1: (90.90%) (17569/19328)\n",
      "Epoch: 185 | Batch_idx: 160 |  Loss_1: (0.2357) | Acc_1: (90.91%) (18734/20608)\n",
      "Epoch: 185 | Batch_idx: 170 |  Loss_1: (0.2346) | Acc_1: (90.92%) (19901/21888)\n",
      "Epoch: 185 | Batch_idx: 180 |  Loss_1: (0.2358) | Acc_1: (90.88%) (21054/23168)\n",
      "Epoch: 185 | Batch_idx: 190 |  Loss_1: (0.2355) | Acc_1: (90.86%) (22214/24448)\n",
      "Epoch: 185 | Batch_idx: 200 |  Loss_1: (0.2342) | Acc_1: (90.90%) (23388/25728)\n",
      "Epoch: 185 | Batch_idx: 210 |  Loss_1: (0.2336) | Acc_1: (90.93%) (24558/27008)\n",
      "Epoch: 185 | Batch_idx: 220 |  Loss_1: (0.2336) | Acc_1: (90.92%) (25720/28288)\n",
      "Epoch: 185 | Batch_idx: 230 |  Loss_1: (0.2340) | Acc_1: (90.92%) (26884/29568)\n",
      "Epoch: 185 | Batch_idx: 240 |  Loss_1: (0.2346) | Acc_1: (90.88%) (28036/30848)\n",
      "Epoch: 185 | Batch_idx: 250 |  Loss_1: (0.2339) | Acc_1: (90.93%) (29214/32128)\n",
      "Epoch: 185 | Batch_idx: 260 |  Loss_1: (0.2338) | Acc_1: (90.95%) (30386/33408)\n",
      "Epoch: 185 | Batch_idx: 270 |  Loss_1: (0.2334) | Acc_1: (90.95%) (31550/34688)\n",
      "Epoch: 185 | Batch_idx: 280 |  Loss_1: (0.2340) | Acc_1: (90.93%) (32707/35968)\n",
      "Epoch: 185 | Batch_idx: 290 |  Loss_1: (0.2338) | Acc_1: (90.96%) (33879/37248)\n",
      "Epoch: 185 | Batch_idx: 300 |  Loss_1: (0.2331) | Acc_1: (91.00%) (35061/38528)\n",
      "Epoch: 185 | Batch_idx: 310 |  Loss_1: (0.2338) | Acc_1: (91.00%) (36224/39808)\n",
      "Epoch: 185 | Batch_idx: 320 |  Loss_1: (0.2347) | Acc_1: (90.97%) (37377/41088)\n",
      "Epoch: 185 | Batch_idx: 330 |  Loss_1: (0.2352) | Acc_1: (90.95%) (38534/42368)\n",
      "Epoch: 185 | Batch_idx: 340 |  Loss_1: (0.2357) | Acc_1: (90.92%) (39683/43648)\n",
      "Epoch: 185 | Batch_idx: 350 |  Loss_1: (0.2352) | Acc_1: (90.93%) (40851/44928)\n",
      "Epoch: 185 | Batch_idx: 360 |  Loss_1: (0.2355) | Acc_1: (90.91%) (42007/46208)\n",
      "Epoch: 185 | Batch_idx: 370 |  Loss_1: (0.2342) | Acc_1: (90.97%) (43199/47488)\n",
      "Epoch: 185 | Batch_idx: 380 |  Loss_1: (0.2343) | Acc_1: (90.97%) (44366/48768)\n",
      "Epoch: 185 | Batch_idx: 390 |  Loss_1: (0.2353) | Acc_1: (90.93%) (45464/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2960) | Acc: (93.33%) (9333/10000)\n",
      "Epoch: 186 | Batch_idx: 0 |  Loss_1: (0.2337) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 186 | Batch_idx: 10 |  Loss_1: (0.2296) | Acc_1: (90.98%) (1281/1408)\n",
      "Epoch: 186 | Batch_idx: 20 |  Loss_1: (0.2348) | Acc_1: (90.92%) (2444/2688)\n",
      "Epoch: 186 | Batch_idx: 30 |  Loss_1: (0.2474) | Acc_1: (90.42%) (3588/3968)\n",
      "Epoch: 186 | Batch_idx: 40 |  Loss_1: (0.2454) | Acc_1: (90.45%) (4747/5248)\n",
      "Epoch: 186 | Batch_idx: 50 |  Loss_1: (0.2433) | Acc_1: (90.62%) (5916/6528)\n",
      "Epoch: 186 | Batch_idx: 60 |  Loss_1: (0.2374) | Acc_1: (90.83%) (7092/7808)\n",
      "Epoch: 186 | Batch_idx: 70 |  Loss_1: (0.2403) | Acc_1: (90.71%) (8244/9088)\n",
      "Epoch: 186 | Batch_idx: 80 |  Loss_1: (0.2388) | Acc_1: (90.73%) (9407/10368)\n",
      "Epoch: 186 | Batch_idx: 90 |  Loss_1: (0.2400) | Acc_1: (90.64%) (10558/11648)\n",
      "Epoch: 186 | Batch_idx: 100 |  Loss_1: (0.2424) | Acc_1: (90.55%) (11706/12928)\n",
      "Epoch: 186 | Batch_idx: 110 |  Loss_1: (0.2408) | Acc_1: (90.62%) (12876/14208)\n",
      "Epoch: 186 | Batch_idx: 120 |  Loss_1: (0.2388) | Acc_1: (90.72%) (14051/15488)\n",
      "Epoch: 186 | Batch_idx: 130 |  Loss_1: (0.2391) | Acc_1: (90.71%) (15210/16768)\n",
      "Epoch: 186 | Batch_idx: 140 |  Loss_1: (0.2390) | Acc_1: (90.71%) (16371/18048)\n",
      "Epoch: 186 | Batch_idx: 150 |  Loss_1: (0.2379) | Acc_1: (90.78%) (17545/19328)\n",
      "Epoch: 186 | Batch_idx: 160 |  Loss_1: (0.2379) | Acc_1: (90.79%) (18710/20608)\n",
      "Epoch: 186 | Batch_idx: 170 |  Loss_1: (0.2379) | Acc_1: (90.81%) (19877/21888)\n",
      "Epoch: 186 | Batch_idx: 180 |  Loss_1: (0.2391) | Acc_1: (90.78%) (21033/23168)\n",
      "Epoch: 186 | Batch_idx: 190 |  Loss_1: (0.2385) | Acc_1: (90.80%) (22200/24448)\n",
      "Epoch: 186 | Batch_idx: 200 |  Loss_1: (0.2375) | Acc_1: (90.82%) (23367/25728)\n",
      "Epoch: 186 | Batch_idx: 210 |  Loss_1: (0.2374) | Acc_1: (90.82%) (24529/27008)\n",
      "Epoch: 186 | Batch_idx: 220 |  Loss_1: (0.2368) | Acc_1: (90.84%) (25698/28288)\n",
      "Epoch: 186 | Batch_idx: 230 |  Loss_1: (0.2375) | Acc_1: (90.80%) (26849/29568)\n",
      "Epoch: 186 | Batch_idx: 240 |  Loss_1: (0.2382) | Acc_1: (90.76%) (27998/30848)\n",
      "Epoch: 186 | Batch_idx: 250 |  Loss_1: (0.2387) | Acc_1: (90.75%) (29155/32128)\n",
      "Epoch: 186 | Batch_idx: 260 |  Loss_1: (0.2386) | Acc_1: (90.74%) (30313/33408)\n",
      "Epoch: 186 | Batch_idx: 270 |  Loss_1: (0.2399) | Acc_1: (90.69%) (31457/34688)\n",
      "Epoch: 186 | Batch_idx: 280 |  Loss_1: (0.2401) | Acc_1: (90.68%) (32616/35968)\n",
      "Epoch: 186 | Batch_idx: 290 |  Loss_1: (0.2402) | Acc_1: (90.66%) (33768/37248)\n",
      "Epoch: 186 | Batch_idx: 300 |  Loss_1: (0.2405) | Acc_1: (90.66%) (34930/38528)\n",
      "Epoch: 186 | Batch_idx: 310 |  Loss_1: (0.2413) | Acc_1: (90.64%) (36083/39808)\n",
      "Epoch: 186 | Batch_idx: 320 |  Loss_1: (0.2406) | Acc_1: (90.67%) (37253/41088)\n",
      "Epoch: 186 | Batch_idx: 330 |  Loss_1: (0.2413) | Acc_1: (90.64%) (38403/42368)\n",
      "Epoch: 186 | Batch_idx: 340 |  Loss_1: (0.2406) | Acc_1: (90.67%) (39574/43648)\n",
      "Epoch: 186 | Batch_idx: 350 |  Loss_1: (0.2413) | Acc_1: (90.63%) (40718/44928)\n",
      "Epoch: 186 | Batch_idx: 360 |  Loss_1: (0.2411) | Acc_1: (90.64%) (41884/46208)\n",
      "Epoch: 186 | Batch_idx: 370 |  Loss_1: (0.2406) | Acc_1: (90.65%) (43048/47488)\n",
      "Epoch: 186 | Batch_idx: 380 |  Loss_1: (0.2411) | Acc_1: (90.64%) (44202/48768)\n",
      "Epoch: 186 | Batch_idx: 390 |  Loss_1: (0.2411) | Acc_1: (90.64%) (45322/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3148) | Acc: (93.36%) (9336/10000)\n",
      "Epoch: 187 | Batch_idx: 0 |  Loss_1: (0.2877) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 187 | Batch_idx: 10 |  Loss_1: (0.2452) | Acc_1: (90.34%) (1272/1408)\n",
      "Epoch: 187 | Batch_idx: 20 |  Loss_1: (0.2525) | Acc_1: (89.99%) (2419/2688)\n",
      "Epoch: 187 | Batch_idx: 30 |  Loss_1: (0.2463) | Acc_1: (90.35%) (3585/3968)\n",
      "Epoch: 187 | Batch_idx: 40 |  Loss_1: (0.2463) | Acc_1: (90.36%) (4742/5248)\n",
      "Epoch: 187 | Batch_idx: 50 |  Loss_1: (0.2540) | Acc_1: (90.21%) (5889/6528)\n",
      "Epoch: 187 | Batch_idx: 60 |  Loss_1: (0.2491) | Acc_1: (90.37%) (7056/7808)\n",
      "Epoch: 187 | Batch_idx: 70 |  Loss_1: (0.2465) | Acc_1: (90.56%) (8230/9088)\n",
      "Epoch: 187 | Batch_idx: 80 |  Loss_1: (0.2478) | Acc_1: (90.50%) (9383/10368)\n",
      "Epoch: 187 | Batch_idx: 90 |  Loss_1: (0.2446) | Acc_1: (90.59%) (10552/11648)\n",
      "Epoch: 187 | Batch_idx: 100 |  Loss_1: (0.2433) | Acc_1: (90.62%) (11716/12928)\n",
      "Epoch: 187 | Batch_idx: 110 |  Loss_1: (0.2409) | Acc_1: (90.70%) (12886/14208)\n",
      "Epoch: 187 | Batch_idx: 120 |  Loss_1: (0.2434) | Acc_1: (90.62%) (14036/15488)\n",
      "Epoch: 187 | Batch_idx: 130 |  Loss_1: (0.2437) | Acc_1: (90.62%) (15196/16768)\n",
      "Epoch: 187 | Batch_idx: 140 |  Loss_1: (0.2427) | Acc_1: (90.66%) (16363/18048)\n",
      "Epoch: 187 | Batch_idx: 150 |  Loss_1: (0.2430) | Acc_1: (90.65%) (17521/19328)\n",
      "Epoch: 187 | Batch_idx: 160 |  Loss_1: (0.2437) | Acc_1: (90.64%) (18680/20608)\n",
      "Epoch: 187 | Batch_idx: 170 |  Loss_1: (0.2438) | Acc_1: (90.66%) (19843/21888)\n",
      "Epoch: 187 | Batch_idx: 180 |  Loss_1: (0.2444) | Acc_1: (90.62%) (20995/23168)\n",
      "Epoch: 187 | Batch_idx: 190 |  Loss_1: (0.2434) | Acc_1: (90.68%) (22170/24448)\n",
      "Epoch: 187 | Batch_idx: 200 |  Loss_1: (0.2444) | Acc_1: (90.63%) (23318/25728)\n",
      "Epoch: 187 | Batch_idx: 210 |  Loss_1: (0.2462) | Acc_1: (90.54%) (24452/27008)\n",
      "Epoch: 187 | Batch_idx: 220 |  Loss_1: (0.2460) | Acc_1: (90.55%) (25615/28288)\n",
      "Epoch: 187 | Batch_idx: 230 |  Loss_1: (0.2453) | Acc_1: (90.56%) (26777/29568)\n",
      "Epoch: 187 | Batch_idx: 240 |  Loss_1: (0.2441) | Acc_1: (90.61%) (27951/30848)\n",
      "Epoch: 187 | Batch_idx: 250 |  Loss_1: (0.2430) | Acc_1: (90.67%) (29129/32128)\n",
      "Epoch: 187 | Batch_idx: 260 |  Loss_1: (0.2441) | Acc_1: (90.61%) (30271/33408)\n",
      "Epoch: 187 | Batch_idx: 270 |  Loss_1: (0.2433) | Acc_1: (90.63%) (31438/34688)\n",
      "Epoch: 187 | Batch_idx: 280 |  Loss_1: (0.2433) | Acc_1: (90.65%) (32604/35968)\n",
      "Epoch: 187 | Batch_idx: 290 |  Loss_1: (0.2432) | Acc_1: (90.66%) (33768/37248)\n",
      "Epoch: 187 | Batch_idx: 300 |  Loss_1: (0.2419) | Acc_1: (90.70%) (34944/38528)\n",
      "Epoch: 187 | Batch_idx: 310 |  Loss_1: (0.2419) | Acc_1: (90.70%) (36106/39808)\n",
      "Epoch: 187 | Batch_idx: 320 |  Loss_1: (0.2415) | Acc_1: (90.71%) (37271/41088)\n",
      "Epoch: 187 | Batch_idx: 330 |  Loss_1: (0.2417) | Acc_1: (90.71%) (38430/42368)\n",
      "Epoch: 187 | Batch_idx: 340 |  Loss_1: (0.2419) | Acc_1: (90.69%) (39584/43648)\n",
      "Epoch: 187 | Batch_idx: 350 |  Loss_1: (0.2420) | Acc_1: (90.69%) (40745/44928)\n",
      "Epoch: 187 | Batch_idx: 360 |  Loss_1: (0.2412) | Acc_1: (90.71%) (41916/46208)\n",
      "Epoch: 187 | Batch_idx: 370 |  Loss_1: (0.2413) | Acc_1: (90.70%) (43071/47488)\n",
      "Epoch: 187 | Batch_idx: 380 |  Loss_1: (0.2414) | Acc_1: (90.69%) (44230/48768)\n",
      "Epoch: 187 | Batch_idx: 390 |  Loss_1: (0.2413) | Acc_1: (90.70%) (45348/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3584) | Acc: (92.97%) (9297/10000)\n",
      "Epoch: 188 | Batch_idx: 0 |  Loss_1: (0.2859) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 188 | Batch_idx: 10 |  Loss_1: (0.2411) | Acc_1: (91.05%) (1282/1408)\n",
      "Epoch: 188 | Batch_idx: 20 |  Loss_1: (0.2451) | Acc_1: (90.70%) (2438/2688)\n",
      "Epoch: 188 | Batch_idx: 30 |  Loss_1: (0.2475) | Acc_1: (90.47%) (3590/3968)\n",
      "Epoch: 188 | Batch_idx: 40 |  Loss_1: (0.2484) | Acc_1: (90.49%) (4749/5248)\n",
      "Epoch: 188 | Batch_idx: 50 |  Loss_1: (0.2464) | Acc_1: (90.55%) (5911/6528)\n",
      "Epoch: 188 | Batch_idx: 60 |  Loss_1: (0.2508) | Acc_1: (90.42%) (7060/7808)\n",
      "Epoch: 188 | Batch_idx: 70 |  Loss_1: (0.2456) | Acc_1: (90.59%) (8233/9088)\n",
      "Epoch: 188 | Batch_idx: 80 |  Loss_1: (0.2421) | Acc_1: (90.69%) (9403/10368)\n",
      "Epoch: 188 | Batch_idx: 90 |  Loss_1: (0.2413) | Acc_1: (90.75%) (10570/11648)\n",
      "Epoch: 188 | Batch_idx: 100 |  Loss_1: (0.2413) | Acc_1: (90.74%) (11731/12928)\n",
      "Epoch: 188 | Batch_idx: 110 |  Loss_1: (0.2415) | Acc_1: (90.74%) (12892/14208)\n",
      "Epoch: 188 | Batch_idx: 120 |  Loss_1: (0.2447) | Acc_1: (90.67%) (14043/15488)\n",
      "Epoch: 188 | Batch_idx: 130 |  Loss_1: (0.2459) | Acc_1: (90.62%) (15196/16768)\n",
      "Epoch: 188 | Batch_idx: 140 |  Loss_1: (0.2468) | Acc_1: (90.63%) (16357/18048)\n",
      "Epoch: 188 | Batch_idx: 150 |  Loss_1: (0.2459) | Acc_1: (90.72%) (17534/19328)\n",
      "Epoch: 188 | Batch_idx: 160 |  Loss_1: (0.2453) | Acc_1: (90.73%) (18698/20608)\n",
      "Epoch: 188 | Batch_idx: 170 |  Loss_1: (0.2440) | Acc_1: (90.78%) (19871/21888)\n",
      "Epoch: 188 | Batch_idx: 180 |  Loss_1: (0.2440) | Acc_1: (90.80%) (21036/23168)\n",
      "Epoch: 188 | Batch_idx: 190 |  Loss_1: (0.2428) | Acc_1: (90.84%) (22209/24448)\n",
      "Epoch: 188 | Batch_idx: 200 |  Loss_1: (0.2432) | Acc_1: (90.81%) (23364/25728)\n",
      "Epoch: 188 | Batch_idx: 210 |  Loss_1: (0.2437) | Acc_1: (90.80%) (24522/27008)\n",
      "Epoch: 188 | Batch_idx: 220 |  Loss_1: (0.2428) | Acc_1: (90.79%) (25684/28288)\n",
      "Epoch: 188 | Batch_idx: 230 |  Loss_1: (0.2427) | Acc_1: (90.79%) (26846/29568)\n",
      "Epoch: 188 | Batch_idx: 240 |  Loss_1: (0.2420) | Acc_1: (90.83%) (28018/30848)\n",
      "Epoch: 188 | Batch_idx: 250 |  Loss_1: (0.2415) | Acc_1: (90.84%) (29185/32128)\n",
      "Epoch: 188 | Batch_idx: 260 |  Loss_1: (0.2415) | Acc_1: (90.84%) (30348/33408)\n",
      "Epoch: 188 | Batch_idx: 270 |  Loss_1: (0.2410) | Acc_1: (90.86%) (31516/34688)\n",
      "Epoch: 188 | Batch_idx: 280 |  Loss_1: (0.2412) | Acc_1: (90.85%) (32678/35968)\n",
      "Epoch: 188 | Batch_idx: 290 |  Loss_1: (0.2410) | Acc_1: (90.86%) (33845/37248)\n",
      "Epoch: 188 | Batch_idx: 300 |  Loss_1: (0.2409) | Acc_1: (90.86%) (35005/38528)\n",
      "Epoch: 188 | Batch_idx: 310 |  Loss_1: (0.2415) | Acc_1: (90.83%) (36156/39808)\n",
      "Epoch: 188 | Batch_idx: 320 |  Loss_1: (0.2412) | Acc_1: (90.83%) (37320/41088)\n",
      "Epoch: 188 | Batch_idx: 330 |  Loss_1: (0.2415) | Acc_1: (90.81%) (38474/42368)\n",
      "Epoch: 188 | Batch_idx: 340 |  Loss_1: (0.2408) | Acc_1: (90.82%) (39643/43648)\n",
      "Epoch: 188 | Batch_idx: 350 |  Loss_1: (0.2410) | Acc_1: (90.81%) (40800/44928)\n",
      "Epoch: 188 | Batch_idx: 360 |  Loss_1: (0.2405) | Acc_1: (90.81%) (41962/46208)\n",
      "Epoch: 188 | Batch_idx: 370 |  Loss_1: (0.2406) | Acc_1: (90.79%) (43115/47488)\n",
      "Epoch: 188 | Batch_idx: 380 |  Loss_1: (0.2404) | Acc_1: (90.81%) (44285/48768)\n",
      "Epoch: 188 | Batch_idx: 390 |  Loss_1: (0.2420) | Acc_1: (90.76%) (45378/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3195) | Acc: (93.32%) (9332/10000)\n",
      "Epoch: 189 | Batch_idx: 0 |  Loss_1: (0.1482) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 189 | Batch_idx: 10 |  Loss_1: (0.2664) | Acc_1: (90.55%) (1275/1408)\n",
      "Epoch: 189 | Batch_idx: 20 |  Loss_1: (0.2460) | Acc_1: (90.81%) (2441/2688)\n",
      "Epoch: 189 | Batch_idx: 30 |  Loss_1: (0.2383) | Acc_1: (91.03%) (3612/3968)\n",
      "Epoch: 189 | Batch_idx: 40 |  Loss_1: (0.2393) | Acc_1: (90.89%) (4770/5248)\n",
      "Epoch: 189 | Batch_idx: 50 |  Loss_1: (0.2371) | Acc_1: (90.87%) (5932/6528)\n",
      "Epoch: 189 | Batch_idx: 60 |  Loss_1: (0.2456) | Acc_1: (90.52%) (7068/7808)\n",
      "Epoch: 189 | Batch_idx: 70 |  Loss_1: (0.2462) | Acc_1: (90.50%) (8225/9088)\n",
      "Epoch: 189 | Batch_idx: 80 |  Loss_1: (0.2427) | Acc_1: (90.59%) (9392/10368)\n",
      "Epoch: 189 | Batch_idx: 90 |  Loss_1: (0.2405) | Acc_1: (90.68%) (10562/11648)\n",
      "Epoch: 189 | Batch_idx: 100 |  Loss_1: (0.2406) | Acc_1: (90.70%) (11726/12928)\n",
      "Epoch: 189 | Batch_idx: 110 |  Loss_1: (0.2394) | Acc_1: (90.72%) (12890/14208)\n",
      "Epoch: 189 | Batch_idx: 120 |  Loss_1: (0.2411) | Acc_1: (90.66%) (14041/15488)\n",
      "Epoch: 189 | Batch_idx: 130 |  Loss_1: (0.2405) | Acc_1: (90.68%) (15206/16768)\n",
      "Epoch: 189 | Batch_idx: 140 |  Loss_1: (0.2404) | Acc_1: (90.67%) (16365/18048)\n",
      "Epoch: 189 | Batch_idx: 150 |  Loss_1: (0.2398) | Acc_1: (90.70%) (17530/19328)\n",
      "Epoch: 189 | Batch_idx: 160 |  Loss_1: (0.2396) | Acc_1: (90.71%) (18693/20608)\n",
      "Epoch: 189 | Batch_idx: 170 |  Loss_1: (0.2399) | Acc_1: (90.75%) (19863/21888)\n",
      "Epoch: 189 | Batch_idx: 180 |  Loss_1: (0.2419) | Acc_1: (90.66%) (21004/23168)\n",
      "Epoch: 189 | Batch_idx: 190 |  Loss_1: (0.2409) | Acc_1: (90.70%) (22175/24448)\n",
      "Epoch: 189 | Batch_idx: 200 |  Loss_1: (0.2403) | Acc_1: (90.73%) (23344/25728)\n",
      "Epoch: 189 | Batch_idx: 210 |  Loss_1: (0.2412) | Acc_1: (90.71%) (24500/27008)\n",
      "Epoch: 189 | Batch_idx: 220 |  Loss_1: (0.2422) | Acc_1: (90.66%) (25645/28288)\n",
      "Epoch: 189 | Batch_idx: 230 |  Loss_1: (0.2421) | Acc_1: (90.63%) (26798/29568)\n",
      "Epoch: 189 | Batch_idx: 240 |  Loss_1: (0.2423) | Acc_1: (90.63%) (27957/30848)\n",
      "Epoch: 189 | Batch_idx: 250 |  Loss_1: (0.2418) | Acc_1: (90.66%) (29126/32128)\n",
      "Epoch: 189 | Batch_idx: 260 |  Loss_1: (0.2414) | Acc_1: (90.67%) (30291/33408)\n",
      "Epoch: 189 | Batch_idx: 270 |  Loss_1: (0.2420) | Acc_1: (90.65%) (31443/34688)\n",
      "Epoch: 189 | Batch_idx: 280 |  Loss_1: (0.2432) | Acc_1: (90.59%) (32584/35968)\n",
      "Epoch: 189 | Batch_idx: 290 |  Loss_1: (0.2434) | Acc_1: (90.57%) (33736/37248)\n",
      "Epoch: 189 | Batch_idx: 300 |  Loss_1: (0.2430) | Acc_1: (90.58%) (34899/38528)\n",
      "Epoch: 189 | Batch_idx: 310 |  Loss_1: (0.2419) | Acc_1: (90.62%) (36075/39808)\n",
      "Epoch: 189 | Batch_idx: 320 |  Loss_1: (0.2429) | Acc_1: (90.58%) (37218/41088)\n",
      "Epoch: 189 | Batch_idx: 330 |  Loss_1: (0.2421) | Acc_1: (90.61%) (38390/42368)\n",
      "Epoch: 189 | Batch_idx: 340 |  Loss_1: (0.2421) | Acc_1: (90.60%) (39545/43648)\n",
      "Epoch: 189 | Batch_idx: 350 |  Loss_1: (0.2420) | Acc_1: (90.59%) (40702/44928)\n",
      "Epoch: 189 | Batch_idx: 360 |  Loss_1: (0.2426) | Acc_1: (90.57%) (41849/46208)\n",
      "Epoch: 189 | Batch_idx: 370 |  Loss_1: (0.2429) | Acc_1: (90.57%) (43009/47488)\n",
      "Epoch: 189 | Batch_idx: 380 |  Loss_1: (0.2427) | Acc_1: (90.57%) (44169/48768)\n",
      "Epoch: 189 | Batch_idx: 390 |  Loss_1: (0.2430) | Acc_1: (90.55%) (45273/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3225) | Acc: (93.34%) (9334/10000)\n",
      "Epoch: 190 | Batch_idx: 0 |  Loss_1: (0.1280) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 190 | Batch_idx: 10 |  Loss_1: (0.2107) | Acc_1: (91.62%) (1290/1408)\n",
      "Epoch: 190 | Batch_idx: 20 |  Loss_1: (0.2157) | Acc_1: (91.48%) (2459/2688)\n",
      "Epoch: 190 | Batch_idx: 30 |  Loss_1: (0.2150) | Acc_1: (91.46%) (3629/3968)\n",
      "Epoch: 190 | Batch_idx: 40 |  Loss_1: (0.2228) | Acc_1: (91.18%) (4785/5248)\n",
      "Epoch: 190 | Batch_idx: 50 |  Loss_1: (0.2195) | Acc_1: (91.30%) (5960/6528)\n",
      "Epoch: 190 | Batch_idx: 60 |  Loss_1: (0.2219) | Acc_1: (91.25%) (7125/7808)\n",
      "Epoch: 190 | Batch_idx: 70 |  Loss_1: (0.2229) | Acc_1: (91.26%) (8294/9088)\n",
      "Epoch: 190 | Batch_idx: 80 |  Loss_1: (0.2281) | Acc_1: (91.07%) (9442/10368)\n",
      "Epoch: 190 | Batch_idx: 90 |  Loss_1: (0.2304) | Acc_1: (91.01%) (10601/11648)\n",
      "Epoch: 190 | Batch_idx: 100 |  Loss_1: (0.2326) | Acc_1: (90.93%) (11756/12928)\n",
      "Epoch: 190 | Batch_idx: 110 |  Loss_1: (0.2335) | Acc_1: (90.92%) (12918/14208)\n",
      "Epoch: 190 | Batch_idx: 120 |  Loss_1: (0.2384) | Acc_1: (90.77%) (14059/15488)\n",
      "Epoch: 190 | Batch_idx: 130 |  Loss_1: (0.2394) | Acc_1: (90.71%) (15210/16768)\n",
      "Epoch: 190 | Batch_idx: 140 |  Loss_1: (0.2373) | Acc_1: (90.78%) (16384/18048)\n",
      "Epoch: 190 | Batch_idx: 150 |  Loss_1: (0.2399) | Acc_1: (90.70%) (17530/19328)\n",
      "Epoch: 190 | Batch_idx: 160 |  Loss_1: (0.2396) | Acc_1: (90.72%) (18696/20608)\n",
      "Epoch: 190 | Batch_idx: 170 |  Loss_1: (0.2405) | Acc_1: (90.66%) (19844/21888)\n",
      "Epoch: 190 | Batch_idx: 180 |  Loss_1: (0.2389) | Acc_1: (90.73%) (21021/23168)\n",
      "Epoch: 190 | Batch_idx: 190 |  Loss_1: (0.2389) | Acc_1: (90.76%) (22189/24448)\n",
      "Epoch: 190 | Batch_idx: 200 |  Loss_1: (0.2394) | Acc_1: (90.75%) (23348/25728)\n",
      "Epoch: 190 | Batch_idx: 210 |  Loss_1: (0.2396) | Acc_1: (90.75%) (24509/27008)\n",
      "Epoch: 190 | Batch_idx: 220 |  Loss_1: (0.2395) | Acc_1: (90.76%) (25674/28288)\n",
      "Epoch: 190 | Batch_idx: 230 |  Loss_1: (0.2384) | Acc_1: (90.80%) (26848/29568)\n",
      "Epoch: 190 | Batch_idx: 240 |  Loss_1: (0.2385) | Acc_1: (90.79%) (28006/30848)\n",
      "Epoch: 190 | Batch_idx: 250 |  Loss_1: (0.2385) | Acc_1: (90.83%) (29181/32128)\n",
      "Epoch: 190 | Batch_idx: 260 |  Loss_1: (0.2380) | Acc_1: (90.84%) (30347/33408)\n",
      "Epoch: 190 | Batch_idx: 270 |  Loss_1: (0.2380) | Acc_1: (90.84%) (31511/34688)\n",
      "Epoch: 190 | Batch_idx: 280 |  Loss_1: (0.2374) | Acc_1: (90.87%) (32683/35968)\n",
      "Epoch: 190 | Batch_idx: 290 |  Loss_1: (0.2382) | Acc_1: (90.85%) (33838/37248)\n",
      "Epoch: 190 | Batch_idx: 300 |  Loss_1: (0.2379) | Acc_1: (90.85%) (35003/38528)\n",
      "Epoch: 190 | Batch_idx: 310 |  Loss_1: (0.2376) | Acc_1: (90.86%) (36169/39808)\n",
      "Epoch: 190 | Batch_idx: 320 |  Loss_1: (0.2376) | Acc_1: (90.86%) (37333/41088)\n",
      "Epoch: 190 | Batch_idx: 330 |  Loss_1: (0.2374) | Acc_1: (90.87%) (38498/42368)\n",
      "Epoch: 190 | Batch_idx: 340 |  Loss_1: (0.2376) | Acc_1: (90.87%) (39662/43648)\n",
      "Epoch: 190 | Batch_idx: 350 |  Loss_1: (0.2385) | Acc_1: (90.84%) (40813/44928)\n",
      "Epoch: 190 | Batch_idx: 360 |  Loss_1: (0.2381) | Acc_1: (90.86%) (41985/46208)\n",
      "Epoch: 190 | Batch_idx: 370 |  Loss_1: (0.2383) | Acc_1: (90.85%) (43144/47488)\n",
      "Epoch: 190 | Batch_idx: 380 |  Loss_1: (0.2385) | Acc_1: (90.84%) (44300/48768)\n",
      "Epoch: 190 | Batch_idx: 390 |  Loss_1: (0.2393) | Acc_1: (90.81%) (45407/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3175) | Acc: (93.18%) (9318/10000)\n",
      "Epoch: 191 | Batch_idx: 0 |  Loss_1: (0.3016) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 191 | Batch_idx: 10 |  Loss_1: (0.2229) | Acc_1: (92.12%) (1297/1408)\n",
      "Epoch: 191 | Batch_idx: 20 |  Loss_1: (0.2235) | Acc_1: (91.82%) (2468/2688)\n",
      "Epoch: 191 | Batch_idx: 30 |  Loss_1: (0.2273) | Acc_1: (91.48%) (3630/3968)\n",
      "Epoch: 191 | Batch_idx: 40 |  Loss_1: (0.2260) | Acc_1: (91.44%) (4799/5248)\n",
      "Epoch: 191 | Batch_idx: 50 |  Loss_1: (0.2359) | Acc_1: (91.07%) (5945/6528)\n",
      "Epoch: 191 | Batch_idx: 60 |  Loss_1: (0.2387) | Acc_1: (90.91%) (7098/7808)\n",
      "Epoch: 191 | Batch_idx: 70 |  Loss_1: (0.2383) | Acc_1: (90.92%) (8263/9088)\n",
      "Epoch: 191 | Batch_idx: 80 |  Loss_1: (0.2387) | Acc_1: (90.90%) (9425/10368)\n",
      "Epoch: 191 | Batch_idx: 90 |  Loss_1: (0.2429) | Acc_1: (90.67%) (10561/11648)\n",
      "Epoch: 191 | Batch_idx: 100 |  Loss_1: (0.2415) | Acc_1: (90.70%) (11726/12928)\n",
      "Epoch: 191 | Batch_idx: 110 |  Loss_1: (0.2416) | Acc_1: (90.69%) (12885/14208)\n",
      "Epoch: 191 | Batch_idx: 120 |  Loss_1: (0.2426) | Acc_1: (90.62%) (14036/15488)\n",
      "Epoch: 191 | Batch_idx: 130 |  Loss_1: (0.2404) | Acc_1: (90.68%) (15205/16768)\n",
      "Epoch: 191 | Batch_idx: 140 |  Loss_1: (0.2397) | Acc_1: (90.69%) (16367/18048)\n",
      "Epoch: 191 | Batch_idx: 150 |  Loss_1: (0.2405) | Acc_1: (90.68%) (17526/19328)\n",
      "Epoch: 191 | Batch_idx: 160 |  Loss_1: (0.2410) | Acc_1: (90.66%) (18683/20608)\n",
      "Epoch: 191 | Batch_idx: 170 |  Loss_1: (0.2396) | Acc_1: (90.72%) (19856/21888)\n",
      "Epoch: 191 | Batch_idx: 180 |  Loss_1: (0.2397) | Acc_1: (90.73%) (21021/23168)\n",
      "Epoch: 191 | Batch_idx: 190 |  Loss_1: (0.2399) | Acc_1: (90.73%) (22182/24448)\n",
      "Epoch: 191 | Batch_idx: 200 |  Loss_1: (0.2401) | Acc_1: (90.73%) (23342/25728)\n",
      "Epoch: 191 | Batch_idx: 210 |  Loss_1: (0.2410) | Acc_1: (90.69%) (24493/27008)\n",
      "Epoch: 191 | Batch_idx: 220 |  Loss_1: (0.2405) | Acc_1: (90.70%) (25658/28288)\n",
      "Epoch: 191 | Batch_idx: 230 |  Loss_1: (0.2406) | Acc_1: (90.70%) (26817/29568)\n",
      "Epoch: 191 | Batch_idx: 240 |  Loss_1: (0.2404) | Acc_1: (90.69%) (27975/30848)\n",
      "Epoch: 191 | Batch_idx: 250 |  Loss_1: (0.2402) | Acc_1: (90.68%) (29133/32128)\n",
      "Epoch: 191 | Batch_idx: 260 |  Loss_1: (0.2401) | Acc_1: (90.68%) (30296/33408)\n",
      "Epoch: 191 | Batch_idx: 270 |  Loss_1: (0.2397) | Acc_1: (90.70%) (31463/34688)\n",
      "Epoch: 191 | Batch_idx: 280 |  Loss_1: (0.2402) | Acc_1: (90.69%) (32621/35968)\n",
      "Epoch: 191 | Batch_idx: 290 |  Loss_1: (0.2391) | Acc_1: (90.75%) (33801/37248)\n",
      "Epoch: 191 | Batch_idx: 300 |  Loss_1: (0.2388) | Acc_1: (90.76%) (34968/38528)\n",
      "Epoch: 191 | Batch_idx: 310 |  Loss_1: (0.2384) | Acc_1: (90.79%) (36140/39808)\n",
      "Epoch: 191 | Batch_idx: 320 |  Loss_1: (0.2385) | Acc_1: (90.79%) (37302/41088)\n",
      "Epoch: 191 | Batch_idx: 330 |  Loss_1: (0.2378) | Acc_1: (90.81%) (38476/42368)\n",
      "Epoch: 191 | Batch_idx: 340 |  Loss_1: (0.2382) | Acc_1: (90.79%) (39629/43648)\n",
      "Epoch: 191 | Batch_idx: 350 |  Loss_1: (0.2385) | Acc_1: (90.79%) (40792/44928)\n",
      "Epoch: 191 | Batch_idx: 360 |  Loss_1: (0.2379) | Acc_1: (90.81%) (41960/46208)\n",
      "Epoch: 191 | Batch_idx: 370 |  Loss_1: (0.2380) | Acc_1: (90.80%) (43118/47488)\n",
      "Epoch: 191 | Batch_idx: 380 |  Loss_1: (0.2376) | Acc_1: (90.82%) (44290/48768)\n",
      "Epoch: 191 | Batch_idx: 390 |  Loss_1: (0.2381) | Acc_1: (90.80%) (45398/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3225) | Acc: (93.15%) (9315/10000)\n",
      "Epoch: 192 | Batch_idx: 0 |  Loss_1: (0.3272) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 192 | Batch_idx: 10 |  Loss_1: (0.2526) | Acc_1: (90.62%) (1276/1408)\n",
      "Epoch: 192 | Batch_idx: 20 |  Loss_1: (0.2500) | Acc_1: (90.44%) (2431/2688)\n",
      "Epoch: 192 | Batch_idx: 30 |  Loss_1: (0.2476) | Acc_1: (90.70%) (3599/3968)\n",
      "Epoch: 192 | Batch_idx: 40 |  Loss_1: (0.2471) | Acc_1: (90.57%) (4753/5248)\n",
      "Epoch: 192 | Batch_idx: 50 |  Loss_1: (0.2452) | Acc_1: (90.66%) (5918/6528)\n",
      "Epoch: 192 | Batch_idx: 60 |  Loss_1: (0.2447) | Acc_1: (90.66%) (7079/7808)\n",
      "Epoch: 192 | Batch_idx: 70 |  Loss_1: (0.2435) | Acc_1: (90.62%) (8236/9088)\n",
      "Epoch: 192 | Batch_idx: 80 |  Loss_1: (0.2455) | Acc_1: (90.52%) (9385/10368)\n",
      "Epoch: 192 | Batch_idx: 90 |  Loss_1: (0.2431) | Acc_1: (90.63%) (10557/11648)\n",
      "Epoch: 192 | Batch_idx: 100 |  Loss_1: (0.2414) | Acc_1: (90.70%) (11726/12928)\n",
      "Epoch: 192 | Batch_idx: 110 |  Loss_1: (0.2405) | Acc_1: (90.74%) (12893/14208)\n",
      "Epoch: 192 | Batch_idx: 120 |  Loss_1: (0.2421) | Acc_1: (90.70%) (14048/15488)\n",
      "Epoch: 192 | Batch_idx: 130 |  Loss_1: (0.2433) | Acc_1: (90.67%) (15204/16768)\n",
      "Epoch: 192 | Batch_idx: 140 |  Loss_1: (0.2425) | Acc_1: (90.71%) (16371/18048)\n",
      "Epoch: 192 | Batch_idx: 150 |  Loss_1: (0.2419) | Acc_1: (90.74%) (17539/19328)\n",
      "Epoch: 192 | Batch_idx: 160 |  Loss_1: (0.2410) | Acc_1: (90.77%) (18705/20608)\n",
      "Epoch: 192 | Batch_idx: 170 |  Loss_1: (0.2418) | Acc_1: (90.73%) (19858/21888)\n",
      "Epoch: 192 | Batch_idx: 180 |  Loss_1: (0.2409) | Acc_1: (90.75%) (21026/23168)\n",
      "Epoch: 192 | Batch_idx: 190 |  Loss_1: (0.2418) | Acc_1: (90.70%) (22175/24448)\n",
      "Epoch: 192 | Batch_idx: 200 |  Loss_1: (0.2418) | Acc_1: (90.71%) (23339/25728)\n",
      "Epoch: 192 | Batch_idx: 210 |  Loss_1: (0.2417) | Acc_1: (90.73%) (24504/27008)\n",
      "Epoch: 192 | Batch_idx: 220 |  Loss_1: (0.2411) | Acc_1: (90.73%) (25667/28288)\n",
      "Epoch: 192 | Batch_idx: 230 |  Loss_1: (0.2394) | Acc_1: (90.78%) (26841/29568)\n",
      "Epoch: 192 | Batch_idx: 240 |  Loss_1: (0.2379) | Acc_1: (90.84%) (28023/30848)\n",
      "Epoch: 192 | Batch_idx: 250 |  Loss_1: (0.2385) | Acc_1: (90.82%) (29179/32128)\n",
      "Epoch: 192 | Batch_idx: 260 |  Loss_1: (0.2382) | Acc_1: (90.84%) (30348/33408)\n",
      "Epoch: 192 | Batch_idx: 270 |  Loss_1: (0.2382) | Acc_1: (90.84%) (31512/34688)\n",
      "Epoch: 192 | Batch_idx: 280 |  Loss_1: (0.2394) | Acc_1: (90.77%) (32648/35968)\n",
      "Epoch: 192 | Batch_idx: 290 |  Loss_1: (0.2395) | Acc_1: (90.75%) (33804/37248)\n",
      "Epoch: 192 | Batch_idx: 300 |  Loss_1: (0.2398) | Acc_1: (90.75%) (34964/38528)\n",
      "Epoch: 192 | Batch_idx: 310 |  Loss_1: (0.2410) | Acc_1: (90.69%) (36103/39808)\n",
      "Epoch: 192 | Batch_idx: 320 |  Loss_1: (0.2411) | Acc_1: (90.68%) (37260/41088)\n",
      "Epoch: 192 | Batch_idx: 330 |  Loss_1: (0.2398) | Acc_1: (90.74%) (38445/42368)\n",
      "Epoch: 192 | Batch_idx: 340 |  Loss_1: (0.2394) | Acc_1: (90.74%) (39608/43648)\n",
      "Epoch: 192 | Batch_idx: 350 |  Loss_1: (0.2392) | Acc_1: (90.76%) (40778/44928)\n",
      "Epoch: 192 | Batch_idx: 360 |  Loss_1: (0.2384) | Acc_1: (90.79%) (41952/46208)\n",
      "Epoch: 192 | Batch_idx: 370 |  Loss_1: (0.2385) | Acc_1: (90.79%) (43114/47488)\n",
      "Epoch: 192 | Batch_idx: 380 |  Loss_1: (0.2373) | Acc_1: (90.83%) (44294/48768)\n",
      "Epoch: 192 | Batch_idx: 390 |  Loss_1: (0.2381) | Acc_1: (90.79%) (45395/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3402) | Acc: (93.51%) (9351/10000)\n",
      "Epoch: 193 | Batch_idx: 0 |  Loss_1: (0.1141) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 193 | Batch_idx: 10 |  Loss_1: (0.2255) | Acc_1: (91.62%) (1290/1408)\n",
      "Epoch: 193 | Batch_idx: 20 |  Loss_1: (0.2358) | Acc_1: (90.92%) (2444/2688)\n",
      "Epoch: 193 | Batch_idx: 30 |  Loss_1: (0.2417) | Acc_1: (90.88%) (3606/3968)\n",
      "Epoch: 193 | Batch_idx: 40 |  Loss_1: (0.2483) | Acc_1: (90.57%) (4753/5248)\n",
      "Epoch: 193 | Batch_idx: 50 |  Loss_1: (0.2472) | Acc_1: (90.58%) (5913/6528)\n",
      "Epoch: 193 | Batch_idx: 60 |  Loss_1: (0.2496) | Acc_1: (90.46%) (7063/7808)\n",
      "Epoch: 193 | Batch_idx: 70 |  Loss_1: (0.2529) | Acc_1: (90.40%) (8216/9088)\n",
      "Epoch: 193 | Batch_idx: 80 |  Loss_1: (0.2503) | Acc_1: (90.43%) (9376/10368)\n",
      "Epoch: 193 | Batch_idx: 90 |  Loss_1: (0.2505) | Acc_1: (90.40%) (10530/11648)\n",
      "Epoch: 193 | Batch_idx: 100 |  Loss_1: (0.2511) | Acc_1: (90.35%) (11680/12928)\n",
      "Epoch: 193 | Batch_idx: 110 |  Loss_1: (0.2488) | Acc_1: (90.43%) (12848/14208)\n",
      "Epoch: 193 | Batch_idx: 120 |  Loss_1: (0.2498) | Acc_1: (90.40%) (14001/15488)\n",
      "Epoch: 193 | Batch_idx: 130 |  Loss_1: (0.2506) | Acc_1: (90.37%) (15153/16768)\n",
      "Epoch: 193 | Batch_idx: 140 |  Loss_1: (0.2504) | Acc_1: (90.39%) (16314/18048)\n",
      "Epoch: 193 | Batch_idx: 150 |  Loss_1: (0.2507) | Acc_1: (90.37%) (17467/19328)\n",
      "Epoch: 193 | Batch_idx: 160 |  Loss_1: (0.2488) | Acc_1: (90.44%) (18638/20608)\n",
      "Epoch: 193 | Batch_idx: 170 |  Loss_1: (0.2475) | Acc_1: (90.50%) (19809/21888)\n",
      "Epoch: 193 | Batch_idx: 180 |  Loss_1: (0.2467) | Acc_1: (90.55%) (20978/23168)\n",
      "Epoch: 193 | Batch_idx: 190 |  Loss_1: (0.2460) | Acc_1: (90.56%) (22139/24448)\n",
      "Epoch: 193 | Batch_idx: 200 |  Loss_1: (0.2445) | Acc_1: (90.62%) (23314/25728)\n",
      "Epoch: 193 | Batch_idx: 210 |  Loss_1: (0.2428) | Acc_1: (90.70%) (24496/27008)\n",
      "Epoch: 193 | Batch_idx: 220 |  Loss_1: (0.2423) | Acc_1: (90.71%) (25661/28288)\n",
      "Epoch: 193 | Batch_idx: 230 |  Loss_1: (0.2425) | Acc_1: (90.70%) (26817/29568)\n",
      "Epoch: 193 | Batch_idx: 240 |  Loss_1: (0.2424) | Acc_1: (90.70%) (27978/30848)\n",
      "Epoch: 193 | Batch_idx: 250 |  Loss_1: (0.2417) | Acc_1: (90.71%) (29144/32128)\n",
      "Epoch: 193 | Batch_idx: 260 |  Loss_1: (0.2419) | Acc_1: (90.69%) (30298/33408)\n",
      "Epoch: 193 | Batch_idx: 270 |  Loss_1: (0.2411) | Acc_1: (90.71%) (31466/34688)\n",
      "Epoch: 193 | Batch_idx: 280 |  Loss_1: (0.2412) | Acc_1: (90.72%) (32629/35968)\n",
      "Epoch: 193 | Batch_idx: 290 |  Loss_1: (0.2413) | Acc_1: (90.70%) (33785/37248)\n",
      "Epoch: 193 | Batch_idx: 300 |  Loss_1: (0.2402) | Acc_1: (90.75%) (34963/38528)\n",
      "Epoch: 193 | Batch_idx: 310 |  Loss_1: (0.2410) | Acc_1: (90.71%) (36110/39808)\n",
      "Epoch: 193 | Batch_idx: 320 |  Loss_1: (0.2416) | Acc_1: (90.69%) (37263/41088)\n",
      "Epoch: 193 | Batch_idx: 330 |  Loss_1: (0.2420) | Acc_1: (90.68%) (38419/42368)\n",
      "Epoch: 193 | Batch_idx: 340 |  Loss_1: (0.2413) | Acc_1: (90.70%) (39589/43648)\n",
      "Epoch: 193 | Batch_idx: 350 |  Loss_1: (0.2407) | Acc_1: (90.71%) (40756/44928)\n",
      "Epoch: 193 | Batch_idx: 360 |  Loss_1: (0.2401) | Acc_1: (90.74%) (41928/46208)\n",
      "Epoch: 193 | Batch_idx: 370 |  Loss_1: (0.2400) | Acc_1: (90.74%) (43092/47488)\n",
      "Epoch: 193 | Batch_idx: 380 |  Loss_1: (0.2408) | Acc_1: (90.72%) (44240/48768)\n",
      "Epoch: 193 | Batch_idx: 390 |  Loss_1: (0.2404) | Acc_1: (90.74%) (45369/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3277) | Acc: (93.34%) (9334/10000)\n",
      "Epoch: 194 | Batch_idx: 0 |  Loss_1: (0.2247) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 194 | Batch_idx: 10 |  Loss_1: (0.2456) | Acc_1: (90.77%) (1278/1408)\n",
      "Epoch: 194 | Batch_idx: 20 |  Loss_1: (0.2217) | Acc_1: (91.59%) (2462/2688)\n",
      "Epoch: 194 | Batch_idx: 30 |  Loss_1: (0.2219) | Acc_1: (91.51%) (3631/3968)\n",
      "Epoch: 194 | Batch_idx: 40 |  Loss_1: (0.2223) | Acc_1: (91.44%) (4799/5248)\n",
      "Epoch: 194 | Batch_idx: 50 |  Loss_1: (0.2248) | Acc_1: (91.34%) (5963/6528)\n",
      "Epoch: 194 | Batch_idx: 60 |  Loss_1: (0.2269) | Acc_1: (91.34%) (7132/7808)\n",
      "Epoch: 194 | Batch_idx: 70 |  Loss_1: (0.2266) | Acc_1: (91.42%) (8308/9088)\n",
      "Epoch: 194 | Batch_idx: 80 |  Loss_1: (0.2299) | Acc_1: (91.26%) (9462/10368)\n",
      "Epoch: 194 | Batch_idx: 90 |  Loss_1: (0.2312) | Acc_1: (91.23%) (10627/11648)\n",
      "Epoch: 194 | Batch_idx: 100 |  Loss_1: (0.2357) | Acc_1: (91.07%) (11773/12928)\n",
      "Epoch: 194 | Batch_idx: 110 |  Loss_1: (0.2342) | Acc_1: (91.15%) (12950/14208)\n",
      "Epoch: 194 | Batch_idx: 120 |  Loss_1: (0.2351) | Acc_1: (91.10%) (14110/15488)\n",
      "Epoch: 194 | Batch_idx: 130 |  Loss_1: (0.2350) | Acc_1: (91.14%) (15282/16768)\n",
      "Epoch: 194 | Batch_idx: 140 |  Loss_1: (0.2351) | Acc_1: (91.15%) (16450/18048)\n",
      "Epoch: 194 | Batch_idx: 150 |  Loss_1: (0.2338) | Acc_1: (91.18%) (17624/19328)\n",
      "Epoch: 194 | Batch_idx: 160 |  Loss_1: (0.2327) | Acc_1: (91.23%) (18800/20608)\n",
      "Epoch: 194 | Batch_idx: 170 |  Loss_1: (0.2324) | Acc_1: (91.22%) (19967/21888)\n",
      "Epoch: 194 | Batch_idx: 180 |  Loss_1: (0.2341) | Acc_1: (91.13%) (21113/23168)\n",
      "Epoch: 194 | Batch_idx: 190 |  Loss_1: (0.2342) | Acc_1: (91.12%) (22276/24448)\n",
      "Epoch: 194 | Batch_idx: 200 |  Loss_1: (0.2324) | Acc_1: (91.20%) (23464/25728)\n",
      "Epoch: 194 | Batch_idx: 210 |  Loss_1: (0.2322) | Acc_1: (91.19%) (24628/27008)\n",
      "Epoch: 194 | Batch_idx: 220 |  Loss_1: (0.2333) | Acc_1: (91.14%) (25782/28288)\n",
      "Epoch: 194 | Batch_idx: 230 |  Loss_1: (0.2333) | Acc_1: (91.14%) (26948/29568)\n",
      "Epoch: 194 | Batch_idx: 240 |  Loss_1: (0.2331) | Acc_1: (91.13%) (28113/30848)\n",
      "Epoch: 194 | Batch_idx: 250 |  Loss_1: (0.2339) | Acc_1: (91.09%) (29266/32128)\n",
      "Epoch: 194 | Batch_idx: 260 |  Loss_1: (0.2345) | Acc_1: (91.06%) (30421/33408)\n",
      "Epoch: 194 | Batch_idx: 270 |  Loss_1: (0.2351) | Acc_1: (91.03%) (31577/34688)\n",
      "Epoch: 194 | Batch_idx: 280 |  Loss_1: (0.2354) | Acc_1: (91.01%) (32736/35968)\n",
      "Epoch: 194 | Batch_idx: 290 |  Loss_1: (0.2366) | Acc_1: (90.97%) (33884/37248)\n",
      "Epoch: 194 | Batch_idx: 300 |  Loss_1: (0.2376) | Acc_1: (90.93%) (35033/38528)\n",
      "Epoch: 194 | Batch_idx: 310 |  Loss_1: (0.2371) | Acc_1: (90.94%) (36203/39808)\n",
      "Epoch: 194 | Batch_idx: 320 |  Loss_1: (0.2364) | Acc_1: (90.96%) (37374/41088)\n",
      "Epoch: 194 | Batch_idx: 330 |  Loss_1: (0.2377) | Acc_1: (90.91%) (38515/42368)\n",
      "Epoch: 194 | Batch_idx: 340 |  Loss_1: (0.2382) | Acc_1: (90.88%) (39669/43648)\n",
      "Epoch: 194 | Batch_idx: 350 |  Loss_1: (0.2377) | Acc_1: (90.92%) (40847/44928)\n",
      "Epoch: 194 | Batch_idx: 360 |  Loss_1: (0.2382) | Acc_1: (90.89%) (41998/46208)\n",
      "Epoch: 194 | Batch_idx: 370 |  Loss_1: (0.2388) | Acc_1: (90.87%) (43151/47488)\n",
      "Epoch: 194 | Batch_idx: 380 |  Loss_1: (0.2384) | Acc_1: (90.88%) (44319/48768)\n",
      "Epoch: 194 | Batch_idx: 390 |  Loss_1: (0.2383) | Acc_1: (90.88%) (45439/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3521) | Acc: (93.25%) (9325/10000)\n",
      "Epoch: 195 | Batch_idx: 0 |  Loss_1: (0.2423) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 195 | Batch_idx: 10 |  Loss_1: (0.2087) | Acc_1: (92.26%) (1299/1408)\n",
      "Epoch: 195 | Batch_idx: 20 |  Loss_1: (0.2350) | Acc_1: (91.07%) (2448/2688)\n",
      "Epoch: 195 | Batch_idx: 30 |  Loss_1: (0.2406) | Acc_1: (90.78%) (3602/3968)\n",
      "Epoch: 195 | Batch_idx: 40 |  Loss_1: (0.2474) | Acc_1: (90.49%) (4749/5248)\n",
      "Epoch: 195 | Batch_idx: 50 |  Loss_1: (0.2530) | Acc_1: (90.21%) (5889/6528)\n",
      "Epoch: 195 | Batch_idx: 60 |  Loss_1: (0.2499) | Acc_1: (90.29%) (7050/7808)\n",
      "Epoch: 195 | Batch_idx: 70 |  Loss_1: (0.2495) | Acc_1: (90.31%) (8207/9088)\n",
      "Epoch: 195 | Batch_idx: 80 |  Loss_1: (0.2514) | Acc_1: (90.28%) (9360/10368)\n",
      "Epoch: 195 | Batch_idx: 90 |  Loss_1: (0.2458) | Acc_1: (90.53%) (10545/11648)\n",
      "Epoch: 195 | Batch_idx: 100 |  Loss_1: (0.2446) | Acc_1: (90.59%) (11711/12928)\n",
      "Epoch: 195 | Batch_idx: 110 |  Loss_1: (0.2473) | Acc_1: (90.46%) (12852/14208)\n",
      "Epoch: 195 | Batch_idx: 120 |  Loss_1: (0.2459) | Acc_1: (90.51%) (14018/15488)\n",
      "Epoch: 195 | Batch_idx: 130 |  Loss_1: (0.2459) | Acc_1: (90.51%) (15177/16768)\n",
      "Epoch: 195 | Batch_idx: 140 |  Loss_1: (0.2460) | Acc_1: (90.50%) (16333/18048)\n",
      "Epoch: 195 | Batch_idx: 150 |  Loss_1: (0.2449) | Acc_1: (90.54%) (17499/19328)\n",
      "Epoch: 195 | Batch_idx: 160 |  Loss_1: (0.2455) | Acc_1: (90.48%) (18646/20608)\n",
      "Epoch: 195 | Batch_idx: 170 |  Loss_1: (0.2456) | Acc_1: (90.50%) (19809/21888)\n",
      "Epoch: 195 | Batch_idx: 180 |  Loss_1: (0.2432) | Acc_1: (90.56%) (20982/23168)\n",
      "Epoch: 195 | Batch_idx: 190 |  Loss_1: (0.2440) | Acc_1: (90.54%) (22135/24448)\n",
      "Epoch: 195 | Batch_idx: 200 |  Loss_1: (0.2428) | Acc_1: (90.60%) (23309/25728)\n",
      "Epoch: 195 | Batch_idx: 210 |  Loss_1: (0.2427) | Acc_1: (90.61%) (24472/27008)\n",
      "Epoch: 195 | Batch_idx: 220 |  Loss_1: (0.2422) | Acc_1: (90.64%) (25639/28288)\n",
      "Epoch: 195 | Batch_idx: 230 |  Loss_1: (0.2428) | Acc_1: (90.62%) (26796/29568)\n",
      "Epoch: 195 | Batch_idx: 240 |  Loss_1: (0.2432) | Acc_1: (90.60%) (27947/30848)\n",
      "Epoch: 195 | Batch_idx: 250 |  Loss_1: (0.2440) | Acc_1: (90.57%) (29097/32128)\n",
      "Epoch: 195 | Batch_idx: 260 |  Loss_1: (0.2433) | Acc_1: (90.60%) (30269/33408)\n",
      "Epoch: 195 | Batch_idx: 270 |  Loss_1: (0.2432) | Acc_1: (90.62%) (31433/34688)\n",
      "Epoch: 195 | Batch_idx: 280 |  Loss_1: (0.2423) | Acc_1: (90.67%) (32612/35968)\n",
      "Epoch: 195 | Batch_idx: 290 |  Loss_1: (0.2425) | Acc_1: (90.68%) (33775/37248)\n",
      "Epoch: 195 | Batch_idx: 300 |  Loss_1: (0.2425) | Acc_1: (90.68%) (34936/38528)\n",
      "Epoch: 195 | Batch_idx: 310 |  Loss_1: (0.2428) | Acc_1: (90.67%) (36093/39808)\n",
      "Epoch: 195 | Batch_idx: 320 |  Loss_1: (0.2426) | Acc_1: (90.69%) (37262/41088)\n",
      "Epoch: 195 | Batch_idx: 330 |  Loss_1: (0.2422) | Acc_1: (90.70%) (38428/42368)\n",
      "Epoch: 195 | Batch_idx: 340 |  Loss_1: (0.2422) | Acc_1: (90.69%) (39583/43648)\n",
      "Epoch: 195 | Batch_idx: 350 |  Loss_1: (0.2423) | Acc_1: (90.69%) (40745/44928)\n",
      "Epoch: 195 | Batch_idx: 360 |  Loss_1: (0.2426) | Acc_1: (90.67%) (41898/46208)\n",
      "Epoch: 195 | Batch_idx: 370 |  Loss_1: (0.2426) | Acc_1: (90.67%) (43057/47488)\n",
      "Epoch: 195 | Batch_idx: 380 |  Loss_1: (0.2418) | Acc_1: (90.70%) (44231/48768)\n",
      "Epoch: 195 | Batch_idx: 390 |  Loss_1: (0.2407) | Acc_1: (90.74%) (45369/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3556) | Acc: (93.21%) (9321/10000)\n",
      "Epoch: 196 | Batch_idx: 0 |  Loss_1: (0.2768) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 196 | Batch_idx: 10 |  Loss_1: (0.2347) | Acc_1: (90.98%) (1281/1408)\n",
      "Epoch: 196 | Batch_idx: 20 |  Loss_1: (0.2418) | Acc_1: (90.85%) (2442/2688)\n",
      "Epoch: 196 | Batch_idx: 30 |  Loss_1: (0.2418) | Acc_1: (90.78%) (3602/3968)\n",
      "Epoch: 196 | Batch_idx: 40 |  Loss_1: (0.2384) | Acc_1: (90.87%) (4769/5248)\n",
      "Epoch: 196 | Batch_idx: 50 |  Loss_1: (0.2366) | Acc_1: (90.98%) (5939/6528)\n",
      "Epoch: 196 | Batch_idx: 60 |  Loss_1: (0.2340) | Acc_1: (91.06%) (7110/7808)\n",
      "Epoch: 196 | Batch_idx: 70 |  Loss_1: (0.2302) | Acc_1: (91.24%) (8292/9088)\n",
      "Epoch: 196 | Batch_idx: 80 |  Loss_1: (0.2327) | Acc_1: (91.13%) (9448/10368)\n",
      "Epoch: 196 | Batch_idx: 90 |  Loss_1: (0.2333) | Acc_1: (91.08%) (10609/11648)\n",
      "Epoch: 196 | Batch_idx: 100 |  Loss_1: (0.2346) | Acc_1: (91.01%) (11766/12928)\n",
      "Epoch: 196 | Batch_idx: 110 |  Loss_1: (0.2345) | Acc_1: (91.05%) (12936/14208)\n",
      "Epoch: 196 | Batch_idx: 120 |  Loss_1: (0.2336) | Acc_1: (91.10%) (14110/15488)\n",
      "Epoch: 196 | Batch_idx: 130 |  Loss_1: (0.2330) | Acc_1: (91.14%) (15282/16768)\n",
      "Epoch: 196 | Batch_idx: 140 |  Loss_1: (0.2322) | Acc_1: (91.18%) (16456/18048)\n",
      "Epoch: 196 | Batch_idx: 150 |  Loss_1: (0.2335) | Acc_1: (91.11%) (17609/19328)\n",
      "Epoch: 196 | Batch_idx: 160 |  Loss_1: (0.2335) | Acc_1: (91.11%) (18775/20608)\n",
      "Epoch: 196 | Batch_idx: 170 |  Loss_1: (0.2333) | Acc_1: (91.08%) (19935/21888)\n",
      "Epoch: 196 | Batch_idx: 180 |  Loss_1: (0.2333) | Acc_1: (91.07%) (21098/23168)\n",
      "Epoch: 196 | Batch_idx: 190 |  Loss_1: (0.2335) | Acc_1: (91.07%) (22264/24448)\n",
      "Epoch: 196 | Batch_idx: 200 |  Loss_1: (0.2338) | Acc_1: (91.04%) (23424/25728)\n",
      "Epoch: 196 | Batch_idx: 210 |  Loss_1: (0.2333) | Acc_1: (91.05%) (24592/27008)\n",
      "Epoch: 196 | Batch_idx: 220 |  Loss_1: (0.2317) | Acc_1: (91.10%) (25771/28288)\n",
      "Epoch: 196 | Batch_idx: 230 |  Loss_1: (0.2321) | Acc_1: (91.08%) (26932/29568)\n",
      "Epoch: 196 | Batch_idx: 240 |  Loss_1: (0.2317) | Acc_1: (91.09%) (28100/30848)\n",
      "Epoch: 196 | Batch_idx: 250 |  Loss_1: (0.2314) | Acc_1: (91.10%) (29268/32128)\n",
      "Epoch: 196 | Batch_idx: 260 |  Loss_1: (0.2311) | Acc_1: (91.11%) (30438/33408)\n",
      "Epoch: 196 | Batch_idx: 270 |  Loss_1: (0.2308) | Acc_1: (91.11%) (31603/34688)\n",
      "Epoch: 196 | Batch_idx: 280 |  Loss_1: (0.2315) | Acc_1: (91.09%) (32762/35968)\n",
      "Epoch: 196 | Batch_idx: 290 |  Loss_1: (0.2311) | Acc_1: (91.10%) (33932/37248)\n",
      "Epoch: 196 | Batch_idx: 300 |  Loss_1: (0.2308) | Acc_1: (91.12%) (35105/38528)\n",
      "Epoch: 196 | Batch_idx: 310 |  Loss_1: (0.2314) | Acc_1: (91.11%) (36268/39808)\n",
      "Epoch: 196 | Batch_idx: 320 |  Loss_1: (0.2326) | Acc_1: (91.05%) (37411/41088)\n",
      "Epoch: 196 | Batch_idx: 330 |  Loss_1: (0.2332) | Acc_1: (91.03%) (38567/42368)\n",
      "Epoch: 196 | Batch_idx: 340 |  Loss_1: (0.2338) | Acc_1: (91.00%) (39720/43648)\n",
      "Epoch: 196 | Batch_idx: 350 |  Loss_1: (0.2332) | Acc_1: (91.02%) (40893/44928)\n",
      "Epoch: 196 | Batch_idx: 360 |  Loss_1: (0.2339) | Acc_1: (90.99%) (42044/46208)\n",
      "Epoch: 196 | Batch_idx: 370 |  Loss_1: (0.2338) | Acc_1: (91.00%) (43213/47488)\n",
      "Epoch: 196 | Batch_idx: 380 |  Loss_1: (0.2322) | Acc_1: (91.06%) (44407/48768)\n",
      "Epoch: 196 | Batch_idx: 390 |  Loss_1: (0.2323) | Acc_1: (91.06%) (45532/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3063) | Acc: (93.37%) (9337/10000)\n",
      "Epoch: 197 | Batch_idx: 0 |  Loss_1: (0.2337) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 197 | Batch_idx: 10 |  Loss_1: (0.2542) | Acc_1: (90.62%) (1276/1408)\n",
      "Epoch: 197 | Batch_idx: 20 |  Loss_1: (0.2549) | Acc_1: (90.29%) (2427/2688)\n",
      "Epoch: 197 | Batch_idx: 30 |  Loss_1: (0.2447) | Acc_1: (90.50%) (3591/3968)\n",
      "Epoch: 197 | Batch_idx: 40 |  Loss_1: (0.2414) | Acc_1: (90.61%) (4755/5248)\n",
      "Epoch: 197 | Batch_idx: 50 |  Loss_1: (0.2448) | Acc_1: (90.55%) (5911/6528)\n",
      "Epoch: 197 | Batch_idx: 60 |  Loss_1: (0.2444) | Acc_1: (90.52%) (7068/7808)\n",
      "Epoch: 197 | Batch_idx: 70 |  Loss_1: (0.2457) | Acc_1: (90.45%) (8220/9088)\n",
      "Epoch: 197 | Batch_idx: 80 |  Loss_1: (0.2441) | Acc_1: (90.53%) (9386/10368)\n",
      "Epoch: 197 | Batch_idx: 90 |  Loss_1: (0.2448) | Acc_1: (90.55%) (10547/11648)\n",
      "Epoch: 197 | Batch_idx: 100 |  Loss_1: (0.2438) | Acc_1: (90.58%) (11710/12928)\n",
      "Epoch: 197 | Batch_idx: 110 |  Loss_1: (0.2462) | Acc_1: (90.47%) (12854/14208)\n",
      "Epoch: 197 | Batch_idx: 120 |  Loss_1: (0.2433) | Acc_1: (90.59%) (14031/15488)\n",
      "Epoch: 197 | Batch_idx: 130 |  Loss_1: (0.2421) | Acc_1: (90.67%) (15203/16768)\n",
      "Epoch: 197 | Batch_idx: 140 |  Loss_1: (0.2419) | Acc_1: (90.66%) (16363/18048)\n",
      "Epoch: 197 | Batch_idx: 150 |  Loss_1: (0.2402) | Acc_1: (90.73%) (17536/19328)\n",
      "Epoch: 197 | Batch_idx: 160 |  Loss_1: (0.2388) | Acc_1: (90.79%) (18710/20608)\n",
      "Epoch: 197 | Batch_idx: 170 |  Loss_1: (0.2408) | Acc_1: (90.71%) (19854/21888)\n",
      "Epoch: 197 | Batch_idx: 180 |  Loss_1: (0.2385) | Acc_1: (90.80%) (21036/23168)\n",
      "Epoch: 197 | Batch_idx: 190 |  Loss_1: (0.2383) | Acc_1: (90.80%) (22199/24448)\n",
      "Epoch: 197 | Batch_idx: 200 |  Loss_1: (0.2391) | Acc_1: (90.76%) (23351/25728)\n",
      "Epoch: 197 | Batch_idx: 210 |  Loss_1: (0.2385) | Acc_1: (90.80%) (24523/27008)\n",
      "Epoch: 197 | Batch_idx: 220 |  Loss_1: (0.2377) | Acc_1: (90.82%) (25692/28288)\n",
      "Epoch: 197 | Batch_idx: 230 |  Loss_1: (0.2375) | Acc_1: (90.83%) (26858/29568)\n",
      "Epoch: 197 | Batch_idx: 240 |  Loss_1: (0.2368) | Acc_1: (90.86%) (28030/30848)\n",
      "Epoch: 197 | Batch_idx: 250 |  Loss_1: (0.2379) | Acc_1: (90.84%) (29186/32128)\n",
      "Epoch: 197 | Batch_idx: 260 |  Loss_1: (0.2371) | Acc_1: (90.86%) (30355/33408)\n",
      "Epoch: 197 | Batch_idx: 270 |  Loss_1: (0.2369) | Acc_1: (90.87%) (31521/34688)\n",
      "Epoch: 197 | Batch_idx: 280 |  Loss_1: (0.2379) | Acc_1: (90.83%) (32671/35968)\n",
      "Epoch: 197 | Batch_idx: 290 |  Loss_1: (0.2381) | Acc_1: (90.81%) (33826/37248)\n",
      "Epoch: 197 | Batch_idx: 300 |  Loss_1: (0.2386) | Acc_1: (90.78%) (34975/38528)\n",
      "Epoch: 197 | Batch_idx: 310 |  Loss_1: (0.2380) | Acc_1: (90.80%) (36147/39808)\n",
      "Epoch: 197 | Batch_idx: 320 |  Loss_1: (0.2376) | Acc_1: (90.82%) (37317/41088)\n",
      "Epoch: 197 | Batch_idx: 330 |  Loss_1: (0.2375) | Acc_1: (90.81%) (38476/42368)\n",
      "Epoch: 197 | Batch_idx: 340 |  Loss_1: (0.2380) | Acc_1: (90.80%) (39634/43648)\n",
      "Epoch: 197 | Batch_idx: 350 |  Loss_1: (0.2381) | Acc_1: (90.81%) (40800/44928)\n",
      "Epoch: 197 | Batch_idx: 360 |  Loss_1: (0.2385) | Acc_1: (90.79%) (41950/46208)\n",
      "Epoch: 197 | Batch_idx: 370 |  Loss_1: (0.2379) | Acc_1: (90.81%) (43126/47488)\n",
      "Epoch: 197 | Batch_idx: 380 |  Loss_1: (0.2377) | Acc_1: (90.81%) (44286/48768)\n",
      "Epoch: 197 | Batch_idx: 390 |  Loss_1: (0.2379) | Acc_1: (90.80%) (45400/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3416) | Acc: (93.27%) (9327/10000)\n",
      "Epoch: 198 | Batch_idx: 0 |  Loss_1: (0.2223) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 198 | Batch_idx: 10 |  Loss_1: (0.2167) | Acc_1: (91.48%) (1288/1408)\n",
      "Epoch: 198 | Batch_idx: 20 |  Loss_1: (0.2127) | Acc_1: (91.74%) (2466/2688)\n",
      "Epoch: 198 | Batch_idx: 30 |  Loss_1: (0.2117) | Acc_1: (91.71%) (3639/3968)\n",
      "Epoch: 198 | Batch_idx: 40 |  Loss_1: (0.2119) | Acc_1: (91.75%) (4815/5248)\n",
      "Epoch: 198 | Batch_idx: 50 |  Loss_1: (0.2183) | Acc_1: (91.38%) (5965/6528)\n",
      "Epoch: 198 | Batch_idx: 60 |  Loss_1: (0.2208) | Acc_1: (91.36%) (7133/7808)\n",
      "Epoch: 198 | Batch_idx: 70 |  Loss_1: (0.2203) | Acc_1: (91.37%) (8304/9088)\n",
      "Epoch: 198 | Batch_idx: 80 |  Loss_1: (0.2215) | Acc_1: (91.31%) (9467/10368)\n",
      "Epoch: 198 | Batch_idx: 90 |  Loss_1: (0.2275) | Acc_1: (91.07%) (10608/11648)\n",
      "Epoch: 198 | Batch_idx: 100 |  Loss_1: (0.2290) | Acc_1: (91.05%) (11771/12928)\n",
      "Epoch: 198 | Batch_idx: 110 |  Loss_1: (0.2312) | Acc_1: (91.01%) (12930/14208)\n",
      "Epoch: 198 | Batch_idx: 120 |  Loss_1: (0.2286) | Acc_1: (91.14%) (14115/15488)\n",
      "Epoch: 198 | Batch_idx: 130 |  Loss_1: (0.2283) | Acc_1: (91.16%) (15286/16768)\n",
      "Epoch: 198 | Batch_idx: 140 |  Loss_1: (0.2310) | Acc_1: (91.06%) (16434/18048)\n",
      "Epoch: 198 | Batch_idx: 150 |  Loss_1: (0.2306) | Acc_1: (91.07%) (17602/19328)\n",
      "Epoch: 198 | Batch_idx: 160 |  Loss_1: (0.2304) | Acc_1: (91.09%) (18771/20608)\n",
      "Epoch: 198 | Batch_idx: 170 |  Loss_1: (0.2310) | Acc_1: (91.06%) (19932/21888)\n",
      "Epoch: 198 | Batch_idx: 180 |  Loss_1: (0.2310) | Acc_1: (91.04%) (21092/23168)\n",
      "Epoch: 198 | Batch_idx: 190 |  Loss_1: (0.2327) | Acc_1: (90.97%) (22241/24448)\n",
      "Epoch: 198 | Batch_idx: 200 |  Loss_1: (0.2325) | Acc_1: (90.99%) (23410/25728)\n",
      "Epoch: 198 | Batch_idx: 210 |  Loss_1: (0.2319) | Acc_1: (91.01%) (24580/27008)\n",
      "Epoch: 198 | Batch_idx: 220 |  Loss_1: (0.2332) | Acc_1: (90.99%) (25738/28288)\n",
      "Epoch: 198 | Batch_idx: 230 |  Loss_1: (0.2327) | Acc_1: (91.00%) (26906/29568)\n",
      "Epoch: 198 | Batch_idx: 240 |  Loss_1: (0.2336) | Acc_1: (90.95%) (28056/30848)\n",
      "Epoch: 198 | Batch_idx: 250 |  Loss_1: (0.2337) | Acc_1: (90.96%) (29223/32128)\n",
      "Epoch: 198 | Batch_idx: 260 |  Loss_1: (0.2333) | Acc_1: (90.97%) (30390/33408)\n",
      "Epoch: 198 | Batch_idx: 270 |  Loss_1: (0.2327) | Acc_1: (90.99%) (31561/34688)\n",
      "Epoch: 198 | Batch_idx: 280 |  Loss_1: (0.2324) | Acc_1: (91.01%) (32733/35968)\n",
      "Epoch: 198 | Batch_idx: 290 |  Loss_1: (0.2326) | Acc_1: (90.99%) (33893/37248)\n",
      "Epoch: 198 | Batch_idx: 300 |  Loss_1: (0.2337) | Acc_1: (90.97%) (35048/38528)\n",
      "Epoch: 198 | Batch_idx: 310 |  Loss_1: (0.2341) | Acc_1: (90.96%) (36209/39808)\n",
      "Epoch: 198 | Batch_idx: 320 |  Loss_1: (0.2353) | Acc_1: (90.91%) (37353/41088)\n",
      "Epoch: 198 | Batch_idx: 330 |  Loss_1: (0.2367) | Acc_1: (90.87%) (38501/42368)\n",
      "Epoch: 198 | Batch_idx: 340 |  Loss_1: (0.2368) | Acc_1: (90.87%) (39661/43648)\n",
      "Epoch: 198 | Batch_idx: 350 |  Loss_1: (0.2372) | Acc_1: (90.85%) (40816/44928)\n",
      "Epoch: 198 | Batch_idx: 360 |  Loss_1: (0.2378) | Acc_1: (90.81%) (41963/46208)\n",
      "Epoch: 198 | Batch_idx: 370 |  Loss_1: (0.2375) | Acc_1: (90.83%) (43132/47488)\n",
      "Epoch: 198 | Batch_idx: 380 |  Loss_1: (0.2376) | Acc_1: (90.83%) (44298/48768)\n",
      "Epoch: 198 | Batch_idx: 390 |  Loss_1: (0.2372) | Acc_1: (90.86%) (45431/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3482) | Acc: (93.36%) (9336/10000)\n",
      "Epoch: 199 | Batch_idx: 0 |  Loss_1: (0.2042) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 199 | Batch_idx: 10 |  Loss_1: (0.2360) | Acc_1: (90.70%) (1277/1408)\n",
      "Epoch: 199 | Batch_idx: 20 |  Loss_1: (0.2217) | Acc_1: (91.22%) (2452/2688)\n",
      "Epoch: 199 | Batch_idx: 30 |  Loss_1: (0.2187) | Acc_1: (91.38%) (3626/3968)\n",
      "Epoch: 199 | Batch_idx: 40 |  Loss_1: (0.2163) | Acc_1: (91.48%) (4801/5248)\n",
      "Epoch: 199 | Batch_idx: 50 |  Loss_1: (0.2170) | Acc_1: (91.48%) (5972/6528)\n",
      "Epoch: 199 | Batch_idx: 60 |  Loss_1: (0.2207) | Acc_1: (91.46%) (7141/7808)\n",
      "Epoch: 199 | Batch_idx: 70 |  Loss_1: (0.2207) | Acc_1: (91.44%) (8310/9088)\n",
      "Epoch: 199 | Batch_idx: 80 |  Loss_1: (0.2235) | Acc_1: (91.33%) (9469/10368)\n",
      "Epoch: 199 | Batch_idx: 90 |  Loss_1: (0.2220) | Acc_1: (91.41%) (10648/11648)\n",
      "Epoch: 199 | Batch_idx: 100 |  Loss_1: (0.2214) | Acc_1: (91.42%) (11819/12928)\n",
      "Epoch: 199 | Batch_idx: 110 |  Loss_1: (0.2240) | Acc_1: (91.29%) (12971/14208)\n",
      "Epoch: 199 | Batch_idx: 120 |  Loss_1: (0.2261) | Acc_1: (91.20%) (14125/15488)\n",
      "Epoch: 199 | Batch_idx: 130 |  Loss_1: (0.2278) | Acc_1: (91.16%) (15285/16768)\n",
      "Epoch: 199 | Batch_idx: 140 |  Loss_1: (0.2298) | Acc_1: (91.08%) (16439/18048)\n",
      "Epoch: 199 | Batch_idx: 150 |  Loss_1: (0.2324) | Acc_1: (91.00%) (17589/19328)\n",
      "Epoch: 199 | Batch_idx: 160 |  Loss_1: (0.2305) | Acc_1: (91.07%) (18767/20608)\n",
      "Epoch: 199 | Batch_idx: 170 |  Loss_1: (0.2307) | Acc_1: (91.04%) (19927/21888)\n",
      "Epoch: 199 | Batch_idx: 180 |  Loss_1: (0.2308) | Acc_1: (91.05%) (21094/23168)\n",
      "Epoch: 199 | Batch_idx: 190 |  Loss_1: (0.2319) | Acc_1: (91.01%) (22250/24448)\n",
      "Epoch: 199 | Batch_idx: 200 |  Loss_1: (0.2302) | Acc_1: (91.07%) (23430/25728)\n",
      "Epoch: 199 | Batch_idx: 210 |  Loss_1: (0.2323) | Acc_1: (91.01%) (24579/27008)\n",
      "Epoch: 199 | Batch_idx: 220 |  Loss_1: (0.2331) | Acc_1: (90.99%) (25739/28288)\n",
      "Epoch: 199 | Batch_idx: 230 |  Loss_1: (0.2341) | Acc_1: (90.96%) (26894/29568)\n",
      "Epoch: 199 | Batch_idx: 240 |  Loss_1: (0.2327) | Acc_1: (91.00%) (28073/30848)\n",
      "Epoch: 199 | Batch_idx: 250 |  Loss_1: (0.2332) | Acc_1: (91.00%) (29236/32128)\n",
      "Epoch: 199 | Batch_idx: 260 |  Loss_1: (0.2329) | Acc_1: (91.00%) (30401/33408)\n",
      "Epoch: 199 | Batch_idx: 270 |  Loss_1: (0.2325) | Acc_1: (91.02%) (31573/34688)\n",
      "Epoch: 199 | Batch_idx: 280 |  Loss_1: (0.2336) | Acc_1: (90.98%) (32724/35968)\n",
      "Epoch: 199 | Batch_idx: 290 |  Loss_1: (0.2335) | Acc_1: (90.98%) (33889/37248)\n",
      "Epoch: 199 | Batch_idx: 300 |  Loss_1: (0.2335) | Acc_1: (90.97%) (35050/38528)\n",
      "Epoch: 199 | Batch_idx: 310 |  Loss_1: (0.2334) | Acc_1: (90.97%) (36215/39808)\n",
      "Epoch: 199 | Batch_idx: 320 |  Loss_1: (0.2331) | Acc_1: (90.99%) (37388/41088)\n",
      "Epoch: 199 | Batch_idx: 330 |  Loss_1: (0.2339) | Acc_1: (90.97%) (38541/42368)\n",
      "Epoch: 199 | Batch_idx: 340 |  Loss_1: (0.2345) | Acc_1: (90.95%) (39697/43648)\n",
      "Epoch: 199 | Batch_idx: 350 |  Loss_1: (0.2346) | Acc_1: (90.94%) (40859/44928)\n",
      "Epoch: 199 | Batch_idx: 360 |  Loss_1: (0.2351) | Acc_1: (90.94%) (42021/46208)\n",
      "Epoch: 199 | Batch_idx: 370 |  Loss_1: (0.2356) | Acc_1: (90.91%) (43173/47488)\n",
      "Epoch: 199 | Batch_idx: 380 |  Loss_1: (0.2356) | Acc_1: (90.92%) (44341/48768)\n",
      "Epoch: 199 | Batch_idx: 390 |  Loss_1: (0.2350) | Acc_1: (90.95%) (45473/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3328) | Acc: (93.29%) (9329/10000)\n",
      "6 hours 52 mins 32 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet_101.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 200):\n",
    "\n",
    "    \n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet_101.tar.gz')\n",
    "    \n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8766a17f2584f17ae9875767170f3464b2a051bfe2b6423fb227ac503acbc200"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hw2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
