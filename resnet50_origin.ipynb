{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.003\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet_50_origin') #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformer_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    #transforms.ColorJitter(brightness=(0.2, 2), contrast=(0.3, 2), saturation=(0.2, 2), hue=(-0.3, 0.3)),\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_2 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_3 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "validation_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_2 = torch.utils.data.DataLoader(dataset=training_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_3 = torch.utils.data.DataLoader(dataset=training_dataset_3, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "       \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #self.dropblock.step()\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        #out = self.dropblock(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        #out = self.dropblock(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BottleNeck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 1 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
      "             ReLU-13          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-14          [-1, 256, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "           Conv2d-18           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
      "             ReLU-20           [-1, 64, 32, 32]               0\n",
      "           Conv2d-21          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-22          [-1, 256, 32, 32]             512\n",
      "             ReLU-23          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
      "             ReLU-27           [-1, 64, 32, 32]               0\n",
      "           Conv2d-28           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 32, 32]             128\n",
      "             ReLU-30           [-1, 64, 32, 32]               0\n",
      "           Conv2d-31          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
      "             ReLU-33          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "             ReLU-40          [-1, 128, 16, 16]               0\n",
      "           Conv2d-41          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-42          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-43          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-45          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-46          [-1, 512, 16, 16]               0\n",
      "           Conv2d-47          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-48          [-1, 128, 16, 16]             256\n",
      "             ReLU-49          [-1, 128, 16, 16]               0\n",
      "           Conv2d-50          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-51          [-1, 128, 16, 16]             256\n",
      "             ReLU-52          [-1, 128, 16, 16]               0\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-55          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-56          [-1, 512, 16, 16]               0\n",
      "           Conv2d-57          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-58          [-1, 128, 16, 16]             256\n",
      "             ReLU-59          [-1, 128, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-66          [-1, 512, 16, 16]               0\n",
      "           Conv2d-67          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-68          [-1, 128, 16, 16]             256\n",
      "             ReLU-69          [-1, 128, 16, 16]               0\n",
      "           Conv2d-70          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-71          [-1, 128, 16, 16]             256\n",
      "             ReLU-72          [-1, 128, 16, 16]               0\n",
      "           Conv2d-73          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-74          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-75          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-76          [-1, 512, 16, 16]               0\n",
      "           Conv2d-77          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-78          [-1, 256, 16, 16]             512\n",
      "             ReLU-79          [-1, 256, 16, 16]               0\n",
      "           Conv2d-80            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-81            [-1, 256, 8, 8]             512\n",
      "             ReLU-82            [-1, 256, 8, 8]               0\n",
      "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-85           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-87           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-88           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-89            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-90            [-1, 256, 8, 8]             512\n",
      "             ReLU-91            [-1, 256, 8, 8]               0\n",
      "           Conv2d-92            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-93            [-1, 256, 8, 8]             512\n",
      "             ReLU-94            [-1, 256, 8, 8]               0\n",
      "           Conv2d-95           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-96           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-97           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-98           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-99            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-100            [-1, 256, 8, 8]             512\n",
      "            ReLU-101            [-1, 256, 8, 8]               0\n",
      "          Conv2d-102            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-103            [-1, 256, 8, 8]             512\n",
      "            ReLU-104            [-1, 256, 8, 8]               0\n",
      "          Conv2d-105           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-106           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-107           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-108           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-109            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-110            [-1, 256, 8, 8]             512\n",
      "            ReLU-111            [-1, 256, 8, 8]               0\n",
      "          Conv2d-112            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-113            [-1, 256, 8, 8]             512\n",
      "            ReLU-114            [-1, 256, 8, 8]               0\n",
      "          Conv2d-115           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-116           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-117           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-118           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-119            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-120            [-1, 256, 8, 8]             512\n",
      "            ReLU-121            [-1, 256, 8, 8]               0\n",
      "          Conv2d-122            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "            ReLU-124            [-1, 256, 8, 8]               0\n",
      "          Conv2d-125           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-126           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-127           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-128           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-129            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-130            [-1, 256, 8, 8]             512\n",
      "            ReLU-131            [-1, 256, 8, 8]               0\n",
      "          Conv2d-132            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-133            [-1, 256, 8, 8]             512\n",
      "            ReLU-134            [-1, 256, 8, 8]               0\n",
      "          Conv2d-135           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-136           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-137           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-138           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-139            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-140            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-141            [-1, 512, 8, 8]               0\n",
      "          Conv2d-142            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-143            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-144            [-1, 512, 4, 4]               0\n",
      "          Conv2d-145           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-146           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-147           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-148           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-149           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-150           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-151            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-152            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-153            [-1, 512, 4, 4]               0\n",
      "          Conv2d-154            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-155            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-156            [-1, 512, 4, 4]               0\n",
      "          Conv2d-157           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-158           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-159           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-160           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-161            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-162            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-163            [-1, 512, 4, 4]               0\n",
      "          Conv2d-164            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-165            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-166            [-1, 512, 4, 4]               0\n",
      "          Conv2d-167           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-168           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-169           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-170           [-1, 2048, 4, 4]               0\n",
      "          Linear-171                   [-1, 10]          20,490\n",
      "          ResNet-172                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 23,520,842\n",
      "Trainable params: 23,520,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 88.06\n",
      "Params size (MB): 89.72\n",
      "Estimated Total Size (MB): 177.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(training_loader)\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(training_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(validation_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(validation_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (2.5548) | Acc_1: (12.50%) (16/128)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (2.3642) | Acc_1: (13.42%) (189/1408)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (2.3528) | Acc_1: (14.47%) (389/2688)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (2.2994) | Acc_1: (15.68%) (622/3968)\n",
      "Epoch: 0 | Batch_idx: 40 |  Loss_1: (2.2438) | Acc_1: (17.82%) (935/5248)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss_1: (2.2002) | Acc_1: (19.19%) (1253/6528)\n",
      "Epoch: 0 | Batch_idx: 60 |  Loss_1: (2.1539) | Acc_1: (20.58%) (1607/7808)\n",
      "Epoch: 0 | Batch_idx: 70 |  Loss_1: (2.1209) | Acc_1: (21.64%) (1967/9088)\n",
      "Epoch: 0 | Batch_idx: 80 |  Loss_1: (2.0906) | Acc_1: (22.60%) (2343/10368)\n",
      "Epoch: 0 | Batch_idx: 90 |  Loss_1: (2.0571) | Acc_1: (23.74%) (2765/11648)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss_1: (2.0332) | Acc_1: (24.57%) (3176/12928)\n",
      "Epoch: 0 | Batch_idx: 110 |  Loss_1: (2.0077) | Acc_1: (25.53%) (3628/14208)\n",
      "Epoch: 0 | Batch_idx: 120 |  Loss_1: (1.9867) | Acc_1: (26.32%) (4077/15488)\n",
      "Epoch: 0 | Batch_idx: 130 |  Loss_1: (1.9672) | Acc_1: (27.14%) (4551/16768)\n",
      "Epoch: 0 | Batch_idx: 140 |  Loss_1: (1.9479) | Acc_1: (27.81%) (5020/18048)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss_1: (1.9307) | Acc_1: (28.58%) (5524/19328)\n",
      "Epoch: 0 | Batch_idx: 160 |  Loss_1: (1.9112) | Acc_1: (29.23%) (6023/20608)\n",
      "Epoch: 0 | Batch_idx: 170 |  Loss_1: (1.8949) | Acc_1: (29.86%) (6535/21888)\n",
      "Epoch: 0 | Batch_idx: 180 |  Loss_1: (1.8768) | Acc_1: (30.49%) (7063/23168)\n",
      "Epoch: 0 | Batch_idx: 190 |  Loss_1: (1.8651) | Acc_1: (30.91%) (7558/24448)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss_1: (1.8500) | Acc_1: (31.47%) (8096/25728)\n",
      "Epoch: 0 | Batch_idx: 210 |  Loss_1: (1.8372) | Acc_1: (31.98%) (8638/27008)\n",
      "Epoch: 0 | Batch_idx: 220 |  Loss_1: (1.8252) | Acc_1: (32.44%) (9177/28288)\n",
      "Epoch: 0 | Batch_idx: 230 |  Loss_1: (1.8168) | Acc_1: (32.77%) (9690/29568)\n",
      "Epoch: 0 | Batch_idx: 240 |  Loss_1: (1.8044) | Acc_1: (33.26%) (10259/30848)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss_1: (1.7936) | Acc_1: (33.61%) (10799/32128)\n",
      "Epoch: 0 | Batch_idx: 260 |  Loss_1: (1.7817) | Acc_1: (34.05%) (11374/33408)\n",
      "Epoch: 0 | Batch_idx: 270 |  Loss_1: (1.7719) | Acc_1: (34.37%) (11921/34688)\n",
      "Epoch: 0 | Batch_idx: 280 |  Loss_1: (1.7617) | Acc_1: (34.74%) (12494/35968)\n",
      "Epoch: 0 | Batch_idx: 290 |  Loss_1: (1.7532) | Acc_1: (35.13%) (13086/37248)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss_1: (1.7445) | Acc_1: (35.48%) (13669/38528)\n",
      "Epoch: 0 | Batch_idx: 310 |  Loss_1: (1.7341) | Acc_1: (35.89%) (14288/39808)\n",
      "Epoch: 0 | Batch_idx: 320 |  Loss_1: (1.7240) | Acc_1: (36.31%) (14918/41088)\n",
      "Epoch: 0 | Batch_idx: 330 |  Loss_1: (1.7150) | Acc_1: (36.67%) (15536/42368)\n",
      "Epoch: 0 | Batch_idx: 340 |  Loss_1: (1.7087) | Acc_1: (36.94%) (16122/43648)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss_1: (1.7006) | Acc_1: (37.24%) (16731/44928)\n",
      "Epoch: 0 | Batch_idx: 360 |  Loss_1: (1.6945) | Acc_1: (37.49%) (17324/46208)\n",
      "Epoch: 0 | Batch_idx: 370 |  Loss_1: (1.6871) | Acc_1: (37.79%) (17948/47488)\n",
      "Epoch: 0 | Batch_idx: 380 |  Loss_1: (1.6807) | Acc_1: (38.07%) (18564/48768)\n",
      "Epoch: 0 | Batch_idx: 390 |  Loss_1: (1.6736) | Acc_1: (38.35%) (19176/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4306) | Acc: (48.56%) (4856/10000)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.3809) | Acc_1: (48.44%) (62/128)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.3755) | Acc_1: (49.50%) (697/1408)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.3867) | Acc_1: (49.59%) (1333/2688)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.3823) | Acc_1: (49.50%) (1964/3968)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss_1: (1.3576) | Acc_1: (50.30%) (2640/5248)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss_1: (1.3497) | Acc_1: (50.80%) (3316/6528)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss_1: (1.3431) | Acc_1: (50.96%) (3979/7808)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss_1: (1.3373) | Acc_1: (51.20%) (4653/9088)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss_1: (1.3326) | Acc_1: (51.20%) (5308/10368)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss_1: (1.3321) | Acc_1: (51.10%) (5952/11648)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss_1: (1.3305) | Acc_1: (51.28%) (6630/12928)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss_1: (1.3273) | Acc_1: (51.42%) (7306/14208)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss_1: (1.3230) | Acc_1: (51.67%) (8003/15488)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss_1: (1.3159) | Acc_1: (51.93%) (8707/16768)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss_1: (1.3145) | Acc_1: (51.97%) (9380/18048)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss_1: (1.3077) | Acc_1: (52.29%) (10107/19328)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss_1: (1.3037) | Acc_1: (52.38%) (10795/20608)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss_1: (1.2961) | Acc_1: (52.74%) (11543/21888)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss_1: (1.2900) | Acc_1: (53.00%) (12278/23168)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss_1: (1.2901) | Acc_1: (53.01%) (12961/24448)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss_1: (1.2887) | Acc_1: (53.07%) (13655/25728)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss_1: (1.2864) | Acc_1: (53.20%) (14367/27008)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss_1: (1.2792) | Acc_1: (53.45%) (15119/28288)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss_1: (1.2738) | Acc_1: (53.70%) (15877/29568)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss_1: (1.2709) | Acc_1: (53.87%) (16618/30848)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss_1: (1.2678) | Acc_1: (54.02%) (17356/32128)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss_1: (1.2658) | Acc_1: (54.13%) (18085/33408)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss_1: (1.2627) | Acc_1: (54.27%) (18825/34688)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss_1: (1.2587) | Acc_1: (54.37%) (19555/35968)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss_1: (1.2545) | Acc_1: (54.52%) (20306/37248)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss_1: (1.2505) | Acc_1: (54.69%) (21071/38528)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss_1: (1.2471) | Acc_1: (54.81%) (21820/39808)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss_1: (1.2450) | Acc_1: (54.94%) (22574/41088)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss_1: (1.2409) | Acc_1: (55.09%) (23342/42368)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss_1: (1.2387) | Acc_1: (55.13%) (24062/43648)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss_1: (1.2338) | Acc_1: (55.32%) (24853/44928)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss_1: (1.2310) | Acc_1: (55.45%) (25623/46208)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss_1: (1.2282) | Acc_1: (55.57%) (26387/47488)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss_1: (1.2246) | Acc_1: (55.72%) (27174/48768)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss_1: (1.2212) | Acc_1: (55.84%) (27922/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0619) | Acc: (62.30%) (6230/10000)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (0.9292) | Acc_1: (67.97%) (87/128)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.0612) | Acc_1: (63.85%) (899/1408)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.0409) | Acc_1: (63.84%) (1716/2688)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.0653) | Acc_1: (62.68%) (2487/3968)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss_1: (1.0557) | Acc_1: (62.80%) (3296/5248)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss_1: (1.0587) | Acc_1: (62.70%) (4093/6528)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss_1: (1.0567) | Acc_1: (62.63%) (4890/7808)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss_1: (1.0553) | Acc_1: (62.61%) (5690/9088)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss_1: (1.0471) | Acc_1: (62.92%) (6524/10368)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss_1: (1.0432) | Acc_1: (63.03%) (7342/11648)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss_1: (1.0432) | Acc_1: (63.00%) (8144/12928)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss_1: (1.0406) | Acc_1: (62.99%) (8949/14208)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss_1: (1.0378) | Acc_1: (63.15%) (9781/15488)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss_1: (1.0306) | Acc_1: (63.39%) (10630/16768)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss_1: (1.0276) | Acc_1: (63.50%) (11461/18048)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss_1: (1.0256) | Acc_1: (63.57%) (12287/19328)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss_1: (1.0219) | Acc_1: (63.66%) (13120/20608)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss_1: (1.0184) | Acc_1: (63.78%) (13961/21888)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss_1: (1.0181) | Acc_1: (63.80%) (14782/23168)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss_1: (1.0147) | Acc_1: (63.92%) (15627/24448)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss_1: (1.0120) | Acc_1: (64.01%) (16468/25728)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss_1: (1.0103) | Acc_1: (64.10%) (17312/27008)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss_1: (1.0084) | Acc_1: (64.13%) (18141/28288)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss_1: (1.0056) | Acc_1: (64.19%) (18980/29568)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss_1: (1.0062) | Acc_1: (64.20%) (19803/30848)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss_1: (1.0029) | Acc_1: (64.26%) (20646/32128)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss_1: (1.0006) | Acc_1: (64.35%) (21498/33408)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss_1: (1.0001) | Acc_1: (64.42%) (22346/34688)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss_1: (1.0009) | Acc_1: (64.39%) (23159/35968)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss_1: (1.0001) | Acc_1: (64.43%) (23999/37248)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss_1: (0.9976) | Acc_1: (64.49%) (24845/38528)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss_1: (0.9951) | Acc_1: (64.54%) (25694/39808)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss_1: (0.9922) | Acc_1: (64.65%) (26563/41088)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss_1: (0.9897) | Acc_1: (64.73%) (27426/42368)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss_1: (0.9869) | Acc_1: (64.85%) (28307/43648)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss_1: (0.9836) | Acc_1: (64.93%) (29173/44928)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss_1: (0.9806) | Acc_1: (65.05%) (30058/46208)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss_1: (0.9786) | Acc_1: (65.13%) (30927/47488)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss_1: (0.9763) | Acc_1: (65.20%) (31795/48768)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss_1: (0.9745) | Acc_1: (65.28%) (32642/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0364) | Acc: (64.31%) (6431/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (0.8483) | Acc_1: (69.53%) (89/128)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (0.8831) | Acc_1: (67.47%) (950/1408)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (0.8710) | Acc_1: (68.27%) (1835/2688)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (0.8547) | Acc_1: (69.35%) (2752/3968)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss_1: (0.8481) | Acc_1: (69.86%) (3666/5248)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss_1: (0.8611) | Acc_1: (69.30%) (4524/6528)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss_1: (0.8592) | Acc_1: (69.42%) (5420/7808)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss_1: (0.8554) | Acc_1: (69.61%) (6326/9088)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss_1: (0.8515) | Acc_1: (69.72%) (7229/10368)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss_1: (0.8423) | Acc_1: (70.05%) (8160/11648)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss_1: (0.8459) | Acc_1: (69.93%) (9041/12928)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss_1: (0.8453) | Acc_1: (69.94%) (9937/14208)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss_1: (0.8442) | Acc_1: (69.94%) (10833/15488)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss_1: (0.8432) | Acc_1: (70.04%) (11744/16768)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss_1: (0.8418) | Acc_1: (70.06%) (12645/18048)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss_1: (0.8386) | Acc_1: (70.19%) (13567/19328)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss_1: (0.8367) | Acc_1: (70.25%) (14478/20608)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss_1: (0.8360) | Acc_1: (70.29%) (15384/21888)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss_1: (0.8314) | Acc_1: (70.44%) (16320/23168)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss_1: (0.8284) | Acc_1: (70.53%) (17244/24448)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss_1: (0.8257) | Acc_1: (70.67%) (18181/25728)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss_1: (0.8242) | Acc_1: (70.67%) (19087/27008)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss_1: (0.8254) | Acc_1: (70.72%) (20005/28288)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss_1: (0.8248) | Acc_1: (70.77%) (20926/29568)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss_1: (0.8226) | Acc_1: (70.85%) (21855/30848)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss_1: (0.8214) | Acc_1: (70.90%) (22778/32128)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss_1: (0.8220) | Acc_1: (70.90%) (23686/33408)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss_1: (0.8190) | Acc_1: (71.02%) (24637/34688)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss_1: (0.8157) | Acc_1: (71.12%) (25580/35968)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss_1: (0.8151) | Acc_1: (71.10%) (26485/37248)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss_1: (0.8132) | Acc_1: (71.19%) (27430/38528)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss_1: (0.8133) | Acc_1: (71.15%) (28324/39808)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss_1: (0.8134) | Acc_1: (71.15%) (29235/41088)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss_1: (0.8132) | Acc_1: (71.19%) (30162/42368)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss_1: (0.8123) | Acc_1: (71.22%) (31087/43648)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss_1: (0.8114) | Acc_1: (71.27%) (32020/44928)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss_1: (0.8097) | Acc_1: (71.31%) (32952/46208)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss_1: (0.8080) | Acc_1: (71.38%) (33898/47488)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss_1: (0.8069) | Acc_1: (71.45%) (34847/48768)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss_1: (0.8049) | Acc_1: (71.54%) (35768/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7915) | Acc: (72.53%) (7253/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (0.7547) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (0.6967) | Acc_1: (74.64%) (1051/1408)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (0.6961) | Acc_1: (75.30%) (2024/2688)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (0.6868) | Acc_1: (75.86%) (3010/3968)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss_1: (0.6861) | Acc_1: (75.88%) (3982/5248)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss_1: (0.6808) | Acc_1: (75.87%) (4953/6528)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss_1: (0.6823) | Acc_1: (75.79%) (5918/7808)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss_1: (0.6880) | Acc_1: (75.65%) (6875/9088)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss_1: (0.6868) | Acc_1: (75.82%) (7861/10368)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss_1: (0.6877) | Acc_1: (75.82%) (8832/11648)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss_1: (0.6898) | Acc_1: (75.76%) (9794/12928)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss_1: (0.6905) | Acc_1: (75.72%) (10758/14208)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss_1: (0.6927) | Acc_1: (75.70%) (11724/15488)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss_1: (0.6976) | Acc_1: (75.48%) (12657/16768)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss_1: (0.6987) | Acc_1: (75.48%) (13622/18048)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss_1: (0.6977) | Acc_1: (75.49%) (14590/19328)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss_1: (0.6974) | Acc_1: (75.63%) (15586/20608)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss_1: (0.6967) | Acc_1: (75.59%) (16545/21888)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss_1: (0.6949) | Acc_1: (75.67%) (17531/23168)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss_1: (0.6923) | Acc_1: (75.79%) (18529/24448)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss_1: (0.6899) | Acc_1: (75.90%) (19527/25728)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss_1: (0.6907) | Acc_1: (75.85%) (20486/27008)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss_1: (0.6893) | Acc_1: (75.87%) (21462/28288)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss_1: (0.6873) | Acc_1: (76.01%) (22475/29568)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss_1: (0.6848) | Acc_1: (76.10%) (23474/30848)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss_1: (0.6827) | Acc_1: (76.21%) (24484/32128)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss_1: (0.6829) | Acc_1: (76.22%) (25464/33408)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss_1: (0.6817) | Acc_1: (76.25%) (26451/34688)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss_1: (0.6779) | Acc_1: (76.35%) (27460/35968)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss_1: (0.6787) | Acc_1: (76.35%) (28437/37248)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss_1: (0.6765) | Acc_1: (76.39%) (29431/38528)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss_1: (0.6746) | Acc_1: (76.47%) (30441/39808)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss_1: (0.6755) | Acc_1: (76.44%) (31408/41088)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss_1: (0.6744) | Acc_1: (76.45%) (32389/42368)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss_1: (0.6743) | Acc_1: (76.43%) (33361/43648)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss_1: (0.6726) | Acc_1: (76.50%) (34371/44928)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss_1: (0.6720) | Acc_1: (76.53%) (35365/46208)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss_1: (0.6719) | Acc_1: (76.54%) (36348/47488)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss_1: (0.6710) | Acc_1: (76.58%) (37346/48768)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss_1: (0.6701) | Acc_1: (76.59%) (38296/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8265) | Acc: (72.82%) (7282/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (0.5267) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (0.6031) | Acc_1: (79.19%) (1115/1408)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (0.6089) | Acc_1: (78.83%) (2119/2688)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (0.6038) | Acc_1: (79.13%) (3140/3968)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss_1: (0.6152) | Acc_1: (78.93%) (4142/5248)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss_1: (0.6024) | Acc_1: (79.27%) (5175/6528)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss_1: (0.6005) | Acc_1: (79.23%) (6186/7808)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss_1: (0.5965) | Acc_1: (79.30%) (7207/9088)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss_1: (0.5943) | Acc_1: (79.28%) (8220/10368)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss_1: (0.5959) | Acc_1: (79.33%) (9240/11648)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss_1: (0.5965) | Acc_1: (79.26%) (10247/12928)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss_1: (0.6006) | Acc_1: (79.14%) (11244/14208)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss_1: (0.6030) | Acc_1: (79.05%) (12243/15488)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss_1: (0.6022) | Acc_1: (78.99%) (13245/16768)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss_1: (0.6063) | Acc_1: (78.92%) (14243/18048)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss_1: (0.6000) | Acc_1: (79.19%) (15305/19328)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss_1: (0.5991) | Acc_1: (79.17%) (16316/20608)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss_1: (0.5951) | Acc_1: (79.30%) (17358/21888)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss_1: (0.5914) | Acc_1: (79.45%) (18406/23168)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss_1: (0.5913) | Acc_1: (79.43%) (19419/24448)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss_1: (0.5893) | Acc_1: (79.53%) (20461/25728)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss_1: (0.5899) | Acc_1: (79.48%) (21467/27008)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss_1: (0.5919) | Acc_1: (79.40%) (22461/28288)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss_1: (0.5923) | Acc_1: (79.44%) (23488/29568)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss_1: (0.5893) | Acc_1: (79.54%) (24536/30848)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss_1: (0.5868) | Acc_1: (79.62%) (25579/32128)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss_1: (0.5872) | Acc_1: (79.64%) (26606/33408)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss_1: (0.5878) | Acc_1: (79.64%) (27624/34688)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss_1: (0.5870) | Acc_1: (79.63%) (28643/35968)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss_1: (0.5862) | Acc_1: (79.65%) (29667/37248)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss_1: (0.5856) | Acc_1: (79.67%) (30696/38528)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss_1: (0.5850) | Acc_1: (79.67%) (31717/39808)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss_1: (0.5843) | Acc_1: (79.71%) (32750/41088)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss_1: (0.5832) | Acc_1: (79.70%) (33766/42368)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss_1: (0.5823) | Acc_1: (79.73%) (34801/43648)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss_1: (0.5828) | Acc_1: (79.72%) (35817/44928)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss_1: (0.5811) | Acc_1: (79.79%) (36871/46208)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss_1: (0.5811) | Acc_1: (79.79%) (37890/47488)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss_1: (0.5811) | Acc_1: (79.78%) (38907/48768)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss_1: (0.5816) | Acc_1: (79.77%) (39885/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6696) | Acc: (77.43%) (7743/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (0.4348) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (0.5026) | Acc_1: (82.46%) (1161/1408)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (0.5190) | Acc_1: (82.07%) (2206/2688)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (0.5078) | Acc_1: (82.26%) (3264/3968)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss_1: (0.5092) | Acc_1: (82.49%) (4329/5248)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss_1: (0.5195) | Acc_1: (82.12%) (5361/6528)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss_1: (0.5236) | Acc_1: (81.95%) (6399/7808)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss_1: (0.5198) | Acc_1: (82.01%) (7453/9088)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss_1: (0.5259) | Acc_1: (81.94%) (8496/10368)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss_1: (0.5231) | Acc_1: (82.00%) (9551/11648)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss_1: (0.5180) | Acc_1: (82.11%) (10615/12928)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss_1: (0.5169) | Acc_1: (82.23%) (11683/14208)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss_1: (0.5218) | Acc_1: (82.10%) (12716/15488)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss_1: (0.5215) | Acc_1: (82.08%) (13764/16768)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss_1: (0.5221) | Acc_1: (82.09%) (14816/18048)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss_1: (0.5193) | Acc_1: (82.15%) (15878/19328)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss_1: (0.5185) | Acc_1: (82.22%) (16944/20608)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss_1: (0.5211) | Acc_1: (82.10%) (17969/21888)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss_1: (0.5207) | Acc_1: (82.10%) (19021/23168)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss_1: (0.5223) | Acc_1: (82.06%) (20063/24448)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss_1: (0.5230) | Acc_1: (82.08%) (21117/25728)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss_1: (0.5210) | Acc_1: (82.15%) (22188/27008)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss_1: (0.5212) | Acc_1: (82.15%) (23240/28288)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss_1: (0.5190) | Acc_1: (82.16%) (24293/29568)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss_1: (0.5189) | Acc_1: (82.12%) (25331/30848)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss_1: (0.5168) | Acc_1: (82.19%) (26406/32128)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss_1: (0.5165) | Acc_1: (82.17%) (27450/33408)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss_1: (0.5171) | Acc_1: (82.13%) (28488/34688)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss_1: (0.5171) | Acc_1: (82.15%) (29546/35968)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss_1: (0.5158) | Acc_1: (82.20%) (30617/37248)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss_1: (0.5141) | Acc_1: (82.26%) (31693/38528)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss_1: (0.5136) | Acc_1: (82.26%) (32748/39808)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss_1: (0.5129) | Acc_1: (82.29%) (33812/41088)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss_1: (0.5128) | Acc_1: (82.29%) (34865/42368)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss_1: (0.5110) | Acc_1: (82.32%) (35931/43648)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss_1: (0.5097) | Acc_1: (82.36%) (37003/44928)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss_1: (0.5105) | Acc_1: (82.36%) (38058/46208)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss_1: (0.5096) | Acc_1: (82.39%) (39126/47488)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss_1: (0.5104) | Acc_1: (82.38%) (40175/48768)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss_1: (0.5123) | Acc_1: (82.33%) (41166/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5655) | Acc: (80.98%) (8098/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (0.3900) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (0.4762) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (0.4687) | Acc_1: (82.78%) (2225/2688)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (0.4653) | Acc_1: (83.22%) (3302/3968)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss_1: (0.4654) | Acc_1: (83.29%) (4371/5248)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss_1: (0.4594) | Acc_1: (83.81%) (5471/6528)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss_1: (0.4613) | Acc_1: (83.91%) (6552/7808)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss_1: (0.4616) | Acc_1: (83.85%) (7620/9088)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss_1: (0.4641) | Acc_1: (83.91%) (8700/10368)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss_1: (0.4687) | Acc_1: (83.87%) (9769/11648)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss_1: (0.4655) | Acc_1: (84.00%) (10859/12928)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss_1: (0.4644) | Acc_1: (83.92%) (11923/14208)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss_1: (0.4648) | Acc_1: (83.89%) (12993/15488)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss_1: (0.4664) | Acc_1: (83.86%) (14061/16768)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss_1: (0.4632) | Acc_1: (83.93%) (15148/18048)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss_1: (0.4642) | Acc_1: (83.87%) (16211/19328)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss_1: (0.4647) | Acc_1: (83.87%) (17283/20608)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss_1: (0.4649) | Acc_1: (83.86%) (18355/21888)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss_1: (0.4648) | Acc_1: (83.86%) (19429/23168)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss_1: (0.4653) | Acc_1: (83.86%) (20501/24448)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss_1: (0.4641) | Acc_1: (83.87%) (21577/25728)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss_1: (0.4641) | Acc_1: (83.83%) (22641/27008)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss_1: (0.4634) | Acc_1: (83.82%) (23712/28288)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss_1: (0.4638) | Acc_1: (83.85%) (24793/29568)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss_1: (0.4646) | Acc_1: (83.84%) (25864/30848)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss_1: (0.4641) | Acc_1: (83.88%) (26950/32128)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss_1: (0.4651) | Acc_1: (83.84%) (28008/33408)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss_1: (0.4635) | Acc_1: (83.86%) (29091/34688)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss_1: (0.4621) | Acc_1: (83.92%) (30183/35968)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss_1: (0.4629) | Acc_1: (83.85%) (31231/37248)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss_1: (0.4622) | Acc_1: (83.87%) (32313/38528)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss_1: (0.4615) | Acc_1: (83.88%) (33390/39808)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss_1: (0.4616) | Acc_1: (83.90%) (34472/41088)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss_1: (0.4618) | Acc_1: (83.91%) (35551/42368)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss_1: (0.4613) | Acc_1: (83.94%) (36639/43648)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss_1: (0.4623) | Acc_1: (83.88%) (37684/44928)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss_1: (0.4622) | Acc_1: (83.89%) (38762/46208)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss_1: (0.4615) | Acc_1: (83.90%) (39841/47488)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss_1: (0.4604) | Acc_1: (83.91%) (40922/48768)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss_1: (0.4590) | Acc_1: (83.96%) (41981/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5947) | Acc: (80.62%) (8062/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (0.4906) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (0.4337) | Acc_1: (85.01%) (1197/1408)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (0.4101) | Acc_1: (85.71%) (2304/2688)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.4275) | Acc_1: (85.23%) (3382/3968)\n",
      "Epoch: 8 | Batch_idx: 40 |  Loss_1: (0.4215) | Acc_1: (85.69%) (4497/5248)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss_1: (0.4177) | Acc_1: (85.71%) (5595/6528)\n",
      "Epoch: 8 | Batch_idx: 60 |  Loss_1: (0.4143) | Acc_1: (85.66%) (6688/7808)\n",
      "Epoch: 8 | Batch_idx: 70 |  Loss_1: (0.4140) | Acc_1: (85.56%) (7776/9088)\n",
      "Epoch: 8 | Batch_idx: 80 |  Loss_1: (0.4157) | Acc_1: (85.66%) (8881/10368)\n",
      "Epoch: 8 | Batch_idx: 90 |  Loss_1: (0.4202) | Acc_1: (85.42%) (9950/11648)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss_1: (0.4201) | Acc_1: (85.29%) (11026/12928)\n",
      "Epoch: 8 | Batch_idx: 110 |  Loss_1: (0.4243) | Acc_1: (85.22%) (12108/14208)\n",
      "Epoch: 8 | Batch_idx: 120 |  Loss_1: (0.4251) | Acc_1: (85.27%) (13207/15488)\n",
      "Epoch: 8 | Batch_idx: 130 |  Loss_1: (0.4238) | Acc_1: (85.23%) (14292/16768)\n",
      "Epoch: 8 | Batch_idx: 140 |  Loss_1: (0.4206) | Acc_1: (85.35%) (15404/18048)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss_1: (0.4176) | Acc_1: (85.46%) (16517/19328)\n",
      "Epoch: 8 | Batch_idx: 160 |  Loss_1: (0.4175) | Acc_1: (85.46%) (17612/20608)\n",
      "Epoch: 8 | Batch_idx: 170 |  Loss_1: (0.4196) | Acc_1: (85.46%) (18706/21888)\n",
      "Epoch: 8 | Batch_idx: 180 |  Loss_1: (0.4228) | Acc_1: (85.35%) (19775/23168)\n",
      "Epoch: 8 | Batch_idx: 190 |  Loss_1: (0.4253) | Acc_1: (85.32%) (20860/24448)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss_1: (0.4276) | Acc_1: (85.25%) (21934/25728)\n",
      "Epoch: 8 | Batch_idx: 210 |  Loss_1: (0.4268) | Acc_1: (85.30%) (23039/27008)\n",
      "Epoch: 8 | Batch_idx: 220 |  Loss_1: (0.4260) | Acc_1: (85.35%) (24143/28288)\n",
      "Epoch: 8 | Batch_idx: 230 |  Loss_1: (0.4255) | Acc_1: (85.38%) (25246/29568)\n",
      "Epoch: 8 | Batch_idx: 240 |  Loss_1: (0.4225) | Acc_1: (85.47%) (26367/30848)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss_1: (0.4214) | Acc_1: (85.50%) (27469/32128)\n",
      "Epoch: 8 | Batch_idx: 260 |  Loss_1: (0.4209) | Acc_1: (85.54%) (28578/33408)\n",
      "Epoch: 8 | Batch_idx: 270 |  Loss_1: (0.4194) | Acc_1: (85.59%) (29689/34688)\n",
      "Epoch: 8 | Batch_idx: 280 |  Loss_1: (0.4185) | Acc_1: (85.61%) (30792/35968)\n",
      "Epoch: 8 | Batch_idx: 290 |  Loss_1: (0.4181) | Acc_1: (85.62%) (31890/37248)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss_1: (0.4176) | Acc_1: (85.63%) (32991/38528)\n",
      "Epoch: 8 | Batch_idx: 310 |  Loss_1: (0.4183) | Acc_1: (85.62%) (34083/39808)\n",
      "Epoch: 8 | Batch_idx: 320 |  Loss_1: (0.4177) | Acc_1: (85.62%) (35181/41088)\n",
      "Epoch: 8 | Batch_idx: 330 |  Loss_1: (0.4165) | Acc_1: (85.70%) (36311/42368)\n",
      "Epoch: 8 | Batch_idx: 340 |  Loss_1: (0.4161) | Acc_1: (85.75%) (37428/43648)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss_1: (0.4157) | Acc_1: (85.76%) (38530/44928)\n",
      "Epoch: 8 | Batch_idx: 360 |  Loss_1: (0.4166) | Acc_1: (85.71%) (39607/46208)\n",
      "Epoch: 8 | Batch_idx: 370 |  Loss_1: (0.4170) | Acc_1: (85.70%) (40698/47488)\n",
      "Epoch: 8 | Batch_idx: 380 |  Loss_1: (0.4174) | Acc_1: (85.70%) (41794/48768)\n",
      "Epoch: 8 | Batch_idx: 390 |  Loss_1: (0.4171) | Acc_1: (85.71%) (42853/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5609) | Acc: (81.47%) (8147/10000)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (0.3318) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (0.3510) | Acc_1: (87.93%) (1238/1408)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (0.3557) | Acc_1: (87.72%) (2358/2688)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (0.3626) | Acc_1: (87.68%) (3479/3968)\n",
      "Epoch: 9 | Batch_idx: 40 |  Loss_1: (0.3633) | Acc_1: (87.50%) (4592/5248)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss_1: (0.3649) | Acc_1: (87.58%) (5717/6528)\n",
      "Epoch: 9 | Batch_idx: 60 |  Loss_1: (0.3596) | Acc_1: (87.74%) (6851/7808)\n",
      "Epoch: 9 | Batch_idx: 70 |  Loss_1: (0.3597) | Acc_1: (87.59%) (7960/9088)\n",
      "Epoch: 9 | Batch_idx: 80 |  Loss_1: (0.3630) | Acc_1: (87.43%) (9065/10368)\n",
      "Epoch: 9 | Batch_idx: 90 |  Loss_1: (0.3682) | Acc_1: (87.32%) (10171/11648)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss_1: (0.3679) | Acc_1: (87.32%) (11289/12928)\n",
      "Epoch: 9 | Batch_idx: 110 |  Loss_1: (0.3723) | Acc_1: (87.15%) (12382/14208)\n",
      "Epoch: 9 | Batch_idx: 120 |  Loss_1: (0.3747) | Acc_1: (87.06%) (13484/15488)\n",
      "Epoch: 9 | Batch_idx: 130 |  Loss_1: (0.3753) | Acc_1: (86.96%) (14582/16768)\n",
      "Epoch: 9 | Batch_idx: 140 |  Loss_1: (0.3741) | Acc_1: (87.02%) (15705/18048)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss_1: (0.3752) | Acc_1: (87.01%) (16818/19328)\n",
      "Epoch: 9 | Batch_idx: 160 |  Loss_1: (0.3740) | Acc_1: (87.06%) (17942/20608)\n",
      "Epoch: 9 | Batch_idx: 170 |  Loss_1: (0.3741) | Acc_1: (87.05%) (19053/21888)\n",
      "Epoch: 9 | Batch_idx: 180 |  Loss_1: (0.3758) | Acc_1: (86.98%) (20152/23168)\n",
      "Epoch: 9 | Batch_idx: 190 |  Loss_1: (0.3762) | Acc_1: (87.02%) (21275/24448)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss_1: (0.3762) | Acc_1: (87.00%) (22384/25728)\n",
      "Epoch: 9 | Batch_idx: 210 |  Loss_1: (0.3777) | Acc_1: (86.96%) (23487/27008)\n",
      "Epoch: 9 | Batch_idx: 220 |  Loss_1: (0.3789) | Acc_1: (86.93%) (24591/28288)\n",
      "Epoch: 9 | Batch_idx: 230 |  Loss_1: (0.3795) | Acc_1: (86.88%) (25688/29568)\n",
      "Epoch: 9 | Batch_idx: 240 |  Loss_1: (0.3784) | Acc_1: (86.91%) (26809/30848)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss_1: (0.3784) | Acc_1: (86.92%) (27926/32128)\n",
      "Epoch: 9 | Batch_idx: 260 |  Loss_1: (0.3771) | Acc_1: (86.95%) (29047/33408)\n",
      "Epoch: 9 | Batch_idx: 270 |  Loss_1: (0.3769) | Acc_1: (87.00%) (30178/34688)\n",
      "Epoch: 9 | Batch_idx: 280 |  Loss_1: (0.3753) | Acc_1: (87.06%) (31312/35968)\n",
      "Epoch: 9 | Batch_idx: 290 |  Loss_1: (0.3768) | Acc_1: (86.99%) (32401/37248)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss_1: (0.3776) | Acc_1: (86.99%) (33514/38528)\n",
      "Epoch: 9 | Batch_idx: 310 |  Loss_1: (0.3767) | Acc_1: (87.05%) (34651/39808)\n",
      "Epoch: 9 | Batch_idx: 320 |  Loss_1: (0.3777) | Acc_1: (87.02%) (35753/41088)\n",
      "Epoch: 9 | Batch_idx: 330 |  Loss_1: (0.3782) | Acc_1: (86.99%) (36854/42368)\n",
      "Epoch: 9 | Batch_idx: 340 |  Loss_1: (0.3781) | Acc_1: (87.01%) (37978/43648)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss_1: (0.3773) | Acc_1: (87.06%) (39113/44928)\n",
      "Epoch: 9 | Batch_idx: 360 |  Loss_1: (0.3754) | Acc_1: (87.12%) (40257/46208)\n",
      "Epoch: 9 | Batch_idx: 370 |  Loss_1: (0.3749) | Acc_1: (87.15%) (41386/47488)\n",
      "Epoch: 9 | Batch_idx: 380 |  Loss_1: (0.3755) | Acc_1: (87.14%) (42494/48768)\n",
      "Epoch: 9 | Batch_idx: 390 |  Loss_1: (0.3752) | Acc_1: (87.15%) (43574/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4750) | Acc: (84.47%) (8447/10000)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (0.3739) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.3571) | Acc_1: (87.50%) (1232/1408)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.3402) | Acc_1: (88.02%) (2366/2688)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.3353) | Acc_1: (88.05%) (3494/3968)\n",
      "Epoch: 10 | Batch_idx: 40 |  Loss_1: (0.3413) | Acc_1: (87.84%) (4610/5248)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss_1: (0.3505) | Acc_1: (87.87%) (5736/6528)\n",
      "Epoch: 10 | Batch_idx: 60 |  Loss_1: (0.3453) | Acc_1: (88.04%) (6874/7808)\n",
      "Epoch: 10 | Batch_idx: 70 |  Loss_1: (0.3523) | Acc_1: (87.86%) (7985/9088)\n",
      "Epoch: 10 | Batch_idx: 80 |  Loss_1: (0.3520) | Acc_1: (87.86%) (9109/10368)\n",
      "Epoch: 10 | Batch_idx: 90 |  Loss_1: (0.3551) | Acc_1: (87.81%) (10228/11648)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss_1: (0.3559) | Acc_1: (87.80%) (11351/12928)\n",
      "Epoch: 10 | Batch_idx: 110 |  Loss_1: (0.3536) | Acc_1: (87.82%) (12478/14208)\n",
      "Epoch: 10 | Batch_idx: 120 |  Loss_1: (0.3519) | Acc_1: (87.82%) (13601/15488)\n",
      "Epoch: 10 | Batch_idx: 130 |  Loss_1: (0.3531) | Acc_1: (87.74%) (14712/16768)\n",
      "Epoch: 10 | Batch_idx: 140 |  Loss_1: (0.3503) | Acc_1: (87.85%) (15856/18048)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss_1: (0.3470) | Acc_1: (87.95%) (16999/19328)\n",
      "Epoch: 10 | Batch_idx: 160 |  Loss_1: (0.3450) | Acc_1: (87.98%) (18130/20608)\n",
      "Epoch: 10 | Batch_idx: 170 |  Loss_1: (0.3440) | Acc_1: (88.04%) (19271/21888)\n",
      "Epoch: 10 | Batch_idx: 180 |  Loss_1: (0.3451) | Acc_1: (87.99%) (20386/23168)\n",
      "Epoch: 10 | Batch_idx: 190 |  Loss_1: (0.3473) | Acc_1: (87.94%) (21500/24448)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss_1: (0.3478) | Acc_1: (87.93%) (22623/25728)\n",
      "Epoch: 10 | Batch_idx: 210 |  Loss_1: (0.3470) | Acc_1: (87.94%) (23751/27008)\n",
      "Epoch: 10 | Batch_idx: 220 |  Loss_1: (0.3479) | Acc_1: (87.89%) (24863/28288)\n",
      "Epoch: 10 | Batch_idx: 230 |  Loss_1: (0.3477) | Acc_1: (87.93%) (25999/29568)\n",
      "Epoch: 10 | Batch_idx: 240 |  Loss_1: (0.3490) | Acc_1: (87.90%) (27115/30848)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss_1: (0.3498) | Acc_1: (87.85%) (28225/32128)\n",
      "Epoch: 10 | Batch_idx: 260 |  Loss_1: (0.3496) | Acc_1: (87.90%) (29366/33408)\n",
      "Epoch: 10 | Batch_idx: 270 |  Loss_1: (0.3480) | Acc_1: (87.97%) (30515/34688)\n",
      "Epoch: 10 | Batch_idx: 280 |  Loss_1: (0.3479) | Acc_1: (87.94%) (31630/35968)\n",
      "Epoch: 10 | Batch_idx: 290 |  Loss_1: (0.3469) | Acc_1: (87.98%) (32771/37248)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss_1: (0.3484) | Acc_1: (87.93%) (33877/38528)\n",
      "Epoch: 10 | Batch_idx: 310 |  Loss_1: (0.3486) | Acc_1: (87.92%) (34999/39808)\n",
      "Epoch: 10 | Batch_idx: 320 |  Loss_1: (0.3511) | Acc_1: (87.86%) (36098/41088)\n",
      "Epoch: 10 | Batch_idx: 330 |  Loss_1: (0.3514) | Acc_1: (87.84%) (37217/42368)\n",
      "Epoch: 10 | Batch_idx: 340 |  Loss_1: (0.3522) | Acc_1: (87.84%) (38339/43648)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss_1: (0.3528) | Acc_1: (87.83%) (39459/44928)\n",
      "Epoch: 10 | Batch_idx: 360 |  Loss_1: (0.3519) | Acc_1: (87.86%) (40598/46208)\n",
      "Epoch: 10 | Batch_idx: 370 |  Loss_1: (0.3514) | Acc_1: (87.88%) (41731/47488)\n",
      "Epoch: 10 | Batch_idx: 380 |  Loss_1: (0.3513) | Acc_1: (87.86%) (42847/48768)\n",
      "Epoch: 10 | Batch_idx: 390 |  Loss_1: (0.3505) | Acc_1: (87.87%) (43933/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5169) | Acc: (83.47%) (8347/10000)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.2616) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.2955) | Acc_1: (89.91%) (1266/1408)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.2973) | Acc_1: (89.73%) (2412/2688)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.3216) | Acc_1: (88.91%) (3528/3968)\n",
      "Epoch: 11 | Batch_idx: 40 |  Loss_1: (0.3150) | Acc_1: (89.27%) (4685/5248)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss_1: (0.3200) | Acc_1: (89.19%) (5822/6528)\n",
      "Epoch: 11 | Batch_idx: 60 |  Loss_1: (0.3199) | Acc_1: (89.15%) (6961/7808)\n",
      "Epoch: 11 | Batch_idx: 70 |  Loss_1: (0.3229) | Acc_1: (88.94%) (8083/9088)\n",
      "Epoch: 11 | Batch_idx: 80 |  Loss_1: (0.3174) | Acc_1: (89.03%) (9231/10368)\n",
      "Epoch: 11 | Batch_idx: 90 |  Loss_1: (0.3130) | Acc_1: (89.17%) (10386/11648)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss_1: (0.3142) | Acc_1: (89.11%) (11520/12928)\n",
      "Epoch: 11 | Batch_idx: 110 |  Loss_1: (0.3148) | Acc_1: (89.10%) (12659/14208)\n",
      "Epoch: 11 | Batch_idx: 120 |  Loss_1: (0.3147) | Acc_1: (89.05%) (13792/15488)\n",
      "Epoch: 11 | Batch_idx: 130 |  Loss_1: (0.3151) | Acc_1: (88.95%) (14915/16768)\n",
      "Epoch: 11 | Batch_idx: 140 |  Loss_1: (0.3156) | Acc_1: (88.99%) (16061/18048)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss_1: (0.3144) | Acc_1: (88.98%) (17199/19328)\n",
      "Epoch: 11 | Batch_idx: 160 |  Loss_1: (0.3169) | Acc_1: (88.87%) (18315/20608)\n",
      "Epoch: 11 | Batch_idx: 170 |  Loss_1: (0.3178) | Acc_1: (88.86%) (19450/21888)\n",
      "Epoch: 11 | Batch_idx: 180 |  Loss_1: (0.3171) | Acc_1: (88.84%) (20582/23168)\n",
      "Epoch: 11 | Batch_idx: 190 |  Loss_1: (0.3198) | Acc_1: (88.77%) (21702/24448)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss_1: (0.3205) | Acc_1: (88.72%) (22826/25728)\n",
      "Epoch: 11 | Batch_idx: 210 |  Loss_1: (0.3224) | Acc_1: (88.67%) (23947/27008)\n",
      "Epoch: 11 | Batch_idx: 220 |  Loss_1: (0.3230) | Acc_1: (88.63%) (25072/28288)\n",
      "Epoch: 11 | Batch_idx: 230 |  Loss_1: (0.3233) | Acc_1: (88.63%) (26206/29568)\n",
      "Epoch: 11 | Batch_idx: 240 |  Loss_1: (0.3232) | Acc_1: (88.65%) (27348/30848)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss_1: (0.3241) | Acc_1: (88.63%) (28474/32128)\n",
      "Epoch: 11 | Batch_idx: 260 |  Loss_1: (0.3235) | Acc_1: (88.67%) (29622/33408)\n",
      "Epoch: 11 | Batch_idx: 270 |  Loss_1: (0.3235) | Acc_1: (88.68%) (30760/34688)\n",
      "Epoch: 11 | Batch_idx: 280 |  Loss_1: (0.3232) | Acc_1: (88.69%) (31901/35968)\n",
      "Epoch: 11 | Batch_idx: 290 |  Loss_1: (0.3228) | Acc_1: (88.70%) (33038/37248)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss_1: (0.3235) | Acc_1: (88.67%) (34164/38528)\n",
      "Epoch: 11 | Batch_idx: 310 |  Loss_1: (0.3235) | Acc_1: (88.71%) (35314/39808)\n",
      "Epoch: 11 | Batch_idx: 320 |  Loss_1: (0.3243) | Acc_1: (88.69%) (36439/41088)\n",
      "Epoch: 11 | Batch_idx: 330 |  Loss_1: (0.3237) | Acc_1: (88.69%) (37576/42368)\n",
      "Epoch: 11 | Batch_idx: 340 |  Loss_1: (0.3242) | Acc_1: (88.67%) (38702/43648)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss_1: (0.3236) | Acc_1: (88.69%) (39846/44928)\n",
      "Epoch: 11 | Batch_idx: 360 |  Loss_1: (0.3239) | Acc_1: (88.66%) (40970/46208)\n",
      "Epoch: 11 | Batch_idx: 370 |  Loss_1: (0.3251) | Acc_1: (88.63%) (42087/47488)\n",
      "Epoch: 11 | Batch_idx: 380 |  Loss_1: (0.3252) | Acc_1: (88.60%) (43210/48768)\n",
      "Epoch: 11 | Batch_idx: 390 |  Loss_1: (0.3263) | Acc_1: (88.57%) (44285/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4674) | Acc: (85.23%) (8523/10000)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.3620) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.2979) | Acc_1: (89.99%) (1267/1408)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.3015) | Acc_1: (89.81%) (2414/2688)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.2957) | Acc_1: (89.59%) (3555/3968)\n",
      "Epoch: 12 | Batch_idx: 40 |  Loss_1: (0.2873) | Acc_1: (89.90%) (4718/5248)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss_1: (0.2839) | Acc_1: (90.18%) (5887/6528)\n",
      "Epoch: 12 | Batch_idx: 60 |  Loss_1: (0.2821) | Acc_1: (90.14%) (7038/7808)\n",
      "Epoch: 12 | Batch_idx: 70 |  Loss_1: (0.2856) | Acc_1: (90.03%) (8182/9088)\n",
      "Epoch: 12 | Batch_idx: 80 |  Loss_1: (0.2843) | Acc_1: (90.01%) (9332/10368)\n",
      "Epoch: 12 | Batch_idx: 90 |  Loss_1: (0.2803) | Acc_1: (90.14%) (10499/11648)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss_1: (0.2829) | Acc_1: (90.08%) (11645/12928)\n",
      "Epoch: 12 | Batch_idx: 110 |  Loss_1: (0.2871) | Acc_1: (89.92%) (12776/14208)\n",
      "Epoch: 12 | Batch_idx: 120 |  Loss_1: (0.2863) | Acc_1: (90.00%) (13939/15488)\n",
      "Epoch: 12 | Batch_idx: 130 |  Loss_1: (0.2874) | Acc_1: (90.01%) (15093/16768)\n",
      "Epoch: 12 | Batch_idx: 140 |  Loss_1: (0.2881) | Acc_1: (89.98%) (16240/18048)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss_1: (0.2874) | Acc_1: (89.98%) (17391/19328)\n",
      "Epoch: 12 | Batch_idx: 160 |  Loss_1: (0.2862) | Acc_1: (90.04%) (18556/20608)\n",
      "Epoch: 12 | Batch_idx: 170 |  Loss_1: (0.2858) | Acc_1: (90.04%) (19709/21888)\n",
      "Epoch: 12 | Batch_idx: 180 |  Loss_1: (0.2876) | Acc_1: (89.95%) (20840/23168)\n",
      "Epoch: 12 | Batch_idx: 190 |  Loss_1: (0.2882) | Acc_1: (89.94%) (21988/24448)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss_1: (0.2923) | Acc_1: (89.78%) (23099/25728)\n",
      "Epoch: 12 | Batch_idx: 210 |  Loss_1: (0.2935) | Acc_1: (89.71%) (24228/27008)\n",
      "Epoch: 12 | Batch_idx: 220 |  Loss_1: (0.2943) | Acc_1: (89.70%) (25374/28288)\n",
      "Epoch: 12 | Batch_idx: 230 |  Loss_1: (0.2939) | Acc_1: (89.69%) (26521/29568)\n",
      "Epoch: 12 | Batch_idx: 240 |  Loss_1: (0.2945) | Acc_1: (89.70%) (27672/30848)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss_1: (0.2952) | Acc_1: (89.71%) (28821/32128)\n",
      "Epoch: 12 | Batch_idx: 260 |  Loss_1: (0.2961) | Acc_1: (89.70%) (29967/33408)\n",
      "Epoch: 12 | Batch_idx: 270 |  Loss_1: (0.2961) | Acc_1: (89.70%) (31115/34688)\n",
      "Epoch: 12 | Batch_idx: 280 |  Loss_1: (0.2955) | Acc_1: (89.72%) (32269/35968)\n",
      "Epoch: 12 | Batch_idx: 290 |  Loss_1: (0.2962) | Acc_1: (89.72%) (33420/37248)\n",
      "Epoch: 12 | Batch_idx: 300 |  Loss_1: (0.2956) | Acc_1: (89.77%) (34588/38528)\n",
      "Epoch: 12 | Batch_idx: 310 |  Loss_1: (0.2954) | Acc_1: (89.76%) (35733/39808)\n",
      "Epoch: 12 | Batch_idx: 320 |  Loss_1: (0.2965) | Acc_1: (89.73%) (36869/41088)\n",
      "Epoch: 12 | Batch_idx: 330 |  Loss_1: (0.2959) | Acc_1: (89.73%) (38017/42368)\n",
      "Epoch: 12 | Batch_idx: 340 |  Loss_1: (0.2952) | Acc_1: (89.73%) (39166/43648)\n",
      "Epoch: 12 | Batch_idx: 350 |  Loss_1: (0.2942) | Acc_1: (89.77%) (40334/44928)\n",
      "Epoch: 12 | Batch_idx: 360 |  Loss_1: (0.2934) | Acc_1: (89.82%) (41504/46208)\n",
      "Epoch: 12 | Batch_idx: 370 |  Loss_1: (0.2942) | Acc_1: (89.79%) (42641/47488)\n",
      "Epoch: 12 | Batch_idx: 380 |  Loss_1: (0.2946) | Acc_1: (89.77%) (43781/48768)\n",
      "Epoch: 12 | Batch_idx: 390 |  Loss_1: (0.2947) | Acc_1: (89.80%) (44900/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4876) | Acc: (84.30%) (8430/10000)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.1939) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.2613) | Acc_1: (90.77%) (1278/1408)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.2628) | Acc_1: (90.66%) (2437/2688)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.2650) | Acc_1: (90.70%) (3599/3968)\n",
      "Epoch: 13 | Batch_idx: 40 |  Loss_1: (0.2674) | Acc_1: (90.70%) (4760/5248)\n",
      "Epoch: 13 | Batch_idx: 50 |  Loss_1: (0.2711) | Acc_1: (90.61%) (5915/6528)\n",
      "Epoch: 13 | Batch_idx: 60 |  Loss_1: (0.2742) | Acc_1: (90.42%) (7060/7808)\n",
      "Epoch: 13 | Batch_idx: 70 |  Loss_1: (0.2759) | Acc_1: (90.44%) (8219/9088)\n",
      "Epoch: 13 | Batch_idx: 80 |  Loss_1: (0.2766) | Acc_1: (90.51%) (9384/10368)\n",
      "Epoch: 13 | Batch_idx: 90 |  Loss_1: (0.2736) | Acc_1: (90.64%) (10558/11648)\n",
      "Epoch: 13 | Batch_idx: 100 |  Loss_1: (0.2727) | Acc_1: (90.67%) (11722/12928)\n",
      "Epoch: 13 | Batch_idx: 110 |  Loss_1: (0.2713) | Acc_1: (90.67%) (12882/14208)\n",
      "Epoch: 13 | Batch_idx: 120 |  Loss_1: (0.2718) | Acc_1: (90.70%) (14047/15488)\n",
      "Epoch: 13 | Batch_idx: 130 |  Loss_1: (0.2705) | Acc_1: (90.70%) (15209/16768)\n",
      "Epoch: 13 | Batch_idx: 140 |  Loss_1: (0.2708) | Acc_1: (90.66%) (16363/18048)\n",
      "Epoch: 13 | Batch_idx: 150 |  Loss_1: (0.2717) | Acc_1: (90.66%) (17522/19328)\n",
      "Epoch: 13 | Batch_idx: 160 |  Loss_1: (0.2730) | Acc_1: (90.61%) (18673/20608)\n",
      "Epoch: 13 | Batch_idx: 170 |  Loss_1: (0.2738) | Acc_1: (90.56%) (19822/21888)\n",
      "Epoch: 13 | Batch_idx: 180 |  Loss_1: (0.2749) | Acc_1: (90.52%) (20971/23168)\n",
      "Epoch: 13 | Batch_idx: 190 |  Loss_1: (0.2753) | Acc_1: (90.44%) (22110/24448)\n",
      "Epoch: 13 | Batch_idx: 200 |  Loss_1: (0.2745) | Acc_1: (90.44%) (23268/25728)\n",
      "Epoch: 13 | Batch_idx: 210 |  Loss_1: (0.2750) | Acc_1: (90.45%) (24428/27008)\n",
      "Epoch: 13 | Batch_idx: 220 |  Loss_1: (0.2743) | Acc_1: (90.46%) (25590/28288)\n",
      "Epoch: 13 | Batch_idx: 230 |  Loss_1: (0.2724) | Acc_1: (90.50%) (26760/29568)\n",
      "Epoch: 13 | Batch_idx: 240 |  Loss_1: (0.2723) | Acc_1: (90.51%) (27919/30848)\n",
      "Epoch: 13 | Batch_idx: 250 |  Loss_1: (0.2744) | Acc_1: (90.43%) (29052/32128)\n",
      "Epoch: 13 | Batch_idx: 260 |  Loss_1: (0.2735) | Acc_1: (90.45%) (30219/33408)\n",
      "Epoch: 13 | Batch_idx: 270 |  Loss_1: (0.2730) | Acc_1: (90.48%) (31385/34688)\n",
      "Epoch: 13 | Batch_idx: 280 |  Loss_1: (0.2744) | Acc_1: (90.44%) (32528/35968)\n",
      "Epoch: 13 | Batch_idx: 290 |  Loss_1: (0.2750) | Acc_1: (90.41%) (33676/37248)\n",
      "Epoch: 13 | Batch_idx: 300 |  Loss_1: (0.2756) | Acc_1: (90.39%) (34825/38528)\n",
      "Epoch: 13 | Batch_idx: 310 |  Loss_1: (0.2750) | Acc_1: (90.40%) (35987/39808)\n",
      "Epoch: 13 | Batch_idx: 320 |  Loss_1: (0.2759) | Acc_1: (90.35%) (37124/41088)\n",
      "Epoch: 13 | Batch_idx: 330 |  Loss_1: (0.2757) | Acc_1: (90.35%) (38280/42368)\n",
      "Epoch: 13 | Batch_idx: 340 |  Loss_1: (0.2752) | Acc_1: (90.38%) (39447/43648)\n",
      "Epoch: 13 | Batch_idx: 350 |  Loss_1: (0.2743) | Acc_1: (90.41%) (40618/44928)\n",
      "Epoch: 13 | Batch_idx: 360 |  Loss_1: (0.2760) | Acc_1: (90.36%) (41754/46208)\n",
      "Epoch: 13 | Batch_idx: 370 |  Loss_1: (0.2761) | Acc_1: (90.37%) (42915/47488)\n",
      "Epoch: 13 | Batch_idx: 380 |  Loss_1: (0.2766) | Acc_1: (90.39%) (44083/48768)\n",
      "Epoch: 13 | Batch_idx: 390 |  Loss_1: (0.2769) | Acc_1: (90.37%) (45187/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4518) | Acc: (85.97%) (8597/10000)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.2964) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.2664) | Acc_1: (90.91%) (1280/1408)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.2600) | Acc_1: (91.15%) (2450/2688)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.2493) | Acc_1: (91.36%) (3625/3968)\n",
      "Epoch: 14 | Batch_idx: 40 |  Loss_1: (0.2515) | Acc_1: (91.27%) (4790/5248)\n",
      "Epoch: 14 | Batch_idx: 50 |  Loss_1: (0.2563) | Acc_1: (91.04%) (5943/6528)\n",
      "Epoch: 14 | Batch_idx: 60 |  Loss_1: (0.2568) | Acc_1: (91.02%) (7107/7808)\n",
      "Epoch: 14 | Batch_idx: 70 |  Loss_1: (0.2533) | Acc_1: (91.08%) (8277/9088)\n",
      "Epoch: 14 | Batch_idx: 80 |  Loss_1: (0.2556) | Acc_1: (91.14%) (9449/10368)\n",
      "Epoch: 14 | Batch_idx: 90 |  Loss_1: (0.2544) | Acc_1: (91.13%) (10615/11648)\n",
      "Epoch: 14 | Batch_idx: 100 |  Loss_1: (0.2532) | Acc_1: (91.17%) (11786/12928)\n",
      "Epoch: 14 | Batch_idx: 110 |  Loss_1: (0.2518) | Acc_1: (91.22%) (12961/14208)\n",
      "Epoch: 14 | Batch_idx: 120 |  Loss_1: (0.2517) | Acc_1: (91.21%) (14127/15488)\n",
      "Epoch: 14 | Batch_idx: 130 |  Loss_1: (0.2527) | Acc_1: (91.17%) (15288/16768)\n",
      "Epoch: 14 | Batch_idx: 140 |  Loss_1: (0.2514) | Acc_1: (91.28%) (16475/18048)\n",
      "Epoch: 14 | Batch_idx: 150 |  Loss_1: (0.2517) | Acc_1: (91.26%) (17639/19328)\n",
      "Epoch: 14 | Batch_idx: 160 |  Loss_1: (0.2510) | Acc_1: (91.28%) (18811/20608)\n",
      "Epoch: 14 | Batch_idx: 170 |  Loss_1: (0.2529) | Acc_1: (91.18%) (19958/21888)\n",
      "Epoch: 14 | Batch_idx: 180 |  Loss_1: (0.2564) | Acc_1: (91.06%) (21097/23168)\n",
      "Epoch: 14 | Batch_idx: 190 |  Loss_1: (0.2560) | Acc_1: (91.08%) (22267/24448)\n",
      "Epoch: 14 | Batch_idx: 200 |  Loss_1: (0.2558) | Acc_1: (91.08%) (23433/25728)\n",
      "Epoch: 14 | Batch_idx: 210 |  Loss_1: (0.2543) | Acc_1: (91.14%) (24616/27008)\n",
      "Epoch: 14 | Batch_idx: 220 |  Loss_1: (0.2549) | Acc_1: (91.13%) (25778/28288)\n",
      "Epoch: 14 | Batch_idx: 230 |  Loss_1: (0.2549) | Acc_1: (91.13%) (26946/29568)\n",
      "Epoch: 14 | Batch_idx: 240 |  Loss_1: (0.2553) | Acc_1: (91.14%) (28114/30848)\n",
      "Epoch: 14 | Batch_idx: 250 |  Loss_1: (0.2549) | Acc_1: (91.17%) (29292/32128)\n",
      "Epoch: 14 | Batch_idx: 260 |  Loss_1: (0.2564) | Acc_1: (91.13%) (30445/33408)\n",
      "Epoch: 14 | Batch_idx: 270 |  Loss_1: (0.2560) | Acc_1: (91.15%) (31619/34688)\n",
      "Epoch: 14 | Batch_idx: 280 |  Loss_1: (0.2559) | Acc_1: (91.13%) (32776/35968)\n",
      "Epoch: 14 | Batch_idx: 290 |  Loss_1: (0.2552) | Acc_1: (91.16%) (33956/37248)\n",
      "Epoch: 14 | Batch_idx: 300 |  Loss_1: (0.2554) | Acc_1: (91.15%) (35119/38528)\n",
      "Epoch: 14 | Batch_idx: 310 |  Loss_1: (0.2550) | Acc_1: (91.15%) (36286/39808)\n",
      "Epoch: 14 | Batch_idx: 320 |  Loss_1: (0.2557) | Acc_1: (91.14%) (37446/41088)\n",
      "Epoch: 14 | Batch_idx: 330 |  Loss_1: (0.2548) | Acc_1: (91.14%) (38616/42368)\n",
      "Epoch: 14 | Batch_idx: 340 |  Loss_1: (0.2553) | Acc_1: (91.10%) (39763/43648)\n",
      "Epoch: 14 | Batch_idx: 350 |  Loss_1: (0.2558) | Acc_1: (91.07%) (40917/44928)\n",
      "Epoch: 14 | Batch_idx: 360 |  Loss_1: (0.2558) | Acc_1: (91.08%) (42086/46208)\n",
      "Epoch: 14 | Batch_idx: 370 |  Loss_1: (0.2568) | Acc_1: (91.07%) (43246/47488)\n",
      "Epoch: 14 | Batch_idx: 380 |  Loss_1: (0.2571) | Acc_1: (91.06%) (44407/48768)\n",
      "Epoch: 14 | Batch_idx: 390 |  Loss_1: (0.2575) | Acc_1: (91.06%) (45530/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4430) | Acc: (86.09%) (8609/10000)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.2287) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.2421) | Acc_1: (91.55%) (1289/1408)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.2367) | Acc_1: (91.78%) (2467/2688)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.2424) | Acc_1: (91.61%) (3635/3968)\n",
      "Epoch: 15 | Batch_idx: 40 |  Loss_1: (0.2380) | Acc_1: (91.90%) (4823/5248)\n",
      "Epoch: 15 | Batch_idx: 50 |  Loss_1: (0.2399) | Acc_1: (91.71%) (5987/6528)\n",
      "Epoch: 15 | Batch_idx: 60 |  Loss_1: (0.2392) | Acc_1: (91.65%) (7156/7808)\n",
      "Epoch: 15 | Batch_idx: 70 |  Loss_1: (0.2375) | Acc_1: (91.69%) (8333/9088)\n",
      "Epoch: 15 | Batch_idx: 80 |  Loss_1: (0.2343) | Acc_1: (91.80%) (9518/10368)\n",
      "Epoch: 15 | Batch_idx: 90 |  Loss_1: (0.2327) | Acc_1: (91.90%) (10704/11648)\n",
      "Epoch: 15 | Batch_idx: 100 |  Loss_1: (0.2349) | Acc_1: (91.82%) (11871/12928)\n",
      "Epoch: 15 | Batch_idx: 110 |  Loss_1: (0.2335) | Acc_1: (91.86%) (13052/14208)\n",
      "Epoch: 15 | Batch_idx: 120 |  Loss_1: (0.2312) | Acc_1: (91.98%) (14246/15488)\n",
      "Epoch: 15 | Batch_idx: 130 |  Loss_1: (0.2309) | Acc_1: (92.00%) (15426/16768)\n",
      "Epoch: 15 | Batch_idx: 140 |  Loss_1: (0.2303) | Acc_1: (92.00%) (16604/18048)\n",
      "Epoch: 15 | Batch_idx: 150 |  Loss_1: (0.2290) | Acc_1: (92.02%) (17785/19328)\n",
      "Epoch: 15 | Batch_idx: 160 |  Loss_1: (0.2299) | Acc_1: (92.00%) (18960/20608)\n",
      "Epoch: 15 | Batch_idx: 170 |  Loss_1: (0.2301) | Acc_1: (92.00%) (20138/21888)\n",
      "Epoch: 15 | Batch_idx: 180 |  Loss_1: (0.2286) | Acc_1: (92.03%) (21322/23168)\n",
      "Epoch: 15 | Batch_idx: 190 |  Loss_1: (0.2278) | Acc_1: (92.04%) (22501/24448)\n",
      "Epoch: 15 | Batch_idx: 200 |  Loss_1: (0.2275) | Acc_1: (92.07%) (23688/25728)\n",
      "Epoch: 15 | Batch_idx: 210 |  Loss_1: (0.2277) | Acc_1: (92.05%) (24861/27008)\n",
      "Epoch: 15 | Batch_idx: 220 |  Loss_1: (0.2274) | Acc_1: (92.06%) (26041/28288)\n",
      "Epoch: 15 | Batch_idx: 230 |  Loss_1: (0.2297) | Acc_1: (91.96%) (27191/29568)\n",
      "Epoch: 15 | Batch_idx: 240 |  Loss_1: (0.2294) | Acc_1: (91.94%) (28362/30848)\n",
      "Epoch: 15 | Batch_idx: 250 |  Loss_1: (0.2318) | Acc_1: (91.86%) (29513/32128)\n",
      "Epoch: 15 | Batch_idx: 260 |  Loss_1: (0.2333) | Acc_1: (91.80%) (30670/33408)\n",
      "Epoch: 15 | Batch_idx: 270 |  Loss_1: (0.2332) | Acc_1: (91.82%) (31850/34688)\n",
      "Epoch: 15 | Batch_idx: 280 |  Loss_1: (0.2346) | Acc_1: (91.77%) (33007/35968)\n",
      "Epoch: 15 | Batch_idx: 290 |  Loss_1: (0.2337) | Acc_1: (91.79%) (34191/37248)\n",
      "Epoch: 15 | Batch_idx: 300 |  Loss_1: (0.2350) | Acc_1: (91.73%) (35341/38528)\n",
      "Epoch: 15 | Batch_idx: 310 |  Loss_1: (0.2357) | Acc_1: (91.70%) (36502/39808)\n",
      "Epoch: 15 | Batch_idx: 320 |  Loss_1: (0.2366) | Acc_1: (91.65%) (37658/41088)\n",
      "Epoch: 15 | Batch_idx: 330 |  Loss_1: (0.2369) | Acc_1: (91.67%) (38837/42368)\n",
      "Epoch: 15 | Batch_idx: 340 |  Loss_1: (0.2380) | Acc_1: (91.64%) (40001/43648)\n",
      "Epoch: 15 | Batch_idx: 350 |  Loss_1: (0.2384) | Acc_1: (91.64%) (41170/44928)\n",
      "Epoch: 15 | Batch_idx: 360 |  Loss_1: (0.2382) | Acc_1: (91.66%) (42355/46208)\n",
      "Epoch: 15 | Batch_idx: 370 |  Loss_1: (0.2383) | Acc_1: (91.67%) (43531/47488)\n",
      "Epoch: 15 | Batch_idx: 380 |  Loss_1: (0.2382) | Acc_1: (91.67%) (44705/48768)\n",
      "Epoch: 15 | Batch_idx: 390 |  Loss_1: (0.2387) | Acc_1: (91.64%) (45820/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4124) | Acc: (87.23%) (8723/10000)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.1444) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.2199) | Acc_1: (92.47%) (1302/1408)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.2092) | Acc_1: (92.86%) (2496/2688)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.2171) | Acc_1: (92.57%) (3673/3968)\n",
      "Epoch: 16 | Batch_idx: 40 |  Loss_1: (0.2150) | Acc_1: (92.59%) (4859/5248)\n",
      "Epoch: 16 | Batch_idx: 50 |  Loss_1: (0.2175) | Acc_1: (92.56%) (6042/6528)\n",
      "Epoch: 16 | Batch_idx: 60 |  Loss_1: (0.2212) | Acc_1: (92.42%) (7216/7808)\n",
      "Epoch: 16 | Batch_idx: 70 |  Loss_1: (0.2214) | Acc_1: (92.47%) (8404/9088)\n",
      "Epoch: 16 | Batch_idx: 80 |  Loss_1: (0.2183) | Acc_1: (92.54%) (9595/10368)\n",
      "Epoch: 16 | Batch_idx: 90 |  Loss_1: (0.2196) | Acc_1: (92.34%) (10756/11648)\n",
      "Epoch: 16 | Batch_idx: 100 |  Loss_1: (0.2207) | Acc_1: (92.25%) (11926/12928)\n",
      "Epoch: 16 | Batch_idx: 110 |  Loss_1: (0.2250) | Acc_1: (92.10%) (13086/14208)\n",
      "Epoch: 16 | Batch_idx: 120 |  Loss_1: (0.2249) | Acc_1: (92.02%) (14252/15488)\n",
      "Epoch: 16 | Batch_idx: 130 |  Loss_1: (0.2228) | Acc_1: (92.11%) (15445/16768)\n",
      "Epoch: 16 | Batch_idx: 140 |  Loss_1: (0.2217) | Acc_1: (92.14%) (16629/18048)\n",
      "Epoch: 16 | Batch_idx: 150 |  Loss_1: (0.2197) | Acc_1: (92.21%) (17823/19328)\n",
      "Epoch: 16 | Batch_idx: 160 |  Loss_1: (0.2204) | Acc_1: (92.12%) (18984/20608)\n",
      "Epoch: 16 | Batch_idx: 170 |  Loss_1: (0.2225) | Acc_1: (92.04%) (20146/21888)\n",
      "Epoch: 16 | Batch_idx: 180 |  Loss_1: (0.2222) | Acc_1: (92.08%) (21334/23168)\n",
      "Epoch: 16 | Batch_idx: 190 |  Loss_1: (0.2209) | Acc_1: (92.17%) (22534/24448)\n",
      "Epoch: 16 | Batch_idx: 200 |  Loss_1: (0.2208) | Acc_1: (92.16%) (23710/25728)\n",
      "Epoch: 16 | Batch_idx: 210 |  Loss_1: (0.2213) | Acc_1: (92.12%) (24879/27008)\n",
      "Epoch: 16 | Batch_idx: 220 |  Loss_1: (0.2204) | Acc_1: (92.14%) (26065/28288)\n",
      "Epoch: 16 | Batch_idx: 230 |  Loss_1: (0.2204) | Acc_1: (92.16%) (27250/29568)\n",
      "Epoch: 16 | Batch_idx: 240 |  Loss_1: (0.2215) | Acc_1: (92.11%) (28414/30848)\n",
      "Epoch: 16 | Batch_idx: 250 |  Loss_1: (0.2200) | Acc_1: (92.17%) (29613/32128)\n",
      "Epoch: 16 | Batch_idx: 260 |  Loss_1: (0.2189) | Acc_1: (92.20%) (30801/33408)\n",
      "Epoch: 16 | Batch_idx: 270 |  Loss_1: (0.2195) | Acc_1: (92.16%) (31967/34688)\n",
      "Epoch: 16 | Batch_idx: 280 |  Loss_1: (0.2193) | Acc_1: (92.15%) (33146/35968)\n",
      "Epoch: 16 | Batch_idx: 290 |  Loss_1: (0.2189) | Acc_1: (92.15%) (34325/37248)\n",
      "Epoch: 16 | Batch_idx: 300 |  Loss_1: (0.2199) | Acc_1: (92.12%) (35492/38528)\n",
      "Epoch: 16 | Batch_idx: 310 |  Loss_1: (0.2196) | Acc_1: (92.14%) (36681/39808)\n",
      "Epoch: 16 | Batch_idx: 320 |  Loss_1: (0.2191) | Acc_1: (92.16%) (37867/41088)\n",
      "Epoch: 16 | Batch_idx: 330 |  Loss_1: (0.2196) | Acc_1: (92.14%) (39037/42368)\n",
      "Epoch: 16 | Batch_idx: 340 |  Loss_1: (0.2205) | Acc_1: (92.13%) (40213/43648)\n",
      "Epoch: 16 | Batch_idx: 350 |  Loss_1: (0.2213) | Acc_1: (92.11%) (41382/44928)\n",
      "Epoch: 16 | Batch_idx: 360 |  Loss_1: (0.2218) | Acc_1: (92.11%) (42560/46208)\n",
      "Epoch: 16 | Batch_idx: 370 |  Loss_1: (0.2221) | Acc_1: (92.10%) (43735/47488)\n",
      "Epoch: 16 | Batch_idx: 380 |  Loss_1: (0.2239) | Acc_1: (92.06%) (44895/48768)\n",
      "Epoch: 16 | Batch_idx: 390 |  Loss_1: (0.2250) | Acc_1: (92.01%) (46005/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4310) | Acc: (86.84%) (8684/10000)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.2550) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.2005) | Acc_1: (92.26%) (1299/1408)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.2020) | Acc_1: (92.41%) (2484/2688)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.1945) | Acc_1: (92.87%) (3685/3968)\n",
      "Epoch: 17 | Batch_idx: 40 |  Loss_1: (0.1916) | Acc_1: (93.08%) (4885/5248)\n",
      "Epoch: 17 | Batch_idx: 50 |  Loss_1: (0.1929) | Acc_1: (93.06%) (6075/6528)\n",
      "Epoch: 17 | Batch_idx: 60 |  Loss_1: (0.1977) | Acc_1: (92.94%) (7257/7808)\n",
      "Epoch: 17 | Batch_idx: 70 |  Loss_1: (0.1978) | Acc_1: (93.01%) (8453/9088)\n",
      "Epoch: 17 | Batch_idx: 80 |  Loss_1: (0.1980) | Acc_1: (93.11%) (9654/10368)\n",
      "Epoch: 17 | Batch_idx: 90 |  Loss_1: (0.1965) | Acc_1: (93.14%) (10849/11648)\n",
      "Epoch: 17 | Batch_idx: 100 |  Loss_1: (0.1968) | Acc_1: (93.12%) (12038/12928)\n",
      "Epoch: 17 | Batch_idx: 110 |  Loss_1: (0.1958) | Acc_1: (93.10%) (13227/14208)\n",
      "Epoch: 17 | Batch_idx: 120 |  Loss_1: (0.1946) | Acc_1: (93.11%) (14421/15488)\n",
      "Epoch: 17 | Batch_idx: 130 |  Loss_1: (0.1936) | Acc_1: (93.18%) (15624/16768)\n",
      "Epoch: 17 | Batch_idx: 140 |  Loss_1: (0.1943) | Acc_1: (93.11%) (16804/18048)\n",
      "Epoch: 17 | Batch_idx: 150 |  Loss_1: (0.1949) | Acc_1: (93.08%) (17991/19328)\n",
      "Epoch: 17 | Batch_idx: 160 |  Loss_1: (0.1952) | Acc_1: (93.02%) (19170/20608)\n",
      "Epoch: 17 | Batch_idx: 170 |  Loss_1: (0.1972) | Acc_1: (92.95%) (20344/21888)\n",
      "Epoch: 17 | Batch_idx: 180 |  Loss_1: (0.1974) | Acc_1: (92.94%) (21532/23168)\n",
      "Epoch: 17 | Batch_idx: 190 |  Loss_1: (0.1987) | Acc_1: (92.87%) (22704/24448)\n",
      "Epoch: 17 | Batch_idx: 200 |  Loss_1: (0.1997) | Acc_1: (92.80%) (23876/25728)\n",
      "Epoch: 17 | Batch_idx: 210 |  Loss_1: (0.2016) | Acc_1: (92.77%) (25055/27008)\n",
      "Epoch: 17 | Batch_idx: 220 |  Loss_1: (0.2018) | Acc_1: (92.77%) (26243/28288)\n",
      "Epoch: 17 | Batch_idx: 230 |  Loss_1: (0.2011) | Acc_1: (92.79%) (27437/29568)\n",
      "Epoch: 17 | Batch_idx: 240 |  Loss_1: (0.2021) | Acc_1: (92.77%) (28617/30848)\n",
      "Epoch: 17 | Batch_idx: 250 |  Loss_1: (0.2016) | Acc_1: (92.78%) (29809/32128)\n",
      "Epoch: 17 | Batch_idx: 260 |  Loss_1: (0.2016) | Acc_1: (92.81%) (31006/33408)\n",
      "Epoch: 17 | Batch_idx: 270 |  Loss_1: (0.2022) | Acc_1: (92.76%) (32176/34688)\n",
      "Epoch: 17 | Batch_idx: 280 |  Loss_1: (0.2023) | Acc_1: (92.74%) (33357/35968)\n",
      "Epoch: 17 | Batch_idx: 290 |  Loss_1: (0.2033) | Acc_1: (92.69%) (34524/37248)\n",
      "Epoch: 17 | Batch_idx: 300 |  Loss_1: (0.2048) | Acc_1: (92.66%) (35699/38528)\n",
      "Epoch: 17 | Batch_idx: 310 |  Loss_1: (0.2048) | Acc_1: (92.68%) (36896/39808)\n",
      "Epoch: 17 | Batch_idx: 320 |  Loss_1: (0.2058) | Acc_1: (92.65%) (38068/41088)\n",
      "Epoch: 17 | Batch_idx: 330 |  Loss_1: (0.2064) | Acc_1: (92.63%) (39245/42368)\n",
      "Epoch: 17 | Batch_idx: 340 |  Loss_1: (0.2074) | Acc_1: (92.59%) (40413/43648)\n",
      "Epoch: 17 | Batch_idx: 350 |  Loss_1: (0.2076) | Acc_1: (92.56%) (41585/44928)\n",
      "Epoch: 17 | Batch_idx: 360 |  Loss_1: (0.2078) | Acc_1: (92.57%) (42775/46208)\n",
      "Epoch: 17 | Batch_idx: 370 |  Loss_1: (0.2077) | Acc_1: (92.58%) (43966/47488)\n",
      "Epoch: 17 | Batch_idx: 380 |  Loss_1: (0.2074) | Acc_1: (92.59%) (45155/48768)\n",
      "Epoch: 17 | Batch_idx: 390 |  Loss_1: (0.2081) | Acc_1: (92.57%) (46287/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4020) | Acc: (87.58%) (8758/10000)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.2724) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.1689) | Acc_1: (94.46%) (1330/1408)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.1701) | Acc_1: (94.12%) (2530/2688)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.1708) | Acc_1: (94.10%) (3734/3968)\n",
      "Epoch: 18 | Batch_idx: 40 |  Loss_1: (0.1727) | Acc_1: (93.92%) (4929/5248)\n",
      "Epoch: 18 | Batch_idx: 50 |  Loss_1: (0.1706) | Acc_1: (94.09%) (6142/6528)\n",
      "Epoch: 18 | Batch_idx: 60 |  Loss_1: (0.1709) | Acc_1: (93.98%) (7338/7808)\n",
      "Epoch: 18 | Batch_idx: 70 |  Loss_1: (0.1758) | Acc_1: (93.71%) (8516/9088)\n",
      "Epoch: 18 | Batch_idx: 80 |  Loss_1: (0.1801) | Acc_1: (93.51%) (9695/10368)\n",
      "Epoch: 18 | Batch_idx: 90 |  Loss_1: (0.1806) | Acc_1: (93.48%) (10889/11648)\n",
      "Epoch: 18 | Batch_idx: 100 |  Loss_1: (0.1810) | Acc_1: (93.53%) (12092/12928)\n",
      "Epoch: 18 | Batch_idx: 110 |  Loss_1: (0.1806) | Acc_1: (93.51%) (13286/14208)\n",
      "Epoch: 18 | Batch_idx: 120 |  Loss_1: (0.1825) | Acc_1: (93.45%) (14473/15488)\n",
      "Epoch: 18 | Batch_idx: 130 |  Loss_1: (0.1859) | Acc_1: (93.35%) (15653/16768)\n",
      "Epoch: 18 | Batch_idx: 140 |  Loss_1: (0.1851) | Acc_1: (93.38%) (16853/18048)\n",
      "Epoch: 18 | Batch_idx: 150 |  Loss_1: (0.1862) | Acc_1: (93.32%) (18037/19328)\n",
      "Epoch: 18 | Batch_idx: 160 |  Loss_1: (0.1879) | Acc_1: (93.26%) (19220/20608)\n",
      "Epoch: 18 | Batch_idx: 170 |  Loss_1: (0.1889) | Acc_1: (93.25%) (20411/21888)\n",
      "Epoch: 18 | Batch_idx: 180 |  Loss_1: (0.1891) | Acc_1: (93.27%) (21609/23168)\n",
      "Epoch: 18 | Batch_idx: 190 |  Loss_1: (0.1900) | Acc_1: (93.22%) (22790/24448)\n",
      "Epoch: 18 | Batch_idx: 200 |  Loss_1: (0.1917) | Acc_1: (93.18%) (23973/25728)\n",
      "Epoch: 18 | Batch_idx: 210 |  Loss_1: (0.1921) | Acc_1: (93.17%) (25163/27008)\n",
      "Epoch: 18 | Batch_idx: 220 |  Loss_1: (0.1909) | Acc_1: (93.21%) (26368/28288)\n",
      "Epoch: 18 | Batch_idx: 230 |  Loss_1: (0.1921) | Acc_1: (93.17%) (27548/29568)\n",
      "Epoch: 18 | Batch_idx: 240 |  Loss_1: (0.1927) | Acc_1: (93.16%) (28739/30848)\n",
      "Epoch: 18 | Batch_idx: 250 |  Loss_1: (0.1933) | Acc_1: (93.16%) (29929/32128)\n",
      "Epoch: 18 | Batch_idx: 260 |  Loss_1: (0.1931) | Acc_1: (93.16%) (31124/33408)\n",
      "Epoch: 18 | Batch_idx: 270 |  Loss_1: (0.1930) | Acc_1: (93.16%) (32314/34688)\n",
      "Epoch: 18 | Batch_idx: 280 |  Loss_1: (0.1933) | Acc_1: (93.14%) (33499/35968)\n",
      "Epoch: 18 | Batch_idx: 290 |  Loss_1: (0.1932) | Acc_1: (93.15%) (34696/37248)\n",
      "Epoch: 18 | Batch_idx: 300 |  Loss_1: (0.1929) | Acc_1: (93.17%) (35898/38528)\n",
      "Epoch: 18 | Batch_idx: 310 |  Loss_1: (0.1928) | Acc_1: (93.17%) (37090/39808)\n",
      "Epoch: 18 | Batch_idx: 320 |  Loss_1: (0.1928) | Acc_1: (93.18%) (38286/41088)\n",
      "Epoch: 18 | Batch_idx: 330 |  Loss_1: (0.1934) | Acc_1: (93.17%) (39475/42368)\n",
      "Epoch: 18 | Batch_idx: 340 |  Loss_1: (0.1936) | Acc_1: (93.17%) (40665/43648)\n",
      "Epoch: 18 | Batch_idx: 350 |  Loss_1: (0.1939) | Acc_1: (93.18%) (41864/44928)\n",
      "Epoch: 18 | Batch_idx: 360 |  Loss_1: (0.1940) | Acc_1: (93.18%) (43056/46208)\n",
      "Epoch: 18 | Batch_idx: 370 |  Loss_1: (0.1942) | Acc_1: (93.17%) (44245/47488)\n",
      "Epoch: 18 | Batch_idx: 380 |  Loss_1: (0.1937) | Acc_1: (93.18%) (45443/48768)\n",
      "Epoch: 18 | Batch_idx: 390 |  Loss_1: (0.1933) | Acc_1: (93.20%) (46602/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4173) | Acc: (87.64%) (8764/10000)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.1230) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.1545) | Acc_1: (94.46%) (1330/1408)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.1540) | Acc_1: (94.16%) (2531/2688)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.1558) | Acc_1: (94.25%) (3740/3968)\n",
      "Epoch: 19 | Batch_idx: 40 |  Loss_1: (0.1622) | Acc_1: (94.19%) (4943/5248)\n",
      "Epoch: 19 | Batch_idx: 50 |  Loss_1: (0.1640) | Acc_1: (94.06%) (6140/6528)\n",
      "Epoch: 19 | Batch_idx: 60 |  Loss_1: (0.1654) | Acc_1: (94.04%) (7343/7808)\n",
      "Epoch: 19 | Batch_idx: 70 |  Loss_1: (0.1658) | Acc_1: (93.96%) (8539/9088)\n",
      "Epoch: 19 | Batch_idx: 80 |  Loss_1: (0.1638) | Acc_1: (94.11%) (9757/10368)\n",
      "Epoch: 19 | Batch_idx: 90 |  Loss_1: (0.1659) | Acc_1: (94.05%) (10955/11648)\n",
      "Epoch: 19 | Batch_idx: 100 |  Loss_1: (0.1665) | Acc_1: (94.09%) (12164/12928)\n",
      "Epoch: 19 | Batch_idx: 110 |  Loss_1: (0.1670) | Acc_1: (94.09%) (13368/14208)\n",
      "Epoch: 19 | Batch_idx: 120 |  Loss_1: (0.1675) | Acc_1: (94.03%) (14564/15488)\n",
      "Epoch: 19 | Batch_idx: 130 |  Loss_1: (0.1688) | Acc_1: (93.94%) (15752/16768)\n",
      "Epoch: 19 | Batch_idx: 140 |  Loss_1: (0.1704) | Acc_1: (93.94%) (16954/18048)\n",
      "Epoch: 19 | Batch_idx: 150 |  Loss_1: (0.1704) | Acc_1: (93.97%) (18163/19328)\n",
      "Epoch: 19 | Batch_idx: 160 |  Loss_1: (0.1707) | Acc_1: (94.03%) (19377/20608)\n",
      "Epoch: 19 | Batch_idx: 170 |  Loss_1: (0.1709) | Acc_1: (93.99%) (20572/21888)\n",
      "Epoch: 19 | Batch_idx: 180 |  Loss_1: (0.1703) | Acc_1: (93.98%) (21773/23168)\n",
      "Epoch: 19 | Batch_idx: 190 |  Loss_1: (0.1698) | Acc_1: (93.97%) (22973/24448)\n",
      "Epoch: 19 | Batch_idx: 200 |  Loss_1: (0.1710) | Acc_1: (93.92%) (24165/25728)\n",
      "Epoch: 19 | Batch_idx: 210 |  Loss_1: (0.1705) | Acc_1: (93.96%) (25378/27008)\n",
      "Epoch: 19 | Batch_idx: 220 |  Loss_1: (0.1700) | Acc_1: (93.99%) (26587/28288)\n",
      "Epoch: 19 | Batch_idx: 230 |  Loss_1: (0.1714) | Acc_1: (93.97%) (27784/29568)\n",
      "Epoch: 19 | Batch_idx: 240 |  Loss_1: (0.1709) | Acc_1: (93.97%) (28988/30848)\n",
      "Epoch: 19 | Batch_idx: 250 |  Loss_1: (0.1715) | Acc_1: (93.97%) (30192/32128)\n",
      "Epoch: 19 | Batch_idx: 260 |  Loss_1: (0.1715) | Acc_1: (93.98%) (31398/33408)\n",
      "Epoch: 19 | Batch_idx: 270 |  Loss_1: (0.1721) | Acc_1: (93.95%) (32589/34688)\n",
      "Epoch: 19 | Batch_idx: 280 |  Loss_1: (0.1729) | Acc_1: (93.92%) (33781/35968)\n",
      "Epoch: 19 | Batch_idx: 290 |  Loss_1: (0.1747) | Acc_1: (93.85%) (34956/37248)\n",
      "Epoch: 19 | Batch_idx: 300 |  Loss_1: (0.1749) | Acc_1: (93.83%) (36152/38528)\n",
      "Epoch: 19 | Batch_idx: 310 |  Loss_1: (0.1761) | Acc_1: (93.79%) (37335/39808)\n",
      "Epoch: 19 | Batch_idx: 320 |  Loss_1: (0.1766) | Acc_1: (93.76%) (38524/41088)\n",
      "Epoch: 19 | Batch_idx: 330 |  Loss_1: (0.1766) | Acc_1: (93.78%) (39731/42368)\n",
      "Epoch: 19 | Batch_idx: 340 |  Loss_1: (0.1770) | Acc_1: (93.75%) (40919/43648)\n",
      "Epoch: 19 | Batch_idx: 350 |  Loss_1: (0.1766) | Acc_1: (93.76%) (42126/44928)\n",
      "Epoch: 19 | Batch_idx: 360 |  Loss_1: (0.1762) | Acc_1: (93.78%) (43333/46208)\n",
      "Epoch: 19 | Batch_idx: 370 |  Loss_1: (0.1770) | Acc_1: (93.75%) (44520/47488)\n",
      "Epoch: 19 | Batch_idx: 380 |  Loss_1: (0.1764) | Acc_1: (93.77%) (45728/48768)\n",
      "Epoch: 19 | Batch_idx: 390 |  Loss_1: (0.1768) | Acc_1: (93.74%) (46870/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4789) | Acc: (86.14%) (8614/10000)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.1166) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.1701) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.1627) | Acc_1: (94.38%) (2537/2688)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.1669) | Acc_1: (94.10%) (3734/3968)\n",
      "Epoch: 20 | Batch_idx: 40 |  Loss_1: (0.1590) | Acc_1: (94.55%) (4962/5248)\n",
      "Epoch: 20 | Batch_idx: 50 |  Loss_1: (0.1617) | Acc_1: (94.41%) (6163/6528)\n",
      "Epoch: 20 | Batch_idx: 60 |  Loss_1: (0.1634) | Acc_1: (94.28%) (7361/7808)\n",
      "Epoch: 20 | Batch_idx: 70 |  Loss_1: (0.1616) | Acc_1: (94.36%) (8575/9088)\n",
      "Epoch: 20 | Batch_idx: 80 |  Loss_1: (0.1625) | Acc_1: (94.21%) (9768/10368)\n",
      "Epoch: 20 | Batch_idx: 90 |  Loss_1: (0.1657) | Acc_1: (94.09%) (10960/11648)\n",
      "Epoch: 20 | Batch_idx: 100 |  Loss_1: (0.1647) | Acc_1: (94.13%) (12169/12928)\n",
      "Epoch: 20 | Batch_idx: 110 |  Loss_1: (0.1628) | Acc_1: (94.18%) (13381/14208)\n",
      "Epoch: 20 | Batch_idx: 120 |  Loss_1: (0.1606) | Acc_1: (94.25%) (14597/15488)\n",
      "Epoch: 20 | Batch_idx: 130 |  Loss_1: (0.1592) | Acc_1: (94.28%) (15809/16768)\n",
      "Epoch: 20 | Batch_idx: 140 |  Loss_1: (0.1591) | Acc_1: (94.33%) (17025/18048)\n",
      "Epoch: 20 | Batch_idx: 150 |  Loss_1: (0.1600) | Acc_1: (94.32%) (18231/19328)\n",
      "Epoch: 20 | Batch_idx: 160 |  Loss_1: (0.1596) | Acc_1: (94.38%) (19449/20608)\n",
      "Epoch: 20 | Batch_idx: 170 |  Loss_1: (0.1606) | Acc_1: (94.37%) (20655/21888)\n",
      "Epoch: 20 | Batch_idx: 180 |  Loss_1: (0.1609) | Acc_1: (94.35%) (21859/23168)\n",
      "Epoch: 20 | Batch_idx: 190 |  Loss_1: (0.1607) | Acc_1: (94.33%) (23063/24448)\n",
      "Epoch: 20 | Batch_idx: 200 |  Loss_1: (0.1588) | Acc_1: (94.41%) (24290/25728)\n",
      "Epoch: 20 | Batch_idx: 210 |  Loss_1: (0.1588) | Acc_1: (94.42%) (25500/27008)\n",
      "Epoch: 20 | Batch_idx: 220 |  Loss_1: (0.1580) | Acc_1: (94.47%) (26724/28288)\n",
      "Epoch: 20 | Batch_idx: 230 |  Loss_1: (0.1583) | Acc_1: (94.46%) (27930/29568)\n",
      "Epoch: 20 | Batch_idx: 240 |  Loss_1: (0.1593) | Acc_1: (94.42%) (29126/30848)\n",
      "Epoch: 20 | Batch_idx: 250 |  Loss_1: (0.1605) | Acc_1: (94.38%) (30322/32128)\n",
      "Epoch: 20 | Batch_idx: 260 |  Loss_1: (0.1604) | Acc_1: (94.38%) (31530/33408)\n",
      "Epoch: 20 | Batch_idx: 270 |  Loss_1: (0.1613) | Acc_1: (94.35%) (32728/34688)\n",
      "Epoch: 20 | Batch_idx: 280 |  Loss_1: (0.1610) | Acc_1: (94.34%) (33931/35968)\n",
      "Epoch: 20 | Batch_idx: 290 |  Loss_1: (0.1620) | Acc_1: (94.31%) (35129/37248)\n",
      "Epoch: 20 | Batch_idx: 300 |  Loss_1: (0.1636) | Acc_1: (94.26%) (36316/38528)\n",
      "Epoch: 20 | Batch_idx: 310 |  Loss_1: (0.1639) | Acc_1: (94.24%) (37515/39808)\n",
      "Epoch: 20 | Batch_idx: 320 |  Loss_1: (0.1641) | Acc_1: (94.24%) (38721/41088)\n",
      "Epoch: 20 | Batch_idx: 330 |  Loss_1: (0.1640) | Acc_1: (94.25%) (39932/42368)\n",
      "Epoch: 20 | Batch_idx: 340 |  Loss_1: (0.1643) | Acc_1: (94.24%) (41133/43648)\n",
      "Epoch: 20 | Batch_idx: 350 |  Loss_1: (0.1643) | Acc_1: (94.22%) (42333/44928)\n",
      "Epoch: 20 | Batch_idx: 360 |  Loss_1: (0.1643) | Acc_1: (94.22%) (43539/46208)\n",
      "Epoch: 20 | Batch_idx: 370 |  Loss_1: (0.1654) | Acc_1: (94.19%) (44727/47488)\n",
      "Epoch: 20 | Batch_idx: 380 |  Loss_1: (0.1655) | Acc_1: (94.17%) (45926/48768)\n",
      "Epoch: 20 | Batch_idx: 390 |  Loss_1: (0.1655) | Acc_1: (94.16%) (47082/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4602) | Acc: (87.69%) (8769/10000)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.1578) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.1503) | Acc_1: (93.82%) (1321/1408)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.1466) | Acc_1: (94.64%) (2544/2688)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.1488) | Acc_1: (94.58%) (3753/3968)\n",
      "Epoch: 21 | Batch_idx: 40 |  Loss_1: (0.1471) | Acc_1: (94.82%) (4976/5248)\n",
      "Epoch: 21 | Batch_idx: 50 |  Loss_1: (0.1450) | Acc_1: (94.79%) (6188/6528)\n",
      "Epoch: 21 | Batch_idx: 60 |  Loss_1: (0.1446) | Acc_1: (94.75%) (7398/7808)\n",
      "Epoch: 21 | Batch_idx: 70 |  Loss_1: (0.1482) | Acc_1: (94.55%) (8593/9088)\n",
      "Epoch: 21 | Batch_idx: 80 |  Loss_1: (0.1492) | Acc_1: (94.64%) (9812/10368)\n",
      "Epoch: 21 | Batch_idx: 90 |  Loss_1: (0.1513) | Acc_1: (94.60%) (11019/11648)\n",
      "Epoch: 21 | Batch_idx: 100 |  Loss_1: (0.1519) | Acc_1: (94.62%) (12232/12928)\n",
      "Epoch: 21 | Batch_idx: 110 |  Loss_1: (0.1514) | Acc_1: (94.59%) (13440/14208)\n",
      "Epoch: 21 | Batch_idx: 120 |  Loss_1: (0.1483) | Acc_1: (94.69%) (14666/15488)\n",
      "Epoch: 21 | Batch_idx: 130 |  Loss_1: (0.1493) | Acc_1: (94.69%) (15877/16768)\n",
      "Epoch: 21 | Batch_idx: 140 |  Loss_1: (0.1505) | Acc_1: (94.56%) (17067/18048)\n",
      "Epoch: 21 | Batch_idx: 150 |  Loss_1: (0.1511) | Acc_1: (94.54%) (18273/19328)\n",
      "Epoch: 21 | Batch_idx: 160 |  Loss_1: (0.1523) | Acc_1: (94.51%) (19476/20608)\n",
      "Epoch: 21 | Batch_idx: 170 |  Loss_1: (0.1543) | Acc_1: (94.43%) (20669/21888)\n",
      "Epoch: 21 | Batch_idx: 180 |  Loss_1: (0.1527) | Acc_1: (94.52%) (21898/23168)\n",
      "Epoch: 21 | Batch_idx: 190 |  Loss_1: (0.1533) | Acc_1: (94.51%) (23107/24448)\n",
      "Epoch: 21 | Batch_idx: 200 |  Loss_1: (0.1543) | Acc_1: (94.47%) (24304/25728)\n",
      "Epoch: 21 | Batch_idx: 210 |  Loss_1: (0.1552) | Acc_1: (94.44%) (25506/27008)\n",
      "Epoch: 21 | Batch_idx: 220 |  Loss_1: (0.1551) | Acc_1: (94.46%) (26722/28288)\n",
      "Epoch: 21 | Batch_idx: 230 |  Loss_1: (0.1546) | Acc_1: (94.45%) (27927/29568)\n",
      "Epoch: 21 | Batch_idx: 240 |  Loss_1: (0.1547) | Acc_1: (94.43%) (29130/30848)\n",
      "Epoch: 21 | Batch_idx: 250 |  Loss_1: (0.1537) | Acc_1: (94.48%) (30353/32128)\n",
      "Epoch: 21 | Batch_idx: 260 |  Loss_1: (0.1540) | Acc_1: (94.46%) (31556/33408)\n",
      "Epoch: 21 | Batch_idx: 270 |  Loss_1: (0.1536) | Acc_1: (94.45%) (32764/34688)\n",
      "Epoch: 21 | Batch_idx: 280 |  Loss_1: (0.1544) | Acc_1: (94.45%) (33970/35968)\n",
      "Epoch: 21 | Batch_idx: 290 |  Loss_1: (0.1543) | Acc_1: (94.45%) (35179/37248)\n",
      "Epoch: 21 | Batch_idx: 300 |  Loss_1: (0.1550) | Acc_1: (94.41%) (36373/38528)\n",
      "Epoch: 21 | Batch_idx: 310 |  Loss_1: (0.1558) | Acc_1: (94.39%) (37575/39808)\n",
      "Epoch: 21 | Batch_idx: 320 |  Loss_1: (0.1559) | Acc_1: (94.39%) (38783/41088)\n",
      "Epoch: 21 | Batch_idx: 330 |  Loss_1: (0.1566) | Acc_1: (94.37%) (39982/42368)\n",
      "Epoch: 21 | Batch_idx: 340 |  Loss_1: (0.1573) | Acc_1: (94.34%) (41178/43648)\n",
      "Epoch: 21 | Batch_idx: 350 |  Loss_1: (0.1578) | Acc_1: (94.32%) (42376/44928)\n",
      "Epoch: 21 | Batch_idx: 360 |  Loss_1: (0.1578) | Acc_1: (94.32%) (43585/46208)\n",
      "Epoch: 21 | Batch_idx: 370 |  Loss_1: (0.1574) | Acc_1: (94.34%) (44800/47488)\n",
      "Epoch: 21 | Batch_idx: 380 |  Loss_1: (0.1582) | Acc_1: (94.31%) (45992/48768)\n",
      "Epoch: 21 | Batch_idx: 390 |  Loss_1: (0.1581) | Acc_1: (94.31%) (47155/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4335) | Acc: (88.35%) (8835/10000)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.1746) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.1496) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.1491) | Acc_1: (94.72%) (2546/2688)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.1498) | Acc_1: (94.63%) (3755/3968)\n",
      "Epoch: 22 | Batch_idx: 40 |  Loss_1: (0.1427) | Acc_1: (94.91%) (4981/5248)\n",
      "Epoch: 22 | Batch_idx: 50 |  Loss_1: (0.1465) | Acc_1: (94.68%) (6181/6528)\n",
      "Epoch: 22 | Batch_idx: 60 |  Loss_1: (0.1415) | Acc_1: (94.85%) (7406/7808)\n",
      "Epoch: 22 | Batch_idx: 70 |  Loss_1: (0.1442) | Acc_1: (94.77%) (8613/9088)\n",
      "Epoch: 22 | Batch_idx: 80 |  Loss_1: (0.1433) | Acc_1: (94.79%) (9828/10368)\n",
      "Epoch: 22 | Batch_idx: 90 |  Loss_1: (0.1445) | Acc_1: (94.69%) (11030/11648)\n",
      "Epoch: 22 | Batch_idx: 100 |  Loss_1: (0.1462) | Acc_1: (94.65%) (12236/12928)\n",
      "Epoch: 22 | Batch_idx: 110 |  Loss_1: (0.1477) | Acc_1: (94.62%) (13443/14208)\n",
      "Epoch: 22 | Batch_idx: 120 |  Loss_1: (0.1487) | Acc_1: (94.60%) (14652/15488)\n",
      "Epoch: 22 | Batch_idx: 130 |  Loss_1: (0.1473) | Acc_1: (94.68%) (15876/16768)\n",
      "Epoch: 22 | Batch_idx: 140 |  Loss_1: (0.1493) | Acc_1: (94.66%) (17084/18048)\n",
      "Epoch: 22 | Batch_idx: 150 |  Loss_1: (0.1498) | Acc_1: (94.68%) (18300/19328)\n",
      "Epoch: 22 | Batch_idx: 160 |  Loss_1: (0.1481) | Acc_1: (94.74%) (19525/20608)\n",
      "Epoch: 22 | Batch_idx: 170 |  Loss_1: (0.1485) | Acc_1: (94.70%) (20729/21888)\n",
      "Epoch: 22 | Batch_idx: 180 |  Loss_1: (0.1483) | Acc_1: (94.70%) (21941/23168)\n",
      "Epoch: 22 | Batch_idx: 190 |  Loss_1: (0.1484) | Acc_1: (94.69%) (23151/24448)\n",
      "Epoch: 22 | Batch_idx: 200 |  Loss_1: (0.1484) | Acc_1: (94.69%) (24361/25728)\n",
      "Epoch: 22 | Batch_idx: 210 |  Loss_1: (0.1475) | Acc_1: (94.70%) (25577/27008)\n",
      "Epoch: 22 | Batch_idx: 220 |  Loss_1: (0.1485) | Acc_1: (94.69%) (26787/28288)\n",
      "Epoch: 22 | Batch_idx: 230 |  Loss_1: (0.1475) | Acc_1: (94.73%) (28011/29568)\n",
      "Epoch: 22 | Batch_idx: 240 |  Loss_1: (0.1474) | Acc_1: (94.73%) (29223/30848)\n",
      "Epoch: 22 | Batch_idx: 250 |  Loss_1: (0.1479) | Acc_1: (94.72%) (30433/32128)\n",
      "Epoch: 22 | Batch_idx: 260 |  Loss_1: (0.1482) | Acc_1: (94.72%) (31643/33408)\n",
      "Epoch: 22 | Batch_idx: 270 |  Loss_1: (0.1496) | Acc_1: (94.66%) (32837/34688)\n",
      "Epoch: 22 | Batch_idx: 280 |  Loss_1: (0.1493) | Acc_1: (94.68%) (34053/35968)\n",
      "Epoch: 22 | Batch_idx: 290 |  Loss_1: (0.1499) | Acc_1: (94.67%) (35263/37248)\n",
      "Epoch: 22 | Batch_idx: 300 |  Loss_1: (0.1495) | Acc_1: (94.68%) (36478/38528)\n",
      "Epoch: 22 | Batch_idx: 310 |  Loss_1: (0.1481) | Acc_1: (94.72%) (37708/39808)\n",
      "Epoch: 22 | Batch_idx: 320 |  Loss_1: (0.1480) | Acc_1: (94.73%) (38921/41088)\n",
      "Epoch: 22 | Batch_idx: 330 |  Loss_1: (0.1480) | Acc_1: (94.71%) (40127/42368)\n",
      "Epoch: 22 | Batch_idx: 340 |  Loss_1: (0.1489) | Acc_1: (94.68%) (41328/43648)\n",
      "Epoch: 22 | Batch_idx: 350 |  Loss_1: (0.1486) | Acc_1: (94.69%) (42542/44928)\n",
      "Epoch: 22 | Batch_idx: 360 |  Loss_1: (0.1483) | Acc_1: (94.68%) (43749/46208)\n",
      "Epoch: 22 | Batch_idx: 370 |  Loss_1: (0.1490) | Acc_1: (94.64%) (44945/47488)\n",
      "Epoch: 22 | Batch_idx: 380 |  Loss_1: (0.1497) | Acc_1: (94.63%) (46149/48768)\n",
      "Epoch: 22 | Batch_idx: 390 |  Loss_1: (0.1496) | Acc_1: (94.62%) (47310/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4877) | Acc: (87.03%) (8703/10000)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.1346) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.1529) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.1517) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.1453) | Acc_1: (95.01%) (3770/3968)\n",
      "Epoch: 23 | Batch_idx: 40 |  Loss_1: (0.1397) | Acc_1: (95.16%) (4994/5248)\n",
      "Epoch: 23 | Batch_idx: 50 |  Loss_1: (0.1391) | Acc_1: (95.05%) (6205/6528)\n",
      "Epoch: 23 | Batch_idx: 60 |  Loss_1: (0.1349) | Acc_1: (95.17%) (7431/7808)\n",
      "Epoch: 23 | Batch_idx: 70 |  Loss_1: (0.1334) | Acc_1: (95.24%) (8655/9088)\n",
      "Epoch: 23 | Batch_idx: 80 |  Loss_1: (0.1335) | Acc_1: (95.19%) (9869/10368)\n",
      "Epoch: 23 | Batch_idx: 90 |  Loss_1: (0.1357) | Acc_1: (95.05%) (11072/11648)\n",
      "Epoch: 23 | Batch_idx: 100 |  Loss_1: (0.1379) | Acc_1: (95.03%) (12286/12928)\n",
      "Epoch: 23 | Batch_idx: 110 |  Loss_1: (0.1361) | Acc_1: (95.08%) (13509/14208)\n",
      "Epoch: 23 | Batch_idx: 120 |  Loss_1: (0.1345) | Acc_1: (95.14%) (14735/15488)\n",
      "Epoch: 23 | Batch_idx: 130 |  Loss_1: (0.1360) | Acc_1: (95.09%) (15944/16768)\n",
      "Epoch: 23 | Batch_idx: 140 |  Loss_1: (0.1368) | Acc_1: (95.05%) (17154/18048)\n",
      "Epoch: 23 | Batch_idx: 150 |  Loss_1: (0.1358) | Acc_1: (95.08%) (18377/19328)\n",
      "Epoch: 23 | Batch_idx: 160 |  Loss_1: (0.1352) | Acc_1: (95.10%) (19599/20608)\n",
      "Epoch: 23 | Batch_idx: 170 |  Loss_1: (0.1348) | Acc_1: (95.13%) (20821/21888)\n",
      "Epoch: 23 | Batch_idx: 180 |  Loss_1: (0.1347) | Acc_1: (95.14%) (22042/23168)\n",
      "Epoch: 23 | Batch_idx: 190 |  Loss_1: (0.1361) | Acc_1: (95.09%) (23248/24448)\n",
      "Epoch: 23 | Batch_idx: 200 |  Loss_1: (0.1376) | Acc_1: (95.05%) (24455/25728)\n",
      "Epoch: 23 | Batch_idx: 210 |  Loss_1: (0.1387) | Acc_1: (95.02%) (25664/27008)\n",
      "Epoch: 23 | Batch_idx: 220 |  Loss_1: (0.1392) | Acc_1: (95.03%) (26882/28288)\n",
      "Epoch: 23 | Batch_idx: 230 |  Loss_1: (0.1398) | Acc_1: (95.03%) (28097/29568)\n",
      "Epoch: 23 | Batch_idx: 240 |  Loss_1: (0.1390) | Acc_1: (95.04%) (29318/30848)\n",
      "Epoch: 23 | Batch_idx: 250 |  Loss_1: (0.1391) | Acc_1: (95.06%) (30540/32128)\n",
      "Epoch: 23 | Batch_idx: 260 |  Loss_1: (0.1399) | Acc_1: (95.03%) (31747/33408)\n",
      "Epoch: 23 | Batch_idx: 270 |  Loss_1: (0.1392) | Acc_1: (95.04%) (32967/34688)\n",
      "Epoch: 23 | Batch_idx: 280 |  Loss_1: (0.1396) | Acc_1: (95.06%) (34190/35968)\n",
      "Epoch: 23 | Batch_idx: 290 |  Loss_1: (0.1401) | Acc_1: (95.04%) (35400/37248)\n",
      "Epoch: 23 | Batch_idx: 300 |  Loss_1: (0.1401) | Acc_1: (95.04%) (36618/38528)\n",
      "Epoch: 23 | Batch_idx: 310 |  Loss_1: (0.1404) | Acc_1: (95.06%) (37840/39808)\n",
      "Epoch: 23 | Batch_idx: 320 |  Loss_1: (0.1396) | Acc_1: (95.07%) (39062/41088)\n",
      "Epoch: 23 | Batch_idx: 330 |  Loss_1: (0.1396) | Acc_1: (95.07%) (40279/42368)\n",
      "Epoch: 23 | Batch_idx: 340 |  Loss_1: (0.1392) | Acc_1: (95.10%) (41508/43648)\n",
      "Epoch: 23 | Batch_idx: 350 |  Loss_1: (0.1393) | Acc_1: (95.10%) (42727/44928)\n",
      "Epoch: 23 | Batch_idx: 360 |  Loss_1: (0.1395) | Acc_1: (95.09%) (43940/46208)\n",
      "Epoch: 23 | Batch_idx: 370 |  Loss_1: (0.1391) | Acc_1: (95.11%) (45167/47488)\n",
      "Epoch: 23 | Batch_idx: 380 |  Loss_1: (0.1396) | Acc_1: (95.11%) (46385/48768)\n",
      "Epoch: 23 | Batch_idx: 390 |  Loss_1: (0.1404) | Acc_1: (95.08%) (47541/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5426) | Acc: (85.92%) (8592/10000)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.2478) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.1425) | Acc_1: (94.53%) (1331/1408)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.1306) | Acc_1: (95.35%) (2563/2688)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.1305) | Acc_1: (95.36%) (3784/3968)\n",
      "Epoch: 24 | Batch_idx: 40 |  Loss_1: (0.1256) | Acc_1: (95.52%) (5013/5248)\n",
      "Epoch: 24 | Batch_idx: 50 |  Loss_1: (0.1233) | Acc_1: (95.66%) (6245/6528)\n",
      "Epoch: 24 | Batch_idx: 60 |  Loss_1: (0.1177) | Acc_1: (95.89%) (7487/7808)\n",
      "Epoch: 24 | Batch_idx: 70 |  Loss_1: (0.1155) | Acc_1: (95.92%) (8717/9088)\n",
      "Epoch: 24 | Batch_idx: 80 |  Loss_1: (0.1159) | Acc_1: (95.94%) (9947/10368)\n",
      "Epoch: 24 | Batch_idx: 90 |  Loss_1: (0.1180) | Acc_1: (95.86%) (11166/11648)\n",
      "Epoch: 24 | Batch_idx: 100 |  Loss_1: (0.1198) | Acc_1: (95.73%) (12376/12928)\n",
      "Epoch: 24 | Batch_idx: 110 |  Loss_1: (0.1188) | Acc_1: (95.75%) (13604/14208)\n",
      "Epoch: 24 | Batch_idx: 120 |  Loss_1: (0.1224) | Acc_1: (95.63%) (14811/15488)\n",
      "Epoch: 24 | Batch_idx: 130 |  Loss_1: (0.1226) | Acc_1: (95.61%) (16032/16768)\n",
      "Epoch: 24 | Batch_idx: 140 |  Loss_1: (0.1235) | Acc_1: (95.57%) (17248/18048)\n",
      "Epoch: 24 | Batch_idx: 150 |  Loss_1: (0.1260) | Acc_1: (95.48%) (18454/19328)\n",
      "Epoch: 24 | Batch_idx: 160 |  Loss_1: (0.1257) | Acc_1: (95.49%) (19679/20608)\n",
      "Epoch: 24 | Batch_idx: 170 |  Loss_1: (0.1248) | Acc_1: (95.54%) (20912/21888)\n",
      "Epoch: 24 | Batch_idx: 180 |  Loss_1: (0.1252) | Acc_1: (95.49%) (22122/23168)\n",
      "Epoch: 24 | Batch_idx: 190 |  Loss_1: (0.1255) | Acc_1: (95.49%) (23345/24448)\n",
      "Epoch: 24 | Batch_idx: 200 |  Loss_1: (0.1249) | Acc_1: (95.54%) (24580/25728)\n",
      "Epoch: 24 | Batch_idx: 210 |  Loss_1: (0.1241) | Acc_1: (95.58%) (25814/27008)\n",
      "Epoch: 24 | Batch_idx: 220 |  Loss_1: (0.1255) | Acc_1: (95.55%) (27030/28288)\n",
      "Epoch: 24 | Batch_idx: 230 |  Loss_1: (0.1255) | Acc_1: (95.55%) (28252/29568)\n",
      "Epoch: 24 | Batch_idx: 240 |  Loss_1: (0.1258) | Acc_1: (95.55%) (29474/30848)\n",
      "Epoch: 24 | Batch_idx: 250 |  Loss_1: (0.1252) | Acc_1: (95.58%) (30709/32128)\n",
      "Epoch: 24 | Batch_idx: 260 |  Loss_1: (0.1251) | Acc_1: (95.59%) (31935/33408)\n",
      "Epoch: 24 | Batch_idx: 270 |  Loss_1: (0.1253) | Acc_1: (95.56%) (33147/34688)\n",
      "Epoch: 24 | Batch_idx: 280 |  Loss_1: (0.1260) | Acc_1: (95.53%) (34360/35968)\n",
      "Epoch: 24 | Batch_idx: 290 |  Loss_1: (0.1271) | Acc_1: (95.48%) (35566/37248)\n",
      "Epoch: 24 | Batch_idx: 300 |  Loss_1: (0.1279) | Acc_1: (95.47%) (36782/38528)\n",
      "Epoch: 24 | Batch_idx: 310 |  Loss_1: (0.1280) | Acc_1: (95.45%) (37997/39808)\n",
      "Epoch: 24 | Batch_idx: 320 |  Loss_1: (0.1285) | Acc_1: (95.45%) (39219/41088)\n",
      "Epoch: 24 | Batch_idx: 330 |  Loss_1: (0.1298) | Acc_1: (95.42%) (40428/42368)\n",
      "Epoch: 24 | Batch_idx: 340 |  Loss_1: (0.1307) | Acc_1: (95.39%) (41635/43648)\n",
      "Epoch: 24 | Batch_idx: 350 |  Loss_1: (0.1309) | Acc_1: (95.37%) (42850/44928)\n",
      "Epoch: 24 | Batch_idx: 360 |  Loss_1: (0.1304) | Acc_1: (95.39%) (44079/46208)\n",
      "Epoch: 24 | Batch_idx: 370 |  Loss_1: (0.1310) | Acc_1: (95.38%) (45294/47488)\n",
      "Epoch: 24 | Batch_idx: 380 |  Loss_1: (0.1314) | Acc_1: (95.38%) (46513/48768)\n",
      "Epoch: 24 | Batch_idx: 390 |  Loss_1: (0.1314) | Acc_1: (95.36%) (47682/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4482) | Acc: (88.09%) (8809/10000)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.0299) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.1074) | Acc_1: (96.31%) (1356/1408)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.1071) | Acc_1: (96.32%) (2589/2688)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.1069) | Acc_1: (96.35%) (3823/3968)\n",
      "Epoch: 25 | Batch_idx: 40 |  Loss_1: (0.1076) | Acc_1: (96.40%) (5059/5248)\n",
      "Epoch: 25 | Batch_idx: 50 |  Loss_1: (0.1072) | Acc_1: (96.42%) (6294/6528)\n",
      "Epoch: 25 | Batch_idx: 60 |  Loss_1: (0.1107) | Acc_1: (96.21%) (7512/7808)\n",
      "Epoch: 25 | Batch_idx: 70 |  Loss_1: (0.1089) | Acc_1: (96.24%) (8746/9088)\n",
      "Epoch: 25 | Batch_idx: 80 |  Loss_1: (0.1095) | Acc_1: (96.16%) (9970/10368)\n",
      "Epoch: 25 | Batch_idx: 90 |  Loss_1: (0.1103) | Acc_1: (96.09%) (11193/11648)\n",
      "Epoch: 25 | Batch_idx: 100 |  Loss_1: (0.1098) | Acc_1: (96.14%) (12429/12928)\n",
      "Epoch: 25 | Batch_idx: 110 |  Loss_1: (0.1097) | Acc_1: (96.11%) (13656/14208)\n",
      "Epoch: 25 | Batch_idx: 120 |  Loss_1: (0.1111) | Acc_1: (96.06%) (14877/15488)\n",
      "Epoch: 25 | Batch_idx: 130 |  Loss_1: (0.1117) | Acc_1: (96.03%) (16102/16768)\n",
      "Epoch: 25 | Batch_idx: 140 |  Loss_1: (0.1142) | Acc_1: (95.96%) (17318/18048)\n",
      "Epoch: 25 | Batch_idx: 150 |  Loss_1: (0.1168) | Acc_1: (95.87%) (18529/19328)\n",
      "Epoch: 25 | Batch_idx: 160 |  Loss_1: (0.1166) | Acc_1: (95.88%) (19758/20608)\n",
      "Epoch: 25 | Batch_idx: 170 |  Loss_1: (0.1177) | Acc_1: (95.84%) (20978/21888)\n",
      "Epoch: 25 | Batch_idx: 180 |  Loss_1: (0.1183) | Acc_1: (95.84%) (22205/23168)\n",
      "Epoch: 25 | Batch_idx: 190 |  Loss_1: (0.1204) | Acc_1: (95.74%) (23406/24448)\n",
      "Epoch: 25 | Batch_idx: 200 |  Loss_1: (0.1207) | Acc_1: (95.72%) (24628/25728)\n",
      "Epoch: 25 | Batch_idx: 210 |  Loss_1: (0.1213) | Acc_1: (95.72%) (25852/27008)\n",
      "Epoch: 25 | Batch_idx: 220 |  Loss_1: (0.1220) | Acc_1: (95.69%) (27069/28288)\n",
      "Epoch: 25 | Batch_idx: 230 |  Loss_1: (0.1223) | Acc_1: (95.65%) (28281/29568)\n",
      "Epoch: 25 | Batch_idx: 240 |  Loss_1: (0.1217) | Acc_1: (95.69%) (29519/30848)\n",
      "Epoch: 25 | Batch_idx: 250 |  Loss_1: (0.1215) | Acc_1: (95.70%) (30747/32128)\n",
      "Epoch: 25 | Batch_idx: 260 |  Loss_1: (0.1210) | Acc_1: (95.69%) (31969/33408)\n",
      "Epoch: 25 | Batch_idx: 270 |  Loss_1: (0.1210) | Acc_1: (95.72%) (33203/34688)\n",
      "Epoch: 25 | Batch_idx: 280 |  Loss_1: (0.1214) | Acc_1: (95.69%) (34418/35968)\n",
      "Epoch: 25 | Batch_idx: 290 |  Loss_1: (0.1215) | Acc_1: (95.69%) (35641/37248)\n",
      "Epoch: 25 | Batch_idx: 300 |  Loss_1: (0.1220) | Acc_1: (95.67%) (36859/38528)\n",
      "Epoch: 25 | Batch_idx: 310 |  Loss_1: (0.1223) | Acc_1: (95.66%) (38081/39808)\n",
      "Epoch: 25 | Batch_idx: 320 |  Loss_1: (0.1229) | Acc_1: (95.64%) (39297/41088)\n",
      "Epoch: 25 | Batch_idx: 330 |  Loss_1: (0.1232) | Acc_1: (95.63%) (40518/42368)\n",
      "Epoch: 25 | Batch_idx: 340 |  Loss_1: (0.1227) | Acc_1: (95.64%) (41746/43648)\n",
      "Epoch: 25 | Batch_idx: 350 |  Loss_1: (0.1228) | Acc_1: (95.63%) (42963/44928)\n",
      "Epoch: 25 | Batch_idx: 360 |  Loss_1: (0.1224) | Acc_1: (95.62%) (44186/46208)\n",
      "Epoch: 25 | Batch_idx: 370 |  Loss_1: (0.1224) | Acc_1: (95.63%) (45411/47488)\n",
      "Epoch: 25 | Batch_idx: 380 |  Loss_1: (0.1220) | Acc_1: (95.65%) (46645/48768)\n",
      "Epoch: 25 | Batch_idx: 390 |  Loss_1: (0.1230) | Acc_1: (95.63%) (47815/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4765) | Acc: (87.72%) (8772/10000)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.1310) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.1112) | Acc_1: (96.31%) (1356/1408)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.1181) | Acc_1: (96.32%) (2589/2688)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.1202) | Acc_1: (96.12%) (3814/3968)\n",
      "Epoch: 26 | Batch_idx: 40 |  Loss_1: (0.1148) | Acc_1: (96.28%) (5053/5248)\n",
      "Epoch: 26 | Batch_idx: 50 |  Loss_1: (0.1161) | Acc_1: (96.19%) (6279/6528)\n",
      "Epoch: 26 | Batch_idx: 60 |  Loss_1: (0.1139) | Acc_1: (96.34%) (7522/7808)\n",
      "Epoch: 26 | Batch_idx: 70 |  Loss_1: (0.1122) | Acc_1: (96.41%) (8762/9088)\n",
      "Epoch: 26 | Batch_idx: 80 |  Loss_1: (0.1109) | Acc_1: (96.44%) (9999/10368)\n",
      "Epoch: 26 | Batch_idx: 90 |  Loss_1: (0.1143) | Acc_1: (96.21%) (11206/11648)\n",
      "Epoch: 26 | Batch_idx: 100 |  Loss_1: (0.1146) | Acc_1: (96.16%) (12431/12928)\n",
      "Epoch: 26 | Batch_idx: 110 |  Loss_1: (0.1148) | Acc_1: (96.14%) (13660/14208)\n",
      "Epoch: 26 | Batch_idx: 120 |  Loss_1: (0.1158) | Acc_1: (96.03%) (14873/15488)\n",
      "Epoch: 26 | Batch_idx: 130 |  Loss_1: (0.1164) | Acc_1: (96.00%) (16097/16768)\n",
      "Epoch: 26 | Batch_idx: 140 |  Loss_1: (0.1158) | Acc_1: (96.02%) (17329/18048)\n",
      "Epoch: 26 | Batch_idx: 150 |  Loss_1: (0.1160) | Acc_1: (95.99%) (18552/19328)\n",
      "Epoch: 26 | Batch_idx: 160 |  Loss_1: (0.1157) | Acc_1: (95.99%) (19782/20608)\n",
      "Epoch: 26 | Batch_idx: 170 |  Loss_1: (0.1141) | Acc_1: (96.08%) (21030/21888)\n",
      "Epoch: 26 | Batch_idx: 180 |  Loss_1: (0.1131) | Acc_1: (96.13%) (22271/23168)\n",
      "Epoch: 26 | Batch_idx: 190 |  Loss_1: (0.1124) | Acc_1: (96.16%) (23510/24448)\n",
      "Epoch: 26 | Batch_idx: 200 |  Loss_1: (0.1131) | Acc_1: (96.11%) (24728/25728)\n",
      "Epoch: 26 | Batch_idx: 210 |  Loss_1: (0.1138) | Acc_1: (96.10%) (25954/27008)\n",
      "Epoch: 26 | Batch_idx: 220 |  Loss_1: (0.1139) | Acc_1: (96.09%) (27181/28288)\n",
      "Epoch: 26 | Batch_idx: 230 |  Loss_1: (0.1137) | Acc_1: (96.08%) (28408/29568)\n",
      "Epoch: 26 | Batch_idx: 240 |  Loss_1: (0.1146) | Acc_1: (96.07%) (29635/30848)\n",
      "Epoch: 26 | Batch_idx: 250 |  Loss_1: (0.1145) | Acc_1: (96.06%) (30861/32128)\n",
      "Epoch: 26 | Batch_idx: 260 |  Loss_1: (0.1148) | Acc_1: (96.06%) (32093/33408)\n",
      "Epoch: 26 | Batch_idx: 270 |  Loss_1: (0.1154) | Acc_1: (96.04%) (33315/34688)\n",
      "Epoch: 26 | Batch_idx: 280 |  Loss_1: (0.1164) | Acc_1: (96.03%) (34539/35968)\n",
      "Epoch: 26 | Batch_idx: 290 |  Loss_1: (0.1168) | Acc_1: (96.01%) (35760/37248)\n",
      "Epoch: 26 | Batch_idx: 300 |  Loss_1: (0.1166) | Acc_1: (96.00%) (36986/38528)\n",
      "Epoch: 26 | Batch_idx: 310 |  Loss_1: (0.1166) | Acc_1: (95.99%) (38210/39808)\n",
      "Epoch: 26 | Batch_idx: 320 |  Loss_1: (0.1169) | Acc_1: (95.98%) (39438/41088)\n",
      "Epoch: 26 | Batch_idx: 330 |  Loss_1: (0.1183) | Acc_1: (95.94%) (40649/42368)\n",
      "Epoch: 26 | Batch_idx: 340 |  Loss_1: (0.1184) | Acc_1: (95.93%) (41872/43648)\n",
      "Epoch: 26 | Batch_idx: 350 |  Loss_1: (0.1189) | Acc_1: (95.91%) (43092/44928)\n",
      "Epoch: 26 | Batch_idx: 360 |  Loss_1: (0.1188) | Acc_1: (95.90%) (44315/46208)\n",
      "Epoch: 26 | Batch_idx: 370 |  Loss_1: (0.1186) | Acc_1: (95.92%) (45550/47488)\n",
      "Epoch: 26 | Batch_idx: 380 |  Loss_1: (0.1189) | Acc_1: (95.92%) (46777/48768)\n",
      "Epoch: 26 | Batch_idx: 390 |  Loss_1: (0.1185) | Acc_1: (95.93%) (47964/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4307) | Acc: (88.71%) (8871/10000)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.0890) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.0948) | Acc_1: (96.59%) (1360/1408)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.0940) | Acc_1: (96.58%) (2596/2688)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.0940) | Acc_1: (96.72%) (3838/3968)\n",
      "Epoch: 27 | Batch_idx: 40 |  Loss_1: (0.0933) | Acc_1: (96.72%) (5076/5248)\n",
      "Epoch: 27 | Batch_idx: 50 |  Loss_1: (0.0943) | Acc_1: (96.75%) (6316/6528)\n",
      "Epoch: 27 | Batch_idx: 60 |  Loss_1: (0.0943) | Acc_1: (96.76%) (7555/7808)\n",
      "Epoch: 27 | Batch_idx: 70 |  Loss_1: (0.0961) | Acc_1: (96.64%) (8783/9088)\n",
      "Epoch: 27 | Batch_idx: 80 |  Loss_1: (0.0992) | Acc_1: (96.53%) (10008/10368)\n",
      "Epoch: 27 | Batch_idx: 90 |  Loss_1: (0.0991) | Acc_1: (96.54%) (11245/11648)\n",
      "Epoch: 27 | Batch_idx: 100 |  Loss_1: (0.1018) | Acc_1: (96.37%) (12459/12928)\n",
      "Epoch: 27 | Batch_idx: 110 |  Loss_1: (0.1015) | Acc_1: (96.39%) (13695/14208)\n",
      "Epoch: 27 | Batch_idx: 120 |  Loss_1: (0.1024) | Acc_1: (96.33%) (14920/15488)\n",
      "Epoch: 27 | Batch_idx: 130 |  Loss_1: (0.1043) | Acc_1: (96.22%) (16135/16768)\n",
      "Epoch: 27 | Batch_idx: 140 |  Loss_1: (0.1055) | Acc_1: (96.20%) (17363/18048)\n",
      "Epoch: 27 | Batch_idx: 150 |  Loss_1: (0.1044) | Acc_1: (96.24%) (18602/19328)\n",
      "Epoch: 27 | Batch_idx: 160 |  Loss_1: (0.1042) | Acc_1: (96.29%) (19843/20608)\n",
      "Epoch: 27 | Batch_idx: 170 |  Loss_1: (0.1058) | Acc_1: (96.23%) (21063/21888)\n",
      "Epoch: 27 | Batch_idx: 180 |  Loss_1: (0.1054) | Acc_1: (96.27%) (22303/23168)\n",
      "Epoch: 27 | Batch_idx: 190 |  Loss_1: (0.1055) | Acc_1: (96.28%) (23539/24448)\n",
      "Epoch: 27 | Batch_idx: 200 |  Loss_1: (0.1043) | Acc_1: (96.32%) (24780/25728)\n",
      "Epoch: 27 | Batch_idx: 210 |  Loss_1: (0.1054) | Acc_1: (96.28%) (26004/27008)\n",
      "Epoch: 27 | Batch_idx: 220 |  Loss_1: (0.1052) | Acc_1: (96.26%) (27229/28288)\n",
      "Epoch: 27 | Batch_idx: 230 |  Loss_1: (0.1069) | Acc_1: (96.21%) (28448/29568)\n",
      "Epoch: 27 | Batch_idx: 240 |  Loss_1: (0.1069) | Acc_1: (96.19%) (29673/30848)\n",
      "Epoch: 27 | Batch_idx: 250 |  Loss_1: (0.1078) | Acc_1: (96.15%) (30890/32128)\n",
      "Epoch: 27 | Batch_idx: 260 |  Loss_1: (0.1082) | Acc_1: (96.12%) (32112/33408)\n",
      "Epoch: 27 | Batch_idx: 270 |  Loss_1: (0.1080) | Acc_1: (96.11%) (33340/34688)\n",
      "Epoch: 27 | Batch_idx: 280 |  Loss_1: (0.1076) | Acc_1: (96.15%) (34584/35968)\n",
      "Epoch: 27 | Batch_idx: 290 |  Loss_1: (0.1074) | Acc_1: (96.16%) (35819/37248)\n",
      "Epoch: 27 | Batch_idx: 300 |  Loss_1: (0.1070) | Acc_1: (96.16%) (37050/38528)\n",
      "Epoch: 27 | Batch_idx: 310 |  Loss_1: (0.1071) | Acc_1: (96.18%) (38286/39808)\n",
      "Epoch: 27 | Batch_idx: 320 |  Loss_1: (0.1073) | Acc_1: (96.16%) (39512/41088)\n",
      "Epoch: 27 | Batch_idx: 330 |  Loss_1: (0.1075) | Acc_1: (96.17%) (40745/42368)\n",
      "Epoch: 27 | Batch_idx: 340 |  Loss_1: (0.1073) | Acc_1: (96.18%) (41981/43648)\n",
      "Epoch: 27 | Batch_idx: 350 |  Loss_1: (0.1077) | Acc_1: (96.17%) (43209/44928)\n",
      "Epoch: 27 | Batch_idx: 360 |  Loss_1: (0.1086) | Acc_1: (96.16%) (44432/46208)\n",
      "Epoch: 27 | Batch_idx: 370 |  Loss_1: (0.1090) | Acc_1: (96.15%) (45660/47488)\n",
      "Epoch: 27 | Batch_idx: 380 |  Loss_1: (0.1091) | Acc_1: (96.12%) (46877/48768)\n",
      "Epoch: 27 | Batch_idx: 390 |  Loss_1: (0.1091) | Acc_1: (96.11%) (48057/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5182) | Acc: (86.76%) (8676/10000)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.1543) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.0982) | Acc_1: (96.52%) (1359/1408)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.1068) | Acc_1: (96.13%) (2584/2688)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.1052) | Acc_1: (96.19%) (3817/3968)\n",
      "Epoch: 28 | Batch_idx: 40 |  Loss_1: (0.1044) | Acc_1: (96.21%) (5049/5248)\n",
      "Epoch: 28 | Batch_idx: 50 |  Loss_1: (0.0991) | Acc_1: (96.42%) (6294/6528)\n",
      "Epoch: 28 | Batch_idx: 60 |  Loss_1: (0.0945) | Acc_1: (96.61%) (7543/7808)\n",
      "Epoch: 28 | Batch_idx: 70 |  Loss_1: (0.0918) | Acc_1: (96.73%) (8791/9088)\n",
      "Epoch: 28 | Batch_idx: 80 |  Loss_1: (0.0931) | Acc_1: (96.65%) (10021/10368)\n",
      "Epoch: 28 | Batch_idx: 90 |  Loss_1: (0.0923) | Acc_1: (96.69%) (11263/11648)\n",
      "Epoch: 28 | Batch_idx: 100 |  Loss_1: (0.0922) | Acc_1: (96.70%) (12502/12928)\n",
      "Epoch: 28 | Batch_idx: 110 |  Loss_1: (0.0907) | Acc_1: (96.78%) (13750/14208)\n",
      "Epoch: 28 | Batch_idx: 120 |  Loss_1: (0.0921) | Acc_1: (96.71%) (14979/15488)\n",
      "Epoch: 28 | Batch_idx: 130 |  Loss_1: (0.0931) | Acc_1: (96.67%) (16210/16768)\n",
      "Epoch: 28 | Batch_idx: 140 |  Loss_1: (0.0935) | Acc_1: (96.69%) (17451/18048)\n",
      "Epoch: 28 | Batch_idx: 150 |  Loss_1: (0.0937) | Acc_1: (96.69%) (18688/19328)\n",
      "Epoch: 28 | Batch_idx: 160 |  Loss_1: (0.0951) | Acc_1: (96.66%) (19920/20608)\n",
      "Epoch: 28 | Batch_idx: 170 |  Loss_1: (0.0960) | Acc_1: (96.63%) (21150/21888)\n",
      "Epoch: 28 | Batch_idx: 180 |  Loss_1: (0.0969) | Acc_1: (96.60%) (22381/23168)\n",
      "Epoch: 28 | Batch_idx: 190 |  Loss_1: (0.0957) | Acc_1: (96.65%) (23629/24448)\n",
      "Epoch: 28 | Batch_idx: 200 |  Loss_1: (0.0949) | Acc_1: (96.66%) (24869/25728)\n",
      "Epoch: 28 | Batch_idx: 210 |  Loss_1: (0.0948) | Acc_1: (96.66%) (26107/27008)\n",
      "Epoch: 28 | Batch_idx: 220 |  Loss_1: (0.0954) | Acc_1: (96.66%) (27342/28288)\n",
      "Epoch: 28 | Batch_idx: 230 |  Loss_1: (0.0955) | Acc_1: (96.64%) (28574/29568)\n",
      "Epoch: 28 | Batch_idx: 240 |  Loss_1: (0.0963) | Acc_1: (96.63%) (29807/30848)\n",
      "Epoch: 28 | Batch_idx: 250 |  Loss_1: (0.0970) | Acc_1: (96.60%) (31035/32128)\n",
      "Epoch: 28 | Batch_idx: 260 |  Loss_1: (0.0980) | Acc_1: (96.56%) (32258/33408)\n",
      "Epoch: 28 | Batch_idx: 270 |  Loss_1: (0.0987) | Acc_1: (96.53%) (33483/34688)\n",
      "Epoch: 28 | Batch_idx: 280 |  Loss_1: (0.0997) | Acc_1: (96.50%) (34710/35968)\n",
      "Epoch: 28 | Batch_idx: 290 |  Loss_1: (0.1002) | Acc_1: (96.49%) (35941/37248)\n",
      "Epoch: 28 | Batch_idx: 300 |  Loss_1: (0.1002) | Acc_1: (96.49%) (37174/38528)\n",
      "Epoch: 28 | Batch_idx: 310 |  Loss_1: (0.1003) | Acc_1: (96.48%) (38405/39808)\n",
      "Epoch: 28 | Batch_idx: 320 |  Loss_1: (0.1005) | Acc_1: (96.47%) (39636/41088)\n",
      "Epoch: 28 | Batch_idx: 330 |  Loss_1: (0.1011) | Acc_1: (96.45%) (40865/42368)\n",
      "Epoch: 28 | Batch_idx: 340 |  Loss_1: (0.1016) | Acc_1: (96.45%) (42098/43648)\n",
      "Epoch: 28 | Batch_idx: 350 |  Loss_1: (0.1020) | Acc_1: (96.42%) (43318/44928)\n",
      "Epoch: 28 | Batch_idx: 360 |  Loss_1: (0.1017) | Acc_1: (96.43%) (44560/46208)\n",
      "Epoch: 28 | Batch_idx: 370 |  Loss_1: (0.1025) | Acc_1: (96.41%) (45781/47488)\n",
      "Epoch: 28 | Batch_idx: 380 |  Loss_1: (0.1031) | Acc_1: (96.39%) (47008/48768)\n",
      "Epoch: 28 | Batch_idx: 390 |  Loss_1: (0.1033) | Acc_1: (96.38%) (48189/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4354) | Acc: (89.16%) (8916/10000)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.1017) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.1021) | Acc_1: (96.45%) (1358/1408)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.0940) | Acc_1: (96.80%) (2602/2688)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.0945) | Acc_1: (96.75%) (3839/3968)\n",
      "Epoch: 29 | Batch_idx: 40 |  Loss_1: (0.0940) | Acc_1: (96.84%) (5082/5248)\n",
      "Epoch: 29 | Batch_idx: 50 |  Loss_1: (0.0900) | Acc_1: (97.00%) (6332/6528)\n",
      "Epoch: 29 | Batch_idx: 60 |  Loss_1: (0.0899) | Acc_1: (96.90%) (7566/7808)\n",
      "Epoch: 29 | Batch_idx: 70 |  Loss_1: (0.0906) | Acc_1: (96.97%) (8813/9088)\n",
      "Epoch: 29 | Batch_idx: 80 |  Loss_1: (0.0906) | Acc_1: (96.95%) (10052/10368)\n",
      "Epoch: 29 | Batch_idx: 90 |  Loss_1: (0.0885) | Acc_1: (96.99%) (11297/11648)\n",
      "Epoch: 29 | Batch_idx: 100 |  Loss_1: (0.0865) | Acc_1: (97.03%) (12544/12928)\n",
      "Epoch: 29 | Batch_idx: 110 |  Loss_1: (0.0863) | Acc_1: (97.04%) (13787/14208)\n",
      "Epoch: 29 | Batch_idx: 120 |  Loss_1: (0.0867) | Acc_1: (97.00%) (15024/15488)\n",
      "Epoch: 29 | Batch_idx: 130 |  Loss_1: (0.0868) | Acc_1: (96.97%) (16260/16768)\n",
      "Epoch: 29 | Batch_idx: 140 |  Loss_1: (0.0878) | Acc_1: (96.92%) (17493/18048)\n",
      "Epoch: 29 | Batch_idx: 150 |  Loss_1: (0.0881) | Acc_1: (96.89%) (18727/19328)\n",
      "Epoch: 29 | Batch_idx: 160 |  Loss_1: (0.0884) | Acc_1: (96.90%) (19969/20608)\n",
      "Epoch: 29 | Batch_idx: 170 |  Loss_1: (0.0881) | Acc_1: (96.91%) (21211/21888)\n",
      "Epoch: 29 | Batch_idx: 180 |  Loss_1: (0.0890) | Acc_1: (96.89%) (22447/23168)\n",
      "Epoch: 29 | Batch_idx: 190 |  Loss_1: (0.0890) | Acc_1: (96.85%) (23679/24448)\n",
      "Epoch: 29 | Batch_idx: 200 |  Loss_1: (0.0885) | Acc_1: (96.86%) (24920/25728)\n",
      "Epoch: 29 | Batch_idx: 210 |  Loss_1: (0.0887) | Acc_1: (96.84%) (26155/27008)\n",
      "Epoch: 29 | Batch_idx: 220 |  Loss_1: (0.0882) | Acc_1: (96.85%) (27396/28288)\n",
      "Epoch: 29 | Batch_idx: 230 |  Loss_1: (0.0885) | Acc_1: (96.84%) (28633/29568)\n",
      "Epoch: 29 | Batch_idx: 240 |  Loss_1: (0.0902) | Acc_1: (96.80%) (29861/30848)\n",
      "Epoch: 29 | Batch_idx: 250 |  Loss_1: (0.0903) | Acc_1: (96.81%) (31102/32128)\n",
      "Epoch: 29 | Batch_idx: 260 |  Loss_1: (0.0918) | Acc_1: (96.74%) (32319/33408)\n",
      "Epoch: 29 | Batch_idx: 270 |  Loss_1: (0.0923) | Acc_1: (96.71%) (33546/34688)\n",
      "Epoch: 29 | Batch_idx: 280 |  Loss_1: (0.0918) | Acc_1: (96.73%) (34793/35968)\n",
      "Epoch: 29 | Batch_idx: 290 |  Loss_1: (0.0920) | Acc_1: (96.72%) (36026/37248)\n",
      "Epoch: 29 | Batch_idx: 300 |  Loss_1: (0.0921) | Acc_1: (96.72%) (37263/38528)\n",
      "Epoch: 29 | Batch_idx: 310 |  Loss_1: (0.0923) | Acc_1: (96.72%) (38501/39808)\n",
      "Epoch: 29 | Batch_idx: 320 |  Loss_1: (0.0932) | Acc_1: (96.69%) (39729/41088)\n",
      "Epoch: 29 | Batch_idx: 330 |  Loss_1: (0.0941) | Acc_1: (96.66%) (40953/42368)\n",
      "Epoch: 29 | Batch_idx: 340 |  Loss_1: (0.0942) | Acc_1: (96.66%) (42191/43648)\n",
      "Epoch: 29 | Batch_idx: 350 |  Loss_1: (0.0939) | Acc_1: (96.69%) (43441/44928)\n",
      "Epoch: 29 | Batch_idx: 360 |  Loss_1: (0.0939) | Acc_1: (96.70%) (44681/46208)\n",
      "Epoch: 29 | Batch_idx: 370 |  Loss_1: (0.0939) | Acc_1: (96.69%) (45915/47488)\n",
      "Epoch: 29 | Batch_idx: 380 |  Loss_1: (0.0938) | Acc_1: (96.69%) (47156/48768)\n",
      "Epoch: 29 | Batch_idx: 390 |  Loss_1: (0.0940) | Acc_1: (96.68%) (48341/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4812) | Acc: (88.78%) (8878/10000)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.1089) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.0961) | Acc_1: (96.66%) (1361/1408)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.1006) | Acc_1: (96.35%) (2590/2688)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.0936) | Acc_1: (96.65%) (3835/3968)\n",
      "Epoch: 30 | Batch_idx: 40 |  Loss_1: (0.0987) | Acc_1: (96.40%) (5059/5248)\n",
      "Epoch: 30 | Batch_idx: 50 |  Loss_1: (0.0962) | Acc_1: (96.52%) (6301/6528)\n",
      "Epoch: 30 | Batch_idx: 60 |  Loss_1: (0.0981) | Acc_1: (96.44%) (7530/7808)\n",
      "Epoch: 30 | Batch_idx: 70 |  Loss_1: (0.0963) | Acc_1: (96.57%) (8776/9088)\n",
      "Epoch: 30 | Batch_idx: 80 |  Loss_1: (0.0972) | Acc_1: (96.55%) (10010/10368)\n",
      "Epoch: 30 | Batch_idx: 90 |  Loss_1: (0.0971) | Acc_1: (96.57%) (11248/11648)\n",
      "Epoch: 30 | Batch_idx: 100 |  Loss_1: (0.0960) | Acc_1: (96.58%) (12486/12928)\n",
      "Epoch: 30 | Batch_idx: 110 |  Loss_1: (0.0954) | Acc_1: (96.59%) (13724/14208)\n",
      "Epoch: 30 | Batch_idx: 120 |  Loss_1: (0.0946) | Acc_1: (96.61%) (14963/15488)\n",
      "Epoch: 30 | Batch_idx: 130 |  Loss_1: (0.0928) | Acc_1: (96.65%) (16206/16768)\n",
      "Epoch: 30 | Batch_idx: 140 |  Loss_1: (0.0932) | Acc_1: (96.63%) (17439/18048)\n",
      "Epoch: 30 | Batch_idx: 150 |  Loss_1: (0.0928) | Acc_1: (96.64%) (18679/19328)\n",
      "Epoch: 30 | Batch_idx: 160 |  Loss_1: (0.0928) | Acc_1: (96.67%) (19921/20608)\n",
      "Epoch: 30 | Batch_idx: 170 |  Loss_1: (0.0944) | Acc_1: (96.61%) (21147/21888)\n",
      "Epoch: 30 | Batch_idx: 180 |  Loss_1: (0.0947) | Acc_1: (96.63%) (22388/23168)\n",
      "Epoch: 30 | Batch_idx: 190 |  Loss_1: (0.0948) | Acc_1: (96.64%) (23627/24448)\n",
      "Epoch: 30 | Batch_idx: 200 |  Loss_1: (0.0952) | Acc_1: (96.65%) (24866/25728)\n",
      "Epoch: 30 | Batch_idx: 210 |  Loss_1: (0.0941) | Acc_1: (96.71%) (26120/27008)\n",
      "Epoch: 30 | Batch_idx: 220 |  Loss_1: (0.0941) | Acc_1: (96.73%) (27363/28288)\n",
      "Epoch: 30 | Batch_idx: 230 |  Loss_1: (0.0941) | Acc_1: (96.74%) (28603/29568)\n",
      "Epoch: 30 | Batch_idx: 240 |  Loss_1: (0.0931) | Acc_1: (96.78%) (29854/30848)\n",
      "Epoch: 30 | Batch_idx: 250 |  Loss_1: (0.0921) | Acc_1: (96.81%) (31102/32128)\n",
      "Epoch: 30 | Batch_idx: 260 |  Loss_1: (0.0922) | Acc_1: (96.81%) (32342/33408)\n",
      "Epoch: 30 | Batch_idx: 270 |  Loss_1: (0.0929) | Acc_1: (96.78%) (33572/34688)\n",
      "Epoch: 30 | Batch_idx: 280 |  Loss_1: (0.0931) | Acc_1: (96.75%) (34799/35968)\n",
      "Epoch: 30 | Batch_idx: 290 |  Loss_1: (0.0937) | Acc_1: (96.70%) (36020/37248)\n",
      "Epoch: 30 | Batch_idx: 300 |  Loss_1: (0.0936) | Acc_1: (96.72%) (37266/38528)\n",
      "Epoch: 30 | Batch_idx: 310 |  Loss_1: (0.0938) | Acc_1: (96.73%) (38505/39808)\n",
      "Epoch: 30 | Batch_idx: 320 |  Loss_1: (0.0937) | Acc_1: (96.72%) (39741/41088)\n",
      "Epoch: 30 | Batch_idx: 330 |  Loss_1: (0.0940) | Acc_1: (96.70%) (40970/42368)\n",
      "Epoch: 30 | Batch_idx: 340 |  Loss_1: (0.0942) | Acc_1: (96.68%) (42201/43648)\n",
      "Epoch: 30 | Batch_idx: 350 |  Loss_1: (0.0937) | Acc_1: (96.71%) (43452/44928)\n",
      "Epoch: 30 | Batch_idx: 360 |  Loss_1: (0.0939) | Acc_1: (96.70%) (44684/46208)\n",
      "Epoch: 30 | Batch_idx: 370 |  Loss_1: (0.0935) | Acc_1: (96.71%) (45927/47488)\n",
      "Epoch: 30 | Batch_idx: 380 |  Loss_1: (0.0937) | Acc_1: (96.71%) (47164/48768)\n",
      "Epoch: 30 | Batch_idx: 390 |  Loss_1: (0.0938) | Acc_1: (96.72%) (48359/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4366) | Acc: (89.34%) (8934/10000)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.1198) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.0939) | Acc_1: (96.24%) (1355/1408)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.0877) | Acc_1: (96.69%) (2599/2688)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.0855) | Acc_1: (96.82%) (3842/3968)\n",
      "Epoch: 31 | Batch_idx: 40 |  Loss_1: (0.0796) | Acc_1: (97.03%) (5092/5248)\n",
      "Epoch: 31 | Batch_idx: 50 |  Loss_1: (0.0822) | Acc_1: (97.07%) (6337/6528)\n",
      "Epoch: 31 | Batch_idx: 60 |  Loss_1: (0.0812) | Acc_1: (97.07%) (7579/7808)\n",
      "Epoch: 31 | Batch_idx: 70 |  Loss_1: (0.0804) | Acc_1: (97.07%) (8822/9088)\n",
      "Epoch: 31 | Batch_idx: 80 |  Loss_1: (0.0803) | Acc_1: (97.05%) (10062/10368)\n",
      "Epoch: 31 | Batch_idx: 90 |  Loss_1: (0.0799) | Acc_1: (97.00%) (11299/11648)\n",
      "Epoch: 31 | Batch_idx: 100 |  Loss_1: (0.0816) | Acc_1: (96.92%) (12530/12928)\n",
      "Epoch: 31 | Batch_idx: 110 |  Loss_1: (0.0813) | Acc_1: (96.96%) (13776/14208)\n",
      "Epoch: 31 | Batch_idx: 120 |  Loss_1: (0.0803) | Acc_1: (96.98%) (15021/15488)\n",
      "Epoch: 31 | Batch_idx: 130 |  Loss_1: (0.0812) | Acc_1: (96.96%) (16259/16768)\n",
      "Epoch: 31 | Batch_idx: 140 |  Loss_1: (0.0822) | Acc_1: (96.95%) (17498/18048)\n",
      "Epoch: 31 | Batch_idx: 150 |  Loss_1: (0.0822) | Acc_1: (96.95%) (18738/19328)\n",
      "Epoch: 31 | Batch_idx: 160 |  Loss_1: (0.0828) | Acc_1: (96.94%) (19978/20608)\n",
      "Epoch: 31 | Batch_idx: 170 |  Loss_1: (0.0833) | Acc_1: (96.92%) (21213/21888)\n",
      "Epoch: 31 | Batch_idx: 180 |  Loss_1: (0.0840) | Acc_1: (96.91%) (22453/23168)\n",
      "Epoch: 31 | Batch_idx: 190 |  Loss_1: (0.0833) | Acc_1: (96.94%) (23701/24448)\n",
      "Epoch: 31 | Batch_idx: 200 |  Loss_1: (0.0849) | Acc_1: (96.89%) (24929/25728)\n",
      "Epoch: 31 | Batch_idx: 210 |  Loss_1: (0.0851) | Acc_1: (96.91%) (26173/27008)\n",
      "Epoch: 31 | Batch_idx: 220 |  Loss_1: (0.0862) | Acc_1: (96.89%) (27407/28288)\n",
      "Epoch: 31 | Batch_idx: 230 |  Loss_1: (0.0867) | Acc_1: (96.85%) (28638/29568)\n",
      "Epoch: 31 | Batch_idx: 240 |  Loss_1: (0.0874) | Acc_1: (96.84%) (29873/30848)\n",
      "Epoch: 31 | Batch_idx: 250 |  Loss_1: (0.0874) | Acc_1: (96.83%) (31109/32128)\n",
      "Epoch: 31 | Batch_idx: 260 |  Loss_1: (0.0869) | Acc_1: (96.84%) (32352/33408)\n",
      "Epoch: 31 | Batch_idx: 270 |  Loss_1: (0.0869) | Acc_1: (96.84%) (33591/34688)\n",
      "Epoch: 31 | Batch_idx: 280 |  Loss_1: (0.0871) | Acc_1: (96.83%) (34829/35968)\n",
      "Epoch: 31 | Batch_idx: 290 |  Loss_1: (0.0870) | Acc_1: (96.83%) (36069/37248)\n",
      "Epoch: 31 | Batch_idx: 300 |  Loss_1: (0.0876) | Acc_1: (96.81%) (37298/38528)\n",
      "Epoch: 31 | Batch_idx: 310 |  Loss_1: (0.0882) | Acc_1: (96.79%) (38529/39808)\n",
      "Epoch: 31 | Batch_idx: 320 |  Loss_1: (0.0878) | Acc_1: (96.79%) (39768/41088)\n",
      "Epoch: 31 | Batch_idx: 330 |  Loss_1: (0.0879) | Acc_1: (96.80%) (41011/42368)\n",
      "Epoch: 31 | Batch_idx: 340 |  Loss_1: (0.0880) | Acc_1: (96.80%) (42250/43648)\n",
      "Epoch: 31 | Batch_idx: 350 |  Loss_1: (0.0879) | Acc_1: (96.81%) (43495/44928)\n",
      "Epoch: 31 | Batch_idx: 360 |  Loss_1: (0.0878) | Acc_1: (96.81%) (44733/46208)\n",
      "Epoch: 31 | Batch_idx: 370 |  Loss_1: (0.0884) | Acc_1: (96.79%) (45964/47488)\n",
      "Epoch: 31 | Batch_idx: 380 |  Loss_1: (0.0891) | Acc_1: (96.77%) (47193/48768)\n",
      "Epoch: 31 | Batch_idx: 390 |  Loss_1: (0.0894) | Acc_1: (96.76%) (48382/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4518) | Acc: (88.97%) (8897/10000)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.1536) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.0797) | Acc_1: (96.80%) (1363/1408)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.0815) | Acc_1: (96.99%) (2607/2688)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.0802) | Acc_1: (96.82%) (3842/3968)\n",
      "Epoch: 32 | Batch_idx: 40 |  Loss_1: (0.0775) | Acc_1: (96.99%) (5090/5248)\n",
      "Epoch: 32 | Batch_idx: 50 |  Loss_1: (0.0781) | Acc_1: (97.12%) (6340/6528)\n",
      "Epoch: 32 | Batch_idx: 60 |  Loss_1: (0.0781) | Acc_1: (97.11%) (7582/7808)\n",
      "Epoch: 32 | Batch_idx: 70 |  Loss_1: (0.0787) | Acc_1: (97.14%) (8828/9088)\n",
      "Epoch: 32 | Batch_idx: 80 |  Loss_1: (0.0788) | Acc_1: (97.08%) (10065/10368)\n",
      "Epoch: 32 | Batch_idx: 90 |  Loss_1: (0.0786) | Acc_1: (97.10%) (11310/11648)\n",
      "Epoch: 32 | Batch_idx: 100 |  Loss_1: (0.0786) | Acc_1: (97.10%) (12553/12928)\n",
      "Epoch: 32 | Batch_idx: 110 |  Loss_1: (0.0788) | Acc_1: (97.13%) (13800/14208)\n",
      "Epoch: 32 | Batch_idx: 120 |  Loss_1: (0.0786) | Acc_1: (97.15%) (15047/15488)\n",
      "Epoch: 32 | Batch_idx: 130 |  Loss_1: (0.0768) | Acc_1: (97.23%) (16303/16768)\n",
      "Epoch: 32 | Batch_idx: 140 |  Loss_1: (0.0772) | Acc_1: (97.20%) (17543/18048)\n",
      "Epoch: 32 | Batch_idx: 150 |  Loss_1: (0.0764) | Acc_1: (97.26%) (18799/19328)\n",
      "Epoch: 32 | Batch_idx: 160 |  Loss_1: (0.0758) | Acc_1: (97.29%) (20050/20608)\n",
      "Epoch: 32 | Batch_idx: 170 |  Loss_1: (0.0769) | Acc_1: (97.26%) (21289/21888)\n",
      "Epoch: 32 | Batch_idx: 180 |  Loss_1: (0.0765) | Acc_1: (97.26%) (22533/23168)\n",
      "Epoch: 32 | Batch_idx: 190 |  Loss_1: (0.0762) | Acc_1: (97.27%) (23781/24448)\n",
      "Epoch: 32 | Batch_idx: 200 |  Loss_1: (0.0766) | Acc_1: (97.28%) (25027/25728)\n",
      "Epoch: 32 | Batch_idx: 210 |  Loss_1: (0.0762) | Acc_1: (97.30%) (26278/27008)\n",
      "Epoch: 32 | Batch_idx: 220 |  Loss_1: (0.0761) | Acc_1: (97.30%) (27524/28288)\n",
      "Epoch: 32 | Batch_idx: 230 |  Loss_1: (0.0758) | Acc_1: (97.32%) (28775/29568)\n",
      "Epoch: 32 | Batch_idx: 240 |  Loss_1: (0.0766) | Acc_1: (97.31%) (30017/30848)\n",
      "Epoch: 32 | Batch_idx: 250 |  Loss_1: (0.0778) | Acc_1: (97.28%) (31253/32128)\n",
      "Epoch: 32 | Batch_idx: 260 |  Loss_1: (0.0785) | Acc_1: (97.26%) (32492/33408)\n",
      "Epoch: 32 | Batch_idx: 270 |  Loss_1: (0.0795) | Acc_1: (97.22%) (33724/34688)\n",
      "Epoch: 32 | Batch_idx: 280 |  Loss_1: (0.0793) | Acc_1: (97.23%) (34973/35968)\n",
      "Epoch: 32 | Batch_idx: 290 |  Loss_1: (0.0793) | Acc_1: (97.22%) (36212/37248)\n",
      "Epoch: 32 | Batch_idx: 300 |  Loss_1: (0.0791) | Acc_1: (97.21%) (37454/38528)\n",
      "Epoch: 32 | Batch_idx: 310 |  Loss_1: (0.0792) | Acc_1: (97.22%) (38702/39808)\n",
      "Epoch: 32 | Batch_idx: 320 |  Loss_1: (0.0798) | Acc_1: (97.19%) (39934/41088)\n",
      "Epoch: 32 | Batch_idx: 330 |  Loss_1: (0.0798) | Acc_1: (97.20%) (41182/42368)\n",
      "Epoch: 32 | Batch_idx: 340 |  Loss_1: (0.0805) | Acc_1: (97.19%) (42421/43648)\n",
      "Epoch: 32 | Batch_idx: 350 |  Loss_1: (0.0810) | Acc_1: (97.17%) (43658/44928)\n",
      "Epoch: 32 | Batch_idx: 360 |  Loss_1: (0.0819) | Acc_1: (97.13%) (44884/46208)\n",
      "Epoch: 32 | Batch_idx: 370 |  Loss_1: (0.0825) | Acc_1: (97.12%) (46122/47488)\n",
      "Epoch: 32 | Batch_idx: 380 |  Loss_1: (0.0827) | Acc_1: (97.12%) (47362/48768)\n",
      "Epoch: 32 | Batch_idx: 390 |  Loss_1: (0.0835) | Acc_1: (97.08%) (48541/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5463) | Acc: (87.62%) (8762/10000)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.0751) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.0795) | Acc_1: (97.16%) (1368/1408)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.0799) | Acc_1: (97.06%) (2609/2688)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.0799) | Acc_1: (97.10%) (3853/3968)\n",
      "Epoch: 33 | Batch_idx: 40 |  Loss_1: (0.0787) | Acc_1: (96.93%) (5087/5248)\n",
      "Epoch: 33 | Batch_idx: 50 |  Loss_1: (0.0740) | Acc_1: (97.15%) (6342/6528)\n",
      "Epoch: 33 | Batch_idx: 60 |  Loss_1: (0.0726) | Acc_1: (97.23%) (7592/7808)\n",
      "Epoch: 33 | Batch_idx: 70 |  Loss_1: (0.0740) | Acc_1: (97.22%) (8835/9088)\n",
      "Epoch: 33 | Batch_idx: 80 |  Loss_1: (0.0739) | Acc_1: (97.24%) (10082/10368)\n",
      "Epoch: 33 | Batch_idx: 90 |  Loss_1: (0.0711) | Acc_1: (97.37%) (11342/11648)\n",
      "Epoch: 33 | Batch_idx: 100 |  Loss_1: (0.0705) | Acc_1: (97.39%) (12591/12928)\n",
      "Epoch: 33 | Batch_idx: 110 |  Loss_1: (0.0711) | Acc_1: (97.41%) (13840/14208)\n",
      "Epoch: 33 | Batch_idx: 120 |  Loss_1: (0.0733) | Acc_1: (97.35%) (15078/15488)\n",
      "Epoch: 33 | Batch_idx: 130 |  Loss_1: (0.0749) | Acc_1: (97.30%) (16316/16768)\n",
      "Epoch: 33 | Batch_idx: 140 |  Loss_1: (0.0745) | Acc_1: (97.32%) (17565/18048)\n",
      "Epoch: 33 | Batch_idx: 150 |  Loss_1: (0.0762) | Acc_1: (97.26%) (18798/19328)\n",
      "Epoch: 33 | Batch_idx: 160 |  Loss_1: (0.0765) | Acc_1: (97.22%) (20035/20608)\n",
      "Epoch: 33 | Batch_idx: 170 |  Loss_1: (0.0767) | Acc_1: (97.19%) (21273/21888)\n",
      "Epoch: 33 | Batch_idx: 180 |  Loss_1: (0.0774) | Acc_1: (97.17%) (22512/23168)\n",
      "Epoch: 33 | Batch_idx: 190 |  Loss_1: (0.0778) | Acc_1: (97.18%) (23758/24448)\n",
      "Epoch: 33 | Batch_idx: 200 |  Loss_1: (0.0783) | Acc_1: (97.15%) (24995/25728)\n",
      "Epoch: 33 | Batch_idx: 210 |  Loss_1: (0.0780) | Acc_1: (97.15%) (26239/27008)\n",
      "Epoch: 33 | Batch_idx: 220 |  Loss_1: (0.0774) | Acc_1: (97.16%) (27484/28288)\n",
      "Epoch: 33 | Batch_idx: 230 |  Loss_1: (0.0773) | Acc_1: (97.17%) (28732/29568)\n",
      "Epoch: 33 | Batch_idx: 240 |  Loss_1: (0.0768) | Acc_1: (97.19%) (29981/30848)\n",
      "Epoch: 33 | Batch_idx: 250 |  Loss_1: (0.0763) | Acc_1: (97.20%) (31228/32128)\n",
      "Epoch: 33 | Batch_idx: 260 |  Loss_1: (0.0762) | Acc_1: (97.20%) (32473/33408)\n",
      "Epoch: 33 | Batch_idx: 270 |  Loss_1: (0.0771) | Acc_1: (97.18%) (33709/34688)\n",
      "Epoch: 33 | Batch_idx: 280 |  Loss_1: (0.0768) | Acc_1: (97.18%) (34955/35968)\n",
      "Epoch: 33 | Batch_idx: 290 |  Loss_1: (0.0764) | Acc_1: (97.20%) (36204/37248)\n",
      "Epoch: 33 | Batch_idx: 300 |  Loss_1: (0.0765) | Acc_1: (97.19%) (37446/38528)\n",
      "Epoch: 33 | Batch_idx: 310 |  Loss_1: (0.0773) | Acc_1: (97.17%) (38682/39808)\n",
      "Epoch: 33 | Batch_idx: 320 |  Loss_1: (0.0775) | Acc_1: (97.16%) (39922/41088)\n",
      "Epoch: 33 | Batch_idx: 330 |  Loss_1: (0.0778) | Acc_1: (97.16%) (41165/42368)\n",
      "Epoch: 33 | Batch_idx: 340 |  Loss_1: (0.0782) | Acc_1: (97.15%) (42403/43648)\n",
      "Epoch: 33 | Batch_idx: 350 |  Loss_1: (0.0785) | Acc_1: (97.15%) (43647/44928)\n",
      "Epoch: 33 | Batch_idx: 360 |  Loss_1: (0.0785) | Acc_1: (97.14%) (44888/46208)\n",
      "Epoch: 33 | Batch_idx: 370 |  Loss_1: (0.0783) | Acc_1: (97.15%) (46135/47488)\n",
      "Epoch: 33 | Batch_idx: 380 |  Loss_1: (0.0781) | Acc_1: (97.15%) (47378/48768)\n",
      "Epoch: 33 | Batch_idx: 390 |  Loss_1: (0.0778) | Acc_1: (97.16%) (48578/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4529) | Acc: (89.34%) (8934/10000)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.0554) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.0746) | Acc_1: (97.87%) (1378/1408)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.0751) | Acc_1: (97.73%) (2627/2688)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.0744) | Acc_1: (97.56%) (3871/3968)\n",
      "Epoch: 34 | Batch_idx: 40 |  Loss_1: (0.0741) | Acc_1: (97.52%) (5118/5248)\n",
      "Epoch: 34 | Batch_idx: 50 |  Loss_1: (0.0752) | Acc_1: (97.52%) (6366/6528)\n",
      "Epoch: 34 | Batch_idx: 60 |  Loss_1: (0.0739) | Acc_1: (97.54%) (7616/7808)\n",
      "Epoch: 34 | Batch_idx: 70 |  Loss_1: (0.0753) | Acc_1: (97.45%) (8856/9088)\n",
      "Epoch: 34 | Batch_idx: 80 |  Loss_1: (0.0761) | Acc_1: (97.42%) (10101/10368)\n",
      "Epoch: 34 | Batch_idx: 90 |  Loss_1: (0.0760) | Acc_1: (97.42%) (11348/11648)\n",
      "Epoch: 34 | Batch_idx: 100 |  Loss_1: (0.0763) | Acc_1: (97.39%) (12591/12928)\n",
      "Epoch: 34 | Batch_idx: 110 |  Loss_1: (0.0750) | Acc_1: (97.41%) (13840/14208)\n",
      "Epoch: 34 | Batch_idx: 120 |  Loss_1: (0.0748) | Acc_1: (97.41%) (15087/15488)\n",
      "Epoch: 34 | Batch_idx: 130 |  Loss_1: (0.0759) | Acc_1: (97.36%) (16326/16768)\n",
      "Epoch: 34 | Batch_idx: 140 |  Loss_1: (0.0765) | Acc_1: (97.35%) (17569/18048)\n",
      "Epoch: 34 | Batch_idx: 150 |  Loss_1: (0.0760) | Acc_1: (97.36%) (18818/19328)\n",
      "Epoch: 34 | Batch_idx: 160 |  Loss_1: (0.0759) | Acc_1: (97.34%) (20060/20608)\n",
      "Epoch: 34 | Batch_idx: 170 |  Loss_1: (0.0767) | Acc_1: (97.31%) (21300/21888)\n",
      "Epoch: 34 | Batch_idx: 180 |  Loss_1: (0.0767) | Acc_1: (97.30%) (22543/23168)\n",
      "Epoch: 34 | Batch_idx: 190 |  Loss_1: (0.0756) | Acc_1: (97.35%) (23799/24448)\n",
      "Epoch: 34 | Batch_idx: 200 |  Loss_1: (0.0757) | Acc_1: (97.33%) (25042/25728)\n",
      "Epoch: 34 | Batch_idx: 210 |  Loss_1: (0.0761) | Acc_1: (97.33%) (26286/27008)\n",
      "Epoch: 34 | Batch_idx: 220 |  Loss_1: (0.0765) | Acc_1: (97.33%) (27534/28288)\n",
      "Epoch: 34 | Batch_idx: 230 |  Loss_1: (0.0764) | Acc_1: (97.33%) (28778/29568)\n",
      "Epoch: 34 | Batch_idx: 240 |  Loss_1: (0.0772) | Acc_1: (97.30%) (30014/30848)\n",
      "Epoch: 34 | Batch_idx: 250 |  Loss_1: (0.0776) | Acc_1: (97.29%) (31258/32128)\n",
      "Epoch: 34 | Batch_idx: 260 |  Loss_1: (0.0771) | Acc_1: (97.30%) (32507/33408)\n",
      "Epoch: 34 | Batch_idx: 270 |  Loss_1: (0.0773) | Acc_1: (97.29%) (33748/34688)\n",
      "Epoch: 34 | Batch_idx: 280 |  Loss_1: (0.0773) | Acc_1: (97.27%) (34986/35968)\n",
      "Epoch: 34 | Batch_idx: 290 |  Loss_1: (0.0774) | Acc_1: (97.26%) (36226/37248)\n",
      "Epoch: 34 | Batch_idx: 300 |  Loss_1: (0.0774) | Acc_1: (97.26%) (37473/38528)\n",
      "Epoch: 34 | Batch_idx: 310 |  Loss_1: (0.0772) | Acc_1: (97.27%) (38722/39808)\n",
      "Epoch: 34 | Batch_idx: 320 |  Loss_1: (0.0768) | Acc_1: (97.28%) (39972/41088)\n",
      "Epoch: 34 | Batch_idx: 330 |  Loss_1: (0.0775) | Acc_1: (97.26%) (41209/42368)\n",
      "Epoch: 34 | Batch_idx: 340 |  Loss_1: (0.0778) | Acc_1: (97.26%) (42454/43648)\n",
      "Epoch: 34 | Batch_idx: 350 |  Loss_1: (0.0780) | Acc_1: (97.27%) (43701/44928)\n",
      "Epoch: 34 | Batch_idx: 360 |  Loss_1: (0.0781) | Acc_1: (97.26%) (44940/46208)\n",
      "Epoch: 34 | Batch_idx: 370 |  Loss_1: (0.0786) | Acc_1: (97.23%) (46172/47488)\n",
      "Epoch: 34 | Batch_idx: 380 |  Loss_1: (0.0789) | Acc_1: (97.21%) (47408/48768)\n",
      "Epoch: 34 | Batch_idx: 390 |  Loss_1: (0.0788) | Acc_1: (97.22%) (48609/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4854) | Acc: (88.94%) (8894/10000)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.0845) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.0769) | Acc_1: (97.16%) (1368/1408)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.0649) | Acc_1: (97.77%) (2628/2688)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.0641) | Acc_1: (97.88%) (3884/3968)\n",
      "Epoch: 35 | Batch_idx: 40 |  Loss_1: (0.0630) | Acc_1: (97.87%) (5136/5248)\n",
      "Epoch: 35 | Batch_idx: 50 |  Loss_1: (0.0633) | Acc_1: (97.76%) (6382/6528)\n",
      "Epoch: 35 | Batch_idx: 60 |  Loss_1: (0.0625) | Acc_1: (97.78%) (7635/7808)\n",
      "Epoch: 35 | Batch_idx: 70 |  Loss_1: (0.0651) | Acc_1: (97.68%) (8877/9088)\n",
      "Epoch: 35 | Batch_idx: 80 |  Loss_1: (0.0662) | Acc_1: (97.66%) (10125/10368)\n",
      "Epoch: 35 | Batch_idx: 90 |  Loss_1: (0.0652) | Acc_1: (97.65%) (11374/11648)\n",
      "Epoch: 35 | Batch_idx: 100 |  Loss_1: (0.0660) | Acc_1: (97.65%) (12624/12928)\n",
      "Epoch: 35 | Batch_idx: 110 |  Loss_1: (0.0679) | Acc_1: (97.56%) (13861/14208)\n",
      "Epoch: 35 | Batch_idx: 120 |  Loss_1: (0.0686) | Acc_1: (97.57%) (15111/15488)\n",
      "Epoch: 35 | Batch_idx: 130 |  Loss_1: (0.0684) | Acc_1: (97.58%) (16363/16768)\n",
      "Epoch: 35 | Batch_idx: 140 |  Loss_1: (0.0687) | Acc_1: (97.60%) (17615/18048)\n",
      "Epoch: 35 | Batch_idx: 150 |  Loss_1: (0.0690) | Acc_1: (97.57%) (18859/19328)\n",
      "Epoch: 35 | Batch_idx: 160 |  Loss_1: (0.0692) | Acc_1: (97.55%) (20103/20608)\n",
      "Epoch: 35 | Batch_idx: 170 |  Loss_1: (0.0697) | Acc_1: (97.53%) (21348/21888)\n",
      "Epoch: 35 | Batch_idx: 180 |  Loss_1: (0.0702) | Acc_1: (97.53%) (22596/23168)\n",
      "Epoch: 35 | Batch_idx: 190 |  Loss_1: (0.0702) | Acc_1: (97.52%) (23841/24448)\n",
      "Epoch: 35 | Batch_idx: 200 |  Loss_1: (0.0701) | Acc_1: (97.52%) (25089/25728)\n",
      "Epoch: 35 | Batch_idx: 210 |  Loss_1: (0.0709) | Acc_1: (97.46%) (26323/27008)\n",
      "Epoch: 35 | Batch_idx: 220 |  Loss_1: (0.0715) | Acc_1: (97.45%) (27566/28288)\n",
      "Epoch: 35 | Batch_idx: 230 |  Loss_1: (0.0717) | Acc_1: (97.44%) (28812/29568)\n",
      "Epoch: 35 | Batch_idx: 240 |  Loss_1: (0.0720) | Acc_1: (97.42%) (30053/30848)\n",
      "Epoch: 35 | Batch_idx: 250 |  Loss_1: (0.0723) | Acc_1: (97.42%) (31300/32128)\n",
      "Epoch: 35 | Batch_idx: 260 |  Loss_1: (0.0716) | Acc_1: (97.46%) (32558/33408)\n",
      "Epoch: 35 | Batch_idx: 270 |  Loss_1: (0.0717) | Acc_1: (97.46%) (33808/34688)\n",
      "Epoch: 35 | Batch_idx: 280 |  Loss_1: (0.0716) | Acc_1: (97.47%) (35058/35968)\n",
      "Epoch: 35 | Batch_idx: 290 |  Loss_1: (0.0713) | Acc_1: (97.49%) (36312/37248)\n",
      "Epoch: 35 | Batch_idx: 300 |  Loss_1: (0.0713) | Acc_1: (97.48%) (37558/38528)\n",
      "Epoch: 35 | Batch_idx: 310 |  Loss_1: (0.0714) | Acc_1: (97.47%) (38801/39808)\n",
      "Epoch: 35 | Batch_idx: 320 |  Loss_1: (0.0716) | Acc_1: (97.46%) (40043/41088)\n",
      "Epoch: 35 | Batch_idx: 330 |  Loss_1: (0.0714) | Acc_1: (97.47%) (41297/42368)\n",
      "Epoch: 35 | Batch_idx: 340 |  Loss_1: (0.0710) | Acc_1: (97.48%) (42549/43648)\n",
      "Epoch: 35 | Batch_idx: 350 |  Loss_1: (0.0710) | Acc_1: (97.48%) (43797/44928)\n",
      "Epoch: 35 | Batch_idx: 360 |  Loss_1: (0.0711) | Acc_1: (97.48%) (45045/46208)\n",
      "Epoch: 35 | Batch_idx: 370 |  Loss_1: (0.0712) | Acc_1: (97.48%) (46291/47488)\n",
      "Epoch: 35 | Batch_idx: 380 |  Loss_1: (0.0716) | Acc_1: (97.47%) (47532/48768)\n",
      "Epoch: 35 | Batch_idx: 390 |  Loss_1: (0.0716) | Acc_1: (97.46%) (48730/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4873) | Acc: (88.88%) (8888/10000)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.0279) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.0601) | Acc_1: (98.01%) (1380/1408)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.0654) | Acc_1: (97.77%) (2628/2688)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.0635) | Acc_1: (97.78%) (3880/3968)\n",
      "Epoch: 36 | Batch_idx: 40 |  Loss_1: (0.0624) | Acc_1: (97.85%) (5135/5248)\n",
      "Epoch: 36 | Batch_idx: 50 |  Loss_1: (0.0593) | Acc_1: (97.96%) (6395/6528)\n",
      "Epoch: 36 | Batch_idx: 60 |  Loss_1: (0.0606) | Acc_1: (97.87%) (7642/7808)\n",
      "Epoch: 36 | Batch_idx: 70 |  Loss_1: (0.0602) | Acc_1: (97.88%) (8895/9088)\n",
      "Epoch: 36 | Batch_idx: 80 |  Loss_1: (0.0606) | Acc_1: (97.84%) (10144/10368)\n",
      "Epoch: 36 | Batch_idx: 90 |  Loss_1: (0.0602) | Acc_1: (97.85%) (11397/11648)\n",
      "Epoch: 36 | Batch_idx: 100 |  Loss_1: (0.0585) | Acc_1: (97.88%) (12654/12928)\n",
      "Epoch: 36 | Batch_idx: 110 |  Loss_1: (0.0583) | Acc_1: (97.90%) (13910/14208)\n",
      "Epoch: 36 | Batch_idx: 120 |  Loss_1: (0.0580) | Acc_1: (97.91%) (15165/15488)\n",
      "Epoch: 36 | Batch_idx: 130 |  Loss_1: (0.0586) | Acc_1: (97.87%) (16411/16768)\n",
      "Epoch: 36 | Batch_idx: 140 |  Loss_1: (0.0595) | Acc_1: (97.87%) (17664/18048)\n",
      "Epoch: 36 | Batch_idx: 150 |  Loss_1: (0.0585) | Acc_1: (97.89%) (18921/19328)\n",
      "Epoch: 36 | Batch_idx: 160 |  Loss_1: (0.0586) | Acc_1: (97.89%) (20174/20608)\n",
      "Epoch: 36 | Batch_idx: 170 |  Loss_1: (0.0593) | Acc_1: (97.85%) (21418/21888)\n",
      "Epoch: 36 | Batch_idx: 180 |  Loss_1: (0.0600) | Acc_1: (97.81%) (22661/23168)\n",
      "Epoch: 36 | Batch_idx: 190 |  Loss_1: (0.0610) | Acc_1: (97.79%) (23908/24448)\n",
      "Epoch: 36 | Batch_idx: 200 |  Loss_1: (0.0619) | Acc_1: (97.77%) (25154/25728)\n",
      "Epoch: 36 | Batch_idx: 210 |  Loss_1: (0.0626) | Acc_1: (97.76%) (26402/27008)\n",
      "Epoch: 36 | Batch_idx: 220 |  Loss_1: (0.0630) | Acc_1: (97.75%) (27652/28288)\n",
      "Epoch: 36 | Batch_idx: 230 |  Loss_1: (0.0635) | Acc_1: (97.74%) (28900/29568)\n",
      "Epoch: 36 | Batch_idx: 240 |  Loss_1: (0.0635) | Acc_1: (97.74%) (30152/30848)\n",
      "Epoch: 36 | Batch_idx: 250 |  Loss_1: (0.0632) | Acc_1: (97.75%) (31405/32128)\n",
      "Epoch: 36 | Batch_idx: 260 |  Loss_1: (0.0638) | Acc_1: (97.73%) (32650/33408)\n",
      "Epoch: 36 | Batch_idx: 270 |  Loss_1: (0.0644) | Acc_1: (97.71%) (33893/34688)\n",
      "Epoch: 36 | Batch_idx: 280 |  Loss_1: (0.0652) | Acc_1: (97.70%) (35140/35968)\n",
      "Epoch: 36 | Batch_idx: 290 |  Loss_1: (0.0658) | Acc_1: (97.66%) (36377/37248)\n",
      "Epoch: 36 | Batch_idx: 300 |  Loss_1: (0.0654) | Acc_1: (97.67%) (37632/38528)\n",
      "Epoch: 36 | Batch_idx: 310 |  Loss_1: (0.0656) | Acc_1: (97.67%) (38882/39808)\n",
      "Epoch: 36 | Batch_idx: 320 |  Loss_1: (0.0659) | Acc_1: (97.67%) (40129/41088)\n",
      "Epoch: 36 | Batch_idx: 330 |  Loss_1: (0.0658) | Acc_1: (97.67%) (41379/42368)\n",
      "Epoch: 36 | Batch_idx: 340 |  Loss_1: (0.0661) | Acc_1: (97.66%) (42627/43648)\n",
      "Epoch: 36 | Batch_idx: 350 |  Loss_1: (0.0661) | Acc_1: (97.66%) (43875/44928)\n",
      "Epoch: 36 | Batch_idx: 360 |  Loss_1: (0.0663) | Acc_1: (97.65%) (45120/46208)\n",
      "Epoch: 36 | Batch_idx: 370 |  Loss_1: (0.0662) | Acc_1: (97.65%) (46370/47488)\n",
      "Epoch: 36 | Batch_idx: 380 |  Loss_1: (0.0663) | Acc_1: (97.64%) (47618/48768)\n",
      "Epoch: 36 | Batch_idx: 390 |  Loss_1: (0.0666) | Acc_1: (97.63%) (48815/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4888) | Acc: (89.26%) (8926/10000)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.0481) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.0573) | Acc_1: (97.94%) (1379/1408)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.0635) | Acc_1: (97.77%) (2628/2688)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.0691) | Acc_1: (97.68%) (3876/3968)\n",
      "Epoch: 37 | Batch_idx: 40 |  Loss_1: (0.0717) | Acc_1: (97.56%) (5120/5248)\n",
      "Epoch: 37 | Batch_idx: 50 |  Loss_1: (0.0725) | Acc_1: (97.52%) (6366/6528)\n",
      "Epoch: 37 | Batch_idx: 60 |  Loss_1: (0.0716) | Acc_1: (97.54%) (7616/7808)\n",
      "Epoch: 37 | Batch_idx: 70 |  Loss_1: (0.0687) | Acc_1: (97.69%) (8878/9088)\n",
      "Epoch: 37 | Batch_idx: 80 |  Loss_1: (0.0698) | Acc_1: (97.67%) (10126/10368)\n",
      "Epoch: 37 | Batch_idx: 90 |  Loss_1: (0.0680) | Acc_1: (97.67%) (11377/11648)\n",
      "Epoch: 37 | Batch_idx: 100 |  Loss_1: (0.0660) | Acc_1: (97.74%) (12636/12928)\n",
      "Epoch: 37 | Batch_idx: 110 |  Loss_1: (0.0656) | Acc_1: (97.73%) (13886/14208)\n",
      "Epoch: 37 | Batch_idx: 120 |  Loss_1: (0.0650) | Acc_1: (97.75%) (15139/15488)\n",
      "Epoch: 37 | Batch_idx: 130 |  Loss_1: (0.0652) | Acc_1: (97.74%) (16389/16768)\n",
      "Epoch: 37 | Batch_idx: 140 |  Loss_1: (0.0672) | Acc_1: (97.62%) (17619/18048)\n",
      "Epoch: 37 | Batch_idx: 150 |  Loss_1: (0.0662) | Acc_1: (97.65%) (18873/19328)\n",
      "Epoch: 37 | Batch_idx: 160 |  Loss_1: (0.0671) | Acc_1: (97.63%) (20120/20608)\n",
      "Epoch: 37 | Batch_idx: 170 |  Loss_1: (0.0667) | Acc_1: (97.67%) (21377/21888)\n",
      "Epoch: 37 | Batch_idx: 180 |  Loss_1: (0.0676) | Acc_1: (97.62%) (22616/23168)\n",
      "Epoch: 37 | Batch_idx: 190 |  Loss_1: (0.0671) | Acc_1: (97.65%) (23874/24448)\n",
      "Epoch: 37 | Batch_idx: 200 |  Loss_1: (0.0671) | Acc_1: (97.66%) (25125/25728)\n",
      "Epoch: 37 | Batch_idx: 210 |  Loss_1: (0.0665) | Acc_1: (97.68%) (26381/27008)\n",
      "Epoch: 37 | Batch_idx: 220 |  Loss_1: (0.0667) | Acc_1: (97.67%) (27629/28288)\n",
      "Epoch: 37 | Batch_idx: 230 |  Loss_1: (0.0662) | Acc_1: (97.67%) (28878/29568)\n",
      "Epoch: 37 | Batch_idx: 240 |  Loss_1: (0.0659) | Acc_1: (97.68%) (30133/30848)\n",
      "Epoch: 37 | Batch_idx: 250 |  Loss_1: (0.0655) | Acc_1: (97.69%) (31386/32128)\n",
      "Epoch: 37 | Batch_idx: 260 |  Loss_1: (0.0650) | Acc_1: (97.71%) (32642/33408)\n",
      "Epoch: 37 | Batch_idx: 270 |  Loss_1: (0.0647) | Acc_1: (97.72%) (33897/34688)\n",
      "Epoch: 37 | Batch_idx: 280 |  Loss_1: (0.0647) | Acc_1: (97.72%) (35149/35968)\n",
      "Epoch: 37 | Batch_idx: 290 |  Loss_1: (0.0650) | Acc_1: (97.71%) (36396/37248)\n",
      "Epoch: 37 | Batch_idx: 300 |  Loss_1: (0.0650) | Acc_1: (97.72%) (37648/38528)\n",
      "Epoch: 37 | Batch_idx: 310 |  Loss_1: (0.0655) | Acc_1: (97.68%) (38884/39808)\n",
      "Epoch: 37 | Batch_idx: 320 |  Loss_1: (0.0663) | Acc_1: (97.66%) (40128/41088)\n",
      "Epoch: 37 | Batch_idx: 330 |  Loss_1: (0.0674) | Acc_1: (97.61%) (41357/42368)\n",
      "Epoch: 37 | Batch_idx: 340 |  Loss_1: (0.0683) | Acc_1: (97.58%) (42593/43648)\n",
      "Epoch: 37 | Batch_idx: 350 |  Loss_1: (0.0683) | Acc_1: (97.59%) (43845/44928)\n",
      "Epoch: 37 | Batch_idx: 360 |  Loss_1: (0.0681) | Acc_1: (97.60%) (45100/46208)\n",
      "Epoch: 37 | Batch_idx: 370 |  Loss_1: (0.0681) | Acc_1: (97.59%) (46342/47488)\n",
      "Epoch: 37 | Batch_idx: 380 |  Loss_1: (0.0679) | Acc_1: (97.60%) (47597/48768)\n",
      "Epoch: 37 | Batch_idx: 390 |  Loss_1: (0.0683) | Acc_1: (97.59%) (48793/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4991) | Acc: (88.74%) (8874/10000)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.0386) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.0569) | Acc_1: (98.15%) (1382/1408)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.0557) | Acc_1: (98.18%) (2639/2688)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.0534) | Acc_1: (98.19%) (3896/3968)\n",
      "Epoch: 38 | Batch_idx: 40 |  Loss_1: (0.0503) | Acc_1: (98.21%) (5154/5248)\n",
      "Epoch: 38 | Batch_idx: 50 |  Loss_1: (0.0496) | Acc_1: (98.21%) (6411/6528)\n",
      "Epoch: 38 | Batch_idx: 60 |  Loss_1: (0.0504) | Acc_1: (98.10%) (7660/7808)\n",
      "Epoch: 38 | Batch_idx: 70 |  Loss_1: (0.0513) | Acc_1: (98.06%) (8912/9088)\n",
      "Epoch: 38 | Batch_idx: 80 |  Loss_1: (0.0507) | Acc_1: (98.12%) (10173/10368)\n",
      "Epoch: 38 | Batch_idx: 90 |  Loss_1: (0.0521) | Acc_1: (98.09%) (11425/11648)\n",
      "Epoch: 38 | Batch_idx: 100 |  Loss_1: (0.0512) | Acc_1: (98.14%) (12688/12928)\n",
      "Epoch: 38 | Batch_idx: 110 |  Loss_1: (0.0511) | Acc_1: (98.15%) (13945/14208)\n",
      "Epoch: 38 | Batch_idx: 120 |  Loss_1: (0.0520) | Acc_1: (98.16%) (15203/15488)\n",
      "Epoch: 38 | Batch_idx: 130 |  Loss_1: (0.0524) | Acc_1: (98.15%) (16458/16768)\n",
      "Epoch: 38 | Batch_idx: 140 |  Loss_1: (0.0524) | Acc_1: (98.14%) (17713/18048)\n",
      "Epoch: 38 | Batch_idx: 150 |  Loss_1: (0.0535) | Acc_1: (98.10%) (18961/19328)\n",
      "Epoch: 38 | Batch_idx: 160 |  Loss_1: (0.0546) | Acc_1: (98.06%) (20209/20608)\n",
      "Epoch: 38 | Batch_idx: 170 |  Loss_1: (0.0557) | Acc_1: (98.01%) (21452/21888)\n",
      "Epoch: 38 | Batch_idx: 180 |  Loss_1: (0.0560) | Acc_1: (98.00%) (22705/23168)\n",
      "Epoch: 38 | Batch_idx: 190 |  Loss_1: (0.0566) | Acc_1: (97.98%) (23955/24448)\n",
      "Epoch: 38 | Batch_idx: 200 |  Loss_1: (0.0578) | Acc_1: (97.94%) (25197/25728)\n",
      "Epoch: 38 | Batch_idx: 210 |  Loss_1: (0.0568) | Acc_1: (97.97%) (26461/27008)\n",
      "Epoch: 38 | Batch_idx: 220 |  Loss_1: (0.0574) | Acc_1: (97.95%) (27707/28288)\n",
      "Epoch: 38 | Batch_idx: 230 |  Loss_1: (0.0574) | Acc_1: (97.95%) (28963/29568)\n",
      "Epoch: 38 | Batch_idx: 240 |  Loss_1: (0.0573) | Acc_1: (97.97%) (30222/30848)\n",
      "Epoch: 38 | Batch_idx: 250 |  Loss_1: (0.0565) | Acc_1: (98.00%) (31487/32128)\n",
      "Epoch: 38 | Batch_idx: 260 |  Loss_1: (0.0566) | Acc_1: (98.00%) (32741/33408)\n",
      "Epoch: 38 | Batch_idx: 270 |  Loss_1: (0.0563) | Acc_1: (98.01%) (33998/34688)\n",
      "Epoch: 38 | Batch_idx: 280 |  Loss_1: (0.0565) | Acc_1: (98.01%) (35251/35968)\n",
      "Epoch: 38 | Batch_idx: 290 |  Loss_1: (0.0565) | Acc_1: (98.02%) (36509/37248)\n",
      "Epoch: 38 | Batch_idx: 300 |  Loss_1: (0.0564) | Acc_1: (98.02%) (37764/38528)\n",
      "Epoch: 38 | Batch_idx: 310 |  Loss_1: (0.0564) | Acc_1: (98.01%) (39017/39808)\n",
      "Epoch: 38 | Batch_idx: 320 |  Loss_1: (0.0564) | Acc_1: (98.01%) (40271/41088)\n",
      "Epoch: 38 | Batch_idx: 330 |  Loss_1: (0.0562) | Acc_1: (98.02%) (41527/42368)\n",
      "Epoch: 38 | Batch_idx: 340 |  Loss_1: (0.0574) | Acc_1: (97.97%) (42762/43648)\n",
      "Epoch: 38 | Batch_idx: 350 |  Loss_1: (0.0583) | Acc_1: (97.95%) (44007/44928)\n",
      "Epoch: 38 | Batch_idx: 360 |  Loss_1: (0.0589) | Acc_1: (97.92%) (45246/46208)\n",
      "Epoch: 38 | Batch_idx: 370 |  Loss_1: (0.0591) | Acc_1: (97.90%) (46491/47488)\n",
      "Epoch: 38 | Batch_idx: 380 |  Loss_1: (0.0595) | Acc_1: (97.89%) (47739/48768)\n",
      "Epoch: 38 | Batch_idx: 390 |  Loss_1: (0.0600) | Acc_1: (97.88%) (48939/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4579) | Acc: (89.83%) (8983/10000)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.0220) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.0849) | Acc_1: (97.02%) (1366/1408)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.0694) | Acc_1: (97.58%) (2623/2688)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.0737) | Acc_1: (97.40%) (3865/3968)\n",
      "Epoch: 39 | Batch_idx: 40 |  Loss_1: (0.0703) | Acc_1: (97.56%) (5120/5248)\n",
      "Epoch: 39 | Batch_idx: 50 |  Loss_1: (0.0673) | Acc_1: (97.66%) (6375/6528)\n",
      "Epoch: 39 | Batch_idx: 60 |  Loss_1: (0.0652) | Acc_1: (97.69%) (7628/7808)\n",
      "Epoch: 39 | Batch_idx: 70 |  Loss_1: (0.0644) | Acc_1: (97.69%) (8878/9088)\n",
      "Epoch: 39 | Batch_idx: 80 |  Loss_1: (0.0621) | Acc_1: (97.75%) (10135/10368)\n",
      "Epoch: 39 | Batch_idx: 90 |  Loss_1: (0.0612) | Acc_1: (97.78%) (11389/11648)\n",
      "Epoch: 39 | Batch_idx: 100 |  Loss_1: (0.0603) | Acc_1: (97.81%) (12645/12928)\n",
      "Epoch: 39 | Batch_idx: 110 |  Loss_1: (0.0597) | Acc_1: (97.85%) (13902/14208)\n",
      "Epoch: 39 | Batch_idx: 120 |  Loss_1: (0.0600) | Acc_1: (97.83%) (15152/15488)\n",
      "Epoch: 39 | Batch_idx: 130 |  Loss_1: (0.0602) | Acc_1: (97.82%) (16403/16768)\n",
      "Epoch: 39 | Batch_idx: 140 |  Loss_1: (0.0591) | Acc_1: (97.87%) (17663/18048)\n",
      "Epoch: 39 | Batch_idx: 150 |  Loss_1: (0.0585) | Acc_1: (97.89%) (18921/19328)\n",
      "Epoch: 39 | Batch_idx: 160 |  Loss_1: (0.0587) | Acc_1: (97.89%) (20173/20608)\n",
      "Epoch: 39 | Batch_idx: 170 |  Loss_1: (0.0587) | Acc_1: (97.91%) (21431/21888)\n",
      "Epoch: 39 | Batch_idx: 180 |  Loss_1: (0.0585) | Acc_1: (97.91%) (22683/23168)\n",
      "Epoch: 39 | Batch_idx: 190 |  Loss_1: (0.0587) | Acc_1: (97.92%) (23940/24448)\n",
      "Epoch: 39 | Batch_idx: 200 |  Loss_1: (0.0577) | Acc_1: (97.95%) (25201/25728)\n",
      "Epoch: 39 | Batch_idx: 210 |  Loss_1: (0.0579) | Acc_1: (97.93%) (26450/27008)\n",
      "Epoch: 39 | Batch_idx: 220 |  Loss_1: (0.0573) | Acc_1: (97.95%) (27709/28288)\n",
      "Epoch: 39 | Batch_idx: 230 |  Loss_1: (0.0574) | Acc_1: (97.95%) (28962/29568)\n",
      "Epoch: 39 | Batch_idx: 240 |  Loss_1: (0.0575) | Acc_1: (97.95%) (30215/30848)\n",
      "Epoch: 39 | Batch_idx: 250 |  Loss_1: (0.0577) | Acc_1: (97.94%) (31467/32128)\n",
      "Epoch: 39 | Batch_idx: 260 |  Loss_1: (0.0582) | Acc_1: (97.93%) (32717/33408)\n",
      "Epoch: 39 | Batch_idx: 270 |  Loss_1: (0.0584) | Acc_1: (97.92%) (33966/34688)\n",
      "Epoch: 39 | Batch_idx: 280 |  Loss_1: (0.0583) | Acc_1: (97.93%) (35225/35968)\n",
      "Epoch: 39 | Batch_idx: 290 |  Loss_1: (0.0586) | Acc_1: (97.91%) (36468/37248)\n",
      "Epoch: 39 | Batch_idx: 300 |  Loss_1: (0.0589) | Acc_1: (97.91%) (37721/38528)\n",
      "Epoch: 39 | Batch_idx: 310 |  Loss_1: (0.0597) | Acc_1: (97.89%) (38968/39808)\n",
      "Epoch: 39 | Batch_idx: 320 |  Loss_1: (0.0599) | Acc_1: (97.89%) (40219/41088)\n",
      "Epoch: 39 | Batch_idx: 330 |  Loss_1: (0.0605) | Acc_1: (97.88%) (41468/42368)\n",
      "Epoch: 39 | Batch_idx: 340 |  Loss_1: (0.0608) | Acc_1: (97.87%) (42719/43648)\n",
      "Epoch: 39 | Batch_idx: 350 |  Loss_1: (0.0611) | Acc_1: (97.86%) (43968/44928)\n",
      "Epoch: 39 | Batch_idx: 360 |  Loss_1: (0.0610) | Acc_1: (97.87%) (45225/46208)\n",
      "Epoch: 39 | Batch_idx: 370 |  Loss_1: (0.0607) | Acc_1: (97.88%) (46483/47488)\n",
      "Epoch: 39 | Batch_idx: 380 |  Loss_1: (0.0608) | Acc_1: (97.88%) (47734/48768)\n",
      "Epoch: 39 | Batch_idx: 390 |  Loss_1: (0.0607) | Acc_1: (97.88%) (48942/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4436) | Acc: (89.90%) (8990/10000)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.0362) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.0353) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.0507) | Acc_1: (98.36%) (2644/2688)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.0481) | Acc_1: (98.44%) (3906/3968)\n",
      "Epoch: 40 | Batch_idx: 40 |  Loss_1: (0.0525) | Acc_1: (98.30%) (5159/5248)\n",
      "Epoch: 40 | Batch_idx: 50 |  Loss_1: (0.0511) | Acc_1: (98.25%) (6414/6528)\n",
      "Epoch: 40 | Batch_idx: 60 |  Loss_1: (0.0508) | Acc_1: (98.28%) (7674/7808)\n",
      "Epoch: 40 | Batch_idx: 70 |  Loss_1: (0.0517) | Acc_1: (98.23%) (8927/9088)\n",
      "Epoch: 40 | Batch_idx: 80 |  Loss_1: (0.0522) | Acc_1: (98.19%) (10180/10368)\n",
      "Epoch: 40 | Batch_idx: 90 |  Loss_1: (0.0526) | Acc_1: (98.17%) (11435/11648)\n",
      "Epoch: 40 | Batch_idx: 100 |  Loss_1: (0.0549) | Acc_1: (98.10%) (12683/12928)\n",
      "Epoch: 40 | Batch_idx: 110 |  Loss_1: (0.0547) | Acc_1: (98.13%) (13942/14208)\n",
      "Epoch: 40 | Batch_idx: 120 |  Loss_1: (0.0545) | Acc_1: (98.15%) (15202/15488)\n",
      "Epoch: 40 | Batch_idx: 130 |  Loss_1: (0.0536) | Acc_1: (98.18%) (16462/16768)\n",
      "Epoch: 40 | Batch_idx: 140 |  Loss_1: (0.0524) | Acc_1: (98.19%) (17722/18048)\n",
      "Epoch: 40 | Batch_idx: 150 |  Loss_1: (0.0530) | Acc_1: (98.18%) (18976/19328)\n",
      "Epoch: 40 | Batch_idx: 160 |  Loss_1: (0.0534) | Acc_1: (98.17%) (20231/20608)\n",
      "Epoch: 40 | Batch_idx: 170 |  Loss_1: (0.0538) | Acc_1: (98.17%) (21487/21888)\n",
      "Epoch: 40 | Batch_idx: 180 |  Loss_1: (0.0540) | Acc_1: (98.14%) (22738/23168)\n",
      "Epoch: 40 | Batch_idx: 190 |  Loss_1: (0.0539) | Acc_1: (98.15%) (23995/24448)\n",
      "Epoch: 40 | Batch_idx: 200 |  Loss_1: (0.0540) | Acc_1: (98.14%) (25249/25728)\n",
      "Epoch: 40 | Batch_idx: 210 |  Loss_1: (0.0539) | Acc_1: (98.13%) (26504/27008)\n",
      "Epoch: 40 | Batch_idx: 220 |  Loss_1: (0.0538) | Acc_1: (98.13%) (27758/28288)\n",
      "Epoch: 40 | Batch_idx: 230 |  Loss_1: (0.0531) | Acc_1: (98.15%) (29022/29568)\n",
      "Epoch: 40 | Batch_idx: 240 |  Loss_1: (0.0532) | Acc_1: (98.14%) (30273/30848)\n",
      "Epoch: 40 | Batch_idx: 250 |  Loss_1: (0.0532) | Acc_1: (98.13%) (31526/32128)\n",
      "Epoch: 40 | Batch_idx: 260 |  Loss_1: (0.0532) | Acc_1: (98.13%) (32783/33408)\n",
      "Epoch: 40 | Batch_idx: 270 |  Loss_1: (0.0532) | Acc_1: (98.12%) (34037/34688)\n",
      "Epoch: 40 | Batch_idx: 280 |  Loss_1: (0.0528) | Acc_1: (98.13%) (35297/35968)\n",
      "Epoch: 40 | Batch_idx: 290 |  Loss_1: (0.0525) | Acc_1: (98.14%) (36555/37248)\n",
      "Epoch: 40 | Batch_idx: 300 |  Loss_1: (0.0534) | Acc_1: (98.10%) (37797/38528)\n",
      "Epoch: 40 | Batch_idx: 310 |  Loss_1: (0.0538) | Acc_1: (98.10%) (39051/39808)\n",
      "Epoch: 40 | Batch_idx: 320 |  Loss_1: (0.0539) | Acc_1: (98.10%) (40308/41088)\n",
      "Epoch: 40 | Batch_idx: 330 |  Loss_1: (0.0538) | Acc_1: (98.10%) (41565/42368)\n",
      "Epoch: 40 | Batch_idx: 340 |  Loss_1: (0.0540) | Acc_1: (98.09%) (42814/43648)\n",
      "Epoch: 40 | Batch_idx: 350 |  Loss_1: (0.0539) | Acc_1: (98.08%) (44067/44928)\n",
      "Epoch: 40 | Batch_idx: 360 |  Loss_1: (0.0536) | Acc_1: (98.09%) (45326/46208)\n",
      "Epoch: 40 | Batch_idx: 370 |  Loss_1: (0.0539) | Acc_1: (98.08%) (46577/47488)\n",
      "Epoch: 40 | Batch_idx: 380 |  Loss_1: (0.0541) | Acc_1: (98.08%) (47830/48768)\n",
      "Epoch: 40 | Batch_idx: 390 |  Loss_1: (0.0539) | Acc_1: (98.07%) (49035/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4689) | Acc: (89.87%) (8987/10000)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.0602) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.0413) | Acc_1: (98.58%) (1388/1408)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.0461) | Acc_1: (98.40%) (2645/2688)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.0488) | Acc_1: (98.21%) (3897/3968)\n",
      "Epoch: 41 | Batch_idx: 40 |  Loss_1: (0.0525) | Acc_1: (98.06%) (5146/5248)\n",
      "Epoch: 41 | Batch_idx: 50 |  Loss_1: (0.0508) | Acc_1: (98.18%) (6409/6528)\n",
      "Epoch: 41 | Batch_idx: 60 |  Loss_1: (0.0509) | Acc_1: (98.22%) (7669/7808)\n",
      "Epoch: 41 | Batch_idx: 70 |  Loss_1: (0.0492) | Acc_1: (98.27%) (8931/9088)\n",
      "Epoch: 41 | Batch_idx: 80 |  Loss_1: (0.0488) | Acc_1: (98.28%) (10190/10368)\n",
      "Epoch: 41 | Batch_idx: 90 |  Loss_1: (0.0482) | Acc_1: (98.33%) (11453/11648)\n",
      "Epoch: 41 | Batch_idx: 100 |  Loss_1: (0.0493) | Acc_1: (98.31%) (12710/12928)\n",
      "Epoch: 41 | Batch_idx: 110 |  Loss_1: (0.0478) | Acc_1: (98.34%) (13972/14208)\n",
      "Epoch: 41 | Batch_idx: 120 |  Loss_1: (0.0470) | Acc_1: (98.37%) (15236/15488)\n",
      "Epoch: 41 | Batch_idx: 130 |  Loss_1: (0.0458) | Acc_1: (98.42%) (16503/16768)\n",
      "Epoch: 41 | Batch_idx: 140 |  Loss_1: (0.0456) | Acc_1: (98.44%) (17767/18048)\n",
      "Epoch: 41 | Batch_idx: 150 |  Loss_1: (0.0458) | Acc_1: (98.41%) (19021/19328)\n",
      "Epoch: 41 | Batch_idx: 160 |  Loss_1: (0.0468) | Acc_1: (98.36%) (20270/20608)\n",
      "Epoch: 41 | Batch_idx: 170 |  Loss_1: (0.0473) | Acc_1: (98.34%) (21524/21888)\n",
      "Epoch: 41 | Batch_idx: 180 |  Loss_1: (0.0470) | Acc_1: (98.36%) (22788/23168)\n",
      "Epoch: 41 | Batch_idx: 190 |  Loss_1: (0.0471) | Acc_1: (98.37%) (24049/24448)\n",
      "Epoch: 41 | Batch_idx: 200 |  Loss_1: (0.0471) | Acc_1: (98.36%) (25307/25728)\n",
      "Epoch: 41 | Batch_idx: 210 |  Loss_1: (0.0476) | Acc_1: (98.36%) (26565/27008)\n",
      "Epoch: 41 | Batch_idx: 220 |  Loss_1: (0.0480) | Acc_1: (98.31%) (27811/28288)\n",
      "Epoch: 41 | Batch_idx: 230 |  Loss_1: (0.0486) | Acc_1: (98.30%) (29064/29568)\n",
      "Epoch: 41 | Batch_idx: 240 |  Loss_1: (0.0497) | Acc_1: (98.26%) (30310/30848)\n",
      "Epoch: 41 | Batch_idx: 250 |  Loss_1: (0.0503) | Acc_1: (98.23%) (31560/32128)\n",
      "Epoch: 41 | Batch_idx: 260 |  Loss_1: (0.0508) | Acc_1: (98.21%) (32809/33408)\n",
      "Epoch: 41 | Batch_idx: 270 |  Loss_1: (0.0514) | Acc_1: (98.19%) (34061/34688)\n",
      "Epoch: 41 | Batch_idx: 280 |  Loss_1: (0.0519) | Acc_1: (98.19%) (35317/35968)\n",
      "Epoch: 41 | Batch_idx: 290 |  Loss_1: (0.0520) | Acc_1: (98.18%) (36569/37248)\n",
      "Epoch: 41 | Batch_idx: 300 |  Loss_1: (0.0531) | Acc_1: (98.16%) (37818/38528)\n",
      "Epoch: 41 | Batch_idx: 310 |  Loss_1: (0.0530) | Acc_1: (98.15%) (39073/39808)\n",
      "Epoch: 41 | Batch_idx: 320 |  Loss_1: (0.0536) | Acc_1: (98.13%) (40320/41088)\n",
      "Epoch: 41 | Batch_idx: 330 |  Loss_1: (0.0533) | Acc_1: (98.13%) (41576/42368)\n",
      "Epoch: 41 | Batch_idx: 340 |  Loss_1: (0.0531) | Acc_1: (98.15%) (42839/43648)\n",
      "Epoch: 41 | Batch_idx: 350 |  Loss_1: (0.0531) | Acc_1: (98.15%) (44099/44928)\n",
      "Epoch: 41 | Batch_idx: 360 |  Loss_1: (0.0538) | Acc_1: (98.13%) (45346/46208)\n",
      "Epoch: 41 | Batch_idx: 370 |  Loss_1: (0.0541) | Acc_1: (98.12%) (46595/47488)\n",
      "Epoch: 41 | Batch_idx: 380 |  Loss_1: (0.0538) | Acc_1: (98.13%) (47854/48768)\n",
      "Epoch: 41 | Batch_idx: 390 |  Loss_1: (0.0537) | Acc_1: (98.13%) (49063/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4833) | Acc: (89.30%) (8930/10000)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.0189) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.0384) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.0459) | Acc_1: (98.44%) (2646/2688)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.0462) | Acc_1: (98.51%) (3909/3968)\n",
      "Epoch: 42 | Batch_idx: 40 |  Loss_1: (0.0480) | Acc_1: (98.46%) (5167/5248)\n",
      "Epoch: 42 | Batch_idx: 50 |  Loss_1: (0.0466) | Acc_1: (98.59%) (6436/6528)\n",
      "Epoch: 42 | Batch_idx: 60 |  Loss_1: (0.0443) | Acc_1: (98.64%) (7702/7808)\n",
      "Epoch: 42 | Batch_idx: 70 |  Loss_1: (0.0447) | Acc_1: (98.57%) (8958/9088)\n",
      "Epoch: 42 | Batch_idx: 80 |  Loss_1: (0.0473) | Acc_1: (98.53%) (10216/10368)\n",
      "Epoch: 42 | Batch_idx: 90 |  Loss_1: (0.0468) | Acc_1: (98.51%) (11475/11648)\n",
      "Epoch: 42 | Batch_idx: 100 |  Loss_1: (0.0464) | Acc_1: (98.50%) (12734/12928)\n",
      "Epoch: 42 | Batch_idx: 110 |  Loss_1: (0.0455) | Acc_1: (98.54%) (14000/14208)\n",
      "Epoch: 42 | Batch_idx: 120 |  Loss_1: (0.0445) | Acc_1: (98.57%) (15266/15488)\n",
      "Epoch: 42 | Batch_idx: 130 |  Loss_1: (0.0437) | Acc_1: (98.57%) (16528/16768)\n",
      "Epoch: 42 | Batch_idx: 140 |  Loss_1: (0.0443) | Acc_1: (98.51%) (17779/18048)\n",
      "Epoch: 42 | Batch_idx: 150 |  Loss_1: (0.0447) | Acc_1: (98.47%) (19033/19328)\n",
      "Epoch: 42 | Batch_idx: 160 |  Loss_1: (0.0443) | Acc_1: (98.50%) (20299/20608)\n",
      "Epoch: 42 | Batch_idx: 170 |  Loss_1: (0.0441) | Acc_1: (98.50%) (21560/21888)\n",
      "Epoch: 42 | Batch_idx: 180 |  Loss_1: (0.0460) | Acc_1: (98.44%) (22807/23168)\n",
      "Epoch: 42 | Batch_idx: 190 |  Loss_1: (0.0470) | Acc_1: (98.42%) (24061/24448)\n",
      "Epoch: 42 | Batch_idx: 200 |  Loss_1: (0.0474) | Acc_1: (98.40%) (25316/25728)\n",
      "Epoch: 42 | Batch_idx: 210 |  Loss_1: (0.0487) | Acc_1: (98.35%) (26562/27008)\n",
      "Epoch: 42 | Batch_idx: 220 |  Loss_1: (0.0487) | Acc_1: (98.34%) (27819/28288)\n",
      "Epoch: 42 | Batch_idx: 230 |  Loss_1: (0.0487) | Acc_1: (98.33%) (29074/29568)\n",
      "Epoch: 42 | Batch_idx: 240 |  Loss_1: (0.0487) | Acc_1: (98.33%) (30333/30848)\n",
      "Epoch: 42 | Batch_idx: 250 |  Loss_1: (0.0491) | Acc_1: (98.30%) (31583/32128)\n",
      "Epoch: 42 | Batch_idx: 260 |  Loss_1: (0.0487) | Acc_1: (98.31%) (32843/33408)\n",
      "Epoch: 42 | Batch_idx: 270 |  Loss_1: (0.0487) | Acc_1: (98.31%) (34102/34688)\n",
      "Epoch: 42 | Batch_idx: 280 |  Loss_1: (0.0488) | Acc_1: (98.32%) (35362/35968)\n",
      "Epoch: 42 | Batch_idx: 290 |  Loss_1: (0.0486) | Acc_1: (98.32%) (36621/37248)\n",
      "Epoch: 42 | Batch_idx: 300 |  Loss_1: (0.0490) | Acc_1: (98.29%) (37870/38528)\n",
      "Epoch: 42 | Batch_idx: 310 |  Loss_1: (0.0490) | Acc_1: (98.30%) (39131/39808)\n",
      "Epoch: 42 | Batch_idx: 320 |  Loss_1: (0.0490) | Acc_1: (98.30%) (40388/41088)\n",
      "Epoch: 42 | Batch_idx: 330 |  Loss_1: (0.0487) | Acc_1: (98.31%) (41654/42368)\n",
      "Epoch: 42 | Batch_idx: 340 |  Loss_1: (0.0490) | Acc_1: (98.31%) (42910/43648)\n",
      "Epoch: 42 | Batch_idx: 350 |  Loss_1: (0.0487) | Acc_1: (98.32%) (44174/44928)\n",
      "Epoch: 42 | Batch_idx: 360 |  Loss_1: (0.0488) | Acc_1: (98.31%) (45428/46208)\n",
      "Epoch: 42 | Batch_idx: 370 |  Loss_1: (0.0492) | Acc_1: (98.30%) (46680/47488)\n",
      "Epoch: 42 | Batch_idx: 380 |  Loss_1: (0.0500) | Acc_1: (98.26%) (47920/48768)\n",
      "Epoch: 42 | Batch_idx: 390 |  Loss_1: (0.0503) | Acc_1: (98.25%) (49123/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4766) | Acc: (90.02%) (9002/10000)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.0623) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.0559) | Acc_1: (98.15%) (1382/1408)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.0608) | Acc_1: (98.03%) (2635/2688)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.0582) | Acc_1: (97.88%) (3884/3968)\n",
      "Epoch: 43 | Batch_idx: 40 |  Loss_1: (0.0560) | Acc_1: (98.02%) (5144/5248)\n",
      "Epoch: 43 | Batch_idx: 50 |  Loss_1: (0.0531) | Acc_1: (98.21%) (6411/6528)\n",
      "Epoch: 43 | Batch_idx: 60 |  Loss_1: (0.0542) | Acc_1: (98.14%) (7663/7808)\n",
      "Epoch: 43 | Batch_idx: 70 |  Loss_1: (0.0535) | Acc_1: (98.17%) (8922/9088)\n",
      "Epoch: 43 | Batch_idx: 80 |  Loss_1: (0.0519) | Acc_1: (98.21%) (10182/10368)\n",
      "Epoch: 43 | Batch_idx: 90 |  Loss_1: (0.0505) | Acc_1: (98.26%) (11445/11648)\n",
      "Epoch: 43 | Batch_idx: 100 |  Loss_1: (0.0501) | Acc_1: (98.24%) (12701/12928)\n",
      "Epoch: 43 | Batch_idx: 110 |  Loss_1: (0.0488) | Acc_1: (98.29%) (13965/14208)\n",
      "Epoch: 43 | Batch_idx: 120 |  Loss_1: (0.0482) | Acc_1: (98.32%) (15228/15488)\n",
      "Epoch: 43 | Batch_idx: 130 |  Loss_1: (0.0478) | Acc_1: (98.35%) (16492/16768)\n",
      "Epoch: 43 | Batch_idx: 140 |  Loss_1: (0.0466) | Acc_1: (98.40%) (17760/18048)\n",
      "Epoch: 43 | Batch_idx: 150 |  Loss_1: (0.0470) | Acc_1: (98.37%) (19012/19328)\n",
      "Epoch: 43 | Batch_idx: 160 |  Loss_1: (0.0462) | Acc_1: (98.39%) (20276/20608)\n",
      "Epoch: 43 | Batch_idx: 170 |  Loss_1: (0.0457) | Acc_1: (98.41%) (21539/21888)\n",
      "Epoch: 43 | Batch_idx: 180 |  Loss_1: (0.0463) | Acc_1: (98.37%) (22791/23168)\n",
      "Epoch: 43 | Batch_idx: 190 |  Loss_1: (0.0459) | Acc_1: (98.38%) (24051/24448)\n",
      "Epoch: 43 | Batch_idx: 200 |  Loss_1: (0.0458) | Acc_1: (98.39%) (25313/25728)\n",
      "Epoch: 43 | Batch_idx: 210 |  Loss_1: (0.0454) | Acc_1: (98.40%) (26577/27008)\n",
      "Epoch: 43 | Batch_idx: 220 |  Loss_1: (0.0449) | Acc_1: (98.41%) (27839/28288)\n",
      "Epoch: 43 | Batch_idx: 230 |  Loss_1: (0.0453) | Acc_1: (98.39%) (29093/29568)\n",
      "Epoch: 43 | Batch_idx: 240 |  Loss_1: (0.0462) | Acc_1: (98.35%) (30340/30848)\n",
      "Epoch: 43 | Batch_idx: 250 |  Loss_1: (0.0470) | Acc_1: (98.32%) (31588/32128)\n",
      "Epoch: 43 | Batch_idx: 260 |  Loss_1: (0.0476) | Acc_1: (98.29%) (32838/33408)\n",
      "Epoch: 43 | Batch_idx: 270 |  Loss_1: (0.0477) | Acc_1: (98.29%) (34094/34688)\n",
      "Epoch: 43 | Batch_idx: 280 |  Loss_1: (0.0476) | Acc_1: (98.29%) (35354/35968)\n",
      "Epoch: 43 | Batch_idx: 290 |  Loss_1: (0.0483) | Acc_1: (98.27%) (36603/37248)\n",
      "Epoch: 43 | Batch_idx: 300 |  Loss_1: (0.0482) | Acc_1: (98.27%) (37863/38528)\n",
      "Epoch: 43 | Batch_idx: 310 |  Loss_1: (0.0479) | Acc_1: (98.28%) (39123/39808)\n",
      "Epoch: 43 | Batch_idx: 320 |  Loss_1: (0.0484) | Acc_1: (98.27%) (40376/41088)\n",
      "Epoch: 43 | Batch_idx: 330 |  Loss_1: (0.0482) | Acc_1: (98.28%) (41639/42368)\n",
      "Epoch: 43 | Batch_idx: 340 |  Loss_1: (0.0492) | Acc_1: (98.25%) (42882/43648)\n",
      "Epoch: 43 | Batch_idx: 350 |  Loss_1: (0.0490) | Acc_1: (98.25%) (44141/44928)\n",
      "Epoch: 43 | Batch_idx: 360 |  Loss_1: (0.0491) | Acc_1: (98.26%) (45404/46208)\n",
      "Epoch: 43 | Batch_idx: 370 |  Loss_1: (0.0493) | Acc_1: (98.25%) (46659/47488)\n",
      "Epoch: 43 | Batch_idx: 380 |  Loss_1: (0.0491) | Acc_1: (98.26%) (47919/48768)\n",
      "Epoch: 43 | Batch_idx: 390 |  Loss_1: (0.0490) | Acc_1: (98.26%) (49128/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4727) | Acc: (89.95%) (8995/10000)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.0360) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.0419) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.0425) | Acc_1: (98.44%) (2646/2688)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.0439) | Acc_1: (98.39%) (3904/3968)\n",
      "Epoch: 44 | Batch_idx: 40 |  Loss_1: (0.0435) | Acc_1: (98.42%) (5165/5248)\n",
      "Epoch: 44 | Batch_idx: 50 |  Loss_1: (0.0414) | Acc_1: (98.44%) (6426/6528)\n",
      "Epoch: 44 | Batch_idx: 60 |  Loss_1: (0.0422) | Acc_1: (98.48%) (7689/7808)\n",
      "Epoch: 44 | Batch_idx: 70 |  Loss_1: (0.0416) | Acc_1: (98.45%) (8947/9088)\n",
      "Epoch: 44 | Batch_idx: 80 |  Loss_1: (0.0429) | Acc_1: (98.46%) (10208/10368)\n",
      "Epoch: 44 | Batch_idx: 90 |  Loss_1: (0.0440) | Acc_1: (98.42%) (11464/11648)\n",
      "Epoch: 44 | Batch_idx: 100 |  Loss_1: (0.0454) | Acc_1: (98.39%) (12720/12928)\n",
      "Epoch: 44 | Batch_idx: 110 |  Loss_1: (0.0447) | Acc_1: (98.40%) (13981/14208)\n",
      "Epoch: 44 | Batch_idx: 120 |  Loss_1: (0.0441) | Acc_1: (98.41%) (15242/15488)\n",
      "Epoch: 44 | Batch_idx: 130 |  Loss_1: (0.0442) | Acc_1: (98.39%) (16498/16768)\n",
      "Epoch: 44 | Batch_idx: 140 |  Loss_1: (0.0435) | Acc_1: (98.41%) (17761/18048)\n",
      "Epoch: 44 | Batch_idx: 150 |  Loss_1: (0.0444) | Acc_1: (98.41%) (19020/19328)\n",
      "Epoch: 44 | Batch_idx: 160 |  Loss_1: (0.0444) | Acc_1: (98.42%) (20282/20608)\n",
      "Epoch: 44 | Batch_idx: 170 |  Loss_1: (0.0441) | Acc_1: (98.45%) (21548/21888)\n",
      "Epoch: 44 | Batch_idx: 180 |  Loss_1: (0.0441) | Acc_1: (98.43%) (22804/23168)\n",
      "Epoch: 44 | Batch_idx: 190 |  Loss_1: (0.0435) | Acc_1: (98.45%) (24069/24448)\n",
      "Epoch: 44 | Batch_idx: 200 |  Loss_1: (0.0434) | Acc_1: (98.46%) (25331/25728)\n",
      "Epoch: 44 | Batch_idx: 210 |  Loss_1: (0.0428) | Acc_1: (98.49%) (26599/27008)\n",
      "Epoch: 44 | Batch_idx: 220 |  Loss_1: (0.0434) | Acc_1: (98.47%) (27855/28288)\n",
      "Epoch: 44 | Batch_idx: 230 |  Loss_1: (0.0435) | Acc_1: (98.47%) (29115/29568)\n",
      "Epoch: 44 | Batch_idx: 240 |  Loss_1: (0.0435) | Acc_1: (98.46%) (30373/30848)\n",
      "Epoch: 44 | Batch_idx: 250 |  Loss_1: (0.0438) | Acc_1: (98.46%) (31632/32128)\n",
      "Epoch: 44 | Batch_idx: 260 |  Loss_1: (0.0442) | Acc_1: (98.45%) (32889/33408)\n",
      "Epoch: 44 | Batch_idx: 270 |  Loss_1: (0.0455) | Acc_1: (98.42%) (34141/34688)\n",
      "Epoch: 44 | Batch_idx: 280 |  Loss_1: (0.0461) | Acc_1: (98.41%) (35395/35968)\n",
      "Epoch: 44 | Batch_idx: 290 |  Loss_1: (0.0463) | Acc_1: (98.39%) (36650/37248)\n",
      "Epoch: 44 | Batch_idx: 300 |  Loss_1: (0.0467) | Acc_1: (98.38%) (37902/38528)\n",
      "Epoch: 44 | Batch_idx: 310 |  Loss_1: (0.0468) | Acc_1: (98.36%) (39154/39808)\n",
      "Epoch: 44 | Batch_idx: 320 |  Loss_1: (0.0468) | Acc_1: (98.36%) (40415/41088)\n",
      "Epoch: 44 | Batch_idx: 330 |  Loss_1: (0.0473) | Acc_1: (98.34%) (41664/42368)\n",
      "Epoch: 44 | Batch_idx: 340 |  Loss_1: (0.0472) | Acc_1: (98.34%) (42922/43648)\n",
      "Epoch: 44 | Batch_idx: 350 |  Loss_1: (0.0480) | Acc_1: (98.32%) (44174/44928)\n",
      "Epoch: 44 | Batch_idx: 360 |  Loss_1: (0.0475) | Acc_1: (98.34%) (45442/46208)\n",
      "Epoch: 44 | Batch_idx: 370 |  Loss_1: (0.0476) | Acc_1: (98.34%) (46702/47488)\n",
      "Epoch: 44 | Batch_idx: 380 |  Loss_1: (0.0471) | Acc_1: (98.36%) (47969/48768)\n",
      "Epoch: 44 | Batch_idx: 390 |  Loss_1: (0.0469) | Acc_1: (98.37%) (49187/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4685) | Acc: (90.05%) (9005/10000)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.0274) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.0494) | Acc_1: (98.37%) (1385/1408)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.0482) | Acc_1: (98.40%) (2645/2688)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.0496) | Acc_1: (98.34%) (3902/3968)\n",
      "Epoch: 45 | Batch_idx: 40 |  Loss_1: (0.0523) | Acc_1: (98.21%) (5154/5248)\n",
      "Epoch: 45 | Batch_idx: 50 |  Loss_1: (0.0493) | Acc_1: (98.31%) (6418/6528)\n",
      "Epoch: 45 | Batch_idx: 60 |  Loss_1: (0.0497) | Acc_1: (98.32%) (7677/7808)\n",
      "Epoch: 45 | Batch_idx: 70 |  Loss_1: (0.0491) | Acc_1: (98.29%) (8933/9088)\n",
      "Epoch: 45 | Batch_idx: 80 |  Loss_1: (0.0480) | Acc_1: (98.31%) (10193/10368)\n",
      "Epoch: 45 | Batch_idx: 90 |  Loss_1: (0.0489) | Acc_1: (98.33%) (11453/11648)\n",
      "Epoch: 45 | Batch_idx: 100 |  Loss_1: (0.0479) | Acc_1: (98.32%) (12711/12928)\n",
      "Epoch: 45 | Batch_idx: 110 |  Loss_1: (0.0468) | Acc_1: (98.32%) (13970/14208)\n",
      "Epoch: 45 | Batch_idx: 120 |  Loss_1: (0.0471) | Acc_1: (98.31%) (15227/15488)\n",
      "Epoch: 45 | Batch_idx: 130 |  Loss_1: (0.0466) | Acc_1: (98.31%) (16485/16768)\n",
      "Epoch: 45 | Batch_idx: 140 |  Loss_1: (0.0458) | Acc_1: (98.33%) (17747/18048)\n",
      "Epoch: 45 | Batch_idx: 150 |  Loss_1: (0.0450) | Acc_1: (98.35%) (19010/19328)\n",
      "Epoch: 45 | Batch_idx: 160 |  Loss_1: (0.0446) | Acc_1: (98.36%) (20271/20608)\n",
      "Epoch: 45 | Batch_idx: 170 |  Loss_1: (0.0445) | Acc_1: (98.36%) (21530/21888)\n",
      "Epoch: 45 | Batch_idx: 180 |  Loss_1: (0.0443) | Acc_1: (98.36%) (22789/23168)\n",
      "Epoch: 45 | Batch_idx: 190 |  Loss_1: (0.0449) | Acc_1: (98.36%) (24048/24448)\n",
      "Epoch: 45 | Batch_idx: 200 |  Loss_1: (0.0453) | Acc_1: (98.36%) (25305/25728)\n",
      "Epoch: 45 | Batch_idx: 210 |  Loss_1: (0.0454) | Acc_1: (98.36%) (26564/27008)\n",
      "Epoch: 45 | Batch_idx: 220 |  Loss_1: (0.0457) | Acc_1: (98.36%) (27823/28288)\n",
      "Epoch: 45 | Batch_idx: 230 |  Loss_1: (0.0460) | Acc_1: (98.34%) (29077/29568)\n",
      "Epoch: 45 | Batch_idx: 240 |  Loss_1: (0.0467) | Acc_1: (98.31%) (30328/30848)\n",
      "Epoch: 45 | Batch_idx: 250 |  Loss_1: (0.0475) | Acc_1: (98.28%) (31576/32128)\n",
      "Epoch: 45 | Batch_idx: 260 |  Loss_1: (0.0476) | Acc_1: (98.28%) (32834/33408)\n",
      "Epoch: 45 | Batch_idx: 270 |  Loss_1: (0.0475) | Acc_1: (98.28%) (34093/34688)\n",
      "Epoch: 45 | Batch_idx: 280 |  Loss_1: (0.0479) | Acc_1: (98.29%) (35353/35968)\n",
      "Epoch: 45 | Batch_idx: 290 |  Loss_1: (0.0478) | Acc_1: (98.29%) (36610/37248)\n",
      "Epoch: 45 | Batch_idx: 300 |  Loss_1: (0.0478) | Acc_1: (98.29%) (37870/38528)\n",
      "Epoch: 45 | Batch_idx: 310 |  Loss_1: (0.0476) | Acc_1: (98.30%) (39131/39808)\n",
      "Epoch: 45 | Batch_idx: 320 |  Loss_1: (0.0471) | Acc_1: (98.31%) (40394/41088)\n",
      "Epoch: 45 | Batch_idx: 330 |  Loss_1: (0.0472) | Acc_1: (98.32%) (41655/42368)\n",
      "Epoch: 45 | Batch_idx: 340 |  Loss_1: (0.0482) | Acc_1: (98.31%) (42909/43648)\n",
      "Epoch: 45 | Batch_idx: 350 |  Loss_1: (0.0481) | Acc_1: (98.30%) (44166/44928)\n",
      "Epoch: 45 | Batch_idx: 360 |  Loss_1: (0.0481) | Acc_1: (98.31%) (45429/46208)\n",
      "Epoch: 45 | Batch_idx: 370 |  Loss_1: (0.0484) | Acc_1: (98.30%) (46681/47488)\n",
      "Epoch: 45 | Batch_idx: 380 |  Loss_1: (0.0488) | Acc_1: (98.29%) (47934/48768)\n",
      "Epoch: 45 | Batch_idx: 390 |  Loss_1: (0.0487) | Acc_1: (98.29%) (49146/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5158) | Acc: (89.33%) (8933/10000)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.0965) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.0654) | Acc_1: (97.87%) (1378/1408)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.0549) | Acc_1: (98.21%) (2640/2688)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.0505) | Acc_1: (98.36%) (3903/3968)\n",
      "Epoch: 46 | Batch_idx: 40 |  Loss_1: (0.0496) | Acc_1: (98.32%) (5160/5248)\n",
      "Epoch: 46 | Batch_idx: 50 |  Loss_1: (0.0490) | Acc_1: (98.30%) (6417/6528)\n",
      "Epoch: 46 | Batch_idx: 60 |  Loss_1: (0.0470) | Acc_1: (98.39%) (7682/7808)\n",
      "Epoch: 46 | Batch_idx: 70 |  Loss_1: (0.0479) | Acc_1: (98.36%) (8939/9088)\n",
      "Epoch: 46 | Batch_idx: 80 |  Loss_1: (0.0490) | Acc_1: (98.33%) (10195/10368)\n",
      "Epoch: 46 | Batch_idx: 90 |  Loss_1: (0.0487) | Acc_1: (98.31%) (11451/11648)\n",
      "Epoch: 46 | Batch_idx: 100 |  Loss_1: (0.0485) | Acc_1: (98.30%) (12708/12928)\n",
      "Epoch: 46 | Batch_idx: 110 |  Loss_1: (0.0475) | Acc_1: (98.32%) (13970/14208)\n",
      "Epoch: 46 | Batch_idx: 120 |  Loss_1: (0.0467) | Acc_1: (98.36%) (15234/15488)\n",
      "Epoch: 46 | Batch_idx: 130 |  Loss_1: (0.0460) | Acc_1: (98.35%) (16491/16768)\n",
      "Epoch: 46 | Batch_idx: 140 |  Loss_1: (0.0457) | Acc_1: (98.36%) (17752/18048)\n",
      "Epoch: 46 | Batch_idx: 150 |  Loss_1: (0.0453) | Acc_1: (98.38%) (19015/19328)\n",
      "Epoch: 46 | Batch_idx: 160 |  Loss_1: (0.0447) | Acc_1: (98.38%) (20274/20608)\n",
      "Epoch: 46 | Batch_idx: 170 |  Loss_1: (0.0450) | Acc_1: (98.40%) (21537/21888)\n",
      "Epoch: 46 | Batch_idx: 180 |  Loss_1: (0.0456) | Acc_1: (98.39%) (22794/23168)\n",
      "Epoch: 46 | Batch_idx: 190 |  Loss_1: (0.0460) | Acc_1: (98.38%) (24051/24448)\n",
      "Epoch: 46 | Batch_idx: 200 |  Loss_1: (0.0455) | Acc_1: (98.41%) (25318/25728)\n",
      "Epoch: 46 | Batch_idx: 210 |  Loss_1: (0.0452) | Acc_1: (98.41%) (26578/27008)\n",
      "Epoch: 46 | Batch_idx: 220 |  Loss_1: (0.0453) | Acc_1: (98.40%) (27836/28288)\n",
      "Epoch: 46 | Batch_idx: 230 |  Loss_1: (0.0455) | Acc_1: (98.38%) (29090/29568)\n",
      "Epoch: 46 | Batch_idx: 240 |  Loss_1: (0.0453) | Acc_1: (98.38%) (30347/30848)\n",
      "Epoch: 46 | Batch_idx: 250 |  Loss_1: (0.0456) | Acc_1: (98.37%) (31604/32128)\n",
      "Epoch: 46 | Batch_idx: 260 |  Loss_1: (0.0455) | Acc_1: (98.38%) (32866/33408)\n",
      "Epoch: 46 | Batch_idx: 270 |  Loss_1: (0.0454) | Acc_1: (98.37%) (34124/34688)\n",
      "Epoch: 46 | Batch_idx: 280 |  Loss_1: (0.0449) | Acc_1: (98.40%) (35391/35968)\n",
      "Epoch: 46 | Batch_idx: 290 |  Loss_1: (0.0451) | Acc_1: (98.37%) (36642/37248)\n",
      "Epoch: 46 | Batch_idx: 300 |  Loss_1: (0.0446) | Acc_1: (98.40%) (37910/38528)\n",
      "Epoch: 46 | Batch_idx: 310 |  Loss_1: (0.0445) | Acc_1: (98.39%) (39168/39808)\n",
      "Epoch: 46 | Batch_idx: 320 |  Loss_1: (0.0446) | Acc_1: (98.39%) (40427/41088)\n",
      "Epoch: 46 | Batch_idx: 330 |  Loss_1: (0.0449) | Acc_1: (98.38%) (41681/42368)\n",
      "Epoch: 46 | Batch_idx: 340 |  Loss_1: (0.0450) | Acc_1: (98.38%) (42942/43648)\n",
      "Epoch: 46 | Batch_idx: 350 |  Loss_1: (0.0452) | Acc_1: (98.38%) (44201/44928)\n",
      "Epoch: 46 | Batch_idx: 360 |  Loss_1: (0.0451) | Acc_1: (98.39%) (45463/46208)\n",
      "Epoch: 46 | Batch_idx: 370 |  Loss_1: (0.0453) | Acc_1: (98.40%) (46726/47488)\n",
      "Epoch: 46 | Batch_idx: 380 |  Loss_1: (0.0456) | Acc_1: (98.38%) (47978/48768)\n",
      "Epoch: 46 | Batch_idx: 390 |  Loss_1: (0.0454) | Acc_1: (98.39%) (49195/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4927) | Acc: (89.90%) (8990/10000)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.0177) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.0348) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.0338) | Acc_1: (98.81%) (2656/2688)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.0354) | Acc_1: (98.74%) (3918/3968)\n",
      "Epoch: 47 | Batch_idx: 40 |  Loss_1: (0.0413) | Acc_1: (98.67%) (5178/5248)\n",
      "Epoch: 47 | Batch_idx: 50 |  Loss_1: (0.0415) | Acc_1: (98.61%) (6437/6528)\n",
      "Epoch: 47 | Batch_idx: 60 |  Loss_1: (0.0413) | Acc_1: (98.64%) (7702/7808)\n",
      "Epoch: 47 | Batch_idx: 70 |  Loss_1: (0.0419) | Acc_1: (98.56%) (8957/9088)\n",
      "Epoch: 47 | Batch_idx: 80 |  Loss_1: (0.0406) | Acc_1: (98.57%) (10220/10368)\n",
      "Epoch: 47 | Batch_idx: 90 |  Loss_1: (0.0415) | Acc_1: (98.53%) (11477/11648)\n",
      "Epoch: 47 | Batch_idx: 100 |  Loss_1: (0.0417) | Acc_1: (98.53%) (12738/12928)\n",
      "Epoch: 47 | Batch_idx: 110 |  Loss_1: (0.0418) | Acc_1: (98.52%) (13998/14208)\n",
      "Epoch: 47 | Batch_idx: 120 |  Loss_1: (0.0430) | Acc_1: (98.48%) (15252/15488)\n",
      "Epoch: 47 | Batch_idx: 130 |  Loss_1: (0.0422) | Acc_1: (98.52%) (16520/16768)\n",
      "Epoch: 47 | Batch_idx: 140 |  Loss_1: (0.0421) | Acc_1: (98.55%) (17786/18048)\n",
      "Epoch: 47 | Batch_idx: 150 |  Loss_1: (0.0425) | Acc_1: (98.54%) (19046/19328)\n",
      "Epoch: 47 | Batch_idx: 160 |  Loss_1: (0.0420) | Acc_1: (98.56%) (20311/20608)\n",
      "Epoch: 47 | Batch_idx: 170 |  Loss_1: (0.0417) | Acc_1: (98.57%) (21575/21888)\n",
      "Epoch: 47 | Batch_idx: 180 |  Loss_1: (0.0417) | Acc_1: (98.57%) (22836/23168)\n",
      "Epoch: 47 | Batch_idx: 190 |  Loss_1: (0.0411) | Acc_1: (98.59%) (24103/24448)\n",
      "Epoch: 47 | Batch_idx: 200 |  Loss_1: (0.0410) | Acc_1: (98.58%) (25362/25728)\n",
      "Epoch: 47 | Batch_idx: 210 |  Loss_1: (0.0409) | Acc_1: (98.59%) (26628/27008)\n",
      "Epoch: 47 | Batch_idx: 220 |  Loss_1: (0.0409) | Acc_1: (98.59%) (27888/28288)\n",
      "Epoch: 47 | Batch_idx: 230 |  Loss_1: (0.0409) | Acc_1: (98.58%) (29147/29568)\n",
      "Epoch: 47 | Batch_idx: 240 |  Loss_1: (0.0412) | Acc_1: (98.56%) (30404/30848)\n",
      "Epoch: 47 | Batch_idx: 250 |  Loss_1: (0.0412) | Acc_1: (98.57%) (31667/32128)\n",
      "Epoch: 47 | Batch_idx: 260 |  Loss_1: (0.0408) | Acc_1: (98.57%) (32931/33408)\n",
      "Epoch: 47 | Batch_idx: 270 |  Loss_1: (0.0412) | Acc_1: (98.55%) (34186/34688)\n",
      "Epoch: 47 | Batch_idx: 280 |  Loss_1: (0.0412) | Acc_1: (98.55%) (35446/35968)\n",
      "Epoch: 47 | Batch_idx: 290 |  Loss_1: (0.0411) | Acc_1: (98.54%) (36705/37248)\n",
      "Epoch: 47 | Batch_idx: 300 |  Loss_1: (0.0409) | Acc_1: (98.55%) (37971/38528)\n",
      "Epoch: 47 | Batch_idx: 310 |  Loss_1: (0.0410) | Acc_1: (98.55%) (39229/39808)\n",
      "Epoch: 47 | Batch_idx: 320 |  Loss_1: (0.0413) | Acc_1: (98.55%) (40491/41088)\n",
      "Epoch: 47 | Batch_idx: 330 |  Loss_1: (0.0415) | Acc_1: (98.54%) (41748/42368)\n",
      "Epoch: 47 | Batch_idx: 340 |  Loss_1: (0.0417) | Acc_1: (98.53%) (43006/43648)\n",
      "Epoch: 47 | Batch_idx: 350 |  Loss_1: (0.0423) | Acc_1: (98.51%) (44257/44928)\n",
      "Epoch: 47 | Batch_idx: 360 |  Loss_1: (0.0424) | Acc_1: (98.51%) (45519/46208)\n",
      "Epoch: 47 | Batch_idx: 370 |  Loss_1: (0.0425) | Acc_1: (98.50%) (46775/47488)\n",
      "Epoch: 47 | Batch_idx: 380 |  Loss_1: (0.0426) | Acc_1: (98.50%) (48038/48768)\n",
      "Epoch: 47 | Batch_idx: 390 |  Loss_1: (0.0428) | Acc_1: (98.51%) (49253/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5139) | Acc: (89.77%) (8977/10000)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.0308) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.0251) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.0325) | Acc_1: (98.81%) (2656/2688)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.0302) | Acc_1: (98.92%) (3925/3968)\n",
      "Epoch: 48 | Batch_idx: 40 |  Loss_1: (0.0320) | Acc_1: (98.88%) (5189/5248)\n",
      "Epoch: 48 | Batch_idx: 50 |  Loss_1: (0.0335) | Acc_1: (98.90%) (6456/6528)\n",
      "Epoch: 48 | Batch_idx: 60 |  Loss_1: (0.0329) | Acc_1: (98.90%) (7722/7808)\n",
      "Epoch: 48 | Batch_idx: 70 |  Loss_1: (0.0330) | Acc_1: (98.90%) (8988/9088)\n",
      "Epoch: 48 | Batch_idx: 80 |  Loss_1: (0.0355) | Acc_1: (98.77%) (10240/10368)\n",
      "Epoch: 48 | Batch_idx: 90 |  Loss_1: (0.0368) | Acc_1: (98.72%) (11499/11648)\n",
      "Epoch: 48 | Batch_idx: 100 |  Loss_1: (0.0374) | Acc_1: (98.68%) (12757/12928)\n",
      "Epoch: 48 | Batch_idx: 110 |  Loss_1: (0.0375) | Acc_1: (98.66%) (14018/14208)\n",
      "Epoch: 48 | Batch_idx: 120 |  Loss_1: (0.0373) | Acc_1: (98.67%) (15282/15488)\n",
      "Epoch: 48 | Batch_idx: 130 |  Loss_1: (0.0364) | Acc_1: (98.71%) (16551/16768)\n",
      "Epoch: 48 | Batch_idx: 140 |  Loss_1: (0.0365) | Acc_1: (98.69%) (17812/18048)\n",
      "Epoch: 48 | Batch_idx: 150 |  Loss_1: (0.0362) | Acc_1: (98.70%) (19077/19328)\n",
      "Epoch: 48 | Batch_idx: 160 |  Loss_1: (0.0366) | Acc_1: (98.68%) (20337/20608)\n",
      "Epoch: 48 | Batch_idx: 170 |  Loss_1: (0.0370) | Acc_1: (98.66%) (21595/21888)\n",
      "Epoch: 48 | Batch_idx: 180 |  Loss_1: (0.0369) | Acc_1: (98.66%) (22858/23168)\n",
      "Epoch: 48 | Batch_idx: 190 |  Loss_1: (0.0367) | Acc_1: (98.68%) (24125/24448)\n",
      "Epoch: 48 | Batch_idx: 200 |  Loss_1: (0.0365) | Acc_1: (98.70%) (25393/25728)\n",
      "Epoch: 48 | Batch_idx: 210 |  Loss_1: (0.0368) | Acc_1: (98.68%) (26651/27008)\n",
      "Epoch: 48 | Batch_idx: 220 |  Loss_1: (0.0368) | Acc_1: (98.67%) (27913/28288)\n",
      "Epoch: 48 | Batch_idx: 230 |  Loss_1: (0.0366) | Acc_1: (98.69%) (29180/29568)\n",
      "Epoch: 48 | Batch_idx: 240 |  Loss_1: (0.0364) | Acc_1: (98.69%) (30445/30848)\n",
      "Epoch: 48 | Batch_idx: 250 |  Loss_1: (0.0359) | Acc_1: (98.71%) (31714/32128)\n",
      "Epoch: 48 | Batch_idx: 260 |  Loss_1: (0.0362) | Acc_1: (98.70%) (32973/33408)\n",
      "Epoch: 48 | Batch_idx: 270 |  Loss_1: (0.0362) | Acc_1: (98.70%) (34237/34688)\n",
      "Epoch: 48 | Batch_idx: 280 |  Loss_1: (0.0363) | Acc_1: (98.69%) (35497/35968)\n",
      "Epoch: 48 | Batch_idx: 290 |  Loss_1: (0.0366) | Acc_1: (98.68%) (36756/37248)\n",
      "Epoch: 48 | Batch_idx: 300 |  Loss_1: (0.0366) | Acc_1: (98.68%) (38019/38528)\n",
      "Epoch: 48 | Batch_idx: 310 |  Loss_1: (0.0366) | Acc_1: (98.68%) (39284/39808)\n",
      "Epoch: 48 | Batch_idx: 320 |  Loss_1: (0.0368) | Acc_1: (98.66%) (40539/41088)\n",
      "Epoch: 48 | Batch_idx: 330 |  Loss_1: (0.0370) | Acc_1: (98.66%) (41802/42368)\n",
      "Epoch: 48 | Batch_idx: 340 |  Loss_1: (0.0373) | Acc_1: (98.66%) (43061/43648)\n",
      "Epoch: 48 | Batch_idx: 350 |  Loss_1: (0.0373) | Acc_1: (98.65%) (44322/44928)\n",
      "Epoch: 48 | Batch_idx: 360 |  Loss_1: (0.0373) | Acc_1: (98.65%) (45583/46208)\n",
      "Epoch: 48 | Batch_idx: 370 |  Loss_1: (0.0376) | Acc_1: (98.62%) (46835/47488)\n",
      "Epoch: 48 | Batch_idx: 380 |  Loss_1: (0.0377) | Acc_1: (98.62%) (48093/48768)\n",
      "Epoch: 48 | Batch_idx: 390 |  Loss_1: (0.0381) | Acc_1: (98.60%) (49301/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5251) | Acc: (89.59%) (8959/10000)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.0362) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.0407) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.0342) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.0377) | Acc_1: (98.66%) (3915/3968)\n",
      "Epoch: 49 | Batch_idx: 40 |  Loss_1: (0.0360) | Acc_1: (98.72%) (5181/5248)\n",
      "Epoch: 49 | Batch_idx: 50 |  Loss_1: (0.0348) | Acc_1: (98.74%) (6446/6528)\n",
      "Epoch: 49 | Batch_idx: 60 |  Loss_1: (0.0358) | Acc_1: (98.69%) (7706/7808)\n",
      "Epoch: 49 | Batch_idx: 70 |  Loss_1: (0.0361) | Acc_1: (98.68%) (8968/9088)\n",
      "Epoch: 49 | Batch_idx: 80 |  Loss_1: (0.0370) | Acc_1: (98.61%) (10224/10368)\n",
      "Epoch: 49 | Batch_idx: 90 |  Loss_1: (0.0377) | Acc_1: (98.59%) (11484/11648)\n",
      "Epoch: 49 | Batch_idx: 100 |  Loss_1: (0.0399) | Acc_1: (98.54%) (12739/12928)\n",
      "Epoch: 49 | Batch_idx: 110 |  Loss_1: (0.0404) | Acc_1: (98.51%) (13996/14208)\n",
      "Epoch: 49 | Batch_idx: 120 |  Loss_1: (0.0409) | Acc_1: (98.51%) (15257/15488)\n",
      "Epoch: 49 | Batch_idx: 130 |  Loss_1: (0.0406) | Acc_1: (98.53%) (16522/16768)\n",
      "Epoch: 49 | Batch_idx: 140 |  Loss_1: (0.0411) | Acc_1: (98.52%) (17781/18048)\n",
      "Epoch: 49 | Batch_idx: 150 |  Loss_1: (0.0412) | Acc_1: (98.54%) (19045/19328)\n",
      "Epoch: 49 | Batch_idx: 160 |  Loss_1: (0.0407) | Acc_1: (98.56%) (20312/20608)\n",
      "Epoch: 49 | Batch_idx: 170 |  Loss_1: (0.0413) | Acc_1: (98.56%) (21573/21888)\n",
      "Epoch: 49 | Batch_idx: 180 |  Loss_1: (0.0412) | Acc_1: (98.56%) (22835/23168)\n",
      "Epoch: 49 | Batch_idx: 190 |  Loss_1: (0.0416) | Acc_1: (98.54%) (24091/24448)\n",
      "Epoch: 49 | Batch_idx: 200 |  Loss_1: (0.0414) | Acc_1: (98.54%) (25353/25728)\n",
      "Epoch: 49 | Batch_idx: 210 |  Loss_1: (0.0416) | Acc_1: (98.54%) (26614/27008)\n",
      "Epoch: 49 | Batch_idx: 220 |  Loss_1: (0.0417) | Acc_1: (98.53%) (27873/28288)\n",
      "Epoch: 49 | Batch_idx: 230 |  Loss_1: (0.0414) | Acc_1: (98.55%) (29138/29568)\n",
      "Epoch: 49 | Batch_idx: 240 |  Loss_1: (0.0414) | Acc_1: (98.55%) (30400/30848)\n",
      "Epoch: 49 | Batch_idx: 250 |  Loss_1: (0.0415) | Acc_1: (98.54%) (31660/32128)\n",
      "Epoch: 49 | Batch_idx: 260 |  Loss_1: (0.0415) | Acc_1: (98.56%) (32927/33408)\n",
      "Epoch: 49 | Batch_idx: 270 |  Loss_1: (0.0412) | Acc_1: (98.58%) (34195/34688)\n",
      "Epoch: 49 | Batch_idx: 280 |  Loss_1: (0.0416) | Acc_1: (98.56%) (35451/35968)\n",
      "Epoch: 49 | Batch_idx: 290 |  Loss_1: (0.0417) | Acc_1: (98.56%) (36711/37248)\n",
      "Epoch: 49 | Batch_idx: 300 |  Loss_1: (0.0416) | Acc_1: (98.57%) (37977/38528)\n",
      "Epoch: 49 | Batch_idx: 310 |  Loss_1: (0.0414) | Acc_1: (98.58%) (39243/39808)\n",
      "Epoch: 49 | Batch_idx: 320 |  Loss_1: (0.0412) | Acc_1: (98.58%) (40506/41088)\n",
      "Epoch: 49 | Batch_idx: 330 |  Loss_1: (0.0415) | Acc_1: (98.57%) (41762/42368)\n",
      "Epoch: 49 | Batch_idx: 340 |  Loss_1: (0.0411) | Acc_1: (98.58%) (43029/43648)\n",
      "Epoch: 49 | Batch_idx: 350 |  Loss_1: (0.0408) | Acc_1: (98.59%) (44293/44928)\n",
      "Epoch: 49 | Batch_idx: 360 |  Loss_1: (0.0407) | Acc_1: (98.60%) (45559/46208)\n",
      "Epoch: 49 | Batch_idx: 370 |  Loss_1: (0.0403) | Acc_1: (98.61%) (46829/47488)\n",
      "Epoch: 49 | Batch_idx: 380 |  Loss_1: (0.0398) | Acc_1: (98.62%) (48097/48768)\n",
      "Epoch: 49 | Batch_idx: 390 |  Loss_1: (0.0397) | Acc_1: (98.63%) (49316/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5309) | Acc: (89.69%) (8969/10000)\n",
      "Epoch: 50 | Batch_idx: 0 |  Loss_1: (0.0335) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 50 | Batch_idx: 10 |  Loss_1: (0.0277) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 50 | Batch_idx: 20 |  Loss_1: (0.0294) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 50 | Batch_idx: 30 |  Loss_1: (0.0275) | Acc_1: (98.99%) (3928/3968)\n",
      "Epoch: 50 | Batch_idx: 40 |  Loss_1: (0.0313) | Acc_1: (98.89%) (5190/5248)\n",
      "Epoch: 50 | Batch_idx: 50 |  Loss_1: (0.0300) | Acc_1: (98.93%) (6458/6528)\n",
      "Epoch: 50 | Batch_idx: 60 |  Loss_1: (0.0320) | Acc_1: (98.82%) (7716/7808)\n",
      "Epoch: 50 | Batch_idx: 70 |  Loss_1: (0.0315) | Acc_1: (98.88%) (8986/9088)\n",
      "Epoch: 50 | Batch_idx: 80 |  Loss_1: (0.0331) | Acc_1: (98.80%) (10244/10368)\n",
      "Epoch: 50 | Batch_idx: 90 |  Loss_1: (0.0352) | Acc_1: (98.76%) (11504/11648)\n",
      "Epoch: 50 | Batch_idx: 100 |  Loss_1: (0.0358) | Acc_1: (98.75%) (12766/12928)\n",
      "Epoch: 50 | Batch_idx: 110 |  Loss_1: (0.0355) | Acc_1: (98.76%) (14032/14208)\n",
      "Epoch: 50 | Batch_idx: 120 |  Loss_1: (0.0350) | Acc_1: (98.76%) (15296/15488)\n",
      "Epoch: 50 | Batch_idx: 130 |  Loss_1: (0.0360) | Acc_1: (98.75%) (16559/16768)\n",
      "Epoch: 50 | Batch_idx: 140 |  Loss_1: (0.0363) | Acc_1: (98.71%) (17815/18048)\n",
      "Epoch: 50 | Batch_idx: 150 |  Loss_1: (0.0371) | Acc_1: (98.67%) (19070/19328)\n",
      "Epoch: 50 | Batch_idx: 160 |  Loss_1: (0.0379) | Acc_1: (98.62%) (20324/20608)\n",
      "Epoch: 50 | Batch_idx: 170 |  Loss_1: (0.0376) | Acc_1: (98.65%) (21593/21888)\n",
      "Epoch: 50 | Batch_idx: 180 |  Loss_1: (0.0378) | Acc_1: (98.64%) (22854/23168)\n",
      "Epoch: 50 | Batch_idx: 190 |  Loss_1: (0.0376) | Acc_1: (98.65%) (24119/24448)\n",
      "Epoch: 50 | Batch_idx: 200 |  Loss_1: (0.0372) | Acc_1: (98.66%) (25382/25728)\n",
      "Epoch: 50 | Batch_idx: 210 |  Loss_1: (0.0376) | Acc_1: (98.64%) (26642/27008)\n",
      "Epoch: 50 | Batch_idx: 220 |  Loss_1: (0.0373) | Acc_1: (98.67%) (27911/28288)\n",
      "Epoch: 50 | Batch_idx: 230 |  Loss_1: (0.0377) | Acc_1: (98.65%) (29168/29568)\n",
      "Epoch: 50 | Batch_idx: 240 |  Loss_1: (0.0383) | Acc_1: (98.63%) (30426/30848)\n",
      "Epoch: 50 | Batch_idx: 250 |  Loss_1: (0.0379) | Acc_1: (98.64%) (31692/32128)\n",
      "Epoch: 50 | Batch_idx: 260 |  Loss_1: (0.0378) | Acc_1: (98.63%) (32951/33408)\n",
      "Epoch: 50 | Batch_idx: 270 |  Loss_1: (0.0380) | Acc_1: (98.62%) (34210/34688)\n",
      "Epoch: 50 | Batch_idx: 280 |  Loss_1: (0.0378) | Acc_1: (98.63%) (35477/35968)\n",
      "Epoch: 50 | Batch_idx: 290 |  Loss_1: (0.0373) | Acc_1: (98.66%) (36749/37248)\n",
      "Epoch: 50 | Batch_idx: 300 |  Loss_1: (0.0370) | Acc_1: (98.67%) (38016/38528)\n",
      "Epoch: 50 | Batch_idx: 310 |  Loss_1: (0.0368) | Acc_1: (98.67%) (39280/39808)\n",
      "Epoch: 50 | Batch_idx: 320 |  Loss_1: (0.0364) | Acc_1: (98.69%) (40549/41088)\n",
      "Epoch: 50 | Batch_idx: 330 |  Loss_1: (0.0361) | Acc_1: (98.70%) (41816/42368)\n",
      "Epoch: 50 | Batch_idx: 340 |  Loss_1: (0.0362) | Acc_1: (98.69%) (43077/43648)\n",
      "Epoch: 50 | Batch_idx: 350 |  Loss_1: (0.0365) | Acc_1: (98.68%) (44336/44928)\n",
      "Epoch: 50 | Batch_idx: 360 |  Loss_1: (0.0364) | Acc_1: (98.68%) (45600/46208)\n",
      "Epoch: 50 | Batch_idx: 370 |  Loss_1: (0.0364) | Acc_1: (98.69%) (46867/47488)\n",
      "Epoch: 50 | Batch_idx: 380 |  Loss_1: (0.0367) | Acc_1: (98.68%) (48126/48768)\n",
      "Epoch: 50 | Batch_idx: 390 |  Loss_1: (0.0368) | Acc_1: (98.68%) (49340/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5290) | Acc: (90.27%) (9027/10000)\n",
      "Epoch: 51 | Batch_idx: 0 |  Loss_1: (0.0513) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 51 | Batch_idx: 10 |  Loss_1: (0.0450) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 51 | Batch_idx: 20 |  Loss_1: (0.0438) | Acc_1: (98.74%) (2654/2688)\n",
      "Epoch: 51 | Batch_idx: 30 |  Loss_1: (0.0443) | Acc_1: (98.79%) (3920/3968)\n",
      "Epoch: 51 | Batch_idx: 40 |  Loss_1: (0.0424) | Acc_1: (98.84%) (5187/5248)\n",
      "Epoch: 51 | Batch_idx: 50 |  Loss_1: (0.0406) | Acc_1: (98.76%) (6447/6528)\n",
      "Epoch: 51 | Batch_idx: 60 |  Loss_1: (0.0398) | Acc_1: (98.78%) (7713/7808)\n",
      "Epoch: 51 | Batch_idx: 70 |  Loss_1: (0.0395) | Acc_1: (98.78%) (8977/9088)\n",
      "Epoch: 51 | Batch_idx: 80 |  Loss_1: (0.0406) | Acc_1: (98.78%) (10242/10368)\n",
      "Epoch: 51 | Batch_idx: 90 |  Loss_1: (0.0402) | Acc_1: (98.78%) (11506/11648)\n",
      "Epoch: 51 | Batch_idx: 100 |  Loss_1: (0.0396) | Acc_1: (98.76%) (12768/12928)\n",
      "Epoch: 51 | Batch_idx: 110 |  Loss_1: (0.0395) | Acc_1: (98.74%) (14029/14208)\n",
      "Epoch: 51 | Batch_idx: 120 |  Loss_1: (0.0387) | Acc_1: (98.77%) (15297/15488)\n",
      "Epoch: 51 | Batch_idx: 130 |  Loss_1: (0.0379) | Acc_1: (98.77%) (16562/16768)\n",
      "Epoch: 51 | Batch_idx: 140 |  Loss_1: (0.0368) | Acc_1: (98.82%) (17835/18048)\n",
      "Epoch: 51 | Batch_idx: 150 |  Loss_1: (0.0356) | Acc_1: (98.87%) (19109/19328)\n",
      "Epoch: 51 | Batch_idx: 160 |  Loss_1: (0.0358) | Acc_1: (98.85%) (20372/20608)\n",
      "Epoch: 51 | Batch_idx: 170 |  Loss_1: (0.0354) | Acc_1: (98.87%) (21640/21888)\n",
      "Epoch: 51 | Batch_idx: 180 |  Loss_1: (0.0353) | Acc_1: (98.87%) (22906/23168)\n",
      "Epoch: 51 | Batch_idx: 190 |  Loss_1: (0.0347) | Acc_1: (98.90%) (24178/24448)\n",
      "Epoch: 51 | Batch_idx: 200 |  Loss_1: (0.0340) | Acc_1: (98.91%) (25447/25728)\n",
      "Epoch: 51 | Batch_idx: 210 |  Loss_1: (0.0335) | Acc_1: (98.92%) (26716/27008)\n",
      "Epoch: 51 | Batch_idx: 220 |  Loss_1: (0.0336) | Acc_1: (98.91%) (27981/28288)\n",
      "Epoch: 51 | Batch_idx: 230 |  Loss_1: (0.0335) | Acc_1: (98.93%) (29251/29568)\n",
      "Epoch: 51 | Batch_idx: 240 |  Loss_1: (0.0337) | Acc_1: (98.90%) (30510/30848)\n",
      "Epoch: 51 | Batch_idx: 250 |  Loss_1: (0.0338) | Acc_1: (98.90%) (31776/32128)\n",
      "Epoch: 51 | Batch_idx: 260 |  Loss_1: (0.0341) | Acc_1: (98.88%) (33035/33408)\n",
      "Epoch: 51 | Batch_idx: 270 |  Loss_1: (0.0339) | Acc_1: (98.90%) (34307/34688)\n",
      "Epoch: 51 | Batch_idx: 280 |  Loss_1: (0.0342) | Acc_1: (98.89%) (35570/35968)\n",
      "Epoch: 51 | Batch_idx: 290 |  Loss_1: (0.0343) | Acc_1: (98.89%) (36835/37248)\n",
      "Epoch: 51 | Batch_idx: 300 |  Loss_1: (0.0339) | Acc_1: (98.90%) (38105/38528)\n",
      "Epoch: 51 | Batch_idx: 310 |  Loss_1: (0.0338) | Acc_1: (98.91%) (39373/39808)\n",
      "Epoch: 51 | Batch_idx: 320 |  Loss_1: (0.0342) | Acc_1: (98.90%) (40634/41088)\n",
      "Epoch: 51 | Batch_idx: 330 |  Loss_1: (0.0340) | Acc_1: (98.90%) (41904/42368)\n",
      "Epoch: 51 | Batch_idx: 340 |  Loss_1: (0.0340) | Acc_1: (98.91%) (43171/43648)\n",
      "Epoch: 51 | Batch_idx: 350 |  Loss_1: (0.0339) | Acc_1: (98.91%) (44439/44928)\n",
      "Epoch: 51 | Batch_idx: 360 |  Loss_1: (0.0340) | Acc_1: (98.90%) (45702/46208)\n",
      "Epoch: 51 | Batch_idx: 370 |  Loss_1: (0.0343) | Acc_1: (98.89%) (46962/47488)\n",
      "Epoch: 51 | Batch_idx: 380 |  Loss_1: (0.0347) | Acc_1: (98.87%) (48216/48768)\n",
      "Epoch: 51 | Batch_idx: 390 |  Loss_1: (0.0352) | Acc_1: (98.84%) (49422/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5295) | Acc: (90.02%) (9002/10000)\n",
      "Epoch: 52 | Batch_idx: 0 |  Loss_1: (0.0102) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 52 | Batch_idx: 10 |  Loss_1: (0.0469) | Acc_1: (98.37%) (1385/1408)\n",
      "Epoch: 52 | Batch_idx: 20 |  Loss_1: (0.0417) | Acc_1: (98.55%) (2649/2688)\n",
      "Epoch: 52 | Batch_idx: 30 |  Loss_1: (0.0399) | Acc_1: (98.71%) (3917/3968)\n",
      "Epoch: 52 | Batch_idx: 40 |  Loss_1: (0.0407) | Acc_1: (98.65%) (5177/5248)\n",
      "Epoch: 52 | Batch_idx: 50 |  Loss_1: (0.0456) | Acc_1: (98.48%) (6429/6528)\n",
      "Epoch: 52 | Batch_idx: 60 |  Loss_1: (0.0442) | Acc_1: (98.50%) (7691/7808)\n",
      "Epoch: 52 | Batch_idx: 70 |  Loss_1: (0.0443) | Acc_1: (98.53%) (8954/9088)\n",
      "Epoch: 52 | Batch_idx: 80 |  Loss_1: (0.0426) | Acc_1: (98.57%) (10220/10368)\n",
      "Epoch: 52 | Batch_idx: 90 |  Loss_1: (0.0427) | Acc_1: (98.58%) (11483/11648)\n",
      "Epoch: 52 | Batch_idx: 100 |  Loss_1: (0.0437) | Acc_1: (98.55%) (12741/12928)\n",
      "Epoch: 52 | Batch_idx: 110 |  Loss_1: (0.0428) | Acc_1: (98.56%) (14004/14208)\n",
      "Epoch: 52 | Batch_idx: 120 |  Loss_1: (0.0422) | Acc_1: (98.59%) (15269/15488)\n",
      "Epoch: 52 | Batch_idx: 130 |  Loss_1: (0.0416) | Acc_1: (98.61%) (16535/16768)\n",
      "Epoch: 52 | Batch_idx: 140 |  Loss_1: (0.0413) | Acc_1: (98.61%) (17797/18048)\n",
      "Epoch: 52 | Batch_idx: 150 |  Loss_1: (0.0414) | Acc_1: (98.62%) (19062/19328)\n",
      "Epoch: 52 | Batch_idx: 160 |  Loss_1: (0.0412) | Acc_1: (98.64%) (20327/20608)\n",
      "Epoch: 52 | Batch_idx: 170 |  Loss_1: (0.0404) | Acc_1: (98.66%) (21595/21888)\n",
      "Epoch: 52 | Batch_idx: 180 |  Loss_1: (0.0406) | Acc_1: (98.66%) (22857/23168)\n",
      "Epoch: 52 | Batch_idx: 190 |  Loss_1: (0.0410) | Acc_1: (98.65%) (24117/24448)\n",
      "Epoch: 52 | Batch_idx: 200 |  Loss_1: (0.0409) | Acc_1: (98.65%) (25380/25728)\n",
      "Epoch: 52 | Batch_idx: 210 |  Loss_1: (0.0409) | Acc_1: (98.64%) (26642/27008)\n",
      "Epoch: 52 | Batch_idx: 220 |  Loss_1: (0.0402) | Acc_1: (98.66%) (27909/28288)\n",
      "Epoch: 52 | Batch_idx: 230 |  Loss_1: (0.0405) | Acc_1: (98.64%) (29167/29568)\n",
      "Epoch: 52 | Batch_idx: 240 |  Loss_1: (0.0402) | Acc_1: (98.64%) (30430/30848)\n",
      "Epoch: 52 | Batch_idx: 250 |  Loss_1: (0.0399) | Acc_1: (98.65%) (31693/32128)\n",
      "Epoch: 52 | Batch_idx: 260 |  Loss_1: (0.0400) | Acc_1: (98.65%) (32956/33408)\n",
      "Epoch: 52 | Batch_idx: 270 |  Loss_1: (0.0398) | Acc_1: (98.65%) (34219/34688)\n",
      "Epoch: 52 | Batch_idx: 280 |  Loss_1: (0.0394) | Acc_1: (98.66%) (35487/35968)\n",
      "Epoch: 52 | Batch_idx: 290 |  Loss_1: (0.0399) | Acc_1: (98.63%) (36739/37248)\n",
      "Epoch: 52 | Batch_idx: 300 |  Loss_1: (0.0395) | Acc_1: (98.64%) (38003/38528)\n",
      "Epoch: 52 | Batch_idx: 310 |  Loss_1: (0.0393) | Acc_1: (98.64%) (39268/39808)\n",
      "Epoch: 52 | Batch_idx: 320 |  Loss_1: (0.0390) | Acc_1: (98.66%) (40537/41088)\n",
      "Epoch: 52 | Batch_idx: 330 |  Loss_1: (0.0389) | Acc_1: (98.67%) (41803/42368)\n",
      "Epoch: 52 | Batch_idx: 340 |  Loss_1: (0.0390) | Acc_1: (98.66%) (43065/43648)\n",
      "Epoch: 52 | Batch_idx: 350 |  Loss_1: (0.0390) | Acc_1: (98.66%) (44326/44928)\n",
      "Epoch: 52 | Batch_idx: 360 |  Loss_1: (0.0389) | Acc_1: (98.67%) (45593/46208)\n",
      "Epoch: 52 | Batch_idx: 370 |  Loss_1: (0.0390) | Acc_1: (98.66%) (46853/47488)\n",
      "Epoch: 52 | Batch_idx: 380 |  Loss_1: (0.0388) | Acc_1: (98.68%) (48122/48768)\n",
      "Epoch: 52 | Batch_idx: 390 |  Loss_1: (0.0387) | Acc_1: (98.68%) (49339/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4766) | Acc: (90.54%) (9054/10000)\n",
      "Epoch: 53 | Batch_idx: 0 |  Loss_1: (0.0126) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 53 | Batch_idx: 10 |  Loss_1: (0.0360) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 53 | Batch_idx: 20 |  Loss_1: (0.0363) | Acc_1: (98.59%) (2650/2688)\n",
      "Epoch: 53 | Batch_idx: 30 |  Loss_1: (0.0384) | Acc_1: (98.66%) (3915/3968)\n",
      "Epoch: 53 | Batch_idx: 40 |  Loss_1: (0.0361) | Acc_1: (98.76%) (5183/5248)\n",
      "Epoch: 53 | Batch_idx: 50 |  Loss_1: (0.0375) | Acc_1: (98.73%) (6445/6528)\n",
      "Epoch: 53 | Batch_idx: 60 |  Loss_1: (0.0363) | Acc_1: (98.81%) (7715/7808)\n",
      "Epoch: 53 | Batch_idx: 70 |  Loss_1: (0.0338) | Acc_1: (98.89%) (8987/9088)\n",
      "Epoch: 53 | Batch_idx: 80 |  Loss_1: (0.0326) | Acc_1: (98.89%) (10253/10368)\n",
      "Epoch: 53 | Batch_idx: 90 |  Loss_1: (0.0327) | Acc_1: (98.89%) (11519/11648)\n",
      "Epoch: 53 | Batch_idx: 100 |  Loss_1: (0.0326) | Acc_1: (98.91%) (12787/12928)\n",
      "Epoch: 53 | Batch_idx: 110 |  Loss_1: (0.0325) | Acc_1: (98.91%) (14053/14208)\n",
      "Epoch: 53 | Batch_idx: 120 |  Loss_1: (0.0324) | Acc_1: (98.91%) (15319/15488)\n",
      "Epoch: 53 | Batch_idx: 130 |  Loss_1: (0.0323) | Acc_1: (98.92%) (16587/16768)\n",
      "Epoch: 53 | Batch_idx: 140 |  Loss_1: (0.0319) | Acc_1: (98.93%) (17854/18048)\n",
      "Epoch: 53 | Batch_idx: 150 |  Loss_1: (0.0330) | Acc_1: (98.89%) (19114/19328)\n",
      "Epoch: 53 | Batch_idx: 160 |  Loss_1: (0.0325) | Acc_1: (98.90%) (20382/20608)\n",
      "Epoch: 53 | Batch_idx: 170 |  Loss_1: (0.0322) | Acc_1: (98.92%) (21652/21888)\n",
      "Epoch: 53 | Batch_idx: 180 |  Loss_1: (0.0325) | Acc_1: (98.90%) (22914/23168)\n",
      "Epoch: 53 | Batch_idx: 190 |  Loss_1: (0.0326) | Acc_1: (98.91%) (24181/24448)\n",
      "Epoch: 53 | Batch_idx: 200 |  Loss_1: (0.0327) | Acc_1: (98.90%) (25445/25728)\n",
      "Epoch: 53 | Batch_idx: 210 |  Loss_1: (0.0330) | Acc_1: (98.88%) (26706/27008)\n",
      "Epoch: 53 | Batch_idx: 220 |  Loss_1: (0.0331) | Acc_1: (98.88%) (27970/28288)\n",
      "Epoch: 53 | Batch_idx: 230 |  Loss_1: (0.0330) | Acc_1: (98.89%) (29240/29568)\n",
      "Epoch: 53 | Batch_idx: 240 |  Loss_1: (0.0333) | Acc_1: (98.88%) (30503/30848)\n",
      "Epoch: 53 | Batch_idx: 250 |  Loss_1: (0.0333) | Acc_1: (98.87%) (31766/32128)\n",
      "Epoch: 53 | Batch_idx: 260 |  Loss_1: (0.0336) | Acc_1: (98.87%) (33030/33408)\n",
      "Epoch: 53 | Batch_idx: 270 |  Loss_1: (0.0333) | Acc_1: (98.88%) (34300/34688)\n",
      "Epoch: 53 | Batch_idx: 280 |  Loss_1: (0.0329) | Acc_1: (98.89%) (35567/35968)\n",
      "Epoch: 53 | Batch_idx: 290 |  Loss_1: (0.0329) | Acc_1: (98.88%) (36830/37248)\n",
      "Epoch: 53 | Batch_idx: 300 |  Loss_1: (0.0332) | Acc_1: (98.87%) (38094/38528)\n",
      "Epoch: 53 | Batch_idx: 310 |  Loss_1: (0.0328) | Acc_1: (98.89%) (39367/39808)\n",
      "Epoch: 53 | Batch_idx: 320 |  Loss_1: (0.0331) | Acc_1: (98.88%) (40629/41088)\n",
      "Epoch: 53 | Batch_idx: 330 |  Loss_1: (0.0329) | Acc_1: (98.89%) (41899/42368)\n",
      "Epoch: 53 | Batch_idx: 340 |  Loss_1: (0.0331) | Acc_1: (98.88%) (43160/43648)\n",
      "Epoch: 53 | Batch_idx: 350 |  Loss_1: (0.0329) | Acc_1: (98.89%) (44430/44928)\n",
      "Epoch: 53 | Batch_idx: 360 |  Loss_1: (0.0333) | Acc_1: (98.88%) (45691/46208)\n",
      "Epoch: 53 | Batch_idx: 370 |  Loss_1: (0.0336) | Acc_1: (98.88%) (46954/47488)\n",
      "Epoch: 53 | Batch_idx: 380 |  Loss_1: (0.0334) | Acc_1: (98.88%) (48223/48768)\n",
      "Epoch: 53 | Batch_idx: 390 |  Loss_1: (0.0337) | Acc_1: (98.88%) (49438/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4931) | Acc: (90.36%) (9036/10000)\n",
      "Epoch: 54 | Batch_idx: 0 |  Loss_1: (0.0085) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 54 | Batch_idx: 10 |  Loss_1: (0.0243) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 54 | Batch_idx: 20 |  Loss_1: (0.0310) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 54 | Batch_idx: 30 |  Loss_1: (0.0312) | Acc_1: (99.02%) (3929/3968)\n",
      "Epoch: 54 | Batch_idx: 40 |  Loss_1: (0.0313) | Acc_1: (98.99%) (5195/5248)\n",
      "Epoch: 54 | Batch_idx: 50 |  Loss_1: (0.0316) | Acc_1: (99.00%) (6463/6528)\n",
      "Epoch: 54 | Batch_idx: 60 |  Loss_1: (0.0305) | Acc_1: (99.01%) (7731/7808)\n",
      "Epoch: 54 | Batch_idx: 70 |  Loss_1: (0.0288) | Acc_1: (99.06%) (9003/9088)\n",
      "Epoch: 54 | Batch_idx: 80 |  Loss_1: (0.0269) | Acc_1: (99.14%) (10279/10368)\n",
      "Epoch: 54 | Batch_idx: 90 |  Loss_1: (0.0262) | Acc_1: (99.15%) (11549/11648)\n",
      "Epoch: 54 | Batch_idx: 100 |  Loss_1: (0.0255) | Acc_1: (99.16%) (12819/12928)\n",
      "Epoch: 54 | Batch_idx: 110 |  Loss_1: (0.0248) | Acc_1: (99.17%) (14090/14208)\n",
      "Epoch: 54 | Batch_idx: 120 |  Loss_1: (0.0251) | Acc_1: (99.19%) (15362/15488)\n",
      "Epoch: 54 | Batch_idx: 130 |  Loss_1: (0.0251) | Acc_1: (99.17%) (16629/16768)\n",
      "Epoch: 54 | Batch_idx: 140 |  Loss_1: (0.0266) | Acc_1: (99.12%) (17889/18048)\n",
      "Epoch: 54 | Batch_idx: 150 |  Loss_1: (0.0269) | Acc_1: (99.09%) (19152/19328)\n",
      "Epoch: 54 | Batch_idx: 160 |  Loss_1: (0.0273) | Acc_1: (99.07%) (20416/20608)\n",
      "Epoch: 54 | Batch_idx: 170 |  Loss_1: (0.0272) | Acc_1: (99.07%) (21684/21888)\n",
      "Epoch: 54 | Batch_idx: 180 |  Loss_1: (0.0277) | Acc_1: (99.04%) (22946/23168)\n",
      "Epoch: 54 | Batch_idx: 190 |  Loss_1: (0.0287) | Acc_1: (99.00%) (24204/24448)\n",
      "Epoch: 54 | Batch_idx: 200 |  Loss_1: (0.0289) | Acc_1: (98.99%) (25468/25728)\n",
      "Epoch: 54 | Batch_idx: 210 |  Loss_1: (0.0290) | Acc_1: (99.00%) (26738/27008)\n",
      "Epoch: 54 | Batch_idx: 220 |  Loss_1: (0.0293) | Acc_1: (98.97%) (27998/28288)\n",
      "Epoch: 54 | Batch_idx: 230 |  Loss_1: (0.0293) | Acc_1: (98.98%) (29266/29568)\n",
      "Epoch: 54 | Batch_idx: 240 |  Loss_1: (0.0299) | Acc_1: (98.96%) (30527/30848)\n",
      "Epoch: 54 | Batch_idx: 250 |  Loss_1: (0.0307) | Acc_1: (98.93%) (31783/32128)\n",
      "Epoch: 54 | Batch_idx: 260 |  Loss_1: (0.0309) | Acc_1: (98.91%) (33044/33408)\n",
      "Epoch: 54 | Batch_idx: 270 |  Loss_1: (0.0311) | Acc_1: (98.90%) (34306/34688)\n",
      "Epoch: 54 | Batch_idx: 280 |  Loss_1: (0.0317) | Acc_1: (98.89%) (35568/35968)\n",
      "Epoch: 54 | Batch_idx: 290 |  Loss_1: (0.0317) | Acc_1: (98.88%) (36831/37248)\n",
      "Epoch: 54 | Batch_idx: 300 |  Loss_1: (0.0317) | Acc_1: (98.88%) (38097/38528)\n",
      "Epoch: 54 | Batch_idx: 310 |  Loss_1: (0.0321) | Acc_1: (98.88%) (39362/39808)\n",
      "Epoch: 54 | Batch_idx: 320 |  Loss_1: (0.0323) | Acc_1: (98.88%) (40628/41088)\n",
      "Epoch: 54 | Batch_idx: 330 |  Loss_1: (0.0324) | Acc_1: (98.88%) (41892/42368)\n",
      "Epoch: 54 | Batch_idx: 340 |  Loss_1: (0.0325) | Acc_1: (98.87%) (43155/43648)\n",
      "Epoch: 54 | Batch_idx: 350 |  Loss_1: (0.0325) | Acc_1: (98.87%) (44422/44928)\n",
      "Epoch: 54 | Batch_idx: 360 |  Loss_1: (0.0328) | Acc_1: (98.86%) (45683/46208)\n",
      "Epoch: 54 | Batch_idx: 370 |  Loss_1: (0.0328) | Acc_1: (98.85%) (46940/47488)\n",
      "Epoch: 54 | Batch_idx: 380 |  Loss_1: (0.0329) | Acc_1: (98.85%) (48205/48768)\n",
      "Epoch: 54 | Batch_idx: 390 |  Loss_1: (0.0329) | Acc_1: (98.85%) (49423/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4795) | Acc: (90.71%) (9071/10000)\n",
      "Epoch: 55 | Batch_idx: 0 |  Loss_1: (0.0437) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 55 | Batch_idx: 10 |  Loss_1: (0.0240) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 55 | Batch_idx: 20 |  Loss_1: (0.0231) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 55 | Batch_idx: 30 |  Loss_1: (0.0260) | Acc_1: (98.99%) (3928/3968)\n",
      "Epoch: 55 | Batch_idx: 40 |  Loss_1: (0.0262) | Acc_1: (99.07%) (5199/5248)\n",
      "Epoch: 55 | Batch_idx: 50 |  Loss_1: (0.0304) | Acc_1: (98.99%) (6462/6528)\n",
      "Epoch: 55 | Batch_idx: 60 |  Loss_1: (0.0306) | Acc_1: (98.99%) (7729/7808)\n",
      "Epoch: 55 | Batch_idx: 70 |  Loss_1: (0.0305) | Acc_1: (98.99%) (8996/9088)\n",
      "Epoch: 55 | Batch_idx: 80 |  Loss_1: (0.0306) | Acc_1: (98.97%) (10261/10368)\n",
      "Epoch: 55 | Batch_idx: 90 |  Loss_1: (0.0294) | Acc_1: (99.00%) (11531/11648)\n",
      "Epoch: 55 | Batch_idx: 100 |  Loss_1: (0.0288) | Acc_1: (99.02%) (12801/12928)\n",
      "Epoch: 55 | Batch_idx: 110 |  Loss_1: (0.0309) | Acc_1: (98.95%) (14059/14208)\n",
      "Epoch: 55 | Batch_idx: 120 |  Loss_1: (0.0318) | Acc_1: (98.92%) (15321/15488)\n",
      "Epoch: 55 | Batch_idx: 130 |  Loss_1: (0.0314) | Acc_1: (98.94%) (16591/16768)\n",
      "Epoch: 55 | Batch_idx: 140 |  Loss_1: (0.0321) | Acc_1: (98.94%) (17856/18048)\n",
      "Epoch: 55 | Batch_idx: 150 |  Loss_1: (0.0329) | Acc_1: (98.89%) (19114/19328)\n",
      "Epoch: 55 | Batch_idx: 160 |  Loss_1: (0.0330) | Acc_1: (98.86%) (20374/20608)\n",
      "Epoch: 55 | Batch_idx: 170 |  Loss_1: (0.0333) | Acc_1: (98.87%) (21640/21888)\n",
      "Epoch: 55 | Batch_idx: 180 |  Loss_1: (0.0331) | Acc_1: (98.87%) (22907/23168)\n",
      "Epoch: 55 | Batch_idx: 190 |  Loss_1: (0.0332) | Acc_1: (98.85%) (24166/24448)\n",
      "Epoch: 55 | Batch_idx: 200 |  Loss_1: (0.0332) | Acc_1: (98.83%) (25428/25728)\n",
      "Epoch: 55 | Batch_idx: 210 |  Loss_1: (0.0329) | Acc_1: (98.85%) (26697/27008)\n",
      "Epoch: 55 | Batch_idx: 220 |  Loss_1: (0.0322) | Acc_1: (98.87%) (27967/28288)\n",
      "Epoch: 55 | Batch_idx: 230 |  Loss_1: (0.0321) | Acc_1: (98.87%) (29234/29568)\n",
      "Epoch: 55 | Batch_idx: 240 |  Loss_1: (0.0321) | Acc_1: (98.88%) (30503/30848)\n",
      "Epoch: 55 | Batch_idx: 250 |  Loss_1: (0.0322) | Acc_1: (98.89%) (31771/32128)\n",
      "Epoch: 55 | Batch_idx: 260 |  Loss_1: (0.0320) | Acc_1: (98.89%) (33037/33408)\n",
      "Epoch: 55 | Batch_idx: 270 |  Loss_1: (0.0318) | Acc_1: (98.90%) (34308/34688)\n",
      "Epoch: 55 | Batch_idx: 280 |  Loss_1: (0.0314) | Acc_1: (98.92%) (35579/35968)\n",
      "Epoch: 55 | Batch_idx: 290 |  Loss_1: (0.0310) | Acc_1: (98.94%) (36852/37248)\n",
      "Epoch: 55 | Batch_idx: 300 |  Loss_1: (0.0308) | Acc_1: (98.96%) (38126/38528)\n",
      "Epoch: 55 | Batch_idx: 310 |  Loss_1: (0.0305) | Acc_1: (98.97%) (39396/39808)\n",
      "Epoch: 55 | Batch_idx: 320 |  Loss_1: (0.0303) | Acc_1: (98.97%) (40663/41088)\n",
      "Epoch: 55 | Batch_idx: 330 |  Loss_1: (0.0303) | Acc_1: (98.97%) (41931/42368)\n",
      "Epoch: 55 | Batch_idx: 340 |  Loss_1: (0.0305) | Acc_1: (98.95%) (43190/43648)\n",
      "Epoch: 55 | Batch_idx: 350 |  Loss_1: (0.0302) | Acc_1: (98.97%) (44465/44928)\n",
      "Epoch: 55 | Batch_idx: 360 |  Loss_1: (0.0302) | Acc_1: (98.96%) (45729/46208)\n",
      "Epoch: 55 | Batch_idx: 370 |  Loss_1: (0.0301) | Acc_1: (98.97%) (46998/47488)\n",
      "Epoch: 55 | Batch_idx: 380 |  Loss_1: (0.0301) | Acc_1: (98.97%) (48267/48768)\n",
      "Epoch: 55 | Batch_idx: 390 |  Loss_1: (0.0301) | Acc_1: (98.97%) (49486/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4887) | Acc: (90.73%) (9073/10000)\n",
      "Epoch: 56 | Batch_idx: 0 |  Loss_1: (0.0176) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 56 | Batch_idx: 10 |  Loss_1: (0.0170) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 56 | Batch_idx: 20 |  Loss_1: (0.0185) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 56 | Batch_idx: 30 |  Loss_1: (0.0225) | Acc_1: (99.29%) (3940/3968)\n",
      "Epoch: 56 | Batch_idx: 40 |  Loss_1: (0.0246) | Acc_1: (99.10%) (5201/5248)\n",
      "Epoch: 56 | Batch_idx: 50 |  Loss_1: (0.0244) | Acc_1: (99.14%) (6472/6528)\n",
      "Epoch: 56 | Batch_idx: 60 |  Loss_1: (0.0269) | Acc_1: (99.08%) (7736/7808)\n",
      "Epoch: 56 | Batch_idx: 70 |  Loss_1: (0.0267) | Acc_1: (99.08%) (9004/9088)\n",
      "Epoch: 56 | Batch_idx: 80 |  Loss_1: (0.0272) | Acc_1: (99.05%) (10269/10368)\n",
      "Epoch: 56 | Batch_idx: 90 |  Loss_1: (0.0284) | Acc_1: (99.02%) (11534/11648)\n",
      "Epoch: 56 | Batch_idx: 100 |  Loss_1: (0.0279) | Acc_1: (99.03%) (12802/12928)\n",
      "Epoch: 56 | Batch_idx: 110 |  Loss_1: (0.0284) | Acc_1: (98.99%) (14065/14208)\n",
      "Epoch: 56 | Batch_idx: 120 |  Loss_1: (0.0290) | Acc_1: (98.96%) (15327/15488)\n",
      "Epoch: 56 | Batch_idx: 130 |  Loss_1: (0.0285) | Acc_1: (98.97%) (16596/16768)\n",
      "Epoch: 56 | Batch_idx: 140 |  Loss_1: (0.0290) | Acc_1: (98.97%) (17862/18048)\n",
      "Epoch: 56 | Batch_idx: 150 |  Loss_1: (0.0293) | Acc_1: (98.98%) (19131/19328)\n",
      "Epoch: 56 | Batch_idx: 160 |  Loss_1: (0.0312) | Acc_1: (98.90%) (20382/20608)\n",
      "Epoch: 56 | Batch_idx: 170 |  Loss_1: (0.0306) | Acc_1: (98.94%) (21656/21888)\n",
      "Epoch: 56 | Batch_idx: 180 |  Loss_1: (0.0301) | Acc_1: (98.95%) (22925/23168)\n",
      "Epoch: 56 | Batch_idx: 190 |  Loss_1: (0.0306) | Acc_1: (98.93%) (24187/24448)\n",
      "Epoch: 56 | Batch_idx: 200 |  Loss_1: (0.0303) | Acc_1: (98.94%) (25456/25728)\n",
      "Epoch: 56 | Batch_idx: 210 |  Loss_1: (0.0303) | Acc_1: (98.93%) (26720/27008)\n",
      "Epoch: 56 | Batch_idx: 220 |  Loss_1: (0.0305) | Acc_1: (98.92%) (27983/28288)\n",
      "Epoch: 56 | Batch_idx: 230 |  Loss_1: (0.0299) | Acc_1: (98.95%) (29257/29568)\n",
      "Epoch: 56 | Batch_idx: 240 |  Loss_1: (0.0302) | Acc_1: (98.94%) (30521/30848)\n",
      "Epoch: 56 | Batch_idx: 250 |  Loss_1: (0.0306) | Acc_1: (98.92%) (31780/32128)\n",
      "Epoch: 56 | Batch_idx: 260 |  Loss_1: (0.0307) | Acc_1: (98.92%) (33047/33408)\n",
      "Epoch: 56 | Batch_idx: 270 |  Loss_1: (0.0315) | Acc_1: (98.91%) (34310/34688)\n",
      "Epoch: 56 | Batch_idx: 280 |  Loss_1: (0.0318) | Acc_1: (98.89%) (35570/35968)\n",
      "Epoch: 56 | Batch_idx: 290 |  Loss_1: (0.0321) | Acc_1: (98.88%) (36832/37248)\n",
      "Epoch: 56 | Batch_idx: 300 |  Loss_1: (0.0320) | Acc_1: (98.89%) (38101/38528)\n",
      "Epoch: 56 | Batch_idx: 310 |  Loss_1: (0.0323) | Acc_1: (98.89%) (39365/39808)\n",
      "Epoch: 56 | Batch_idx: 320 |  Loss_1: (0.0322) | Acc_1: (98.90%) (40636/41088)\n",
      "Epoch: 56 | Batch_idx: 330 |  Loss_1: (0.0324) | Acc_1: (98.89%) (41897/42368)\n",
      "Epoch: 56 | Batch_idx: 340 |  Loss_1: (0.0324) | Acc_1: (98.88%) (43161/43648)\n",
      "Epoch: 56 | Batch_idx: 350 |  Loss_1: (0.0322) | Acc_1: (98.88%) (44426/44928)\n",
      "Epoch: 56 | Batch_idx: 360 |  Loss_1: (0.0324) | Acc_1: (98.87%) (45688/46208)\n",
      "Epoch: 56 | Batch_idx: 370 |  Loss_1: (0.0323) | Acc_1: (98.86%) (46949/47488)\n",
      "Epoch: 56 | Batch_idx: 380 |  Loss_1: (0.0321) | Acc_1: (98.87%) (48216/48768)\n",
      "Epoch: 56 | Batch_idx: 390 |  Loss_1: (0.0319) | Acc_1: (98.87%) (49437/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5181) | Acc: (90.19%) (9019/10000)\n",
      "Epoch: 57 | Batch_idx: 0 |  Loss_1: (0.0214) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 57 | Batch_idx: 10 |  Loss_1: (0.0277) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 57 | Batch_idx: 20 |  Loss_1: (0.0269) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 57 | Batch_idx: 30 |  Loss_1: (0.0290) | Acc_1: (98.94%) (3926/3968)\n",
      "Epoch: 57 | Batch_idx: 40 |  Loss_1: (0.0284) | Acc_1: (98.91%) (5191/5248)\n",
      "Epoch: 57 | Batch_idx: 50 |  Loss_1: (0.0294) | Acc_1: (98.88%) (6455/6528)\n",
      "Epoch: 57 | Batch_idx: 60 |  Loss_1: (0.0317) | Acc_1: (98.83%) (7717/7808)\n",
      "Epoch: 57 | Batch_idx: 70 |  Loss_1: (0.0331) | Acc_1: (98.81%) (8980/9088)\n",
      "Epoch: 57 | Batch_idx: 80 |  Loss_1: (0.0347) | Acc_1: (98.75%) (10238/10368)\n",
      "Epoch: 57 | Batch_idx: 90 |  Loss_1: (0.0347) | Acc_1: (98.75%) (11502/11648)\n",
      "Epoch: 57 | Batch_idx: 100 |  Loss_1: (0.0330) | Acc_1: (98.85%) (12779/12928)\n",
      "Epoch: 57 | Batch_idx: 110 |  Loss_1: (0.0328) | Acc_1: (98.87%) (14048/14208)\n",
      "Epoch: 57 | Batch_idx: 120 |  Loss_1: (0.0338) | Acc_1: (98.84%) (15309/15488)\n",
      "Epoch: 57 | Batch_idx: 130 |  Loss_1: (0.0331) | Acc_1: (98.87%) (16579/16768)\n",
      "Epoch: 57 | Batch_idx: 140 |  Loss_1: (0.0340) | Acc_1: (98.86%) (17843/18048)\n",
      "Epoch: 57 | Batch_idx: 150 |  Loss_1: (0.0340) | Acc_1: (98.84%) (19104/19328)\n",
      "Epoch: 57 | Batch_idx: 160 |  Loss_1: (0.0339) | Acc_1: (98.85%) (20371/20608)\n",
      "Epoch: 57 | Batch_idx: 170 |  Loss_1: (0.0342) | Acc_1: (98.83%) (21632/21888)\n",
      "Epoch: 57 | Batch_idx: 180 |  Loss_1: (0.0340) | Acc_1: (98.85%) (22902/23168)\n",
      "Epoch: 57 | Batch_idx: 190 |  Loss_1: (0.0339) | Acc_1: (98.85%) (24168/24448)\n",
      "Epoch: 57 | Batch_idx: 200 |  Loss_1: (0.0334) | Acc_1: (98.87%) (25438/25728)\n",
      "Epoch: 57 | Batch_idx: 210 |  Loss_1: (0.0330) | Acc_1: (98.88%) (26706/27008)\n",
      "Epoch: 57 | Batch_idx: 220 |  Loss_1: (0.0331) | Acc_1: (98.86%) (27966/28288)\n",
      "Epoch: 57 | Batch_idx: 230 |  Loss_1: (0.0333) | Acc_1: (98.87%) (29233/29568)\n",
      "Epoch: 57 | Batch_idx: 240 |  Loss_1: (0.0328) | Acc_1: (98.88%) (30503/30848)\n",
      "Epoch: 57 | Batch_idx: 250 |  Loss_1: (0.0328) | Acc_1: (98.89%) (31770/32128)\n",
      "Epoch: 57 | Batch_idx: 260 |  Loss_1: (0.0323) | Acc_1: (98.90%) (33039/33408)\n",
      "Epoch: 57 | Batch_idx: 270 |  Loss_1: (0.0322) | Acc_1: (98.90%) (34305/34688)\n",
      "Epoch: 57 | Batch_idx: 280 |  Loss_1: (0.0320) | Acc_1: (98.90%) (35574/35968)\n",
      "Epoch: 57 | Batch_idx: 290 |  Loss_1: (0.0321) | Acc_1: (98.90%) (36837/37248)\n",
      "Epoch: 57 | Batch_idx: 300 |  Loss_1: (0.0320) | Acc_1: (98.90%) (38105/38528)\n",
      "Epoch: 57 | Batch_idx: 310 |  Loss_1: (0.0326) | Acc_1: (98.90%) (39370/39808)\n",
      "Epoch: 57 | Batch_idx: 320 |  Loss_1: (0.0326) | Acc_1: (98.91%) (40639/41088)\n",
      "Epoch: 57 | Batch_idx: 330 |  Loss_1: (0.0323) | Acc_1: (98.91%) (41907/42368)\n",
      "Epoch: 57 | Batch_idx: 340 |  Loss_1: (0.0323) | Acc_1: (98.90%) (43169/43648)\n",
      "Epoch: 57 | Batch_idx: 350 |  Loss_1: (0.0325) | Acc_1: (98.90%) (44434/44928)\n",
      "Epoch: 57 | Batch_idx: 360 |  Loss_1: (0.0327) | Acc_1: (98.88%) (45692/46208)\n",
      "Epoch: 57 | Batch_idx: 370 |  Loss_1: (0.0333) | Acc_1: (98.88%) (46956/47488)\n",
      "Epoch: 57 | Batch_idx: 380 |  Loss_1: (0.0334) | Acc_1: (98.87%) (48219/48768)\n",
      "Epoch: 57 | Batch_idx: 390 |  Loss_1: (0.0333) | Acc_1: (98.88%) (49441/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5001) | Acc: (90.41%) (9041/10000)\n",
      "Epoch: 58 | Batch_idx: 0 |  Loss_1: (0.0272) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 58 | Batch_idx: 10 |  Loss_1: (0.0305) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 58 | Batch_idx: 20 |  Loss_1: (0.0292) | Acc_1: (98.88%) (2658/2688)\n",
      "Epoch: 58 | Batch_idx: 30 |  Loss_1: (0.0315) | Acc_1: (98.87%) (3923/3968)\n",
      "Epoch: 58 | Batch_idx: 40 |  Loss_1: (0.0315) | Acc_1: (98.80%) (5185/5248)\n",
      "Epoch: 58 | Batch_idx: 50 |  Loss_1: (0.0322) | Acc_1: (98.81%) (6450/6528)\n",
      "Epoch: 58 | Batch_idx: 60 |  Loss_1: (0.0306) | Acc_1: (98.87%) (7720/7808)\n",
      "Epoch: 58 | Batch_idx: 70 |  Loss_1: (0.0294) | Acc_1: (98.93%) (8991/9088)\n",
      "Epoch: 58 | Batch_idx: 80 |  Loss_1: (0.0296) | Acc_1: (98.92%) (10256/10368)\n",
      "Epoch: 58 | Batch_idx: 90 |  Loss_1: (0.0289) | Acc_1: (98.96%) (11527/11648)\n",
      "Epoch: 58 | Batch_idx: 100 |  Loss_1: (0.0292) | Acc_1: (98.96%) (12794/12928)\n",
      "Epoch: 58 | Batch_idx: 110 |  Loss_1: (0.0282) | Acc_1: (98.98%) (14063/14208)\n",
      "Epoch: 58 | Batch_idx: 120 |  Loss_1: (0.0290) | Acc_1: (98.97%) (15329/15488)\n",
      "Epoch: 58 | Batch_idx: 130 |  Loss_1: (0.0280) | Acc_1: (99.02%) (16603/16768)\n",
      "Epoch: 58 | Batch_idx: 140 |  Loss_1: (0.0275) | Acc_1: (99.03%) (17873/18048)\n",
      "Epoch: 58 | Batch_idx: 150 |  Loss_1: (0.0273) | Acc_1: (99.04%) (19142/19328)\n",
      "Epoch: 58 | Batch_idx: 160 |  Loss_1: (0.0283) | Acc_1: (98.99%) (20399/20608)\n",
      "Epoch: 58 | Batch_idx: 170 |  Loss_1: (0.0292) | Acc_1: (98.94%) (21657/21888)\n",
      "Epoch: 58 | Batch_idx: 180 |  Loss_1: (0.0301) | Acc_1: (98.90%) (22913/23168)\n",
      "Epoch: 58 | Batch_idx: 190 |  Loss_1: (0.0302) | Acc_1: (98.90%) (24178/24448)\n",
      "Epoch: 58 | Batch_idx: 200 |  Loss_1: (0.0305) | Acc_1: (98.89%) (25442/25728)\n",
      "Epoch: 58 | Batch_idx: 210 |  Loss_1: (0.0313) | Acc_1: (98.87%) (26702/27008)\n",
      "Epoch: 58 | Batch_idx: 220 |  Loss_1: (0.0313) | Acc_1: (98.87%) (27969/28288)\n",
      "Epoch: 58 | Batch_idx: 230 |  Loss_1: (0.0310) | Acc_1: (98.88%) (29236/29568)\n",
      "Epoch: 58 | Batch_idx: 240 |  Loss_1: (0.0308) | Acc_1: (98.88%) (30501/30848)\n",
      "Epoch: 58 | Batch_idx: 250 |  Loss_1: (0.0305) | Acc_1: (98.89%) (31772/32128)\n",
      "Epoch: 58 | Batch_idx: 260 |  Loss_1: (0.0306) | Acc_1: (98.88%) (33034/33408)\n",
      "Epoch: 58 | Batch_idx: 270 |  Loss_1: (0.0306) | Acc_1: (98.89%) (34303/34688)\n",
      "Epoch: 58 | Batch_idx: 280 |  Loss_1: (0.0310) | Acc_1: (98.87%) (35561/35968)\n",
      "Epoch: 58 | Batch_idx: 290 |  Loss_1: (0.0311) | Acc_1: (98.88%) (36829/37248)\n",
      "Epoch: 58 | Batch_idx: 300 |  Loss_1: (0.0316) | Acc_1: (98.86%) (38088/38528)\n",
      "Epoch: 58 | Batch_idx: 310 |  Loss_1: (0.0314) | Acc_1: (98.87%) (39357/39808)\n",
      "Epoch: 58 | Batch_idx: 320 |  Loss_1: (0.0311) | Acc_1: (98.88%) (40629/41088)\n",
      "Epoch: 58 | Batch_idx: 330 |  Loss_1: (0.0312) | Acc_1: (98.87%) (41890/42368)\n",
      "Epoch: 58 | Batch_idx: 340 |  Loss_1: (0.0312) | Acc_1: (98.88%) (43159/43648)\n",
      "Epoch: 58 | Batch_idx: 350 |  Loss_1: (0.0315) | Acc_1: (98.88%) (44423/44928)\n",
      "Epoch: 58 | Batch_idx: 360 |  Loss_1: (0.0314) | Acc_1: (98.87%) (45687/46208)\n",
      "Epoch: 58 | Batch_idx: 370 |  Loss_1: (0.0316) | Acc_1: (98.87%) (46953/47488)\n",
      "Epoch: 58 | Batch_idx: 380 |  Loss_1: (0.0319) | Acc_1: (98.86%) (48212/48768)\n",
      "Epoch: 58 | Batch_idx: 390 |  Loss_1: (0.0318) | Acc_1: (98.87%) (49433/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5121) | Acc: (90.26%) (9026/10000)\n",
      "Epoch: 59 | Batch_idx: 0 |  Loss_1: (0.0670) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 59 | Batch_idx: 10 |  Loss_1: (0.0435) | Acc_1: (98.30%) (1384/1408)\n",
      "Epoch: 59 | Batch_idx: 20 |  Loss_1: (0.0430) | Acc_1: (98.44%) (2646/2688)\n",
      "Epoch: 59 | Batch_idx: 30 |  Loss_1: (0.0436) | Acc_1: (98.59%) (3912/3968)\n",
      "Epoch: 59 | Batch_idx: 40 |  Loss_1: (0.0408) | Acc_1: (98.69%) (5179/5248)\n",
      "Epoch: 59 | Batch_idx: 50 |  Loss_1: (0.0409) | Acc_1: (98.64%) (6439/6528)\n",
      "Epoch: 59 | Batch_idx: 60 |  Loss_1: (0.0411) | Acc_1: (98.59%) (7698/7808)\n",
      "Epoch: 59 | Batch_idx: 70 |  Loss_1: (0.0415) | Acc_1: (98.54%) (8955/9088)\n",
      "Epoch: 59 | Batch_idx: 80 |  Loss_1: (0.0419) | Acc_1: (98.57%) (10220/10368)\n",
      "Epoch: 59 | Batch_idx: 90 |  Loss_1: (0.0403) | Acc_1: (98.63%) (11488/11648)\n",
      "Epoch: 59 | Batch_idx: 100 |  Loss_1: (0.0398) | Acc_1: (98.64%) (12752/12928)\n",
      "Epoch: 59 | Batch_idx: 110 |  Loss_1: (0.0394) | Acc_1: (98.64%) (14015/14208)\n",
      "Epoch: 59 | Batch_idx: 120 |  Loss_1: (0.0383) | Acc_1: (98.68%) (15283/15488)\n",
      "Epoch: 59 | Batch_idx: 130 |  Loss_1: (0.0378) | Acc_1: (98.69%) (16549/16768)\n",
      "Epoch: 59 | Batch_idx: 140 |  Loss_1: (0.0376) | Acc_1: (98.69%) (17812/18048)\n",
      "Epoch: 59 | Batch_idx: 150 |  Loss_1: (0.0380) | Acc_1: (98.67%) (19071/19328)\n",
      "Epoch: 59 | Batch_idx: 160 |  Loss_1: (0.0374) | Acc_1: (98.69%) (20338/20608)\n",
      "Epoch: 59 | Batch_idx: 170 |  Loss_1: (0.0366) | Acc_1: (98.73%) (21609/21888)\n",
      "Epoch: 59 | Batch_idx: 180 |  Loss_1: (0.0358) | Acc_1: (98.74%) (22877/23168)\n",
      "Epoch: 59 | Batch_idx: 190 |  Loss_1: (0.0361) | Acc_1: (98.74%) (24140/24448)\n",
      "Epoch: 59 | Batch_idx: 200 |  Loss_1: (0.0361) | Acc_1: (98.73%) (25402/25728)\n",
      "Epoch: 59 | Batch_idx: 210 |  Loss_1: (0.0357) | Acc_1: (98.74%) (26668/27008)\n",
      "Epoch: 59 | Batch_idx: 220 |  Loss_1: (0.0357) | Acc_1: (98.73%) (27930/28288)\n",
      "Epoch: 59 | Batch_idx: 230 |  Loss_1: (0.0355) | Acc_1: (98.74%) (29195/29568)\n",
      "Epoch: 59 | Batch_idx: 240 |  Loss_1: (0.0356) | Acc_1: (98.73%) (30456/30848)\n",
      "Epoch: 59 | Batch_idx: 250 |  Loss_1: (0.0354) | Acc_1: (98.73%) (31721/32128)\n",
      "Epoch: 59 | Batch_idx: 260 |  Loss_1: (0.0352) | Acc_1: (98.74%) (32988/33408)\n",
      "Epoch: 59 | Batch_idx: 270 |  Loss_1: (0.0347) | Acc_1: (98.76%) (34259/34688)\n",
      "Epoch: 59 | Batch_idx: 280 |  Loss_1: (0.0346) | Acc_1: (98.77%) (35526/35968)\n",
      "Epoch: 59 | Batch_idx: 290 |  Loss_1: (0.0343) | Acc_1: (98.77%) (36791/37248)\n",
      "Epoch: 59 | Batch_idx: 300 |  Loss_1: (0.0343) | Acc_1: (98.78%) (38059/38528)\n",
      "Epoch: 59 | Batch_idx: 310 |  Loss_1: (0.0343) | Acc_1: (98.78%) (39322/39808)\n",
      "Epoch: 59 | Batch_idx: 320 |  Loss_1: (0.0343) | Acc_1: (98.79%) (40589/41088)\n",
      "Epoch: 59 | Batch_idx: 330 |  Loss_1: (0.0341) | Acc_1: (98.79%) (41854/42368)\n",
      "Epoch: 59 | Batch_idx: 340 |  Loss_1: (0.0341) | Acc_1: (98.79%) (43118/43648)\n",
      "Epoch: 59 | Batch_idx: 350 |  Loss_1: (0.0343) | Acc_1: (98.78%) (44381/44928)\n",
      "Epoch: 59 | Batch_idx: 360 |  Loss_1: (0.0341) | Acc_1: (98.80%) (45652/46208)\n",
      "Epoch: 59 | Batch_idx: 370 |  Loss_1: (0.0338) | Acc_1: (98.81%) (46923/47488)\n",
      "Epoch: 59 | Batch_idx: 380 |  Loss_1: (0.0339) | Acc_1: (98.81%) (48188/48768)\n",
      "Epoch: 59 | Batch_idx: 390 |  Loss_1: (0.0338) | Acc_1: (98.80%) (49401/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5356) | Acc: (89.63%) (8963/10000)\n",
      "Epoch: 60 | Batch_idx: 0 |  Loss_1: (0.0444) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 60 | Batch_idx: 10 |  Loss_1: (0.0311) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 60 | Batch_idx: 20 |  Loss_1: (0.0287) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 60 | Batch_idx: 30 |  Loss_1: (0.0322) | Acc_1: (98.87%) (3923/3968)\n",
      "Epoch: 60 | Batch_idx: 40 |  Loss_1: (0.0331) | Acc_1: (98.78%) (5184/5248)\n",
      "Epoch: 60 | Batch_idx: 50 |  Loss_1: (0.0334) | Acc_1: (98.84%) (6452/6528)\n",
      "Epoch: 60 | Batch_idx: 60 |  Loss_1: (0.0334) | Acc_1: (98.89%) (7721/7808)\n",
      "Epoch: 60 | Batch_idx: 70 |  Loss_1: (0.0327) | Acc_1: (98.93%) (8991/9088)\n",
      "Epoch: 60 | Batch_idx: 80 |  Loss_1: (0.0325) | Acc_1: (98.92%) (10256/10368)\n",
      "Epoch: 60 | Batch_idx: 90 |  Loss_1: (0.0321) | Acc_1: (98.90%) (11520/11648)\n",
      "Epoch: 60 | Batch_idx: 100 |  Loss_1: (0.0319) | Acc_1: (98.89%) (12785/12928)\n",
      "Epoch: 60 | Batch_idx: 110 |  Loss_1: (0.0320) | Acc_1: (98.89%) (14050/14208)\n",
      "Epoch: 60 | Batch_idx: 120 |  Loss_1: (0.0312) | Acc_1: (98.92%) (15320/15488)\n",
      "Epoch: 60 | Batch_idx: 130 |  Loss_1: (0.0303) | Acc_1: (98.94%) (16590/16768)\n",
      "Epoch: 60 | Batch_idx: 140 |  Loss_1: (0.0304) | Acc_1: (98.94%) (17857/18048)\n",
      "Epoch: 60 | Batch_idx: 150 |  Loss_1: (0.0308) | Acc_1: (98.95%) (19125/19328)\n",
      "Epoch: 60 | Batch_idx: 160 |  Loss_1: (0.0304) | Acc_1: (98.95%) (20391/20608)\n",
      "Epoch: 60 | Batch_idx: 170 |  Loss_1: (0.0301) | Acc_1: (98.97%) (21662/21888)\n",
      "Epoch: 60 | Batch_idx: 180 |  Loss_1: (0.0300) | Acc_1: (98.97%) (22930/23168)\n",
      "Epoch: 60 | Batch_idx: 190 |  Loss_1: (0.0295) | Acc_1: (98.99%) (24200/24448)\n",
      "Epoch: 60 | Batch_idx: 200 |  Loss_1: (0.0295) | Acc_1: (98.99%) (25468/25728)\n",
      "Epoch: 60 | Batch_idx: 210 |  Loss_1: (0.0294) | Acc_1: (98.99%) (26735/27008)\n",
      "Epoch: 60 | Batch_idx: 220 |  Loss_1: (0.0291) | Acc_1: (98.99%) (28003/28288)\n",
      "Epoch: 60 | Batch_idx: 230 |  Loss_1: (0.0286) | Acc_1: (99.01%) (29275/29568)\n",
      "Epoch: 60 | Batch_idx: 240 |  Loss_1: (0.0283) | Acc_1: (99.02%) (30547/30848)\n",
      "Epoch: 60 | Batch_idx: 250 |  Loss_1: (0.0283) | Acc_1: (99.02%) (31812/32128)\n",
      "Epoch: 60 | Batch_idx: 260 |  Loss_1: (0.0281) | Acc_1: (99.03%) (33084/33408)\n",
      "Epoch: 60 | Batch_idx: 270 |  Loss_1: (0.0279) | Acc_1: (99.03%) (34353/34688)\n",
      "Epoch: 60 | Batch_idx: 280 |  Loss_1: (0.0279) | Acc_1: (99.04%) (35621/35968)\n",
      "Epoch: 60 | Batch_idx: 290 |  Loss_1: (0.0279) | Acc_1: (99.04%) (36889/37248)\n",
      "Epoch: 60 | Batch_idx: 300 |  Loss_1: (0.0280) | Acc_1: (99.04%) (38159/38528)\n",
      "Epoch: 60 | Batch_idx: 310 |  Loss_1: (0.0285) | Acc_1: (99.03%) (39420/39808)\n",
      "Epoch: 60 | Batch_idx: 320 |  Loss_1: (0.0282) | Acc_1: (99.03%) (40690/41088)\n",
      "Epoch: 60 | Batch_idx: 330 |  Loss_1: (0.0286) | Acc_1: (99.02%) (41951/42368)\n",
      "Epoch: 60 | Batch_idx: 340 |  Loss_1: (0.0288) | Acc_1: (99.01%) (43217/43648)\n",
      "Epoch: 60 | Batch_idx: 350 |  Loss_1: (0.0288) | Acc_1: (99.01%) (44485/44928)\n",
      "Epoch: 60 | Batch_idx: 360 |  Loss_1: (0.0288) | Acc_1: (99.01%) (45752/46208)\n",
      "Epoch: 60 | Batch_idx: 370 |  Loss_1: (0.0286) | Acc_1: (99.03%) (47025/47488)\n",
      "Epoch: 60 | Batch_idx: 380 |  Loss_1: (0.0285) | Acc_1: (99.03%) (48297/48768)\n",
      "Epoch: 60 | Batch_idx: 390 |  Loss_1: (0.0288) | Acc_1: (99.03%) (49514/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4976) | Acc: (90.62%) (9062/10000)\n",
      "Epoch: 61 | Batch_idx: 0 |  Loss_1: (0.0163) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 61 | Batch_idx: 10 |  Loss_1: (0.0326) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 61 | Batch_idx: 20 |  Loss_1: (0.0300) | Acc_1: (99.03%) (2662/2688)\n",
      "Epoch: 61 | Batch_idx: 30 |  Loss_1: (0.0272) | Acc_1: (99.07%) (3931/3968)\n",
      "Epoch: 61 | Batch_idx: 40 |  Loss_1: (0.0287) | Acc_1: (99.01%) (5196/5248)\n",
      "Epoch: 61 | Batch_idx: 50 |  Loss_1: (0.0265) | Acc_1: (99.07%) (6467/6528)\n",
      "Epoch: 61 | Batch_idx: 60 |  Loss_1: (0.0248) | Acc_1: (99.15%) (7742/7808)\n",
      "Epoch: 61 | Batch_idx: 70 |  Loss_1: (0.0231) | Acc_1: (99.22%) (9017/9088)\n",
      "Epoch: 61 | Batch_idx: 80 |  Loss_1: (0.0235) | Acc_1: (99.19%) (10284/10368)\n",
      "Epoch: 61 | Batch_idx: 90 |  Loss_1: (0.0232) | Acc_1: (99.19%) (11554/11648)\n",
      "Epoch: 61 | Batch_idx: 100 |  Loss_1: (0.0236) | Acc_1: (99.16%) (12819/12928)\n",
      "Epoch: 61 | Batch_idx: 110 |  Loss_1: (0.0241) | Acc_1: (99.16%) (14089/14208)\n",
      "Epoch: 61 | Batch_idx: 120 |  Loss_1: (0.0245) | Acc_1: (99.15%) (15357/15488)\n",
      "Epoch: 61 | Batch_idx: 130 |  Loss_1: (0.0244) | Acc_1: (99.14%) (16623/16768)\n",
      "Epoch: 61 | Batch_idx: 140 |  Loss_1: (0.0246) | Acc_1: (99.12%) (17890/18048)\n",
      "Epoch: 61 | Batch_idx: 150 |  Loss_1: (0.0247) | Acc_1: (99.13%) (19159/19328)\n",
      "Epoch: 61 | Batch_idx: 160 |  Loss_1: (0.0250) | Acc_1: (99.11%) (20424/20608)\n",
      "Epoch: 61 | Batch_idx: 170 |  Loss_1: (0.0247) | Acc_1: (99.12%) (21695/21888)\n",
      "Epoch: 61 | Batch_idx: 180 |  Loss_1: (0.0253) | Acc_1: (99.10%) (22960/23168)\n",
      "Epoch: 61 | Batch_idx: 190 |  Loss_1: (0.0250) | Acc_1: (99.10%) (24228/24448)\n",
      "Epoch: 61 | Batch_idx: 200 |  Loss_1: (0.0253) | Acc_1: (99.10%) (25496/25728)\n",
      "Epoch: 61 | Batch_idx: 210 |  Loss_1: (0.0253) | Acc_1: (99.09%) (26763/27008)\n",
      "Epoch: 61 | Batch_idx: 220 |  Loss_1: (0.0257) | Acc_1: (99.08%) (28027/28288)\n",
      "Epoch: 61 | Batch_idx: 230 |  Loss_1: (0.0259) | Acc_1: (99.06%) (29291/29568)\n",
      "Epoch: 61 | Batch_idx: 240 |  Loss_1: (0.0255) | Acc_1: (99.08%) (30563/30848)\n",
      "Epoch: 61 | Batch_idx: 250 |  Loss_1: (0.0261) | Acc_1: (99.06%) (31827/32128)\n",
      "Epoch: 61 | Batch_idx: 260 |  Loss_1: (0.0263) | Acc_1: (99.05%) (33092/33408)\n",
      "Epoch: 61 | Batch_idx: 270 |  Loss_1: (0.0267) | Acc_1: (99.05%) (34358/34688)\n",
      "Epoch: 61 | Batch_idx: 280 |  Loss_1: (0.0269) | Acc_1: (99.04%) (35623/35968)\n",
      "Epoch: 61 | Batch_idx: 290 |  Loss_1: (0.0268) | Acc_1: (99.05%) (36894/37248)\n",
      "Epoch: 61 | Batch_idx: 300 |  Loss_1: (0.0269) | Acc_1: (99.04%) (38160/38528)\n",
      "Epoch: 61 | Batch_idx: 310 |  Loss_1: (0.0270) | Acc_1: (99.03%) (39422/39808)\n",
      "Epoch: 61 | Batch_idx: 320 |  Loss_1: (0.0270) | Acc_1: (99.03%) (40689/41088)\n",
      "Epoch: 61 | Batch_idx: 330 |  Loss_1: (0.0270) | Acc_1: (99.03%) (41958/42368)\n",
      "Epoch: 61 | Batch_idx: 340 |  Loss_1: (0.0270) | Acc_1: (99.03%) (43224/43648)\n",
      "Epoch: 61 | Batch_idx: 350 |  Loss_1: (0.0272) | Acc_1: (99.03%) (44490/44928)\n",
      "Epoch: 61 | Batch_idx: 360 |  Loss_1: (0.0270) | Acc_1: (99.04%) (45764/46208)\n",
      "Epoch: 61 | Batch_idx: 370 |  Loss_1: (0.0271) | Acc_1: (99.03%) (47029/47488)\n",
      "Epoch: 61 | Batch_idx: 380 |  Loss_1: (0.0270) | Acc_1: (99.03%) (48297/48768)\n",
      "Epoch: 61 | Batch_idx: 390 |  Loss_1: (0.0269) | Acc_1: (99.04%) (49519/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4870) | Acc: (90.67%) (9067/10000)\n",
      "Epoch: 62 | Batch_idx: 0 |  Loss_1: (0.0208) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 62 | Batch_idx: 10 |  Loss_1: (0.0259) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 62 | Batch_idx: 20 |  Loss_1: (0.0288) | Acc_1: (98.81%) (2656/2688)\n",
      "Epoch: 62 | Batch_idx: 30 |  Loss_1: (0.0301) | Acc_1: (98.74%) (3918/3968)\n",
      "Epoch: 62 | Batch_idx: 40 |  Loss_1: (0.0278) | Acc_1: (98.86%) (5188/5248)\n",
      "Epoch: 62 | Batch_idx: 50 |  Loss_1: (0.0311) | Acc_1: (98.81%) (6450/6528)\n",
      "Epoch: 62 | Batch_idx: 60 |  Loss_1: (0.0291) | Acc_1: (98.87%) (7720/7808)\n",
      "Epoch: 62 | Batch_idx: 70 |  Loss_1: (0.0298) | Acc_1: (98.89%) (8987/9088)\n",
      "Epoch: 62 | Batch_idx: 80 |  Loss_1: (0.0294) | Acc_1: (98.91%) (10255/10368)\n",
      "Epoch: 62 | Batch_idx: 90 |  Loss_1: (0.0297) | Acc_1: (98.88%) (11518/11648)\n",
      "Epoch: 62 | Batch_idx: 100 |  Loss_1: (0.0290) | Acc_1: (98.95%) (12792/12928)\n",
      "Epoch: 62 | Batch_idx: 110 |  Loss_1: (0.0295) | Acc_1: (98.93%) (14056/14208)\n",
      "Epoch: 62 | Batch_idx: 120 |  Loss_1: (0.0291) | Acc_1: (98.95%) (15326/15488)\n",
      "Epoch: 62 | Batch_idx: 130 |  Loss_1: (0.0302) | Acc_1: (98.93%) (16588/16768)\n",
      "Epoch: 62 | Batch_idx: 140 |  Loss_1: (0.0301) | Acc_1: (98.90%) (17850/18048)\n",
      "Epoch: 62 | Batch_idx: 150 |  Loss_1: (0.0305) | Acc_1: (98.91%) (19117/19328)\n",
      "Epoch: 62 | Batch_idx: 160 |  Loss_1: (0.0300) | Acc_1: (98.92%) (20385/20608)\n",
      "Epoch: 62 | Batch_idx: 170 |  Loss_1: (0.0302) | Acc_1: (98.92%) (21651/21888)\n",
      "Epoch: 62 | Batch_idx: 180 |  Loss_1: (0.0296) | Acc_1: (98.94%) (22922/23168)\n",
      "Epoch: 62 | Batch_idx: 190 |  Loss_1: (0.0295) | Acc_1: (98.93%) (24187/24448)\n",
      "Epoch: 62 | Batch_idx: 200 |  Loss_1: (0.0289) | Acc_1: (98.96%) (25461/25728)\n",
      "Epoch: 62 | Batch_idx: 210 |  Loss_1: (0.0286) | Acc_1: (98.98%) (26733/27008)\n",
      "Epoch: 62 | Batch_idx: 220 |  Loss_1: (0.0282) | Acc_1: (99.00%) (28004/28288)\n",
      "Epoch: 62 | Batch_idx: 230 |  Loss_1: (0.0279) | Acc_1: (99.01%) (29274/29568)\n",
      "Epoch: 62 | Batch_idx: 240 |  Loss_1: (0.0275) | Acc_1: (99.02%) (30545/30848)\n",
      "Epoch: 62 | Batch_idx: 250 |  Loss_1: (0.0272) | Acc_1: (99.04%) (31819/32128)\n",
      "Epoch: 62 | Batch_idx: 260 |  Loss_1: (0.0270) | Acc_1: (99.04%) (33086/33408)\n",
      "Epoch: 62 | Batch_idx: 270 |  Loss_1: (0.0266) | Acc_1: (99.05%) (34357/34688)\n",
      "Epoch: 62 | Batch_idx: 280 |  Loss_1: (0.0264) | Acc_1: (99.05%) (35627/35968)\n",
      "Epoch: 62 | Batch_idx: 290 |  Loss_1: (0.0260) | Acc_1: (99.07%) (36900/37248)\n",
      "Epoch: 62 | Batch_idx: 300 |  Loss_1: (0.0260) | Acc_1: (99.07%) (38170/38528)\n",
      "Epoch: 62 | Batch_idx: 310 |  Loss_1: (0.0260) | Acc_1: (99.07%) (39436/39808)\n",
      "Epoch: 62 | Batch_idx: 320 |  Loss_1: (0.0263) | Acc_1: (99.06%) (40703/41088)\n",
      "Epoch: 62 | Batch_idx: 330 |  Loss_1: (0.0264) | Acc_1: (99.05%) (41967/42368)\n",
      "Epoch: 62 | Batch_idx: 340 |  Loss_1: (0.0269) | Acc_1: (99.04%) (43230/43648)\n",
      "Epoch: 62 | Batch_idx: 350 |  Loss_1: (0.0267) | Acc_1: (99.05%) (44500/44928)\n",
      "Epoch: 62 | Batch_idx: 360 |  Loss_1: (0.0270) | Acc_1: (99.03%) (45762/46208)\n",
      "Epoch: 62 | Batch_idx: 370 |  Loss_1: (0.0271) | Acc_1: (99.03%) (47026/47488)\n",
      "Epoch: 62 | Batch_idx: 380 |  Loss_1: (0.0272) | Acc_1: (99.03%) (48293/48768)\n",
      "Epoch: 62 | Batch_idx: 390 |  Loss_1: (0.0270) | Acc_1: (99.04%) (49518/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4992) | Acc: (90.33%) (9033/10000)\n",
      "Epoch: 63 | Batch_idx: 0 |  Loss_1: (0.0072) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 63 | Batch_idx: 10 |  Loss_1: (0.0182) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 63 | Batch_idx: 20 |  Loss_1: (0.0202) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 63 | Batch_idx: 30 |  Loss_1: (0.0203) | Acc_1: (99.34%) (3942/3968)\n",
      "Epoch: 63 | Batch_idx: 40 |  Loss_1: (0.0203) | Acc_1: (99.26%) (5209/5248)\n",
      "Epoch: 63 | Batch_idx: 50 |  Loss_1: (0.0229) | Acc_1: (99.19%) (6475/6528)\n",
      "Epoch: 63 | Batch_idx: 60 |  Loss_1: (0.0220) | Acc_1: (99.26%) (7750/7808)\n",
      "Epoch: 63 | Batch_idx: 70 |  Loss_1: (0.0230) | Acc_1: (99.20%) (9015/9088)\n",
      "Epoch: 63 | Batch_idx: 80 |  Loss_1: (0.0234) | Acc_1: (99.20%) (10285/10368)\n",
      "Epoch: 63 | Batch_idx: 90 |  Loss_1: (0.0232) | Acc_1: (99.22%) (11557/11648)\n",
      "Epoch: 63 | Batch_idx: 100 |  Loss_1: (0.0241) | Acc_1: (99.19%) (12823/12928)\n",
      "Epoch: 63 | Batch_idx: 110 |  Loss_1: (0.0243) | Acc_1: (99.19%) (14093/14208)\n",
      "Epoch: 63 | Batch_idx: 120 |  Loss_1: (0.0236) | Acc_1: (99.21%) (15366/15488)\n",
      "Epoch: 63 | Batch_idx: 130 |  Loss_1: (0.0228) | Acc_1: (99.23%) (16639/16768)\n",
      "Epoch: 63 | Batch_idx: 140 |  Loss_1: (0.0227) | Acc_1: (99.23%) (17909/18048)\n",
      "Epoch: 63 | Batch_idx: 150 |  Loss_1: (0.0227) | Acc_1: (99.25%) (19183/19328)\n",
      "Epoch: 63 | Batch_idx: 160 |  Loss_1: (0.0235) | Acc_1: (99.20%) (20444/20608)\n",
      "Epoch: 63 | Batch_idx: 170 |  Loss_1: (0.0238) | Acc_1: (99.20%) (21712/21888)\n",
      "Epoch: 63 | Batch_idx: 180 |  Loss_1: (0.0251) | Acc_1: (99.15%) (22971/23168)\n",
      "Epoch: 63 | Batch_idx: 190 |  Loss_1: (0.0251) | Acc_1: (99.16%) (24242/24448)\n",
      "Epoch: 63 | Batch_idx: 200 |  Loss_1: (0.0258) | Acc_1: (99.14%) (25507/25728)\n",
      "Epoch: 63 | Batch_idx: 210 |  Loss_1: (0.0261) | Acc_1: (99.13%) (26773/27008)\n",
      "Epoch: 63 | Batch_idx: 220 |  Loss_1: (0.0261) | Acc_1: (99.13%) (28042/28288)\n",
      "Epoch: 63 | Batch_idx: 230 |  Loss_1: (0.0258) | Acc_1: (99.14%) (29315/29568)\n",
      "Epoch: 63 | Batch_idx: 240 |  Loss_1: (0.0260) | Acc_1: (99.15%) (30585/30848)\n",
      "Epoch: 63 | Batch_idx: 250 |  Loss_1: (0.0261) | Acc_1: (99.13%) (31850/32128)\n",
      "Epoch: 63 | Batch_idx: 260 |  Loss_1: (0.0262) | Acc_1: (99.13%) (33118/33408)\n",
      "Epoch: 63 | Batch_idx: 270 |  Loss_1: (0.0259) | Acc_1: (99.14%) (34390/34688)\n",
      "Epoch: 63 | Batch_idx: 280 |  Loss_1: (0.0258) | Acc_1: (99.14%) (35660/35968)\n",
      "Epoch: 63 | Batch_idx: 290 |  Loss_1: (0.0257) | Acc_1: (99.14%) (36928/37248)\n",
      "Epoch: 63 | Batch_idx: 300 |  Loss_1: (0.0260) | Acc_1: (99.13%) (38193/38528)\n",
      "Epoch: 63 | Batch_idx: 310 |  Loss_1: (0.0256) | Acc_1: (99.14%) (39467/39808)\n",
      "Epoch: 63 | Batch_idx: 320 |  Loss_1: (0.0256) | Acc_1: (99.13%) (40732/41088)\n",
      "Epoch: 63 | Batch_idx: 330 |  Loss_1: (0.0253) | Acc_1: (99.15%) (42009/42368)\n",
      "Epoch: 63 | Batch_idx: 340 |  Loss_1: (0.0252) | Acc_1: (99.16%) (43280/43648)\n",
      "Epoch: 63 | Batch_idx: 350 |  Loss_1: (0.0253) | Acc_1: (99.15%) (44548/44928)\n",
      "Epoch: 63 | Batch_idx: 360 |  Loss_1: (0.0253) | Acc_1: (99.15%) (45815/46208)\n",
      "Epoch: 63 | Batch_idx: 370 |  Loss_1: (0.0252) | Acc_1: (99.16%) (47088/47488)\n",
      "Epoch: 63 | Batch_idx: 380 |  Loss_1: (0.0254) | Acc_1: (99.14%) (48351/48768)\n",
      "Epoch: 63 | Batch_idx: 390 |  Loss_1: (0.0254) | Acc_1: (99.15%) (49573/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5051) | Acc: (90.22%) (9022/10000)\n",
      "Epoch: 64 | Batch_idx: 0 |  Loss_1: (0.0076) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 64 | Batch_idx: 10 |  Loss_1: (0.0251) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 64 | Batch_idx: 20 |  Loss_1: (0.0240) | Acc_1: (99.07%) (2663/2688)\n",
      "Epoch: 64 | Batch_idx: 30 |  Loss_1: (0.0219) | Acc_1: (99.14%) (3934/3968)\n",
      "Epoch: 64 | Batch_idx: 40 |  Loss_1: (0.0190) | Acc_1: (99.24%) (5208/5248)\n",
      "Epoch: 64 | Batch_idx: 50 |  Loss_1: (0.0193) | Acc_1: (99.26%) (6480/6528)\n",
      "Epoch: 64 | Batch_idx: 60 |  Loss_1: (0.0207) | Acc_1: (99.24%) (7749/7808)\n",
      "Epoch: 64 | Batch_idx: 70 |  Loss_1: (0.0220) | Acc_1: (99.23%) (9018/9088)\n",
      "Epoch: 64 | Batch_idx: 80 |  Loss_1: (0.0212) | Acc_1: (99.24%) (10289/10368)\n",
      "Epoch: 64 | Batch_idx: 90 |  Loss_1: (0.0212) | Acc_1: (99.26%) (11562/11648)\n",
      "Epoch: 64 | Batch_idx: 100 |  Loss_1: (0.0217) | Acc_1: (99.24%) (12830/12928)\n",
      "Epoch: 64 | Batch_idx: 110 |  Loss_1: (0.0216) | Acc_1: (99.22%) (14097/14208)\n",
      "Epoch: 64 | Batch_idx: 120 |  Loss_1: (0.0218) | Acc_1: (99.20%) (15364/15488)\n",
      "Epoch: 64 | Batch_idx: 130 |  Loss_1: (0.0214) | Acc_1: (99.22%) (16637/16768)\n",
      "Epoch: 64 | Batch_idx: 140 |  Loss_1: (0.0212) | Acc_1: (99.24%) (17910/18048)\n",
      "Epoch: 64 | Batch_idx: 150 |  Loss_1: (0.0216) | Acc_1: (99.22%) (19177/19328)\n",
      "Epoch: 64 | Batch_idx: 160 |  Loss_1: (0.0222) | Acc_1: (99.21%) (20446/20608)\n",
      "Epoch: 64 | Batch_idx: 170 |  Loss_1: (0.0227) | Acc_1: (99.21%) (21715/21888)\n",
      "Epoch: 64 | Batch_idx: 180 |  Loss_1: (0.0226) | Acc_1: (99.21%) (22986/23168)\n",
      "Epoch: 64 | Batch_idx: 190 |  Loss_1: (0.0223) | Acc_1: (99.23%) (24260/24448)\n",
      "Epoch: 64 | Batch_idx: 200 |  Loss_1: (0.0224) | Acc_1: (99.22%) (25528/25728)\n",
      "Epoch: 64 | Batch_idx: 210 |  Loss_1: (0.0225) | Acc_1: (99.22%) (26797/27008)\n",
      "Epoch: 64 | Batch_idx: 220 |  Loss_1: (0.0232) | Acc_1: (99.20%) (28063/28288)\n",
      "Epoch: 64 | Batch_idx: 230 |  Loss_1: (0.0234) | Acc_1: (99.19%) (29328/29568)\n",
      "Epoch: 64 | Batch_idx: 240 |  Loss_1: (0.0235) | Acc_1: (99.18%) (30596/30848)\n",
      "Epoch: 64 | Batch_idx: 250 |  Loss_1: (0.0241) | Acc_1: (99.16%) (31857/32128)\n",
      "Epoch: 64 | Batch_idx: 260 |  Loss_1: (0.0247) | Acc_1: (99.13%) (33117/33408)\n",
      "Epoch: 64 | Batch_idx: 270 |  Loss_1: (0.0244) | Acc_1: (99.14%) (34389/34688)\n",
      "Epoch: 64 | Batch_idx: 280 |  Loss_1: (0.0243) | Acc_1: (99.14%) (35659/35968)\n",
      "Epoch: 64 | Batch_idx: 290 |  Loss_1: (0.0243) | Acc_1: (99.14%) (36927/37248)\n",
      "Epoch: 64 | Batch_idx: 300 |  Loss_1: (0.0241) | Acc_1: (99.14%) (38198/38528)\n",
      "Epoch: 64 | Batch_idx: 310 |  Loss_1: (0.0242) | Acc_1: (99.14%) (39467/39808)\n",
      "Epoch: 64 | Batch_idx: 320 |  Loss_1: (0.0243) | Acc_1: (99.14%) (40736/41088)\n",
      "Epoch: 64 | Batch_idx: 330 |  Loss_1: (0.0245) | Acc_1: (99.14%) (42005/42368)\n",
      "Epoch: 64 | Batch_idx: 340 |  Loss_1: (0.0244) | Acc_1: (99.14%) (43274/43648)\n",
      "Epoch: 64 | Batch_idx: 350 |  Loss_1: (0.0248) | Acc_1: (99.13%) (44537/44928)\n",
      "Epoch: 64 | Batch_idx: 360 |  Loss_1: (0.0248) | Acc_1: (99.14%) (45811/46208)\n",
      "Epoch: 64 | Batch_idx: 370 |  Loss_1: (0.0248) | Acc_1: (99.14%) (47081/47488)\n",
      "Epoch: 64 | Batch_idx: 380 |  Loss_1: (0.0249) | Acc_1: (99.14%) (48348/48768)\n",
      "Epoch: 64 | Batch_idx: 390 |  Loss_1: (0.0247) | Acc_1: (99.15%) (49574/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5284) | Acc: (90.45%) (9045/10000)\n",
      "Epoch: 65 | Batch_idx: 0 |  Loss_1: (0.0310) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 65 | Batch_idx: 10 |  Loss_1: (0.0211) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 65 | Batch_idx: 20 |  Loss_1: (0.0186) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 65 | Batch_idx: 30 |  Loss_1: (0.0187) | Acc_1: (99.29%) (3940/3968)\n",
      "Epoch: 65 | Batch_idx: 40 |  Loss_1: (0.0181) | Acc_1: (99.35%) (5214/5248)\n",
      "Epoch: 65 | Batch_idx: 50 |  Loss_1: (0.0167) | Acc_1: (99.45%) (6492/6528)\n",
      "Epoch: 65 | Batch_idx: 60 |  Loss_1: (0.0168) | Acc_1: (99.44%) (7764/7808)\n",
      "Epoch: 65 | Batch_idx: 70 |  Loss_1: (0.0174) | Acc_1: (99.44%) (9037/9088)\n",
      "Epoch: 65 | Batch_idx: 80 |  Loss_1: (0.0175) | Acc_1: (99.40%) (10306/10368)\n",
      "Epoch: 65 | Batch_idx: 90 |  Loss_1: (0.0184) | Acc_1: (99.38%) (11576/11648)\n",
      "Epoch: 65 | Batch_idx: 100 |  Loss_1: (0.0190) | Acc_1: (99.35%) (12844/12928)\n",
      "Epoch: 65 | Batch_idx: 110 |  Loss_1: (0.0194) | Acc_1: (99.34%) (14114/14208)\n",
      "Epoch: 65 | Batch_idx: 120 |  Loss_1: (0.0193) | Acc_1: (99.33%) (15385/15488)\n",
      "Epoch: 65 | Batch_idx: 130 |  Loss_1: (0.0189) | Acc_1: (99.34%) (16658/16768)\n",
      "Epoch: 65 | Batch_idx: 140 |  Loss_1: (0.0186) | Acc_1: (99.35%) (17930/18048)\n",
      "Epoch: 65 | Batch_idx: 150 |  Loss_1: (0.0184) | Acc_1: (99.34%) (19200/19328)\n",
      "Epoch: 65 | Batch_idx: 160 |  Loss_1: (0.0190) | Acc_1: (99.32%) (20468/20608)\n",
      "Epoch: 65 | Batch_idx: 170 |  Loss_1: (0.0207) | Acc_1: (99.27%) (21729/21888)\n",
      "Epoch: 65 | Batch_idx: 180 |  Loss_1: (0.0208) | Acc_1: (99.27%) (22998/23168)\n",
      "Epoch: 65 | Batch_idx: 190 |  Loss_1: (0.0213) | Acc_1: (99.24%) (24262/24448)\n",
      "Epoch: 65 | Batch_idx: 200 |  Loss_1: (0.0217) | Acc_1: (99.22%) (25528/25728)\n",
      "Epoch: 65 | Batch_idx: 210 |  Loss_1: (0.0214) | Acc_1: (99.24%) (26804/27008)\n",
      "Epoch: 65 | Batch_idx: 220 |  Loss_1: (0.0217) | Acc_1: (99.24%) (28072/28288)\n",
      "Epoch: 65 | Batch_idx: 230 |  Loss_1: (0.0216) | Acc_1: (99.24%) (29344/29568)\n",
      "Epoch: 65 | Batch_idx: 240 |  Loss_1: (0.0222) | Acc_1: (99.22%) (30607/30848)\n",
      "Epoch: 65 | Batch_idx: 250 |  Loss_1: (0.0225) | Acc_1: (99.21%) (31875/32128)\n",
      "Epoch: 65 | Batch_idx: 260 |  Loss_1: (0.0228) | Acc_1: (99.21%) (33144/33408)\n",
      "Epoch: 65 | Batch_idx: 270 |  Loss_1: (0.0230) | Acc_1: (99.21%) (34413/34688)\n",
      "Epoch: 65 | Batch_idx: 280 |  Loss_1: (0.0230) | Acc_1: (99.20%) (35682/35968)\n",
      "Epoch: 65 | Batch_idx: 290 |  Loss_1: (0.0231) | Acc_1: (99.20%) (36951/37248)\n",
      "Epoch: 65 | Batch_idx: 300 |  Loss_1: (0.0233) | Acc_1: (99.21%) (38223/38528)\n",
      "Epoch: 65 | Batch_idx: 310 |  Loss_1: (0.0235) | Acc_1: (99.20%) (39491/39808)\n",
      "Epoch: 65 | Batch_idx: 320 |  Loss_1: (0.0236) | Acc_1: (99.20%) (40761/41088)\n",
      "Epoch: 65 | Batch_idx: 330 |  Loss_1: (0.0236) | Acc_1: (99.21%) (42033/42368)\n",
      "Epoch: 65 | Batch_idx: 340 |  Loss_1: (0.0236) | Acc_1: (99.21%) (43303/43648)\n",
      "Epoch: 65 | Batch_idx: 350 |  Loss_1: (0.0238) | Acc_1: (99.20%) (44569/44928)\n",
      "Epoch: 65 | Batch_idx: 360 |  Loss_1: (0.0240) | Acc_1: (99.18%) (45830/46208)\n",
      "Epoch: 65 | Batch_idx: 370 |  Loss_1: (0.0239) | Acc_1: (99.19%) (47104/47488)\n",
      "Epoch: 65 | Batch_idx: 380 |  Loss_1: (0.0243) | Acc_1: (99.17%) (48363/48768)\n",
      "Epoch: 65 | Batch_idx: 390 |  Loss_1: (0.0244) | Acc_1: (99.17%) (49584/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5438) | Acc: (89.79%) (8979/10000)\n",
      "Epoch: 66 | Batch_idx: 0 |  Loss_1: (0.0410) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 66 | Batch_idx: 10 |  Loss_1: (0.0276) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 66 | Batch_idx: 20 |  Loss_1: (0.0269) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 66 | Batch_idx: 30 |  Loss_1: (0.0281) | Acc_1: (98.92%) (3925/3968)\n",
      "Epoch: 66 | Batch_idx: 40 |  Loss_1: (0.0262) | Acc_1: (98.95%) (5193/5248)\n",
      "Epoch: 66 | Batch_idx: 50 |  Loss_1: (0.0262) | Acc_1: (98.96%) (6460/6528)\n",
      "Epoch: 66 | Batch_idx: 60 |  Loss_1: (0.0248) | Acc_1: (99.04%) (7733/7808)\n",
      "Epoch: 66 | Batch_idx: 70 |  Loss_1: (0.0253) | Acc_1: (99.02%) (8999/9088)\n",
      "Epoch: 66 | Batch_idx: 80 |  Loss_1: (0.0257) | Acc_1: (99.01%) (10265/10368)\n",
      "Epoch: 66 | Batch_idx: 90 |  Loss_1: (0.0253) | Acc_1: (99.00%) (11532/11648)\n",
      "Epoch: 66 | Batch_idx: 100 |  Loss_1: (0.0264) | Acc_1: (98.98%) (12796/12928)\n",
      "Epoch: 66 | Batch_idx: 110 |  Loss_1: (0.0256) | Acc_1: (99.01%) (14068/14208)\n",
      "Epoch: 66 | Batch_idx: 120 |  Loss_1: (0.0244) | Acc_1: (99.08%) (15345/15488)\n",
      "Epoch: 66 | Batch_idx: 130 |  Loss_1: (0.0243) | Acc_1: (99.09%) (16616/16768)\n",
      "Epoch: 66 | Batch_idx: 140 |  Loss_1: (0.0237) | Acc_1: (99.12%) (17889/18048)\n",
      "Epoch: 66 | Batch_idx: 150 |  Loss_1: (0.0235) | Acc_1: (99.13%) (19159/19328)\n",
      "Epoch: 66 | Batch_idx: 160 |  Loss_1: (0.0234) | Acc_1: (99.16%) (20434/20608)\n",
      "Epoch: 66 | Batch_idx: 170 |  Loss_1: (0.0233) | Acc_1: (99.15%) (21703/21888)\n",
      "Epoch: 66 | Batch_idx: 180 |  Loss_1: (0.0232) | Acc_1: (99.16%) (22973/23168)\n",
      "Epoch: 66 | Batch_idx: 190 |  Loss_1: (0.0230) | Acc_1: (99.16%) (24242/24448)\n",
      "Epoch: 66 | Batch_idx: 200 |  Loss_1: (0.0229) | Acc_1: (99.16%) (25512/25728)\n",
      "Epoch: 66 | Batch_idx: 210 |  Loss_1: (0.0232) | Acc_1: (99.16%) (26780/27008)\n",
      "Epoch: 66 | Batch_idx: 220 |  Loss_1: (0.0233) | Acc_1: (99.14%) (28046/28288)\n",
      "Epoch: 66 | Batch_idx: 230 |  Loss_1: (0.0234) | Acc_1: (99.14%) (29315/29568)\n",
      "Epoch: 66 | Batch_idx: 240 |  Loss_1: (0.0230) | Acc_1: (99.15%) (30587/30848)\n",
      "Epoch: 66 | Batch_idx: 250 |  Loss_1: (0.0236) | Acc_1: (99.13%) (31848/32128)\n",
      "Epoch: 66 | Batch_idx: 260 |  Loss_1: (0.0238) | Acc_1: (99.11%) (33111/33408)\n",
      "Epoch: 66 | Batch_idx: 270 |  Loss_1: (0.0246) | Acc_1: (99.09%) (34371/34688)\n",
      "Epoch: 66 | Batch_idx: 280 |  Loss_1: (0.0250) | Acc_1: (99.07%) (35634/35968)\n",
      "Epoch: 66 | Batch_idx: 290 |  Loss_1: (0.0248) | Acc_1: (99.07%) (36903/37248)\n",
      "Epoch: 66 | Batch_idx: 300 |  Loss_1: (0.0247) | Acc_1: (99.08%) (38172/38528)\n",
      "Epoch: 66 | Batch_idx: 310 |  Loss_1: (0.0249) | Acc_1: (99.07%) (39439/39808)\n",
      "Epoch: 66 | Batch_idx: 320 |  Loss_1: (0.0247) | Acc_1: (99.08%) (40710/41088)\n",
      "Epoch: 66 | Batch_idx: 330 |  Loss_1: (0.0249) | Acc_1: (99.07%) (41976/42368)\n",
      "Epoch: 66 | Batch_idx: 340 |  Loss_1: (0.0252) | Acc_1: (99.06%) (43239/43648)\n",
      "Epoch: 66 | Batch_idx: 350 |  Loss_1: (0.0255) | Acc_1: (99.06%) (44504/44928)\n",
      "Epoch: 66 | Batch_idx: 360 |  Loss_1: (0.0255) | Acc_1: (99.06%) (45772/46208)\n",
      "Epoch: 66 | Batch_idx: 370 |  Loss_1: (0.0258) | Acc_1: (99.06%) (47040/47488)\n",
      "Epoch: 66 | Batch_idx: 380 |  Loss_1: (0.0259) | Acc_1: (99.05%) (48306/48768)\n",
      "Epoch: 66 | Batch_idx: 390 |  Loss_1: (0.0260) | Acc_1: (99.05%) (49526/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5946) | Acc: (89.60%) (8960/10000)\n",
      "Epoch: 67 | Batch_idx: 0 |  Loss_1: (0.0091) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 67 | Batch_idx: 10 |  Loss_1: (0.0375) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 67 | Batch_idx: 20 |  Loss_1: (0.0323) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 67 | Batch_idx: 30 |  Loss_1: (0.0292) | Acc_1: (99.02%) (3929/3968)\n",
      "Epoch: 67 | Batch_idx: 40 |  Loss_1: (0.0302) | Acc_1: (98.97%) (5194/5248)\n",
      "Epoch: 67 | Batch_idx: 50 |  Loss_1: (0.0300) | Acc_1: (98.99%) (6462/6528)\n",
      "Epoch: 67 | Batch_idx: 60 |  Loss_1: (0.0295) | Acc_1: (98.98%) (7728/7808)\n",
      "Epoch: 67 | Batch_idx: 70 |  Loss_1: (0.0317) | Acc_1: (98.87%) (8985/9088)\n",
      "Epoch: 67 | Batch_idx: 80 |  Loss_1: (0.0314) | Acc_1: (98.85%) (10249/10368)\n",
      "Epoch: 67 | Batch_idx: 90 |  Loss_1: (0.0327) | Acc_1: (98.82%) (11510/11648)\n",
      "Epoch: 67 | Batch_idx: 100 |  Loss_1: (0.0318) | Acc_1: (98.83%) (12777/12928)\n",
      "Epoch: 67 | Batch_idx: 110 |  Loss_1: (0.0321) | Acc_1: (98.85%) (14044/14208)\n",
      "Epoch: 67 | Batch_idx: 120 |  Loss_1: (0.0311) | Acc_1: (98.89%) (15316/15488)\n",
      "Epoch: 67 | Batch_idx: 130 |  Loss_1: (0.0305) | Acc_1: (98.92%) (16587/16768)\n",
      "Epoch: 67 | Batch_idx: 140 |  Loss_1: (0.0293) | Acc_1: (98.97%) (17863/18048)\n",
      "Epoch: 67 | Batch_idx: 150 |  Loss_1: (0.0291) | Acc_1: (98.97%) (19129/19328)\n",
      "Epoch: 67 | Batch_idx: 160 |  Loss_1: (0.0286) | Acc_1: (98.99%) (20400/20608)\n",
      "Epoch: 67 | Batch_idx: 170 |  Loss_1: (0.0284) | Acc_1: (98.99%) (21667/21888)\n",
      "Epoch: 67 | Batch_idx: 180 |  Loss_1: (0.0278) | Acc_1: (99.00%) (22936/23168)\n",
      "Epoch: 67 | Batch_idx: 190 |  Loss_1: (0.0276) | Acc_1: (99.00%) (24204/24448)\n",
      "Epoch: 67 | Batch_idx: 200 |  Loss_1: (0.0276) | Acc_1: (99.02%) (25475/25728)\n",
      "Epoch: 67 | Batch_idx: 210 |  Loss_1: (0.0275) | Acc_1: (99.02%) (26743/27008)\n",
      "Epoch: 67 | Batch_idx: 220 |  Loss_1: (0.0273) | Acc_1: (99.02%) (28011/28288)\n",
      "Epoch: 67 | Batch_idx: 230 |  Loss_1: (0.0272) | Acc_1: (99.02%) (29279/29568)\n",
      "Epoch: 67 | Batch_idx: 240 |  Loss_1: (0.0266) | Acc_1: (99.04%) (30553/30848)\n",
      "Epoch: 67 | Batch_idx: 250 |  Loss_1: (0.0270) | Acc_1: (99.03%) (31816/32128)\n",
      "Epoch: 67 | Batch_idx: 260 |  Loss_1: (0.0274) | Acc_1: (99.00%) (33075/33408)\n",
      "Epoch: 67 | Batch_idx: 270 |  Loss_1: (0.0275) | Acc_1: (99.00%) (34342/34688)\n",
      "Epoch: 67 | Batch_idx: 280 |  Loss_1: (0.0273) | Acc_1: (99.01%) (35612/35968)\n",
      "Epoch: 67 | Batch_idx: 290 |  Loss_1: (0.0270) | Acc_1: (99.01%) (36880/37248)\n",
      "Epoch: 67 | Batch_idx: 300 |  Loss_1: (0.0268) | Acc_1: (99.02%) (38151/38528)\n",
      "Epoch: 67 | Batch_idx: 310 |  Loss_1: (0.0268) | Acc_1: (99.02%) (39419/39808)\n",
      "Epoch: 67 | Batch_idx: 320 |  Loss_1: (0.0269) | Acc_1: (99.03%) (40691/41088)\n",
      "Epoch: 67 | Batch_idx: 330 |  Loss_1: (0.0270) | Acc_1: (99.03%) (41957/42368)\n",
      "Epoch: 67 | Batch_idx: 340 |  Loss_1: (0.0268) | Acc_1: (99.04%) (43230/43648)\n",
      "Epoch: 67 | Batch_idx: 350 |  Loss_1: (0.0267) | Acc_1: (99.05%) (44499/44928)\n",
      "Epoch: 67 | Batch_idx: 360 |  Loss_1: (0.0268) | Acc_1: (99.05%) (45767/46208)\n",
      "Epoch: 67 | Batch_idx: 370 |  Loss_1: (0.0267) | Acc_1: (99.05%) (47038/47488)\n",
      "Epoch: 67 | Batch_idx: 380 |  Loss_1: (0.0266) | Acc_1: (99.06%) (48308/48768)\n",
      "Epoch: 67 | Batch_idx: 390 |  Loss_1: (0.0267) | Acc_1: (99.05%) (49525/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5008) | Acc: (90.86%) (9086/10000)\n",
      "Epoch: 68 | Batch_idx: 0 |  Loss_1: (0.0049) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 68 | Batch_idx: 10 |  Loss_1: (0.0262) | Acc_1: (99.01%) (1394/1408)\n",
      "Epoch: 68 | Batch_idx: 20 |  Loss_1: (0.0210) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 68 | Batch_idx: 30 |  Loss_1: (0.0198) | Acc_1: (99.40%) (3944/3968)\n",
      "Epoch: 68 | Batch_idx: 40 |  Loss_1: (0.0202) | Acc_1: (99.33%) (5213/5248)\n",
      "Epoch: 68 | Batch_idx: 50 |  Loss_1: (0.0199) | Acc_1: (99.34%) (6485/6528)\n",
      "Epoch: 68 | Batch_idx: 60 |  Loss_1: (0.0194) | Acc_1: (99.33%) (7756/7808)\n",
      "Epoch: 68 | Batch_idx: 70 |  Loss_1: (0.0183) | Acc_1: (99.38%) (9032/9088)\n",
      "Epoch: 68 | Batch_idx: 80 |  Loss_1: (0.0191) | Acc_1: (99.31%) (10296/10368)\n",
      "Epoch: 68 | Batch_idx: 90 |  Loss_1: (0.0185) | Acc_1: (99.36%) (11573/11648)\n",
      "Epoch: 68 | Batch_idx: 100 |  Loss_1: (0.0193) | Acc_1: (99.35%) (12844/12928)\n",
      "Epoch: 68 | Batch_idx: 110 |  Loss_1: (0.0192) | Acc_1: (99.35%) (14115/14208)\n",
      "Epoch: 68 | Batch_idx: 120 |  Loss_1: (0.0191) | Acc_1: (99.32%) (15383/15488)\n",
      "Epoch: 68 | Batch_idx: 130 |  Loss_1: (0.0191) | Acc_1: (99.33%) (16656/16768)\n",
      "Epoch: 68 | Batch_idx: 140 |  Loss_1: (0.0190) | Acc_1: (99.35%) (17930/18048)\n",
      "Epoch: 68 | Batch_idx: 150 |  Loss_1: (0.0186) | Acc_1: (99.36%) (19204/19328)\n",
      "Epoch: 68 | Batch_idx: 160 |  Loss_1: (0.0185) | Acc_1: (99.36%) (20476/20608)\n",
      "Epoch: 68 | Batch_idx: 170 |  Loss_1: (0.0185) | Acc_1: (99.34%) (21743/21888)\n",
      "Epoch: 68 | Batch_idx: 180 |  Loss_1: (0.0186) | Acc_1: (99.34%) (23014/23168)\n",
      "Epoch: 68 | Batch_idx: 190 |  Loss_1: (0.0185) | Acc_1: (99.33%) (24284/24448)\n",
      "Epoch: 68 | Batch_idx: 200 |  Loss_1: (0.0184) | Acc_1: (99.33%) (25556/25728)\n",
      "Epoch: 68 | Batch_idx: 210 |  Loss_1: (0.0182) | Acc_1: (99.34%) (26829/27008)\n",
      "Epoch: 68 | Batch_idx: 220 |  Loss_1: (0.0184) | Acc_1: (99.34%) (28100/28288)\n",
      "Epoch: 68 | Batch_idx: 230 |  Loss_1: (0.0182) | Acc_1: (99.34%) (29372/29568)\n",
      "Epoch: 68 | Batch_idx: 240 |  Loss_1: (0.0184) | Acc_1: (99.34%) (30645/30848)\n",
      "Epoch: 68 | Batch_idx: 250 |  Loss_1: (0.0182) | Acc_1: (99.35%) (31918/32128)\n",
      "Epoch: 68 | Batch_idx: 260 |  Loss_1: (0.0183) | Acc_1: (99.35%) (33190/33408)\n",
      "Epoch: 68 | Batch_idx: 270 |  Loss_1: (0.0184) | Acc_1: (99.35%) (34461/34688)\n",
      "Epoch: 68 | Batch_idx: 280 |  Loss_1: (0.0185) | Acc_1: (99.34%) (35732/35968)\n",
      "Epoch: 68 | Batch_idx: 290 |  Loss_1: (0.0186) | Acc_1: (99.34%) (37003/37248)\n",
      "Epoch: 68 | Batch_idx: 300 |  Loss_1: (0.0187) | Acc_1: (99.34%) (38273/38528)\n",
      "Epoch: 68 | Batch_idx: 310 |  Loss_1: (0.0191) | Acc_1: (99.33%) (39540/39808)\n",
      "Epoch: 68 | Batch_idx: 320 |  Loss_1: (0.0189) | Acc_1: (99.33%) (40814/41088)\n",
      "Epoch: 68 | Batch_idx: 330 |  Loss_1: (0.0190) | Acc_1: (99.33%) (42085/42368)\n",
      "Epoch: 68 | Batch_idx: 340 |  Loss_1: (0.0193) | Acc_1: (99.32%) (43350/43648)\n",
      "Epoch: 68 | Batch_idx: 350 |  Loss_1: (0.0195) | Acc_1: (99.32%) (44622/44928)\n",
      "Epoch: 68 | Batch_idx: 360 |  Loss_1: (0.0201) | Acc_1: (99.30%) (45884/46208)\n",
      "Epoch: 68 | Batch_idx: 370 |  Loss_1: (0.0202) | Acc_1: (99.30%) (47155/47488)\n",
      "Epoch: 68 | Batch_idx: 380 |  Loss_1: (0.0202) | Acc_1: (99.29%) (48421/48768)\n",
      "Epoch: 68 | Batch_idx: 390 |  Loss_1: (0.0204) | Acc_1: (99.29%) (49643/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5002) | Acc: (90.99%) (9099/10000)\n",
      "Epoch: 69 | Batch_idx: 0 |  Loss_1: (0.0308) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 69 | Batch_idx: 10 |  Loss_1: (0.0276) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 69 | Batch_idx: 20 |  Loss_1: (0.0240) | Acc_1: (99.11%) (2664/2688)\n",
      "Epoch: 69 | Batch_idx: 30 |  Loss_1: (0.0221) | Acc_1: (99.19%) (3936/3968)\n",
      "Epoch: 69 | Batch_idx: 40 |  Loss_1: (0.0236) | Acc_1: (99.28%) (5210/5248)\n",
      "Epoch: 69 | Batch_idx: 50 |  Loss_1: (0.0222) | Acc_1: (99.30%) (6482/6528)\n",
      "Epoch: 69 | Batch_idx: 60 |  Loss_1: (0.0223) | Acc_1: (99.27%) (7751/7808)\n",
      "Epoch: 69 | Batch_idx: 70 |  Loss_1: (0.0220) | Acc_1: (99.30%) (9024/9088)\n",
      "Epoch: 69 | Batch_idx: 80 |  Loss_1: (0.0229) | Acc_1: (99.27%) (10292/10368)\n",
      "Epoch: 69 | Batch_idx: 90 |  Loss_1: (0.0230) | Acc_1: (99.27%) (11563/11648)\n",
      "Epoch: 69 | Batch_idx: 100 |  Loss_1: (0.0226) | Acc_1: (99.28%) (12835/12928)\n",
      "Epoch: 69 | Batch_idx: 110 |  Loss_1: (0.0217) | Acc_1: (99.30%) (14109/14208)\n",
      "Epoch: 69 | Batch_idx: 120 |  Loss_1: (0.0213) | Acc_1: (99.32%) (15383/15488)\n",
      "Epoch: 69 | Batch_idx: 130 |  Loss_1: (0.0211) | Acc_1: (99.31%) (16652/16768)\n",
      "Epoch: 69 | Batch_idx: 140 |  Loss_1: (0.0215) | Acc_1: (99.31%) (17923/18048)\n",
      "Epoch: 69 | Batch_idx: 150 |  Loss_1: (0.0210) | Acc_1: (99.32%) (19196/19328)\n",
      "Epoch: 69 | Batch_idx: 160 |  Loss_1: (0.0207) | Acc_1: (99.33%) (20469/20608)\n",
      "Epoch: 69 | Batch_idx: 170 |  Loss_1: (0.0205) | Acc_1: (99.32%) (21739/21888)\n",
      "Epoch: 69 | Batch_idx: 180 |  Loss_1: (0.0201) | Acc_1: (99.33%) (23012/23168)\n",
      "Epoch: 69 | Batch_idx: 190 |  Loss_1: (0.0207) | Acc_1: (99.30%) (24276/24448)\n",
      "Epoch: 69 | Batch_idx: 200 |  Loss_1: (0.0208) | Acc_1: (99.29%) (25545/25728)\n",
      "Epoch: 69 | Batch_idx: 210 |  Loss_1: (0.0212) | Acc_1: (99.27%) (26810/27008)\n",
      "Epoch: 69 | Batch_idx: 220 |  Loss_1: (0.0216) | Acc_1: (99.24%) (28073/28288)\n",
      "Epoch: 69 | Batch_idx: 230 |  Loss_1: (0.0217) | Acc_1: (99.24%) (29342/29568)\n",
      "Epoch: 69 | Batch_idx: 240 |  Loss_1: (0.0218) | Acc_1: (99.23%) (30611/30848)\n",
      "Epoch: 69 | Batch_idx: 250 |  Loss_1: (0.0220) | Acc_1: (99.22%) (31879/32128)\n",
      "Epoch: 69 | Batch_idx: 260 |  Loss_1: (0.0223) | Acc_1: (99.21%) (33143/33408)\n",
      "Epoch: 69 | Batch_idx: 270 |  Loss_1: (0.0228) | Acc_1: (99.19%) (34408/34688)\n",
      "Epoch: 69 | Batch_idx: 280 |  Loss_1: (0.0228) | Acc_1: (99.19%) (35676/35968)\n",
      "Epoch: 69 | Batch_idx: 290 |  Loss_1: (0.0232) | Acc_1: (99.17%) (36939/37248)\n",
      "Epoch: 69 | Batch_idx: 300 |  Loss_1: (0.0232) | Acc_1: (99.16%) (38206/38528)\n",
      "Epoch: 69 | Batch_idx: 310 |  Loss_1: (0.0234) | Acc_1: (99.15%) (39471/39808)\n",
      "Epoch: 69 | Batch_idx: 320 |  Loss_1: (0.0232) | Acc_1: (99.17%) (40745/41088)\n",
      "Epoch: 69 | Batch_idx: 330 |  Loss_1: (0.0229) | Acc_1: (99.18%) (42021/42368)\n",
      "Epoch: 69 | Batch_idx: 340 |  Loss_1: (0.0229) | Acc_1: (99.19%) (43295/43648)\n",
      "Epoch: 69 | Batch_idx: 350 |  Loss_1: (0.0228) | Acc_1: (99.20%) (44567/44928)\n",
      "Epoch: 69 | Batch_idx: 360 |  Loss_1: (0.0227) | Acc_1: (99.20%) (45838/46208)\n",
      "Epoch: 69 | Batch_idx: 370 |  Loss_1: (0.0226) | Acc_1: (99.20%) (47108/47488)\n",
      "Epoch: 69 | Batch_idx: 380 |  Loss_1: (0.0227) | Acc_1: (99.19%) (48375/48768)\n",
      "Epoch: 69 | Batch_idx: 390 |  Loss_1: (0.0227) | Acc_1: (99.19%) (49596/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4910) | Acc: (90.79%) (9079/10000)\n",
      "Epoch: 70 | Batch_idx: 0 |  Loss_1: (0.0098) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 70 | Batch_idx: 10 |  Loss_1: (0.0147) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 70 | Batch_idx: 20 |  Loss_1: (0.0218) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 70 | Batch_idx: 30 |  Loss_1: (0.0235) | Acc_1: (98.97%) (3927/3968)\n",
      "Epoch: 70 | Batch_idx: 40 |  Loss_1: (0.0260) | Acc_1: (98.86%) (5188/5248)\n",
      "Epoch: 70 | Batch_idx: 50 |  Loss_1: (0.0250) | Acc_1: (98.96%) (6460/6528)\n",
      "Epoch: 70 | Batch_idx: 60 |  Loss_1: (0.0244) | Acc_1: (98.98%) (7728/7808)\n",
      "Epoch: 70 | Batch_idx: 70 |  Loss_1: (0.0236) | Acc_1: (99.04%) (9001/9088)\n",
      "Epoch: 70 | Batch_idx: 80 |  Loss_1: (0.0229) | Acc_1: (99.06%) (10271/10368)\n",
      "Epoch: 70 | Batch_idx: 90 |  Loss_1: (0.0220) | Acc_1: (99.10%) (11543/11648)\n",
      "Epoch: 70 | Batch_idx: 100 |  Loss_1: (0.0212) | Acc_1: (99.16%) (12820/12928)\n",
      "Epoch: 70 | Batch_idx: 110 |  Loss_1: (0.0208) | Acc_1: (99.19%) (14093/14208)\n",
      "Epoch: 70 | Batch_idx: 120 |  Loss_1: (0.0206) | Acc_1: (99.19%) (15362/15488)\n",
      "Epoch: 70 | Batch_idx: 130 |  Loss_1: (0.0203) | Acc_1: (99.21%) (16635/16768)\n",
      "Epoch: 70 | Batch_idx: 140 |  Loss_1: (0.0194) | Acc_1: (99.25%) (17912/18048)\n",
      "Epoch: 70 | Batch_idx: 150 |  Loss_1: (0.0195) | Acc_1: (99.24%) (19182/19328)\n",
      "Epoch: 70 | Batch_idx: 160 |  Loss_1: (0.0196) | Acc_1: (99.25%) (20454/20608)\n",
      "Epoch: 70 | Batch_idx: 170 |  Loss_1: (0.0195) | Acc_1: (99.26%) (21725/21888)\n",
      "Epoch: 70 | Batch_idx: 180 |  Loss_1: (0.0198) | Acc_1: (99.25%) (22995/23168)\n",
      "Epoch: 70 | Batch_idx: 190 |  Loss_1: (0.0197) | Acc_1: (99.26%) (24266/24448)\n",
      "Epoch: 70 | Batch_idx: 200 |  Loss_1: (0.0201) | Acc_1: (99.23%) (25530/25728)\n",
      "Epoch: 70 | Batch_idx: 210 |  Loss_1: (0.0200) | Acc_1: (99.25%) (26805/27008)\n",
      "Epoch: 70 | Batch_idx: 220 |  Loss_1: (0.0198) | Acc_1: (99.25%) (28077/28288)\n",
      "Epoch: 70 | Batch_idx: 230 |  Loss_1: (0.0201) | Acc_1: (99.25%) (29347/29568)\n",
      "Epoch: 70 | Batch_idx: 240 |  Loss_1: (0.0201) | Acc_1: (99.25%) (30618/30848)\n",
      "Epoch: 70 | Batch_idx: 250 |  Loss_1: (0.0203) | Acc_1: (99.26%) (31889/32128)\n",
      "Epoch: 70 | Batch_idx: 260 |  Loss_1: (0.0205) | Acc_1: (99.23%) (33152/33408)\n",
      "Epoch: 70 | Batch_idx: 270 |  Loss_1: (0.0204) | Acc_1: (99.24%) (34423/34688)\n",
      "Epoch: 70 | Batch_idx: 280 |  Loss_1: (0.0203) | Acc_1: (99.24%) (35696/35968)\n",
      "Epoch: 70 | Batch_idx: 290 |  Loss_1: (0.0202) | Acc_1: (99.25%) (36967/37248)\n",
      "Epoch: 70 | Batch_idx: 300 |  Loss_1: (0.0200) | Acc_1: (99.25%) (38240/38528)\n",
      "Epoch: 70 | Batch_idx: 310 |  Loss_1: (0.0199) | Acc_1: (99.26%) (39512/39808)\n",
      "Epoch: 70 | Batch_idx: 320 |  Loss_1: (0.0200) | Acc_1: (99.25%) (40781/41088)\n",
      "Epoch: 70 | Batch_idx: 330 |  Loss_1: (0.0203) | Acc_1: (99.24%) (42047/42368)\n",
      "Epoch: 70 | Batch_idx: 340 |  Loss_1: (0.0201) | Acc_1: (99.26%) (43323/43648)\n",
      "Epoch: 70 | Batch_idx: 350 |  Loss_1: (0.0199) | Acc_1: (99.27%) (44598/44928)\n",
      "Epoch: 70 | Batch_idx: 360 |  Loss_1: (0.0200) | Acc_1: (99.26%) (45867/46208)\n",
      "Epoch: 70 | Batch_idx: 370 |  Loss_1: (0.0201) | Acc_1: (99.25%) (47133/47488)\n",
      "Epoch: 70 | Batch_idx: 380 |  Loss_1: (0.0204) | Acc_1: (99.24%) (48398/48768)\n",
      "Epoch: 70 | Batch_idx: 390 |  Loss_1: (0.0206) | Acc_1: (99.23%) (49617/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5129) | Acc: (90.67%) (9067/10000)\n",
      "Epoch: 71 | Batch_idx: 0 |  Loss_1: (0.0413) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 71 | Batch_idx: 10 |  Loss_1: (0.0193) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 71 | Batch_idx: 20 |  Loss_1: (0.0192) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 71 | Batch_idx: 30 |  Loss_1: (0.0192) | Acc_1: (99.42%) (3945/3968)\n",
      "Epoch: 71 | Batch_idx: 40 |  Loss_1: (0.0197) | Acc_1: (99.41%) (5217/5248)\n",
      "Epoch: 71 | Batch_idx: 50 |  Loss_1: (0.0194) | Acc_1: (99.40%) (6489/6528)\n",
      "Epoch: 71 | Batch_idx: 60 |  Loss_1: (0.0191) | Acc_1: (99.41%) (7762/7808)\n",
      "Epoch: 71 | Batch_idx: 70 |  Loss_1: (0.0202) | Acc_1: (99.37%) (9031/9088)\n",
      "Epoch: 71 | Batch_idx: 80 |  Loss_1: (0.0204) | Acc_1: (99.37%) (10303/10368)\n",
      "Epoch: 71 | Batch_idx: 90 |  Loss_1: (0.0199) | Acc_1: (99.41%) (11579/11648)\n",
      "Epoch: 71 | Batch_idx: 100 |  Loss_1: (0.0196) | Acc_1: (99.41%) (12852/12928)\n",
      "Epoch: 71 | Batch_idx: 110 |  Loss_1: (0.0198) | Acc_1: (99.42%) (14125/14208)\n",
      "Epoch: 71 | Batch_idx: 120 |  Loss_1: (0.0191) | Acc_1: (99.43%) (15400/15488)\n",
      "Epoch: 71 | Batch_idx: 130 |  Loss_1: (0.0187) | Acc_1: (99.46%) (16677/16768)\n",
      "Epoch: 71 | Batch_idx: 140 |  Loss_1: (0.0190) | Acc_1: (99.45%) (17948/18048)\n",
      "Epoch: 71 | Batch_idx: 150 |  Loss_1: (0.0188) | Acc_1: (99.45%) (19221/19328)\n",
      "Epoch: 71 | Batch_idx: 160 |  Loss_1: (0.0191) | Acc_1: (99.42%) (20488/20608)\n",
      "Epoch: 71 | Batch_idx: 170 |  Loss_1: (0.0192) | Acc_1: (99.42%) (21761/21888)\n",
      "Epoch: 71 | Batch_idx: 180 |  Loss_1: (0.0193) | Acc_1: (99.40%) (23030/23168)\n",
      "Epoch: 71 | Batch_idx: 190 |  Loss_1: (0.0190) | Acc_1: (99.41%) (24304/24448)\n",
      "Epoch: 71 | Batch_idx: 200 |  Loss_1: (0.0189) | Acc_1: (99.41%) (25576/25728)\n",
      "Epoch: 71 | Batch_idx: 210 |  Loss_1: (0.0188) | Acc_1: (99.41%) (26848/27008)\n",
      "Epoch: 71 | Batch_idx: 220 |  Loss_1: (0.0190) | Acc_1: (99.41%) (28120/28288)\n",
      "Epoch: 71 | Batch_idx: 230 |  Loss_1: (0.0195) | Acc_1: (99.39%) (29389/29568)\n",
      "Epoch: 71 | Batch_idx: 240 |  Loss_1: (0.0193) | Acc_1: (99.41%) (30665/30848)\n",
      "Epoch: 71 | Batch_idx: 250 |  Loss_1: (0.0193) | Acc_1: (99.41%) (31938/32128)\n",
      "Epoch: 71 | Batch_idx: 260 |  Loss_1: (0.0193) | Acc_1: (99.41%) (33212/33408)\n",
      "Epoch: 71 | Batch_idx: 270 |  Loss_1: (0.0193) | Acc_1: (99.41%) (34482/34688)\n",
      "Epoch: 71 | Batch_idx: 280 |  Loss_1: (0.0192) | Acc_1: (99.41%) (35757/35968)\n",
      "Epoch: 71 | Batch_idx: 290 |  Loss_1: (0.0196) | Acc_1: (99.41%) (37027/37248)\n",
      "Epoch: 71 | Batch_idx: 300 |  Loss_1: (0.0200) | Acc_1: (99.40%) (38295/38528)\n",
      "Epoch: 71 | Batch_idx: 310 |  Loss_1: (0.0199) | Acc_1: (99.39%) (39567/39808)\n",
      "Epoch: 71 | Batch_idx: 320 |  Loss_1: (0.0197) | Acc_1: (99.40%) (40841/41088)\n",
      "Epoch: 71 | Batch_idx: 330 |  Loss_1: (0.0196) | Acc_1: (99.40%) (42113/42368)\n",
      "Epoch: 71 | Batch_idx: 340 |  Loss_1: (0.0194) | Acc_1: (99.41%) (43389/43648)\n",
      "Epoch: 71 | Batch_idx: 350 |  Loss_1: (0.0194) | Acc_1: (99.41%) (44661/44928)\n",
      "Epoch: 71 | Batch_idx: 360 |  Loss_1: (0.0193) | Acc_1: (99.40%) (45933/46208)\n",
      "Epoch: 71 | Batch_idx: 370 |  Loss_1: (0.0195) | Acc_1: (99.40%) (47201/47488)\n",
      "Epoch: 71 | Batch_idx: 380 |  Loss_1: (0.0196) | Acc_1: (99.39%) (48470/48768)\n",
      "Epoch: 71 | Batch_idx: 390 |  Loss_1: (0.0196) | Acc_1: (99.39%) (49695/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5435) | Acc: (90.12%) (9012/10000)\n",
      "Epoch: 72 | Batch_idx: 0 |  Loss_1: (0.0065) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 72 | Batch_idx: 10 |  Loss_1: (0.0159) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 72 | Batch_idx: 20 |  Loss_1: (0.0154) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 72 | Batch_idx: 30 |  Loss_1: (0.0147) | Acc_1: (99.50%) (3948/3968)\n",
      "Epoch: 72 | Batch_idx: 40 |  Loss_1: (0.0145) | Acc_1: (99.50%) (5222/5248)\n",
      "Epoch: 72 | Batch_idx: 50 |  Loss_1: (0.0154) | Acc_1: (99.53%) (6497/6528)\n",
      "Epoch: 72 | Batch_idx: 60 |  Loss_1: (0.0158) | Acc_1: (99.50%) (7769/7808)\n",
      "Epoch: 72 | Batch_idx: 70 |  Loss_1: (0.0151) | Acc_1: (99.54%) (9046/9088)\n",
      "Epoch: 72 | Batch_idx: 80 |  Loss_1: (0.0154) | Acc_1: (99.54%) (10320/10368)\n",
      "Epoch: 72 | Batch_idx: 90 |  Loss_1: (0.0156) | Acc_1: (99.49%) (11589/11648)\n",
      "Epoch: 72 | Batch_idx: 100 |  Loss_1: (0.0157) | Acc_1: (99.48%) (12861/12928)\n",
      "Epoch: 72 | Batch_idx: 110 |  Loss_1: (0.0161) | Acc_1: (99.47%) (14133/14208)\n",
      "Epoch: 72 | Batch_idx: 120 |  Loss_1: (0.0168) | Acc_1: (99.44%) (15402/15488)\n",
      "Epoch: 72 | Batch_idx: 130 |  Loss_1: (0.0173) | Acc_1: (99.43%) (16672/16768)\n",
      "Epoch: 72 | Batch_idx: 140 |  Loss_1: (0.0185) | Acc_1: (99.36%) (17933/18048)\n",
      "Epoch: 72 | Batch_idx: 150 |  Loss_1: (0.0187) | Acc_1: (99.35%) (19202/19328)\n",
      "Epoch: 72 | Batch_idx: 160 |  Loss_1: (0.0194) | Acc_1: (99.32%) (20467/20608)\n",
      "Epoch: 72 | Batch_idx: 170 |  Loss_1: (0.0203) | Acc_1: (99.29%) (21732/21888)\n",
      "Epoch: 72 | Batch_idx: 180 |  Loss_1: (0.0205) | Acc_1: (99.29%) (23004/23168)\n",
      "Epoch: 72 | Batch_idx: 190 |  Loss_1: (0.0204) | Acc_1: (99.30%) (24277/24448)\n",
      "Epoch: 72 | Batch_idx: 200 |  Loss_1: (0.0207) | Acc_1: (99.29%) (25545/25728)\n",
      "Epoch: 72 | Batch_idx: 210 |  Loss_1: (0.0202) | Acc_1: (99.31%) (26821/27008)\n",
      "Epoch: 72 | Batch_idx: 220 |  Loss_1: (0.0201) | Acc_1: (99.31%) (28092/28288)\n",
      "Epoch: 72 | Batch_idx: 230 |  Loss_1: (0.0200) | Acc_1: (99.31%) (29365/29568)\n",
      "Epoch: 72 | Batch_idx: 240 |  Loss_1: (0.0198) | Acc_1: (99.31%) (30636/30848)\n",
      "Epoch: 72 | Batch_idx: 250 |  Loss_1: (0.0198) | Acc_1: (99.31%) (31907/32128)\n",
      "Epoch: 72 | Batch_idx: 260 |  Loss_1: (0.0200) | Acc_1: (99.31%) (33176/33408)\n",
      "Epoch: 72 | Batch_idx: 270 |  Loss_1: (0.0203) | Acc_1: (99.29%) (34443/34688)\n",
      "Epoch: 72 | Batch_idx: 280 |  Loss_1: (0.0205) | Acc_1: (99.29%) (35713/35968)\n",
      "Epoch: 72 | Batch_idx: 290 |  Loss_1: (0.0204) | Acc_1: (99.29%) (36984/37248)\n",
      "Epoch: 72 | Batch_idx: 300 |  Loss_1: (0.0202) | Acc_1: (99.30%) (38258/38528)\n",
      "Epoch: 72 | Batch_idx: 310 |  Loss_1: (0.0201) | Acc_1: (99.30%) (39529/39808)\n",
      "Epoch: 72 | Batch_idx: 320 |  Loss_1: (0.0202) | Acc_1: (99.30%) (40799/41088)\n",
      "Epoch: 72 | Batch_idx: 330 |  Loss_1: (0.0200) | Acc_1: (99.30%) (42071/42368)\n",
      "Epoch: 72 | Batch_idx: 340 |  Loss_1: (0.0201) | Acc_1: (99.30%) (43341/43648)\n",
      "Epoch: 72 | Batch_idx: 350 |  Loss_1: (0.0203) | Acc_1: (99.29%) (44611/44928)\n",
      "Epoch: 72 | Batch_idx: 360 |  Loss_1: (0.0202) | Acc_1: (99.30%) (45885/46208)\n",
      "Epoch: 72 | Batch_idx: 370 |  Loss_1: (0.0204) | Acc_1: (99.29%) (47151/47488)\n",
      "Epoch: 72 | Batch_idx: 380 |  Loss_1: (0.0205) | Acc_1: (99.29%) (48421/48768)\n",
      "Epoch: 72 | Batch_idx: 390 |  Loss_1: (0.0206) | Acc_1: (99.29%) (49645/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5243) | Acc: (90.44%) (9044/10000)\n",
      "Epoch: 73 | Batch_idx: 0 |  Loss_1: (0.0163) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 73 | Batch_idx: 10 |  Loss_1: (0.0163) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 73 | Batch_idx: 20 |  Loss_1: (0.0209) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 73 | Batch_idx: 30 |  Loss_1: (0.0236) | Acc_1: (99.37%) (3943/3968)\n",
      "Epoch: 73 | Batch_idx: 40 |  Loss_1: (0.0264) | Acc_1: (99.29%) (5211/5248)\n",
      "Epoch: 73 | Batch_idx: 50 |  Loss_1: (0.0259) | Acc_1: (99.30%) (6482/6528)\n",
      "Epoch: 73 | Batch_idx: 60 |  Loss_1: (0.0288) | Acc_1: (99.17%) (7743/7808)\n",
      "Epoch: 73 | Batch_idx: 70 |  Loss_1: (0.0285) | Acc_1: (99.16%) (9012/9088)\n",
      "Epoch: 73 | Batch_idx: 80 |  Loss_1: (0.0278) | Acc_1: (99.16%) (10281/10368)\n",
      "Epoch: 73 | Batch_idx: 90 |  Loss_1: (0.0279) | Acc_1: (99.12%) (11545/11648)\n",
      "Epoch: 73 | Batch_idx: 100 |  Loss_1: (0.0268) | Acc_1: (99.13%) (12816/12928)\n",
      "Epoch: 73 | Batch_idx: 110 |  Loss_1: (0.0263) | Acc_1: (99.15%) (14087/14208)\n",
      "Epoch: 73 | Batch_idx: 120 |  Loss_1: (0.0260) | Acc_1: (99.17%) (15359/15488)\n",
      "Epoch: 73 | Batch_idx: 130 |  Loss_1: (0.0259) | Acc_1: (99.17%) (16628/16768)\n",
      "Epoch: 73 | Batch_idx: 140 |  Loss_1: (0.0253) | Acc_1: (99.19%) (17902/18048)\n",
      "Epoch: 73 | Batch_idx: 150 |  Loss_1: (0.0252) | Acc_1: (99.20%) (19174/19328)\n",
      "Epoch: 73 | Batch_idx: 160 |  Loss_1: (0.0248) | Acc_1: (99.21%) (20446/20608)\n",
      "Epoch: 73 | Batch_idx: 170 |  Loss_1: (0.0245) | Acc_1: (99.21%) (21715/21888)\n",
      "Epoch: 73 | Batch_idx: 180 |  Loss_1: (0.0247) | Acc_1: (99.18%) (22979/23168)\n",
      "Epoch: 73 | Batch_idx: 190 |  Loss_1: (0.0249) | Acc_1: (99.16%) (24242/24448)\n",
      "Epoch: 73 | Batch_idx: 200 |  Loss_1: (0.0248) | Acc_1: (99.15%) (25510/25728)\n",
      "Epoch: 73 | Batch_idx: 210 |  Loss_1: (0.0245) | Acc_1: (99.17%) (26783/27008)\n",
      "Epoch: 73 | Batch_idx: 220 |  Loss_1: (0.0242) | Acc_1: (99.17%) (28054/28288)\n",
      "Epoch: 73 | Batch_idx: 230 |  Loss_1: (0.0241) | Acc_1: (99.18%) (29326/29568)\n",
      "Epoch: 73 | Batch_idx: 240 |  Loss_1: (0.0239) | Acc_1: (99.19%) (30599/30848)\n",
      "Epoch: 73 | Batch_idx: 250 |  Loss_1: (0.0241) | Acc_1: (99.20%) (31870/32128)\n",
      "Epoch: 73 | Batch_idx: 260 |  Loss_1: (0.0240) | Acc_1: (99.19%) (33138/33408)\n",
      "Epoch: 73 | Batch_idx: 270 |  Loss_1: (0.0238) | Acc_1: (99.20%) (34411/34688)\n",
      "Epoch: 73 | Batch_idx: 280 |  Loss_1: (0.0236) | Acc_1: (99.20%) (35681/35968)\n",
      "Epoch: 73 | Batch_idx: 290 |  Loss_1: (0.0234) | Acc_1: (99.21%) (36952/37248)\n",
      "Epoch: 73 | Batch_idx: 300 |  Loss_1: (0.0232) | Acc_1: (99.21%) (38225/38528)\n",
      "Epoch: 73 | Batch_idx: 310 |  Loss_1: (0.0232) | Acc_1: (99.21%) (39495/39808)\n",
      "Epoch: 73 | Batch_idx: 320 |  Loss_1: (0.0233) | Acc_1: (99.20%) (40760/41088)\n",
      "Epoch: 73 | Batch_idx: 330 |  Loss_1: (0.0231) | Acc_1: (99.20%) (42031/42368)\n",
      "Epoch: 73 | Batch_idx: 340 |  Loss_1: (0.0228) | Acc_1: (99.21%) (43303/43648)\n",
      "Epoch: 73 | Batch_idx: 350 |  Loss_1: (0.0225) | Acc_1: (99.22%) (44578/44928)\n",
      "Epoch: 73 | Batch_idx: 360 |  Loss_1: (0.0225) | Acc_1: (99.22%) (45849/46208)\n",
      "Epoch: 73 | Batch_idx: 370 |  Loss_1: (0.0225) | Acc_1: (99.22%) (47119/47488)\n",
      "Epoch: 73 | Batch_idx: 380 |  Loss_1: (0.0225) | Acc_1: (99.21%) (48384/48768)\n",
      "Epoch: 73 | Batch_idx: 390 |  Loss_1: (0.0226) | Acc_1: (99.21%) (49606/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5202) | Acc: (90.84%) (9084/10000)\n",
      "Epoch: 74 | Batch_idx: 0 |  Loss_1: (0.0429) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 74 | Batch_idx: 10 |  Loss_1: (0.0246) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 74 | Batch_idx: 20 |  Loss_1: (0.0226) | Acc_1: (99.18%) (2666/2688)\n",
      "Epoch: 74 | Batch_idx: 30 |  Loss_1: (0.0205) | Acc_1: (99.27%) (3939/3968)\n",
      "Epoch: 74 | Batch_idx: 40 |  Loss_1: (0.0199) | Acc_1: (99.26%) (5209/5248)\n",
      "Epoch: 74 | Batch_idx: 50 |  Loss_1: (0.0191) | Acc_1: (99.31%) (6483/6528)\n",
      "Epoch: 74 | Batch_idx: 60 |  Loss_1: (0.0193) | Acc_1: (99.32%) (7755/7808)\n",
      "Epoch: 74 | Batch_idx: 70 |  Loss_1: (0.0203) | Acc_1: (99.27%) (9022/9088)\n",
      "Epoch: 74 | Batch_idx: 80 |  Loss_1: (0.0203) | Acc_1: (99.28%) (10293/10368)\n",
      "Epoch: 74 | Batch_idx: 90 |  Loss_1: (0.0202) | Acc_1: (99.28%) (11564/11648)\n",
      "Epoch: 74 | Batch_idx: 100 |  Loss_1: (0.0202) | Acc_1: (99.24%) (12830/12928)\n",
      "Epoch: 74 | Batch_idx: 110 |  Loss_1: (0.0203) | Acc_1: (99.23%) (14099/14208)\n",
      "Epoch: 74 | Batch_idx: 120 |  Loss_1: (0.0199) | Acc_1: (99.25%) (15372/15488)\n",
      "Epoch: 74 | Batch_idx: 130 |  Loss_1: (0.0198) | Acc_1: (99.25%) (16643/16768)\n",
      "Epoch: 74 | Batch_idx: 140 |  Loss_1: (0.0205) | Acc_1: (99.24%) (17911/18048)\n",
      "Epoch: 74 | Batch_idx: 150 |  Loss_1: (0.0204) | Acc_1: (99.24%) (19182/19328)\n",
      "Epoch: 74 | Batch_idx: 160 |  Loss_1: (0.0210) | Acc_1: (99.24%) (20451/20608)\n",
      "Epoch: 74 | Batch_idx: 170 |  Loss_1: (0.0207) | Acc_1: (99.25%) (21723/21888)\n",
      "Epoch: 74 | Batch_idx: 180 |  Loss_1: (0.0210) | Acc_1: (99.24%) (22993/23168)\n",
      "Epoch: 74 | Batch_idx: 190 |  Loss_1: (0.0207) | Acc_1: (99.26%) (24267/24448)\n",
      "Epoch: 74 | Batch_idx: 200 |  Loss_1: (0.0206) | Acc_1: (99.27%) (25540/25728)\n",
      "Epoch: 74 | Batch_idx: 210 |  Loss_1: (0.0209) | Acc_1: (99.26%) (26809/27008)\n",
      "Epoch: 74 | Batch_idx: 220 |  Loss_1: (0.0211) | Acc_1: (99.26%) (28079/28288)\n",
      "Epoch: 74 | Batch_idx: 230 |  Loss_1: (0.0214) | Acc_1: (99.26%) (29348/29568)\n",
      "Epoch: 74 | Batch_idx: 240 |  Loss_1: (0.0215) | Acc_1: (99.25%) (30618/30848)\n",
      "Epoch: 74 | Batch_idx: 250 |  Loss_1: (0.0215) | Acc_1: (99.25%) (31888/32128)\n",
      "Epoch: 74 | Batch_idx: 260 |  Loss_1: (0.0223) | Acc_1: (99.24%) (33153/33408)\n",
      "Epoch: 74 | Batch_idx: 270 |  Loss_1: (0.0220) | Acc_1: (99.24%) (34425/34688)\n",
      "Epoch: 74 | Batch_idx: 280 |  Loss_1: (0.0223) | Acc_1: (99.23%) (35691/35968)\n",
      "Epoch: 74 | Batch_idx: 290 |  Loss_1: (0.0222) | Acc_1: (99.23%) (36962/37248)\n",
      "Epoch: 74 | Batch_idx: 300 |  Loss_1: (0.0225) | Acc_1: (99.22%) (38227/38528)\n",
      "Epoch: 74 | Batch_idx: 310 |  Loss_1: (0.0225) | Acc_1: (99.21%) (39494/39808)\n",
      "Epoch: 74 | Batch_idx: 320 |  Loss_1: (0.0227) | Acc_1: (99.21%) (40763/41088)\n",
      "Epoch: 74 | Batch_idx: 330 |  Loss_1: (0.0225) | Acc_1: (99.21%) (42035/42368)\n",
      "Epoch: 74 | Batch_idx: 340 |  Loss_1: (0.0227) | Acc_1: (99.22%) (43306/43648)\n",
      "Epoch: 74 | Batch_idx: 350 |  Loss_1: (0.0227) | Acc_1: (99.22%) (44578/44928)\n",
      "Epoch: 74 | Batch_idx: 360 |  Loss_1: (0.0227) | Acc_1: (99.23%) (45850/46208)\n",
      "Epoch: 74 | Batch_idx: 370 |  Loss_1: (0.0227) | Acc_1: (99.22%) (47119/47488)\n",
      "Epoch: 74 | Batch_idx: 380 |  Loss_1: (0.0225) | Acc_1: (99.23%) (48391/48768)\n",
      "Epoch: 74 | Batch_idx: 390 |  Loss_1: (0.0227) | Acc_1: (99.22%) (49610/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4835) | Acc: (91.08%) (9108/10000)\n",
      "Epoch: 75 | Batch_idx: 0 |  Loss_1: (0.0137) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 75 | Batch_idx: 10 |  Loss_1: (0.0140) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 75 | Batch_idx: 20 |  Loss_1: (0.0138) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 75 | Batch_idx: 30 |  Loss_1: (0.0158) | Acc_1: (99.52%) (3949/3968)\n",
      "Epoch: 75 | Batch_idx: 40 |  Loss_1: (0.0178) | Acc_1: (99.45%) (5219/5248)\n",
      "Epoch: 75 | Batch_idx: 50 |  Loss_1: (0.0220) | Acc_1: (99.30%) (6482/6528)\n",
      "Epoch: 75 | Batch_idx: 60 |  Loss_1: (0.0229) | Acc_1: (99.28%) (7752/7808)\n",
      "Epoch: 75 | Batch_idx: 70 |  Loss_1: (0.0211) | Acc_1: (99.33%) (9027/9088)\n",
      "Epoch: 75 | Batch_idx: 80 |  Loss_1: (0.0229) | Acc_1: (99.20%) (10285/10368)\n",
      "Epoch: 75 | Batch_idx: 90 |  Loss_1: (0.0226) | Acc_1: (99.20%) (11555/11648)\n",
      "Epoch: 75 | Batch_idx: 100 |  Loss_1: (0.0234) | Acc_1: (99.16%) (12820/12928)\n",
      "Epoch: 75 | Batch_idx: 110 |  Loss_1: (0.0246) | Acc_1: (99.11%) (14081/14208)\n",
      "Epoch: 75 | Batch_idx: 120 |  Loss_1: (0.0248) | Acc_1: (99.12%) (15351/15488)\n",
      "Epoch: 75 | Batch_idx: 130 |  Loss_1: (0.0247) | Acc_1: (99.14%) (16623/16768)\n",
      "Epoch: 75 | Batch_idx: 140 |  Loss_1: (0.0247) | Acc_1: (99.14%) (17892/18048)\n",
      "Epoch: 75 | Batch_idx: 150 |  Loss_1: (0.0245) | Acc_1: (99.13%) (19160/19328)\n",
      "Epoch: 75 | Batch_idx: 160 |  Loss_1: (0.0242) | Acc_1: (99.13%) (20429/20608)\n",
      "Epoch: 75 | Batch_idx: 170 |  Loss_1: (0.0237) | Acc_1: (99.16%) (21704/21888)\n",
      "Epoch: 75 | Batch_idx: 180 |  Loss_1: (0.0236) | Acc_1: (99.16%) (22974/23168)\n",
      "Epoch: 75 | Batch_idx: 190 |  Loss_1: (0.0236) | Acc_1: (99.17%) (24245/24448)\n",
      "Epoch: 75 | Batch_idx: 200 |  Loss_1: (0.0238) | Acc_1: (99.16%) (25512/25728)\n",
      "Epoch: 75 | Batch_idx: 210 |  Loss_1: (0.0235) | Acc_1: (99.16%) (26782/27008)\n",
      "Epoch: 75 | Batch_idx: 220 |  Loss_1: (0.0238) | Acc_1: (99.17%) (28054/28288)\n",
      "Epoch: 75 | Batch_idx: 230 |  Loss_1: (0.0236) | Acc_1: (99.18%) (29325/29568)\n",
      "Epoch: 75 | Batch_idx: 240 |  Loss_1: (0.0234) | Acc_1: (99.18%) (30596/30848)\n",
      "Epoch: 75 | Batch_idx: 250 |  Loss_1: (0.0233) | Acc_1: (99.18%) (31866/32128)\n",
      "Epoch: 75 | Batch_idx: 260 |  Loss_1: (0.0235) | Acc_1: (99.17%) (33132/33408)\n",
      "Epoch: 75 | Batch_idx: 270 |  Loss_1: (0.0236) | Acc_1: (99.17%) (34401/34688)\n",
      "Epoch: 75 | Batch_idx: 280 |  Loss_1: (0.0234) | Acc_1: (99.18%) (35672/35968)\n",
      "Epoch: 75 | Batch_idx: 290 |  Loss_1: (0.0230) | Acc_1: (99.20%) (36949/37248)\n",
      "Epoch: 75 | Batch_idx: 300 |  Loss_1: (0.0227) | Acc_1: (99.21%) (38222/38528)\n",
      "Epoch: 75 | Batch_idx: 310 |  Loss_1: (0.0227) | Acc_1: (99.21%) (39492/39808)\n",
      "Epoch: 75 | Batch_idx: 320 |  Loss_1: (0.0227) | Acc_1: (99.21%) (40762/41088)\n",
      "Epoch: 75 | Batch_idx: 330 |  Loss_1: (0.0225) | Acc_1: (99.21%) (42032/42368)\n",
      "Epoch: 75 | Batch_idx: 340 |  Loss_1: (0.0225) | Acc_1: (99.21%) (43304/43648)\n",
      "Epoch: 75 | Batch_idx: 350 |  Loss_1: (0.0226) | Acc_1: (99.21%) (44573/44928)\n",
      "Epoch: 75 | Batch_idx: 360 |  Loss_1: (0.0227) | Acc_1: (99.20%) (45838/46208)\n",
      "Epoch: 75 | Batch_idx: 370 |  Loss_1: (0.0226) | Acc_1: (99.21%) (47113/47488)\n",
      "Epoch: 75 | Batch_idx: 380 |  Loss_1: (0.0225) | Acc_1: (99.21%) (48383/48768)\n",
      "Epoch: 75 | Batch_idx: 390 |  Loss_1: (0.0224) | Acc_1: (99.21%) (49604/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5321) | Acc: (90.62%) (9062/10000)\n",
      "Epoch: 76 | Batch_idx: 0 |  Loss_1: (0.0091) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 76 | Batch_idx: 10 |  Loss_1: (0.0158) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 76 | Batch_idx: 20 |  Loss_1: (0.0161) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 76 | Batch_idx: 30 |  Loss_1: (0.0184) | Acc_1: (99.29%) (3940/3968)\n",
      "Epoch: 76 | Batch_idx: 40 |  Loss_1: (0.0182) | Acc_1: (99.29%) (5211/5248)\n",
      "Epoch: 76 | Batch_idx: 50 |  Loss_1: (0.0206) | Acc_1: (99.20%) (6476/6528)\n",
      "Epoch: 76 | Batch_idx: 60 |  Loss_1: (0.0214) | Acc_1: (99.19%) (7745/7808)\n",
      "Epoch: 76 | Batch_idx: 70 |  Loss_1: (0.0207) | Acc_1: (99.22%) (9017/9088)\n",
      "Epoch: 76 | Batch_idx: 80 |  Loss_1: (0.0203) | Acc_1: (99.22%) (10287/10368)\n",
      "Epoch: 76 | Batch_idx: 90 |  Loss_1: (0.0204) | Acc_1: (99.24%) (11559/11648)\n",
      "Epoch: 76 | Batch_idx: 100 |  Loss_1: (0.0206) | Acc_1: (99.22%) (12827/12928)\n",
      "Epoch: 76 | Batch_idx: 110 |  Loss_1: (0.0201) | Acc_1: (99.23%) (14099/14208)\n",
      "Epoch: 76 | Batch_idx: 120 |  Loss_1: (0.0205) | Acc_1: (99.20%) (15364/15488)\n",
      "Epoch: 76 | Batch_idx: 130 |  Loss_1: (0.0204) | Acc_1: (99.22%) (16638/16768)\n",
      "Epoch: 76 | Batch_idx: 140 |  Loss_1: (0.0199) | Acc_1: (99.24%) (17910/18048)\n",
      "Epoch: 76 | Batch_idx: 150 |  Loss_1: (0.0194) | Acc_1: (99.25%) (19184/19328)\n",
      "Epoch: 76 | Batch_idx: 160 |  Loss_1: (0.0199) | Acc_1: (99.23%) (20450/20608)\n",
      "Epoch: 76 | Batch_idx: 170 |  Loss_1: (0.0193) | Acc_1: (99.27%) (21728/21888)\n",
      "Epoch: 76 | Batch_idx: 180 |  Loss_1: (0.0190) | Acc_1: (99.29%) (23003/23168)\n",
      "Epoch: 76 | Batch_idx: 190 |  Loss_1: (0.0190) | Acc_1: (99.28%) (24273/24448)\n",
      "Epoch: 76 | Batch_idx: 200 |  Loss_1: (0.0191) | Acc_1: (99.29%) (25545/25728)\n",
      "Epoch: 76 | Batch_idx: 210 |  Loss_1: (0.0193) | Acc_1: (99.28%) (26814/27008)\n",
      "Epoch: 76 | Batch_idx: 220 |  Loss_1: (0.0197) | Acc_1: (99.28%) (28083/28288)\n",
      "Epoch: 76 | Batch_idx: 230 |  Loss_1: (0.0198) | Acc_1: (99.26%) (29349/29568)\n",
      "Epoch: 76 | Batch_idx: 240 |  Loss_1: (0.0198) | Acc_1: (99.26%) (30620/30848)\n",
      "Epoch: 76 | Batch_idx: 250 |  Loss_1: (0.0202) | Acc_1: (99.25%) (31887/32128)\n",
      "Epoch: 76 | Batch_idx: 260 |  Loss_1: (0.0200) | Acc_1: (99.25%) (33159/33408)\n",
      "Epoch: 76 | Batch_idx: 270 |  Loss_1: (0.0203) | Acc_1: (99.24%) (34423/34688)\n",
      "Epoch: 76 | Batch_idx: 280 |  Loss_1: (0.0205) | Acc_1: (99.24%) (35694/35968)\n",
      "Epoch: 76 | Batch_idx: 290 |  Loss_1: (0.0208) | Acc_1: (99.23%) (36961/37248)\n",
      "Epoch: 76 | Batch_idx: 300 |  Loss_1: (0.0209) | Acc_1: (99.24%) (38234/38528)\n",
      "Epoch: 76 | Batch_idx: 310 |  Loss_1: (0.0208) | Acc_1: (99.24%) (39505/39808)\n",
      "Epoch: 76 | Batch_idx: 320 |  Loss_1: (0.0208) | Acc_1: (99.24%) (40775/41088)\n",
      "Epoch: 76 | Batch_idx: 330 |  Loss_1: (0.0209) | Acc_1: (99.24%) (42045/42368)\n",
      "Epoch: 76 | Batch_idx: 340 |  Loss_1: (0.0208) | Acc_1: (99.24%) (43316/43648)\n",
      "Epoch: 76 | Batch_idx: 350 |  Loss_1: (0.0209) | Acc_1: (99.25%) (44589/44928)\n",
      "Epoch: 76 | Batch_idx: 360 |  Loss_1: (0.0209) | Acc_1: (99.25%) (45860/46208)\n",
      "Epoch: 76 | Batch_idx: 370 |  Loss_1: (0.0209) | Acc_1: (99.24%) (47129/47488)\n",
      "Epoch: 76 | Batch_idx: 380 |  Loss_1: (0.0207) | Acc_1: (99.25%) (48402/48768)\n",
      "Epoch: 76 | Batch_idx: 390 |  Loss_1: (0.0207) | Acc_1: (99.25%) (49626/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4928) | Acc: (90.83%) (9083/10000)\n",
      "Epoch: 77 | Batch_idx: 0 |  Loss_1: (0.0039) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 77 | Batch_idx: 10 |  Loss_1: (0.0157) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 77 | Batch_idx: 20 |  Loss_1: (0.0143) | Acc_1: (99.52%) (2675/2688)\n",
      "Epoch: 77 | Batch_idx: 30 |  Loss_1: (0.0158) | Acc_1: (99.42%) (3945/3968)\n",
      "Epoch: 77 | Batch_idx: 40 |  Loss_1: (0.0153) | Acc_1: (99.45%) (5219/5248)\n",
      "Epoch: 77 | Batch_idx: 50 |  Loss_1: (0.0160) | Acc_1: (99.42%) (6490/6528)\n",
      "Epoch: 77 | Batch_idx: 60 |  Loss_1: (0.0152) | Acc_1: (99.47%) (7767/7808)\n",
      "Epoch: 77 | Batch_idx: 70 |  Loss_1: (0.0151) | Acc_1: (99.49%) (9042/9088)\n",
      "Epoch: 77 | Batch_idx: 80 |  Loss_1: (0.0158) | Acc_1: (99.48%) (10314/10368)\n",
      "Epoch: 77 | Batch_idx: 90 |  Loss_1: (0.0158) | Acc_1: (99.48%) (11587/11648)\n",
      "Epoch: 77 | Batch_idx: 100 |  Loss_1: (0.0152) | Acc_1: (99.49%) (12862/12928)\n",
      "Epoch: 77 | Batch_idx: 110 |  Loss_1: (0.0152) | Acc_1: (99.49%) (14135/14208)\n",
      "Epoch: 77 | Batch_idx: 120 |  Loss_1: (0.0158) | Acc_1: (99.47%) (15406/15488)\n",
      "Epoch: 77 | Batch_idx: 130 |  Loss_1: (0.0159) | Acc_1: (99.46%) (16677/16768)\n",
      "Epoch: 77 | Batch_idx: 140 |  Loss_1: (0.0164) | Acc_1: (99.45%) (17949/18048)\n",
      "Epoch: 77 | Batch_idx: 150 |  Loss_1: (0.0166) | Acc_1: (99.43%) (19218/19328)\n",
      "Epoch: 77 | Batch_idx: 160 |  Loss_1: (0.0167) | Acc_1: (99.43%) (20491/20608)\n",
      "Epoch: 77 | Batch_idx: 170 |  Loss_1: (0.0166) | Acc_1: (99.44%) (21765/21888)\n",
      "Epoch: 77 | Batch_idx: 180 |  Loss_1: (0.0163) | Acc_1: (99.45%) (23040/23168)\n",
      "Epoch: 77 | Batch_idx: 190 |  Loss_1: (0.0160) | Acc_1: (99.46%) (24315/24448)\n",
      "Epoch: 77 | Batch_idx: 200 |  Loss_1: (0.0156) | Acc_1: (99.47%) (25592/25728)\n",
      "Epoch: 77 | Batch_idx: 210 |  Loss_1: (0.0156) | Acc_1: (99.47%) (26866/27008)\n",
      "Epoch: 77 | Batch_idx: 220 |  Loss_1: (0.0152) | Acc_1: (99.49%) (28144/28288)\n",
      "Epoch: 77 | Batch_idx: 230 |  Loss_1: (0.0151) | Acc_1: (99.50%) (29419/29568)\n",
      "Epoch: 77 | Batch_idx: 240 |  Loss_1: (0.0151) | Acc_1: (99.50%) (30695/30848)\n",
      "Epoch: 77 | Batch_idx: 250 |  Loss_1: (0.0149) | Acc_1: (99.51%) (31969/32128)\n",
      "Epoch: 77 | Batch_idx: 260 |  Loss_1: (0.0149) | Acc_1: (99.51%) (33245/33408)\n",
      "Epoch: 77 | Batch_idx: 270 |  Loss_1: (0.0149) | Acc_1: (99.51%) (34517/34688)\n",
      "Epoch: 77 | Batch_idx: 280 |  Loss_1: (0.0149) | Acc_1: (99.50%) (35789/35968)\n",
      "Epoch: 77 | Batch_idx: 290 |  Loss_1: (0.0152) | Acc_1: (99.49%) (37059/37248)\n",
      "Epoch: 77 | Batch_idx: 300 |  Loss_1: (0.0154) | Acc_1: (99.48%) (38329/38528)\n",
      "Epoch: 77 | Batch_idx: 310 |  Loss_1: (0.0153) | Acc_1: (99.49%) (39604/39808)\n",
      "Epoch: 77 | Batch_idx: 320 |  Loss_1: (0.0154) | Acc_1: (99.49%) (40878/41088)\n",
      "Epoch: 77 | Batch_idx: 330 |  Loss_1: (0.0153) | Acc_1: (99.49%) (42151/42368)\n",
      "Epoch: 77 | Batch_idx: 340 |  Loss_1: (0.0153) | Acc_1: (99.49%) (43424/43648)\n",
      "Epoch: 77 | Batch_idx: 350 |  Loss_1: (0.0159) | Acc_1: (99.48%) (44695/44928)\n",
      "Epoch: 77 | Batch_idx: 360 |  Loss_1: (0.0160) | Acc_1: (99.47%) (45965/46208)\n",
      "Epoch: 77 | Batch_idx: 370 |  Loss_1: (0.0164) | Acc_1: (99.47%) (47234/47488)\n",
      "Epoch: 77 | Batch_idx: 380 |  Loss_1: (0.0165) | Acc_1: (99.46%) (48505/48768)\n",
      "Epoch: 77 | Batch_idx: 390 |  Loss_1: (0.0167) | Acc_1: (99.45%) (49726/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4667) | Acc: (91.64%) (9164/10000)\n",
      "Epoch: 78 | Batch_idx: 0 |  Loss_1: (0.0032) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 78 | Batch_idx: 10 |  Loss_1: (0.0146) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 78 | Batch_idx: 20 |  Loss_1: (0.0194) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 78 | Batch_idx: 30 |  Loss_1: (0.0161) | Acc_1: (99.55%) (3950/3968)\n",
      "Epoch: 78 | Batch_idx: 40 |  Loss_1: (0.0141) | Acc_1: (99.62%) (5228/5248)\n",
      "Epoch: 78 | Batch_idx: 50 |  Loss_1: (0.0149) | Acc_1: (99.56%) (6499/6528)\n",
      "Epoch: 78 | Batch_idx: 60 |  Loss_1: (0.0148) | Acc_1: (99.59%) (7776/7808)\n",
      "Epoch: 78 | Batch_idx: 70 |  Loss_1: (0.0148) | Acc_1: (99.56%) (9048/9088)\n",
      "Epoch: 78 | Batch_idx: 80 |  Loss_1: (0.0139) | Acc_1: (99.59%) (10326/10368)\n",
      "Epoch: 78 | Batch_idx: 90 |  Loss_1: (0.0136) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 78 | Batch_idx: 100 |  Loss_1: (0.0139) | Acc_1: (99.60%) (12876/12928)\n",
      "Epoch: 78 | Batch_idx: 110 |  Loss_1: (0.0134) | Acc_1: (99.61%) (14153/14208)\n",
      "Epoch: 78 | Batch_idx: 120 |  Loss_1: (0.0134) | Acc_1: (99.61%) (15428/15488)\n",
      "Epoch: 78 | Batch_idx: 130 |  Loss_1: (0.0133) | Acc_1: (99.61%) (16703/16768)\n",
      "Epoch: 78 | Batch_idx: 140 |  Loss_1: (0.0133) | Acc_1: (99.61%) (17977/18048)\n",
      "Epoch: 78 | Batch_idx: 150 |  Loss_1: (0.0135) | Acc_1: (99.60%) (19250/19328)\n",
      "Epoch: 78 | Batch_idx: 160 |  Loss_1: (0.0130) | Acc_1: (99.61%) (20527/20608)\n",
      "Epoch: 78 | Batch_idx: 170 |  Loss_1: (0.0129) | Acc_1: (99.61%) (21803/21888)\n",
      "Epoch: 78 | Batch_idx: 180 |  Loss_1: (0.0129) | Acc_1: (99.61%) (23078/23168)\n",
      "Epoch: 78 | Batch_idx: 190 |  Loss_1: (0.0133) | Acc_1: (99.59%) (24348/24448)\n",
      "Epoch: 78 | Batch_idx: 200 |  Loss_1: (0.0133) | Acc_1: (99.58%) (25620/25728)\n",
      "Epoch: 78 | Batch_idx: 210 |  Loss_1: (0.0132) | Acc_1: (99.58%) (26895/27008)\n",
      "Epoch: 78 | Batch_idx: 220 |  Loss_1: (0.0130) | Acc_1: (99.58%) (28170/28288)\n",
      "Epoch: 78 | Batch_idx: 230 |  Loss_1: (0.0128) | Acc_1: (99.59%) (29447/29568)\n",
      "Epoch: 78 | Batch_idx: 240 |  Loss_1: (0.0129) | Acc_1: (99.58%) (30719/30848)\n",
      "Epoch: 78 | Batch_idx: 250 |  Loss_1: (0.0133) | Acc_1: (99.56%) (31988/32128)\n",
      "Epoch: 78 | Batch_idx: 260 |  Loss_1: (0.0132) | Acc_1: (99.57%) (33264/33408)\n",
      "Epoch: 78 | Batch_idx: 270 |  Loss_1: (0.0130) | Acc_1: (99.58%) (34543/34688)\n",
      "Epoch: 78 | Batch_idx: 280 |  Loss_1: (0.0129) | Acc_1: (99.59%) (35819/35968)\n",
      "Epoch: 78 | Batch_idx: 290 |  Loss_1: (0.0128) | Acc_1: (99.59%) (37097/37248)\n",
      "Epoch: 78 | Batch_idx: 300 |  Loss_1: (0.0127) | Acc_1: (99.60%) (38374/38528)\n",
      "Epoch: 78 | Batch_idx: 310 |  Loss_1: (0.0128) | Acc_1: (99.60%) (39648/39808)\n",
      "Epoch: 78 | Batch_idx: 320 |  Loss_1: (0.0125) | Acc_1: (99.61%) (40928/41088)\n",
      "Epoch: 78 | Batch_idx: 330 |  Loss_1: (0.0125) | Acc_1: (99.61%) (42201/42368)\n",
      "Epoch: 78 | Batch_idx: 340 |  Loss_1: (0.0124) | Acc_1: (99.61%) (43476/43648)\n",
      "Epoch: 78 | Batch_idx: 350 |  Loss_1: (0.0122) | Acc_1: (99.61%) (44752/44928)\n",
      "Epoch: 78 | Batch_idx: 360 |  Loss_1: (0.0122) | Acc_1: (99.61%) (46026/46208)\n",
      "Epoch: 78 | Batch_idx: 370 |  Loss_1: (0.0123) | Acc_1: (99.60%) (47299/47488)\n",
      "Epoch: 78 | Batch_idx: 380 |  Loss_1: (0.0123) | Acc_1: (99.60%) (48575/48768)\n",
      "Epoch: 78 | Batch_idx: 390 |  Loss_1: (0.0122) | Acc_1: (99.61%) (49804/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4763) | Acc: (91.56%) (9156/10000)\n",
      "Epoch: 79 | Batch_idx: 0 |  Loss_1: (0.0091) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 79 | Batch_idx: 10 |  Loss_1: (0.0076) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 79 | Batch_idx: 20 |  Loss_1: (0.0088) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 79 | Batch_idx: 30 |  Loss_1: (0.0101) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 79 | Batch_idx: 40 |  Loss_1: (0.0102) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 79 | Batch_idx: 50 |  Loss_1: (0.0102) | Acc_1: (99.59%) (6501/6528)\n",
      "Epoch: 79 | Batch_idx: 60 |  Loss_1: (0.0101) | Acc_1: (99.60%) (7777/7808)\n",
      "Epoch: 79 | Batch_idx: 70 |  Loss_1: (0.0100) | Acc_1: (99.59%) (9051/9088)\n",
      "Epoch: 79 | Batch_idx: 80 |  Loss_1: (0.0100) | Acc_1: (99.60%) (10327/10368)\n",
      "Epoch: 79 | Batch_idx: 90 |  Loss_1: (0.0103) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 79 | Batch_idx: 100 |  Loss_1: (0.0102) | Acc_1: (99.61%) (12878/12928)\n",
      "Epoch: 79 | Batch_idx: 110 |  Loss_1: (0.0106) | Acc_1: (99.61%) (14152/14208)\n",
      "Epoch: 79 | Batch_idx: 120 |  Loss_1: (0.0111) | Acc_1: (99.58%) (15423/15488)\n",
      "Epoch: 79 | Batch_idx: 130 |  Loss_1: (0.0110) | Acc_1: (99.59%) (16699/16768)\n",
      "Epoch: 79 | Batch_idx: 140 |  Loss_1: (0.0110) | Acc_1: (99.60%) (17975/18048)\n",
      "Epoch: 79 | Batch_idx: 150 |  Loss_1: (0.0115) | Acc_1: (99.58%) (19246/19328)\n",
      "Epoch: 79 | Batch_idx: 160 |  Loss_1: (0.0113) | Acc_1: (99.58%) (20522/20608)\n",
      "Epoch: 79 | Batch_idx: 170 |  Loss_1: (0.0113) | Acc_1: (99.59%) (21798/21888)\n",
      "Epoch: 79 | Batch_idx: 180 |  Loss_1: (0.0112) | Acc_1: (99.59%) (23073/23168)\n",
      "Epoch: 79 | Batch_idx: 190 |  Loss_1: (0.0110) | Acc_1: (99.61%) (24352/24448)\n",
      "Epoch: 79 | Batch_idx: 200 |  Loss_1: (0.0107) | Acc_1: (99.62%) (25629/25728)\n",
      "Epoch: 79 | Batch_idx: 210 |  Loss_1: (0.0108) | Acc_1: (99.62%) (26906/27008)\n",
      "Epoch: 79 | Batch_idx: 220 |  Loss_1: (0.0111) | Acc_1: (99.60%) (28176/28288)\n",
      "Epoch: 79 | Batch_idx: 230 |  Loss_1: (0.0110) | Acc_1: (99.61%) (29453/29568)\n",
      "Epoch: 79 | Batch_idx: 240 |  Loss_1: (0.0111) | Acc_1: (99.61%) (30728/30848)\n",
      "Epoch: 79 | Batch_idx: 250 |  Loss_1: (0.0112) | Acc_1: (99.61%) (32004/32128)\n",
      "Epoch: 79 | Batch_idx: 260 |  Loss_1: (0.0114) | Acc_1: (99.61%) (33278/33408)\n",
      "Epoch: 79 | Batch_idx: 270 |  Loss_1: (0.0117) | Acc_1: (99.61%) (34552/34688)\n",
      "Epoch: 79 | Batch_idx: 280 |  Loss_1: (0.0118) | Acc_1: (99.61%) (35828/35968)\n",
      "Epoch: 79 | Batch_idx: 290 |  Loss_1: (0.0123) | Acc_1: (99.59%) (37096/37248)\n",
      "Epoch: 79 | Batch_idx: 300 |  Loss_1: (0.0125) | Acc_1: (99.59%) (38370/38528)\n",
      "Epoch: 79 | Batch_idx: 310 |  Loss_1: (0.0126) | Acc_1: (99.59%) (39644/39808)\n",
      "Epoch: 79 | Batch_idx: 320 |  Loss_1: (0.0124) | Acc_1: (99.60%) (40922/41088)\n",
      "Epoch: 79 | Batch_idx: 330 |  Loss_1: (0.0125) | Acc_1: (99.59%) (42196/42368)\n",
      "Epoch: 79 | Batch_idx: 340 |  Loss_1: (0.0124) | Acc_1: (99.59%) (43471/43648)\n",
      "Epoch: 79 | Batch_idx: 350 |  Loss_1: (0.0124) | Acc_1: (99.59%) (44743/44928)\n",
      "Epoch: 79 | Batch_idx: 360 |  Loss_1: (0.0125) | Acc_1: (99.58%) (46015/46208)\n",
      "Epoch: 79 | Batch_idx: 370 |  Loss_1: (0.0126) | Acc_1: (99.58%) (47288/47488)\n",
      "Epoch: 79 | Batch_idx: 380 |  Loss_1: (0.0127) | Acc_1: (99.57%) (48558/48768)\n",
      "Epoch: 79 | Batch_idx: 390 |  Loss_1: (0.0127) | Acc_1: (99.56%) (49782/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5284) | Acc: (91.18%) (9118/10000)\n",
      "Epoch: 80 | Batch_idx: 0 |  Loss_1: (0.0018) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 80 | Batch_idx: 10 |  Loss_1: (0.0116) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 80 | Batch_idx: 20 |  Loss_1: (0.0118) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 80 | Batch_idx: 30 |  Loss_1: (0.0120) | Acc_1: (99.62%) (3953/3968)\n",
      "Epoch: 80 | Batch_idx: 40 |  Loss_1: (0.0136) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 80 | Batch_idx: 50 |  Loss_1: (0.0142) | Acc_1: (99.56%) (6499/6528)\n",
      "Epoch: 80 | Batch_idx: 60 |  Loss_1: (0.0148) | Acc_1: (99.54%) (7772/7808)\n",
      "Epoch: 80 | Batch_idx: 70 |  Loss_1: (0.0170) | Acc_1: (99.50%) (9043/9088)\n",
      "Epoch: 80 | Batch_idx: 80 |  Loss_1: (0.0168) | Acc_1: (99.51%) (10317/10368)\n",
      "Epoch: 80 | Batch_idx: 90 |  Loss_1: (0.0164) | Acc_1: (99.50%) (11590/11648)\n",
      "Epoch: 80 | Batch_idx: 100 |  Loss_1: (0.0168) | Acc_1: (99.47%) (12859/12928)\n",
      "Epoch: 80 | Batch_idx: 110 |  Loss_1: (0.0168) | Acc_1: (99.46%) (14131/14208)\n",
      "Epoch: 80 | Batch_idx: 120 |  Loss_1: (0.0174) | Acc_1: (99.44%) (15401/15488)\n",
      "Epoch: 80 | Batch_idx: 130 |  Loss_1: (0.0174) | Acc_1: (99.45%) (16675/16768)\n",
      "Epoch: 80 | Batch_idx: 140 |  Loss_1: (0.0172) | Acc_1: (99.44%) (17947/18048)\n",
      "Epoch: 80 | Batch_idx: 150 |  Loss_1: (0.0180) | Acc_1: (99.43%) (19218/19328)\n",
      "Epoch: 80 | Batch_idx: 160 |  Loss_1: (0.0182) | Acc_1: (99.41%) (20487/20608)\n",
      "Epoch: 80 | Batch_idx: 170 |  Loss_1: (0.0182) | Acc_1: (99.42%) (21761/21888)\n",
      "Epoch: 80 | Batch_idx: 180 |  Loss_1: (0.0179) | Acc_1: (99.43%) (23036/23168)\n",
      "Epoch: 80 | Batch_idx: 190 |  Loss_1: (0.0175) | Acc_1: (99.44%) (24312/24448)\n",
      "Epoch: 80 | Batch_idx: 200 |  Loss_1: (0.0174) | Acc_1: (99.45%) (25586/25728)\n",
      "Epoch: 80 | Batch_idx: 210 |  Loss_1: (0.0171) | Acc_1: (99.46%) (26861/27008)\n",
      "Epoch: 80 | Batch_idx: 220 |  Loss_1: (0.0171) | Acc_1: (99.44%) (28131/28288)\n",
      "Epoch: 80 | Batch_idx: 230 |  Loss_1: (0.0168) | Acc_1: (99.46%) (29408/29568)\n",
      "Epoch: 80 | Batch_idx: 240 |  Loss_1: (0.0164) | Acc_1: (99.47%) (30686/30848)\n",
      "Epoch: 80 | Batch_idx: 250 |  Loss_1: (0.0166) | Acc_1: (99.46%) (31956/32128)\n",
      "Epoch: 80 | Batch_idx: 260 |  Loss_1: (0.0166) | Acc_1: (99.46%) (33229/33408)\n",
      "Epoch: 80 | Batch_idx: 270 |  Loss_1: (0.0166) | Acc_1: (99.46%) (34500/34688)\n",
      "Epoch: 80 | Batch_idx: 280 |  Loss_1: (0.0169) | Acc_1: (99.44%) (35767/35968)\n",
      "Epoch: 80 | Batch_idx: 290 |  Loss_1: (0.0168) | Acc_1: (99.44%) (37040/37248)\n",
      "Epoch: 80 | Batch_idx: 300 |  Loss_1: (0.0167) | Acc_1: (99.45%) (38315/38528)\n",
      "Epoch: 80 | Batch_idx: 310 |  Loss_1: (0.0164) | Acc_1: (99.45%) (39591/39808)\n",
      "Epoch: 80 | Batch_idx: 320 |  Loss_1: (0.0165) | Acc_1: (99.45%) (40862/41088)\n",
      "Epoch: 80 | Batch_idx: 330 |  Loss_1: (0.0166) | Acc_1: (99.45%) (42133/42368)\n",
      "Epoch: 80 | Batch_idx: 340 |  Loss_1: (0.0166) | Acc_1: (99.44%) (43403/43648)\n",
      "Epoch: 80 | Batch_idx: 350 |  Loss_1: (0.0167) | Acc_1: (99.44%) (44675/44928)\n",
      "Epoch: 80 | Batch_idx: 360 |  Loss_1: (0.0167) | Acc_1: (99.43%) (45943/46208)\n",
      "Epoch: 80 | Batch_idx: 370 |  Loss_1: (0.0167) | Acc_1: (99.43%) (47215/47488)\n",
      "Epoch: 80 | Batch_idx: 380 |  Loss_1: (0.0167) | Acc_1: (99.43%) (48488/48768)\n",
      "Epoch: 80 | Batch_idx: 390 |  Loss_1: (0.0167) | Acc_1: (99.43%) (49713/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5279) | Acc: (90.86%) (9086/10000)\n",
      "Epoch: 81 | Batch_idx: 0 |  Loss_1: (0.0021) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 81 | Batch_idx: 10 |  Loss_1: (0.0130) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 81 | Batch_idx: 20 |  Loss_1: (0.0129) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 81 | Batch_idx: 30 |  Loss_1: (0.0159) | Acc_1: (99.40%) (3944/3968)\n",
      "Epoch: 81 | Batch_idx: 40 |  Loss_1: (0.0165) | Acc_1: (99.43%) (5218/5248)\n",
      "Epoch: 81 | Batch_idx: 50 |  Loss_1: (0.0164) | Acc_1: (99.43%) (6491/6528)\n",
      "Epoch: 81 | Batch_idx: 60 |  Loss_1: (0.0178) | Acc_1: (99.37%) (7759/7808)\n",
      "Epoch: 81 | Batch_idx: 70 |  Loss_1: (0.0186) | Acc_1: (99.33%) (9027/9088)\n",
      "Epoch: 81 | Batch_idx: 80 |  Loss_1: (0.0180) | Acc_1: (99.36%) (10302/10368)\n",
      "Epoch: 81 | Batch_idx: 90 |  Loss_1: (0.0177) | Acc_1: (99.38%) (11576/11648)\n",
      "Epoch: 81 | Batch_idx: 100 |  Loss_1: (0.0184) | Acc_1: (99.37%) (12847/12928)\n",
      "Epoch: 81 | Batch_idx: 110 |  Loss_1: (0.0181) | Acc_1: (99.38%) (14120/14208)\n",
      "Epoch: 81 | Batch_idx: 120 |  Loss_1: (0.0178) | Acc_1: (99.39%) (15393/15488)\n",
      "Epoch: 81 | Batch_idx: 130 |  Loss_1: (0.0172) | Acc_1: (99.41%) (16669/16768)\n",
      "Epoch: 81 | Batch_idx: 140 |  Loss_1: (0.0180) | Acc_1: (99.38%) (17937/18048)\n",
      "Epoch: 81 | Batch_idx: 150 |  Loss_1: (0.0176) | Acc_1: (99.39%) (19211/19328)\n",
      "Epoch: 81 | Batch_idx: 160 |  Loss_1: (0.0176) | Acc_1: (99.38%) (20481/20608)\n",
      "Epoch: 81 | Batch_idx: 170 |  Loss_1: (0.0171) | Acc_1: (99.41%) (21759/21888)\n",
      "Epoch: 81 | Batch_idx: 180 |  Loss_1: (0.0167) | Acc_1: (99.43%) (23035/23168)\n",
      "Epoch: 81 | Batch_idx: 190 |  Loss_1: (0.0167) | Acc_1: (99.42%) (24306/24448)\n",
      "Epoch: 81 | Batch_idx: 200 |  Loss_1: (0.0167) | Acc_1: (99.42%) (25579/25728)\n",
      "Epoch: 81 | Batch_idx: 210 |  Loss_1: (0.0166) | Acc_1: (99.43%) (26853/27008)\n",
      "Epoch: 81 | Batch_idx: 220 |  Loss_1: (0.0164) | Acc_1: (99.43%) (28128/28288)\n",
      "Epoch: 81 | Batch_idx: 230 |  Loss_1: (0.0162) | Acc_1: (99.44%) (29403/29568)\n",
      "Epoch: 81 | Batch_idx: 240 |  Loss_1: (0.0163) | Acc_1: (99.44%) (30675/30848)\n",
      "Epoch: 81 | Batch_idx: 250 |  Loss_1: (0.0161) | Acc_1: (99.45%) (31950/32128)\n",
      "Epoch: 81 | Batch_idx: 260 |  Loss_1: (0.0158) | Acc_1: (99.46%) (33226/33408)\n",
      "Epoch: 81 | Batch_idx: 270 |  Loss_1: (0.0156) | Acc_1: (99.46%) (34499/34688)\n",
      "Epoch: 81 | Batch_idx: 280 |  Loss_1: (0.0160) | Acc_1: (99.44%) (35768/35968)\n",
      "Epoch: 81 | Batch_idx: 290 |  Loss_1: (0.0160) | Acc_1: (99.44%) (37039/37248)\n",
      "Epoch: 81 | Batch_idx: 300 |  Loss_1: (0.0161) | Acc_1: (99.43%) (38309/38528)\n",
      "Epoch: 81 | Batch_idx: 310 |  Loss_1: (0.0160) | Acc_1: (99.43%) (39582/39808)\n",
      "Epoch: 81 | Batch_idx: 320 |  Loss_1: (0.0159) | Acc_1: (99.44%) (40857/41088)\n",
      "Epoch: 81 | Batch_idx: 330 |  Loss_1: (0.0160) | Acc_1: (99.44%) (42129/42368)\n",
      "Epoch: 81 | Batch_idx: 340 |  Loss_1: (0.0160) | Acc_1: (99.44%) (43402/43648)\n",
      "Epoch: 81 | Batch_idx: 350 |  Loss_1: (0.0160) | Acc_1: (99.43%) (44674/44928)\n",
      "Epoch: 81 | Batch_idx: 360 |  Loss_1: (0.0159) | Acc_1: (99.44%) (45950/46208)\n",
      "Epoch: 81 | Batch_idx: 370 |  Loss_1: (0.0158) | Acc_1: (99.44%) (47224/47488)\n",
      "Epoch: 81 | Batch_idx: 380 |  Loss_1: (0.0159) | Acc_1: (99.44%) (48495/48768)\n",
      "Epoch: 81 | Batch_idx: 390 |  Loss_1: (0.0160) | Acc_1: (99.44%) (49722/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5582) | Acc: (90.51%) (9051/10000)\n",
      "Epoch: 82 | Batch_idx: 0 |  Loss_1: (0.0100) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 82 | Batch_idx: 10 |  Loss_1: (0.0219) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 82 | Batch_idx: 20 |  Loss_1: (0.0160) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 82 | Batch_idx: 30 |  Loss_1: (0.0155) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 82 | Batch_idx: 40 |  Loss_1: (0.0149) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 82 | Batch_idx: 50 |  Loss_1: (0.0143) | Acc_1: (99.57%) (6500/6528)\n",
      "Epoch: 82 | Batch_idx: 60 |  Loss_1: (0.0132) | Acc_1: (99.62%) (7778/7808)\n",
      "Epoch: 82 | Batch_idx: 70 |  Loss_1: (0.0123) | Acc_1: (99.65%) (9056/9088)\n",
      "Epoch: 82 | Batch_idx: 80 |  Loss_1: (0.0131) | Acc_1: (99.62%) (10329/10368)\n",
      "Epoch: 82 | Batch_idx: 90 |  Loss_1: (0.0127) | Acc_1: (99.63%) (11605/11648)\n",
      "Epoch: 82 | Batch_idx: 100 |  Loss_1: (0.0124) | Acc_1: (99.63%) (12880/12928)\n",
      "Epoch: 82 | Batch_idx: 110 |  Loss_1: (0.0122) | Acc_1: (99.63%) (14156/14208)\n",
      "Epoch: 82 | Batch_idx: 120 |  Loss_1: (0.0126) | Acc_1: (99.63%) (15430/15488)\n",
      "Epoch: 82 | Batch_idx: 130 |  Loss_1: (0.0125) | Acc_1: (99.61%) (16702/16768)\n",
      "Epoch: 82 | Batch_idx: 140 |  Loss_1: (0.0132) | Acc_1: (99.58%) (17973/18048)\n",
      "Epoch: 82 | Batch_idx: 150 |  Loss_1: (0.0129) | Acc_1: (99.59%) (19248/19328)\n",
      "Epoch: 82 | Batch_idx: 160 |  Loss_1: (0.0130) | Acc_1: (99.58%) (20522/20608)\n",
      "Epoch: 82 | Batch_idx: 170 |  Loss_1: (0.0128) | Acc_1: (99.59%) (21799/21888)\n",
      "Epoch: 82 | Batch_idx: 180 |  Loss_1: (0.0129) | Acc_1: (99.59%) (23073/23168)\n",
      "Epoch: 82 | Batch_idx: 190 |  Loss_1: (0.0130) | Acc_1: (99.58%) (24345/24448)\n",
      "Epoch: 82 | Batch_idx: 200 |  Loss_1: (0.0129) | Acc_1: (99.58%) (25621/25728)\n",
      "Epoch: 82 | Batch_idx: 210 |  Loss_1: (0.0129) | Acc_1: (99.59%) (26896/27008)\n",
      "Epoch: 82 | Batch_idx: 220 |  Loss_1: (0.0127) | Acc_1: (99.59%) (28171/28288)\n",
      "Epoch: 82 | Batch_idx: 230 |  Loss_1: (0.0127) | Acc_1: (99.58%) (29445/29568)\n",
      "Epoch: 82 | Batch_idx: 240 |  Loss_1: (0.0128) | Acc_1: (99.58%) (30717/30848)\n",
      "Epoch: 82 | Batch_idx: 250 |  Loss_1: (0.0129) | Acc_1: (99.57%) (31989/32128)\n",
      "Epoch: 82 | Batch_idx: 260 |  Loss_1: (0.0129) | Acc_1: (99.57%) (33263/33408)\n",
      "Epoch: 82 | Batch_idx: 270 |  Loss_1: (0.0130) | Acc_1: (99.56%) (34536/34688)\n",
      "Epoch: 82 | Batch_idx: 280 |  Loss_1: (0.0129) | Acc_1: (99.57%) (35814/35968)\n",
      "Epoch: 82 | Batch_idx: 290 |  Loss_1: (0.0128) | Acc_1: (99.58%) (37090/37248)\n",
      "Epoch: 82 | Batch_idx: 300 |  Loss_1: (0.0128) | Acc_1: (99.58%) (38365/38528)\n",
      "Epoch: 82 | Batch_idx: 310 |  Loss_1: (0.0125) | Acc_1: (99.59%) (39643/39808)\n",
      "Epoch: 82 | Batch_idx: 320 |  Loss_1: (0.0130) | Acc_1: (99.56%) (40909/41088)\n",
      "Epoch: 82 | Batch_idx: 330 |  Loss_1: (0.0130) | Acc_1: (99.56%) (42183/42368)\n",
      "Epoch: 82 | Batch_idx: 340 |  Loss_1: (0.0133) | Acc_1: (99.56%) (43454/43648)\n",
      "Epoch: 82 | Batch_idx: 350 |  Loss_1: (0.0134) | Acc_1: (99.55%) (44728/44928)\n",
      "Epoch: 82 | Batch_idx: 360 |  Loss_1: (0.0137) | Acc_1: (99.55%) (46000/46208)\n",
      "Epoch: 82 | Batch_idx: 370 |  Loss_1: (0.0136) | Acc_1: (99.55%) (47275/47488)\n",
      "Epoch: 82 | Batch_idx: 380 |  Loss_1: (0.0140) | Acc_1: (99.53%) (48541/48768)\n",
      "Epoch: 82 | Batch_idx: 390 |  Loss_1: (0.0139) | Acc_1: (99.54%) (49770/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5087) | Acc: (91.37%) (9137/10000)\n",
      "Epoch: 83 | Batch_idx: 0 |  Loss_1: (0.0180) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 83 | Batch_idx: 10 |  Loss_1: (0.0079) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 83 | Batch_idx: 20 |  Loss_1: (0.0112) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 83 | Batch_idx: 30 |  Loss_1: (0.0100) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 83 | Batch_idx: 40 |  Loss_1: (0.0126) | Acc_1: (99.64%) (5229/5248)\n",
      "Epoch: 83 | Batch_idx: 50 |  Loss_1: (0.0126) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 83 | Batch_idx: 60 |  Loss_1: (0.0123) | Acc_1: (99.65%) (7781/7808)\n",
      "Epoch: 83 | Batch_idx: 70 |  Loss_1: (0.0128) | Acc_1: (99.64%) (9055/9088)\n",
      "Epoch: 83 | Batch_idx: 80 |  Loss_1: (0.0138) | Acc_1: (99.61%) (10328/10368)\n",
      "Epoch: 83 | Batch_idx: 90 |  Loss_1: (0.0144) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 83 | Batch_idx: 100 |  Loss_1: (0.0152) | Acc_1: (99.58%) (12874/12928)\n",
      "Epoch: 83 | Batch_idx: 110 |  Loss_1: (0.0153) | Acc_1: (99.58%) (14148/14208)\n",
      "Epoch: 83 | Batch_idx: 120 |  Loss_1: (0.0154) | Acc_1: (99.56%) (15420/15488)\n",
      "Epoch: 83 | Batch_idx: 130 |  Loss_1: (0.0156) | Acc_1: (99.55%) (16692/16768)\n",
      "Epoch: 83 | Batch_idx: 140 |  Loss_1: (0.0157) | Acc_1: (99.53%) (17963/18048)\n",
      "Epoch: 83 | Batch_idx: 150 |  Loss_1: (0.0157) | Acc_1: (99.52%) (19235/19328)\n",
      "Epoch: 83 | Batch_idx: 160 |  Loss_1: (0.0157) | Acc_1: (99.51%) (20506/20608)\n",
      "Epoch: 83 | Batch_idx: 170 |  Loss_1: (0.0156) | Acc_1: (99.49%) (21777/21888)\n",
      "Epoch: 83 | Batch_idx: 180 |  Loss_1: (0.0161) | Acc_1: (99.48%) (23047/23168)\n",
      "Epoch: 83 | Batch_idx: 190 |  Loss_1: (0.0159) | Acc_1: (99.48%) (24320/24448)\n",
      "Epoch: 83 | Batch_idx: 200 |  Loss_1: (0.0160) | Acc_1: (99.49%) (25596/25728)\n",
      "Epoch: 83 | Batch_idx: 210 |  Loss_1: (0.0163) | Acc_1: (99.48%) (26867/27008)\n",
      "Epoch: 83 | Batch_idx: 220 |  Loss_1: (0.0163) | Acc_1: (99.47%) (28138/28288)\n",
      "Epoch: 83 | Batch_idx: 230 |  Loss_1: (0.0160) | Acc_1: (99.47%) (29412/29568)\n",
      "Epoch: 83 | Batch_idx: 240 |  Loss_1: (0.0167) | Acc_1: (99.44%) (30675/30848)\n",
      "Epoch: 83 | Batch_idx: 250 |  Loss_1: (0.0170) | Acc_1: (99.43%) (31945/32128)\n",
      "Epoch: 83 | Batch_idx: 260 |  Loss_1: (0.0174) | Acc_1: (99.41%) (33210/33408)\n",
      "Epoch: 83 | Batch_idx: 270 |  Loss_1: (0.0177) | Acc_1: (99.39%) (34478/34688)\n",
      "Epoch: 83 | Batch_idx: 280 |  Loss_1: (0.0175) | Acc_1: (99.39%) (35750/35968)\n",
      "Epoch: 83 | Batch_idx: 290 |  Loss_1: (0.0176) | Acc_1: (99.39%) (37019/37248)\n",
      "Epoch: 83 | Batch_idx: 300 |  Loss_1: (0.0178) | Acc_1: (99.39%) (38292/38528)\n",
      "Epoch: 83 | Batch_idx: 310 |  Loss_1: (0.0178) | Acc_1: (99.39%) (39564/39808)\n",
      "Epoch: 83 | Batch_idx: 320 |  Loss_1: (0.0178) | Acc_1: (99.39%) (40838/41088)\n",
      "Epoch: 83 | Batch_idx: 330 |  Loss_1: (0.0176) | Acc_1: (99.40%) (42113/42368)\n",
      "Epoch: 83 | Batch_idx: 340 |  Loss_1: (0.0176) | Acc_1: (99.40%) (43385/43648)\n",
      "Epoch: 83 | Batch_idx: 350 |  Loss_1: (0.0177) | Acc_1: (99.40%) (44660/44928)\n",
      "Epoch: 83 | Batch_idx: 360 |  Loss_1: (0.0178) | Acc_1: (99.40%) (45931/46208)\n",
      "Epoch: 83 | Batch_idx: 370 |  Loss_1: (0.0179) | Acc_1: (99.40%) (47201/47488)\n",
      "Epoch: 83 | Batch_idx: 380 |  Loss_1: (0.0177) | Acc_1: (99.40%) (48476/48768)\n",
      "Epoch: 83 | Batch_idx: 390 |  Loss_1: (0.0176) | Acc_1: (99.40%) (49702/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5015) | Acc: (90.92%) (9092/10000)\n",
      "Epoch: 84 | Batch_idx: 0 |  Loss_1: (0.0323) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 84 | Batch_idx: 10 |  Loss_1: (0.0146) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 84 | Batch_idx: 20 |  Loss_1: (0.0163) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 84 | Batch_idx: 30 |  Loss_1: (0.0165) | Acc_1: (99.34%) (3942/3968)\n",
      "Epoch: 84 | Batch_idx: 40 |  Loss_1: (0.0169) | Acc_1: (99.35%) (5214/5248)\n",
      "Epoch: 84 | Batch_idx: 50 |  Loss_1: (0.0184) | Acc_1: (99.33%) (6484/6528)\n",
      "Epoch: 84 | Batch_idx: 60 |  Loss_1: (0.0177) | Acc_1: (99.37%) (7759/7808)\n",
      "Epoch: 84 | Batch_idx: 70 |  Loss_1: (0.0177) | Acc_1: (99.37%) (9031/9088)\n",
      "Epoch: 84 | Batch_idx: 80 |  Loss_1: (0.0175) | Acc_1: (99.37%) (10303/10368)\n",
      "Epoch: 84 | Batch_idx: 90 |  Loss_1: (0.0173) | Acc_1: (99.37%) (11575/11648)\n",
      "Epoch: 84 | Batch_idx: 100 |  Loss_1: (0.0175) | Acc_1: (99.36%) (12845/12928)\n",
      "Epoch: 84 | Batch_idx: 110 |  Loss_1: (0.0167) | Acc_1: (99.39%) (14121/14208)\n",
      "Epoch: 84 | Batch_idx: 120 |  Loss_1: (0.0162) | Acc_1: (99.41%) (15396/15488)\n",
      "Epoch: 84 | Batch_idx: 130 |  Loss_1: (0.0156) | Acc_1: (99.44%) (16674/16768)\n",
      "Epoch: 84 | Batch_idx: 140 |  Loss_1: (0.0155) | Acc_1: (99.44%) (17947/18048)\n",
      "Epoch: 84 | Batch_idx: 150 |  Loss_1: (0.0158) | Acc_1: (99.44%) (19220/19328)\n",
      "Epoch: 84 | Batch_idx: 160 |  Loss_1: (0.0159) | Acc_1: (99.43%) (20490/20608)\n",
      "Epoch: 84 | Batch_idx: 170 |  Loss_1: (0.0157) | Acc_1: (99.43%) (21763/21888)\n",
      "Epoch: 84 | Batch_idx: 180 |  Loss_1: (0.0159) | Acc_1: (99.42%) (23034/23168)\n",
      "Epoch: 84 | Batch_idx: 190 |  Loss_1: (0.0157) | Acc_1: (99.43%) (24308/24448)\n",
      "Epoch: 84 | Batch_idx: 200 |  Loss_1: (0.0159) | Acc_1: (99.42%) (25580/25728)\n",
      "Epoch: 84 | Batch_idx: 210 |  Loss_1: (0.0159) | Acc_1: (99.43%) (26853/27008)\n",
      "Epoch: 84 | Batch_idx: 220 |  Loss_1: (0.0163) | Acc_1: (99.42%) (28124/28288)\n",
      "Epoch: 84 | Batch_idx: 230 |  Loss_1: (0.0165) | Acc_1: (99.41%) (29395/29568)\n",
      "Epoch: 84 | Batch_idx: 240 |  Loss_1: (0.0163) | Acc_1: (99.42%) (30669/30848)\n",
      "Epoch: 84 | Batch_idx: 250 |  Loss_1: (0.0165) | Acc_1: (99.42%) (31941/32128)\n",
      "Epoch: 84 | Batch_idx: 260 |  Loss_1: (0.0165) | Acc_1: (99.42%) (33214/33408)\n",
      "Epoch: 84 | Batch_idx: 270 |  Loss_1: (0.0164) | Acc_1: (99.43%) (34489/34688)\n",
      "Epoch: 84 | Batch_idx: 280 |  Loss_1: (0.0166) | Acc_1: (99.41%) (35757/35968)\n",
      "Epoch: 84 | Batch_idx: 290 |  Loss_1: (0.0168) | Acc_1: (99.41%) (37027/37248)\n",
      "Epoch: 84 | Batch_idx: 300 |  Loss_1: (0.0167) | Acc_1: (99.42%) (38304/38528)\n",
      "Epoch: 84 | Batch_idx: 310 |  Loss_1: (0.0167) | Acc_1: (99.42%) (39576/39808)\n",
      "Epoch: 84 | Batch_idx: 320 |  Loss_1: (0.0166) | Acc_1: (99.42%) (40848/41088)\n",
      "Epoch: 84 | Batch_idx: 330 |  Loss_1: (0.0166) | Acc_1: (99.42%) (42122/42368)\n",
      "Epoch: 84 | Batch_idx: 340 |  Loss_1: (0.0166) | Acc_1: (99.42%) (43395/43648)\n",
      "Epoch: 84 | Batch_idx: 350 |  Loss_1: (0.0164) | Acc_1: (99.42%) (44667/44928)\n",
      "Epoch: 84 | Batch_idx: 360 |  Loss_1: (0.0163) | Acc_1: (99.43%) (45943/46208)\n",
      "Epoch: 84 | Batch_idx: 370 |  Loss_1: (0.0163) | Acc_1: (99.43%) (47216/47488)\n",
      "Epoch: 84 | Batch_idx: 380 |  Loss_1: (0.0162) | Acc_1: (99.43%) (48491/48768)\n",
      "Epoch: 84 | Batch_idx: 390 |  Loss_1: (0.0162) | Acc_1: (99.43%) (49716/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5205) | Acc: (91.04%) (9104/10000)\n",
      "Epoch: 85 | Batch_idx: 0 |  Loss_1: (0.0220) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 85 | Batch_idx: 10 |  Loss_1: (0.0131) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 85 | Batch_idx: 20 |  Loss_1: (0.0138) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 85 | Batch_idx: 30 |  Loss_1: (0.0145) | Acc_1: (99.45%) (3946/3968)\n",
      "Epoch: 85 | Batch_idx: 40 |  Loss_1: (0.0136) | Acc_1: (99.50%) (5222/5248)\n",
      "Epoch: 85 | Batch_idx: 50 |  Loss_1: (0.0150) | Acc_1: (99.42%) (6490/6528)\n",
      "Epoch: 85 | Batch_idx: 60 |  Loss_1: (0.0166) | Acc_1: (99.39%) (7760/7808)\n",
      "Epoch: 85 | Batch_idx: 70 |  Loss_1: (0.0176) | Acc_1: (99.39%) (9033/9088)\n",
      "Epoch: 85 | Batch_idx: 80 |  Loss_1: (0.0188) | Acc_1: (99.37%) (10303/10368)\n",
      "Epoch: 85 | Batch_idx: 90 |  Loss_1: (0.0192) | Acc_1: (99.34%) (11571/11648)\n",
      "Epoch: 85 | Batch_idx: 100 |  Loss_1: (0.0183) | Acc_1: (99.38%) (12848/12928)\n",
      "Epoch: 85 | Batch_idx: 110 |  Loss_1: (0.0183) | Acc_1: (99.39%) (14121/14208)\n",
      "Epoch: 85 | Batch_idx: 120 |  Loss_1: (0.0179) | Acc_1: (99.41%) (15396/15488)\n",
      "Epoch: 85 | Batch_idx: 130 |  Loss_1: (0.0183) | Acc_1: (99.40%) (16668/16768)\n",
      "Epoch: 85 | Batch_idx: 140 |  Loss_1: (0.0178) | Acc_1: (99.42%) (17943/18048)\n",
      "Epoch: 85 | Batch_idx: 150 |  Loss_1: (0.0178) | Acc_1: (99.42%) (19215/19328)\n",
      "Epoch: 85 | Batch_idx: 160 |  Loss_1: (0.0174) | Acc_1: (99.43%) (20491/20608)\n",
      "Epoch: 85 | Batch_idx: 170 |  Loss_1: (0.0179) | Acc_1: (99.42%) (21762/21888)\n",
      "Epoch: 85 | Batch_idx: 180 |  Loss_1: (0.0178) | Acc_1: (99.43%) (23036/23168)\n",
      "Epoch: 85 | Batch_idx: 190 |  Loss_1: (0.0173) | Acc_1: (99.45%) (24314/24448)\n",
      "Epoch: 85 | Batch_idx: 200 |  Loss_1: (0.0174) | Acc_1: (99.45%) (25586/25728)\n",
      "Epoch: 85 | Batch_idx: 210 |  Loss_1: (0.0176) | Acc_1: (99.45%) (26859/27008)\n",
      "Epoch: 85 | Batch_idx: 220 |  Loss_1: (0.0172) | Acc_1: (99.45%) (28132/28288)\n",
      "Epoch: 85 | Batch_idx: 230 |  Loss_1: (0.0172) | Acc_1: (99.46%) (29408/29568)\n",
      "Epoch: 85 | Batch_idx: 240 |  Loss_1: (0.0173) | Acc_1: (99.47%) (30683/30848)\n",
      "Epoch: 85 | Batch_idx: 250 |  Loss_1: (0.0174) | Acc_1: (99.46%) (31955/32128)\n",
      "Epoch: 85 | Batch_idx: 260 |  Loss_1: (0.0174) | Acc_1: (99.46%) (33228/33408)\n",
      "Epoch: 85 | Batch_idx: 270 |  Loss_1: (0.0175) | Acc_1: (99.45%) (34497/34688)\n",
      "Epoch: 85 | Batch_idx: 280 |  Loss_1: (0.0175) | Acc_1: (99.44%) (35768/35968)\n",
      "Epoch: 85 | Batch_idx: 290 |  Loss_1: (0.0175) | Acc_1: (99.45%) (37042/37248)\n",
      "Epoch: 85 | Batch_idx: 300 |  Loss_1: (0.0174) | Acc_1: (99.44%) (38314/38528)\n",
      "Epoch: 85 | Batch_idx: 310 |  Loss_1: (0.0175) | Acc_1: (99.44%) (39586/39808)\n",
      "Epoch: 85 | Batch_idx: 320 |  Loss_1: (0.0178) | Acc_1: (99.44%) (40856/41088)\n",
      "Epoch: 85 | Batch_idx: 330 |  Loss_1: (0.0177) | Acc_1: (99.44%) (42132/42368)\n",
      "Epoch: 85 | Batch_idx: 340 |  Loss_1: (0.0178) | Acc_1: (99.43%) (43401/43648)\n",
      "Epoch: 85 | Batch_idx: 350 |  Loss_1: (0.0179) | Acc_1: (99.43%) (44672/44928)\n",
      "Epoch: 85 | Batch_idx: 360 |  Loss_1: (0.0182) | Acc_1: (99.42%) (45938/46208)\n",
      "Epoch: 85 | Batch_idx: 370 |  Loss_1: (0.0180) | Acc_1: (99.42%) (47213/47488)\n",
      "Epoch: 85 | Batch_idx: 380 |  Loss_1: (0.0180) | Acc_1: (99.42%) (48485/48768)\n",
      "Epoch: 85 | Batch_idx: 390 |  Loss_1: (0.0180) | Acc_1: (99.42%) (49711/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5203) | Acc: (90.94%) (9094/10000)\n",
      "Epoch: 86 | Batch_idx: 0 |  Loss_1: (0.0031) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 86 | Batch_idx: 10 |  Loss_1: (0.0094) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 86 | Batch_idx: 20 |  Loss_1: (0.0078) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 86 | Batch_idx: 30 |  Loss_1: (0.0082) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 86 | Batch_idx: 40 |  Loss_1: (0.0087) | Acc_1: (99.79%) (5237/5248)\n",
      "Epoch: 86 | Batch_idx: 50 |  Loss_1: (0.0091) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 86 | Batch_idx: 60 |  Loss_1: (0.0102) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 86 | Batch_idx: 70 |  Loss_1: (0.0117) | Acc_1: (99.59%) (9051/9088)\n",
      "Epoch: 86 | Batch_idx: 80 |  Loss_1: (0.0117) | Acc_1: (99.59%) (10326/10368)\n",
      "Epoch: 86 | Batch_idx: 90 |  Loss_1: (0.0114) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 86 | Batch_idx: 100 |  Loss_1: (0.0117) | Acc_1: (99.58%) (12874/12928)\n",
      "Epoch: 86 | Batch_idx: 110 |  Loss_1: (0.0118) | Acc_1: (99.57%) (14147/14208)\n",
      "Epoch: 86 | Batch_idx: 120 |  Loss_1: (0.0121) | Acc_1: (99.56%) (15420/15488)\n",
      "Epoch: 86 | Batch_idx: 130 |  Loss_1: (0.0127) | Acc_1: (99.54%) (16691/16768)\n",
      "Epoch: 86 | Batch_idx: 140 |  Loss_1: (0.0129) | Acc_1: (99.54%) (17965/18048)\n",
      "Epoch: 86 | Batch_idx: 150 |  Loss_1: (0.0129) | Acc_1: (99.54%) (19240/19328)\n",
      "Epoch: 86 | Batch_idx: 160 |  Loss_1: (0.0130) | Acc_1: (99.54%) (20513/20608)\n",
      "Epoch: 86 | Batch_idx: 170 |  Loss_1: (0.0136) | Acc_1: (99.52%) (21784/21888)\n",
      "Epoch: 86 | Batch_idx: 180 |  Loss_1: (0.0134) | Acc_1: (99.53%) (23058/23168)\n",
      "Epoch: 86 | Batch_idx: 190 |  Loss_1: (0.0139) | Acc_1: (99.49%) (24324/24448)\n",
      "Epoch: 86 | Batch_idx: 200 |  Loss_1: (0.0143) | Acc_1: (99.48%) (25594/25728)\n",
      "Epoch: 86 | Batch_idx: 210 |  Loss_1: (0.0146) | Acc_1: (99.47%) (26866/27008)\n",
      "Epoch: 86 | Batch_idx: 220 |  Loss_1: (0.0145) | Acc_1: (99.48%) (28141/28288)\n",
      "Epoch: 86 | Batch_idx: 230 |  Loss_1: (0.0150) | Acc_1: (99.47%) (29410/29568)\n",
      "Epoch: 86 | Batch_idx: 240 |  Loss_1: (0.0158) | Acc_1: (99.45%) (30677/30848)\n",
      "Epoch: 86 | Batch_idx: 250 |  Loss_1: (0.0160) | Acc_1: (99.44%) (31948/32128)\n",
      "Epoch: 86 | Batch_idx: 260 |  Loss_1: (0.0162) | Acc_1: (99.44%) (33221/33408)\n",
      "Epoch: 86 | Batch_idx: 270 |  Loss_1: (0.0163) | Acc_1: (99.44%) (34493/34688)\n",
      "Epoch: 86 | Batch_idx: 280 |  Loss_1: (0.0161) | Acc_1: (99.45%) (35769/35968)\n",
      "Epoch: 86 | Batch_idx: 290 |  Loss_1: (0.0160) | Acc_1: (99.45%) (37043/37248)\n",
      "Epoch: 86 | Batch_idx: 300 |  Loss_1: (0.0162) | Acc_1: (99.45%) (38315/38528)\n",
      "Epoch: 86 | Batch_idx: 310 |  Loss_1: (0.0161) | Acc_1: (99.45%) (39590/39808)\n",
      "Epoch: 86 | Batch_idx: 320 |  Loss_1: (0.0162) | Acc_1: (99.45%) (40862/41088)\n",
      "Epoch: 86 | Batch_idx: 330 |  Loss_1: (0.0163) | Acc_1: (99.45%) (42136/42368)\n",
      "Epoch: 86 | Batch_idx: 340 |  Loss_1: (0.0160) | Acc_1: (99.46%) (43414/43648)\n",
      "Epoch: 86 | Batch_idx: 350 |  Loss_1: (0.0160) | Acc_1: (99.47%) (44688/44928)\n",
      "Epoch: 86 | Batch_idx: 360 |  Loss_1: (0.0159) | Acc_1: (99.47%) (45963/46208)\n",
      "Epoch: 86 | Batch_idx: 370 |  Loss_1: (0.0160) | Acc_1: (99.47%) (47238/47488)\n",
      "Epoch: 86 | Batch_idx: 380 |  Loss_1: (0.0159) | Acc_1: (99.47%) (48510/48768)\n",
      "Epoch: 86 | Batch_idx: 390 |  Loss_1: (0.0161) | Acc_1: (99.46%) (49731/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5162) | Acc: (91.23%) (9123/10000)\n",
      "Epoch: 87 | Batch_idx: 0 |  Loss_1: (0.0181) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 87 | Batch_idx: 10 |  Loss_1: (0.0145) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 87 | Batch_idx: 20 |  Loss_1: (0.0140) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 87 | Batch_idx: 30 |  Loss_1: (0.0147) | Acc_1: (99.45%) (3946/3968)\n",
      "Epoch: 87 | Batch_idx: 40 |  Loss_1: (0.0161) | Acc_1: (99.41%) (5217/5248)\n",
      "Epoch: 87 | Batch_idx: 50 |  Loss_1: (0.0177) | Acc_1: (99.37%) (6487/6528)\n",
      "Epoch: 87 | Batch_idx: 60 |  Loss_1: (0.0178) | Acc_1: (99.36%) (7758/7808)\n",
      "Epoch: 87 | Batch_idx: 70 |  Loss_1: (0.0177) | Acc_1: (99.36%) (9030/9088)\n",
      "Epoch: 87 | Batch_idx: 80 |  Loss_1: (0.0187) | Acc_1: (99.33%) (10299/10368)\n",
      "Epoch: 87 | Batch_idx: 90 |  Loss_1: (0.0183) | Acc_1: (99.36%) (11573/11648)\n",
      "Epoch: 87 | Batch_idx: 100 |  Loss_1: (0.0179) | Acc_1: (99.38%) (12848/12928)\n",
      "Epoch: 87 | Batch_idx: 110 |  Loss_1: (0.0178) | Acc_1: (99.38%) (14120/14208)\n",
      "Epoch: 87 | Batch_idx: 120 |  Loss_1: (0.0178) | Acc_1: (99.37%) (15391/15488)\n",
      "Epoch: 87 | Batch_idx: 130 |  Loss_1: (0.0175) | Acc_1: (99.39%) (16665/16768)\n",
      "Epoch: 87 | Batch_idx: 140 |  Loss_1: (0.0181) | Acc_1: (99.37%) (17935/18048)\n",
      "Epoch: 87 | Batch_idx: 150 |  Loss_1: (0.0187) | Acc_1: (99.35%) (19203/19328)\n",
      "Epoch: 87 | Batch_idx: 160 |  Loss_1: (0.0185) | Acc_1: (99.36%) (20476/20608)\n",
      "Epoch: 87 | Batch_idx: 170 |  Loss_1: (0.0188) | Acc_1: (99.35%) (21746/21888)\n",
      "Epoch: 87 | Batch_idx: 180 |  Loss_1: (0.0193) | Acc_1: (99.32%) (23011/23168)\n",
      "Epoch: 87 | Batch_idx: 190 |  Loss_1: (0.0194) | Acc_1: (99.30%) (24278/24448)\n",
      "Epoch: 87 | Batch_idx: 200 |  Loss_1: (0.0198) | Acc_1: (99.30%) (25548/25728)\n",
      "Epoch: 87 | Batch_idx: 210 |  Loss_1: (0.0197) | Acc_1: (99.30%) (26819/27008)\n",
      "Epoch: 87 | Batch_idx: 220 |  Loss_1: (0.0201) | Acc_1: (99.30%) (28089/28288)\n",
      "Epoch: 87 | Batch_idx: 230 |  Loss_1: (0.0201) | Acc_1: (99.29%) (29359/29568)\n",
      "Epoch: 87 | Batch_idx: 240 |  Loss_1: (0.0200) | Acc_1: (99.30%) (30632/30848)\n",
      "Epoch: 87 | Batch_idx: 250 |  Loss_1: (0.0203) | Acc_1: (99.28%) (31898/32128)\n",
      "Epoch: 87 | Batch_idx: 260 |  Loss_1: (0.0200) | Acc_1: (99.30%) (33173/33408)\n",
      "Epoch: 87 | Batch_idx: 270 |  Loss_1: (0.0198) | Acc_1: (99.31%) (34447/34688)\n",
      "Epoch: 87 | Batch_idx: 280 |  Loss_1: (0.0196) | Acc_1: (99.31%) (35719/35968)\n",
      "Epoch: 87 | Batch_idx: 290 |  Loss_1: (0.0197) | Acc_1: (99.30%) (36986/37248)\n",
      "Epoch: 87 | Batch_idx: 300 |  Loss_1: (0.0201) | Acc_1: (99.28%) (38252/38528)\n",
      "Epoch: 87 | Batch_idx: 310 |  Loss_1: (0.0201) | Acc_1: (99.27%) (39519/39808)\n",
      "Epoch: 87 | Batch_idx: 320 |  Loss_1: (0.0205) | Acc_1: (99.26%) (40784/41088)\n",
      "Epoch: 87 | Batch_idx: 330 |  Loss_1: (0.0206) | Acc_1: (99.24%) (42048/42368)\n",
      "Epoch: 87 | Batch_idx: 340 |  Loss_1: (0.0205) | Acc_1: (99.24%) (43318/43648)\n",
      "Epoch: 87 | Batch_idx: 350 |  Loss_1: (0.0202) | Acc_1: (99.26%) (44594/44928)\n",
      "Epoch: 87 | Batch_idx: 360 |  Loss_1: (0.0200) | Acc_1: (99.26%) (45868/46208)\n",
      "Epoch: 87 | Batch_idx: 370 |  Loss_1: (0.0199) | Acc_1: (99.26%) (47138/47488)\n",
      "Epoch: 87 | Batch_idx: 380 |  Loss_1: (0.0202) | Acc_1: (99.25%) (48403/48768)\n",
      "Epoch: 87 | Batch_idx: 390 |  Loss_1: (0.0202) | Acc_1: (99.26%) (49629/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5640) | Acc: (90.70%) (9070/10000)\n",
      "Epoch: 88 | Batch_idx: 0 |  Loss_1: (0.0098) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 88 | Batch_idx: 10 |  Loss_1: (0.0154) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 88 | Batch_idx: 20 |  Loss_1: (0.0152) | Acc_1: (99.52%) (2675/2688)\n",
      "Epoch: 88 | Batch_idx: 30 |  Loss_1: (0.0177) | Acc_1: (99.47%) (3947/3968)\n",
      "Epoch: 88 | Batch_idx: 40 |  Loss_1: (0.0163) | Acc_1: (99.47%) (5220/5248)\n",
      "Epoch: 88 | Batch_idx: 50 |  Loss_1: (0.0161) | Acc_1: (99.46%) (6493/6528)\n",
      "Epoch: 88 | Batch_idx: 60 |  Loss_1: (0.0160) | Acc_1: (99.47%) (7767/7808)\n",
      "Epoch: 88 | Batch_idx: 70 |  Loss_1: (0.0155) | Acc_1: (99.48%) (9041/9088)\n",
      "Epoch: 88 | Batch_idx: 80 |  Loss_1: (0.0163) | Acc_1: (99.46%) (10312/10368)\n",
      "Epoch: 88 | Batch_idx: 90 |  Loss_1: (0.0159) | Acc_1: (99.48%) (11587/11648)\n",
      "Epoch: 88 | Batch_idx: 100 |  Loss_1: (0.0167) | Acc_1: (99.46%) (12858/12928)\n",
      "Epoch: 88 | Batch_idx: 110 |  Loss_1: (0.0164) | Acc_1: (99.47%) (14132/14208)\n",
      "Epoch: 88 | Batch_idx: 120 |  Loss_1: (0.0158) | Acc_1: (99.48%) (15407/15488)\n",
      "Epoch: 88 | Batch_idx: 130 |  Loss_1: (0.0160) | Acc_1: (99.48%) (16680/16768)\n",
      "Epoch: 88 | Batch_idx: 140 |  Loss_1: (0.0165) | Acc_1: (99.46%) (17951/18048)\n",
      "Epoch: 88 | Batch_idx: 150 |  Loss_1: (0.0160) | Acc_1: (99.48%) (19227/19328)\n",
      "Epoch: 88 | Batch_idx: 160 |  Loss_1: (0.0161) | Acc_1: (99.47%) (20499/20608)\n",
      "Epoch: 88 | Batch_idx: 170 |  Loss_1: (0.0159) | Acc_1: (99.46%) (21770/21888)\n",
      "Epoch: 88 | Batch_idx: 180 |  Loss_1: (0.0156) | Acc_1: (99.47%) (23046/23168)\n",
      "Epoch: 88 | Batch_idx: 190 |  Loss_1: (0.0158) | Acc_1: (99.46%) (24315/24448)\n",
      "Epoch: 88 | Batch_idx: 200 |  Loss_1: (0.0157) | Acc_1: (99.46%) (25590/25728)\n",
      "Epoch: 88 | Batch_idx: 210 |  Loss_1: (0.0153) | Acc_1: (99.47%) (26865/27008)\n",
      "Epoch: 88 | Batch_idx: 220 |  Loss_1: (0.0152) | Acc_1: (99.48%) (28140/28288)\n",
      "Epoch: 88 | Batch_idx: 230 |  Loss_1: (0.0149) | Acc_1: (99.49%) (29416/29568)\n",
      "Epoch: 88 | Batch_idx: 240 |  Loss_1: (0.0147) | Acc_1: (99.49%) (30692/30848)\n",
      "Epoch: 88 | Batch_idx: 250 |  Loss_1: (0.0146) | Acc_1: (99.50%) (31968/32128)\n",
      "Epoch: 88 | Batch_idx: 260 |  Loss_1: (0.0143) | Acc_1: (99.51%) (33243/33408)\n",
      "Epoch: 88 | Batch_idx: 270 |  Loss_1: (0.0144) | Acc_1: (99.51%) (34519/34688)\n",
      "Epoch: 88 | Batch_idx: 280 |  Loss_1: (0.0147) | Acc_1: (99.51%) (35790/35968)\n",
      "Epoch: 88 | Batch_idx: 290 |  Loss_1: (0.0145) | Acc_1: (99.51%) (37067/37248)\n",
      "Epoch: 88 | Batch_idx: 300 |  Loss_1: (0.0144) | Acc_1: (99.52%) (38342/38528)\n",
      "Epoch: 88 | Batch_idx: 310 |  Loss_1: (0.0142) | Acc_1: (99.53%) (39619/39808)\n",
      "Epoch: 88 | Batch_idx: 320 |  Loss_1: (0.0140) | Acc_1: (99.53%) (40896/41088)\n",
      "Epoch: 88 | Batch_idx: 330 |  Loss_1: (0.0139) | Acc_1: (99.54%) (42172/42368)\n",
      "Epoch: 88 | Batch_idx: 340 |  Loss_1: (0.0144) | Acc_1: (99.53%) (43441/43648)\n",
      "Epoch: 88 | Batch_idx: 350 |  Loss_1: (0.0146) | Acc_1: (99.52%) (44711/44928)\n",
      "Epoch: 88 | Batch_idx: 360 |  Loss_1: (0.0146) | Acc_1: (99.52%) (45984/46208)\n",
      "Epoch: 88 | Batch_idx: 370 |  Loss_1: (0.0146) | Acc_1: (99.51%) (47256/47488)\n",
      "Epoch: 88 | Batch_idx: 380 |  Loss_1: (0.0148) | Acc_1: (99.50%) (48526/48768)\n",
      "Epoch: 88 | Batch_idx: 390 |  Loss_1: (0.0148) | Acc_1: (99.50%) (49752/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5031) | Acc: (91.00%) (9100/10000)\n",
      "Epoch: 89 | Batch_idx: 0 |  Loss_1: (0.0020) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 89 | Batch_idx: 10 |  Loss_1: (0.0165) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 89 | Batch_idx: 20 |  Loss_1: (0.0179) | Acc_1: (99.26%) (2668/2688)\n",
      "Epoch: 89 | Batch_idx: 30 |  Loss_1: (0.0169) | Acc_1: (99.32%) (3941/3968)\n",
      "Epoch: 89 | Batch_idx: 40 |  Loss_1: (0.0157) | Acc_1: (99.43%) (5218/5248)\n",
      "Epoch: 89 | Batch_idx: 50 |  Loss_1: (0.0168) | Acc_1: (99.45%) (6492/6528)\n",
      "Epoch: 89 | Batch_idx: 60 |  Loss_1: (0.0156) | Acc_1: (99.50%) (7769/7808)\n",
      "Epoch: 89 | Batch_idx: 70 |  Loss_1: (0.0162) | Acc_1: (99.46%) (9039/9088)\n",
      "Epoch: 89 | Batch_idx: 80 |  Loss_1: (0.0157) | Acc_1: (99.47%) (10313/10368)\n",
      "Epoch: 89 | Batch_idx: 90 |  Loss_1: (0.0159) | Acc_1: (99.46%) (11585/11648)\n",
      "Epoch: 89 | Batch_idx: 100 |  Loss_1: (0.0153) | Acc_1: (99.48%) (12861/12928)\n",
      "Epoch: 89 | Batch_idx: 110 |  Loss_1: (0.0155) | Acc_1: (99.47%) (14132/14208)\n",
      "Epoch: 89 | Batch_idx: 120 |  Loss_1: (0.0154) | Acc_1: (99.48%) (15407/15488)\n",
      "Epoch: 89 | Batch_idx: 130 |  Loss_1: (0.0155) | Acc_1: (99.48%) (16680/16768)\n",
      "Epoch: 89 | Batch_idx: 140 |  Loss_1: (0.0158) | Acc_1: (99.47%) (17953/18048)\n",
      "Epoch: 89 | Batch_idx: 150 |  Loss_1: (0.0160) | Acc_1: (99.47%) (19225/19328)\n",
      "Epoch: 89 | Batch_idx: 160 |  Loss_1: (0.0164) | Acc_1: (99.45%) (20495/20608)\n",
      "Epoch: 89 | Batch_idx: 170 |  Loss_1: (0.0163) | Acc_1: (99.45%) (21767/21888)\n",
      "Epoch: 89 | Batch_idx: 180 |  Loss_1: (0.0162) | Acc_1: (99.46%) (23042/23168)\n",
      "Epoch: 89 | Batch_idx: 190 |  Loss_1: (0.0161) | Acc_1: (99.45%) (24314/24448)\n",
      "Epoch: 89 | Batch_idx: 200 |  Loss_1: (0.0160) | Acc_1: (99.45%) (25587/25728)\n",
      "Epoch: 89 | Batch_idx: 210 |  Loss_1: (0.0162) | Acc_1: (99.45%) (26859/27008)\n",
      "Epoch: 89 | Batch_idx: 220 |  Loss_1: (0.0160) | Acc_1: (99.44%) (28131/28288)\n",
      "Epoch: 89 | Batch_idx: 230 |  Loss_1: (0.0162) | Acc_1: (99.44%) (29401/29568)\n",
      "Epoch: 89 | Batch_idx: 240 |  Loss_1: (0.0165) | Acc_1: (99.42%) (30670/30848)\n",
      "Epoch: 89 | Batch_idx: 250 |  Loss_1: (0.0168) | Acc_1: (99.42%) (31941/32128)\n",
      "Epoch: 89 | Batch_idx: 260 |  Loss_1: (0.0166) | Acc_1: (99.43%) (33216/33408)\n",
      "Epoch: 89 | Batch_idx: 270 |  Loss_1: (0.0168) | Acc_1: (99.41%) (34483/34688)\n",
      "Epoch: 89 | Batch_idx: 280 |  Loss_1: (0.0169) | Acc_1: (99.41%) (35755/35968)\n",
      "Epoch: 89 | Batch_idx: 290 |  Loss_1: (0.0169) | Acc_1: (99.41%) (37028/37248)\n",
      "Epoch: 89 | Batch_idx: 300 |  Loss_1: (0.0171) | Acc_1: (99.41%) (38299/38528)\n",
      "Epoch: 89 | Batch_idx: 310 |  Loss_1: (0.0169) | Acc_1: (99.41%) (39575/39808)\n",
      "Epoch: 89 | Batch_idx: 320 |  Loss_1: (0.0168) | Acc_1: (99.42%) (40848/41088)\n",
      "Epoch: 89 | Batch_idx: 330 |  Loss_1: (0.0167) | Acc_1: (99.42%) (42123/42368)\n",
      "Epoch: 89 | Batch_idx: 340 |  Loss_1: (0.0171) | Acc_1: (99.40%) (43388/43648)\n",
      "Epoch: 89 | Batch_idx: 350 |  Loss_1: (0.0171) | Acc_1: (99.40%) (44660/44928)\n",
      "Epoch: 89 | Batch_idx: 360 |  Loss_1: (0.0171) | Acc_1: (99.40%) (45931/46208)\n",
      "Epoch: 89 | Batch_idx: 370 |  Loss_1: (0.0171) | Acc_1: (99.40%) (47203/47488)\n",
      "Epoch: 89 | Batch_idx: 380 |  Loss_1: (0.0173) | Acc_1: (99.39%) (48471/48768)\n",
      "Epoch: 89 | Batch_idx: 390 |  Loss_1: (0.0173) | Acc_1: (99.40%) (49698/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5291) | Acc: (90.97%) (9097/10000)\n",
      "Epoch: 90 | Batch_idx: 0 |  Loss_1: (0.0073) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 90 | Batch_idx: 10 |  Loss_1: (0.0185) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 90 | Batch_idx: 20 |  Loss_1: (0.0149) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 90 | Batch_idx: 30 |  Loss_1: (0.0150) | Acc_1: (99.45%) (3946/3968)\n",
      "Epoch: 90 | Batch_idx: 40 |  Loss_1: (0.0145) | Acc_1: (99.45%) (5219/5248)\n",
      "Epoch: 90 | Batch_idx: 50 |  Loss_1: (0.0139) | Acc_1: (99.46%) (6493/6528)\n",
      "Epoch: 90 | Batch_idx: 60 |  Loss_1: (0.0145) | Acc_1: (99.45%) (7765/7808)\n",
      "Epoch: 90 | Batch_idx: 70 |  Loss_1: (0.0147) | Acc_1: (99.46%) (9039/9088)\n",
      "Epoch: 90 | Batch_idx: 80 |  Loss_1: (0.0155) | Acc_1: (99.44%) (10310/10368)\n",
      "Epoch: 90 | Batch_idx: 90 |  Loss_1: (0.0152) | Acc_1: (99.42%) (11581/11648)\n",
      "Epoch: 90 | Batch_idx: 100 |  Loss_1: (0.0153) | Acc_1: (99.44%) (12855/12928)\n",
      "Epoch: 90 | Batch_idx: 110 |  Loss_1: (0.0147) | Acc_1: (99.45%) (14130/14208)\n",
      "Epoch: 90 | Batch_idx: 120 |  Loss_1: (0.0146) | Acc_1: (99.45%) (15403/15488)\n",
      "Epoch: 90 | Batch_idx: 130 |  Loss_1: (0.0149) | Acc_1: (99.43%) (16673/16768)\n",
      "Epoch: 90 | Batch_idx: 140 |  Loss_1: (0.0148) | Acc_1: (99.45%) (17948/18048)\n",
      "Epoch: 90 | Batch_idx: 150 |  Loss_1: (0.0156) | Acc_1: (99.42%) (19216/19328)\n",
      "Epoch: 90 | Batch_idx: 160 |  Loss_1: (0.0163) | Acc_1: (99.40%) (20484/20608)\n",
      "Epoch: 90 | Batch_idx: 170 |  Loss_1: (0.0161) | Acc_1: (99.41%) (21758/21888)\n",
      "Epoch: 90 | Batch_idx: 180 |  Loss_1: (0.0160) | Acc_1: (99.42%) (23033/23168)\n",
      "Epoch: 90 | Batch_idx: 190 |  Loss_1: (0.0161) | Acc_1: (99.43%) (24308/24448)\n",
      "Epoch: 90 | Batch_idx: 200 |  Loss_1: (0.0160) | Acc_1: (99.43%) (25582/25728)\n",
      "Epoch: 90 | Batch_idx: 210 |  Loss_1: (0.0157) | Acc_1: (99.44%) (26858/27008)\n",
      "Epoch: 90 | Batch_idx: 220 |  Loss_1: (0.0156) | Acc_1: (99.46%) (28134/28288)\n",
      "Epoch: 90 | Batch_idx: 230 |  Loss_1: (0.0152) | Acc_1: (99.48%) (29413/29568)\n",
      "Epoch: 90 | Batch_idx: 240 |  Loss_1: (0.0152) | Acc_1: (99.47%) (30686/30848)\n",
      "Epoch: 90 | Batch_idx: 250 |  Loss_1: (0.0152) | Acc_1: (99.48%) (31961/32128)\n",
      "Epoch: 90 | Batch_idx: 260 |  Loss_1: (0.0154) | Acc_1: (99.49%) (33236/33408)\n",
      "Epoch: 90 | Batch_idx: 270 |  Loss_1: (0.0154) | Acc_1: (99.48%) (34509/34688)\n",
      "Epoch: 90 | Batch_idx: 280 |  Loss_1: (0.0152) | Acc_1: (99.49%) (35785/35968)\n",
      "Epoch: 90 | Batch_idx: 290 |  Loss_1: (0.0153) | Acc_1: (99.49%) (37058/37248)\n",
      "Epoch: 90 | Batch_idx: 300 |  Loss_1: (0.0153) | Acc_1: (99.49%) (38331/38528)\n",
      "Epoch: 90 | Batch_idx: 310 |  Loss_1: (0.0151) | Acc_1: (99.50%) (39607/39808)\n",
      "Epoch: 90 | Batch_idx: 320 |  Loss_1: (0.0149) | Acc_1: (99.50%) (40883/41088)\n",
      "Epoch: 90 | Batch_idx: 330 |  Loss_1: (0.0148) | Acc_1: (99.50%) (42158/42368)\n",
      "Epoch: 90 | Batch_idx: 340 |  Loss_1: (0.0145) | Acc_1: (99.51%) (43436/43648)\n",
      "Epoch: 90 | Batch_idx: 350 |  Loss_1: (0.0146) | Acc_1: (99.51%) (44710/44928)\n",
      "Epoch: 90 | Batch_idx: 360 |  Loss_1: (0.0145) | Acc_1: (99.51%) (45983/46208)\n",
      "Epoch: 90 | Batch_idx: 370 |  Loss_1: (0.0146) | Acc_1: (99.51%) (47255/47488)\n",
      "Epoch: 90 | Batch_idx: 380 |  Loss_1: (0.0146) | Acc_1: (99.51%) (48529/48768)\n",
      "Epoch: 90 | Batch_idx: 390 |  Loss_1: (0.0148) | Acc_1: (99.51%) (49753/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5015) | Acc: (91.26%) (9126/10000)\n",
      "Epoch: 91 | Batch_idx: 0 |  Loss_1: (0.0124) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 91 | Batch_idx: 10 |  Loss_1: (0.0125) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 91 | Batch_idx: 20 |  Loss_1: (0.0119) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 91 | Batch_idx: 30 |  Loss_1: (0.0128) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 91 | Batch_idx: 40 |  Loss_1: (0.0123) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 91 | Batch_idx: 50 |  Loss_1: (0.0118) | Acc_1: (99.60%) (6502/6528)\n",
      "Epoch: 91 | Batch_idx: 60 |  Loss_1: (0.0126) | Acc_1: (99.59%) (7776/7808)\n",
      "Epoch: 91 | Batch_idx: 70 |  Loss_1: (0.0122) | Acc_1: (99.61%) (9053/9088)\n",
      "Epoch: 91 | Batch_idx: 80 |  Loss_1: (0.0127) | Acc_1: (99.59%) (10326/10368)\n",
      "Epoch: 91 | Batch_idx: 90 |  Loss_1: (0.0125) | Acc_1: (99.59%) (11600/11648)\n",
      "Epoch: 91 | Batch_idx: 100 |  Loss_1: (0.0123) | Acc_1: (99.58%) (12874/12928)\n",
      "Epoch: 91 | Batch_idx: 110 |  Loss_1: (0.0124) | Acc_1: (99.57%) (14147/14208)\n",
      "Epoch: 91 | Batch_idx: 120 |  Loss_1: (0.0119) | Acc_1: (99.59%) (15425/15488)\n",
      "Epoch: 91 | Batch_idx: 130 |  Loss_1: (0.0116) | Acc_1: (99.60%) (16701/16768)\n",
      "Epoch: 91 | Batch_idx: 140 |  Loss_1: (0.0114) | Acc_1: (99.62%) (17979/18048)\n",
      "Epoch: 91 | Batch_idx: 150 |  Loss_1: (0.0112) | Acc_1: (99.63%) (19256/19328)\n",
      "Epoch: 91 | Batch_idx: 160 |  Loss_1: (0.0118) | Acc_1: (99.60%) (20525/20608)\n",
      "Epoch: 91 | Batch_idx: 170 |  Loss_1: (0.0117) | Acc_1: (99.60%) (21801/21888)\n",
      "Epoch: 91 | Batch_idx: 180 |  Loss_1: (0.0116) | Acc_1: (99.60%) (23076/23168)\n",
      "Epoch: 91 | Batch_idx: 190 |  Loss_1: (0.0115) | Acc_1: (99.60%) (24349/24448)\n",
      "Epoch: 91 | Batch_idx: 200 |  Loss_1: (0.0114) | Acc_1: (99.61%) (25627/25728)\n",
      "Epoch: 91 | Batch_idx: 210 |  Loss_1: (0.0114) | Acc_1: (99.61%) (26902/27008)\n",
      "Epoch: 91 | Batch_idx: 220 |  Loss_1: (0.0114) | Acc_1: (99.61%) (28179/28288)\n",
      "Epoch: 91 | Batch_idx: 230 |  Loss_1: (0.0115) | Acc_1: (99.61%) (29453/29568)\n",
      "Epoch: 91 | Batch_idx: 240 |  Loss_1: (0.0114) | Acc_1: (99.61%) (30729/30848)\n",
      "Epoch: 91 | Batch_idx: 250 |  Loss_1: (0.0112) | Acc_1: (99.62%) (32007/32128)\n",
      "Epoch: 91 | Batch_idx: 260 |  Loss_1: (0.0111) | Acc_1: (99.63%) (33284/33408)\n",
      "Epoch: 91 | Batch_idx: 270 |  Loss_1: (0.0111) | Acc_1: (99.63%) (34559/34688)\n",
      "Epoch: 91 | Batch_idx: 280 |  Loss_1: (0.0111) | Acc_1: (99.63%) (35836/35968)\n",
      "Epoch: 91 | Batch_idx: 290 |  Loss_1: (0.0113) | Acc_1: (99.62%) (37108/37248)\n",
      "Epoch: 91 | Batch_idx: 300 |  Loss_1: (0.0111) | Acc_1: (99.63%) (38387/38528)\n",
      "Epoch: 91 | Batch_idx: 310 |  Loss_1: (0.0109) | Acc_1: (99.64%) (39664/39808)\n",
      "Epoch: 91 | Batch_idx: 320 |  Loss_1: (0.0109) | Acc_1: (99.63%) (40938/41088)\n",
      "Epoch: 91 | Batch_idx: 330 |  Loss_1: (0.0112) | Acc_1: (99.62%) (42209/42368)\n",
      "Epoch: 91 | Batch_idx: 340 |  Loss_1: (0.0113) | Acc_1: (99.62%) (43484/43648)\n",
      "Epoch: 91 | Batch_idx: 350 |  Loss_1: (0.0114) | Acc_1: (99.62%) (44757/44928)\n",
      "Epoch: 91 | Batch_idx: 360 |  Loss_1: (0.0114) | Acc_1: (99.62%) (46033/46208)\n",
      "Epoch: 91 | Batch_idx: 370 |  Loss_1: (0.0113) | Acc_1: (99.62%) (47309/47488)\n",
      "Epoch: 91 | Batch_idx: 380 |  Loss_1: (0.0112) | Acc_1: (99.63%) (48587/48768)\n",
      "Epoch: 91 | Batch_idx: 390 |  Loss_1: (0.0113) | Acc_1: (99.63%) (49817/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4875) | Acc: (91.61%) (9161/10000)\n",
      "Epoch: 92 | Batch_idx: 0 |  Loss_1: (0.0055) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 92 | Batch_idx: 10 |  Loss_1: (0.0111) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 92 | Batch_idx: 20 |  Loss_1: (0.0114) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 92 | Batch_idx: 30 |  Loss_1: (0.0139) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 92 | Batch_idx: 40 |  Loss_1: (0.0117) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 92 | Batch_idx: 50 |  Loss_1: (0.0126) | Acc_1: (99.60%) (6502/6528)\n",
      "Epoch: 92 | Batch_idx: 60 |  Loss_1: (0.0135) | Acc_1: (99.55%) (7773/7808)\n",
      "Epoch: 92 | Batch_idx: 70 |  Loss_1: (0.0134) | Acc_1: (99.56%) (9048/9088)\n",
      "Epoch: 92 | Batch_idx: 80 |  Loss_1: (0.0143) | Acc_1: (99.57%) (10323/10368)\n",
      "Epoch: 92 | Batch_idx: 90 |  Loss_1: (0.0148) | Acc_1: (99.54%) (11594/11648)\n",
      "Epoch: 92 | Batch_idx: 100 |  Loss_1: (0.0149) | Acc_1: (99.54%) (12868/12928)\n",
      "Epoch: 92 | Batch_idx: 110 |  Loss_1: (0.0149) | Acc_1: (99.54%) (14143/14208)\n",
      "Epoch: 92 | Batch_idx: 120 |  Loss_1: (0.0144) | Acc_1: (99.55%) (15418/15488)\n",
      "Epoch: 92 | Batch_idx: 130 |  Loss_1: (0.0141) | Acc_1: (99.55%) (16693/16768)\n",
      "Epoch: 92 | Batch_idx: 140 |  Loss_1: (0.0138) | Acc_1: (99.56%) (17969/18048)\n",
      "Epoch: 92 | Batch_idx: 150 |  Loss_1: (0.0139) | Acc_1: (99.56%) (19243/19328)\n",
      "Epoch: 92 | Batch_idx: 160 |  Loss_1: (0.0135) | Acc_1: (99.58%) (20522/20608)\n",
      "Epoch: 92 | Batch_idx: 170 |  Loss_1: (0.0135) | Acc_1: (99.58%) (21797/21888)\n",
      "Epoch: 92 | Batch_idx: 180 |  Loss_1: (0.0133) | Acc_1: (99.59%) (23072/23168)\n",
      "Epoch: 92 | Batch_idx: 190 |  Loss_1: (0.0132) | Acc_1: (99.58%) (24346/24448)\n",
      "Epoch: 92 | Batch_idx: 200 |  Loss_1: (0.0132) | Acc_1: (99.58%) (25619/25728)\n",
      "Epoch: 92 | Batch_idx: 210 |  Loss_1: (0.0130) | Acc_1: (99.57%) (26893/27008)\n",
      "Epoch: 92 | Batch_idx: 220 |  Loss_1: (0.0126) | Acc_1: (99.59%) (28172/28288)\n",
      "Epoch: 92 | Batch_idx: 230 |  Loss_1: (0.0123) | Acc_1: (99.59%) (29447/29568)\n",
      "Epoch: 92 | Batch_idx: 240 |  Loss_1: (0.0124) | Acc_1: (99.59%) (30720/30848)\n",
      "Epoch: 92 | Batch_idx: 250 |  Loss_1: (0.0122) | Acc_1: (99.59%) (31996/32128)\n",
      "Epoch: 92 | Batch_idx: 260 |  Loss_1: (0.0123) | Acc_1: (99.59%) (33271/33408)\n",
      "Epoch: 92 | Batch_idx: 270 |  Loss_1: (0.0122) | Acc_1: (99.60%) (34548/34688)\n",
      "Epoch: 92 | Batch_idx: 280 |  Loss_1: (0.0121) | Acc_1: (99.60%) (35824/35968)\n",
      "Epoch: 92 | Batch_idx: 290 |  Loss_1: (0.0121) | Acc_1: (99.61%) (37102/37248)\n",
      "Epoch: 92 | Batch_idx: 300 |  Loss_1: (0.0121) | Acc_1: (99.61%) (38377/38528)\n",
      "Epoch: 92 | Batch_idx: 310 |  Loss_1: (0.0125) | Acc_1: (99.60%) (39650/39808)\n",
      "Epoch: 92 | Batch_idx: 320 |  Loss_1: (0.0124) | Acc_1: (99.61%) (40927/41088)\n",
      "Epoch: 92 | Batch_idx: 330 |  Loss_1: (0.0123) | Acc_1: (99.61%) (42203/42368)\n",
      "Epoch: 92 | Batch_idx: 340 |  Loss_1: (0.0123) | Acc_1: (99.61%) (43479/43648)\n",
      "Epoch: 92 | Batch_idx: 350 |  Loss_1: (0.0122) | Acc_1: (99.62%) (44757/44928)\n",
      "Epoch: 92 | Batch_idx: 360 |  Loss_1: (0.0121) | Acc_1: (99.62%) (46033/46208)\n",
      "Epoch: 92 | Batch_idx: 370 |  Loss_1: (0.0120) | Acc_1: (99.62%) (47309/47488)\n",
      "Epoch: 92 | Batch_idx: 380 |  Loss_1: (0.0120) | Acc_1: (99.62%) (48582/48768)\n",
      "Epoch: 92 | Batch_idx: 390 |  Loss_1: (0.0120) | Acc_1: (99.62%) (49811/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5058) | Acc: (91.45%) (9145/10000)\n",
      "Epoch: 93 | Batch_idx: 0 |  Loss_1: (0.0026) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 93 | Batch_idx: 10 |  Loss_1: (0.0070) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 93 | Batch_idx: 20 |  Loss_1: (0.0113) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 93 | Batch_idx: 30 |  Loss_1: (0.0119) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 93 | Batch_idx: 40 |  Loss_1: (0.0130) | Acc_1: (99.56%) (5225/5248)\n",
      "Epoch: 93 | Batch_idx: 50 |  Loss_1: (0.0128) | Acc_1: (99.54%) (6498/6528)\n",
      "Epoch: 93 | Batch_idx: 60 |  Loss_1: (0.0125) | Acc_1: (99.56%) (7774/7808)\n",
      "Epoch: 93 | Batch_idx: 70 |  Loss_1: (0.0121) | Acc_1: (99.57%) (9049/9088)\n",
      "Epoch: 93 | Batch_idx: 80 |  Loss_1: (0.0114) | Acc_1: (99.60%) (10327/10368)\n",
      "Epoch: 93 | Batch_idx: 90 |  Loss_1: (0.0110) | Acc_1: (99.62%) (11604/11648)\n",
      "Epoch: 93 | Batch_idx: 100 |  Loss_1: (0.0107) | Acc_1: (99.63%) (12880/12928)\n",
      "Epoch: 93 | Batch_idx: 110 |  Loss_1: (0.0116) | Acc_1: (99.60%) (14151/14208)\n",
      "Epoch: 93 | Batch_idx: 120 |  Loss_1: (0.0110) | Acc_1: (99.63%) (15431/15488)\n",
      "Epoch: 93 | Batch_idx: 130 |  Loss_1: (0.0113) | Acc_1: (99.62%) (16704/16768)\n",
      "Epoch: 93 | Batch_idx: 140 |  Loss_1: (0.0109) | Acc_1: (99.63%) (17981/18048)\n",
      "Epoch: 93 | Batch_idx: 150 |  Loss_1: (0.0107) | Acc_1: (99.64%) (19258/19328)\n",
      "Epoch: 93 | Batch_idx: 160 |  Loss_1: (0.0106) | Acc_1: (99.65%) (20535/20608)\n",
      "Epoch: 93 | Batch_idx: 170 |  Loss_1: (0.0104) | Acc_1: (99.65%) (21812/21888)\n",
      "Epoch: 93 | Batch_idx: 180 |  Loss_1: (0.0105) | Acc_1: (99.65%) (23088/23168)\n",
      "Epoch: 93 | Batch_idx: 190 |  Loss_1: (0.0105) | Acc_1: (99.66%) (24364/24448)\n",
      "Epoch: 93 | Batch_idx: 200 |  Loss_1: (0.0103) | Acc_1: (99.66%) (25641/25728)\n",
      "Epoch: 93 | Batch_idx: 210 |  Loss_1: (0.0103) | Acc_1: (99.66%) (26916/27008)\n",
      "Epoch: 93 | Batch_idx: 220 |  Loss_1: (0.0102) | Acc_1: (99.66%) (28192/28288)\n",
      "Epoch: 93 | Batch_idx: 230 |  Loss_1: (0.0101) | Acc_1: (99.66%) (29468/29568)\n",
      "Epoch: 93 | Batch_idx: 240 |  Loss_1: (0.0103) | Acc_1: (99.65%) (30741/30848)\n",
      "Epoch: 93 | Batch_idx: 250 |  Loss_1: (0.0106) | Acc_1: (99.65%) (32016/32128)\n",
      "Epoch: 93 | Batch_idx: 260 |  Loss_1: (0.0108) | Acc_1: (99.64%) (33287/33408)\n",
      "Epoch: 93 | Batch_idx: 270 |  Loss_1: (0.0111) | Acc_1: (99.63%) (34560/34688)\n",
      "Epoch: 93 | Batch_idx: 280 |  Loss_1: (0.0111) | Acc_1: (99.63%) (35835/35968)\n",
      "Epoch: 93 | Batch_idx: 290 |  Loss_1: (0.0110) | Acc_1: (99.63%) (37110/37248)\n",
      "Epoch: 93 | Batch_idx: 300 |  Loss_1: (0.0111) | Acc_1: (99.63%) (38385/38528)\n",
      "Epoch: 93 | Batch_idx: 310 |  Loss_1: (0.0115) | Acc_1: (99.61%) (39654/39808)\n",
      "Epoch: 93 | Batch_idx: 320 |  Loss_1: (0.0114) | Acc_1: (99.61%) (40929/41088)\n",
      "Epoch: 93 | Batch_idx: 330 |  Loss_1: (0.0113) | Acc_1: (99.62%) (42207/42368)\n",
      "Epoch: 93 | Batch_idx: 340 |  Loss_1: (0.0112) | Acc_1: (99.62%) (43483/43648)\n",
      "Epoch: 93 | Batch_idx: 350 |  Loss_1: (0.0112) | Acc_1: (99.63%) (44760/44928)\n",
      "Epoch: 93 | Batch_idx: 360 |  Loss_1: (0.0114) | Acc_1: (99.62%) (46034/46208)\n",
      "Epoch: 93 | Batch_idx: 370 |  Loss_1: (0.0114) | Acc_1: (99.62%) (47307/47488)\n",
      "Epoch: 93 | Batch_idx: 380 |  Loss_1: (0.0115) | Acc_1: (99.61%) (48578/48768)\n",
      "Epoch: 93 | Batch_idx: 390 |  Loss_1: (0.0115) | Acc_1: (99.60%) (49802/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5190) | Acc: (90.92%) (9092/10000)\n",
      "Epoch: 94 | Batch_idx: 0 |  Loss_1: (0.0038) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 94 | Batch_idx: 10 |  Loss_1: (0.0063) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 94 | Batch_idx: 20 |  Loss_1: (0.0081) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 94 | Batch_idx: 30 |  Loss_1: (0.0087) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 94 | Batch_idx: 40 |  Loss_1: (0.0084) | Acc_1: (99.71%) (5233/5248)\n",
      "Epoch: 94 | Batch_idx: 50 |  Loss_1: (0.0093) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 94 | Batch_idx: 60 |  Loss_1: (0.0093) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 94 | Batch_idx: 70 |  Loss_1: (0.0088) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 94 | Batch_idx: 80 |  Loss_1: (0.0083) | Acc_1: (99.72%) (10339/10368)\n",
      "Epoch: 94 | Batch_idx: 90 |  Loss_1: (0.0084) | Acc_1: (99.73%) (11616/11648)\n",
      "Epoch: 94 | Batch_idx: 100 |  Loss_1: (0.0091) | Acc_1: (99.70%) (12889/12928)\n",
      "Epoch: 94 | Batch_idx: 110 |  Loss_1: (0.0092) | Acc_1: (99.70%) (14165/14208)\n",
      "Epoch: 94 | Batch_idx: 120 |  Loss_1: (0.0091) | Acc_1: (99.70%) (15441/15488)\n",
      "Epoch: 94 | Batch_idx: 130 |  Loss_1: (0.0087) | Acc_1: (99.72%) (16721/16768)\n",
      "Epoch: 94 | Batch_idx: 140 |  Loss_1: (0.0091) | Acc_1: (99.71%) (17996/18048)\n",
      "Epoch: 94 | Batch_idx: 150 |  Loss_1: (0.0094) | Acc_1: (99.70%) (19270/19328)\n",
      "Epoch: 94 | Batch_idx: 160 |  Loss_1: (0.0091) | Acc_1: (99.71%) (20549/20608)\n",
      "Epoch: 94 | Batch_idx: 170 |  Loss_1: (0.0090) | Acc_1: (99.71%) (21825/21888)\n",
      "Epoch: 94 | Batch_idx: 180 |  Loss_1: (0.0092) | Acc_1: (99.70%) (23098/23168)\n",
      "Epoch: 94 | Batch_idx: 190 |  Loss_1: (0.0094) | Acc_1: (99.69%) (24372/24448)\n",
      "Epoch: 94 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.70%) (25650/25728)\n",
      "Epoch: 94 | Batch_idx: 210 |  Loss_1: (0.0098) | Acc_1: (99.69%) (26923/27008)\n",
      "Epoch: 94 | Batch_idx: 220 |  Loss_1: (0.0097) | Acc_1: (99.69%) (28200/28288)\n",
      "Epoch: 94 | Batch_idx: 230 |  Loss_1: (0.0096) | Acc_1: (99.69%) (29477/29568)\n",
      "Epoch: 94 | Batch_idx: 240 |  Loss_1: (0.0099) | Acc_1: (99.69%) (30752/30848)\n",
      "Epoch: 94 | Batch_idx: 250 |  Loss_1: (0.0100) | Acc_1: (99.68%) (32026/32128)\n",
      "Epoch: 94 | Batch_idx: 260 |  Loss_1: (0.0103) | Acc_1: (99.67%) (33298/33408)\n",
      "Epoch: 94 | Batch_idx: 270 |  Loss_1: (0.0104) | Acc_1: (99.66%) (34570/34688)\n",
      "Epoch: 94 | Batch_idx: 280 |  Loss_1: (0.0104) | Acc_1: (99.65%) (35842/35968)\n",
      "Epoch: 94 | Batch_idx: 290 |  Loss_1: (0.0106) | Acc_1: (99.63%) (37112/37248)\n",
      "Epoch: 94 | Batch_idx: 300 |  Loss_1: (0.0106) | Acc_1: (99.64%) (38388/38528)\n",
      "Epoch: 94 | Batch_idx: 310 |  Loss_1: (0.0107) | Acc_1: (99.64%) (39663/39808)\n",
      "Epoch: 94 | Batch_idx: 320 |  Loss_1: (0.0106) | Acc_1: (99.64%) (40941/41088)\n",
      "Epoch: 94 | Batch_idx: 330 |  Loss_1: (0.0107) | Acc_1: (99.63%) (42213/42368)\n",
      "Epoch: 94 | Batch_idx: 340 |  Loss_1: (0.0108) | Acc_1: (99.63%) (43485/43648)\n",
      "Epoch: 94 | Batch_idx: 350 |  Loss_1: (0.0108) | Acc_1: (99.63%) (44760/44928)\n",
      "Epoch: 94 | Batch_idx: 360 |  Loss_1: (0.0107) | Acc_1: (99.63%) (46038/46208)\n",
      "Epoch: 94 | Batch_idx: 370 |  Loss_1: (0.0107) | Acc_1: (99.63%) (47313/47488)\n",
      "Epoch: 94 | Batch_idx: 380 |  Loss_1: (0.0108) | Acc_1: (99.63%) (48588/48768)\n",
      "Epoch: 94 | Batch_idx: 390 |  Loss_1: (0.0109) | Acc_1: (99.63%) (49813/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5322) | Acc: (91.02%) (9102/10000)\n",
      "Epoch: 95 | Batch_idx: 0 |  Loss_1: (0.0058) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 95 | Batch_idx: 10 |  Loss_1: (0.0143) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 95 | Batch_idx: 20 |  Loss_1: (0.0166) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 95 | Batch_idx: 30 |  Loss_1: (0.0154) | Acc_1: (99.55%) (3950/3968)\n",
      "Epoch: 95 | Batch_idx: 40 |  Loss_1: (0.0138) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 95 | Batch_idx: 50 |  Loss_1: (0.0124) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 95 | Batch_idx: 60 |  Loss_1: (0.0126) | Acc_1: (99.63%) (7779/7808)\n",
      "Epoch: 95 | Batch_idx: 70 |  Loss_1: (0.0125) | Acc_1: (99.63%) (9054/9088)\n",
      "Epoch: 95 | Batch_idx: 80 |  Loss_1: (0.0125) | Acc_1: (99.61%) (10328/10368)\n",
      "Epoch: 95 | Batch_idx: 90 |  Loss_1: (0.0117) | Acc_1: (99.64%) (11606/11648)\n",
      "Epoch: 95 | Batch_idx: 100 |  Loss_1: (0.0117) | Acc_1: (99.64%) (12881/12928)\n",
      "Epoch: 95 | Batch_idx: 110 |  Loss_1: (0.0113) | Acc_1: (99.64%) (14157/14208)\n",
      "Epoch: 95 | Batch_idx: 120 |  Loss_1: (0.0121) | Acc_1: (99.63%) (15431/15488)\n",
      "Epoch: 95 | Batch_idx: 130 |  Loss_1: (0.0123) | Acc_1: (99.62%) (16705/16768)\n",
      "Epoch: 95 | Batch_idx: 140 |  Loss_1: (0.0123) | Acc_1: (99.62%) (17979/18048)\n",
      "Epoch: 95 | Batch_idx: 150 |  Loss_1: (0.0124) | Acc_1: (99.62%) (19255/19328)\n",
      "Epoch: 95 | Batch_idx: 160 |  Loss_1: (0.0125) | Acc_1: (99.61%) (20528/20608)\n",
      "Epoch: 95 | Batch_idx: 170 |  Loss_1: (0.0125) | Acc_1: (99.60%) (21801/21888)\n",
      "Epoch: 95 | Batch_idx: 180 |  Loss_1: (0.0123) | Acc_1: (99.61%) (23077/23168)\n",
      "Epoch: 95 | Batch_idx: 190 |  Loss_1: (0.0122) | Acc_1: (99.61%) (24353/24448)\n",
      "Epoch: 95 | Batch_idx: 200 |  Loss_1: (0.0124) | Acc_1: (99.60%) (25625/25728)\n",
      "Epoch: 95 | Batch_idx: 210 |  Loss_1: (0.0122) | Acc_1: (99.61%) (26903/27008)\n",
      "Epoch: 95 | Batch_idx: 220 |  Loss_1: (0.0122) | Acc_1: (99.61%) (28177/28288)\n",
      "Epoch: 95 | Batch_idx: 230 |  Loss_1: (0.0122) | Acc_1: (99.60%) (29449/29568)\n",
      "Epoch: 95 | Batch_idx: 240 |  Loss_1: (0.0124) | Acc_1: (99.59%) (30723/30848)\n",
      "Epoch: 95 | Batch_idx: 250 |  Loss_1: (0.0123) | Acc_1: (99.60%) (31998/32128)\n",
      "Epoch: 95 | Batch_idx: 260 |  Loss_1: (0.0122) | Acc_1: (99.60%) (33273/33408)\n",
      "Epoch: 95 | Batch_idx: 270 |  Loss_1: (0.0122) | Acc_1: (99.59%) (34545/34688)\n",
      "Epoch: 95 | Batch_idx: 280 |  Loss_1: (0.0121) | Acc_1: (99.60%) (35823/35968)\n",
      "Epoch: 95 | Batch_idx: 290 |  Loss_1: (0.0122) | Acc_1: (99.59%) (37096/37248)\n",
      "Epoch: 95 | Batch_idx: 300 |  Loss_1: (0.0121) | Acc_1: (99.60%) (38372/38528)\n",
      "Epoch: 95 | Batch_idx: 310 |  Loss_1: (0.0121) | Acc_1: (99.59%) (39645/39808)\n",
      "Epoch: 95 | Batch_idx: 320 |  Loss_1: (0.0121) | Acc_1: (99.59%) (40919/41088)\n",
      "Epoch: 95 | Batch_idx: 330 |  Loss_1: (0.0121) | Acc_1: (99.59%) (42193/42368)\n",
      "Epoch: 95 | Batch_idx: 340 |  Loss_1: (0.0122) | Acc_1: (99.58%) (43465/43648)\n",
      "Epoch: 95 | Batch_idx: 350 |  Loss_1: (0.0120) | Acc_1: (99.59%) (44743/44928)\n",
      "Epoch: 95 | Batch_idx: 360 |  Loss_1: (0.0120) | Acc_1: (99.59%) (46017/46208)\n",
      "Epoch: 95 | Batch_idx: 370 |  Loss_1: (0.0120) | Acc_1: (99.59%) (47294/47488)\n",
      "Epoch: 95 | Batch_idx: 380 |  Loss_1: (0.0119) | Acc_1: (99.59%) (48569/48768)\n",
      "Epoch: 95 | Batch_idx: 390 |  Loss_1: (0.0118) | Acc_1: (99.60%) (49798/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5348) | Acc: (90.88%) (9088/10000)\n",
      "Epoch: 96 | Batch_idx: 0 |  Loss_1: (0.0286) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 96 | Batch_idx: 10 |  Loss_1: (0.0122) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 96 | Batch_idx: 20 |  Loss_1: (0.0173) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 96 | Batch_idx: 30 |  Loss_1: (0.0157) | Acc_1: (99.50%) (3948/3968)\n",
      "Epoch: 96 | Batch_idx: 40 |  Loss_1: (0.0146) | Acc_1: (99.50%) (5222/5248)\n",
      "Epoch: 96 | Batch_idx: 50 |  Loss_1: (0.0156) | Acc_1: (99.46%) (6493/6528)\n",
      "Epoch: 96 | Batch_idx: 60 |  Loss_1: (0.0160) | Acc_1: (99.45%) (7765/7808)\n",
      "Epoch: 96 | Batch_idx: 70 |  Loss_1: (0.0163) | Acc_1: (99.44%) (9037/9088)\n",
      "Epoch: 96 | Batch_idx: 80 |  Loss_1: (0.0154) | Acc_1: (99.45%) (10311/10368)\n",
      "Epoch: 96 | Batch_idx: 90 |  Loss_1: (0.0148) | Acc_1: (99.47%) (11586/11648)\n",
      "Epoch: 96 | Batch_idx: 100 |  Loss_1: (0.0141) | Acc_1: (99.50%) (12863/12928)\n",
      "Epoch: 96 | Batch_idx: 110 |  Loss_1: (0.0140) | Acc_1: (99.50%) (14137/14208)\n",
      "Epoch: 96 | Batch_idx: 120 |  Loss_1: (0.0138) | Acc_1: (99.50%) (15411/15488)\n",
      "Epoch: 96 | Batch_idx: 130 |  Loss_1: (0.0132) | Acc_1: (99.52%) (16688/16768)\n",
      "Epoch: 96 | Batch_idx: 140 |  Loss_1: (0.0134) | Acc_1: (99.52%) (17962/18048)\n",
      "Epoch: 96 | Batch_idx: 150 |  Loss_1: (0.0133) | Acc_1: (99.53%) (19238/19328)\n",
      "Epoch: 96 | Batch_idx: 160 |  Loss_1: (0.0139) | Acc_1: (99.52%) (20509/20608)\n",
      "Epoch: 96 | Batch_idx: 170 |  Loss_1: (0.0139) | Acc_1: (99.52%) (21782/21888)\n",
      "Epoch: 96 | Batch_idx: 180 |  Loss_1: (0.0137) | Acc_1: (99.52%) (23057/23168)\n",
      "Epoch: 96 | Batch_idx: 190 |  Loss_1: (0.0135) | Acc_1: (99.53%) (24333/24448)\n",
      "Epoch: 96 | Batch_idx: 200 |  Loss_1: (0.0134) | Acc_1: (99.53%) (25606/25728)\n",
      "Epoch: 96 | Batch_idx: 210 |  Loss_1: (0.0134) | Acc_1: (99.53%) (26882/27008)\n",
      "Epoch: 96 | Batch_idx: 220 |  Loss_1: (0.0136) | Acc_1: (99.52%) (28153/28288)\n",
      "Epoch: 96 | Batch_idx: 230 |  Loss_1: (0.0137) | Acc_1: (99.52%) (29426/29568)\n",
      "Epoch: 96 | Batch_idx: 240 |  Loss_1: (0.0136) | Acc_1: (99.52%) (30701/30848)\n",
      "Epoch: 96 | Batch_idx: 250 |  Loss_1: (0.0138) | Acc_1: (99.51%) (31970/32128)\n",
      "Epoch: 96 | Batch_idx: 260 |  Loss_1: (0.0136) | Acc_1: (99.51%) (33244/33408)\n",
      "Epoch: 96 | Batch_idx: 270 |  Loss_1: (0.0142) | Acc_1: (99.50%) (34514/34688)\n",
      "Epoch: 96 | Batch_idx: 280 |  Loss_1: (0.0140) | Acc_1: (99.51%) (35790/35968)\n",
      "Epoch: 96 | Batch_idx: 290 |  Loss_1: (0.0141) | Acc_1: (99.50%) (37061/37248)\n",
      "Epoch: 96 | Batch_idx: 300 |  Loss_1: (0.0140) | Acc_1: (99.50%) (38335/38528)\n",
      "Epoch: 96 | Batch_idx: 310 |  Loss_1: (0.0141) | Acc_1: (99.50%) (39609/39808)\n",
      "Epoch: 96 | Batch_idx: 320 |  Loss_1: (0.0140) | Acc_1: (99.50%) (40884/41088)\n",
      "Epoch: 96 | Batch_idx: 330 |  Loss_1: (0.0141) | Acc_1: (99.51%) (42160/42368)\n",
      "Epoch: 96 | Batch_idx: 340 |  Loss_1: (0.0141) | Acc_1: (99.51%) (43433/43648)\n",
      "Epoch: 96 | Batch_idx: 350 |  Loss_1: (0.0144) | Acc_1: (99.49%) (44701/44928)\n",
      "Epoch: 96 | Batch_idx: 360 |  Loss_1: (0.0146) | Acc_1: (99.49%) (45973/46208)\n",
      "Epoch: 96 | Batch_idx: 370 |  Loss_1: (0.0147) | Acc_1: (99.49%) (47246/47488)\n",
      "Epoch: 96 | Batch_idx: 380 |  Loss_1: (0.0147) | Acc_1: (99.49%) (48519/48768)\n",
      "Epoch: 96 | Batch_idx: 390 |  Loss_1: (0.0146) | Acc_1: (99.50%) (49749/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5385) | Acc: (90.80%) (9080/10000)\n",
      "Epoch: 97 | Batch_idx: 0 |  Loss_1: (0.0067) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 97 | Batch_idx: 10 |  Loss_1: (0.0128) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 97 | Batch_idx: 20 |  Loss_1: (0.0124) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 97 | Batch_idx: 30 |  Loss_1: (0.0113) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 97 | Batch_idx: 40 |  Loss_1: (0.0105) | Acc_1: (99.62%) (5228/5248)\n",
      "Epoch: 97 | Batch_idx: 50 |  Loss_1: (0.0100) | Acc_1: (99.66%) (6506/6528)\n",
      "Epoch: 97 | Batch_idx: 60 |  Loss_1: (0.0090) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 97 | Batch_idx: 70 |  Loss_1: (0.0093) | Acc_1: (99.69%) (9060/9088)\n",
      "Epoch: 97 | Batch_idx: 80 |  Loss_1: (0.0099) | Acc_1: (99.67%) (10334/10368)\n",
      "Epoch: 97 | Batch_idx: 90 |  Loss_1: (0.0107) | Acc_1: (99.66%) (11608/11648)\n",
      "Epoch: 97 | Batch_idx: 100 |  Loss_1: (0.0109) | Acc_1: (99.64%) (12882/12928)\n",
      "Epoch: 97 | Batch_idx: 110 |  Loss_1: (0.0117) | Acc_1: (99.62%) (14154/14208)\n",
      "Epoch: 97 | Batch_idx: 120 |  Loss_1: (0.0120) | Acc_1: (99.62%) (15429/15488)\n",
      "Epoch: 97 | Batch_idx: 130 |  Loss_1: (0.0122) | Acc_1: (99.59%) (16700/16768)\n",
      "Epoch: 97 | Batch_idx: 140 |  Loss_1: (0.0124) | Acc_1: (99.60%) (17975/18048)\n",
      "Epoch: 97 | Batch_idx: 150 |  Loss_1: (0.0124) | Acc_1: (99.59%) (19249/19328)\n",
      "Epoch: 97 | Batch_idx: 160 |  Loss_1: (0.0126) | Acc_1: (99.59%) (20524/20608)\n",
      "Epoch: 97 | Batch_idx: 170 |  Loss_1: (0.0131) | Acc_1: (99.58%) (21795/21888)\n",
      "Epoch: 97 | Batch_idx: 180 |  Loss_1: (0.0134) | Acc_1: (99.56%) (23066/23168)\n",
      "Epoch: 97 | Batch_idx: 190 |  Loss_1: (0.0132) | Acc_1: (99.57%) (24342/24448)\n",
      "Epoch: 97 | Batch_idx: 200 |  Loss_1: (0.0134) | Acc_1: (99.56%) (25614/25728)\n",
      "Epoch: 97 | Batch_idx: 210 |  Loss_1: (0.0139) | Acc_1: (99.54%) (26884/27008)\n",
      "Epoch: 97 | Batch_idx: 220 |  Loss_1: (0.0144) | Acc_1: (99.53%) (28154/28288)\n",
      "Epoch: 97 | Batch_idx: 230 |  Loss_1: (0.0143) | Acc_1: (99.53%) (29429/29568)\n",
      "Epoch: 97 | Batch_idx: 240 |  Loss_1: (0.0141) | Acc_1: (99.54%) (30705/30848)\n",
      "Epoch: 97 | Batch_idx: 250 |  Loss_1: (0.0141) | Acc_1: (99.54%) (31981/32128)\n",
      "Epoch: 97 | Batch_idx: 260 |  Loss_1: (0.0141) | Acc_1: (99.53%) (33252/33408)\n",
      "Epoch: 97 | Batch_idx: 270 |  Loss_1: (0.0142) | Acc_1: (99.53%) (34526/34688)\n",
      "Epoch: 97 | Batch_idx: 280 |  Loss_1: (0.0141) | Acc_1: (99.54%) (35801/35968)\n",
      "Epoch: 97 | Batch_idx: 290 |  Loss_1: (0.0142) | Acc_1: (99.53%) (37073/37248)\n",
      "Epoch: 97 | Batch_idx: 300 |  Loss_1: (0.0145) | Acc_1: (99.52%) (38344/38528)\n",
      "Epoch: 97 | Batch_idx: 310 |  Loss_1: (0.0144) | Acc_1: (99.52%) (39618/39808)\n",
      "Epoch: 97 | Batch_idx: 320 |  Loss_1: (0.0146) | Acc_1: (99.52%) (40889/41088)\n",
      "Epoch: 97 | Batch_idx: 330 |  Loss_1: (0.0145) | Acc_1: (99.51%) (42162/42368)\n",
      "Epoch: 97 | Batch_idx: 340 |  Loss_1: (0.0145) | Acc_1: (99.52%) (43437/43648)\n",
      "Epoch: 97 | Batch_idx: 350 |  Loss_1: (0.0144) | Acc_1: (99.51%) (44710/44928)\n",
      "Epoch: 97 | Batch_idx: 360 |  Loss_1: (0.0142) | Acc_1: (99.52%) (45987/46208)\n",
      "Epoch: 97 | Batch_idx: 370 |  Loss_1: (0.0143) | Acc_1: (99.52%) (47262/47488)\n",
      "Epoch: 97 | Batch_idx: 380 |  Loss_1: (0.0142) | Acc_1: (99.53%) (48537/48768)\n",
      "Epoch: 97 | Batch_idx: 390 |  Loss_1: (0.0142) | Acc_1: (99.53%) (49766/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5502) | Acc: (91.15%) (9115/10000)\n",
      "Epoch: 98 | Batch_idx: 0 |  Loss_1: (0.0269) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 98 | Batch_idx: 10 |  Loss_1: (0.0117) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 98 | Batch_idx: 20 |  Loss_1: (0.0155) | Acc_1: (99.29%) (2669/2688)\n",
      "Epoch: 98 | Batch_idx: 30 |  Loss_1: (0.0147) | Acc_1: (99.37%) (3943/3968)\n",
      "Epoch: 98 | Batch_idx: 40 |  Loss_1: (0.0143) | Acc_1: (99.35%) (5214/5248)\n",
      "Epoch: 98 | Batch_idx: 50 |  Loss_1: (0.0129) | Acc_1: (99.43%) (6491/6528)\n",
      "Epoch: 98 | Batch_idx: 60 |  Loss_1: (0.0153) | Acc_1: (99.41%) (7762/7808)\n",
      "Epoch: 98 | Batch_idx: 70 |  Loss_1: (0.0152) | Acc_1: (99.42%) (9035/9088)\n",
      "Epoch: 98 | Batch_idx: 80 |  Loss_1: (0.0151) | Acc_1: (99.46%) (10312/10368)\n",
      "Epoch: 98 | Batch_idx: 90 |  Loss_1: (0.0146) | Acc_1: (99.48%) (11587/11648)\n",
      "Epoch: 98 | Batch_idx: 100 |  Loss_1: (0.0147) | Acc_1: (99.49%) (12862/12928)\n",
      "Epoch: 98 | Batch_idx: 110 |  Loss_1: (0.0144) | Acc_1: (99.50%) (14137/14208)\n",
      "Epoch: 98 | Batch_idx: 120 |  Loss_1: (0.0142) | Acc_1: (99.50%) (15410/15488)\n",
      "Epoch: 98 | Batch_idx: 130 |  Loss_1: (0.0144) | Acc_1: (99.50%) (16684/16768)\n",
      "Epoch: 98 | Batch_idx: 140 |  Loss_1: (0.0142) | Acc_1: (99.50%) (17957/18048)\n",
      "Epoch: 98 | Batch_idx: 150 |  Loss_1: (0.0143) | Acc_1: (99.50%) (19231/19328)\n",
      "Epoch: 98 | Batch_idx: 160 |  Loss_1: (0.0138) | Acc_1: (99.52%) (20509/20608)\n",
      "Epoch: 98 | Batch_idx: 170 |  Loss_1: (0.0134) | Acc_1: (99.53%) (21786/21888)\n",
      "Epoch: 98 | Batch_idx: 180 |  Loss_1: (0.0135) | Acc_1: (99.53%) (23058/23168)\n",
      "Epoch: 98 | Batch_idx: 190 |  Loss_1: (0.0137) | Acc_1: (99.52%) (24331/24448)\n",
      "Epoch: 98 | Batch_idx: 200 |  Loss_1: (0.0134) | Acc_1: (99.53%) (25607/25728)\n",
      "Epoch: 98 | Batch_idx: 210 |  Loss_1: (0.0134) | Acc_1: (99.53%) (26882/27008)\n",
      "Epoch: 98 | Batch_idx: 220 |  Loss_1: (0.0133) | Acc_1: (99.54%) (28157/28288)\n",
      "Epoch: 98 | Batch_idx: 230 |  Loss_1: (0.0135) | Acc_1: (99.53%) (29428/29568)\n",
      "Epoch: 98 | Batch_idx: 240 |  Loss_1: (0.0135) | Acc_1: (99.53%) (30703/30848)\n",
      "Epoch: 98 | Batch_idx: 250 |  Loss_1: (0.0135) | Acc_1: (99.53%) (31976/32128)\n",
      "Epoch: 98 | Batch_idx: 260 |  Loss_1: (0.0137) | Acc_1: (99.52%) (33249/33408)\n",
      "Epoch: 98 | Batch_idx: 270 |  Loss_1: (0.0135) | Acc_1: (99.53%) (34526/34688)\n",
      "Epoch: 98 | Batch_idx: 280 |  Loss_1: (0.0136) | Acc_1: (99.53%) (35800/35968)\n",
      "Epoch: 98 | Batch_idx: 290 |  Loss_1: (0.0136) | Acc_1: (99.52%) (37071/37248)\n",
      "Epoch: 98 | Batch_idx: 300 |  Loss_1: (0.0138) | Acc_1: (99.53%) (38346/38528)\n",
      "Epoch: 98 | Batch_idx: 310 |  Loss_1: (0.0137) | Acc_1: (99.53%) (39622/39808)\n",
      "Epoch: 98 | Batch_idx: 320 |  Loss_1: (0.0134) | Acc_1: (99.54%) (40900/41088)\n",
      "Epoch: 98 | Batch_idx: 330 |  Loss_1: (0.0136) | Acc_1: (99.54%) (42171/42368)\n",
      "Epoch: 98 | Batch_idx: 340 |  Loss_1: (0.0134) | Acc_1: (99.54%) (43448/43648)\n",
      "Epoch: 98 | Batch_idx: 350 |  Loss_1: (0.0133) | Acc_1: (99.54%) (44723/44928)\n",
      "Epoch: 98 | Batch_idx: 360 |  Loss_1: (0.0132) | Acc_1: (99.55%) (46000/46208)\n",
      "Epoch: 98 | Batch_idx: 370 |  Loss_1: (0.0132) | Acc_1: (99.55%) (47273/47488)\n",
      "Epoch: 98 | Batch_idx: 380 |  Loss_1: (0.0133) | Acc_1: (99.55%) (48548/48768)\n",
      "Epoch: 98 | Batch_idx: 390 |  Loss_1: (0.0134) | Acc_1: (99.55%) (49775/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5173) | Acc: (91.00%) (9100/10000)\n",
      "Epoch: 99 | Batch_idx: 0 |  Loss_1: (0.0083) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 99 | Batch_idx: 10 |  Loss_1: (0.0140) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 99 | Batch_idx: 20 |  Loss_1: (0.0158) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 99 | Batch_idx: 30 |  Loss_1: (0.0145) | Acc_1: (99.47%) (3947/3968)\n",
      "Epoch: 99 | Batch_idx: 40 |  Loss_1: (0.0137) | Acc_1: (99.52%) (5223/5248)\n",
      "Epoch: 99 | Batch_idx: 50 |  Loss_1: (0.0125) | Acc_1: (99.57%) (6500/6528)\n",
      "Epoch: 99 | Batch_idx: 60 |  Loss_1: (0.0124) | Acc_1: (99.56%) (7774/7808)\n",
      "Epoch: 99 | Batch_idx: 70 |  Loss_1: (0.0128) | Acc_1: (99.56%) (9048/9088)\n",
      "Epoch: 99 | Batch_idx: 80 |  Loss_1: (0.0133) | Acc_1: (99.53%) (10319/10368)\n",
      "Epoch: 99 | Batch_idx: 90 |  Loss_1: (0.0137) | Acc_1: (99.53%) (11593/11648)\n",
      "Epoch: 99 | Batch_idx: 100 |  Loss_1: (0.0133) | Acc_1: (99.54%) (12869/12928)\n",
      "Epoch: 99 | Batch_idx: 110 |  Loss_1: (0.0133) | Acc_1: (99.53%) (14141/14208)\n",
      "Epoch: 99 | Batch_idx: 120 |  Loss_1: (0.0135) | Acc_1: (99.52%) (15414/15488)\n",
      "Epoch: 99 | Batch_idx: 130 |  Loss_1: (0.0137) | Acc_1: (99.51%) (16685/16768)\n",
      "Epoch: 99 | Batch_idx: 140 |  Loss_1: (0.0140) | Acc_1: (99.51%) (17959/18048)\n",
      "Epoch: 99 | Batch_idx: 150 |  Loss_1: (0.0143) | Acc_1: (99.49%) (19229/19328)\n",
      "Epoch: 99 | Batch_idx: 160 |  Loss_1: (0.0145) | Acc_1: (99.48%) (20500/20608)\n",
      "Epoch: 99 | Batch_idx: 170 |  Loss_1: (0.0139) | Acc_1: (99.50%) (21779/21888)\n",
      "Epoch: 99 | Batch_idx: 180 |  Loss_1: (0.0137) | Acc_1: (99.51%) (23054/23168)\n",
      "Epoch: 99 | Batch_idx: 190 |  Loss_1: (0.0137) | Acc_1: (99.52%) (24331/24448)\n",
      "Epoch: 99 | Batch_idx: 200 |  Loss_1: (0.0135) | Acc_1: (99.53%) (25606/25728)\n",
      "Epoch: 99 | Batch_idx: 210 |  Loss_1: (0.0135) | Acc_1: (99.52%) (26879/27008)\n",
      "Epoch: 99 | Batch_idx: 220 |  Loss_1: (0.0136) | Acc_1: (99.52%) (28152/28288)\n",
      "Epoch: 99 | Batch_idx: 230 |  Loss_1: (0.0132) | Acc_1: (99.53%) (29430/29568)\n",
      "Epoch: 99 | Batch_idx: 240 |  Loss_1: (0.0132) | Acc_1: (99.53%) (30704/30848)\n",
      "Epoch: 99 | Batch_idx: 250 |  Loss_1: (0.0134) | Acc_1: (99.53%) (31976/32128)\n",
      "Epoch: 99 | Batch_idx: 260 |  Loss_1: (0.0134) | Acc_1: (99.53%) (33250/33408)\n",
      "Epoch: 99 | Batch_idx: 270 |  Loss_1: (0.0135) | Acc_1: (99.52%) (34523/34688)\n",
      "Epoch: 99 | Batch_idx: 280 |  Loss_1: (0.0136) | Acc_1: (99.52%) (35796/35968)\n",
      "Epoch: 99 | Batch_idx: 290 |  Loss_1: (0.0135) | Acc_1: (99.52%) (37069/37248)\n",
      "Epoch: 99 | Batch_idx: 300 |  Loss_1: (0.0133) | Acc_1: (99.53%) (38346/38528)\n",
      "Epoch: 99 | Batch_idx: 310 |  Loss_1: (0.0136) | Acc_1: (99.52%) (39616/39808)\n",
      "Epoch: 99 | Batch_idx: 320 |  Loss_1: (0.0138) | Acc_1: (99.51%) (40886/41088)\n",
      "Epoch: 99 | Batch_idx: 330 |  Loss_1: (0.0136) | Acc_1: (99.52%) (42163/42368)\n",
      "Epoch: 99 | Batch_idx: 340 |  Loss_1: (0.0136) | Acc_1: (99.51%) (43436/43648)\n",
      "Epoch: 99 | Batch_idx: 350 |  Loss_1: (0.0136) | Acc_1: (99.51%) (44708/44928)\n",
      "Epoch: 99 | Batch_idx: 360 |  Loss_1: (0.0135) | Acc_1: (99.51%) (45983/46208)\n",
      "Epoch: 99 | Batch_idx: 370 |  Loss_1: (0.0135) | Acc_1: (99.51%) (47256/47488)\n",
      "Epoch: 99 | Batch_idx: 380 |  Loss_1: (0.0134) | Acc_1: (99.51%) (48531/48768)\n",
      "Epoch: 99 | Batch_idx: 390 |  Loss_1: (0.0135) | Acc_1: (99.51%) (49756/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5296) | Acc: (91.49%) (9149/10000)\n",
      "Epoch: 100 | Batch_idx: 0 |  Loss_1: (0.0032) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 100 | Batch_idx: 10 |  Loss_1: (0.0087) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 100 | Batch_idx: 20 |  Loss_1: (0.0087) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 100 | Batch_idx: 30 |  Loss_1: (0.0081) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 100 | Batch_idx: 40 |  Loss_1: (0.0094) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 100 | Batch_idx: 50 |  Loss_1: (0.0095) | Acc_1: (99.66%) (6506/6528)\n",
      "Epoch: 100 | Batch_idx: 60 |  Loss_1: (0.0098) | Acc_1: (99.64%) (7780/7808)\n",
      "Epoch: 100 | Batch_idx: 70 |  Loss_1: (0.0106) | Acc_1: (99.61%) (9053/9088)\n",
      "Epoch: 100 | Batch_idx: 80 |  Loss_1: (0.0103) | Acc_1: (99.63%) (10330/10368)\n",
      "Epoch: 100 | Batch_idx: 90 |  Loss_1: (0.0106) | Acc_1: (99.62%) (11604/11648)\n",
      "Epoch: 100 | Batch_idx: 100 |  Loss_1: (0.0102) | Acc_1: (99.64%) (12881/12928)\n",
      "Epoch: 100 | Batch_idx: 110 |  Loss_1: (0.0099) | Acc_1: (99.64%) (14157/14208)\n",
      "Epoch: 100 | Batch_idx: 120 |  Loss_1: (0.0102) | Acc_1: (99.64%) (15433/15488)\n",
      "Epoch: 100 | Batch_idx: 130 |  Loss_1: (0.0099) | Acc_1: (99.65%) (16710/16768)\n",
      "Epoch: 100 | Batch_idx: 140 |  Loss_1: (0.0103) | Acc_1: (99.65%) (17984/18048)\n",
      "Epoch: 100 | Batch_idx: 150 |  Loss_1: (0.0102) | Acc_1: (99.64%) (19259/19328)\n",
      "Epoch: 100 | Batch_idx: 160 |  Loss_1: (0.0101) | Acc_1: (99.65%) (20536/20608)\n",
      "Epoch: 100 | Batch_idx: 170 |  Loss_1: (0.0100) | Acc_1: (99.66%) (21813/21888)\n",
      "Epoch: 100 | Batch_idx: 180 |  Loss_1: (0.0101) | Acc_1: (99.65%) (23088/23168)\n",
      "Epoch: 100 | Batch_idx: 190 |  Loss_1: (0.0107) | Acc_1: (99.64%) (24359/24448)\n",
      "Epoch: 100 | Batch_idx: 200 |  Loss_1: (0.0111) | Acc_1: (99.62%) (25631/25728)\n",
      "Epoch: 100 | Batch_idx: 210 |  Loss_1: (0.0118) | Acc_1: (99.59%) (26898/27008)\n",
      "Epoch: 100 | Batch_idx: 220 |  Loss_1: (0.0124) | Acc_1: (99.59%) (28171/28288)\n",
      "Epoch: 100 | Batch_idx: 230 |  Loss_1: (0.0127) | Acc_1: (99.57%) (29442/29568)\n",
      "Epoch: 100 | Batch_idx: 240 |  Loss_1: (0.0128) | Acc_1: (99.58%) (30717/30848)\n",
      "Epoch: 100 | Batch_idx: 250 |  Loss_1: (0.0128) | Acc_1: (99.57%) (31991/32128)\n",
      "Epoch: 100 | Batch_idx: 260 |  Loss_1: (0.0130) | Acc_1: (99.57%) (33266/33408)\n",
      "Epoch: 100 | Batch_idx: 270 |  Loss_1: (0.0133) | Acc_1: (99.57%) (34539/34688)\n",
      "Epoch: 100 | Batch_idx: 280 |  Loss_1: (0.0133) | Acc_1: (99.57%) (35812/35968)\n",
      "Epoch: 100 | Batch_idx: 290 |  Loss_1: (0.0135) | Acc_1: (99.55%) (37082/37248)\n",
      "Epoch: 100 | Batch_idx: 300 |  Loss_1: (0.0132) | Acc_1: (99.57%) (38361/38528)\n",
      "Epoch: 100 | Batch_idx: 310 |  Loss_1: (0.0136) | Acc_1: (99.56%) (39634/39808)\n",
      "Epoch: 100 | Batch_idx: 320 |  Loss_1: (0.0134) | Acc_1: (99.57%) (40911/41088)\n",
      "Epoch: 100 | Batch_idx: 330 |  Loss_1: (0.0132) | Acc_1: (99.58%) (42189/42368)\n",
      "Epoch: 100 | Batch_idx: 340 |  Loss_1: (0.0131) | Acc_1: (99.58%) (43464/43648)\n",
      "Epoch: 100 | Batch_idx: 350 |  Loss_1: (0.0129) | Acc_1: (99.58%) (44740/44928)\n",
      "Epoch: 100 | Batch_idx: 360 |  Loss_1: (0.0128) | Acc_1: (99.58%) (46016/46208)\n",
      "Epoch: 100 | Batch_idx: 370 |  Loss_1: (0.0129) | Acc_1: (99.58%) (47289/47488)\n",
      "Epoch: 100 | Batch_idx: 380 |  Loss_1: (0.0130) | Acc_1: (99.58%) (48565/48768)\n",
      "Epoch: 100 | Batch_idx: 390 |  Loss_1: (0.0129) | Acc_1: (99.59%) (49793/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5236) | Acc: (91.23%) (9123/10000)\n",
      "Epoch: 101 | Batch_idx: 0 |  Loss_1: (0.0641) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 101 | Batch_idx: 10 |  Loss_1: (0.0150) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 101 | Batch_idx: 20 |  Loss_1: (0.0111) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 101 | Batch_idx: 30 |  Loss_1: (0.0114) | Acc_1: (99.62%) (3953/3968)\n",
      "Epoch: 101 | Batch_idx: 40 |  Loss_1: (0.0110) | Acc_1: (99.64%) (5229/5248)\n",
      "Epoch: 101 | Batch_idx: 50 |  Loss_1: (0.0106) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 101 | Batch_idx: 60 |  Loss_1: (0.0109) | Acc_1: (99.65%) (7781/7808)\n",
      "Epoch: 101 | Batch_idx: 70 |  Loss_1: (0.0109) | Acc_1: (99.65%) (9056/9088)\n",
      "Epoch: 101 | Batch_idx: 80 |  Loss_1: (0.0111) | Acc_1: (99.65%) (10332/10368)\n",
      "Epoch: 101 | Batch_idx: 90 |  Loss_1: (0.0111) | Acc_1: (99.66%) (11608/11648)\n",
      "Epoch: 101 | Batch_idx: 100 |  Loss_1: (0.0114) | Acc_1: (99.64%) (12881/12928)\n",
      "Epoch: 101 | Batch_idx: 110 |  Loss_1: (0.0114) | Acc_1: (99.64%) (14157/14208)\n",
      "Epoch: 101 | Batch_idx: 120 |  Loss_1: (0.0115) | Acc_1: (99.63%) (15431/15488)\n",
      "Epoch: 101 | Batch_idx: 130 |  Loss_1: (0.0115) | Acc_1: (99.65%) (16709/16768)\n",
      "Epoch: 101 | Batch_idx: 140 |  Loss_1: (0.0117) | Acc_1: (99.63%) (17981/18048)\n",
      "Epoch: 101 | Batch_idx: 150 |  Loss_1: (0.0116) | Acc_1: (99.63%) (19256/19328)\n",
      "Epoch: 101 | Batch_idx: 160 |  Loss_1: (0.0117) | Acc_1: (99.62%) (20529/20608)\n",
      "Epoch: 101 | Batch_idx: 170 |  Loss_1: (0.0115) | Acc_1: (99.63%) (21807/21888)\n",
      "Epoch: 101 | Batch_idx: 180 |  Loss_1: (0.0117) | Acc_1: (99.63%) (23082/23168)\n",
      "Epoch: 101 | Batch_idx: 190 |  Loss_1: (0.0114) | Acc_1: (99.64%) (24361/24448)\n",
      "Epoch: 101 | Batch_idx: 200 |  Loss_1: (0.0118) | Acc_1: (99.63%) (25632/25728)\n",
      "Epoch: 101 | Batch_idx: 210 |  Loss_1: (0.0118) | Acc_1: (99.62%) (26905/27008)\n",
      "Epoch: 101 | Batch_idx: 220 |  Loss_1: (0.0118) | Acc_1: (99.62%) (28181/28288)\n",
      "Epoch: 101 | Batch_idx: 230 |  Loss_1: (0.0119) | Acc_1: (99.62%) (29455/29568)\n",
      "Epoch: 101 | Batch_idx: 240 |  Loss_1: (0.0118) | Acc_1: (99.62%) (30732/30848)\n",
      "Epoch: 101 | Batch_idx: 250 |  Loss_1: (0.0117) | Acc_1: (99.63%) (32008/32128)\n",
      "Epoch: 101 | Batch_idx: 260 |  Loss_1: (0.0118) | Acc_1: (99.62%) (33281/33408)\n",
      "Epoch: 101 | Batch_idx: 270 |  Loss_1: (0.0119) | Acc_1: (99.62%) (34557/34688)\n",
      "Epoch: 101 | Batch_idx: 280 |  Loss_1: (0.0120) | Acc_1: (99.62%) (35830/35968)\n",
      "Epoch: 101 | Batch_idx: 290 |  Loss_1: (0.0121) | Acc_1: (99.61%) (37104/37248)\n",
      "Epoch: 101 | Batch_idx: 300 |  Loss_1: (0.0120) | Acc_1: (99.62%) (38382/38528)\n",
      "Epoch: 101 | Batch_idx: 310 |  Loss_1: (0.0119) | Acc_1: (99.62%) (39658/39808)\n",
      "Epoch: 101 | Batch_idx: 320 |  Loss_1: (0.0119) | Acc_1: (99.62%) (40933/41088)\n",
      "Epoch: 101 | Batch_idx: 330 |  Loss_1: (0.0119) | Acc_1: (99.62%) (42206/42368)\n",
      "Epoch: 101 | Batch_idx: 340 |  Loss_1: (0.0119) | Acc_1: (99.62%) (43484/43648)\n",
      "Epoch: 101 | Batch_idx: 350 |  Loss_1: (0.0121) | Acc_1: (99.62%) (44758/44928)\n",
      "Epoch: 101 | Batch_idx: 360 |  Loss_1: (0.0120) | Acc_1: (99.62%) (46033/46208)\n",
      "Epoch: 101 | Batch_idx: 370 |  Loss_1: (0.0119) | Acc_1: (99.62%) (47308/47488)\n",
      "Epoch: 101 | Batch_idx: 380 |  Loss_1: (0.0119) | Acc_1: (99.63%) (48586/48768)\n",
      "Epoch: 101 | Batch_idx: 390 |  Loss_1: (0.0119) | Acc_1: (99.63%) (49814/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5170) | Acc: (91.32%) (9132/10000)\n",
      "Epoch: 102 | Batch_idx: 0 |  Loss_1: (0.0222) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 102 | Batch_idx: 10 |  Loss_1: (0.0106) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 102 | Batch_idx: 20 |  Loss_1: (0.0106) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 102 | Batch_idx: 30 |  Loss_1: (0.0132) | Acc_1: (99.50%) (3948/3968)\n",
      "Epoch: 102 | Batch_idx: 40 |  Loss_1: (0.0120) | Acc_1: (99.52%) (5223/5248)\n",
      "Epoch: 102 | Batch_idx: 50 |  Loss_1: (0.0118) | Acc_1: (99.54%) (6498/6528)\n",
      "Epoch: 102 | Batch_idx: 60 |  Loss_1: (0.0117) | Acc_1: (99.55%) (7773/7808)\n",
      "Epoch: 102 | Batch_idx: 70 |  Loss_1: (0.0124) | Acc_1: (99.54%) (9046/9088)\n",
      "Epoch: 102 | Batch_idx: 80 |  Loss_1: (0.0131) | Acc_1: (99.53%) (10319/10368)\n",
      "Epoch: 102 | Batch_idx: 90 |  Loss_1: (0.0128) | Acc_1: (99.56%) (11597/11648)\n",
      "Epoch: 102 | Batch_idx: 100 |  Loss_1: (0.0134) | Acc_1: (99.54%) (12868/12928)\n",
      "Epoch: 102 | Batch_idx: 110 |  Loss_1: (0.0131) | Acc_1: (99.54%) (14143/14208)\n",
      "Epoch: 102 | Batch_idx: 120 |  Loss_1: (0.0130) | Acc_1: (99.53%) (15415/15488)\n",
      "Epoch: 102 | Batch_idx: 130 |  Loss_1: (0.0128) | Acc_1: (99.54%) (16691/16768)\n",
      "Epoch: 102 | Batch_idx: 140 |  Loss_1: (0.0124) | Acc_1: (99.55%) (17966/18048)\n",
      "Epoch: 102 | Batch_idx: 150 |  Loss_1: (0.0126) | Acc_1: (99.54%) (19240/19328)\n",
      "Epoch: 102 | Batch_idx: 160 |  Loss_1: (0.0122) | Acc_1: (99.57%) (20519/20608)\n",
      "Epoch: 102 | Batch_idx: 170 |  Loss_1: (0.0118) | Acc_1: (99.58%) (21796/21888)\n",
      "Epoch: 102 | Batch_idx: 180 |  Loss_1: (0.0113) | Acc_1: (99.60%) (23075/23168)\n",
      "Epoch: 102 | Batch_idx: 190 |  Loss_1: (0.0112) | Acc_1: (99.60%) (24349/24448)\n",
      "Epoch: 102 | Batch_idx: 200 |  Loss_1: (0.0110) | Acc_1: (99.60%) (25626/25728)\n",
      "Epoch: 102 | Batch_idx: 210 |  Loss_1: (0.0110) | Acc_1: (99.60%) (26901/27008)\n",
      "Epoch: 102 | Batch_idx: 220 |  Loss_1: (0.0108) | Acc_1: (99.61%) (28179/28288)\n",
      "Epoch: 102 | Batch_idx: 230 |  Loss_1: (0.0107) | Acc_1: (99.62%) (29457/29568)\n",
      "Epoch: 102 | Batch_idx: 240 |  Loss_1: (0.0106) | Acc_1: (99.63%) (30733/30848)\n",
      "Epoch: 102 | Batch_idx: 250 |  Loss_1: (0.0105) | Acc_1: (99.63%) (32009/32128)\n",
      "Epoch: 102 | Batch_idx: 260 |  Loss_1: (0.0102) | Acc_1: (99.64%) (33289/33408)\n",
      "Epoch: 102 | Batch_idx: 270 |  Loss_1: (0.0101) | Acc_1: (99.65%) (34567/34688)\n",
      "Epoch: 102 | Batch_idx: 280 |  Loss_1: (0.0099) | Acc_1: (99.65%) (35842/35968)\n",
      "Epoch: 102 | Batch_idx: 290 |  Loss_1: (0.0099) | Acc_1: (99.65%) (37119/37248)\n",
      "Epoch: 102 | Batch_idx: 300 |  Loss_1: (0.0099) | Acc_1: (99.66%) (38396/38528)\n",
      "Epoch: 102 | Batch_idx: 310 |  Loss_1: (0.0102) | Acc_1: (99.65%) (39667/39808)\n",
      "Epoch: 102 | Batch_idx: 320 |  Loss_1: (0.0102) | Acc_1: (99.64%) (40942/41088)\n",
      "Epoch: 102 | Batch_idx: 330 |  Loss_1: (0.0101) | Acc_1: (99.65%) (42220/42368)\n",
      "Epoch: 102 | Batch_idx: 340 |  Loss_1: (0.0101) | Acc_1: (99.65%) (43496/43648)\n",
      "Epoch: 102 | Batch_idx: 350 |  Loss_1: (0.0099) | Acc_1: (99.66%) (44774/44928)\n",
      "Epoch: 102 | Batch_idx: 360 |  Loss_1: (0.0098) | Acc_1: (99.66%) (46049/46208)\n",
      "Epoch: 102 | Batch_idx: 370 |  Loss_1: (0.0098) | Acc_1: (99.66%) (47325/47488)\n",
      "Epoch: 102 | Batch_idx: 380 |  Loss_1: (0.0097) | Acc_1: (99.66%) (48603/48768)\n",
      "Epoch: 102 | Batch_idx: 390 |  Loss_1: (0.0096) | Acc_1: (99.66%) (49831/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5039) | Acc: (91.22%) (9122/10000)\n",
      "Epoch: 103 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 103 | Batch_idx: 10 |  Loss_1: (0.0115) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 103 | Batch_idx: 20 |  Loss_1: (0.0112) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 103 | Batch_idx: 30 |  Loss_1: (0.0090) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 103 | Batch_idx: 40 |  Loss_1: (0.0086) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 103 | Batch_idx: 50 |  Loss_1: (0.0083) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 103 | Batch_idx: 60 |  Loss_1: (0.0084) | Acc_1: (99.73%) (7787/7808)\n",
      "Epoch: 103 | Batch_idx: 70 |  Loss_1: (0.0094) | Acc_1: (99.68%) (9059/9088)\n",
      "Epoch: 103 | Batch_idx: 80 |  Loss_1: (0.0099) | Acc_1: (99.67%) (10334/10368)\n",
      "Epoch: 103 | Batch_idx: 90 |  Loss_1: (0.0098) | Acc_1: (99.68%) (11611/11648)\n",
      "Epoch: 103 | Batch_idx: 100 |  Loss_1: (0.0101) | Acc_1: (99.66%) (12884/12928)\n",
      "Epoch: 103 | Batch_idx: 110 |  Loss_1: (0.0102) | Acc_1: (99.65%) (14158/14208)\n",
      "Epoch: 103 | Batch_idx: 120 |  Loss_1: (0.0100) | Acc_1: (99.66%) (15435/15488)\n",
      "Epoch: 103 | Batch_idx: 130 |  Loss_1: (0.0100) | Acc_1: (99.65%) (16709/16768)\n",
      "Epoch: 103 | Batch_idx: 140 |  Loss_1: (0.0097) | Acc_1: (99.67%) (17988/18048)\n",
      "Epoch: 103 | Batch_idx: 150 |  Loss_1: (0.0097) | Acc_1: (99.66%) (19263/19328)\n",
      "Epoch: 103 | Batch_idx: 160 |  Loss_1: (0.0096) | Acc_1: (99.66%) (20537/20608)\n",
      "Epoch: 103 | Batch_idx: 170 |  Loss_1: (0.0094) | Acc_1: (99.66%) (21814/21888)\n",
      "Epoch: 103 | Batch_idx: 180 |  Loss_1: (0.0094) | Acc_1: (99.67%) (23091/23168)\n",
      "Epoch: 103 | Batch_idx: 190 |  Loss_1: (0.0094) | Acc_1: (99.66%) (24366/24448)\n",
      "Epoch: 103 | Batch_idx: 200 |  Loss_1: (0.0094) | Acc_1: (99.66%) (25641/25728)\n",
      "Epoch: 103 | Batch_idx: 210 |  Loss_1: (0.0091) | Acc_1: (99.67%) (26920/27008)\n",
      "Epoch: 103 | Batch_idx: 220 |  Loss_1: (0.0090) | Acc_1: (99.68%) (28197/28288)\n",
      "Epoch: 103 | Batch_idx: 230 |  Loss_1: (0.0092) | Acc_1: (99.68%) (29472/29568)\n",
      "Epoch: 103 | Batch_idx: 240 |  Loss_1: (0.0093) | Acc_1: (99.67%) (30746/30848)\n",
      "Epoch: 103 | Batch_idx: 250 |  Loss_1: (0.0093) | Acc_1: (99.67%) (32021/32128)\n",
      "Epoch: 103 | Batch_idx: 260 |  Loss_1: (0.0092) | Acc_1: (99.67%) (33297/33408)\n",
      "Epoch: 103 | Batch_idx: 270 |  Loss_1: (0.0091) | Acc_1: (99.67%) (34574/34688)\n",
      "Epoch: 103 | Batch_idx: 280 |  Loss_1: (0.0090) | Acc_1: (99.67%) (35851/35968)\n",
      "Epoch: 103 | Batch_idx: 290 |  Loss_1: (0.0090) | Acc_1: (99.67%) (37125/37248)\n",
      "Epoch: 103 | Batch_idx: 300 |  Loss_1: (0.0092) | Acc_1: (99.67%) (38402/38528)\n",
      "Epoch: 103 | Batch_idx: 310 |  Loss_1: (0.0092) | Acc_1: (99.68%) (39680/39808)\n",
      "Epoch: 103 | Batch_idx: 320 |  Loss_1: (0.0093) | Acc_1: (99.67%) (40954/41088)\n",
      "Epoch: 103 | Batch_idx: 330 |  Loss_1: (0.0092) | Acc_1: (99.67%) (42230/42368)\n",
      "Epoch: 103 | Batch_idx: 340 |  Loss_1: (0.0093) | Acc_1: (99.67%) (43502/43648)\n",
      "Epoch: 103 | Batch_idx: 350 |  Loss_1: (0.0093) | Acc_1: (99.66%) (44777/44928)\n",
      "Epoch: 103 | Batch_idx: 360 |  Loss_1: (0.0093) | Acc_1: (99.67%) (46054/46208)\n",
      "Epoch: 103 | Batch_idx: 370 |  Loss_1: (0.0095) | Acc_1: (99.66%) (47325/47488)\n",
      "Epoch: 103 | Batch_idx: 380 |  Loss_1: (0.0097) | Acc_1: (99.65%) (48596/48768)\n",
      "Epoch: 103 | Batch_idx: 390 |  Loss_1: (0.0098) | Acc_1: (99.64%) (49821/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5272) | Acc: (91.46%) (9146/10000)\n",
      "Epoch: 104 | Batch_idx: 0 |  Loss_1: (0.0091) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 104 | Batch_idx: 10 |  Loss_1: (0.0086) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 104 | Batch_idx: 20 |  Loss_1: (0.0164) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 104 | Batch_idx: 30 |  Loss_1: (0.0169) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 104 | Batch_idx: 40 |  Loss_1: (0.0150) | Acc_1: (99.62%) (5228/5248)\n",
      "Epoch: 104 | Batch_idx: 50 |  Loss_1: (0.0146) | Acc_1: (99.60%) (6502/6528)\n",
      "Epoch: 104 | Batch_idx: 60 |  Loss_1: (0.0138) | Acc_1: (99.60%) (7777/7808)\n",
      "Epoch: 104 | Batch_idx: 70 |  Loss_1: (0.0138) | Acc_1: (99.60%) (9052/9088)\n",
      "Epoch: 104 | Batch_idx: 80 |  Loss_1: (0.0138) | Acc_1: (99.61%) (10328/10368)\n",
      "Epoch: 104 | Batch_idx: 90 |  Loss_1: (0.0143) | Acc_1: (99.59%) (11600/11648)\n",
      "Epoch: 104 | Batch_idx: 100 |  Loss_1: (0.0143) | Acc_1: (99.60%) (12876/12928)\n",
      "Epoch: 104 | Batch_idx: 110 |  Loss_1: (0.0140) | Acc_1: (99.61%) (14152/14208)\n",
      "Epoch: 104 | Batch_idx: 120 |  Loss_1: (0.0136) | Acc_1: (99.62%) (15429/15488)\n",
      "Epoch: 104 | Batch_idx: 130 |  Loss_1: (0.0132) | Acc_1: (99.62%) (16705/16768)\n",
      "Epoch: 104 | Batch_idx: 140 |  Loss_1: (0.0128) | Acc_1: (99.63%) (17981/18048)\n",
      "Epoch: 104 | Batch_idx: 150 |  Loss_1: (0.0126) | Acc_1: (99.64%) (19259/19328)\n",
      "Epoch: 104 | Batch_idx: 160 |  Loss_1: (0.0121) | Acc_1: (99.66%) (20538/20608)\n",
      "Epoch: 104 | Batch_idx: 170 |  Loss_1: (0.0121) | Acc_1: (99.65%) (21812/21888)\n",
      "Epoch: 104 | Batch_idx: 180 |  Loss_1: (0.0117) | Acc_1: (99.66%) (23089/23168)\n",
      "Epoch: 104 | Batch_idx: 190 |  Loss_1: (0.0120) | Acc_1: (99.64%) (24360/24448)\n",
      "Epoch: 104 | Batch_idx: 200 |  Loss_1: (0.0118) | Acc_1: (99.63%) (25634/25728)\n",
      "Epoch: 104 | Batch_idx: 210 |  Loss_1: (0.0117) | Acc_1: (99.64%) (26911/27008)\n",
      "Epoch: 104 | Batch_idx: 220 |  Loss_1: (0.0117) | Acc_1: (99.63%) (28184/28288)\n",
      "Epoch: 104 | Batch_idx: 230 |  Loss_1: (0.0118) | Acc_1: (99.62%) (29457/29568)\n",
      "Epoch: 104 | Batch_idx: 240 |  Loss_1: (0.0119) | Acc_1: (99.62%) (30731/30848)\n",
      "Epoch: 104 | Batch_idx: 250 |  Loss_1: (0.0120) | Acc_1: (99.62%) (32005/32128)\n",
      "Epoch: 104 | Batch_idx: 260 |  Loss_1: (0.0124) | Acc_1: (99.59%) (33271/33408)\n",
      "Epoch: 104 | Batch_idx: 270 |  Loss_1: (0.0127) | Acc_1: (99.58%) (34543/34688)\n",
      "Epoch: 104 | Batch_idx: 280 |  Loss_1: (0.0127) | Acc_1: (99.58%) (35816/35968)\n",
      "Epoch: 104 | Batch_idx: 290 |  Loss_1: (0.0128) | Acc_1: (99.57%) (37089/37248)\n",
      "Epoch: 104 | Batch_idx: 300 |  Loss_1: (0.0128) | Acc_1: (99.57%) (38363/38528)\n",
      "Epoch: 104 | Batch_idx: 310 |  Loss_1: (0.0129) | Acc_1: (99.57%) (39637/39808)\n",
      "Epoch: 104 | Batch_idx: 320 |  Loss_1: (0.0131) | Acc_1: (99.56%) (40907/41088)\n",
      "Epoch: 104 | Batch_idx: 330 |  Loss_1: (0.0131) | Acc_1: (99.56%) (42181/42368)\n",
      "Epoch: 104 | Batch_idx: 340 |  Loss_1: (0.0131) | Acc_1: (99.56%) (43457/43648)\n",
      "Epoch: 104 | Batch_idx: 350 |  Loss_1: (0.0130) | Acc_1: (99.56%) (44730/44928)\n",
      "Epoch: 104 | Batch_idx: 360 |  Loss_1: (0.0128) | Acc_1: (99.56%) (46006/46208)\n",
      "Epoch: 104 | Batch_idx: 370 |  Loss_1: (0.0127) | Acc_1: (99.57%) (47282/47488)\n",
      "Epoch: 104 | Batch_idx: 380 |  Loss_1: (0.0126) | Acc_1: (99.57%) (48559/48768)\n",
      "Epoch: 104 | Batch_idx: 390 |  Loss_1: (0.0126) | Acc_1: (99.58%) (49788/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5381) | Acc: (91.15%) (9115/10000)\n",
      "Epoch: 105 | Batch_idx: 0 |  Loss_1: (0.0612) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 105 | Batch_idx: 10 |  Loss_1: (0.0188) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 105 | Batch_idx: 20 |  Loss_1: (0.0156) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 105 | Batch_idx: 30 |  Loss_1: (0.0123) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 105 | Batch_idx: 40 |  Loss_1: (0.0128) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 105 | Batch_idx: 50 |  Loss_1: (0.0121) | Acc_1: (99.62%) (6503/6528)\n",
      "Epoch: 105 | Batch_idx: 60 |  Loss_1: (0.0114) | Acc_1: (99.65%) (7781/7808)\n",
      "Epoch: 105 | Batch_idx: 70 |  Loss_1: (0.0114) | Acc_1: (99.66%) (9057/9088)\n",
      "Epoch: 105 | Batch_idx: 80 |  Loss_1: (0.0114) | Acc_1: (99.65%) (10332/10368)\n",
      "Epoch: 105 | Batch_idx: 90 |  Loss_1: (0.0117) | Acc_1: (99.64%) (11606/11648)\n",
      "Epoch: 105 | Batch_idx: 100 |  Loss_1: (0.0121) | Acc_1: (99.62%) (12879/12928)\n",
      "Epoch: 105 | Batch_idx: 110 |  Loss_1: (0.0116) | Acc_1: (99.64%) (14157/14208)\n",
      "Epoch: 105 | Batch_idx: 120 |  Loss_1: (0.0112) | Acc_1: (99.66%) (15435/15488)\n",
      "Epoch: 105 | Batch_idx: 130 |  Loss_1: (0.0114) | Acc_1: (99.64%) (16707/16768)\n",
      "Epoch: 105 | Batch_idx: 140 |  Loss_1: (0.0110) | Acc_1: (99.65%) (17984/18048)\n",
      "Epoch: 105 | Batch_idx: 150 |  Loss_1: (0.0112) | Acc_1: (99.64%) (19258/19328)\n",
      "Epoch: 105 | Batch_idx: 160 |  Loss_1: (0.0109) | Acc_1: (99.65%) (20536/20608)\n",
      "Epoch: 105 | Batch_idx: 170 |  Loss_1: (0.0106) | Acc_1: (99.65%) (21812/21888)\n",
      "Epoch: 105 | Batch_idx: 180 |  Loss_1: (0.0107) | Acc_1: (99.65%) (23086/23168)\n",
      "Epoch: 105 | Batch_idx: 190 |  Loss_1: (0.0107) | Acc_1: (99.65%) (24362/24448)\n",
      "Epoch: 105 | Batch_idx: 200 |  Loss_1: (0.0108) | Acc_1: (99.65%) (25638/25728)\n",
      "Epoch: 105 | Batch_idx: 210 |  Loss_1: (0.0107) | Acc_1: (99.66%) (26915/27008)\n",
      "Epoch: 105 | Batch_idx: 220 |  Loss_1: (0.0105) | Acc_1: (99.66%) (28192/28288)\n",
      "Epoch: 105 | Batch_idx: 230 |  Loss_1: (0.0105) | Acc_1: (99.66%) (29467/29568)\n",
      "Epoch: 105 | Batch_idx: 240 |  Loss_1: (0.0104) | Acc_1: (99.67%) (30745/30848)\n",
      "Epoch: 105 | Batch_idx: 250 |  Loss_1: (0.0105) | Acc_1: (99.66%) (32020/32128)\n",
      "Epoch: 105 | Batch_idx: 260 |  Loss_1: (0.0106) | Acc_1: (99.66%) (33296/33408)\n",
      "Epoch: 105 | Batch_idx: 270 |  Loss_1: (0.0104) | Acc_1: (99.67%) (34573/34688)\n",
      "Epoch: 105 | Batch_idx: 280 |  Loss_1: (0.0102) | Acc_1: (99.67%) (35850/35968)\n",
      "Epoch: 105 | Batch_idx: 290 |  Loss_1: (0.0103) | Acc_1: (99.67%) (37126/37248)\n",
      "Epoch: 105 | Batch_idx: 300 |  Loss_1: (0.0102) | Acc_1: (99.67%) (38402/38528)\n",
      "Epoch: 105 | Batch_idx: 310 |  Loss_1: (0.0102) | Acc_1: (99.68%) (39679/39808)\n",
      "Epoch: 105 | Batch_idx: 320 |  Loss_1: (0.0104) | Acc_1: (99.67%) (40952/41088)\n",
      "Epoch: 105 | Batch_idx: 330 |  Loss_1: (0.0103) | Acc_1: (99.67%) (42228/42368)\n",
      "Epoch: 105 | Batch_idx: 340 |  Loss_1: (0.0102) | Acc_1: (99.67%) (43505/43648)\n",
      "Epoch: 105 | Batch_idx: 350 |  Loss_1: (0.0103) | Acc_1: (99.67%) (44780/44928)\n",
      "Epoch: 105 | Batch_idx: 360 |  Loss_1: (0.0105) | Acc_1: (99.66%) (46052/46208)\n",
      "Epoch: 105 | Batch_idx: 370 |  Loss_1: (0.0105) | Acc_1: (99.65%) (47324/47488)\n",
      "Epoch: 105 | Batch_idx: 380 |  Loss_1: (0.0107) | Acc_1: (99.65%) (48595/48768)\n",
      "Epoch: 105 | Batch_idx: 390 |  Loss_1: (0.0107) | Acc_1: (99.64%) (49822/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5555) | Acc: (91.13%) (9113/10000)\n",
      "Epoch: 106 | Batch_idx: 0 |  Loss_1: (0.0028) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 106 | Batch_idx: 10 |  Loss_1: (0.0105) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 106 | Batch_idx: 20 |  Loss_1: (0.0096) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 106 | Batch_idx: 30 |  Loss_1: (0.0119) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 106 | Batch_idx: 40 |  Loss_1: (0.0107) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 106 | Batch_idx: 50 |  Loss_1: (0.0115) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 106 | Batch_idx: 60 |  Loss_1: (0.0108) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 106 | Batch_idx: 70 |  Loss_1: (0.0108) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 106 | Batch_idx: 80 |  Loss_1: (0.0099) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 106 | Batch_idx: 90 |  Loss_1: (0.0102) | Acc_1: (99.73%) (11617/11648)\n",
      "Epoch: 106 | Batch_idx: 100 |  Loss_1: (0.0102) | Acc_1: (99.71%) (12891/12928)\n",
      "Epoch: 106 | Batch_idx: 110 |  Loss_1: (0.0103) | Acc_1: (99.71%) (14167/14208)\n",
      "Epoch: 106 | Batch_idx: 120 |  Loss_1: (0.0102) | Acc_1: (99.70%) (15442/15488)\n",
      "Epoch: 106 | Batch_idx: 130 |  Loss_1: (0.0103) | Acc_1: (99.70%) (16718/16768)\n",
      "Epoch: 106 | Batch_idx: 140 |  Loss_1: (0.0102) | Acc_1: (99.70%) (17993/18048)\n",
      "Epoch: 106 | Batch_idx: 150 |  Loss_1: (0.0108) | Acc_1: (99.67%) (19265/19328)\n",
      "Epoch: 106 | Batch_idx: 160 |  Loss_1: (0.0115) | Acc_1: (99.66%) (20538/20608)\n",
      "Epoch: 106 | Batch_idx: 170 |  Loss_1: (0.0117) | Acc_1: (99.66%) (21813/21888)\n",
      "Epoch: 106 | Batch_idx: 180 |  Loss_1: (0.0114) | Acc_1: (99.66%) (23090/23168)\n",
      "Epoch: 106 | Batch_idx: 190 |  Loss_1: (0.0114) | Acc_1: (99.66%) (24365/24448)\n",
      "Epoch: 106 | Batch_idx: 200 |  Loss_1: (0.0113) | Acc_1: (99.65%) (25638/25728)\n",
      "Epoch: 106 | Batch_idx: 210 |  Loss_1: (0.0113) | Acc_1: (99.64%) (26911/27008)\n",
      "Epoch: 106 | Batch_idx: 220 |  Loss_1: (0.0113) | Acc_1: (99.64%) (28186/28288)\n",
      "Epoch: 106 | Batch_idx: 230 |  Loss_1: (0.0112) | Acc_1: (99.64%) (29463/29568)\n",
      "Epoch: 106 | Batch_idx: 240 |  Loss_1: (0.0115) | Acc_1: (99.63%) (30734/30848)\n",
      "Epoch: 106 | Batch_idx: 250 |  Loss_1: (0.0116) | Acc_1: (99.63%) (32009/32128)\n",
      "Epoch: 106 | Batch_idx: 260 |  Loss_1: (0.0115) | Acc_1: (99.63%) (33284/33408)\n",
      "Epoch: 106 | Batch_idx: 270 |  Loss_1: (0.0115) | Acc_1: (99.63%) (34559/34688)\n",
      "Epoch: 106 | Batch_idx: 280 |  Loss_1: (0.0114) | Acc_1: (99.63%) (35835/35968)\n",
      "Epoch: 106 | Batch_idx: 290 |  Loss_1: (0.0113) | Acc_1: (99.64%) (37113/37248)\n",
      "Epoch: 106 | Batch_idx: 300 |  Loss_1: (0.0113) | Acc_1: (99.63%) (38387/38528)\n",
      "Epoch: 106 | Batch_idx: 310 |  Loss_1: (0.0110) | Acc_1: (99.64%) (39665/39808)\n",
      "Epoch: 106 | Batch_idx: 320 |  Loss_1: (0.0112) | Acc_1: (99.63%) (40938/41088)\n",
      "Epoch: 106 | Batch_idx: 330 |  Loss_1: (0.0112) | Acc_1: (99.63%) (42213/42368)\n",
      "Epoch: 106 | Batch_idx: 340 |  Loss_1: (0.0111) | Acc_1: (99.64%) (43490/43648)\n",
      "Epoch: 106 | Batch_idx: 350 |  Loss_1: (0.0111) | Acc_1: (99.64%) (44766/44928)\n",
      "Epoch: 106 | Batch_idx: 360 |  Loss_1: (0.0111) | Acc_1: (99.64%) (46040/46208)\n",
      "Epoch: 106 | Batch_idx: 370 |  Loss_1: (0.0111) | Acc_1: (99.63%) (47313/47488)\n",
      "Epoch: 106 | Batch_idx: 380 |  Loss_1: (0.0112) | Acc_1: (99.63%) (48589/48768)\n",
      "Epoch: 106 | Batch_idx: 390 |  Loss_1: (0.0111) | Acc_1: (99.64%) (49818/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5416) | Acc: (91.19%) (9119/10000)\n",
      "Epoch: 107 | Batch_idx: 0 |  Loss_1: (0.0140) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 107 | Batch_idx: 10 |  Loss_1: (0.0095) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 107 | Batch_idx: 20 |  Loss_1: (0.0079) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 107 | Batch_idx: 30 |  Loss_1: (0.0085) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 107 | Batch_idx: 40 |  Loss_1: (0.0086) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 107 | Batch_idx: 50 |  Loss_1: (0.0086) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 107 | Batch_idx: 60 |  Loss_1: (0.0087) | Acc_1: (99.69%) (7784/7808)\n",
      "Epoch: 107 | Batch_idx: 70 |  Loss_1: (0.0084) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 107 | Batch_idx: 80 |  Loss_1: (0.0087) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 107 | Batch_idx: 90 |  Loss_1: (0.0088) | Acc_1: (99.68%) (11611/11648)\n",
      "Epoch: 107 | Batch_idx: 100 |  Loss_1: (0.0098) | Acc_1: (99.66%) (12884/12928)\n",
      "Epoch: 107 | Batch_idx: 110 |  Loss_1: (0.0099) | Acc_1: (99.66%) (14160/14208)\n",
      "Epoch: 107 | Batch_idx: 120 |  Loss_1: (0.0102) | Acc_1: (99.64%) (15432/15488)\n",
      "Epoch: 107 | Batch_idx: 130 |  Loss_1: (0.0102) | Acc_1: (99.64%) (16707/16768)\n",
      "Epoch: 107 | Batch_idx: 140 |  Loss_1: (0.0102) | Acc_1: (99.64%) (17983/18048)\n",
      "Epoch: 107 | Batch_idx: 150 |  Loss_1: (0.0106) | Acc_1: (99.61%) (19253/19328)\n",
      "Epoch: 107 | Batch_idx: 160 |  Loss_1: (0.0110) | Acc_1: (99.61%) (20527/20608)\n",
      "Epoch: 107 | Batch_idx: 170 |  Loss_1: (0.0112) | Acc_1: (99.60%) (21800/21888)\n",
      "Epoch: 107 | Batch_idx: 180 |  Loss_1: (0.0111) | Acc_1: (99.60%) (23076/23168)\n",
      "Epoch: 107 | Batch_idx: 190 |  Loss_1: (0.0114) | Acc_1: (99.60%) (24349/24448)\n",
      "Epoch: 107 | Batch_idx: 200 |  Loss_1: (0.0115) | Acc_1: (99.60%) (25624/25728)\n",
      "Epoch: 107 | Batch_idx: 210 |  Loss_1: (0.0117) | Acc_1: (99.59%) (26897/27008)\n",
      "Epoch: 107 | Batch_idx: 220 |  Loss_1: (0.0117) | Acc_1: (99.59%) (28173/28288)\n",
      "Epoch: 107 | Batch_idx: 230 |  Loss_1: (0.0113) | Acc_1: (99.61%) (29453/29568)\n",
      "Epoch: 107 | Batch_idx: 240 |  Loss_1: (0.0113) | Acc_1: (99.61%) (30727/30848)\n",
      "Epoch: 107 | Batch_idx: 250 |  Loss_1: (0.0112) | Acc_1: (99.61%) (32002/32128)\n",
      "Epoch: 107 | Batch_idx: 260 |  Loss_1: (0.0114) | Acc_1: (99.60%) (33275/33408)\n",
      "Epoch: 107 | Batch_idx: 270 |  Loss_1: (0.0112) | Acc_1: (99.61%) (34552/34688)\n",
      "Epoch: 107 | Batch_idx: 280 |  Loss_1: (0.0113) | Acc_1: (99.60%) (35824/35968)\n",
      "Epoch: 107 | Batch_idx: 290 |  Loss_1: (0.0113) | Acc_1: (99.61%) (37102/37248)\n",
      "Epoch: 107 | Batch_idx: 300 |  Loss_1: (0.0114) | Acc_1: (99.61%) (38376/38528)\n",
      "Epoch: 107 | Batch_idx: 310 |  Loss_1: (0.0114) | Acc_1: (99.60%) (39650/39808)\n",
      "Epoch: 107 | Batch_idx: 320 |  Loss_1: (0.0119) | Acc_1: (99.59%) (40920/41088)\n",
      "Epoch: 107 | Batch_idx: 330 |  Loss_1: (0.0119) | Acc_1: (99.59%) (42194/42368)\n",
      "Epoch: 107 | Batch_idx: 340 |  Loss_1: (0.0120) | Acc_1: (99.59%) (43468/43648)\n",
      "Epoch: 107 | Batch_idx: 350 |  Loss_1: (0.0120) | Acc_1: (99.59%) (44743/44928)\n",
      "Epoch: 107 | Batch_idx: 360 |  Loss_1: (0.0127) | Acc_1: (99.58%) (46012/46208)\n",
      "Epoch: 107 | Batch_idx: 370 |  Loss_1: (0.0126) | Acc_1: (99.58%) (47287/47488)\n",
      "Epoch: 107 | Batch_idx: 380 |  Loss_1: (0.0125) | Acc_1: (99.58%) (48564/48768)\n",
      "Epoch: 107 | Batch_idx: 390 |  Loss_1: (0.0126) | Acc_1: (99.58%) (49789/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5777) | Acc: (90.81%) (9081/10000)\n",
      "Epoch: 108 | Batch_idx: 0 |  Loss_1: (0.0042) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 108 | Batch_idx: 10 |  Loss_1: (0.0089) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 108 | Batch_idx: 20 |  Loss_1: (0.0107) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 108 | Batch_idx: 30 |  Loss_1: (0.0112) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 108 | Batch_idx: 40 |  Loss_1: (0.0121) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 108 | Batch_idx: 50 |  Loss_1: (0.0105) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 108 | Batch_idx: 60 |  Loss_1: (0.0092) | Acc_1: (99.71%) (7785/7808)\n",
      "Epoch: 108 | Batch_idx: 70 |  Loss_1: (0.0092) | Acc_1: (99.69%) (9060/9088)\n",
      "Epoch: 108 | Batch_idx: 80 |  Loss_1: (0.0095) | Acc_1: (99.67%) (10334/10368)\n",
      "Epoch: 108 | Batch_idx: 90 |  Loss_1: (0.0100) | Acc_1: (99.68%) (11611/11648)\n",
      "Epoch: 108 | Batch_idx: 100 |  Loss_1: (0.0097) | Acc_1: (99.69%) (12888/12928)\n",
      "Epoch: 108 | Batch_idx: 110 |  Loss_1: (0.0097) | Acc_1: (99.68%) (14163/14208)\n",
      "Epoch: 108 | Batch_idx: 120 |  Loss_1: (0.0104) | Acc_1: (99.68%) (15438/15488)\n",
      "Epoch: 108 | Batch_idx: 130 |  Loss_1: (0.0101) | Acc_1: (99.68%) (16715/16768)\n",
      "Epoch: 108 | Batch_idx: 140 |  Loss_1: (0.0105) | Acc_1: (99.67%) (17989/18048)\n",
      "Epoch: 108 | Batch_idx: 150 |  Loss_1: (0.0103) | Acc_1: (99.68%) (19266/19328)\n",
      "Epoch: 108 | Batch_idx: 160 |  Loss_1: (0.0106) | Acc_1: (99.67%) (20539/20608)\n",
      "Epoch: 108 | Batch_idx: 170 |  Loss_1: (0.0105) | Acc_1: (99.66%) (21814/21888)\n",
      "Epoch: 108 | Batch_idx: 180 |  Loss_1: (0.0103) | Acc_1: (99.67%) (23091/23168)\n",
      "Epoch: 108 | Batch_idx: 190 |  Loss_1: (0.0104) | Acc_1: (99.66%) (24364/24448)\n",
      "Epoch: 108 | Batch_idx: 200 |  Loss_1: (0.0103) | Acc_1: (99.66%) (25641/25728)\n",
      "Epoch: 108 | Batch_idx: 210 |  Loss_1: (0.0105) | Acc_1: (99.66%) (26916/27008)\n",
      "Epoch: 108 | Batch_idx: 220 |  Loss_1: (0.0106) | Acc_1: (99.65%) (28189/28288)\n",
      "Epoch: 108 | Batch_idx: 230 |  Loss_1: (0.0104) | Acc_1: (99.66%) (29467/29568)\n",
      "Epoch: 108 | Batch_idx: 240 |  Loss_1: (0.0105) | Acc_1: (99.66%) (30742/30848)\n",
      "Epoch: 108 | Batch_idx: 250 |  Loss_1: (0.0105) | Acc_1: (99.65%) (32016/32128)\n",
      "Epoch: 108 | Batch_idx: 260 |  Loss_1: (0.0104) | Acc_1: (99.65%) (33292/33408)\n",
      "Epoch: 108 | Batch_idx: 270 |  Loss_1: (0.0107) | Acc_1: (99.65%) (34567/34688)\n",
      "Epoch: 108 | Batch_idx: 280 |  Loss_1: (0.0106) | Acc_1: (99.65%) (35842/35968)\n",
      "Epoch: 108 | Batch_idx: 290 |  Loss_1: (0.0106) | Acc_1: (99.65%) (37117/37248)\n",
      "Epoch: 108 | Batch_idx: 300 |  Loss_1: (0.0106) | Acc_1: (99.65%) (38392/38528)\n",
      "Epoch: 108 | Batch_idx: 310 |  Loss_1: (0.0105) | Acc_1: (99.65%) (39669/39808)\n",
      "Epoch: 108 | Batch_idx: 320 |  Loss_1: (0.0103) | Acc_1: (99.66%) (40948/41088)\n",
      "Epoch: 108 | Batch_idx: 330 |  Loss_1: (0.0102) | Acc_1: (99.66%) (42224/42368)\n",
      "Epoch: 108 | Batch_idx: 340 |  Loss_1: (0.0102) | Acc_1: (99.66%) (43500/43648)\n",
      "Epoch: 108 | Batch_idx: 350 |  Loss_1: (0.0104) | Acc_1: (99.66%) (44774/44928)\n",
      "Epoch: 108 | Batch_idx: 360 |  Loss_1: (0.0104) | Acc_1: (99.65%) (46047/46208)\n",
      "Epoch: 108 | Batch_idx: 370 |  Loss_1: (0.0104) | Acc_1: (99.65%) (47324/47488)\n",
      "Epoch: 108 | Batch_idx: 380 |  Loss_1: (0.0105) | Acc_1: (99.65%) (48598/48768)\n",
      "Epoch: 108 | Batch_idx: 390 |  Loss_1: (0.0105) | Acc_1: (99.65%) (49824/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5295) | Acc: (91.37%) (9137/10000)\n",
      "Epoch: 109 | Batch_idx: 0 |  Loss_1: (0.0103) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 109 | Batch_idx: 10 |  Loss_1: (0.0100) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 109 | Batch_idx: 20 |  Loss_1: (0.0096) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 109 | Batch_idx: 30 |  Loss_1: (0.0093) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 109 | Batch_idx: 40 |  Loss_1: (0.0092) | Acc_1: (99.71%) (5233/5248)\n",
      "Epoch: 109 | Batch_idx: 50 |  Loss_1: (0.0093) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 109 | Batch_idx: 60 |  Loss_1: (0.0093) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 109 | Batch_idx: 70 |  Loss_1: (0.0096) | Acc_1: (99.68%) (9059/9088)\n",
      "Epoch: 109 | Batch_idx: 80 |  Loss_1: (0.0095) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 109 | Batch_idx: 90 |  Loss_1: (0.0088) | Acc_1: (99.72%) (11615/11648)\n",
      "Epoch: 109 | Batch_idx: 100 |  Loss_1: (0.0093) | Acc_1: (99.68%) (12887/12928)\n",
      "Epoch: 109 | Batch_idx: 110 |  Loss_1: (0.0090) | Acc_1: (99.70%) (14166/14208)\n",
      "Epoch: 109 | Batch_idx: 120 |  Loss_1: (0.0092) | Acc_1: (99.70%) (15442/15488)\n",
      "Epoch: 109 | Batch_idx: 130 |  Loss_1: (0.0095) | Acc_1: (99.68%) (16715/16768)\n",
      "Epoch: 109 | Batch_idx: 140 |  Loss_1: (0.0099) | Acc_1: (99.65%) (17985/18048)\n",
      "Epoch: 109 | Batch_idx: 150 |  Loss_1: (0.0100) | Acc_1: (99.64%) (19258/19328)\n",
      "Epoch: 109 | Batch_idx: 160 |  Loss_1: (0.0099) | Acc_1: (99.64%) (20534/20608)\n",
      "Epoch: 109 | Batch_idx: 170 |  Loss_1: (0.0099) | Acc_1: (99.64%) (21810/21888)\n",
      "Epoch: 109 | Batch_idx: 180 |  Loss_1: (0.0100) | Acc_1: (99.64%) (23084/23168)\n",
      "Epoch: 109 | Batch_idx: 190 |  Loss_1: (0.0099) | Acc_1: (99.64%) (24359/24448)\n",
      "Epoch: 109 | Batch_idx: 200 |  Loss_1: (0.0100) | Acc_1: (99.64%) (25636/25728)\n",
      "Epoch: 109 | Batch_idx: 210 |  Loss_1: (0.0099) | Acc_1: (99.64%) (26911/27008)\n",
      "Epoch: 109 | Batch_idx: 220 |  Loss_1: (0.0098) | Acc_1: (99.64%) (28187/28288)\n",
      "Epoch: 109 | Batch_idx: 230 |  Loss_1: (0.0100) | Acc_1: (99.65%) (29464/29568)\n",
      "Epoch: 109 | Batch_idx: 240 |  Loss_1: (0.0102) | Acc_1: (99.64%) (30738/30848)\n",
      "Epoch: 109 | Batch_idx: 250 |  Loss_1: (0.0102) | Acc_1: (99.64%) (32013/32128)\n",
      "Epoch: 109 | Batch_idx: 260 |  Loss_1: (0.0101) | Acc_1: (99.65%) (33291/33408)\n",
      "Epoch: 109 | Batch_idx: 270 |  Loss_1: (0.0099) | Acc_1: (99.66%) (34570/34688)\n",
      "Epoch: 109 | Batch_idx: 280 |  Loss_1: (0.0098) | Acc_1: (99.66%) (35847/35968)\n",
      "Epoch: 109 | Batch_idx: 290 |  Loss_1: (0.0098) | Acc_1: (99.67%) (37125/37248)\n",
      "Epoch: 109 | Batch_idx: 300 |  Loss_1: (0.0099) | Acc_1: (99.66%) (38398/38528)\n",
      "Epoch: 109 | Batch_idx: 310 |  Loss_1: (0.0100) | Acc_1: (99.65%) (39670/39808)\n",
      "Epoch: 109 | Batch_idx: 320 |  Loss_1: (0.0099) | Acc_1: (99.66%) (40948/41088)\n",
      "Epoch: 109 | Batch_idx: 330 |  Loss_1: (0.0099) | Acc_1: (99.66%) (42224/42368)\n",
      "Epoch: 109 | Batch_idx: 340 |  Loss_1: (0.0100) | Acc_1: (99.66%) (43499/43648)\n",
      "Epoch: 109 | Batch_idx: 350 |  Loss_1: (0.0102) | Acc_1: (99.66%) (44773/44928)\n",
      "Epoch: 109 | Batch_idx: 360 |  Loss_1: (0.0103) | Acc_1: (99.65%) (46044/46208)\n",
      "Epoch: 109 | Batch_idx: 370 |  Loss_1: (0.0103) | Acc_1: (99.64%) (47319/47488)\n",
      "Epoch: 109 | Batch_idx: 380 |  Loss_1: (0.0107) | Acc_1: (99.64%) (48590/48768)\n",
      "Epoch: 109 | Batch_idx: 390 |  Loss_1: (0.0106) | Acc_1: (99.64%) (49821/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5430) | Acc: (91.05%) (9105/10000)\n",
      "Epoch: 110 | Batch_idx: 0 |  Loss_1: (0.0074) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 110 | Batch_idx: 10 |  Loss_1: (0.0165) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 110 | Batch_idx: 20 |  Loss_1: (0.0166) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 110 | Batch_idx: 30 |  Loss_1: (0.0168) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 110 | Batch_idx: 40 |  Loss_1: (0.0179) | Acc_1: (99.52%) (5223/5248)\n",
      "Epoch: 110 | Batch_idx: 50 |  Loss_1: (0.0162) | Acc_1: (99.56%) (6499/6528)\n",
      "Epoch: 110 | Batch_idx: 60 |  Loss_1: (0.0185) | Acc_1: (99.47%) (7767/7808)\n",
      "Epoch: 110 | Batch_idx: 70 |  Loss_1: (0.0188) | Acc_1: (99.46%) (9039/9088)\n",
      "Epoch: 110 | Batch_idx: 80 |  Loss_1: (0.0187) | Acc_1: (99.46%) (10312/10368)\n",
      "Epoch: 110 | Batch_idx: 90 |  Loss_1: (0.0188) | Acc_1: (99.41%) (11579/11648)\n",
      "Epoch: 110 | Batch_idx: 100 |  Loss_1: (0.0179) | Acc_1: (99.44%) (12855/12928)\n",
      "Epoch: 110 | Batch_idx: 110 |  Loss_1: (0.0176) | Acc_1: (99.44%) (14129/14208)\n",
      "Epoch: 110 | Batch_idx: 120 |  Loss_1: (0.0174) | Acc_1: (99.44%) (15401/15488)\n",
      "Epoch: 110 | Batch_idx: 130 |  Loss_1: (0.0168) | Acc_1: (99.46%) (16677/16768)\n",
      "Epoch: 110 | Batch_idx: 140 |  Loss_1: (0.0165) | Acc_1: (99.47%) (17952/18048)\n",
      "Epoch: 110 | Batch_idx: 150 |  Loss_1: (0.0160) | Acc_1: (99.49%) (19230/19328)\n",
      "Epoch: 110 | Batch_idx: 160 |  Loss_1: (0.0157) | Acc_1: (99.50%) (20505/20608)\n",
      "Epoch: 110 | Batch_idx: 170 |  Loss_1: (0.0154) | Acc_1: (99.51%) (21781/21888)\n",
      "Epoch: 110 | Batch_idx: 180 |  Loss_1: (0.0153) | Acc_1: (99.49%) (23051/23168)\n",
      "Epoch: 110 | Batch_idx: 190 |  Loss_1: (0.0155) | Acc_1: (99.49%) (24324/24448)\n",
      "Epoch: 110 | Batch_idx: 200 |  Loss_1: (0.0153) | Acc_1: (99.50%) (25600/25728)\n",
      "Epoch: 110 | Batch_idx: 210 |  Loss_1: (0.0150) | Acc_1: (99.51%) (26877/27008)\n",
      "Epoch: 110 | Batch_idx: 220 |  Loss_1: (0.0146) | Acc_1: (99.53%) (28155/28288)\n",
      "Epoch: 110 | Batch_idx: 230 |  Loss_1: (0.0147) | Acc_1: (99.53%) (29429/29568)\n",
      "Epoch: 110 | Batch_idx: 240 |  Loss_1: (0.0144) | Acc_1: (99.54%) (30705/30848)\n",
      "Epoch: 110 | Batch_idx: 250 |  Loss_1: (0.0143) | Acc_1: (99.54%) (31980/32128)\n",
      "Epoch: 110 | Batch_idx: 260 |  Loss_1: (0.0143) | Acc_1: (99.54%) (33254/33408)\n",
      "Epoch: 110 | Batch_idx: 270 |  Loss_1: (0.0144) | Acc_1: (99.54%) (34530/34688)\n",
      "Epoch: 110 | Batch_idx: 280 |  Loss_1: (0.0144) | Acc_1: (99.55%) (35805/35968)\n",
      "Epoch: 110 | Batch_idx: 290 |  Loss_1: (0.0141) | Acc_1: (99.55%) (37082/37248)\n",
      "Epoch: 110 | Batch_idx: 300 |  Loss_1: (0.0139) | Acc_1: (99.56%) (38358/38528)\n",
      "Epoch: 110 | Batch_idx: 310 |  Loss_1: (0.0138) | Acc_1: (99.57%) (39635/39808)\n",
      "Epoch: 110 | Batch_idx: 320 |  Loss_1: (0.0139) | Acc_1: (99.56%) (40908/41088)\n",
      "Epoch: 110 | Batch_idx: 330 |  Loss_1: (0.0140) | Acc_1: (99.55%) (42179/42368)\n",
      "Epoch: 110 | Batch_idx: 340 |  Loss_1: (0.0139) | Acc_1: (99.55%) (43453/43648)\n",
      "Epoch: 110 | Batch_idx: 350 |  Loss_1: (0.0138) | Acc_1: (99.55%) (44728/44928)\n",
      "Epoch: 110 | Batch_idx: 360 |  Loss_1: (0.0138) | Acc_1: (99.55%) (46001/46208)\n",
      "Epoch: 110 | Batch_idx: 370 |  Loss_1: (0.0138) | Acc_1: (99.55%) (47275/47488)\n",
      "Epoch: 110 | Batch_idx: 380 |  Loss_1: (0.0139) | Acc_1: (99.54%) (48546/48768)\n",
      "Epoch: 110 | Batch_idx: 390 |  Loss_1: (0.0137) | Acc_1: (99.55%) (49773/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5492) | Acc: (90.99%) (9099/10000)\n",
      "Epoch: 111 | Batch_idx: 0 |  Loss_1: (0.0363) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 111 | Batch_idx: 10 |  Loss_1: (0.0161) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 111 | Batch_idx: 20 |  Loss_1: (0.0191) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 111 | Batch_idx: 30 |  Loss_1: (0.0152) | Acc_1: (99.52%) (3949/3968)\n",
      "Epoch: 111 | Batch_idx: 40 |  Loss_1: (0.0161) | Acc_1: (99.50%) (5222/5248)\n",
      "Epoch: 111 | Batch_idx: 50 |  Loss_1: (0.0159) | Acc_1: (99.48%) (6494/6528)\n",
      "Epoch: 111 | Batch_idx: 60 |  Loss_1: (0.0155) | Acc_1: (99.49%) (7768/7808)\n",
      "Epoch: 111 | Batch_idx: 70 |  Loss_1: (0.0150) | Acc_1: (99.49%) (9042/9088)\n",
      "Epoch: 111 | Batch_idx: 80 |  Loss_1: (0.0151) | Acc_1: (99.48%) (10314/10368)\n",
      "Epoch: 111 | Batch_idx: 90 |  Loss_1: (0.0146) | Acc_1: (99.48%) (11588/11648)\n",
      "Epoch: 111 | Batch_idx: 100 |  Loss_1: (0.0142) | Acc_1: (99.50%) (12864/12928)\n",
      "Epoch: 111 | Batch_idx: 110 |  Loss_1: (0.0142) | Acc_1: (99.50%) (14137/14208)\n",
      "Epoch: 111 | Batch_idx: 120 |  Loss_1: (0.0139) | Acc_1: (99.51%) (15412/15488)\n",
      "Epoch: 111 | Batch_idx: 130 |  Loss_1: (0.0141) | Acc_1: (99.49%) (16683/16768)\n",
      "Epoch: 111 | Batch_idx: 140 |  Loss_1: (0.0136) | Acc_1: (99.50%) (17958/18048)\n",
      "Epoch: 111 | Batch_idx: 150 |  Loss_1: (0.0135) | Acc_1: (99.50%) (19231/19328)\n",
      "Epoch: 111 | Batch_idx: 160 |  Loss_1: (0.0140) | Acc_1: (99.48%) (20501/20608)\n",
      "Epoch: 111 | Batch_idx: 170 |  Loss_1: (0.0146) | Acc_1: (99.47%) (21772/21888)\n",
      "Epoch: 111 | Batch_idx: 180 |  Loss_1: (0.0145) | Acc_1: (99.48%) (23048/23168)\n",
      "Epoch: 111 | Batch_idx: 190 |  Loss_1: (0.0143) | Acc_1: (99.48%) (24321/24448)\n",
      "Epoch: 111 | Batch_idx: 200 |  Loss_1: (0.0140) | Acc_1: (99.50%) (25599/25728)\n",
      "Epoch: 111 | Batch_idx: 210 |  Loss_1: (0.0142) | Acc_1: (99.50%) (26872/27008)\n",
      "Epoch: 111 | Batch_idx: 220 |  Loss_1: (0.0138) | Acc_1: (99.51%) (28148/28288)\n",
      "Epoch: 111 | Batch_idx: 230 |  Loss_1: (0.0135) | Acc_1: (99.52%) (29426/29568)\n",
      "Epoch: 111 | Batch_idx: 240 |  Loss_1: (0.0137) | Acc_1: (99.51%) (30696/30848)\n",
      "Epoch: 111 | Batch_idx: 250 |  Loss_1: (0.0138) | Acc_1: (99.51%) (31969/32128)\n",
      "Epoch: 111 | Batch_idx: 260 |  Loss_1: (0.0137) | Acc_1: (99.52%) (33246/33408)\n",
      "Epoch: 111 | Batch_idx: 270 |  Loss_1: (0.0137) | Acc_1: (99.51%) (34518/34688)\n",
      "Epoch: 111 | Batch_idx: 280 |  Loss_1: (0.0136) | Acc_1: (99.52%) (35794/35968)\n",
      "Epoch: 111 | Batch_idx: 290 |  Loss_1: (0.0135) | Acc_1: (99.52%) (37070/37248)\n",
      "Epoch: 111 | Batch_idx: 300 |  Loss_1: (0.0135) | Acc_1: (99.52%) (38344/38528)\n",
      "Epoch: 111 | Batch_idx: 310 |  Loss_1: (0.0133) | Acc_1: (99.53%) (39620/39808)\n",
      "Epoch: 111 | Batch_idx: 320 |  Loss_1: (0.0134) | Acc_1: (99.53%) (40893/41088)\n",
      "Epoch: 111 | Batch_idx: 330 |  Loss_1: (0.0136) | Acc_1: (99.52%) (42164/42368)\n",
      "Epoch: 111 | Batch_idx: 340 |  Loss_1: (0.0136) | Acc_1: (99.52%) (43437/43648)\n",
      "Epoch: 111 | Batch_idx: 350 |  Loss_1: (0.0137) | Acc_1: (99.51%) (44707/44928)\n",
      "Epoch: 111 | Batch_idx: 360 |  Loss_1: (0.0137) | Acc_1: (99.50%) (45979/46208)\n",
      "Epoch: 111 | Batch_idx: 370 |  Loss_1: (0.0139) | Acc_1: (99.50%) (47251/47488)\n",
      "Epoch: 111 | Batch_idx: 380 |  Loss_1: (0.0139) | Acc_1: (99.50%) (48523/48768)\n",
      "Epoch: 111 | Batch_idx: 390 |  Loss_1: (0.0139) | Acc_1: (99.50%) (49751/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5462) | Acc: (91.24%) (9124/10000)\n",
      "Epoch: 112 | Batch_idx: 0 |  Loss_1: (0.0072) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 112 | Batch_idx: 10 |  Loss_1: (0.0177) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 112 | Batch_idx: 20 |  Loss_1: (0.0128) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 112 | Batch_idx: 30 |  Loss_1: (0.0102) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 112 | Batch_idx: 40 |  Loss_1: (0.0098) | Acc_1: (99.71%) (5233/5248)\n",
      "Epoch: 112 | Batch_idx: 50 |  Loss_1: (0.0095) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 112 | Batch_idx: 60 |  Loss_1: (0.0102) | Acc_1: (99.67%) (7782/7808)\n",
      "Epoch: 112 | Batch_idx: 70 |  Loss_1: (0.0097) | Acc_1: (99.68%) (9059/9088)\n",
      "Epoch: 112 | Batch_idx: 80 |  Loss_1: (0.0097) | Acc_1: (99.68%) (10335/10368)\n",
      "Epoch: 112 | Batch_idx: 90 |  Loss_1: (0.0096) | Acc_1: (99.67%) (11610/11648)\n",
      "Epoch: 112 | Batch_idx: 100 |  Loss_1: (0.0095) | Acc_1: (99.68%) (12886/12928)\n",
      "Epoch: 112 | Batch_idx: 110 |  Loss_1: (0.0093) | Acc_1: (99.69%) (14164/14208)\n",
      "Epoch: 112 | Batch_idx: 120 |  Loss_1: (0.0092) | Acc_1: (99.70%) (15441/15488)\n",
      "Epoch: 112 | Batch_idx: 130 |  Loss_1: (0.0091) | Acc_1: (99.70%) (16717/16768)\n",
      "Epoch: 112 | Batch_idx: 140 |  Loss_1: (0.0095) | Acc_1: (99.68%) (17991/18048)\n",
      "Epoch: 112 | Batch_idx: 150 |  Loss_1: (0.0097) | Acc_1: (99.68%) (19266/19328)\n",
      "Epoch: 112 | Batch_idx: 160 |  Loss_1: (0.0099) | Acc_1: (99.67%) (20540/20608)\n",
      "Epoch: 112 | Batch_idx: 170 |  Loss_1: (0.0102) | Acc_1: (99.65%) (21811/21888)\n",
      "Epoch: 112 | Batch_idx: 180 |  Loss_1: (0.0107) | Acc_1: (99.62%) (23081/23168)\n",
      "Epoch: 112 | Batch_idx: 190 |  Loss_1: (0.0106) | Acc_1: (99.62%) (24355/24448)\n",
      "Epoch: 112 | Batch_idx: 200 |  Loss_1: (0.0108) | Acc_1: (99.62%) (25630/25728)\n",
      "Epoch: 112 | Batch_idx: 210 |  Loss_1: (0.0108) | Acc_1: (99.62%) (26905/27008)\n",
      "Epoch: 112 | Batch_idx: 220 |  Loss_1: (0.0109) | Acc_1: (99.61%) (28179/28288)\n",
      "Epoch: 112 | Batch_idx: 230 |  Loss_1: (0.0109) | Acc_1: (99.61%) (29453/29568)\n",
      "Epoch: 112 | Batch_idx: 240 |  Loss_1: (0.0109) | Acc_1: (99.61%) (30729/30848)\n",
      "Epoch: 112 | Batch_idx: 250 |  Loss_1: (0.0108) | Acc_1: (99.62%) (32005/32128)\n",
      "Epoch: 112 | Batch_idx: 260 |  Loss_1: (0.0108) | Acc_1: (99.62%) (33281/33408)\n",
      "Epoch: 112 | Batch_idx: 270 |  Loss_1: (0.0107) | Acc_1: (99.62%) (34557/34688)\n",
      "Epoch: 112 | Batch_idx: 280 |  Loss_1: (0.0105) | Acc_1: (99.63%) (35835/35968)\n",
      "Epoch: 112 | Batch_idx: 290 |  Loss_1: (0.0106) | Acc_1: (99.63%) (37109/37248)\n",
      "Epoch: 112 | Batch_idx: 300 |  Loss_1: (0.0106) | Acc_1: (99.63%) (38386/38528)\n",
      "Epoch: 112 | Batch_idx: 310 |  Loss_1: (0.0105) | Acc_1: (99.63%) (39660/39808)\n",
      "Epoch: 112 | Batch_idx: 320 |  Loss_1: (0.0105) | Acc_1: (99.63%) (40937/41088)\n",
      "Epoch: 112 | Batch_idx: 330 |  Loss_1: (0.0104) | Acc_1: (99.63%) (42212/42368)\n",
      "Epoch: 112 | Batch_idx: 340 |  Loss_1: (0.0105) | Acc_1: (99.63%) (43486/43648)\n",
      "Epoch: 112 | Batch_idx: 350 |  Loss_1: (0.0104) | Acc_1: (99.63%) (44760/44928)\n",
      "Epoch: 112 | Batch_idx: 360 |  Loss_1: (0.0104) | Acc_1: (99.63%) (46037/46208)\n",
      "Epoch: 112 | Batch_idx: 370 |  Loss_1: (0.0102) | Acc_1: (99.64%) (47316/47488)\n",
      "Epoch: 112 | Batch_idx: 380 |  Loss_1: (0.0101) | Acc_1: (99.64%) (48594/48768)\n",
      "Epoch: 112 | Batch_idx: 390 |  Loss_1: (0.0101) | Acc_1: (99.65%) (49823/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5470) | Acc: (91.44%) (9144/10000)\n",
      "Epoch: 113 | Batch_idx: 0 |  Loss_1: (0.0082) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 113 | Batch_idx: 10 |  Loss_1: (0.0058) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 113 | Batch_idx: 20 |  Loss_1: (0.0051) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 113 | Batch_idx: 30 |  Loss_1: (0.0055) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 113 | Batch_idx: 40 |  Loss_1: (0.0059) | Acc_1: (99.81%) (5238/5248)\n",
      "Epoch: 113 | Batch_idx: 50 |  Loss_1: (0.0078) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 113 | Batch_idx: 60 |  Loss_1: (0.0075) | Acc_1: (99.71%) (7785/7808)\n",
      "Epoch: 113 | Batch_idx: 70 |  Loss_1: (0.0075) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 113 | Batch_idx: 80 |  Loss_1: (0.0080) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 113 | Batch_idx: 90 |  Loss_1: (0.0077) | Acc_1: (99.71%) (11614/11648)\n",
      "Epoch: 113 | Batch_idx: 100 |  Loss_1: (0.0077) | Acc_1: (99.71%) (12891/12928)\n",
      "Epoch: 113 | Batch_idx: 110 |  Loss_1: (0.0077) | Acc_1: (99.71%) (14167/14208)\n",
      "Epoch: 113 | Batch_idx: 120 |  Loss_1: (0.0077) | Acc_1: (99.72%) (15444/15488)\n",
      "Epoch: 113 | Batch_idx: 130 |  Loss_1: (0.0074) | Acc_1: (99.73%) (16723/16768)\n",
      "Epoch: 113 | Batch_idx: 140 |  Loss_1: (0.0073) | Acc_1: (99.73%) (17999/18048)\n",
      "Epoch: 113 | Batch_idx: 150 |  Loss_1: (0.0073) | Acc_1: (99.74%) (19278/19328)\n",
      "Epoch: 113 | Batch_idx: 160 |  Loss_1: (0.0071) | Acc_1: (99.75%) (20556/20608)\n",
      "Epoch: 113 | Batch_idx: 170 |  Loss_1: (0.0072) | Acc_1: (99.74%) (21830/21888)\n",
      "Epoch: 113 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.74%) (23108/23168)\n",
      "Epoch: 113 | Batch_idx: 190 |  Loss_1: (0.0071) | Acc_1: (99.74%) (24384/24448)\n",
      "Epoch: 113 | Batch_idx: 200 |  Loss_1: (0.0072) | Acc_1: (99.74%) (25662/25728)\n",
      "Epoch: 113 | Batch_idx: 210 |  Loss_1: (0.0072) | Acc_1: (99.74%) (26938/27008)\n",
      "Epoch: 113 | Batch_idx: 220 |  Loss_1: (0.0072) | Acc_1: (99.74%) (28215/28288)\n",
      "Epoch: 113 | Batch_idx: 230 |  Loss_1: (0.0074) | Acc_1: (99.73%) (29489/29568)\n",
      "Epoch: 113 | Batch_idx: 240 |  Loss_1: (0.0074) | Acc_1: (99.74%) (30767/30848)\n",
      "Epoch: 113 | Batch_idx: 250 |  Loss_1: (0.0076) | Acc_1: (99.74%) (32045/32128)\n",
      "Epoch: 113 | Batch_idx: 260 |  Loss_1: (0.0076) | Acc_1: (99.74%) (33320/33408)\n",
      "Epoch: 113 | Batch_idx: 270 |  Loss_1: (0.0080) | Acc_1: (99.73%) (34593/34688)\n",
      "Epoch: 113 | Batch_idx: 280 |  Loss_1: (0.0080) | Acc_1: (99.72%) (35869/35968)\n",
      "Epoch: 113 | Batch_idx: 290 |  Loss_1: (0.0079) | Acc_1: (99.73%) (37147/37248)\n",
      "Epoch: 113 | Batch_idx: 300 |  Loss_1: (0.0080) | Acc_1: (99.72%) (38422/38528)\n",
      "Epoch: 113 | Batch_idx: 310 |  Loss_1: (0.0080) | Acc_1: (99.72%) (39697/39808)\n",
      "Epoch: 113 | Batch_idx: 320 |  Loss_1: (0.0081) | Acc_1: (99.72%) (40973/41088)\n",
      "Epoch: 113 | Batch_idx: 330 |  Loss_1: (0.0083) | Acc_1: (99.72%) (42250/42368)\n",
      "Epoch: 113 | Batch_idx: 340 |  Loss_1: (0.0083) | Acc_1: (99.72%) (43526/43648)\n",
      "Epoch: 113 | Batch_idx: 350 |  Loss_1: (0.0084) | Acc_1: (99.72%) (44800/44928)\n",
      "Epoch: 113 | Batch_idx: 360 |  Loss_1: (0.0084) | Acc_1: (99.71%) (46076/46208)\n",
      "Epoch: 113 | Batch_idx: 370 |  Loss_1: (0.0084) | Acc_1: (99.71%) (47352/47488)\n",
      "Epoch: 113 | Batch_idx: 380 |  Loss_1: (0.0083) | Acc_1: (99.72%) (48630/48768)\n",
      "Epoch: 113 | Batch_idx: 390 |  Loss_1: (0.0084) | Acc_1: (99.71%) (49857/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5433) | Acc: (91.54%) (9154/10000)\n",
      "Epoch: 114 | Batch_idx: 0 |  Loss_1: (0.0010) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 114 | Batch_idx: 10 |  Loss_1: (0.0063) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 114 | Batch_idx: 20 |  Loss_1: (0.0064) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 114 | Batch_idx: 30 |  Loss_1: (0.0071) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 114 | Batch_idx: 40 |  Loss_1: (0.0073) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 114 | Batch_idx: 50 |  Loss_1: (0.0075) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 114 | Batch_idx: 60 |  Loss_1: (0.0081) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 114 | Batch_idx: 70 |  Loss_1: (0.0089) | Acc_1: (99.69%) (9060/9088)\n",
      "Epoch: 114 | Batch_idx: 80 |  Loss_1: (0.0090) | Acc_1: (99.68%) (10335/10368)\n",
      "Epoch: 114 | Batch_idx: 90 |  Loss_1: (0.0091) | Acc_1: (99.68%) (11611/11648)\n",
      "Epoch: 114 | Batch_idx: 100 |  Loss_1: (0.0099) | Acc_1: (99.63%) (12880/12928)\n",
      "Epoch: 114 | Batch_idx: 110 |  Loss_1: (0.0095) | Acc_1: (99.65%) (14158/14208)\n",
      "Epoch: 114 | Batch_idx: 120 |  Loss_1: (0.0097) | Acc_1: (99.63%) (15430/15488)\n",
      "Epoch: 114 | Batch_idx: 130 |  Loss_1: (0.0096) | Acc_1: (99.62%) (16704/16768)\n",
      "Epoch: 114 | Batch_idx: 140 |  Loss_1: (0.0094) | Acc_1: (99.63%) (17982/18048)\n",
      "Epoch: 114 | Batch_idx: 150 |  Loss_1: (0.0101) | Acc_1: (99.62%) (19255/19328)\n",
      "Epoch: 114 | Batch_idx: 160 |  Loss_1: (0.0102) | Acc_1: (99.62%) (20530/20608)\n",
      "Epoch: 114 | Batch_idx: 170 |  Loss_1: (0.0106) | Acc_1: (99.61%) (21802/21888)\n",
      "Epoch: 114 | Batch_idx: 180 |  Loss_1: (0.0107) | Acc_1: (99.62%) (23079/23168)\n",
      "Epoch: 114 | Batch_idx: 190 |  Loss_1: (0.0105) | Acc_1: (99.63%) (24357/24448)\n",
      "Epoch: 114 | Batch_idx: 200 |  Loss_1: (0.0105) | Acc_1: (99.63%) (25633/25728)\n",
      "Epoch: 114 | Batch_idx: 210 |  Loss_1: (0.0106) | Acc_1: (99.64%) (26910/27008)\n",
      "Epoch: 114 | Batch_idx: 220 |  Loss_1: (0.0105) | Acc_1: (99.64%) (28185/28288)\n",
      "Epoch: 114 | Batch_idx: 230 |  Loss_1: (0.0105) | Acc_1: (99.63%) (29458/29568)\n",
      "Epoch: 114 | Batch_idx: 240 |  Loss_1: (0.0108) | Acc_1: (99.62%) (30731/30848)\n",
      "Epoch: 114 | Batch_idx: 250 |  Loss_1: (0.0107) | Acc_1: (99.63%) (32008/32128)\n",
      "Epoch: 114 | Batch_idx: 260 |  Loss_1: (0.0107) | Acc_1: (99.62%) (33282/33408)\n",
      "Epoch: 114 | Batch_idx: 270 |  Loss_1: (0.0108) | Acc_1: (99.61%) (34554/34688)\n",
      "Epoch: 114 | Batch_idx: 280 |  Loss_1: (0.0108) | Acc_1: (99.61%) (35829/35968)\n",
      "Epoch: 114 | Batch_idx: 290 |  Loss_1: (0.0109) | Acc_1: (99.61%) (37104/37248)\n",
      "Epoch: 114 | Batch_idx: 300 |  Loss_1: (0.0111) | Acc_1: (99.61%) (38378/38528)\n",
      "Epoch: 114 | Batch_idx: 310 |  Loss_1: (0.0112) | Acc_1: (99.61%) (39651/39808)\n",
      "Epoch: 114 | Batch_idx: 320 |  Loss_1: (0.0112) | Acc_1: (99.60%) (40925/41088)\n",
      "Epoch: 114 | Batch_idx: 330 |  Loss_1: (0.0112) | Acc_1: (99.60%) (42199/42368)\n",
      "Epoch: 114 | Batch_idx: 340 |  Loss_1: (0.0111) | Acc_1: (99.61%) (43476/43648)\n",
      "Epoch: 114 | Batch_idx: 350 |  Loss_1: (0.0112) | Acc_1: (99.60%) (44748/44928)\n",
      "Epoch: 114 | Batch_idx: 360 |  Loss_1: (0.0114) | Acc_1: (99.59%) (46020/46208)\n",
      "Epoch: 114 | Batch_idx: 370 |  Loss_1: (0.0115) | Acc_1: (99.59%) (47294/47488)\n",
      "Epoch: 114 | Batch_idx: 380 |  Loss_1: (0.0116) | Acc_1: (99.59%) (48566/48768)\n",
      "Epoch: 114 | Batch_idx: 390 |  Loss_1: (0.0119) | Acc_1: (99.58%) (49790/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5393) | Acc: (91.21%) (9121/10000)\n",
      "Epoch: 115 | Batch_idx: 0 |  Loss_1: (0.0236) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 115 | Batch_idx: 10 |  Loss_1: (0.0168) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 115 | Batch_idx: 20 |  Loss_1: (0.0162) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 115 | Batch_idx: 30 |  Loss_1: (0.0154) | Acc_1: (99.50%) (3948/3968)\n",
      "Epoch: 115 | Batch_idx: 40 |  Loss_1: (0.0128) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 115 | Batch_idx: 50 |  Loss_1: (0.0130) | Acc_1: (99.57%) (6500/6528)\n",
      "Epoch: 115 | Batch_idx: 60 |  Loss_1: (0.0132) | Acc_1: (99.56%) (7774/7808)\n",
      "Epoch: 115 | Batch_idx: 70 |  Loss_1: (0.0125) | Acc_1: (99.58%) (9050/9088)\n",
      "Epoch: 115 | Batch_idx: 80 |  Loss_1: (0.0122) | Acc_1: (99.58%) (10324/10368)\n",
      "Epoch: 115 | Batch_idx: 90 |  Loss_1: (0.0123) | Acc_1: (99.58%) (11599/11648)\n",
      "Epoch: 115 | Batch_idx: 100 |  Loss_1: (0.0123) | Acc_1: (99.59%) (12875/12928)\n",
      "Epoch: 115 | Batch_idx: 110 |  Loss_1: (0.0120) | Acc_1: (99.59%) (14150/14208)\n",
      "Epoch: 115 | Batch_idx: 120 |  Loss_1: (0.0115) | Acc_1: (99.61%) (15427/15488)\n",
      "Epoch: 115 | Batch_idx: 130 |  Loss_1: (0.0114) | Acc_1: (99.61%) (16702/16768)\n",
      "Epoch: 115 | Batch_idx: 140 |  Loss_1: (0.0111) | Acc_1: (99.62%) (17980/18048)\n",
      "Epoch: 115 | Batch_idx: 150 |  Loss_1: (0.0110) | Acc_1: (99.62%) (19255/19328)\n",
      "Epoch: 115 | Batch_idx: 160 |  Loss_1: (0.0108) | Acc_1: (99.62%) (20530/20608)\n",
      "Epoch: 115 | Batch_idx: 170 |  Loss_1: (0.0112) | Acc_1: (99.61%) (21802/21888)\n",
      "Epoch: 115 | Batch_idx: 180 |  Loss_1: (0.0115) | Acc_1: (99.60%) (23075/23168)\n",
      "Epoch: 115 | Batch_idx: 190 |  Loss_1: (0.0114) | Acc_1: (99.60%) (24350/24448)\n",
      "Epoch: 115 | Batch_idx: 200 |  Loss_1: (0.0113) | Acc_1: (99.60%) (25625/25728)\n",
      "Epoch: 115 | Batch_idx: 210 |  Loss_1: (0.0112) | Acc_1: (99.60%) (26900/27008)\n",
      "Epoch: 115 | Batch_idx: 220 |  Loss_1: (0.0109) | Acc_1: (99.61%) (28178/28288)\n",
      "Epoch: 115 | Batch_idx: 230 |  Loss_1: (0.0107) | Acc_1: (99.62%) (29455/29568)\n",
      "Epoch: 115 | Batch_idx: 240 |  Loss_1: (0.0105) | Acc_1: (99.63%) (30734/30848)\n",
      "Epoch: 115 | Batch_idx: 250 |  Loss_1: (0.0106) | Acc_1: (99.62%) (32007/32128)\n",
      "Epoch: 115 | Batch_idx: 260 |  Loss_1: (0.0106) | Acc_1: (99.63%) (33284/33408)\n",
      "Epoch: 115 | Batch_idx: 270 |  Loss_1: (0.0105) | Acc_1: (99.63%) (34560/34688)\n",
      "Epoch: 115 | Batch_idx: 280 |  Loss_1: (0.0105) | Acc_1: (99.63%) (35835/35968)\n",
      "Epoch: 115 | Batch_idx: 290 |  Loss_1: (0.0106) | Acc_1: (99.62%) (37105/37248)\n",
      "Epoch: 115 | Batch_idx: 300 |  Loss_1: (0.0109) | Acc_1: (99.61%) (38376/38528)\n",
      "Epoch: 115 | Batch_idx: 310 |  Loss_1: (0.0107) | Acc_1: (99.61%) (39652/39808)\n",
      "Epoch: 115 | Batch_idx: 320 |  Loss_1: (0.0106) | Acc_1: (99.62%) (40932/41088)\n",
      "Epoch: 115 | Batch_idx: 330 |  Loss_1: (0.0104) | Acc_1: (99.63%) (42210/42368)\n",
      "Epoch: 115 | Batch_idx: 340 |  Loss_1: (0.0103) | Acc_1: (99.63%) (43488/43648)\n",
      "Epoch: 115 | Batch_idx: 350 |  Loss_1: (0.0104) | Acc_1: (99.63%) (44763/44928)\n",
      "Epoch: 115 | Batch_idx: 360 |  Loss_1: (0.0106) | Acc_1: (99.63%) (46035/46208)\n",
      "Epoch: 115 | Batch_idx: 370 |  Loss_1: (0.0104) | Acc_1: (99.63%) (47314/47488)\n",
      "Epoch: 115 | Batch_idx: 380 |  Loss_1: (0.0105) | Acc_1: (99.63%) (48589/48768)\n",
      "Epoch: 115 | Batch_idx: 390 |  Loss_1: (0.0106) | Acc_1: (99.62%) (49811/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5220) | Acc: (91.44%) (9144/10000)\n",
      "Epoch: 116 | Batch_idx: 0 |  Loss_1: (0.0032) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 116 | Batch_idx: 10 |  Loss_1: (0.0050) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 116 | Batch_idx: 20 |  Loss_1: (0.0090) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 116 | Batch_idx: 30 |  Loss_1: (0.0120) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 116 | Batch_idx: 40 |  Loss_1: (0.0117) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 116 | Batch_idx: 50 |  Loss_1: (0.0120) | Acc_1: (99.60%) (6502/6528)\n",
      "Epoch: 116 | Batch_idx: 60 |  Loss_1: (0.0113) | Acc_1: (99.60%) (7777/7808)\n",
      "Epoch: 116 | Batch_idx: 70 |  Loss_1: (0.0111) | Acc_1: (99.64%) (9055/9088)\n",
      "Epoch: 116 | Batch_idx: 80 |  Loss_1: (0.0106) | Acc_1: (99.64%) (10331/10368)\n",
      "Epoch: 116 | Batch_idx: 90 |  Loss_1: (0.0105) | Acc_1: (99.63%) (11605/11648)\n",
      "Epoch: 116 | Batch_idx: 100 |  Loss_1: (0.0103) | Acc_1: (99.64%) (12882/12928)\n",
      "Epoch: 116 | Batch_idx: 110 |  Loss_1: (0.0100) | Acc_1: (99.66%) (14160/14208)\n",
      "Epoch: 116 | Batch_idx: 120 |  Loss_1: (0.0102) | Acc_1: (99.65%) (15434/15488)\n",
      "Epoch: 116 | Batch_idx: 130 |  Loss_1: (0.0101) | Acc_1: (99.65%) (16710/16768)\n",
      "Epoch: 116 | Batch_idx: 140 |  Loss_1: (0.0099) | Acc_1: (99.67%) (17989/18048)\n",
      "Epoch: 116 | Batch_idx: 150 |  Loss_1: (0.0102) | Acc_1: (99.66%) (19263/19328)\n",
      "Epoch: 116 | Batch_idx: 160 |  Loss_1: (0.0104) | Acc_1: (99.67%) (20539/20608)\n",
      "Epoch: 116 | Batch_idx: 170 |  Loss_1: (0.0107) | Acc_1: (99.66%) (21813/21888)\n",
      "Epoch: 116 | Batch_idx: 180 |  Loss_1: (0.0113) | Acc_1: (99.62%) (23081/23168)\n",
      "Epoch: 116 | Batch_idx: 190 |  Loss_1: (0.0116) | Acc_1: (99.60%) (24351/24448)\n",
      "Epoch: 116 | Batch_idx: 200 |  Loss_1: (0.0112) | Acc_1: (99.62%) (25629/25728)\n",
      "Epoch: 116 | Batch_idx: 210 |  Loss_1: (0.0115) | Acc_1: (99.61%) (26904/27008)\n",
      "Epoch: 116 | Batch_idx: 220 |  Loss_1: (0.0112) | Acc_1: (99.63%) (28182/28288)\n",
      "Epoch: 116 | Batch_idx: 230 |  Loss_1: (0.0112) | Acc_1: (99.63%) (29458/29568)\n",
      "Epoch: 116 | Batch_idx: 240 |  Loss_1: (0.0112) | Acc_1: (99.63%) (30733/30848)\n",
      "Epoch: 116 | Batch_idx: 250 |  Loss_1: (0.0112) | Acc_1: (99.63%) (32009/32128)\n",
      "Epoch: 116 | Batch_idx: 260 |  Loss_1: (0.0113) | Acc_1: (99.62%) (33282/33408)\n",
      "Epoch: 116 | Batch_idx: 270 |  Loss_1: (0.0110) | Acc_1: (99.63%) (34561/34688)\n",
      "Epoch: 116 | Batch_idx: 280 |  Loss_1: (0.0108) | Acc_1: (99.63%) (35836/35968)\n",
      "Epoch: 116 | Batch_idx: 290 |  Loss_1: (0.0106) | Acc_1: (99.64%) (37115/37248)\n",
      "Epoch: 116 | Batch_idx: 300 |  Loss_1: (0.0105) | Acc_1: (99.65%) (38392/38528)\n",
      "Epoch: 116 | Batch_idx: 310 |  Loss_1: (0.0107) | Acc_1: (99.64%) (39664/39808)\n",
      "Epoch: 116 | Batch_idx: 320 |  Loss_1: (0.0105) | Acc_1: (99.64%) (40942/41088)\n",
      "Epoch: 116 | Batch_idx: 330 |  Loss_1: (0.0103) | Acc_1: (99.65%) (42221/42368)\n",
      "Epoch: 116 | Batch_idx: 340 |  Loss_1: (0.0101) | Acc_1: (99.66%) (43499/43648)\n",
      "Epoch: 116 | Batch_idx: 350 |  Loss_1: (0.0100) | Acc_1: (99.66%) (44777/44928)\n",
      "Epoch: 116 | Batch_idx: 360 |  Loss_1: (0.0099) | Acc_1: (99.67%) (46055/46208)\n",
      "Epoch: 116 | Batch_idx: 370 |  Loss_1: (0.0099) | Acc_1: (99.67%) (47329/47488)\n",
      "Epoch: 116 | Batch_idx: 380 |  Loss_1: (0.0099) | Acc_1: (99.66%) (48604/48768)\n",
      "Epoch: 116 | Batch_idx: 390 |  Loss_1: (0.0099) | Acc_1: (99.66%) (49829/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5167) | Acc: (91.57%) (9157/10000)\n",
      "Epoch: 117 | Batch_idx: 0 |  Loss_1: (0.0026) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 117 | Batch_idx: 10 |  Loss_1: (0.0075) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 117 | Batch_idx: 20 |  Loss_1: (0.0068) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 117 | Batch_idx: 30 |  Loss_1: (0.0077) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 117 | Batch_idx: 40 |  Loss_1: (0.0091) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 117 | Batch_idx: 50 |  Loss_1: (0.0090) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 117 | Batch_idx: 60 |  Loss_1: (0.0088) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 117 | Batch_idx: 70 |  Loss_1: (0.0082) | Acc_1: (99.68%) (9059/9088)\n",
      "Epoch: 117 | Batch_idx: 80 |  Loss_1: (0.0081) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 117 | Batch_idx: 90 |  Loss_1: (0.0084) | Acc_1: (99.70%) (11613/11648)\n",
      "Epoch: 117 | Batch_idx: 100 |  Loss_1: (0.0082) | Acc_1: (99.70%) (12889/12928)\n",
      "Epoch: 117 | Batch_idx: 110 |  Loss_1: (0.0081) | Acc_1: (99.71%) (14167/14208)\n",
      "Epoch: 117 | Batch_idx: 120 |  Loss_1: (0.0079) | Acc_1: (99.72%) (15444/15488)\n",
      "Epoch: 117 | Batch_idx: 130 |  Loss_1: (0.0080) | Acc_1: (99.71%) (16720/16768)\n",
      "Epoch: 117 | Batch_idx: 140 |  Loss_1: (0.0084) | Acc_1: (99.70%) (17993/18048)\n",
      "Epoch: 117 | Batch_idx: 150 |  Loss_1: (0.0084) | Acc_1: (99.69%) (19269/19328)\n",
      "Epoch: 117 | Batch_idx: 160 |  Loss_1: (0.0089) | Acc_1: (99.68%) (20542/20608)\n",
      "Epoch: 117 | Batch_idx: 170 |  Loss_1: (0.0087) | Acc_1: (99.68%) (21818/21888)\n",
      "Epoch: 117 | Batch_idx: 180 |  Loss_1: (0.0089) | Acc_1: (99.68%) (23093/23168)\n",
      "Epoch: 117 | Batch_idx: 190 |  Loss_1: (0.0091) | Acc_1: (99.67%) (24367/24448)\n",
      "Epoch: 117 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.67%) (25642/25728)\n",
      "Epoch: 117 | Batch_idx: 210 |  Loss_1: (0.0093) | Acc_1: (99.66%) (26915/27008)\n",
      "Epoch: 117 | Batch_idx: 220 |  Loss_1: (0.0094) | Acc_1: (99.64%) (28187/28288)\n",
      "Epoch: 117 | Batch_idx: 230 |  Loss_1: (0.0097) | Acc_1: (99.64%) (29461/29568)\n",
      "Epoch: 117 | Batch_idx: 240 |  Loss_1: (0.0099) | Acc_1: (99.63%) (30733/30848)\n",
      "Epoch: 117 | Batch_idx: 250 |  Loss_1: (0.0100) | Acc_1: (99.62%) (32007/32128)\n",
      "Epoch: 117 | Batch_idx: 260 |  Loss_1: (0.0099) | Acc_1: (99.63%) (33286/33408)\n",
      "Epoch: 117 | Batch_idx: 270 |  Loss_1: (0.0099) | Acc_1: (99.63%) (34561/34688)\n",
      "Epoch: 117 | Batch_idx: 280 |  Loss_1: (0.0099) | Acc_1: (99.63%) (35836/35968)\n",
      "Epoch: 117 | Batch_idx: 290 |  Loss_1: (0.0097) | Acc_1: (99.64%) (37115/37248)\n",
      "Epoch: 117 | Batch_idx: 300 |  Loss_1: (0.0097) | Acc_1: (99.65%) (38393/38528)\n",
      "Epoch: 117 | Batch_idx: 310 |  Loss_1: (0.0096) | Acc_1: (99.66%) (39671/39808)\n",
      "Epoch: 117 | Batch_idx: 320 |  Loss_1: (0.0097) | Acc_1: (99.65%) (40946/41088)\n",
      "Epoch: 117 | Batch_idx: 330 |  Loss_1: (0.0096) | Acc_1: (99.66%) (42222/42368)\n",
      "Epoch: 117 | Batch_idx: 340 |  Loss_1: (0.0097) | Acc_1: (99.66%) (43498/43648)\n",
      "Epoch: 117 | Batch_idx: 350 |  Loss_1: (0.0096) | Acc_1: (99.66%) (44775/44928)\n",
      "Epoch: 117 | Batch_idx: 360 |  Loss_1: (0.0095) | Acc_1: (99.66%) (46053/46208)\n",
      "Epoch: 117 | Batch_idx: 370 |  Loss_1: (0.0097) | Acc_1: (99.67%) (47329/47488)\n",
      "Epoch: 117 | Batch_idx: 380 |  Loss_1: (0.0097) | Acc_1: (99.66%) (48603/48768)\n",
      "Epoch: 117 | Batch_idx: 390 |  Loss_1: (0.0096) | Acc_1: (99.66%) (49832/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5367) | Acc: (91.37%) (9137/10000)\n",
      "Epoch: 118 | Batch_idx: 0 |  Loss_1: (0.0021) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 118 | Batch_idx: 10 |  Loss_1: (0.0119) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 118 | Batch_idx: 20 |  Loss_1: (0.0098) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 118 | Batch_idx: 30 |  Loss_1: (0.0131) | Acc_1: (99.52%) (3949/3968)\n",
      "Epoch: 118 | Batch_idx: 40 |  Loss_1: (0.0111) | Acc_1: (99.60%) (5227/5248)\n",
      "Epoch: 118 | Batch_idx: 50 |  Loss_1: (0.0099) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 118 | Batch_idx: 60 |  Loss_1: (0.0103) | Acc_1: (99.62%) (7778/7808)\n",
      "Epoch: 118 | Batch_idx: 70 |  Loss_1: (0.0096) | Acc_1: (99.65%) (9056/9088)\n",
      "Epoch: 118 | Batch_idx: 80 |  Loss_1: (0.0094) | Acc_1: (99.65%) (10332/10368)\n",
      "Epoch: 118 | Batch_idx: 90 |  Loss_1: (0.0090) | Acc_1: (99.67%) (11610/11648)\n",
      "Epoch: 118 | Batch_idx: 100 |  Loss_1: (0.0088) | Acc_1: (99.69%) (12888/12928)\n",
      "Epoch: 118 | Batch_idx: 110 |  Loss_1: (0.0087) | Acc_1: (99.69%) (14164/14208)\n",
      "Epoch: 118 | Batch_idx: 120 |  Loss_1: (0.0085) | Acc_1: (99.70%) (15441/15488)\n",
      "Epoch: 118 | Batch_idx: 130 |  Loss_1: (0.0083) | Acc_1: (99.71%) (16719/16768)\n",
      "Epoch: 118 | Batch_idx: 140 |  Loss_1: (0.0081) | Acc_1: (99.71%) (17995/18048)\n",
      "Epoch: 118 | Batch_idx: 150 |  Loss_1: (0.0081) | Acc_1: (99.71%) (19272/19328)\n",
      "Epoch: 118 | Batch_idx: 160 |  Loss_1: (0.0079) | Acc_1: (99.72%) (20550/20608)\n",
      "Epoch: 118 | Batch_idx: 170 |  Loss_1: (0.0080) | Acc_1: (99.72%) (21826/21888)\n",
      "Epoch: 118 | Batch_idx: 180 |  Loss_1: (0.0078) | Acc_1: (99.72%) (23103/23168)\n",
      "Epoch: 118 | Batch_idx: 190 |  Loss_1: (0.0079) | Acc_1: (99.72%) (24379/24448)\n",
      "Epoch: 118 | Batch_idx: 200 |  Loss_1: (0.0077) | Acc_1: (99.73%) (25658/25728)\n",
      "Epoch: 118 | Batch_idx: 210 |  Loss_1: (0.0080) | Acc_1: (99.72%) (26932/27008)\n",
      "Epoch: 118 | Batch_idx: 220 |  Loss_1: (0.0080) | Acc_1: (99.72%) (28208/28288)\n",
      "Epoch: 118 | Batch_idx: 230 |  Loss_1: (0.0081) | Acc_1: (99.71%) (29483/29568)\n",
      "Epoch: 118 | Batch_idx: 240 |  Loss_1: (0.0081) | Acc_1: (99.71%) (30760/30848)\n",
      "Epoch: 118 | Batch_idx: 250 |  Loss_1: (0.0081) | Acc_1: (99.72%) (32037/32128)\n",
      "Epoch: 118 | Batch_idx: 260 |  Loss_1: (0.0082) | Acc_1: (99.72%) (33313/33408)\n",
      "Epoch: 118 | Batch_idx: 270 |  Loss_1: (0.0082) | Acc_1: (99.71%) (34589/34688)\n",
      "Epoch: 118 | Batch_idx: 280 |  Loss_1: (0.0081) | Acc_1: (99.72%) (35867/35968)\n",
      "Epoch: 118 | Batch_idx: 290 |  Loss_1: (0.0082) | Acc_1: (99.71%) (37141/37248)\n",
      "Epoch: 118 | Batch_idx: 300 |  Loss_1: (0.0081) | Acc_1: (99.72%) (38419/38528)\n",
      "Epoch: 118 | Batch_idx: 310 |  Loss_1: (0.0082) | Acc_1: (99.72%) (39696/39808)\n",
      "Epoch: 118 | Batch_idx: 320 |  Loss_1: (0.0083) | Acc_1: (99.72%) (40973/41088)\n",
      "Epoch: 118 | Batch_idx: 330 |  Loss_1: (0.0083) | Acc_1: (99.71%) (42247/42368)\n",
      "Epoch: 118 | Batch_idx: 340 |  Loss_1: (0.0084) | Acc_1: (99.71%) (43523/43648)\n",
      "Epoch: 118 | Batch_idx: 350 |  Loss_1: (0.0085) | Acc_1: (99.71%) (44798/44928)\n",
      "Epoch: 118 | Batch_idx: 360 |  Loss_1: (0.0086) | Acc_1: (99.71%) (46072/46208)\n",
      "Epoch: 118 | Batch_idx: 370 |  Loss_1: (0.0086) | Acc_1: (99.71%) (47348/47488)\n",
      "Epoch: 118 | Batch_idx: 380 |  Loss_1: (0.0087) | Acc_1: (99.70%) (48623/48768)\n",
      "Epoch: 118 | Batch_idx: 390 |  Loss_1: (0.0087) | Acc_1: (99.70%) (49851/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5782) | Acc: (90.89%) (9089/10000)\n",
      "Epoch: 119 | Batch_idx: 0 |  Loss_1: (0.0011) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 119 | Batch_idx: 10 |  Loss_1: (0.0080) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 119 | Batch_idx: 20 |  Loss_1: (0.0103) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 119 | Batch_idx: 30 |  Loss_1: (0.0090) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 119 | Batch_idx: 40 |  Loss_1: (0.0108) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 119 | Batch_idx: 50 |  Loss_1: (0.0100) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 119 | Batch_idx: 60 |  Loss_1: (0.0092) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 119 | Batch_idx: 70 |  Loss_1: (0.0088) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 119 | Batch_idx: 80 |  Loss_1: (0.0081) | Acc_1: (99.75%) (10342/10368)\n",
      "Epoch: 119 | Batch_idx: 90 |  Loss_1: (0.0079) | Acc_1: (99.74%) (11618/11648)\n",
      "Epoch: 119 | Batch_idx: 100 |  Loss_1: (0.0079) | Acc_1: (99.74%) (12894/12928)\n",
      "Epoch: 119 | Batch_idx: 110 |  Loss_1: (0.0080) | Acc_1: (99.74%) (14171/14208)\n",
      "Epoch: 119 | Batch_idx: 120 |  Loss_1: (0.0083) | Acc_1: (99.72%) (15445/15488)\n",
      "Epoch: 119 | Batch_idx: 130 |  Loss_1: (0.0086) | Acc_1: (99.70%) (16717/16768)\n",
      "Epoch: 119 | Batch_idx: 140 |  Loss_1: (0.0092) | Acc_1: (99.68%) (17990/18048)\n",
      "Epoch: 119 | Batch_idx: 150 |  Loss_1: (0.0088) | Acc_1: (99.69%) (19269/19328)\n",
      "Epoch: 119 | Batch_idx: 160 |  Loss_1: (0.0090) | Acc_1: (99.69%) (20545/20608)\n",
      "Epoch: 119 | Batch_idx: 170 |  Loss_1: (0.0088) | Acc_1: (99.70%) (21823/21888)\n",
      "Epoch: 119 | Batch_idx: 180 |  Loss_1: (0.0087) | Acc_1: (99.71%) (23101/23168)\n",
      "Epoch: 119 | Batch_idx: 190 |  Loss_1: (0.0088) | Acc_1: (99.70%) (24374/24448)\n",
      "Epoch: 119 | Batch_idx: 200 |  Loss_1: (0.0089) | Acc_1: (99.69%) (25647/25728)\n",
      "Epoch: 119 | Batch_idx: 210 |  Loss_1: (0.0087) | Acc_1: (99.69%) (26925/27008)\n",
      "Epoch: 119 | Batch_idx: 220 |  Loss_1: (0.0087) | Acc_1: (99.70%) (28202/28288)\n",
      "Epoch: 119 | Batch_idx: 230 |  Loss_1: (0.0086) | Acc_1: (99.71%) (29481/29568)\n",
      "Epoch: 119 | Batch_idx: 240 |  Loss_1: (0.0085) | Acc_1: (99.71%) (30757/30848)\n",
      "Epoch: 119 | Batch_idx: 250 |  Loss_1: (0.0083) | Acc_1: (99.71%) (32036/32128)\n",
      "Epoch: 119 | Batch_idx: 260 |  Loss_1: (0.0083) | Acc_1: (99.71%) (33312/33408)\n",
      "Epoch: 119 | Batch_idx: 270 |  Loss_1: (0.0082) | Acc_1: (99.72%) (34590/34688)\n",
      "Epoch: 119 | Batch_idx: 280 |  Loss_1: (0.0081) | Acc_1: (99.72%) (35869/35968)\n",
      "Epoch: 119 | Batch_idx: 290 |  Loss_1: (0.0080) | Acc_1: (99.73%) (37147/37248)\n",
      "Epoch: 119 | Batch_idx: 300 |  Loss_1: (0.0079) | Acc_1: (99.73%) (38425/38528)\n",
      "Epoch: 119 | Batch_idx: 310 |  Loss_1: (0.0080) | Acc_1: (99.73%) (39702/39808)\n",
      "Epoch: 119 | Batch_idx: 320 |  Loss_1: (0.0079) | Acc_1: (99.73%) (40978/41088)\n",
      "Epoch: 119 | Batch_idx: 330 |  Loss_1: (0.0079) | Acc_1: (99.73%) (42254/42368)\n",
      "Epoch: 119 | Batch_idx: 340 |  Loss_1: (0.0078) | Acc_1: (99.74%) (43533/43648)\n",
      "Epoch: 119 | Batch_idx: 350 |  Loss_1: (0.0078) | Acc_1: (99.73%) (44808/44928)\n",
      "Epoch: 119 | Batch_idx: 360 |  Loss_1: (0.0079) | Acc_1: (99.73%) (46084/46208)\n",
      "Epoch: 119 | Batch_idx: 370 |  Loss_1: (0.0079) | Acc_1: (99.73%) (47360/47488)\n",
      "Epoch: 119 | Batch_idx: 380 |  Loss_1: (0.0079) | Acc_1: (99.73%) (48637/48768)\n",
      "Epoch: 119 | Batch_idx: 390 |  Loss_1: (0.0078) | Acc_1: (99.74%) (49868/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5353) | Acc: (91.50%) (9150/10000)\n",
      "Epoch: 120 | Batch_idx: 0 |  Loss_1: (0.0007) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 120 | Batch_idx: 10 |  Loss_1: (0.0074) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 120 | Batch_idx: 20 |  Loss_1: (0.0056) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 120 | Batch_idx: 30 |  Loss_1: (0.0059) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 120 | Batch_idx: 40 |  Loss_1: (0.0070) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 120 | Batch_idx: 50 |  Loss_1: (0.0071) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 120 | Batch_idx: 60 |  Loss_1: (0.0063) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 120 | Batch_idx: 70 |  Loss_1: (0.0065) | Acc_1: (99.78%) (9068/9088)\n",
      "Epoch: 120 | Batch_idx: 80 |  Loss_1: (0.0066) | Acc_1: (99.77%) (10344/10368)\n",
      "Epoch: 120 | Batch_idx: 90 |  Loss_1: (0.0065) | Acc_1: (99.78%) (11622/11648)\n",
      "Epoch: 120 | Batch_idx: 100 |  Loss_1: (0.0064) | Acc_1: (99.77%) (12898/12928)\n",
      "Epoch: 120 | Batch_idx: 110 |  Loss_1: (0.0064) | Acc_1: (99.77%) (14176/14208)\n",
      "Epoch: 120 | Batch_idx: 120 |  Loss_1: (0.0065) | Acc_1: (99.76%) (15451/15488)\n",
      "Epoch: 120 | Batch_idx: 130 |  Loss_1: (0.0069) | Acc_1: (99.74%) (16725/16768)\n",
      "Epoch: 120 | Batch_idx: 140 |  Loss_1: (0.0071) | Acc_1: (99.73%) (18000/18048)\n",
      "Epoch: 120 | Batch_idx: 150 |  Loss_1: (0.0072) | Acc_1: (99.74%) (19277/19328)\n",
      "Epoch: 120 | Batch_idx: 160 |  Loss_1: (0.0076) | Acc_1: (99.72%) (20551/20608)\n",
      "Epoch: 120 | Batch_idx: 170 |  Loss_1: (0.0075) | Acc_1: (99.72%) (21826/21888)\n",
      "Epoch: 120 | Batch_idx: 180 |  Loss_1: (0.0074) | Acc_1: (99.73%) (23105/23168)\n",
      "Epoch: 120 | Batch_idx: 190 |  Loss_1: (0.0078) | Acc_1: (99.71%) (24377/24448)\n",
      "Epoch: 120 | Batch_idx: 200 |  Loss_1: (0.0079) | Acc_1: (99.72%) (25655/25728)\n",
      "Epoch: 120 | Batch_idx: 210 |  Loss_1: (0.0079) | Acc_1: (99.72%) (26932/27008)\n",
      "Epoch: 120 | Batch_idx: 220 |  Loss_1: (0.0079) | Acc_1: (99.72%) (28209/28288)\n",
      "Epoch: 120 | Batch_idx: 230 |  Loss_1: (0.0081) | Acc_1: (99.71%) (29482/29568)\n",
      "Epoch: 120 | Batch_idx: 240 |  Loss_1: (0.0080) | Acc_1: (99.71%) (30758/30848)\n",
      "Epoch: 120 | Batch_idx: 250 |  Loss_1: (0.0080) | Acc_1: (99.71%) (32035/32128)\n",
      "Epoch: 120 | Batch_idx: 260 |  Loss_1: (0.0082) | Acc_1: (99.70%) (33309/33408)\n",
      "Epoch: 120 | Batch_idx: 270 |  Loss_1: (0.0082) | Acc_1: (99.70%) (34584/34688)\n",
      "Epoch: 120 | Batch_idx: 280 |  Loss_1: (0.0082) | Acc_1: (99.70%) (35860/35968)\n",
      "Epoch: 120 | Batch_idx: 290 |  Loss_1: (0.0082) | Acc_1: (99.70%) (37135/37248)\n",
      "Epoch: 120 | Batch_idx: 300 |  Loss_1: (0.0081) | Acc_1: (99.70%) (38413/38528)\n",
      "Epoch: 120 | Batch_idx: 310 |  Loss_1: (0.0081) | Acc_1: (99.70%) (39690/39808)\n",
      "Epoch: 120 | Batch_idx: 320 |  Loss_1: (0.0084) | Acc_1: (99.69%) (40961/41088)\n",
      "Epoch: 120 | Batch_idx: 330 |  Loss_1: (0.0085) | Acc_1: (99.69%) (42236/42368)\n",
      "Epoch: 120 | Batch_idx: 340 |  Loss_1: (0.0085) | Acc_1: (99.68%) (43510/43648)\n",
      "Epoch: 120 | Batch_idx: 350 |  Loss_1: (0.0085) | Acc_1: (99.68%) (44784/44928)\n",
      "Epoch: 120 | Batch_idx: 360 |  Loss_1: (0.0085) | Acc_1: (99.68%) (46058/46208)\n",
      "Epoch: 120 | Batch_idx: 370 |  Loss_1: (0.0085) | Acc_1: (99.68%) (47334/47488)\n",
      "Epoch: 120 | Batch_idx: 380 |  Loss_1: (0.0085) | Acc_1: (99.68%) (48611/48768)\n",
      "Epoch: 120 | Batch_idx: 390 |  Loss_1: (0.0086) | Acc_1: (99.68%) (49839/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5281) | Acc: (91.61%) (9161/10000)\n",
      "Epoch: 121 | Batch_idx: 0 |  Loss_1: (0.0088) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 121 | Batch_idx: 10 |  Loss_1: (0.0060) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 121 | Batch_idx: 20 |  Loss_1: (0.0062) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 121 | Batch_idx: 30 |  Loss_1: (0.0079) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 121 | Batch_idx: 40 |  Loss_1: (0.0087) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 121 | Batch_idx: 50 |  Loss_1: (0.0093) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 121 | Batch_idx: 60 |  Loss_1: (0.0100) | Acc_1: (99.62%) (7778/7808)\n",
      "Epoch: 121 | Batch_idx: 70 |  Loss_1: (0.0103) | Acc_1: (99.61%) (9053/9088)\n",
      "Epoch: 121 | Batch_idx: 80 |  Loss_1: (0.0099) | Acc_1: (99.62%) (10329/10368)\n",
      "Epoch: 121 | Batch_idx: 90 |  Loss_1: (0.0095) | Acc_1: (99.66%) (11608/11648)\n",
      "Epoch: 121 | Batch_idx: 100 |  Loss_1: (0.0089) | Acc_1: (99.68%) (12887/12928)\n",
      "Epoch: 121 | Batch_idx: 110 |  Loss_1: (0.0094) | Acc_1: (99.64%) (14157/14208)\n",
      "Epoch: 121 | Batch_idx: 120 |  Loss_1: (0.0095) | Acc_1: (99.64%) (15432/15488)\n",
      "Epoch: 121 | Batch_idx: 130 |  Loss_1: (0.0098) | Acc_1: (99.62%) (16704/16768)\n",
      "Epoch: 121 | Batch_idx: 140 |  Loss_1: (0.0098) | Acc_1: (99.61%) (17978/18048)\n",
      "Epoch: 121 | Batch_idx: 150 |  Loss_1: (0.0098) | Acc_1: (99.61%) (19253/19328)\n",
      "Epoch: 121 | Batch_idx: 160 |  Loss_1: (0.0101) | Acc_1: (99.61%) (20528/20608)\n",
      "Epoch: 121 | Batch_idx: 170 |  Loss_1: (0.0109) | Acc_1: (99.58%) (21795/21888)\n",
      "Epoch: 121 | Batch_idx: 180 |  Loss_1: (0.0114) | Acc_1: (99.56%) (23065/23168)\n",
      "Epoch: 121 | Batch_idx: 190 |  Loss_1: (0.0115) | Acc_1: (99.55%) (24338/24448)\n",
      "Epoch: 121 | Batch_idx: 200 |  Loss_1: (0.0114) | Acc_1: (99.55%) (25613/25728)\n",
      "Epoch: 121 | Batch_idx: 210 |  Loss_1: (0.0112) | Acc_1: (99.57%) (26891/27008)\n",
      "Epoch: 121 | Batch_idx: 220 |  Loss_1: (0.0116) | Acc_1: (99.56%) (28164/28288)\n",
      "Epoch: 121 | Batch_idx: 230 |  Loss_1: (0.0117) | Acc_1: (99.56%) (29437/29568)\n",
      "Epoch: 121 | Batch_idx: 240 |  Loss_1: (0.0118) | Acc_1: (99.55%) (30710/30848)\n",
      "Epoch: 121 | Batch_idx: 250 |  Loss_1: (0.0118) | Acc_1: (99.55%) (31985/32128)\n",
      "Epoch: 121 | Batch_idx: 260 |  Loss_1: (0.0116) | Acc_1: (99.56%) (33261/33408)\n",
      "Epoch: 121 | Batch_idx: 270 |  Loss_1: (0.0114) | Acc_1: (99.57%) (34538/34688)\n",
      "Epoch: 121 | Batch_idx: 280 |  Loss_1: (0.0114) | Acc_1: (99.57%) (35815/35968)\n",
      "Epoch: 121 | Batch_idx: 290 |  Loss_1: (0.0114) | Acc_1: (99.57%) (37088/37248)\n",
      "Epoch: 121 | Batch_idx: 300 |  Loss_1: (0.0113) | Acc_1: (99.58%) (38365/38528)\n",
      "Epoch: 121 | Batch_idx: 310 |  Loss_1: (0.0112) | Acc_1: (99.58%) (39642/39808)\n",
      "Epoch: 121 | Batch_idx: 320 |  Loss_1: (0.0110) | Acc_1: (99.59%) (40921/41088)\n",
      "Epoch: 121 | Batch_idx: 330 |  Loss_1: (0.0109) | Acc_1: (99.60%) (42197/42368)\n",
      "Epoch: 121 | Batch_idx: 340 |  Loss_1: (0.0111) | Acc_1: (99.59%) (43471/43648)\n",
      "Epoch: 121 | Batch_idx: 350 |  Loss_1: (0.0110) | Acc_1: (99.59%) (44746/44928)\n",
      "Epoch: 121 | Batch_idx: 360 |  Loss_1: (0.0111) | Acc_1: (99.59%) (46020/46208)\n",
      "Epoch: 121 | Batch_idx: 370 |  Loss_1: (0.0109) | Acc_1: (99.60%) (47298/47488)\n",
      "Epoch: 121 | Batch_idx: 380 |  Loss_1: (0.0112) | Acc_1: (99.59%) (48570/48768)\n",
      "Epoch: 121 | Batch_idx: 390 |  Loss_1: (0.0112) | Acc_1: (99.60%) (49799/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5668) | Acc: (90.92%) (9092/10000)\n",
      "Epoch: 122 | Batch_idx: 0 |  Loss_1: (0.0229) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 122 | Batch_idx: 10 |  Loss_1: (0.0110) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 122 | Batch_idx: 20 |  Loss_1: (0.0129) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 122 | Batch_idx: 30 |  Loss_1: (0.0112) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 122 | Batch_idx: 40 |  Loss_1: (0.0115) | Acc_1: (99.56%) (5225/5248)\n",
      "Epoch: 122 | Batch_idx: 50 |  Loss_1: (0.0100) | Acc_1: (99.62%) (6503/6528)\n",
      "Epoch: 122 | Batch_idx: 60 |  Loss_1: (0.0108) | Acc_1: (99.60%) (7777/7808)\n",
      "Epoch: 122 | Batch_idx: 70 |  Loss_1: (0.0096) | Acc_1: (99.66%) (9057/9088)\n",
      "Epoch: 122 | Batch_idx: 80 |  Loss_1: (0.0091) | Acc_1: (99.67%) (10334/10368)\n",
      "Epoch: 122 | Batch_idx: 90 |  Loss_1: (0.0087) | Acc_1: (99.70%) (11613/11648)\n",
      "Epoch: 122 | Batch_idx: 100 |  Loss_1: (0.0083) | Acc_1: (99.71%) (12891/12928)\n",
      "Epoch: 122 | Batch_idx: 110 |  Loss_1: (0.0083) | Acc_1: (99.72%) (14168/14208)\n",
      "Epoch: 122 | Batch_idx: 120 |  Loss_1: (0.0084) | Acc_1: (99.72%) (15444/15488)\n",
      "Epoch: 122 | Batch_idx: 130 |  Loss_1: (0.0083) | Acc_1: (99.72%) (16721/16768)\n",
      "Epoch: 122 | Batch_idx: 140 |  Loss_1: (0.0083) | Acc_1: (99.72%) (17997/18048)\n",
      "Epoch: 122 | Batch_idx: 150 |  Loss_1: (0.0083) | Acc_1: (99.71%) (19272/19328)\n",
      "Epoch: 122 | Batch_idx: 160 |  Loss_1: (0.0084) | Acc_1: (99.70%) (20547/20608)\n",
      "Epoch: 122 | Batch_idx: 170 |  Loss_1: (0.0085) | Acc_1: (99.69%) (21821/21888)\n",
      "Epoch: 122 | Batch_idx: 180 |  Loss_1: (0.0084) | Acc_1: (99.70%) (23098/23168)\n",
      "Epoch: 122 | Batch_idx: 190 |  Loss_1: (0.0084) | Acc_1: (99.70%) (24375/24448)\n",
      "Epoch: 122 | Batch_idx: 200 |  Loss_1: (0.0086) | Acc_1: (99.69%) (25649/25728)\n",
      "Epoch: 122 | Batch_idx: 210 |  Loss_1: (0.0087) | Acc_1: (99.69%) (26925/27008)\n",
      "Epoch: 122 | Batch_idx: 220 |  Loss_1: (0.0088) | Acc_1: (99.69%) (28200/28288)\n",
      "Epoch: 122 | Batch_idx: 230 |  Loss_1: (0.0089) | Acc_1: (99.68%) (29474/29568)\n",
      "Epoch: 122 | Batch_idx: 240 |  Loss_1: (0.0088) | Acc_1: (99.69%) (30751/30848)\n",
      "Epoch: 122 | Batch_idx: 250 |  Loss_1: (0.0092) | Acc_1: (99.68%) (32025/32128)\n",
      "Epoch: 122 | Batch_idx: 260 |  Loss_1: (0.0093) | Acc_1: (99.68%) (33301/33408)\n",
      "Epoch: 122 | Batch_idx: 270 |  Loss_1: (0.0094) | Acc_1: (99.68%) (34576/34688)\n",
      "Epoch: 122 | Batch_idx: 280 |  Loss_1: (0.0093) | Acc_1: (99.69%) (35855/35968)\n",
      "Epoch: 122 | Batch_idx: 290 |  Loss_1: (0.0094) | Acc_1: (99.68%) (37128/37248)\n",
      "Epoch: 122 | Batch_idx: 300 |  Loss_1: (0.0095) | Acc_1: (99.68%) (38404/38528)\n",
      "Epoch: 122 | Batch_idx: 310 |  Loss_1: (0.0094) | Acc_1: (99.67%) (39678/39808)\n",
      "Epoch: 122 | Batch_idx: 320 |  Loss_1: (0.0094) | Acc_1: (99.68%) (40956/41088)\n",
      "Epoch: 122 | Batch_idx: 330 |  Loss_1: (0.0092) | Acc_1: (99.69%) (42235/42368)\n",
      "Epoch: 122 | Batch_idx: 340 |  Loss_1: (0.0091) | Acc_1: (99.69%) (43513/43648)\n",
      "Epoch: 122 | Batch_idx: 350 |  Loss_1: (0.0090) | Acc_1: (99.69%) (44790/44928)\n",
      "Epoch: 122 | Batch_idx: 360 |  Loss_1: (0.0088) | Acc_1: (99.70%) (46069/46208)\n",
      "Epoch: 122 | Batch_idx: 370 |  Loss_1: (0.0088) | Acc_1: (99.70%) (47345/47488)\n",
      "Epoch: 122 | Batch_idx: 380 |  Loss_1: (0.0088) | Acc_1: (99.70%) (48623/48768)\n",
      "Epoch: 122 | Batch_idx: 390 |  Loss_1: (0.0087) | Acc_1: (99.71%) (49854/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5240) | Acc: (91.38%) (9138/10000)\n",
      "Epoch: 123 | Batch_idx: 0 |  Loss_1: (0.0010) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 123 | Batch_idx: 10 |  Loss_1: (0.0040) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 123 | Batch_idx: 20 |  Loss_1: (0.0056) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 123 | Batch_idx: 30 |  Loss_1: (0.0065) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 123 | Batch_idx: 40 |  Loss_1: (0.0062) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 123 | Batch_idx: 50 |  Loss_1: (0.0058) | Acc_1: (99.83%) (6517/6528)\n",
      "Epoch: 123 | Batch_idx: 60 |  Loss_1: (0.0056) | Acc_1: (99.83%) (7795/7808)\n",
      "Epoch: 123 | Batch_idx: 70 |  Loss_1: (0.0054) | Acc_1: (99.85%) (9074/9088)\n",
      "Epoch: 123 | Batch_idx: 80 |  Loss_1: (0.0058) | Acc_1: (99.83%) (10350/10368)\n",
      "Epoch: 123 | Batch_idx: 90 |  Loss_1: (0.0064) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 123 | Batch_idx: 100 |  Loss_1: (0.0062) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 123 | Batch_idx: 110 |  Loss_1: (0.0065) | Acc_1: (99.80%) (14180/14208)\n",
      "Epoch: 123 | Batch_idx: 120 |  Loss_1: (0.0068) | Acc_1: (99.79%) (15456/15488)\n",
      "Epoch: 123 | Batch_idx: 130 |  Loss_1: (0.0065) | Acc_1: (99.80%) (16735/16768)\n",
      "Epoch: 123 | Batch_idx: 140 |  Loss_1: (0.0067) | Acc_1: (99.79%) (18011/18048)\n",
      "Epoch: 123 | Batch_idx: 150 |  Loss_1: (0.0065) | Acc_1: (99.80%) (19290/19328)\n",
      "Epoch: 123 | Batch_idx: 160 |  Loss_1: (0.0065) | Acc_1: (99.81%) (20568/20608)\n",
      "Epoch: 123 | Batch_idx: 170 |  Loss_1: (0.0067) | Acc_1: (99.79%) (21841/21888)\n",
      "Epoch: 123 | Batch_idx: 180 |  Loss_1: (0.0067) | Acc_1: (99.78%) (23118/23168)\n",
      "Epoch: 123 | Batch_idx: 190 |  Loss_1: (0.0065) | Acc_1: (99.79%) (24397/24448)\n",
      "Epoch: 123 | Batch_idx: 200 |  Loss_1: (0.0065) | Acc_1: (99.79%) (25675/25728)\n",
      "Epoch: 123 | Batch_idx: 210 |  Loss_1: (0.0064) | Acc_1: (99.80%) (26953/27008)\n",
      "Epoch: 123 | Batch_idx: 220 |  Loss_1: (0.0065) | Acc_1: (99.79%) (28229/28288)\n",
      "Epoch: 123 | Batch_idx: 230 |  Loss_1: (0.0063) | Acc_1: (99.80%) (29508/29568)\n",
      "Epoch: 123 | Batch_idx: 240 |  Loss_1: (0.0063) | Acc_1: (99.80%) (30785/30848)\n",
      "Epoch: 123 | Batch_idx: 250 |  Loss_1: (0.0063) | Acc_1: (99.80%) (32063/32128)\n",
      "Epoch: 123 | Batch_idx: 260 |  Loss_1: (0.0064) | Acc_1: (99.79%) (33338/33408)\n",
      "Epoch: 123 | Batch_idx: 270 |  Loss_1: (0.0064) | Acc_1: (99.79%) (34615/34688)\n",
      "Epoch: 123 | Batch_idx: 280 |  Loss_1: (0.0063) | Acc_1: (99.79%) (35892/35968)\n",
      "Epoch: 123 | Batch_idx: 290 |  Loss_1: (0.0066) | Acc_1: (99.78%) (37167/37248)\n",
      "Epoch: 123 | Batch_idx: 300 |  Loss_1: (0.0066) | Acc_1: (99.78%) (38444/38528)\n",
      "Epoch: 123 | Batch_idx: 310 |  Loss_1: (0.0067) | Acc_1: (99.78%) (39721/39808)\n",
      "Epoch: 123 | Batch_idx: 320 |  Loss_1: (0.0069) | Acc_1: (99.77%) (40993/41088)\n",
      "Epoch: 123 | Batch_idx: 330 |  Loss_1: (0.0070) | Acc_1: (99.76%) (42267/42368)\n",
      "Epoch: 123 | Batch_idx: 340 |  Loss_1: (0.0070) | Acc_1: (99.76%) (43545/43648)\n",
      "Epoch: 123 | Batch_idx: 350 |  Loss_1: (0.0072) | Acc_1: (99.76%) (44821/44928)\n",
      "Epoch: 123 | Batch_idx: 360 |  Loss_1: (0.0071) | Acc_1: (99.76%) (46099/46208)\n",
      "Epoch: 123 | Batch_idx: 370 |  Loss_1: (0.0071) | Acc_1: (99.76%) (47375/47488)\n",
      "Epoch: 123 | Batch_idx: 380 |  Loss_1: (0.0071) | Acc_1: (99.76%) (48650/48768)\n",
      "Epoch: 123 | Batch_idx: 390 |  Loss_1: (0.0072) | Acc_1: (99.75%) (49877/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5515) | Acc: (91.04%) (9104/10000)\n",
      "Epoch: 124 | Batch_idx: 0 |  Loss_1: (0.0011) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 124 | Batch_idx: 10 |  Loss_1: (0.0038) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 124 | Batch_idx: 20 |  Loss_1: (0.0092) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 124 | Batch_idx: 30 |  Loss_1: (0.0120) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 124 | Batch_idx: 40 |  Loss_1: (0.0108) | Acc_1: (99.64%) (5229/5248)\n",
      "Epoch: 124 | Batch_idx: 50 |  Loss_1: (0.0102) | Acc_1: (99.63%) (6504/6528)\n",
      "Epoch: 124 | Batch_idx: 60 |  Loss_1: (0.0091) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 124 | Batch_idx: 70 |  Loss_1: (0.0089) | Acc_1: (99.68%) (9059/9088)\n",
      "Epoch: 124 | Batch_idx: 80 |  Loss_1: (0.0087) | Acc_1: (99.70%) (10337/10368)\n",
      "Epoch: 124 | Batch_idx: 90 |  Loss_1: (0.0083) | Acc_1: (99.71%) (11614/11648)\n",
      "Epoch: 124 | Batch_idx: 100 |  Loss_1: (0.0085) | Acc_1: (99.68%) (12887/12928)\n",
      "Epoch: 124 | Batch_idx: 110 |  Loss_1: (0.0084) | Acc_1: (99.68%) (14163/14208)\n",
      "Epoch: 124 | Batch_idx: 120 |  Loss_1: (0.0081) | Acc_1: (99.70%) (15442/15488)\n",
      "Epoch: 124 | Batch_idx: 130 |  Loss_1: (0.0078) | Acc_1: (99.72%) (16721/16768)\n",
      "Epoch: 124 | Batch_idx: 140 |  Loss_1: (0.0075) | Acc_1: (99.73%) (17999/18048)\n",
      "Epoch: 124 | Batch_idx: 150 |  Loss_1: (0.0079) | Acc_1: (99.70%) (19270/19328)\n",
      "Epoch: 124 | Batch_idx: 160 |  Loss_1: (0.0079) | Acc_1: (99.70%) (20547/20608)\n",
      "Epoch: 124 | Batch_idx: 170 |  Loss_1: (0.0079) | Acc_1: (99.71%) (21824/21888)\n",
      "Epoch: 124 | Batch_idx: 180 |  Loss_1: (0.0081) | Acc_1: (99.70%) (23098/23168)\n",
      "Epoch: 124 | Batch_idx: 190 |  Loss_1: (0.0079) | Acc_1: (99.70%) (24374/24448)\n",
      "Epoch: 124 | Batch_idx: 200 |  Loss_1: (0.0078) | Acc_1: (99.70%) (25651/25728)\n",
      "Epoch: 124 | Batch_idx: 210 |  Loss_1: (0.0076) | Acc_1: (99.71%) (26929/27008)\n",
      "Epoch: 124 | Batch_idx: 220 |  Loss_1: (0.0076) | Acc_1: (99.71%) (28206/28288)\n",
      "Epoch: 124 | Batch_idx: 230 |  Loss_1: (0.0073) | Acc_1: (99.72%) (29486/29568)\n",
      "Epoch: 124 | Batch_idx: 240 |  Loss_1: (0.0073) | Acc_1: (99.72%) (30762/30848)\n",
      "Epoch: 124 | Batch_idx: 250 |  Loss_1: (0.0071) | Acc_1: (99.73%) (32041/32128)\n",
      "Epoch: 124 | Batch_idx: 260 |  Loss_1: (0.0072) | Acc_1: (99.72%) (33316/33408)\n",
      "Epoch: 124 | Batch_idx: 270 |  Loss_1: (0.0073) | Acc_1: (99.73%) (34594/34688)\n",
      "Epoch: 124 | Batch_idx: 280 |  Loss_1: (0.0074) | Acc_1: (99.72%) (35868/35968)\n",
      "Epoch: 124 | Batch_idx: 290 |  Loss_1: (0.0075) | Acc_1: (99.72%) (37142/37248)\n",
      "Epoch: 124 | Batch_idx: 300 |  Loss_1: (0.0076) | Acc_1: (99.72%) (38420/38528)\n",
      "Epoch: 124 | Batch_idx: 310 |  Loss_1: (0.0076) | Acc_1: (99.72%) (39695/39808)\n",
      "Epoch: 124 | Batch_idx: 320 |  Loss_1: (0.0075) | Acc_1: (99.72%) (40972/41088)\n",
      "Epoch: 124 | Batch_idx: 330 |  Loss_1: (0.0076) | Acc_1: (99.72%) (42248/42368)\n",
      "Epoch: 124 | Batch_idx: 340 |  Loss_1: (0.0078) | Acc_1: (99.71%) (43522/43648)\n",
      "Epoch: 124 | Batch_idx: 350 |  Loss_1: (0.0078) | Acc_1: (99.71%) (44798/44928)\n",
      "Epoch: 124 | Batch_idx: 360 |  Loss_1: (0.0078) | Acc_1: (99.71%) (46074/46208)\n",
      "Epoch: 124 | Batch_idx: 370 |  Loss_1: (0.0079) | Acc_1: (99.71%) (47349/47488)\n",
      "Epoch: 124 | Batch_idx: 380 |  Loss_1: (0.0078) | Acc_1: (99.71%) (48627/48768)\n",
      "Epoch: 124 | Batch_idx: 390 |  Loss_1: (0.0078) | Acc_1: (99.71%) (49856/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5287) | Acc: (91.48%) (9148/10000)\n",
      "Epoch: 125 | Batch_idx: 0 |  Loss_1: (0.0104) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 125 | Batch_idx: 10 |  Loss_1: (0.0064) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 125 | Batch_idx: 20 |  Loss_1: (0.0050) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 125 | Batch_idx: 30 |  Loss_1: (0.0045) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 125 | Batch_idx: 40 |  Loss_1: (0.0041) | Acc_1: (99.89%) (5242/5248)\n",
      "Epoch: 125 | Batch_idx: 50 |  Loss_1: (0.0043) | Acc_1: (99.88%) (6520/6528)\n",
      "Epoch: 125 | Batch_idx: 60 |  Loss_1: (0.0044) | Acc_1: (99.88%) (7799/7808)\n",
      "Epoch: 125 | Batch_idx: 70 |  Loss_1: (0.0042) | Acc_1: (99.89%) (9078/9088)\n",
      "Epoch: 125 | Batch_idx: 80 |  Loss_1: (0.0045) | Acc_1: (99.87%) (10355/10368)\n",
      "Epoch: 125 | Batch_idx: 90 |  Loss_1: (0.0053) | Acc_1: (99.85%) (11631/11648)\n",
      "Epoch: 125 | Batch_idx: 100 |  Loss_1: (0.0056) | Acc_1: (99.85%) (12908/12928)\n",
      "Epoch: 125 | Batch_idx: 110 |  Loss_1: (0.0062) | Acc_1: (99.81%) (14181/14208)\n",
      "Epoch: 125 | Batch_idx: 120 |  Loss_1: (0.0067) | Acc_1: (99.79%) (15456/15488)\n",
      "Epoch: 125 | Batch_idx: 130 |  Loss_1: (0.0067) | Acc_1: (99.78%) (16731/16768)\n",
      "Epoch: 125 | Batch_idx: 140 |  Loss_1: (0.0077) | Acc_1: (99.77%) (18006/18048)\n",
      "Epoch: 125 | Batch_idx: 150 |  Loss_1: (0.0077) | Acc_1: (99.77%) (19284/19328)\n",
      "Epoch: 125 | Batch_idx: 160 |  Loss_1: (0.0083) | Acc_1: (99.74%) (20555/20608)\n",
      "Epoch: 125 | Batch_idx: 170 |  Loss_1: (0.0085) | Acc_1: (99.73%) (21829/21888)\n",
      "Epoch: 125 | Batch_idx: 180 |  Loss_1: (0.0088) | Acc_1: (99.72%) (23102/23168)\n",
      "Epoch: 125 | Batch_idx: 190 |  Loss_1: (0.0090) | Acc_1: (99.71%) (24377/24448)\n",
      "Epoch: 125 | Batch_idx: 200 |  Loss_1: (0.0093) | Acc_1: (99.70%) (25650/25728)\n",
      "Epoch: 125 | Batch_idx: 210 |  Loss_1: (0.0097) | Acc_1: (99.67%) (26918/27008)\n",
      "Epoch: 125 | Batch_idx: 220 |  Loss_1: (0.0102) | Acc_1: (99.64%) (28187/28288)\n",
      "Epoch: 125 | Batch_idx: 230 |  Loss_1: (0.0103) | Acc_1: (99.63%) (29460/29568)\n",
      "Epoch: 125 | Batch_idx: 240 |  Loss_1: (0.0105) | Acc_1: (99.63%) (30733/30848)\n",
      "Epoch: 125 | Batch_idx: 250 |  Loss_1: (0.0105) | Acc_1: (99.62%) (32005/32128)\n",
      "Epoch: 125 | Batch_idx: 260 |  Loss_1: (0.0103) | Acc_1: (99.62%) (33282/33408)\n",
      "Epoch: 125 | Batch_idx: 270 |  Loss_1: (0.0101) | Acc_1: (99.63%) (34559/34688)\n",
      "Epoch: 125 | Batch_idx: 280 |  Loss_1: (0.0101) | Acc_1: (99.62%) (35833/35968)\n",
      "Epoch: 125 | Batch_idx: 290 |  Loss_1: (0.0100) | Acc_1: (99.63%) (37109/37248)\n",
      "Epoch: 125 | Batch_idx: 300 |  Loss_1: (0.0101) | Acc_1: (99.62%) (38381/38528)\n",
      "Epoch: 125 | Batch_idx: 310 |  Loss_1: (0.0099) | Acc_1: (99.63%) (39659/39808)\n",
      "Epoch: 125 | Batch_idx: 320 |  Loss_1: (0.0098) | Acc_1: (99.63%) (40935/41088)\n",
      "Epoch: 125 | Batch_idx: 330 |  Loss_1: (0.0096) | Acc_1: (99.64%) (42214/42368)\n",
      "Epoch: 125 | Batch_idx: 340 |  Loss_1: (0.0096) | Acc_1: (99.64%) (43490/43648)\n",
      "Epoch: 125 | Batch_idx: 350 |  Loss_1: (0.0094) | Acc_1: (99.64%) (44767/44928)\n",
      "Epoch: 125 | Batch_idx: 360 |  Loss_1: (0.0093) | Acc_1: (99.65%) (46046/46208)\n",
      "Epoch: 125 | Batch_idx: 370 |  Loss_1: (0.0093) | Acc_1: (99.65%) (47321/47488)\n",
      "Epoch: 125 | Batch_idx: 380 |  Loss_1: (0.0094) | Acc_1: (99.65%) (48595/48768)\n",
      "Epoch: 125 | Batch_idx: 390 |  Loss_1: (0.0094) | Acc_1: (99.65%) (49825/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5650) | Acc: (91.58%) (9158/10000)\n",
      "Epoch: 126 | Batch_idx: 0 |  Loss_1: (0.0029) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 126 | Batch_idx: 10 |  Loss_1: (0.0110) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 126 | Batch_idx: 20 |  Loss_1: (0.0104) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 126 | Batch_idx: 30 |  Loss_1: (0.0094) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 126 | Batch_idx: 40 |  Loss_1: (0.0087) | Acc_1: (99.62%) (5228/5248)\n",
      "Epoch: 126 | Batch_idx: 50 |  Loss_1: (0.0091) | Acc_1: (99.60%) (6502/6528)\n",
      "Epoch: 126 | Batch_idx: 60 |  Loss_1: (0.0092) | Acc_1: (99.58%) (7775/7808)\n",
      "Epoch: 126 | Batch_idx: 70 |  Loss_1: (0.0086) | Acc_1: (99.61%) (9053/9088)\n",
      "Epoch: 126 | Batch_idx: 80 |  Loss_1: (0.0095) | Acc_1: (99.59%) (10326/10368)\n",
      "Epoch: 126 | Batch_idx: 90 |  Loss_1: (0.0087) | Acc_1: (99.63%) (11605/11648)\n",
      "Epoch: 126 | Batch_idx: 100 |  Loss_1: (0.0089) | Acc_1: (99.64%) (12881/12928)\n",
      "Epoch: 126 | Batch_idx: 110 |  Loss_1: (0.0088) | Acc_1: (99.63%) (14156/14208)\n",
      "Epoch: 126 | Batch_idx: 120 |  Loss_1: (0.0092) | Acc_1: (99.63%) (15430/15488)\n",
      "Epoch: 126 | Batch_idx: 130 |  Loss_1: (0.0090) | Acc_1: (99.65%) (16709/16768)\n",
      "Epoch: 126 | Batch_idx: 140 |  Loss_1: (0.0095) | Acc_1: (99.64%) (17983/18048)\n",
      "Epoch: 126 | Batch_idx: 150 |  Loss_1: (0.0092) | Acc_1: (99.66%) (19262/19328)\n",
      "Epoch: 126 | Batch_idx: 160 |  Loss_1: (0.0090) | Acc_1: (99.67%) (20539/20608)\n",
      "Epoch: 126 | Batch_idx: 170 |  Loss_1: (0.0089) | Acc_1: (99.66%) (21814/21888)\n",
      "Epoch: 126 | Batch_idx: 180 |  Loss_1: (0.0090) | Acc_1: (99.66%) (23090/23168)\n",
      "Epoch: 126 | Batch_idx: 190 |  Loss_1: (0.0086) | Acc_1: (99.68%) (24370/24448)\n",
      "Epoch: 126 | Batch_idx: 200 |  Loss_1: (0.0084) | Acc_1: (99.69%) (25649/25728)\n",
      "Epoch: 126 | Batch_idx: 210 |  Loss_1: (0.0082) | Acc_1: (99.70%) (26926/27008)\n",
      "Epoch: 126 | Batch_idx: 220 |  Loss_1: (0.0081) | Acc_1: (99.70%) (28202/28288)\n",
      "Epoch: 126 | Batch_idx: 230 |  Loss_1: (0.0082) | Acc_1: (99.70%) (29478/29568)\n",
      "Epoch: 126 | Batch_idx: 240 |  Loss_1: (0.0080) | Acc_1: (99.70%) (30754/30848)\n",
      "Epoch: 126 | Batch_idx: 250 |  Loss_1: (0.0080) | Acc_1: (99.69%) (32030/32128)\n",
      "Epoch: 126 | Batch_idx: 260 |  Loss_1: (0.0080) | Acc_1: (99.70%) (33308/33408)\n",
      "Epoch: 126 | Batch_idx: 270 |  Loss_1: (0.0080) | Acc_1: (99.69%) (34581/34688)\n",
      "Epoch: 126 | Batch_idx: 280 |  Loss_1: (0.0082) | Acc_1: (99.69%) (35855/35968)\n",
      "Epoch: 126 | Batch_idx: 290 |  Loss_1: (0.0082) | Acc_1: (99.69%) (37131/37248)\n",
      "Epoch: 126 | Batch_idx: 300 |  Loss_1: (0.0083) | Acc_1: (99.68%) (38406/38528)\n",
      "Epoch: 126 | Batch_idx: 310 |  Loss_1: (0.0086) | Acc_1: (99.68%) (39679/39808)\n",
      "Epoch: 126 | Batch_idx: 320 |  Loss_1: (0.0088) | Acc_1: (99.67%) (40952/41088)\n",
      "Epoch: 126 | Batch_idx: 330 |  Loss_1: (0.0088) | Acc_1: (99.67%) (42230/42368)\n",
      "Epoch: 126 | Batch_idx: 340 |  Loss_1: (0.0088) | Acc_1: (99.67%) (43506/43648)\n",
      "Epoch: 126 | Batch_idx: 350 |  Loss_1: (0.0089) | Acc_1: (99.67%) (44780/44928)\n",
      "Epoch: 126 | Batch_idx: 360 |  Loss_1: (0.0091) | Acc_1: (99.66%) (46052/46208)\n",
      "Epoch: 126 | Batch_idx: 370 |  Loss_1: (0.0094) | Acc_1: (99.65%) (47320/47488)\n",
      "Epoch: 126 | Batch_idx: 380 |  Loss_1: (0.0094) | Acc_1: (99.64%) (48594/48768)\n",
      "Epoch: 126 | Batch_idx: 390 |  Loss_1: (0.0095) | Acc_1: (99.64%) (49821/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5938) | Acc: (90.68%) (9068/10000)\n",
      "Epoch: 127 | Batch_idx: 0 |  Loss_1: (0.0082) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 127 | Batch_idx: 10 |  Loss_1: (0.0139) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 127 | Batch_idx: 20 |  Loss_1: (0.0118) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 127 | Batch_idx: 30 |  Loss_1: (0.0118) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 127 | Batch_idx: 40 |  Loss_1: (0.0103) | Acc_1: (99.64%) (5229/5248)\n",
      "Epoch: 127 | Batch_idx: 50 |  Loss_1: (0.0087) | Acc_1: (99.71%) (6509/6528)\n",
      "Epoch: 127 | Batch_idx: 60 |  Loss_1: (0.0087) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 127 | Batch_idx: 70 |  Loss_1: (0.0086) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 127 | Batch_idx: 80 |  Loss_1: (0.0082) | Acc_1: (99.71%) (10338/10368)\n",
      "Epoch: 127 | Batch_idx: 90 |  Loss_1: (0.0078) | Acc_1: (99.73%) (11616/11648)\n",
      "Epoch: 127 | Batch_idx: 100 |  Loss_1: (0.0074) | Acc_1: (99.72%) (12892/12928)\n",
      "Epoch: 127 | Batch_idx: 110 |  Loss_1: (0.0073) | Acc_1: (99.72%) (14168/14208)\n",
      "Epoch: 127 | Batch_idx: 120 |  Loss_1: (0.0071) | Acc_1: (99.74%) (15447/15488)\n",
      "Epoch: 127 | Batch_idx: 130 |  Loss_1: (0.0072) | Acc_1: (99.73%) (16722/16768)\n",
      "Epoch: 127 | Batch_idx: 140 |  Loss_1: (0.0071) | Acc_1: (99.73%) (18000/18048)\n",
      "Epoch: 127 | Batch_idx: 150 |  Loss_1: (0.0074) | Acc_1: (99.73%) (19276/19328)\n",
      "Epoch: 127 | Batch_idx: 160 |  Loss_1: (0.0071) | Acc_1: (99.74%) (20554/20608)\n",
      "Epoch: 127 | Batch_idx: 170 |  Loss_1: (0.0072) | Acc_1: (99.74%) (21832/21888)\n",
      "Epoch: 127 | Batch_idx: 180 |  Loss_1: (0.0073) | Acc_1: (99.74%) (23107/23168)\n",
      "Epoch: 127 | Batch_idx: 190 |  Loss_1: (0.0074) | Acc_1: (99.73%) (24383/24448)\n",
      "Epoch: 127 | Batch_idx: 200 |  Loss_1: (0.0074) | Acc_1: (99.74%) (25660/25728)\n",
      "Epoch: 127 | Batch_idx: 210 |  Loss_1: (0.0076) | Acc_1: (99.73%) (26935/27008)\n",
      "Epoch: 127 | Batch_idx: 220 |  Loss_1: (0.0077) | Acc_1: (99.73%) (28211/28288)\n",
      "Epoch: 127 | Batch_idx: 230 |  Loss_1: (0.0079) | Acc_1: (99.72%) (29484/29568)\n",
      "Epoch: 127 | Batch_idx: 240 |  Loss_1: (0.0078) | Acc_1: (99.72%) (30763/30848)\n",
      "Epoch: 127 | Batch_idx: 250 |  Loss_1: (0.0077) | Acc_1: (99.74%) (32043/32128)\n",
      "Epoch: 127 | Batch_idx: 260 |  Loss_1: (0.0075) | Acc_1: (99.74%) (33322/33408)\n",
      "Epoch: 127 | Batch_idx: 270 |  Loss_1: (0.0075) | Acc_1: (99.74%) (34599/34688)\n",
      "Epoch: 127 | Batch_idx: 280 |  Loss_1: (0.0075) | Acc_1: (99.74%) (35876/35968)\n",
      "Epoch: 127 | Batch_idx: 290 |  Loss_1: (0.0075) | Acc_1: (99.74%) (37152/37248)\n",
      "Epoch: 127 | Batch_idx: 300 |  Loss_1: (0.0074) | Acc_1: (99.74%) (38429/38528)\n",
      "Epoch: 127 | Batch_idx: 310 |  Loss_1: (0.0073) | Acc_1: (99.75%) (39708/39808)\n",
      "Epoch: 127 | Batch_idx: 320 |  Loss_1: (0.0073) | Acc_1: (99.75%) (40987/41088)\n",
      "Epoch: 127 | Batch_idx: 330 |  Loss_1: (0.0072) | Acc_1: (99.76%) (42266/42368)\n",
      "Epoch: 127 | Batch_idx: 340 |  Loss_1: (0.0071) | Acc_1: (99.76%) (43543/43648)\n",
      "Epoch: 127 | Batch_idx: 350 |  Loss_1: (0.0072) | Acc_1: (99.76%) (44819/44928)\n",
      "Epoch: 127 | Batch_idx: 360 |  Loss_1: (0.0071) | Acc_1: (99.76%) (46097/46208)\n",
      "Epoch: 127 | Batch_idx: 370 |  Loss_1: (0.0071) | Acc_1: (99.76%) (47374/47488)\n",
      "Epoch: 127 | Batch_idx: 380 |  Loss_1: (0.0072) | Acc_1: (99.75%) (48648/48768)\n",
      "Epoch: 127 | Batch_idx: 390 |  Loss_1: (0.0073) | Acc_1: (99.75%) (49874/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5380) | Acc: (91.51%) (9151/10000)\n",
      "Epoch: 128 | Batch_idx: 0 |  Loss_1: (0.0043) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 128 | Batch_idx: 10 |  Loss_1: (0.0034) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 128 | Batch_idx: 20 |  Loss_1: (0.0075) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 128 | Batch_idx: 30 |  Loss_1: (0.0062) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 128 | Batch_idx: 40 |  Loss_1: (0.0063) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 128 | Batch_idx: 50 |  Loss_1: (0.0067) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 128 | Batch_idx: 60 |  Loss_1: (0.0066) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 128 | Batch_idx: 70 |  Loss_1: (0.0071) | Acc_1: (99.74%) (9064/9088)\n",
      "Epoch: 128 | Batch_idx: 80 |  Loss_1: (0.0072) | Acc_1: (99.72%) (10339/10368)\n",
      "Epoch: 128 | Batch_idx: 90 |  Loss_1: (0.0071) | Acc_1: (99.72%) (11615/11648)\n",
      "Epoch: 128 | Batch_idx: 100 |  Loss_1: (0.0069) | Acc_1: (99.73%) (12893/12928)\n",
      "Epoch: 128 | Batch_idx: 110 |  Loss_1: (0.0070) | Acc_1: (99.71%) (14167/14208)\n",
      "Epoch: 128 | Batch_idx: 120 |  Loss_1: (0.0070) | Acc_1: (99.72%) (15444/15488)\n",
      "Epoch: 128 | Batch_idx: 130 |  Loss_1: (0.0071) | Acc_1: (99.73%) (16722/16768)\n",
      "Epoch: 128 | Batch_idx: 140 |  Loss_1: (0.0077) | Acc_1: (99.71%) (17996/18048)\n",
      "Epoch: 128 | Batch_idx: 150 |  Loss_1: (0.0077) | Acc_1: (99.72%) (19273/19328)\n",
      "Epoch: 128 | Batch_idx: 160 |  Loss_1: (0.0075) | Acc_1: (99.72%) (20551/20608)\n",
      "Epoch: 128 | Batch_idx: 170 |  Loss_1: (0.0073) | Acc_1: (99.74%) (21830/21888)\n",
      "Epoch: 128 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.75%) (23109/23168)\n",
      "Epoch: 128 | Batch_idx: 190 |  Loss_1: (0.0071) | Acc_1: (99.74%) (24385/24448)\n",
      "Epoch: 128 | Batch_idx: 200 |  Loss_1: (0.0071) | Acc_1: (99.75%) (25664/25728)\n",
      "Epoch: 128 | Batch_idx: 210 |  Loss_1: (0.0072) | Acc_1: (99.74%) (26938/27008)\n",
      "Epoch: 128 | Batch_idx: 220 |  Loss_1: (0.0071) | Acc_1: (99.75%) (28216/28288)\n",
      "Epoch: 128 | Batch_idx: 230 |  Loss_1: (0.0075) | Acc_1: (99.74%) (29490/29568)\n",
      "Epoch: 128 | Batch_idx: 240 |  Loss_1: (0.0075) | Acc_1: (99.74%) (30768/30848)\n",
      "Epoch: 128 | Batch_idx: 250 |  Loss_1: (0.0077) | Acc_1: (99.74%) (32043/32128)\n",
      "Epoch: 128 | Batch_idx: 260 |  Loss_1: (0.0076) | Acc_1: (99.74%) (33320/33408)\n",
      "Epoch: 128 | Batch_idx: 270 |  Loss_1: (0.0077) | Acc_1: (99.74%) (34598/34688)\n",
      "Epoch: 128 | Batch_idx: 280 |  Loss_1: (0.0076) | Acc_1: (99.75%) (35877/35968)\n",
      "Epoch: 128 | Batch_idx: 290 |  Loss_1: (0.0075) | Acc_1: (99.75%) (37156/37248)\n",
      "Epoch: 128 | Batch_idx: 300 |  Loss_1: (0.0076) | Acc_1: (99.75%) (38431/38528)\n",
      "Epoch: 128 | Batch_idx: 310 |  Loss_1: (0.0077) | Acc_1: (99.74%) (39704/39808)\n",
      "Epoch: 128 | Batch_idx: 320 |  Loss_1: (0.0077) | Acc_1: (99.74%) (40981/41088)\n",
      "Epoch: 128 | Batch_idx: 330 |  Loss_1: (0.0076) | Acc_1: (99.74%) (42259/42368)\n",
      "Epoch: 128 | Batch_idx: 340 |  Loss_1: (0.0078) | Acc_1: (99.74%) (43535/43648)\n",
      "Epoch: 128 | Batch_idx: 350 |  Loss_1: (0.0077) | Acc_1: (99.74%) (44812/44928)\n",
      "Epoch: 128 | Batch_idx: 360 |  Loss_1: (0.0077) | Acc_1: (99.74%) (46090/46208)\n",
      "Epoch: 128 | Batch_idx: 370 |  Loss_1: (0.0078) | Acc_1: (99.74%) (47364/47488)\n",
      "Epoch: 128 | Batch_idx: 380 |  Loss_1: (0.0078) | Acc_1: (99.74%) (48640/48768)\n",
      "Epoch: 128 | Batch_idx: 390 |  Loss_1: (0.0077) | Acc_1: (99.74%) (49870/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5110) | Acc: (91.81%) (9181/10000)\n",
      "Epoch: 129 | Batch_idx: 0 |  Loss_1: (0.0029) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 129 | Batch_idx: 10 |  Loss_1: (0.0043) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 129 | Batch_idx: 20 |  Loss_1: (0.0076) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 129 | Batch_idx: 30 |  Loss_1: (0.0087) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 129 | Batch_idx: 40 |  Loss_1: (0.0078) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 129 | Batch_idx: 50 |  Loss_1: (0.0079) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 129 | Batch_idx: 60 |  Loss_1: (0.0073) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 129 | Batch_idx: 70 |  Loss_1: (0.0067) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 129 | Batch_idx: 80 |  Loss_1: (0.0073) | Acc_1: (99.75%) (10342/10368)\n",
      "Epoch: 129 | Batch_idx: 90 |  Loss_1: (0.0070) | Acc_1: (99.78%) (11622/11648)\n",
      "Epoch: 129 | Batch_idx: 100 |  Loss_1: (0.0073) | Acc_1: (99.77%) (12898/12928)\n",
      "Epoch: 129 | Batch_idx: 110 |  Loss_1: (0.0073) | Acc_1: (99.77%) (14175/14208)\n",
      "Epoch: 129 | Batch_idx: 120 |  Loss_1: (0.0069) | Acc_1: (99.79%) (15455/15488)\n",
      "Epoch: 129 | Batch_idx: 130 |  Loss_1: (0.0071) | Acc_1: (99.79%) (16732/16768)\n",
      "Epoch: 129 | Batch_idx: 140 |  Loss_1: (0.0068) | Acc_1: (99.80%) (18012/18048)\n",
      "Epoch: 129 | Batch_idx: 150 |  Loss_1: (0.0067) | Acc_1: (99.81%) (19291/19328)\n",
      "Epoch: 129 | Batch_idx: 160 |  Loss_1: (0.0066) | Acc_1: (99.82%) (20570/20608)\n",
      "Epoch: 129 | Batch_idx: 170 |  Loss_1: (0.0064) | Acc_1: (99.83%) (21850/21888)\n",
      "Epoch: 129 | Batch_idx: 180 |  Loss_1: (0.0062) | Acc_1: (99.83%) (23129/23168)\n",
      "Epoch: 129 | Batch_idx: 190 |  Loss_1: (0.0061) | Acc_1: (99.83%) (24406/24448)\n",
      "Epoch: 129 | Batch_idx: 200 |  Loss_1: (0.0061) | Acc_1: (99.83%) (25684/25728)\n",
      "Epoch: 129 | Batch_idx: 210 |  Loss_1: (0.0059) | Acc_1: (99.84%) (26964/27008)\n",
      "Epoch: 129 | Batch_idx: 220 |  Loss_1: (0.0057) | Acc_1: (99.84%) (28243/28288)\n",
      "Epoch: 129 | Batch_idx: 230 |  Loss_1: (0.0057) | Acc_1: (99.84%) (29520/29568)\n",
      "Epoch: 129 | Batch_idx: 240 |  Loss_1: (0.0055) | Acc_1: (99.84%) (30800/30848)\n",
      "Epoch: 129 | Batch_idx: 250 |  Loss_1: (0.0054) | Acc_1: (99.85%) (32079/32128)\n",
      "Epoch: 129 | Batch_idx: 260 |  Loss_1: (0.0055) | Acc_1: (99.85%) (33358/33408)\n",
      "Epoch: 129 | Batch_idx: 270 |  Loss_1: (0.0055) | Acc_1: (99.85%) (34635/34688)\n",
      "Epoch: 129 | Batch_idx: 280 |  Loss_1: (0.0055) | Acc_1: (99.84%) (35911/35968)\n",
      "Epoch: 129 | Batch_idx: 290 |  Loss_1: (0.0055) | Acc_1: (99.84%) (37190/37248)\n",
      "Epoch: 129 | Batch_idx: 300 |  Loss_1: (0.0056) | Acc_1: (99.84%) (38466/38528)\n",
      "Epoch: 129 | Batch_idx: 310 |  Loss_1: (0.0055) | Acc_1: (99.84%) (39745/39808)\n",
      "Epoch: 129 | Batch_idx: 320 |  Loss_1: (0.0056) | Acc_1: (99.84%) (41022/41088)\n",
      "Epoch: 129 | Batch_idx: 330 |  Loss_1: (0.0055) | Acc_1: (99.84%) (42300/42368)\n",
      "Epoch: 129 | Batch_idx: 340 |  Loss_1: (0.0056) | Acc_1: (99.84%) (43577/43648)\n",
      "Epoch: 129 | Batch_idx: 350 |  Loss_1: (0.0057) | Acc_1: (99.83%) (44853/44928)\n",
      "Epoch: 129 | Batch_idx: 360 |  Loss_1: (0.0056) | Acc_1: (99.83%) (46131/46208)\n",
      "Epoch: 129 | Batch_idx: 370 |  Loss_1: (0.0058) | Acc_1: (99.82%) (47404/47488)\n",
      "Epoch: 129 | Batch_idx: 380 |  Loss_1: (0.0059) | Acc_1: (99.82%) (48679/48768)\n",
      "Epoch: 129 | Batch_idx: 390 |  Loss_1: (0.0060) | Acc_1: (99.81%) (49905/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5432) | Acc: (91.62%) (9162/10000)\n",
      "Epoch: 130 | Batch_idx: 0 |  Loss_1: (0.0050) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 130 | Batch_idx: 10 |  Loss_1: (0.0045) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 130 | Batch_idx: 20 |  Loss_1: (0.0042) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 130 | Batch_idx: 30 |  Loss_1: (0.0055) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 130 | Batch_idx: 40 |  Loss_1: (0.0076) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 130 | Batch_idx: 50 |  Loss_1: (0.0089) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 130 | Batch_idx: 60 |  Loss_1: (0.0086) | Acc_1: (99.71%) (7785/7808)\n",
      "Epoch: 130 | Batch_idx: 70 |  Loss_1: (0.0085) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 130 | Batch_idx: 80 |  Loss_1: (0.0078) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 130 | Batch_idx: 90 |  Loss_1: (0.0084) | Acc_1: (99.73%) (11617/11648)\n",
      "Epoch: 130 | Batch_idx: 100 |  Loss_1: (0.0085) | Acc_1: (99.71%) (12891/12928)\n",
      "Epoch: 130 | Batch_idx: 110 |  Loss_1: (0.0089) | Acc_1: (99.69%) (14164/14208)\n",
      "Epoch: 130 | Batch_idx: 120 |  Loss_1: (0.0090) | Acc_1: (99.68%) (15438/15488)\n",
      "Epoch: 130 | Batch_idx: 130 |  Loss_1: (0.0091) | Acc_1: (99.67%) (16712/16768)\n",
      "Epoch: 130 | Batch_idx: 140 |  Loss_1: (0.0089) | Acc_1: (99.67%) (17989/18048)\n",
      "Epoch: 130 | Batch_idx: 150 |  Loss_1: (0.0087) | Acc_1: (99.68%) (19266/19328)\n",
      "Epoch: 130 | Batch_idx: 160 |  Loss_1: (0.0090) | Acc_1: (99.66%) (20537/20608)\n",
      "Epoch: 130 | Batch_idx: 170 |  Loss_1: (0.0088) | Acc_1: (99.67%) (21816/21888)\n",
      "Epoch: 130 | Batch_idx: 180 |  Loss_1: (0.0089) | Acc_1: (99.67%) (23091/23168)\n",
      "Epoch: 130 | Batch_idx: 190 |  Loss_1: (0.0089) | Acc_1: (99.67%) (24367/24448)\n",
      "Epoch: 130 | Batch_idx: 200 |  Loss_1: (0.0088) | Acc_1: (99.67%) (25643/25728)\n",
      "Epoch: 130 | Batch_idx: 210 |  Loss_1: (0.0091) | Acc_1: (99.67%) (26918/27008)\n",
      "Epoch: 130 | Batch_idx: 220 |  Loss_1: (0.0091) | Acc_1: (99.67%) (28195/28288)\n",
      "Epoch: 130 | Batch_idx: 230 |  Loss_1: (0.0095) | Acc_1: (99.67%) (29470/29568)\n",
      "Epoch: 130 | Batch_idx: 240 |  Loss_1: (0.0096) | Acc_1: (99.67%) (30746/30848)\n",
      "Epoch: 130 | Batch_idx: 250 |  Loss_1: (0.0097) | Acc_1: (99.67%) (32021/32128)\n",
      "Epoch: 130 | Batch_idx: 260 |  Loss_1: (0.0098) | Acc_1: (99.66%) (33296/33408)\n",
      "Epoch: 130 | Batch_idx: 270 |  Loss_1: (0.0099) | Acc_1: (99.66%) (34569/34688)\n",
      "Epoch: 130 | Batch_idx: 280 |  Loss_1: (0.0100) | Acc_1: (99.65%) (35842/35968)\n",
      "Epoch: 130 | Batch_idx: 290 |  Loss_1: (0.0103) | Acc_1: (99.65%) (37116/37248)\n",
      "Epoch: 130 | Batch_idx: 300 |  Loss_1: (0.0103) | Acc_1: (99.65%) (38392/38528)\n",
      "Epoch: 130 | Batch_idx: 310 |  Loss_1: (0.0103) | Acc_1: (99.65%) (39668/39808)\n",
      "Epoch: 130 | Batch_idx: 320 |  Loss_1: (0.0104) | Acc_1: (99.65%) (40943/41088)\n",
      "Epoch: 130 | Batch_idx: 330 |  Loss_1: (0.0104) | Acc_1: (99.65%) (42219/42368)\n",
      "Epoch: 130 | Batch_idx: 340 |  Loss_1: (0.0106) | Acc_1: (99.64%) (43491/43648)\n",
      "Epoch: 130 | Batch_idx: 350 |  Loss_1: (0.0105) | Acc_1: (99.64%) (44768/44928)\n",
      "Epoch: 130 | Batch_idx: 360 |  Loss_1: (0.0105) | Acc_1: (99.64%) (46042/46208)\n",
      "Epoch: 130 | Batch_idx: 370 |  Loss_1: (0.0104) | Acc_1: (99.64%) (47319/47488)\n",
      "Epoch: 130 | Batch_idx: 380 |  Loss_1: (0.0103) | Acc_1: (99.65%) (48598/48768)\n",
      "Epoch: 130 | Batch_idx: 390 |  Loss_1: (0.0103) | Acc_1: (99.64%) (49822/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5443) | Acc: (91.47%) (9147/10000)\n",
      "Epoch: 131 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 131 | Batch_idx: 10 |  Loss_1: (0.0076) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 131 | Batch_idx: 20 |  Loss_1: (0.0079) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 131 | Batch_idx: 30 |  Loss_1: (0.0067) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 131 | Batch_idx: 40 |  Loss_1: (0.0083) | Acc_1: (99.68%) (5231/5248)\n",
      "Epoch: 131 | Batch_idx: 50 |  Loss_1: (0.0073) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 131 | Batch_idx: 60 |  Loss_1: (0.0071) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 131 | Batch_idx: 70 |  Loss_1: (0.0071) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 131 | Batch_idx: 80 |  Loss_1: (0.0066) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 131 | Batch_idx: 90 |  Loss_1: (0.0071) | Acc_1: (99.75%) (11619/11648)\n",
      "Epoch: 131 | Batch_idx: 100 |  Loss_1: (0.0070) | Acc_1: (99.75%) (12896/12928)\n",
      "Epoch: 131 | Batch_idx: 110 |  Loss_1: (0.0081) | Acc_1: (99.73%) (14170/14208)\n",
      "Epoch: 131 | Batch_idx: 120 |  Loss_1: (0.0081) | Acc_1: (99.72%) (15445/15488)\n",
      "Epoch: 131 | Batch_idx: 130 |  Loss_1: (0.0081) | Acc_1: (99.72%) (16721/16768)\n",
      "Epoch: 131 | Batch_idx: 140 |  Loss_1: (0.0080) | Acc_1: (99.73%) (18000/18048)\n",
      "Epoch: 131 | Batch_idx: 150 |  Loss_1: (0.0078) | Acc_1: (99.73%) (19276/19328)\n",
      "Epoch: 131 | Batch_idx: 160 |  Loss_1: (0.0081) | Acc_1: (99.72%) (20550/20608)\n",
      "Epoch: 131 | Batch_idx: 170 |  Loss_1: (0.0080) | Acc_1: (99.73%) (21828/21888)\n",
      "Epoch: 131 | Batch_idx: 180 |  Loss_1: (0.0078) | Acc_1: (99.74%) (23107/23168)\n",
      "Epoch: 131 | Batch_idx: 190 |  Loss_1: (0.0078) | Acc_1: (99.73%) (24383/24448)\n",
      "Epoch: 131 | Batch_idx: 200 |  Loss_1: (0.0075) | Acc_1: (99.75%) (25663/25728)\n",
      "Epoch: 131 | Batch_idx: 210 |  Loss_1: (0.0074) | Acc_1: (99.75%) (26941/27008)\n",
      "Epoch: 131 | Batch_idx: 220 |  Loss_1: (0.0072) | Acc_1: (99.76%) (28220/28288)\n",
      "Epoch: 131 | Batch_idx: 230 |  Loss_1: (0.0071) | Acc_1: (99.76%) (29497/29568)\n",
      "Epoch: 131 | Batch_idx: 240 |  Loss_1: (0.0071) | Acc_1: (99.76%) (30775/30848)\n",
      "Epoch: 131 | Batch_idx: 250 |  Loss_1: (0.0071) | Acc_1: (99.77%) (32054/32128)\n",
      "Epoch: 131 | Batch_idx: 260 |  Loss_1: (0.0071) | Acc_1: (99.77%) (33331/33408)\n",
      "Epoch: 131 | Batch_idx: 270 |  Loss_1: (0.0071) | Acc_1: (99.77%) (34608/34688)\n",
      "Epoch: 131 | Batch_idx: 280 |  Loss_1: (0.0071) | Acc_1: (99.77%) (35885/35968)\n",
      "Epoch: 131 | Batch_idx: 290 |  Loss_1: (0.0070) | Acc_1: (99.77%) (37164/37248)\n",
      "Epoch: 131 | Batch_idx: 300 |  Loss_1: (0.0070) | Acc_1: (99.77%) (38439/38528)\n",
      "Epoch: 131 | Batch_idx: 310 |  Loss_1: (0.0069) | Acc_1: (99.78%) (39719/39808)\n",
      "Epoch: 131 | Batch_idx: 320 |  Loss_1: (0.0069) | Acc_1: (99.78%) (40998/41088)\n",
      "Epoch: 131 | Batch_idx: 330 |  Loss_1: (0.0071) | Acc_1: (99.78%) (42273/42368)\n",
      "Epoch: 131 | Batch_idx: 340 |  Loss_1: (0.0071) | Acc_1: (99.78%) (43551/43648)\n",
      "Epoch: 131 | Batch_idx: 350 |  Loss_1: (0.0071) | Acc_1: (99.77%) (44826/44928)\n",
      "Epoch: 131 | Batch_idx: 360 |  Loss_1: (0.0073) | Acc_1: (99.77%) (46100/46208)\n",
      "Epoch: 131 | Batch_idx: 370 |  Loss_1: (0.0073) | Acc_1: (99.77%) (47377/47488)\n",
      "Epoch: 131 | Batch_idx: 380 |  Loss_1: (0.0072) | Acc_1: (99.77%) (48655/48768)\n",
      "Epoch: 131 | Batch_idx: 390 |  Loss_1: (0.0072) | Acc_1: (99.77%) (49884/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5301) | Acc: (91.72%) (9172/10000)\n",
      "Epoch: 132 | Batch_idx: 0 |  Loss_1: (0.0411) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 132 | Batch_idx: 10 |  Loss_1: (0.0105) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 132 | Batch_idx: 20 |  Loss_1: (0.0083) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 132 | Batch_idx: 30 |  Loss_1: (0.0087) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 132 | Batch_idx: 40 |  Loss_1: (0.0071) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 132 | Batch_idx: 50 |  Loss_1: (0.0073) | Acc_1: (99.71%) (6509/6528)\n",
      "Epoch: 132 | Batch_idx: 60 |  Loss_1: (0.0081) | Acc_1: (99.69%) (7784/7808)\n",
      "Epoch: 132 | Batch_idx: 70 |  Loss_1: (0.0083) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 132 | Batch_idx: 80 |  Loss_1: (0.0085) | Acc_1: (99.70%) (10337/10368)\n",
      "Epoch: 132 | Batch_idx: 90 |  Loss_1: (0.0085) | Acc_1: (99.71%) (11614/11648)\n",
      "Epoch: 132 | Batch_idx: 100 |  Loss_1: (0.0081) | Acc_1: (99.72%) (12892/12928)\n",
      "Epoch: 132 | Batch_idx: 110 |  Loss_1: (0.0083) | Acc_1: (99.72%) (14168/14208)\n",
      "Epoch: 132 | Batch_idx: 120 |  Loss_1: (0.0085) | Acc_1: (99.72%) (15444/15488)\n",
      "Epoch: 132 | Batch_idx: 130 |  Loss_1: (0.0085) | Acc_1: (99.72%) (16721/16768)\n",
      "Epoch: 132 | Batch_idx: 140 |  Loss_1: (0.0083) | Acc_1: (99.72%) (17997/18048)\n",
      "Epoch: 132 | Batch_idx: 150 |  Loss_1: (0.0083) | Acc_1: (99.71%) (19272/19328)\n",
      "Epoch: 132 | Batch_idx: 160 |  Loss_1: (0.0086) | Acc_1: (99.70%) (20546/20608)\n",
      "Epoch: 132 | Batch_idx: 170 |  Loss_1: (0.0084) | Acc_1: (99.70%) (21823/21888)\n",
      "Epoch: 132 | Batch_idx: 180 |  Loss_1: (0.0088) | Acc_1: (99.70%) (23098/23168)\n",
      "Epoch: 132 | Batch_idx: 190 |  Loss_1: (0.0090) | Acc_1: (99.69%) (24372/24448)\n",
      "Epoch: 132 | Batch_idx: 200 |  Loss_1: (0.0093) | Acc_1: (99.69%) (25647/25728)\n",
      "Epoch: 132 | Batch_idx: 210 |  Loss_1: (0.0093) | Acc_1: (99.68%) (26922/27008)\n",
      "Epoch: 132 | Batch_idx: 220 |  Loss_1: (0.0092) | Acc_1: (99.69%) (28199/28288)\n",
      "Epoch: 132 | Batch_idx: 230 |  Loss_1: (0.0092) | Acc_1: (99.69%) (29475/29568)\n",
      "Epoch: 132 | Batch_idx: 240 |  Loss_1: (0.0091) | Acc_1: (99.69%) (30752/30848)\n",
      "Epoch: 132 | Batch_idx: 250 |  Loss_1: (0.0090) | Acc_1: (99.69%) (32030/32128)\n",
      "Epoch: 132 | Batch_idx: 260 |  Loss_1: (0.0089) | Acc_1: (99.70%) (33307/33408)\n",
      "Epoch: 132 | Batch_idx: 270 |  Loss_1: (0.0088) | Acc_1: (99.70%) (34585/34688)\n",
      "Epoch: 132 | Batch_idx: 280 |  Loss_1: (0.0086) | Acc_1: (99.71%) (35865/35968)\n",
      "Epoch: 132 | Batch_idx: 290 |  Loss_1: (0.0085) | Acc_1: (99.71%) (37141/37248)\n",
      "Epoch: 132 | Batch_idx: 300 |  Loss_1: (0.0084) | Acc_1: (99.71%) (38418/38528)\n",
      "Epoch: 132 | Batch_idx: 310 |  Loss_1: (0.0083) | Acc_1: (99.72%) (39696/39808)\n",
      "Epoch: 132 | Batch_idx: 320 |  Loss_1: (0.0084) | Acc_1: (99.72%) (40971/41088)\n",
      "Epoch: 132 | Batch_idx: 330 |  Loss_1: (0.0083) | Acc_1: (99.72%) (42249/42368)\n",
      "Epoch: 132 | Batch_idx: 340 |  Loss_1: (0.0082) | Acc_1: (99.72%) (43527/43648)\n",
      "Epoch: 132 | Batch_idx: 350 |  Loss_1: (0.0081) | Acc_1: (99.73%) (44806/44928)\n",
      "Epoch: 132 | Batch_idx: 360 |  Loss_1: (0.0080) | Acc_1: (99.73%) (46085/46208)\n",
      "Epoch: 132 | Batch_idx: 370 |  Loss_1: (0.0079) | Acc_1: (99.74%) (47364/47488)\n",
      "Epoch: 132 | Batch_idx: 380 |  Loss_1: (0.0080) | Acc_1: (99.74%) (48640/48768)\n",
      "Epoch: 132 | Batch_idx: 390 |  Loss_1: (0.0081) | Acc_1: (99.73%) (49865/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5180) | Acc: (91.94%) (9194/10000)\n",
      "Epoch: 133 | Batch_idx: 0 |  Loss_1: (0.0018) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 133 | Batch_idx: 10 |  Loss_1: (0.0044) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 133 | Batch_idx: 20 |  Loss_1: (0.0055) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 133 | Batch_idx: 30 |  Loss_1: (0.0054) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 133 | Batch_idx: 40 |  Loss_1: (0.0061) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 133 | Batch_idx: 50 |  Loss_1: (0.0058) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 133 | Batch_idx: 60 |  Loss_1: (0.0055) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 133 | Batch_idx: 70 |  Loss_1: (0.0052) | Acc_1: (99.78%) (9068/9088)\n",
      "Epoch: 133 | Batch_idx: 80 |  Loss_1: (0.0055) | Acc_1: (99.78%) (10345/10368)\n",
      "Epoch: 133 | Batch_idx: 90 |  Loss_1: (0.0057) | Acc_1: (99.78%) (11622/11648)\n",
      "Epoch: 133 | Batch_idx: 100 |  Loss_1: (0.0058) | Acc_1: (99.78%) (12900/12928)\n",
      "Epoch: 133 | Batch_idx: 110 |  Loss_1: (0.0063) | Acc_1: (99.77%) (14176/14208)\n",
      "Epoch: 133 | Batch_idx: 120 |  Loss_1: (0.0067) | Acc_1: (99.76%) (15451/15488)\n",
      "Epoch: 133 | Batch_idx: 130 |  Loss_1: (0.0066) | Acc_1: (99.77%) (16729/16768)\n",
      "Epoch: 133 | Batch_idx: 140 |  Loss_1: (0.0068) | Acc_1: (99.77%) (18006/18048)\n",
      "Epoch: 133 | Batch_idx: 150 |  Loss_1: (0.0066) | Acc_1: (99.78%) (19285/19328)\n",
      "Epoch: 133 | Batch_idx: 160 |  Loss_1: (0.0067) | Acc_1: (99.77%) (20561/20608)\n",
      "Epoch: 133 | Batch_idx: 170 |  Loss_1: (0.0066) | Acc_1: (99.77%) (21838/21888)\n",
      "Epoch: 133 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.75%) (23109/23168)\n",
      "Epoch: 133 | Batch_idx: 190 |  Loss_1: (0.0070) | Acc_1: (99.75%) (24386/24448)\n",
      "Epoch: 133 | Batch_idx: 200 |  Loss_1: (0.0068) | Acc_1: (99.76%) (25665/25728)\n",
      "Epoch: 133 | Batch_idx: 210 |  Loss_1: (0.0069) | Acc_1: (99.76%) (26942/27008)\n",
      "Epoch: 133 | Batch_idx: 220 |  Loss_1: (0.0069) | Acc_1: (99.76%) (28219/28288)\n",
      "Epoch: 133 | Batch_idx: 230 |  Loss_1: (0.0068) | Acc_1: (99.75%) (29495/29568)\n",
      "Epoch: 133 | Batch_idx: 240 |  Loss_1: (0.0069) | Acc_1: (99.75%) (30772/30848)\n",
      "Epoch: 133 | Batch_idx: 250 |  Loss_1: (0.0068) | Acc_1: (99.76%) (32050/32128)\n",
      "Epoch: 133 | Batch_idx: 260 |  Loss_1: (0.0068) | Acc_1: (99.76%) (33328/33408)\n",
      "Epoch: 133 | Batch_idx: 270 |  Loss_1: (0.0070) | Acc_1: (99.76%) (34604/34688)\n",
      "Epoch: 133 | Batch_idx: 280 |  Loss_1: (0.0070) | Acc_1: (99.76%) (35881/35968)\n",
      "Epoch: 133 | Batch_idx: 290 |  Loss_1: (0.0069) | Acc_1: (99.77%) (37161/37248)\n",
      "Epoch: 133 | Batch_idx: 300 |  Loss_1: (0.0069) | Acc_1: (99.77%) (38438/38528)\n",
      "Epoch: 133 | Batch_idx: 310 |  Loss_1: (0.0067) | Acc_1: (99.77%) (39718/39808)\n",
      "Epoch: 133 | Batch_idx: 320 |  Loss_1: (0.0067) | Acc_1: (99.77%) (40995/41088)\n",
      "Epoch: 133 | Batch_idx: 330 |  Loss_1: (0.0068) | Acc_1: (99.77%) (42271/42368)\n",
      "Epoch: 133 | Batch_idx: 340 |  Loss_1: (0.0069) | Acc_1: (99.77%) (43546/43648)\n",
      "Epoch: 133 | Batch_idx: 350 |  Loss_1: (0.0069) | Acc_1: (99.77%) (44824/44928)\n",
      "Epoch: 133 | Batch_idx: 360 |  Loss_1: (0.0068) | Acc_1: (99.77%) (46103/46208)\n",
      "Epoch: 133 | Batch_idx: 370 |  Loss_1: (0.0067) | Acc_1: (99.78%) (47382/47488)\n",
      "Epoch: 133 | Batch_idx: 380 |  Loss_1: (0.0066) | Acc_1: (99.78%) (48661/48768)\n",
      "Epoch: 133 | Batch_idx: 390 |  Loss_1: (0.0066) | Acc_1: (99.78%) (49889/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5434) | Acc: (91.84%) (9184/10000)\n",
      "Epoch: 134 | Batch_idx: 0 |  Loss_1: (0.0037) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 134 | Batch_idx: 10 |  Loss_1: (0.0026) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 134 | Batch_idx: 20 |  Loss_1: (0.0068) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 134 | Batch_idx: 30 |  Loss_1: (0.0074) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 134 | Batch_idx: 40 |  Loss_1: (0.0067) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 134 | Batch_idx: 50 |  Loss_1: (0.0074) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 134 | Batch_idx: 60 |  Loss_1: (0.0070) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 134 | Batch_idx: 70 |  Loss_1: (0.0082) | Acc_1: (99.75%) (9065/9088)\n",
      "Epoch: 134 | Batch_idx: 80 |  Loss_1: (0.0093) | Acc_1: (99.72%) (10339/10368)\n",
      "Epoch: 134 | Batch_idx: 90 |  Loss_1: (0.0104) | Acc_1: (99.68%) (11611/11648)\n",
      "Epoch: 134 | Batch_idx: 100 |  Loss_1: (0.0107) | Acc_1: (99.65%) (12883/12928)\n",
      "Epoch: 134 | Batch_idx: 110 |  Loss_1: (0.0105) | Acc_1: (99.66%) (14159/14208)\n",
      "Epoch: 134 | Batch_idx: 120 |  Loss_1: (0.0115) | Acc_1: (99.63%) (15430/15488)\n",
      "Epoch: 134 | Batch_idx: 130 |  Loss_1: (0.0116) | Acc_1: (99.62%) (16705/16768)\n",
      "Epoch: 134 | Batch_idx: 140 |  Loss_1: (0.0113) | Acc_1: (99.63%) (17982/18048)\n",
      "Epoch: 134 | Batch_idx: 150 |  Loss_1: (0.0109) | Acc_1: (99.65%) (19260/19328)\n",
      "Epoch: 134 | Batch_idx: 160 |  Loss_1: (0.0112) | Acc_1: (99.64%) (20534/20608)\n",
      "Epoch: 134 | Batch_idx: 170 |  Loss_1: (0.0119) | Acc_1: (99.63%) (21807/21888)\n",
      "Epoch: 134 | Batch_idx: 180 |  Loss_1: (0.0117) | Acc_1: (99.63%) (23083/23168)\n",
      "Epoch: 134 | Batch_idx: 190 |  Loss_1: (0.0122) | Acc_1: (99.62%) (24356/24448)\n",
      "Epoch: 134 | Batch_idx: 200 |  Loss_1: (0.0123) | Acc_1: (99.62%) (25631/25728)\n",
      "Epoch: 134 | Batch_idx: 210 |  Loss_1: (0.0120) | Acc_1: (99.63%) (26907/27008)\n",
      "Epoch: 134 | Batch_idx: 220 |  Loss_1: (0.0117) | Acc_1: (99.64%) (28186/28288)\n",
      "Epoch: 134 | Batch_idx: 230 |  Loss_1: (0.0114) | Acc_1: (99.64%) (29463/29568)\n",
      "Epoch: 134 | Batch_idx: 240 |  Loss_1: (0.0113) | Acc_1: (99.65%) (30739/30848)\n",
      "Epoch: 134 | Batch_idx: 250 |  Loss_1: (0.0111) | Acc_1: (99.65%) (32016/32128)\n",
      "Epoch: 134 | Batch_idx: 260 |  Loss_1: (0.0110) | Acc_1: (99.65%) (33292/33408)\n",
      "Epoch: 134 | Batch_idx: 270 |  Loss_1: (0.0109) | Acc_1: (99.65%) (34568/34688)\n",
      "Epoch: 134 | Batch_idx: 280 |  Loss_1: (0.0108) | Acc_1: (99.65%) (35842/35968)\n",
      "Epoch: 134 | Batch_idx: 290 |  Loss_1: (0.0108) | Acc_1: (99.64%) (37115/37248)\n",
      "Epoch: 134 | Batch_idx: 300 |  Loss_1: (0.0107) | Acc_1: (99.65%) (38393/38528)\n",
      "Epoch: 134 | Batch_idx: 310 |  Loss_1: (0.0106) | Acc_1: (99.65%) (39670/39808)\n",
      "Epoch: 134 | Batch_idx: 320 |  Loss_1: (0.0105) | Acc_1: (99.65%) (40944/41088)\n",
      "Epoch: 134 | Batch_idx: 330 |  Loss_1: (0.0106) | Acc_1: (99.65%) (42218/42368)\n",
      "Epoch: 134 | Batch_idx: 340 |  Loss_1: (0.0108) | Acc_1: (99.64%) (43490/43648)\n",
      "Epoch: 134 | Batch_idx: 350 |  Loss_1: (0.0109) | Acc_1: (99.63%) (44764/44928)\n",
      "Epoch: 134 | Batch_idx: 360 |  Loss_1: (0.0109) | Acc_1: (99.63%) (46039/46208)\n",
      "Epoch: 134 | Batch_idx: 370 |  Loss_1: (0.0108) | Acc_1: (99.64%) (47315/47488)\n",
      "Epoch: 134 | Batch_idx: 380 |  Loss_1: (0.0108) | Acc_1: (99.64%) (48592/48768)\n",
      "Epoch: 134 | Batch_idx: 390 |  Loss_1: (0.0109) | Acc_1: (99.64%) (49820/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5533) | Acc: (91.33%) (9133/10000)\n",
      "Epoch: 135 | Batch_idx: 0 |  Loss_1: (0.0016) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 135 | Batch_idx: 10 |  Loss_1: (0.0064) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 135 | Batch_idx: 20 |  Loss_1: (0.0051) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 135 | Batch_idx: 30 |  Loss_1: (0.0065) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 135 | Batch_idx: 40 |  Loss_1: (0.0058) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 135 | Batch_idx: 50 |  Loss_1: (0.0065) | Acc_1: (99.83%) (6517/6528)\n",
      "Epoch: 135 | Batch_idx: 60 |  Loss_1: (0.0062) | Acc_1: (99.85%) (7796/7808)\n",
      "Epoch: 135 | Batch_idx: 70 |  Loss_1: (0.0060) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 135 | Batch_idx: 80 |  Loss_1: (0.0061) | Acc_1: (99.83%) (10350/10368)\n",
      "Epoch: 135 | Batch_idx: 90 |  Loss_1: (0.0063) | Acc_1: (99.82%) (11627/11648)\n",
      "Epoch: 135 | Batch_idx: 100 |  Loss_1: (0.0062) | Acc_1: (99.83%) (12906/12928)\n",
      "Epoch: 135 | Batch_idx: 110 |  Loss_1: (0.0065) | Acc_1: (99.80%) (14179/14208)\n",
      "Epoch: 135 | Batch_idx: 120 |  Loss_1: (0.0062) | Acc_1: (99.81%) (15458/15488)\n",
      "Epoch: 135 | Batch_idx: 130 |  Loss_1: (0.0066) | Acc_1: (99.79%) (16733/16768)\n",
      "Epoch: 135 | Batch_idx: 140 |  Loss_1: (0.0063) | Acc_1: (99.79%) (18011/18048)\n",
      "Epoch: 135 | Batch_idx: 150 |  Loss_1: (0.0061) | Acc_1: (99.81%) (19291/19328)\n",
      "Epoch: 135 | Batch_idx: 160 |  Loss_1: (0.0060) | Acc_1: (99.81%) (20569/20608)\n",
      "Epoch: 135 | Batch_idx: 170 |  Loss_1: (0.0059) | Acc_1: (99.82%) (21848/21888)\n",
      "Epoch: 135 | Batch_idx: 180 |  Loss_1: (0.0061) | Acc_1: (99.81%) (23125/23168)\n",
      "Epoch: 135 | Batch_idx: 190 |  Loss_1: (0.0061) | Acc_1: (99.82%) (24403/24448)\n",
      "Epoch: 135 | Batch_idx: 200 |  Loss_1: (0.0063) | Acc_1: (99.80%) (25676/25728)\n",
      "Epoch: 135 | Batch_idx: 210 |  Loss_1: (0.0065) | Acc_1: (99.79%) (26952/27008)\n",
      "Epoch: 135 | Batch_idx: 220 |  Loss_1: (0.0065) | Acc_1: (99.79%) (28229/28288)\n",
      "Epoch: 135 | Batch_idx: 230 |  Loss_1: (0.0065) | Acc_1: (99.79%) (29507/29568)\n",
      "Epoch: 135 | Batch_idx: 240 |  Loss_1: (0.0065) | Acc_1: (99.79%) (30784/30848)\n",
      "Epoch: 135 | Batch_idx: 250 |  Loss_1: (0.0064) | Acc_1: (99.79%) (32062/32128)\n",
      "Epoch: 135 | Batch_idx: 260 |  Loss_1: (0.0063) | Acc_1: (99.80%) (33341/33408)\n",
      "Epoch: 135 | Batch_idx: 270 |  Loss_1: (0.0062) | Acc_1: (99.80%) (34619/34688)\n",
      "Epoch: 135 | Batch_idx: 280 |  Loss_1: (0.0061) | Acc_1: (99.80%) (35897/35968)\n",
      "Epoch: 135 | Batch_idx: 290 |  Loss_1: (0.0063) | Acc_1: (99.79%) (37170/37248)\n",
      "Epoch: 135 | Batch_idx: 300 |  Loss_1: (0.0065) | Acc_1: (99.79%) (38447/38528)\n",
      "Epoch: 135 | Batch_idx: 310 |  Loss_1: (0.0066) | Acc_1: (99.79%) (39723/39808)\n",
      "Epoch: 135 | Batch_idx: 320 |  Loss_1: (0.0066) | Acc_1: (99.78%) (40999/41088)\n",
      "Epoch: 135 | Batch_idx: 330 |  Loss_1: (0.0067) | Acc_1: (99.78%) (42275/42368)\n",
      "Epoch: 135 | Batch_idx: 340 |  Loss_1: (0.0068) | Acc_1: (99.78%) (43550/43648)\n",
      "Epoch: 135 | Batch_idx: 350 |  Loss_1: (0.0068) | Acc_1: (99.78%) (44829/44928)\n",
      "Epoch: 135 | Batch_idx: 360 |  Loss_1: (0.0068) | Acc_1: (99.77%) (46103/46208)\n",
      "Epoch: 135 | Batch_idx: 370 |  Loss_1: (0.0068) | Acc_1: (99.77%) (47381/47488)\n",
      "Epoch: 135 | Batch_idx: 380 |  Loss_1: (0.0068) | Acc_1: (99.77%) (48657/48768)\n",
      "Epoch: 135 | Batch_idx: 390 |  Loss_1: (0.0068) | Acc_1: (99.77%) (49885/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5518) | Acc: (91.47%) (9147/10000)\n",
      "Epoch: 136 | Batch_idx: 0 |  Loss_1: (0.0197) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 136 | Batch_idx: 10 |  Loss_1: (0.0070) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 136 | Batch_idx: 20 |  Loss_1: (0.0049) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 136 | Batch_idx: 30 |  Loss_1: (0.0066) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 136 | Batch_idx: 40 |  Loss_1: (0.0067) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 136 | Batch_idx: 50 |  Loss_1: (0.0070) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 136 | Batch_idx: 60 |  Loss_1: (0.0075) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 136 | Batch_idx: 70 |  Loss_1: (0.0077) | Acc_1: (99.76%) (9066/9088)\n",
      "Epoch: 136 | Batch_idx: 80 |  Loss_1: (0.0074) | Acc_1: (99.77%) (10344/10368)\n",
      "Epoch: 136 | Batch_idx: 90 |  Loss_1: (0.0073) | Acc_1: (99.75%) (11619/11648)\n",
      "Epoch: 136 | Batch_idx: 100 |  Loss_1: (0.0068) | Acc_1: (99.78%) (12899/12928)\n",
      "Epoch: 136 | Batch_idx: 110 |  Loss_1: (0.0072) | Acc_1: (99.77%) (14175/14208)\n",
      "Epoch: 136 | Batch_idx: 120 |  Loss_1: (0.0071) | Acc_1: (99.77%) (15453/15488)\n",
      "Epoch: 136 | Batch_idx: 130 |  Loss_1: (0.0070) | Acc_1: (99.78%) (16731/16768)\n",
      "Epoch: 136 | Batch_idx: 140 |  Loss_1: (0.0070) | Acc_1: (99.78%) (18009/18048)\n",
      "Epoch: 136 | Batch_idx: 150 |  Loss_1: (0.0071) | Acc_1: (99.77%) (19284/19328)\n",
      "Epoch: 136 | Batch_idx: 160 |  Loss_1: (0.0070) | Acc_1: (99.78%) (20562/20608)\n",
      "Epoch: 136 | Batch_idx: 170 |  Loss_1: (0.0070) | Acc_1: (99.77%) (21838/21888)\n",
      "Epoch: 136 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.77%) (23114/23168)\n",
      "Epoch: 136 | Batch_idx: 190 |  Loss_1: (0.0071) | Acc_1: (99.76%) (24390/24448)\n",
      "Epoch: 136 | Batch_idx: 200 |  Loss_1: (0.0069) | Acc_1: (99.77%) (25669/25728)\n",
      "Epoch: 136 | Batch_idx: 210 |  Loss_1: (0.0069) | Acc_1: (99.77%) (26945/27008)\n",
      "Epoch: 136 | Batch_idx: 220 |  Loss_1: (0.0071) | Acc_1: (99.76%) (28220/28288)\n",
      "Epoch: 136 | Batch_idx: 230 |  Loss_1: (0.0072) | Acc_1: (99.75%) (29494/29568)\n",
      "Epoch: 136 | Batch_idx: 240 |  Loss_1: (0.0074) | Acc_1: (99.74%) (30769/30848)\n",
      "Epoch: 136 | Batch_idx: 250 |  Loss_1: (0.0074) | Acc_1: (99.75%) (32047/32128)\n",
      "Epoch: 136 | Batch_idx: 260 |  Loss_1: (0.0074) | Acc_1: (99.75%) (33323/33408)\n",
      "Epoch: 136 | Batch_idx: 270 |  Loss_1: (0.0075) | Acc_1: (99.74%) (34598/34688)\n",
      "Epoch: 136 | Batch_idx: 280 |  Loss_1: (0.0076) | Acc_1: (99.74%) (35873/35968)\n",
      "Epoch: 136 | Batch_idx: 290 |  Loss_1: (0.0078) | Acc_1: (99.73%) (37148/37248)\n",
      "Epoch: 136 | Batch_idx: 300 |  Loss_1: (0.0077) | Acc_1: (99.74%) (38426/38528)\n",
      "Epoch: 136 | Batch_idx: 310 |  Loss_1: (0.0079) | Acc_1: (99.73%) (39699/39808)\n",
      "Epoch: 136 | Batch_idx: 320 |  Loss_1: (0.0078) | Acc_1: (99.73%) (40977/41088)\n",
      "Epoch: 136 | Batch_idx: 330 |  Loss_1: (0.0077) | Acc_1: (99.73%) (42254/42368)\n",
      "Epoch: 136 | Batch_idx: 340 |  Loss_1: (0.0079) | Acc_1: (99.73%) (43528/43648)\n",
      "Epoch: 136 | Batch_idx: 350 |  Loss_1: (0.0078) | Acc_1: (99.73%) (44807/44928)\n",
      "Epoch: 136 | Batch_idx: 360 |  Loss_1: (0.0079) | Acc_1: (99.73%) (46083/46208)\n",
      "Epoch: 136 | Batch_idx: 370 |  Loss_1: (0.0080) | Acc_1: (99.72%) (47357/47488)\n",
      "Epoch: 136 | Batch_idx: 380 |  Loss_1: (0.0080) | Acc_1: (99.73%) (48634/48768)\n",
      "Epoch: 136 | Batch_idx: 390 |  Loss_1: (0.0079) | Acc_1: (99.73%) (49864/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5379) | Acc: (91.48%) (9148/10000)\n",
      "Epoch: 137 | Batch_idx: 0 |  Loss_1: (0.0006) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 137 | Batch_idx: 10 |  Loss_1: (0.0070) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 137 | Batch_idx: 20 |  Loss_1: (0.0045) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 137 | Batch_idx: 30 |  Loss_1: (0.0057) | Acc_1: (99.82%) (3961/3968)\n",
      "Epoch: 137 | Batch_idx: 40 |  Loss_1: (0.0056) | Acc_1: (99.81%) (5238/5248)\n",
      "Epoch: 137 | Batch_idx: 50 |  Loss_1: (0.0065) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 137 | Batch_idx: 60 |  Loss_1: (0.0066) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 137 | Batch_idx: 70 |  Loss_1: (0.0062) | Acc_1: (99.78%) (9068/9088)\n",
      "Epoch: 137 | Batch_idx: 80 |  Loss_1: (0.0061) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 137 | Batch_idx: 90 |  Loss_1: (0.0059) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 137 | Batch_idx: 100 |  Loss_1: (0.0061) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 137 | Batch_idx: 110 |  Loss_1: (0.0063) | Acc_1: (99.81%) (14181/14208)\n",
      "Epoch: 137 | Batch_idx: 120 |  Loss_1: (0.0061) | Acc_1: (99.81%) (15459/15488)\n",
      "Epoch: 137 | Batch_idx: 130 |  Loss_1: (0.0062) | Acc_1: (99.80%) (16735/16768)\n",
      "Epoch: 137 | Batch_idx: 140 |  Loss_1: (0.0060) | Acc_1: (99.80%) (18012/18048)\n",
      "Epoch: 137 | Batch_idx: 150 |  Loss_1: (0.0060) | Acc_1: (99.80%) (19289/19328)\n",
      "Epoch: 137 | Batch_idx: 160 |  Loss_1: (0.0063) | Acc_1: (99.79%) (20564/20608)\n",
      "Epoch: 137 | Batch_idx: 170 |  Loss_1: (0.0066) | Acc_1: (99.78%) (21840/21888)\n",
      "Epoch: 137 | Batch_idx: 180 |  Loss_1: (0.0065) | Acc_1: (99.78%) (23117/23168)\n",
      "Epoch: 137 | Batch_idx: 190 |  Loss_1: (0.0064) | Acc_1: (99.78%) (24395/24448)\n",
      "Epoch: 137 | Batch_idx: 200 |  Loss_1: (0.0066) | Acc_1: (99.77%) (25669/25728)\n",
      "Epoch: 137 | Batch_idx: 210 |  Loss_1: (0.0065) | Acc_1: (99.77%) (26947/27008)\n",
      "Epoch: 137 | Batch_idx: 220 |  Loss_1: (0.0065) | Acc_1: (99.78%) (28225/28288)\n",
      "Epoch: 137 | Batch_idx: 230 |  Loss_1: (0.0067) | Acc_1: (99.77%) (29499/29568)\n",
      "Epoch: 137 | Batch_idx: 240 |  Loss_1: (0.0067) | Acc_1: (99.77%) (30776/30848)\n",
      "Epoch: 137 | Batch_idx: 250 |  Loss_1: (0.0066) | Acc_1: (99.77%) (32054/32128)\n",
      "Epoch: 137 | Batch_idx: 260 |  Loss_1: (0.0067) | Acc_1: (99.76%) (33329/33408)\n",
      "Epoch: 137 | Batch_idx: 270 |  Loss_1: (0.0065) | Acc_1: (99.77%) (34609/34688)\n",
      "Epoch: 137 | Batch_idx: 280 |  Loss_1: (0.0067) | Acc_1: (99.77%) (35886/35968)\n",
      "Epoch: 137 | Batch_idx: 290 |  Loss_1: (0.0067) | Acc_1: (99.77%) (37163/37248)\n",
      "Epoch: 137 | Batch_idx: 300 |  Loss_1: (0.0066) | Acc_1: (99.77%) (38439/38528)\n",
      "Epoch: 137 | Batch_idx: 310 |  Loss_1: (0.0066) | Acc_1: (99.77%) (39716/39808)\n",
      "Epoch: 137 | Batch_idx: 320 |  Loss_1: (0.0065) | Acc_1: (99.77%) (40993/41088)\n",
      "Epoch: 137 | Batch_idx: 330 |  Loss_1: (0.0064) | Acc_1: (99.78%) (42273/42368)\n",
      "Epoch: 137 | Batch_idx: 340 |  Loss_1: (0.0063) | Acc_1: (99.78%) (43553/43648)\n",
      "Epoch: 137 | Batch_idx: 350 |  Loss_1: (0.0063) | Acc_1: (99.78%) (44829/44928)\n",
      "Epoch: 137 | Batch_idx: 360 |  Loss_1: (0.0064) | Acc_1: (99.77%) (46104/46208)\n",
      "Epoch: 137 | Batch_idx: 370 |  Loss_1: (0.0064) | Acc_1: (99.77%) (47381/47488)\n",
      "Epoch: 137 | Batch_idx: 380 |  Loss_1: (0.0064) | Acc_1: (99.77%) (48658/48768)\n",
      "Epoch: 137 | Batch_idx: 390 |  Loss_1: (0.0065) | Acc_1: (99.77%) (49887/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5413) | Acc: (91.41%) (9141/10000)\n",
      "Epoch: 138 | Batch_idx: 0 |  Loss_1: (0.0011) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 138 | Batch_idx: 10 |  Loss_1: (0.0055) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 138 | Batch_idx: 20 |  Loss_1: (0.0064) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 138 | Batch_idx: 30 |  Loss_1: (0.0079) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 138 | Batch_idx: 40 |  Loss_1: (0.0078) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 138 | Batch_idx: 50 |  Loss_1: (0.0082) | Acc_1: (99.66%) (6506/6528)\n",
      "Epoch: 138 | Batch_idx: 60 |  Loss_1: (0.0081) | Acc_1: (99.69%) (7784/7808)\n",
      "Epoch: 138 | Batch_idx: 70 |  Loss_1: (0.0082) | Acc_1: (99.68%) (9059/9088)\n",
      "Epoch: 138 | Batch_idx: 80 |  Loss_1: (0.0089) | Acc_1: (99.65%) (10332/10368)\n",
      "Epoch: 138 | Batch_idx: 90 |  Loss_1: (0.0088) | Acc_1: (99.66%) (11608/11648)\n",
      "Epoch: 138 | Batch_idx: 100 |  Loss_1: (0.0090) | Acc_1: (99.66%) (12884/12928)\n",
      "Epoch: 138 | Batch_idx: 110 |  Loss_1: (0.0091) | Acc_1: (99.64%) (14157/14208)\n",
      "Epoch: 138 | Batch_idx: 120 |  Loss_1: (0.0090) | Acc_1: (99.65%) (15434/15488)\n",
      "Epoch: 138 | Batch_idx: 130 |  Loss_1: (0.0088) | Acc_1: (99.67%) (16712/16768)\n",
      "Epoch: 138 | Batch_idx: 140 |  Loss_1: (0.0083) | Acc_1: (99.69%) (17992/18048)\n",
      "Epoch: 138 | Batch_idx: 150 |  Loss_1: (0.0080) | Acc_1: (99.71%) (19271/19328)\n",
      "Epoch: 138 | Batch_idx: 160 |  Loss_1: (0.0079) | Acc_1: (99.71%) (20548/20608)\n",
      "Epoch: 138 | Batch_idx: 170 |  Loss_1: (0.0077) | Acc_1: (99.72%) (21826/21888)\n",
      "Epoch: 138 | Batch_idx: 180 |  Loss_1: (0.0076) | Acc_1: (99.72%) (23104/23168)\n",
      "Epoch: 138 | Batch_idx: 190 |  Loss_1: (0.0076) | Acc_1: (99.72%) (24380/24448)\n",
      "Epoch: 138 | Batch_idx: 200 |  Loss_1: (0.0078) | Acc_1: (99.72%) (25655/25728)\n",
      "Epoch: 138 | Batch_idx: 210 |  Loss_1: (0.0081) | Acc_1: (99.71%) (26931/27008)\n",
      "Epoch: 138 | Batch_idx: 220 |  Loss_1: (0.0082) | Acc_1: (99.72%) (28209/28288)\n",
      "Epoch: 138 | Batch_idx: 230 |  Loss_1: (0.0082) | Acc_1: (99.71%) (29481/29568)\n",
      "Epoch: 138 | Batch_idx: 240 |  Loss_1: (0.0084) | Acc_1: (99.70%) (30756/30848)\n",
      "Epoch: 138 | Batch_idx: 250 |  Loss_1: (0.0082) | Acc_1: (99.70%) (32033/32128)\n",
      "Epoch: 138 | Batch_idx: 260 |  Loss_1: (0.0081) | Acc_1: (99.71%) (33311/33408)\n",
      "Epoch: 138 | Batch_idx: 270 |  Loss_1: (0.0080) | Acc_1: (99.71%) (34589/34688)\n",
      "Epoch: 138 | Batch_idx: 280 |  Loss_1: (0.0081) | Acc_1: (99.71%) (35863/35968)\n",
      "Epoch: 138 | Batch_idx: 290 |  Loss_1: (0.0079) | Acc_1: (99.72%) (37143/37248)\n",
      "Epoch: 138 | Batch_idx: 300 |  Loss_1: (0.0078) | Acc_1: (99.72%) (38422/38528)\n",
      "Epoch: 138 | Batch_idx: 310 |  Loss_1: (0.0076) | Acc_1: (99.73%) (39702/39808)\n",
      "Epoch: 138 | Batch_idx: 320 |  Loss_1: (0.0076) | Acc_1: (99.74%) (40980/41088)\n",
      "Epoch: 138 | Batch_idx: 330 |  Loss_1: (0.0077) | Acc_1: (99.73%) (42255/42368)\n",
      "Epoch: 138 | Batch_idx: 340 |  Loss_1: (0.0078) | Acc_1: (99.73%) (43532/43648)\n",
      "Epoch: 138 | Batch_idx: 350 |  Loss_1: (0.0078) | Acc_1: (99.73%) (44808/44928)\n",
      "Epoch: 138 | Batch_idx: 360 |  Loss_1: (0.0080) | Acc_1: (99.72%) (46080/46208)\n",
      "Epoch: 138 | Batch_idx: 370 |  Loss_1: (0.0083) | Acc_1: (99.72%) (47353/47488)\n",
      "Epoch: 138 | Batch_idx: 380 |  Loss_1: (0.0083) | Acc_1: (99.71%) (48625/48768)\n",
      "Epoch: 138 | Batch_idx: 390 |  Loss_1: (0.0083) | Acc_1: (99.71%) (49854/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5263) | Acc: (91.52%) (9152/10000)\n",
      "Epoch: 139 | Batch_idx: 0 |  Loss_1: (0.0135) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 139 | Batch_idx: 10 |  Loss_1: (0.0096) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 139 | Batch_idx: 20 |  Loss_1: (0.0089) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 139 | Batch_idx: 30 |  Loss_1: (0.0093) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 139 | Batch_idx: 40 |  Loss_1: (0.0097) | Acc_1: (99.68%) (5231/5248)\n",
      "Epoch: 139 | Batch_idx: 50 |  Loss_1: (0.0098) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 139 | Batch_idx: 60 |  Loss_1: (0.0100) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 139 | Batch_idx: 70 |  Loss_1: (0.0092) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 139 | Batch_idx: 80 |  Loss_1: (0.0090) | Acc_1: (99.72%) (10339/10368)\n",
      "Epoch: 139 | Batch_idx: 90 |  Loss_1: (0.0087) | Acc_1: (99.73%) (11616/11648)\n",
      "Epoch: 139 | Batch_idx: 100 |  Loss_1: (0.0082) | Acc_1: (99.74%) (12895/12928)\n",
      "Epoch: 139 | Batch_idx: 110 |  Loss_1: (0.0082) | Acc_1: (99.75%) (14172/14208)\n",
      "Epoch: 139 | Batch_idx: 120 |  Loss_1: (0.0080) | Acc_1: (99.75%) (15450/15488)\n",
      "Epoch: 139 | Batch_idx: 130 |  Loss_1: (0.0076) | Acc_1: (99.77%) (16730/16768)\n",
      "Epoch: 139 | Batch_idx: 140 |  Loss_1: (0.0076) | Acc_1: (99.77%) (18007/18048)\n",
      "Epoch: 139 | Batch_idx: 150 |  Loss_1: (0.0074) | Acc_1: (99.78%) (19286/19328)\n",
      "Epoch: 139 | Batch_idx: 160 |  Loss_1: (0.0074) | Acc_1: (99.79%) (20564/20608)\n",
      "Epoch: 139 | Batch_idx: 170 |  Loss_1: (0.0073) | Acc_1: (99.79%) (21841/21888)\n",
      "Epoch: 139 | Batch_idx: 180 |  Loss_1: (0.0075) | Acc_1: (99.78%) (23116/23168)\n",
      "Epoch: 139 | Batch_idx: 190 |  Loss_1: (0.0077) | Acc_1: (99.77%) (24392/24448)\n",
      "Epoch: 139 | Batch_idx: 200 |  Loss_1: (0.0077) | Acc_1: (99.77%) (25668/25728)\n",
      "Epoch: 139 | Batch_idx: 210 |  Loss_1: (0.0076) | Acc_1: (99.77%) (26945/27008)\n",
      "Epoch: 139 | Batch_idx: 220 |  Loss_1: (0.0079) | Acc_1: (99.76%) (28219/28288)\n",
      "Epoch: 139 | Batch_idx: 230 |  Loss_1: (0.0079) | Acc_1: (99.75%) (29495/29568)\n",
      "Epoch: 139 | Batch_idx: 240 |  Loss_1: (0.0079) | Acc_1: (99.76%) (30773/30848)\n",
      "Epoch: 139 | Batch_idx: 250 |  Loss_1: (0.0081) | Acc_1: (99.75%) (32048/32128)\n",
      "Epoch: 139 | Batch_idx: 260 |  Loss_1: (0.0081) | Acc_1: (99.75%) (33326/33408)\n",
      "Epoch: 139 | Batch_idx: 270 |  Loss_1: (0.0085) | Acc_1: (99.75%) (34600/34688)\n",
      "Epoch: 139 | Batch_idx: 280 |  Loss_1: (0.0083) | Acc_1: (99.75%) (35878/35968)\n",
      "Epoch: 139 | Batch_idx: 290 |  Loss_1: (0.0083) | Acc_1: (99.75%) (37155/37248)\n",
      "Epoch: 139 | Batch_idx: 300 |  Loss_1: (0.0082) | Acc_1: (99.75%) (38432/38528)\n",
      "Epoch: 139 | Batch_idx: 310 |  Loss_1: (0.0084) | Acc_1: (99.74%) (39706/39808)\n",
      "Epoch: 139 | Batch_idx: 320 |  Loss_1: (0.0084) | Acc_1: (99.74%) (40981/41088)\n",
      "Epoch: 139 | Batch_idx: 330 |  Loss_1: (0.0083) | Acc_1: (99.74%) (42259/42368)\n",
      "Epoch: 139 | Batch_idx: 340 |  Loss_1: (0.0082) | Acc_1: (99.74%) (43535/43648)\n",
      "Epoch: 139 | Batch_idx: 350 |  Loss_1: (0.0083) | Acc_1: (99.74%) (44810/44928)\n",
      "Epoch: 139 | Batch_idx: 360 |  Loss_1: (0.0081) | Acc_1: (99.74%) (46089/46208)\n",
      "Epoch: 139 | Batch_idx: 370 |  Loss_1: (0.0081) | Acc_1: (99.74%) (47366/47488)\n",
      "Epoch: 139 | Batch_idx: 380 |  Loss_1: (0.0080) | Acc_1: (99.74%) (48643/48768)\n",
      "Epoch: 139 | Batch_idx: 390 |  Loss_1: (0.0082) | Acc_1: (99.74%) (49871/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5468) | Acc: (91.19%) (9119/10000)\n",
      "Epoch: 140 | Batch_idx: 0 |  Loss_1: (0.0007) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 140 | Batch_idx: 10 |  Loss_1: (0.0047) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 140 | Batch_idx: 20 |  Loss_1: (0.0066) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 140 | Batch_idx: 30 |  Loss_1: (0.0060) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 140 | Batch_idx: 40 |  Loss_1: (0.0067) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 140 | Batch_idx: 50 |  Loss_1: (0.0078) | Acc_1: (99.71%) (6509/6528)\n",
      "Epoch: 140 | Batch_idx: 60 |  Loss_1: (0.0093) | Acc_1: (99.64%) (7780/7808)\n",
      "Epoch: 140 | Batch_idx: 70 |  Loss_1: (0.0095) | Acc_1: (99.65%) (9056/9088)\n",
      "Epoch: 140 | Batch_idx: 80 |  Loss_1: (0.0103) | Acc_1: (99.61%) (10328/10368)\n",
      "Epoch: 140 | Batch_idx: 90 |  Loss_1: (0.0103) | Acc_1: (99.62%) (11604/11648)\n",
      "Epoch: 140 | Batch_idx: 100 |  Loss_1: (0.0111) | Acc_1: (99.61%) (12877/12928)\n",
      "Epoch: 140 | Batch_idx: 110 |  Loss_1: (0.0114) | Acc_1: (99.61%) (14153/14208)\n",
      "Epoch: 140 | Batch_idx: 120 |  Loss_1: (0.0109) | Acc_1: (99.63%) (15430/15488)\n",
      "Epoch: 140 | Batch_idx: 130 |  Loss_1: (0.0107) | Acc_1: (99.63%) (16706/16768)\n",
      "Epoch: 140 | Batch_idx: 140 |  Loss_1: (0.0110) | Acc_1: (99.63%) (17981/18048)\n",
      "Epoch: 140 | Batch_idx: 150 |  Loss_1: (0.0107) | Acc_1: (99.64%) (19258/19328)\n",
      "Epoch: 140 | Batch_idx: 160 |  Loss_1: (0.0110) | Acc_1: (99.64%) (20533/20608)\n",
      "Epoch: 140 | Batch_idx: 170 |  Loss_1: (0.0106) | Acc_1: (99.65%) (21812/21888)\n",
      "Epoch: 140 | Batch_idx: 180 |  Loss_1: (0.0105) | Acc_1: (99.65%) (23088/23168)\n",
      "Epoch: 140 | Batch_idx: 190 |  Loss_1: (0.0106) | Acc_1: (99.66%) (24365/24448)\n",
      "Epoch: 140 | Batch_idx: 200 |  Loss_1: (0.0106) | Acc_1: (99.66%) (25641/25728)\n",
      "Epoch: 140 | Batch_idx: 210 |  Loss_1: (0.0105) | Acc_1: (99.66%) (26916/27008)\n",
      "Epoch: 140 | Batch_idx: 220 |  Loss_1: (0.0109) | Acc_1: (99.64%) (28187/28288)\n",
      "Epoch: 140 | Batch_idx: 230 |  Loss_1: (0.0109) | Acc_1: (99.65%) (29464/29568)\n",
      "Epoch: 140 | Batch_idx: 240 |  Loss_1: (0.0107) | Acc_1: (99.65%) (30740/30848)\n",
      "Epoch: 140 | Batch_idx: 250 |  Loss_1: (0.0107) | Acc_1: (99.65%) (32016/32128)\n",
      "Epoch: 140 | Batch_idx: 260 |  Loss_1: (0.0109) | Acc_1: (99.65%) (33291/33408)\n",
      "Epoch: 140 | Batch_idx: 270 |  Loss_1: (0.0111) | Acc_1: (99.65%) (34565/34688)\n",
      "Epoch: 140 | Batch_idx: 280 |  Loss_1: (0.0110) | Acc_1: (99.65%) (35841/35968)\n",
      "Epoch: 140 | Batch_idx: 290 |  Loss_1: (0.0111) | Acc_1: (99.65%) (37116/37248)\n",
      "Epoch: 140 | Batch_idx: 300 |  Loss_1: (0.0111) | Acc_1: (99.64%) (38391/38528)\n",
      "Epoch: 140 | Batch_idx: 310 |  Loss_1: (0.0111) | Acc_1: (99.64%) (39666/39808)\n",
      "Epoch: 140 | Batch_idx: 320 |  Loss_1: (0.0113) | Acc_1: (99.64%) (40939/41088)\n",
      "Epoch: 140 | Batch_idx: 330 |  Loss_1: (0.0112) | Acc_1: (99.64%) (42215/42368)\n",
      "Epoch: 140 | Batch_idx: 340 |  Loss_1: (0.0113) | Acc_1: (99.64%) (43490/43648)\n",
      "Epoch: 140 | Batch_idx: 350 |  Loss_1: (0.0112) | Acc_1: (99.64%) (44767/44928)\n",
      "Epoch: 140 | Batch_idx: 360 |  Loss_1: (0.0113) | Acc_1: (99.63%) (46039/46208)\n",
      "Epoch: 140 | Batch_idx: 370 |  Loss_1: (0.0112) | Acc_1: (99.64%) (47317/47488)\n",
      "Epoch: 140 | Batch_idx: 380 |  Loss_1: (0.0112) | Acc_1: (99.64%) (48593/48768)\n",
      "Epoch: 140 | Batch_idx: 390 |  Loss_1: (0.0110) | Acc_1: (99.65%) (49823/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5540) | Acc: (91.50%) (9150/10000)\n",
      "Epoch: 141 | Batch_idx: 0 |  Loss_1: (0.0357) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 141 | Batch_idx: 10 |  Loss_1: (0.0066) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 141 | Batch_idx: 20 |  Loss_1: (0.0112) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 141 | Batch_idx: 30 |  Loss_1: (0.0103) | Acc_1: (99.55%) (3950/3968)\n",
      "Epoch: 141 | Batch_idx: 40 |  Loss_1: (0.0112) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 141 | Batch_idx: 50 |  Loss_1: (0.0105) | Acc_1: (99.62%) (6503/6528)\n",
      "Epoch: 141 | Batch_idx: 60 |  Loss_1: (0.0116) | Acc_1: (99.59%) (7776/7808)\n",
      "Epoch: 141 | Batch_idx: 70 |  Loss_1: (0.0114) | Acc_1: (99.58%) (9050/9088)\n",
      "Epoch: 141 | Batch_idx: 80 |  Loss_1: (0.0114) | Acc_1: (99.59%) (10325/10368)\n",
      "Epoch: 141 | Batch_idx: 90 |  Loss_1: (0.0108) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 141 | Batch_idx: 100 |  Loss_1: (0.0102) | Acc_1: (99.63%) (12880/12928)\n",
      "Epoch: 141 | Batch_idx: 110 |  Loss_1: (0.0099) | Acc_1: (99.63%) (14156/14208)\n",
      "Epoch: 141 | Batch_idx: 120 |  Loss_1: (0.0096) | Acc_1: (99.64%) (15433/15488)\n",
      "Epoch: 141 | Batch_idx: 130 |  Loss_1: (0.0092) | Acc_1: (99.65%) (16710/16768)\n",
      "Epoch: 141 | Batch_idx: 140 |  Loss_1: (0.0092) | Acc_1: (99.67%) (17988/18048)\n",
      "Epoch: 141 | Batch_idx: 150 |  Loss_1: (0.0090) | Acc_1: (99.67%) (19265/19328)\n",
      "Epoch: 141 | Batch_idx: 160 |  Loss_1: (0.0086) | Acc_1: (99.69%) (20544/20608)\n",
      "Epoch: 141 | Batch_idx: 170 |  Loss_1: (0.0084) | Acc_1: (99.70%) (21823/21888)\n",
      "Epoch: 141 | Batch_idx: 180 |  Loss_1: (0.0084) | Acc_1: (99.71%) (23100/23168)\n",
      "Epoch: 141 | Batch_idx: 190 |  Loss_1: (0.0081) | Acc_1: (99.72%) (24379/24448)\n",
      "Epoch: 141 | Batch_idx: 200 |  Loss_1: (0.0080) | Acc_1: (99.72%) (25656/25728)\n",
      "Epoch: 141 | Batch_idx: 210 |  Loss_1: (0.0078) | Acc_1: (99.73%) (26936/27008)\n",
      "Epoch: 141 | Batch_idx: 220 |  Loss_1: (0.0078) | Acc_1: (99.73%) (28213/28288)\n",
      "Epoch: 141 | Batch_idx: 230 |  Loss_1: (0.0076) | Acc_1: (99.74%) (29492/29568)\n",
      "Epoch: 141 | Batch_idx: 240 |  Loss_1: (0.0077) | Acc_1: (99.74%) (30768/30848)\n",
      "Epoch: 141 | Batch_idx: 250 |  Loss_1: (0.0079) | Acc_1: (99.73%) (32042/32128)\n",
      "Epoch: 141 | Batch_idx: 260 |  Loss_1: (0.0078) | Acc_1: (99.73%) (33319/33408)\n",
      "Epoch: 141 | Batch_idx: 270 |  Loss_1: (0.0079) | Acc_1: (99.73%) (34593/34688)\n",
      "Epoch: 141 | Batch_idx: 280 |  Loss_1: (0.0078) | Acc_1: (99.73%) (35871/35968)\n",
      "Epoch: 141 | Batch_idx: 290 |  Loss_1: (0.0078) | Acc_1: (99.73%) (37148/37248)\n",
      "Epoch: 141 | Batch_idx: 300 |  Loss_1: (0.0077) | Acc_1: (99.73%) (38424/38528)\n",
      "Epoch: 141 | Batch_idx: 310 |  Loss_1: (0.0076) | Acc_1: (99.73%) (39701/39808)\n",
      "Epoch: 141 | Batch_idx: 320 |  Loss_1: (0.0076) | Acc_1: (99.73%) (40977/41088)\n",
      "Epoch: 141 | Batch_idx: 330 |  Loss_1: (0.0076) | Acc_1: (99.73%) (42255/42368)\n",
      "Epoch: 141 | Batch_idx: 340 |  Loss_1: (0.0075) | Acc_1: (99.74%) (43534/43648)\n",
      "Epoch: 141 | Batch_idx: 350 |  Loss_1: (0.0075) | Acc_1: (99.74%) (44810/44928)\n",
      "Epoch: 141 | Batch_idx: 360 |  Loss_1: (0.0074) | Acc_1: (99.74%) (46087/46208)\n",
      "Epoch: 141 | Batch_idx: 370 |  Loss_1: (0.0074) | Acc_1: (99.74%) (47363/47488)\n",
      "Epoch: 141 | Batch_idx: 380 |  Loss_1: (0.0073) | Acc_1: (99.74%) (48641/48768)\n",
      "Epoch: 141 | Batch_idx: 390 |  Loss_1: (0.0073) | Acc_1: (99.74%) (49869/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5098) | Acc: (91.56%) (9156/10000)\n",
      "Epoch: 142 | Batch_idx: 0 |  Loss_1: (0.0009) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 142 | Batch_idx: 10 |  Loss_1: (0.0137) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 142 | Batch_idx: 20 |  Loss_1: (0.0092) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 142 | Batch_idx: 30 |  Loss_1: (0.0098) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 142 | Batch_idx: 40 |  Loss_1: (0.0081) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 142 | Batch_idx: 50 |  Loss_1: (0.0076) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 142 | Batch_idx: 60 |  Loss_1: (0.0073) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 142 | Batch_idx: 70 |  Loss_1: (0.0070) | Acc_1: (99.76%) (9066/9088)\n",
      "Epoch: 142 | Batch_idx: 80 |  Loss_1: (0.0068) | Acc_1: (99.76%) (10343/10368)\n",
      "Epoch: 142 | Batch_idx: 90 |  Loss_1: (0.0072) | Acc_1: (99.76%) (11620/11648)\n",
      "Epoch: 142 | Batch_idx: 100 |  Loss_1: (0.0076) | Acc_1: (99.75%) (12896/12928)\n",
      "Epoch: 142 | Batch_idx: 110 |  Loss_1: (0.0075) | Acc_1: (99.75%) (14173/14208)\n",
      "Epoch: 142 | Batch_idx: 120 |  Loss_1: (0.0073) | Acc_1: (99.75%) (15450/15488)\n",
      "Epoch: 142 | Batch_idx: 130 |  Loss_1: (0.0072) | Acc_1: (99.75%) (16726/16768)\n",
      "Epoch: 142 | Batch_idx: 140 |  Loss_1: (0.0069) | Acc_1: (99.76%) (18005/18048)\n",
      "Epoch: 142 | Batch_idx: 150 |  Loss_1: (0.0069) | Acc_1: (99.76%) (19282/19328)\n",
      "Epoch: 142 | Batch_idx: 160 |  Loss_1: (0.0069) | Acc_1: (99.76%) (20559/20608)\n",
      "Epoch: 142 | Batch_idx: 170 |  Loss_1: (0.0068) | Acc_1: (99.77%) (21837/21888)\n",
      "Epoch: 142 | Batch_idx: 180 |  Loss_1: (0.0067) | Acc_1: (99.77%) (23115/23168)\n",
      "Epoch: 142 | Batch_idx: 190 |  Loss_1: (0.0068) | Acc_1: (99.77%) (24391/24448)\n",
      "Epoch: 142 | Batch_idx: 200 |  Loss_1: (0.0068) | Acc_1: (99.76%) (25666/25728)\n",
      "Epoch: 142 | Batch_idx: 210 |  Loss_1: (0.0067) | Acc_1: (99.77%) (26945/27008)\n",
      "Epoch: 142 | Batch_idx: 220 |  Loss_1: (0.0066) | Acc_1: (99.77%) (28223/28288)\n",
      "Epoch: 142 | Batch_idx: 230 |  Loss_1: (0.0065) | Acc_1: (99.78%) (29502/29568)\n",
      "Epoch: 142 | Batch_idx: 240 |  Loss_1: (0.0064) | Acc_1: (99.78%) (30780/30848)\n",
      "Epoch: 142 | Batch_idx: 250 |  Loss_1: (0.0064) | Acc_1: (99.78%) (32058/32128)\n",
      "Epoch: 142 | Batch_idx: 260 |  Loss_1: (0.0064) | Acc_1: (99.78%) (33336/33408)\n",
      "Epoch: 142 | Batch_idx: 270 |  Loss_1: (0.0065) | Acc_1: (99.78%) (34613/34688)\n",
      "Epoch: 142 | Batch_idx: 280 |  Loss_1: (0.0068) | Acc_1: (99.77%) (35886/35968)\n",
      "Epoch: 142 | Batch_idx: 290 |  Loss_1: (0.0068) | Acc_1: (99.77%) (37161/37248)\n",
      "Epoch: 142 | Batch_idx: 300 |  Loss_1: (0.0068) | Acc_1: (99.77%) (38438/38528)\n",
      "Epoch: 142 | Batch_idx: 310 |  Loss_1: (0.0069) | Acc_1: (99.76%) (39713/39808)\n",
      "Epoch: 142 | Batch_idx: 320 |  Loss_1: (0.0070) | Acc_1: (99.76%) (40990/41088)\n",
      "Epoch: 142 | Batch_idx: 330 |  Loss_1: (0.0071) | Acc_1: (99.76%) (42265/42368)\n",
      "Epoch: 142 | Batch_idx: 340 |  Loss_1: (0.0073) | Acc_1: (99.75%) (43539/43648)\n",
      "Epoch: 142 | Batch_idx: 350 |  Loss_1: (0.0075) | Acc_1: (99.75%) (44814/44928)\n",
      "Epoch: 142 | Batch_idx: 360 |  Loss_1: (0.0074) | Acc_1: (99.75%) (46093/46208)\n",
      "Epoch: 142 | Batch_idx: 370 |  Loss_1: (0.0075) | Acc_1: (99.75%) (47367/47488)\n",
      "Epoch: 142 | Batch_idx: 380 |  Loss_1: (0.0074) | Acc_1: (99.75%) (48644/48768)\n",
      "Epoch: 142 | Batch_idx: 390 |  Loss_1: (0.0075) | Acc_1: (99.75%) (49874/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5232) | Acc: (91.66%) (9166/10000)\n",
      "Epoch: 143 | Batch_idx: 0 |  Loss_1: (0.0073) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 143 | Batch_idx: 10 |  Loss_1: (0.0066) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 143 | Batch_idx: 20 |  Loss_1: (0.0103) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 143 | Batch_idx: 30 |  Loss_1: (0.0080) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 143 | Batch_idx: 40 |  Loss_1: (0.0090) | Acc_1: (99.68%) (5231/5248)\n",
      "Epoch: 143 | Batch_idx: 50 |  Loss_1: (0.0088) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 143 | Batch_idx: 60 |  Loss_1: (0.0089) | Acc_1: (99.69%) (7784/7808)\n",
      "Epoch: 143 | Batch_idx: 70 |  Loss_1: (0.0085) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 143 | Batch_idx: 80 |  Loss_1: (0.0084) | Acc_1: (99.70%) (10337/10368)\n",
      "Epoch: 143 | Batch_idx: 90 |  Loss_1: (0.0082) | Acc_1: (99.71%) (11614/11648)\n",
      "Epoch: 143 | Batch_idx: 100 |  Loss_1: (0.0079) | Acc_1: (99.72%) (12892/12928)\n",
      "Epoch: 143 | Batch_idx: 110 |  Loss_1: (0.0076) | Acc_1: (99.73%) (14170/14208)\n",
      "Epoch: 143 | Batch_idx: 120 |  Loss_1: (0.0073) | Acc_1: (99.75%) (15449/15488)\n",
      "Epoch: 143 | Batch_idx: 130 |  Loss_1: (0.0078) | Acc_1: (99.73%) (16722/16768)\n",
      "Epoch: 143 | Batch_idx: 140 |  Loss_1: (0.0075) | Acc_1: (99.75%) (18002/18048)\n",
      "Epoch: 143 | Batch_idx: 150 |  Loss_1: (0.0077) | Acc_1: (99.74%) (19278/19328)\n",
      "Epoch: 143 | Batch_idx: 160 |  Loss_1: (0.0077) | Acc_1: (99.74%) (20555/20608)\n",
      "Epoch: 143 | Batch_idx: 170 |  Loss_1: (0.0077) | Acc_1: (99.74%) (21831/21888)\n",
      "Epoch: 143 | Batch_idx: 180 |  Loss_1: (0.0076) | Acc_1: (99.75%) (23109/23168)\n",
      "Epoch: 143 | Batch_idx: 190 |  Loss_1: (0.0076) | Acc_1: (99.74%) (24385/24448)\n",
      "Epoch: 143 | Batch_idx: 200 |  Loss_1: (0.0076) | Acc_1: (99.75%) (25663/25728)\n",
      "Epoch: 143 | Batch_idx: 210 |  Loss_1: (0.0074) | Acc_1: (99.76%) (26942/27008)\n",
      "Epoch: 143 | Batch_idx: 220 |  Loss_1: (0.0073) | Acc_1: (99.76%) (28219/28288)\n",
      "Epoch: 143 | Batch_idx: 230 |  Loss_1: (0.0075) | Acc_1: (99.76%) (29496/29568)\n",
      "Epoch: 143 | Batch_idx: 240 |  Loss_1: (0.0074) | Acc_1: (99.76%) (30775/30848)\n",
      "Epoch: 143 | Batch_idx: 250 |  Loss_1: (0.0072) | Acc_1: (99.77%) (32054/32128)\n",
      "Epoch: 143 | Batch_idx: 260 |  Loss_1: (0.0071) | Acc_1: (99.77%) (33330/33408)\n",
      "Epoch: 143 | Batch_idx: 270 |  Loss_1: (0.0071) | Acc_1: (99.77%) (34607/34688)\n",
      "Epoch: 143 | Batch_idx: 280 |  Loss_1: (0.0073) | Acc_1: (99.76%) (35883/35968)\n",
      "Epoch: 143 | Batch_idx: 290 |  Loss_1: (0.0071) | Acc_1: (99.77%) (37163/37248)\n",
      "Epoch: 143 | Batch_idx: 300 |  Loss_1: (0.0070) | Acc_1: (99.77%) (38441/38528)\n",
      "Epoch: 143 | Batch_idx: 310 |  Loss_1: (0.0070) | Acc_1: (99.77%) (39718/39808)\n",
      "Epoch: 143 | Batch_idx: 320 |  Loss_1: (0.0069) | Acc_1: (99.78%) (40997/41088)\n",
      "Epoch: 143 | Batch_idx: 330 |  Loss_1: (0.0068) | Acc_1: (99.78%) (42275/42368)\n",
      "Epoch: 143 | Batch_idx: 340 |  Loss_1: (0.0067) | Acc_1: (99.78%) (43554/43648)\n",
      "Epoch: 143 | Batch_idx: 350 |  Loss_1: (0.0066) | Acc_1: (99.78%) (44831/44928)\n",
      "Epoch: 143 | Batch_idx: 360 |  Loss_1: (0.0065) | Acc_1: (99.79%) (46111/46208)\n",
      "Epoch: 143 | Batch_idx: 370 |  Loss_1: (0.0064) | Acc_1: (99.79%) (47389/47488)\n",
      "Epoch: 143 | Batch_idx: 380 |  Loss_1: (0.0063) | Acc_1: (99.80%) (48669/48768)\n",
      "Epoch: 143 | Batch_idx: 390 |  Loss_1: (0.0063) | Acc_1: (99.80%) (49899/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4961) | Acc: (92.00%) (9200/10000)\n",
      "Epoch: 144 | Batch_idx: 0 |  Loss_1: (0.0222) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 144 | Batch_idx: 10 |  Loss_1: (0.0056) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 144 | Batch_idx: 20 |  Loss_1: (0.0068) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 144 | Batch_idx: 30 |  Loss_1: (0.0054) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 144 | Batch_idx: 40 |  Loss_1: (0.0048) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 144 | Batch_idx: 50 |  Loss_1: (0.0041) | Acc_1: (99.89%) (6521/6528)\n",
      "Epoch: 144 | Batch_idx: 60 |  Loss_1: (0.0041) | Acc_1: (99.88%) (7799/7808)\n",
      "Epoch: 144 | Batch_idx: 70 |  Loss_1: (0.0040) | Acc_1: (99.88%) (9077/9088)\n",
      "Epoch: 144 | Batch_idx: 80 |  Loss_1: (0.0040) | Acc_1: (99.88%) (10356/10368)\n",
      "Epoch: 144 | Batch_idx: 90 |  Loss_1: (0.0040) | Acc_1: (99.87%) (11633/11648)\n",
      "Epoch: 144 | Batch_idx: 100 |  Loss_1: (0.0039) | Acc_1: (99.88%) (12913/12928)\n",
      "Epoch: 144 | Batch_idx: 110 |  Loss_1: (0.0040) | Acc_1: (99.87%) (14190/14208)\n",
      "Epoch: 144 | Batch_idx: 120 |  Loss_1: (0.0038) | Acc_1: (99.88%) (15470/15488)\n",
      "Epoch: 144 | Batch_idx: 130 |  Loss_1: (0.0039) | Acc_1: (99.87%) (16746/16768)\n",
      "Epoch: 144 | Batch_idx: 140 |  Loss_1: (0.0041) | Acc_1: (99.87%) (18024/18048)\n",
      "Epoch: 144 | Batch_idx: 150 |  Loss_1: (0.0040) | Acc_1: (99.88%) (19304/19328)\n",
      "Epoch: 144 | Batch_idx: 160 |  Loss_1: (0.0041) | Acc_1: (99.87%) (20581/20608)\n",
      "Epoch: 144 | Batch_idx: 170 |  Loss_1: (0.0041) | Acc_1: (99.87%) (21859/21888)\n",
      "Epoch: 144 | Batch_idx: 180 |  Loss_1: (0.0042) | Acc_1: (99.87%) (23138/23168)\n",
      "Epoch: 144 | Batch_idx: 190 |  Loss_1: (0.0044) | Acc_1: (99.86%) (24413/24448)\n",
      "Epoch: 144 | Batch_idx: 200 |  Loss_1: (0.0045) | Acc_1: (99.86%) (25691/25728)\n",
      "Epoch: 144 | Batch_idx: 210 |  Loss_1: (0.0045) | Acc_1: (99.86%) (26970/27008)\n",
      "Epoch: 144 | Batch_idx: 220 |  Loss_1: (0.0044) | Acc_1: (99.87%) (28250/28288)\n",
      "Epoch: 144 | Batch_idx: 230 |  Loss_1: (0.0044) | Acc_1: (99.86%) (29527/29568)\n",
      "Epoch: 144 | Batch_idx: 240 |  Loss_1: (0.0044) | Acc_1: (99.86%) (30805/30848)\n",
      "Epoch: 144 | Batch_idx: 250 |  Loss_1: (0.0047) | Acc_1: (99.85%) (32080/32128)\n",
      "Epoch: 144 | Batch_idx: 260 |  Loss_1: (0.0046) | Acc_1: (99.86%) (33360/33408)\n",
      "Epoch: 144 | Batch_idx: 270 |  Loss_1: (0.0048) | Acc_1: (99.85%) (34635/34688)\n",
      "Epoch: 144 | Batch_idx: 280 |  Loss_1: (0.0047) | Acc_1: (99.85%) (35915/35968)\n",
      "Epoch: 144 | Batch_idx: 290 |  Loss_1: (0.0048) | Acc_1: (99.85%) (37193/37248)\n",
      "Epoch: 144 | Batch_idx: 300 |  Loss_1: (0.0048) | Acc_1: (99.85%) (38470/38528)\n",
      "Epoch: 144 | Batch_idx: 310 |  Loss_1: (0.0050) | Acc_1: (99.85%) (39747/39808)\n",
      "Epoch: 144 | Batch_idx: 320 |  Loss_1: (0.0051) | Acc_1: (99.84%) (41024/41088)\n",
      "Epoch: 144 | Batch_idx: 330 |  Loss_1: (0.0051) | Acc_1: (99.85%) (42303/42368)\n",
      "Epoch: 144 | Batch_idx: 340 |  Loss_1: (0.0051) | Acc_1: (99.85%) (43581/43648)\n",
      "Epoch: 144 | Batch_idx: 350 |  Loss_1: (0.0052) | Acc_1: (99.84%) (44858/44928)\n",
      "Epoch: 144 | Batch_idx: 360 |  Loss_1: (0.0051) | Acc_1: (99.85%) (46137/46208)\n",
      "Epoch: 144 | Batch_idx: 370 |  Loss_1: (0.0051) | Acc_1: (99.84%) (47414/47488)\n",
      "Epoch: 144 | Batch_idx: 380 |  Loss_1: (0.0051) | Acc_1: (99.84%) (48690/48768)\n",
      "Epoch: 144 | Batch_idx: 390 |  Loss_1: (0.0050) | Acc_1: (99.84%) (49922/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5045) | Acc: (91.85%) (9185/10000)\n",
      "Epoch: 145 | Batch_idx: 0 |  Loss_1: (0.0042) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 145 | Batch_idx: 10 |  Loss_1: (0.0020) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 145 | Batch_idx: 20 |  Loss_1: (0.0031) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 145 | Batch_idx: 30 |  Loss_1: (0.0026) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 145 | Batch_idx: 40 |  Loss_1: (0.0024) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 145 | Batch_idx: 50 |  Loss_1: (0.0026) | Acc_1: (99.95%) (6525/6528)\n",
      "Epoch: 145 | Batch_idx: 60 |  Loss_1: (0.0025) | Acc_1: (99.95%) (7804/7808)\n",
      "Epoch: 145 | Batch_idx: 70 |  Loss_1: (0.0025) | Acc_1: (99.96%) (9084/9088)\n",
      "Epoch: 145 | Batch_idx: 80 |  Loss_1: (0.0029) | Acc_1: (99.94%) (10362/10368)\n",
      "Epoch: 145 | Batch_idx: 90 |  Loss_1: (0.0030) | Acc_1: (99.92%) (11639/11648)\n",
      "Epoch: 145 | Batch_idx: 100 |  Loss_1: (0.0030) | Acc_1: (99.91%) (12917/12928)\n",
      "Epoch: 145 | Batch_idx: 110 |  Loss_1: (0.0032) | Acc_1: (99.91%) (14195/14208)\n",
      "Epoch: 145 | Batch_idx: 120 |  Loss_1: (0.0032) | Acc_1: (99.91%) (15474/15488)\n",
      "Epoch: 145 | Batch_idx: 130 |  Loss_1: (0.0033) | Acc_1: (99.91%) (16753/16768)\n",
      "Epoch: 145 | Batch_idx: 140 |  Loss_1: (0.0032) | Acc_1: (99.92%) (18033/18048)\n",
      "Epoch: 145 | Batch_idx: 150 |  Loss_1: (0.0036) | Acc_1: (99.90%) (19309/19328)\n",
      "Epoch: 145 | Batch_idx: 160 |  Loss_1: (0.0035) | Acc_1: (99.90%) (20588/20608)\n",
      "Epoch: 145 | Batch_idx: 170 |  Loss_1: (0.0035) | Acc_1: (99.90%) (21867/21888)\n",
      "Epoch: 145 | Batch_idx: 180 |  Loss_1: (0.0038) | Acc_1: (99.90%) (23144/23168)\n",
      "Epoch: 145 | Batch_idx: 190 |  Loss_1: (0.0041) | Acc_1: (99.89%) (24420/24448)\n",
      "Epoch: 145 | Batch_idx: 200 |  Loss_1: (0.0041) | Acc_1: (99.88%) (25698/25728)\n",
      "Epoch: 145 | Batch_idx: 210 |  Loss_1: (0.0041) | Acc_1: (99.89%) (26977/27008)\n",
      "Epoch: 145 | Batch_idx: 220 |  Loss_1: (0.0041) | Acc_1: (99.89%) (28256/28288)\n",
      "Epoch: 145 | Batch_idx: 230 |  Loss_1: (0.0041) | Acc_1: (99.89%) (29535/29568)\n",
      "Epoch: 145 | Batch_idx: 240 |  Loss_1: (0.0044) | Acc_1: (99.88%) (30811/30848)\n",
      "Epoch: 145 | Batch_idx: 250 |  Loss_1: (0.0045) | Acc_1: (99.88%) (32089/32128)\n",
      "Epoch: 145 | Batch_idx: 260 |  Loss_1: (0.0046) | Acc_1: (99.88%) (33367/33408)\n",
      "Epoch: 145 | Batch_idx: 270 |  Loss_1: (0.0046) | Acc_1: (99.88%) (34645/34688)\n",
      "Epoch: 145 | Batch_idx: 280 |  Loss_1: (0.0047) | Acc_1: (99.87%) (35923/35968)\n",
      "Epoch: 145 | Batch_idx: 290 |  Loss_1: (0.0047) | Acc_1: (99.87%) (37201/37248)\n",
      "Epoch: 145 | Batch_idx: 300 |  Loss_1: (0.0049) | Acc_1: (99.87%) (38477/38528)\n",
      "Epoch: 145 | Batch_idx: 310 |  Loss_1: (0.0049) | Acc_1: (99.87%) (39755/39808)\n",
      "Epoch: 145 | Batch_idx: 320 |  Loss_1: (0.0048) | Acc_1: (99.87%) (41035/41088)\n",
      "Epoch: 145 | Batch_idx: 330 |  Loss_1: (0.0049) | Acc_1: (99.87%) (42312/42368)\n",
      "Epoch: 145 | Batch_idx: 340 |  Loss_1: (0.0048) | Acc_1: (99.87%) (43592/43648)\n",
      "Epoch: 145 | Batch_idx: 350 |  Loss_1: (0.0047) | Acc_1: (99.88%) (44872/44928)\n",
      "Epoch: 145 | Batch_idx: 360 |  Loss_1: (0.0047) | Acc_1: (99.87%) (46150/46208)\n",
      "Epoch: 145 | Batch_idx: 370 |  Loss_1: (0.0047) | Acc_1: (99.88%) (47429/47488)\n",
      "Epoch: 145 | Batch_idx: 380 |  Loss_1: (0.0047) | Acc_1: (99.87%) (48706/48768)\n",
      "Epoch: 145 | Batch_idx: 390 |  Loss_1: (0.0046) | Acc_1: (99.88%) (49938/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5166) | Acc: (91.80%) (9180/10000)\n",
      "Epoch: 146 | Batch_idx: 0 |  Loss_1: (0.0025) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 146 | Batch_idx: 10 |  Loss_1: (0.0019) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 146 | Batch_idx: 20 |  Loss_1: (0.0041) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 146 | Batch_idx: 30 |  Loss_1: (0.0034) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 146 | Batch_idx: 40 |  Loss_1: (0.0042) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 146 | Batch_idx: 50 |  Loss_1: (0.0038) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 146 | Batch_idx: 60 |  Loss_1: (0.0036) | Acc_1: (99.87%) (7798/7808)\n",
      "Epoch: 146 | Batch_idx: 70 |  Loss_1: (0.0044) | Acc_1: (99.86%) (9075/9088)\n",
      "Epoch: 146 | Batch_idx: 80 |  Loss_1: (0.0047) | Acc_1: (99.86%) (10353/10368)\n",
      "Epoch: 146 | Batch_idx: 90 |  Loss_1: (0.0046) | Acc_1: (99.85%) (11631/11648)\n",
      "Epoch: 146 | Batch_idx: 100 |  Loss_1: (0.0045) | Acc_1: (99.86%) (12910/12928)\n",
      "Epoch: 146 | Batch_idx: 110 |  Loss_1: (0.0052) | Acc_1: (99.83%) (14184/14208)\n",
      "Epoch: 146 | Batch_idx: 120 |  Loss_1: (0.0054) | Acc_1: (99.83%) (15461/15488)\n",
      "Epoch: 146 | Batch_idx: 130 |  Loss_1: (0.0053) | Acc_1: (99.83%) (16739/16768)\n",
      "Epoch: 146 | Batch_idx: 140 |  Loss_1: (0.0054) | Acc_1: (99.83%) (18018/18048)\n",
      "Epoch: 146 | Batch_idx: 150 |  Loss_1: (0.0052) | Acc_1: (99.84%) (19298/19328)\n",
      "Epoch: 146 | Batch_idx: 160 |  Loss_1: (0.0051) | Acc_1: (99.85%) (20578/20608)\n",
      "Epoch: 146 | Batch_idx: 170 |  Loss_1: (0.0051) | Acc_1: (99.85%) (21856/21888)\n",
      "Epoch: 146 | Batch_idx: 180 |  Loss_1: (0.0051) | Acc_1: (99.85%) (23134/23168)\n",
      "Epoch: 146 | Batch_idx: 190 |  Loss_1: (0.0052) | Acc_1: (99.84%) (24410/24448)\n",
      "Epoch: 146 | Batch_idx: 200 |  Loss_1: (0.0053) | Acc_1: (99.84%) (25686/25728)\n",
      "Epoch: 146 | Batch_idx: 210 |  Loss_1: (0.0053) | Acc_1: (99.83%) (26963/27008)\n",
      "Epoch: 146 | Batch_idx: 220 |  Loss_1: (0.0052) | Acc_1: (99.84%) (28242/28288)\n",
      "Epoch: 146 | Batch_idx: 230 |  Loss_1: (0.0053) | Acc_1: (99.84%) (29520/29568)\n",
      "Epoch: 146 | Batch_idx: 240 |  Loss_1: (0.0052) | Acc_1: (99.84%) (30800/30848)\n",
      "Epoch: 146 | Batch_idx: 250 |  Loss_1: (0.0052) | Acc_1: (99.85%) (32079/32128)\n",
      "Epoch: 146 | Batch_idx: 260 |  Loss_1: (0.0051) | Acc_1: (99.85%) (33358/33408)\n",
      "Epoch: 146 | Batch_idx: 270 |  Loss_1: (0.0051) | Acc_1: (99.85%) (34636/34688)\n",
      "Epoch: 146 | Batch_idx: 280 |  Loss_1: (0.0051) | Acc_1: (99.85%) (35914/35968)\n",
      "Epoch: 146 | Batch_idx: 290 |  Loss_1: (0.0050) | Acc_1: (99.85%) (37191/37248)\n",
      "Epoch: 146 | Batch_idx: 300 |  Loss_1: (0.0051) | Acc_1: (99.85%) (38469/38528)\n",
      "Epoch: 146 | Batch_idx: 310 |  Loss_1: (0.0053) | Acc_1: (99.84%) (39745/39808)\n",
      "Epoch: 146 | Batch_idx: 320 |  Loss_1: (0.0054) | Acc_1: (99.84%) (41022/41088)\n",
      "Epoch: 146 | Batch_idx: 330 |  Loss_1: (0.0055) | Acc_1: (99.83%) (42298/42368)\n",
      "Epoch: 146 | Batch_idx: 340 |  Loss_1: (0.0054) | Acc_1: (99.84%) (43578/43648)\n",
      "Epoch: 146 | Batch_idx: 350 |  Loss_1: (0.0054) | Acc_1: (99.84%) (44855/44928)\n",
      "Epoch: 146 | Batch_idx: 360 |  Loss_1: (0.0054) | Acc_1: (99.84%) (46132/46208)\n",
      "Epoch: 146 | Batch_idx: 370 |  Loss_1: (0.0053) | Acc_1: (99.84%) (47411/47488)\n",
      "Epoch: 146 | Batch_idx: 380 |  Loss_1: (0.0054) | Acc_1: (99.83%) (48687/48768)\n",
      "Epoch: 146 | Batch_idx: 390 |  Loss_1: (0.0053) | Acc_1: (99.84%) (49918/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5255) | Acc: (91.82%) (9182/10000)\n",
      "Epoch: 147 | Batch_idx: 0 |  Loss_1: (0.0017) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 147 | Batch_idx: 10 |  Loss_1: (0.0014) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 147 | Batch_idx: 20 |  Loss_1: (0.0028) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 147 | Batch_idx: 30 |  Loss_1: (0.0033) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 147 | Batch_idx: 40 |  Loss_1: (0.0034) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 147 | Batch_idx: 50 |  Loss_1: (0.0043) | Acc_1: (99.83%) (6517/6528)\n",
      "Epoch: 147 | Batch_idx: 60 |  Loss_1: (0.0047) | Acc_1: (99.82%) (7794/7808)\n",
      "Epoch: 147 | Batch_idx: 70 |  Loss_1: (0.0049) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 147 | Batch_idx: 80 |  Loss_1: (0.0046) | Acc_1: (99.86%) (10353/10368)\n",
      "Epoch: 147 | Batch_idx: 90 |  Loss_1: (0.0046) | Acc_1: (99.85%) (11630/11648)\n",
      "Epoch: 147 | Batch_idx: 100 |  Loss_1: (0.0046) | Acc_1: (99.85%) (12908/12928)\n",
      "Epoch: 147 | Batch_idx: 110 |  Loss_1: (0.0044) | Acc_1: (99.85%) (14186/14208)\n",
      "Epoch: 147 | Batch_idx: 120 |  Loss_1: (0.0048) | Acc_1: (99.83%) (15462/15488)\n",
      "Epoch: 147 | Batch_idx: 130 |  Loss_1: (0.0046) | Acc_1: (99.84%) (16741/16768)\n",
      "Epoch: 147 | Batch_idx: 140 |  Loss_1: (0.0050) | Acc_1: (99.83%) (18018/18048)\n",
      "Epoch: 147 | Batch_idx: 150 |  Loss_1: (0.0050) | Acc_1: (99.83%) (19295/19328)\n",
      "Epoch: 147 | Batch_idx: 160 |  Loss_1: (0.0051) | Acc_1: (99.83%) (20572/20608)\n",
      "Epoch: 147 | Batch_idx: 170 |  Loss_1: (0.0053) | Acc_1: (99.82%) (21848/21888)\n",
      "Epoch: 147 | Batch_idx: 180 |  Loss_1: (0.0051) | Acc_1: (99.83%) (23128/23168)\n",
      "Epoch: 147 | Batch_idx: 190 |  Loss_1: (0.0051) | Acc_1: (99.83%) (24406/24448)\n",
      "Epoch: 147 | Batch_idx: 200 |  Loss_1: (0.0049) | Acc_1: (99.84%) (25686/25728)\n",
      "Epoch: 147 | Batch_idx: 210 |  Loss_1: (0.0052) | Acc_1: (99.83%) (26963/27008)\n",
      "Epoch: 147 | Batch_idx: 220 |  Loss_1: (0.0053) | Acc_1: (99.83%) (28239/28288)\n",
      "Epoch: 147 | Batch_idx: 230 |  Loss_1: (0.0054) | Acc_1: (99.83%) (29517/29568)\n",
      "Epoch: 147 | Batch_idx: 240 |  Loss_1: (0.0054) | Acc_1: (99.83%) (30795/30848)\n",
      "Epoch: 147 | Batch_idx: 250 |  Loss_1: (0.0056) | Acc_1: (99.82%) (32070/32128)\n",
      "Epoch: 147 | Batch_idx: 260 |  Loss_1: (0.0056) | Acc_1: (99.82%) (33349/33408)\n",
      "Epoch: 147 | Batch_idx: 270 |  Loss_1: (0.0056) | Acc_1: (99.82%) (34624/34688)\n",
      "Epoch: 147 | Batch_idx: 280 |  Loss_1: (0.0056) | Acc_1: (99.82%) (35903/35968)\n",
      "Epoch: 147 | Batch_idx: 290 |  Loss_1: (0.0057) | Acc_1: (99.82%) (37181/37248)\n",
      "Epoch: 147 | Batch_idx: 300 |  Loss_1: (0.0056) | Acc_1: (99.82%) (38460/38528)\n",
      "Epoch: 147 | Batch_idx: 310 |  Loss_1: (0.0056) | Acc_1: (99.82%) (39738/39808)\n",
      "Epoch: 147 | Batch_idx: 320 |  Loss_1: (0.0056) | Acc_1: (99.82%) (41014/41088)\n",
      "Epoch: 147 | Batch_idx: 330 |  Loss_1: (0.0057) | Acc_1: (99.82%) (42290/42368)\n",
      "Epoch: 147 | Batch_idx: 340 |  Loss_1: (0.0057) | Acc_1: (99.82%) (43568/43648)\n",
      "Epoch: 147 | Batch_idx: 350 |  Loss_1: (0.0057) | Acc_1: (99.81%) (44844/44928)\n",
      "Epoch: 147 | Batch_idx: 360 |  Loss_1: (0.0057) | Acc_1: (99.81%) (46120/46208)\n",
      "Epoch: 147 | Batch_idx: 370 |  Loss_1: (0.0059) | Acc_1: (99.80%) (47395/47488)\n",
      "Epoch: 147 | Batch_idx: 380 |  Loss_1: (0.0058) | Acc_1: (99.81%) (48673/48768)\n",
      "Epoch: 147 | Batch_idx: 390 |  Loss_1: (0.0058) | Acc_1: (99.81%) (49904/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5454) | Acc: (91.49%) (9149/10000)\n",
      "Epoch: 148 | Batch_idx: 0 |  Loss_1: (0.0062) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 148 | Batch_idx: 10 |  Loss_1: (0.0038) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 148 | Batch_idx: 20 |  Loss_1: (0.0065) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 148 | Batch_idx: 30 |  Loss_1: (0.0069) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 148 | Batch_idx: 40 |  Loss_1: (0.0077) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 148 | Batch_idx: 50 |  Loss_1: (0.0070) | Acc_1: (99.79%) (6514/6528)\n",
      "Epoch: 148 | Batch_idx: 60 |  Loss_1: (0.0082) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 148 | Batch_idx: 70 |  Loss_1: (0.0076) | Acc_1: (99.78%) (9068/9088)\n",
      "Epoch: 148 | Batch_idx: 80 |  Loss_1: (0.0077) | Acc_1: (99.77%) (10344/10368)\n",
      "Epoch: 148 | Batch_idx: 90 |  Loss_1: (0.0077) | Acc_1: (99.77%) (11621/11648)\n",
      "Epoch: 148 | Batch_idx: 100 |  Loss_1: (0.0086) | Acc_1: (99.74%) (12894/12928)\n",
      "Epoch: 148 | Batch_idx: 110 |  Loss_1: (0.0085) | Acc_1: (99.72%) (14168/14208)\n",
      "Epoch: 148 | Batch_idx: 120 |  Loss_1: (0.0086) | Acc_1: (99.69%) (15440/15488)\n",
      "Epoch: 148 | Batch_idx: 130 |  Loss_1: (0.0089) | Acc_1: (99.64%) (16708/16768)\n",
      "Epoch: 148 | Batch_idx: 140 |  Loss_1: (0.0088) | Acc_1: (99.64%) (17983/18048)\n",
      "Epoch: 148 | Batch_idx: 150 |  Loss_1: (0.0087) | Acc_1: (99.65%) (19261/19328)\n",
      "Epoch: 148 | Batch_idx: 160 |  Loss_1: (0.0088) | Acc_1: (99.66%) (20537/20608)\n",
      "Epoch: 148 | Batch_idx: 170 |  Loss_1: (0.0087) | Acc_1: (99.66%) (21814/21888)\n",
      "Epoch: 148 | Batch_idx: 180 |  Loss_1: (0.0084) | Acc_1: (99.68%) (23093/23168)\n",
      "Epoch: 148 | Batch_idx: 190 |  Loss_1: (0.0081) | Acc_1: (99.69%) (24373/24448)\n",
      "Epoch: 148 | Batch_idx: 200 |  Loss_1: (0.0078) | Acc_1: (99.71%) (25653/25728)\n",
      "Epoch: 148 | Batch_idx: 210 |  Loss_1: (0.0076) | Acc_1: (99.71%) (26931/27008)\n",
      "Epoch: 148 | Batch_idx: 220 |  Loss_1: (0.0074) | Acc_1: (99.72%) (28210/28288)\n",
      "Epoch: 148 | Batch_idx: 230 |  Loss_1: (0.0073) | Acc_1: (99.73%) (29488/29568)\n",
      "Epoch: 148 | Batch_idx: 240 |  Loss_1: (0.0071) | Acc_1: (99.74%) (30767/30848)\n",
      "Epoch: 148 | Batch_idx: 250 |  Loss_1: (0.0070) | Acc_1: (99.74%) (32046/32128)\n",
      "Epoch: 148 | Batch_idx: 260 |  Loss_1: (0.0072) | Acc_1: (99.74%) (33320/33408)\n",
      "Epoch: 148 | Batch_idx: 270 |  Loss_1: (0.0072) | Acc_1: (99.74%) (34597/34688)\n",
      "Epoch: 148 | Batch_idx: 280 |  Loss_1: (0.0073) | Acc_1: (99.73%) (35871/35968)\n",
      "Epoch: 148 | Batch_idx: 290 |  Loss_1: (0.0077) | Acc_1: (99.72%) (37145/37248)\n",
      "Epoch: 148 | Batch_idx: 300 |  Loss_1: (0.0079) | Acc_1: (99.71%) (38416/38528)\n",
      "Epoch: 148 | Batch_idx: 310 |  Loss_1: (0.0079) | Acc_1: (99.71%) (39691/39808)\n",
      "Epoch: 148 | Batch_idx: 320 |  Loss_1: (0.0082) | Acc_1: (99.69%) (40962/41088)\n",
      "Epoch: 148 | Batch_idx: 330 |  Loss_1: (0.0083) | Acc_1: (99.68%) (42234/42368)\n",
      "Epoch: 148 | Batch_idx: 340 |  Loss_1: (0.0084) | Acc_1: (99.68%) (43509/43648)\n",
      "Epoch: 148 | Batch_idx: 350 |  Loss_1: (0.0083) | Acc_1: (99.69%) (44787/44928)\n",
      "Epoch: 148 | Batch_idx: 360 |  Loss_1: (0.0083) | Acc_1: (99.69%) (46063/46208)\n",
      "Epoch: 148 | Batch_idx: 370 |  Loss_1: (0.0083) | Acc_1: (99.69%) (47339/47488)\n",
      "Epoch: 148 | Batch_idx: 380 |  Loss_1: (0.0083) | Acc_1: (99.68%) (48614/48768)\n",
      "Epoch: 148 | Batch_idx: 390 |  Loss_1: (0.0083) | Acc_1: (99.68%) (49842/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5683) | Acc: (91.44%) (9144/10000)\n",
      "Epoch: 149 | Batch_idx: 0 |  Loss_1: (0.0092) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 149 | Batch_idx: 10 |  Loss_1: (0.0138) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 149 | Batch_idx: 20 |  Loss_1: (0.0122) | Acc_1: (99.52%) (2675/2688)\n",
      "Epoch: 149 | Batch_idx: 30 |  Loss_1: (0.0125) | Acc_1: (99.52%) (3949/3968)\n",
      "Epoch: 149 | Batch_idx: 40 |  Loss_1: (0.0117) | Acc_1: (99.58%) (5226/5248)\n",
      "Epoch: 149 | Batch_idx: 50 |  Loss_1: (0.0113) | Acc_1: (99.62%) (6503/6528)\n",
      "Epoch: 149 | Batch_idx: 60 |  Loss_1: (0.0112) | Acc_1: (99.62%) (7778/7808)\n",
      "Epoch: 149 | Batch_idx: 70 |  Loss_1: (0.0117) | Acc_1: (99.60%) (9052/9088)\n",
      "Epoch: 149 | Batch_idx: 80 |  Loss_1: (0.0116) | Acc_1: (99.60%) (10327/10368)\n",
      "Epoch: 149 | Batch_idx: 90 |  Loss_1: (0.0115) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 149 | Batch_idx: 100 |  Loss_1: (0.0112) | Acc_1: (99.61%) (12877/12928)\n",
      "Epoch: 149 | Batch_idx: 110 |  Loss_1: (0.0112) | Acc_1: (99.62%) (14154/14208)\n",
      "Epoch: 149 | Batch_idx: 120 |  Loss_1: (0.0105) | Acc_1: (99.64%) (15432/15488)\n",
      "Epoch: 149 | Batch_idx: 130 |  Loss_1: (0.0108) | Acc_1: (99.62%) (16705/16768)\n",
      "Epoch: 149 | Batch_idx: 140 |  Loss_1: (0.0104) | Acc_1: (99.65%) (17984/18048)\n",
      "Epoch: 149 | Batch_idx: 150 |  Loss_1: (0.0102) | Acc_1: (99.64%) (19259/19328)\n",
      "Epoch: 149 | Batch_idx: 160 |  Loss_1: (0.0103) | Acc_1: (99.64%) (20534/20608)\n",
      "Epoch: 149 | Batch_idx: 170 |  Loss_1: (0.0101) | Acc_1: (99.64%) (21810/21888)\n",
      "Epoch: 149 | Batch_idx: 180 |  Loss_1: (0.0099) | Acc_1: (99.65%) (23088/23168)\n",
      "Epoch: 149 | Batch_idx: 190 |  Loss_1: (0.0101) | Acc_1: (99.65%) (24363/24448)\n",
      "Epoch: 149 | Batch_idx: 200 |  Loss_1: (0.0101) | Acc_1: (99.65%) (25637/25728)\n",
      "Epoch: 149 | Batch_idx: 210 |  Loss_1: (0.0104) | Acc_1: (99.63%) (26908/27008)\n",
      "Epoch: 149 | Batch_idx: 220 |  Loss_1: (0.0103) | Acc_1: (99.63%) (28184/28288)\n",
      "Epoch: 149 | Batch_idx: 230 |  Loss_1: (0.0102) | Acc_1: (99.64%) (29461/29568)\n",
      "Epoch: 149 | Batch_idx: 240 |  Loss_1: (0.0103) | Acc_1: (99.63%) (30735/30848)\n",
      "Epoch: 149 | Batch_idx: 250 |  Loss_1: (0.0107) | Acc_1: (99.63%) (32008/32128)\n",
      "Epoch: 149 | Batch_idx: 260 |  Loss_1: (0.0107) | Acc_1: (99.63%) (33283/33408)\n",
      "Epoch: 149 | Batch_idx: 270 |  Loss_1: (0.0108) | Acc_1: (99.63%) (34558/34688)\n",
      "Epoch: 149 | Batch_idx: 280 |  Loss_1: (0.0109) | Acc_1: (99.62%) (35832/35968)\n",
      "Epoch: 149 | Batch_idx: 290 |  Loss_1: (0.0109) | Acc_1: (99.62%) (37107/37248)\n",
      "Epoch: 149 | Batch_idx: 300 |  Loss_1: (0.0109) | Acc_1: (99.62%) (38380/38528)\n",
      "Epoch: 149 | Batch_idx: 310 |  Loss_1: (0.0110) | Acc_1: (99.61%) (39654/39808)\n",
      "Epoch: 149 | Batch_idx: 320 |  Loss_1: (0.0113) | Acc_1: (99.59%) (40921/41088)\n",
      "Epoch: 149 | Batch_idx: 330 |  Loss_1: (0.0114) | Acc_1: (99.58%) (42192/42368)\n",
      "Epoch: 149 | Batch_idx: 340 |  Loss_1: (0.0115) | Acc_1: (99.58%) (43465/43648)\n",
      "Epoch: 149 | Batch_idx: 350 |  Loss_1: (0.0114) | Acc_1: (99.59%) (44742/44928)\n",
      "Epoch: 149 | Batch_idx: 360 |  Loss_1: (0.0113) | Acc_1: (99.59%) (46018/46208)\n",
      "Epoch: 149 | Batch_idx: 370 |  Loss_1: (0.0114) | Acc_1: (99.59%) (47294/47488)\n",
      "Epoch: 149 | Batch_idx: 380 |  Loss_1: (0.0114) | Acc_1: (99.59%) (48567/48768)\n",
      "Epoch: 149 | Batch_idx: 390 |  Loss_1: (0.0115) | Acc_1: (99.59%) (49794/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5941) | Acc: (90.94%) (9094/10000)\n",
      "Epoch: 150 | Batch_idx: 0 |  Loss_1: (0.0104) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 150 | Batch_idx: 10 |  Loss_1: (0.0064) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 150 | Batch_idx: 20 |  Loss_1: (0.0089) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 150 | Batch_idx: 30 |  Loss_1: (0.0086) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 150 | Batch_idx: 40 |  Loss_1: (0.0096) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 150 | Batch_idx: 50 |  Loss_1: (0.0093) | Acc_1: (99.71%) (6509/6528)\n",
      "Epoch: 150 | Batch_idx: 60 |  Loss_1: (0.0103) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 150 | Batch_idx: 70 |  Loss_1: (0.0094) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 150 | Batch_idx: 80 |  Loss_1: (0.0086) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 150 | Batch_idx: 90 |  Loss_1: (0.0088) | Acc_1: (99.73%) (11617/11648)\n",
      "Epoch: 150 | Batch_idx: 100 |  Loss_1: (0.0085) | Acc_1: (99.74%) (12894/12928)\n",
      "Epoch: 150 | Batch_idx: 110 |  Loss_1: (0.0085) | Acc_1: (99.73%) (14170/14208)\n",
      "Epoch: 150 | Batch_idx: 120 |  Loss_1: (0.0080) | Acc_1: (99.75%) (15449/15488)\n",
      "Epoch: 150 | Batch_idx: 130 |  Loss_1: (0.0083) | Acc_1: (99.74%) (16725/16768)\n",
      "Epoch: 150 | Batch_idx: 140 |  Loss_1: (0.0080) | Acc_1: (99.76%) (18005/18048)\n",
      "Epoch: 150 | Batch_idx: 150 |  Loss_1: (0.0086) | Acc_1: (99.73%) (19275/19328)\n",
      "Epoch: 150 | Batch_idx: 160 |  Loss_1: (0.0086) | Acc_1: (99.73%) (20552/20608)\n",
      "Epoch: 150 | Batch_idx: 170 |  Loss_1: (0.0088) | Acc_1: (99.73%) (21828/21888)\n",
      "Epoch: 150 | Batch_idx: 180 |  Loss_1: (0.0088) | Acc_1: (99.73%) (23105/23168)\n",
      "Epoch: 150 | Batch_idx: 190 |  Loss_1: (0.0086) | Acc_1: (99.73%) (24382/24448)\n",
      "Epoch: 150 | Batch_idx: 200 |  Loss_1: (0.0085) | Acc_1: (99.74%) (25660/25728)\n",
      "Epoch: 150 | Batch_idx: 210 |  Loss_1: (0.0083) | Acc_1: (99.74%) (26939/27008)\n",
      "Epoch: 150 | Batch_idx: 220 |  Loss_1: (0.0081) | Acc_1: (99.75%) (28218/28288)\n",
      "Epoch: 150 | Batch_idx: 230 |  Loss_1: (0.0081) | Acc_1: (99.75%) (29495/29568)\n",
      "Epoch: 150 | Batch_idx: 240 |  Loss_1: (0.0078) | Acc_1: (99.76%) (30774/30848)\n",
      "Epoch: 150 | Batch_idx: 250 |  Loss_1: (0.0080) | Acc_1: (99.74%) (32046/32128)\n",
      "Epoch: 150 | Batch_idx: 260 |  Loss_1: (0.0079) | Acc_1: (99.74%) (33321/33408)\n",
      "Epoch: 150 | Batch_idx: 270 |  Loss_1: (0.0079) | Acc_1: (99.74%) (34599/34688)\n",
      "Epoch: 150 | Batch_idx: 280 |  Loss_1: (0.0078) | Acc_1: (99.74%) (35873/35968)\n",
      "Epoch: 150 | Batch_idx: 290 |  Loss_1: (0.0079) | Acc_1: (99.73%) (37149/37248)\n",
      "Epoch: 150 | Batch_idx: 300 |  Loss_1: (0.0078) | Acc_1: (99.74%) (38427/38528)\n",
      "Epoch: 150 | Batch_idx: 310 |  Loss_1: (0.0079) | Acc_1: (99.73%) (39702/39808)\n",
      "Epoch: 150 | Batch_idx: 320 |  Loss_1: (0.0081) | Acc_1: (99.73%) (40978/41088)\n",
      "Epoch: 150 | Batch_idx: 330 |  Loss_1: (0.0081) | Acc_1: (99.73%) (42254/42368)\n",
      "Epoch: 150 | Batch_idx: 340 |  Loss_1: (0.0081) | Acc_1: (99.73%) (43530/43648)\n",
      "Epoch: 150 | Batch_idx: 350 |  Loss_1: (0.0082) | Acc_1: (99.73%) (44807/44928)\n",
      "Epoch: 150 | Batch_idx: 360 |  Loss_1: (0.0081) | Acc_1: (99.73%) (46085/46208)\n",
      "Epoch: 150 | Batch_idx: 370 |  Loss_1: (0.0081) | Acc_1: (99.73%) (47362/47488)\n",
      "Epoch: 150 | Batch_idx: 380 |  Loss_1: (0.0080) | Acc_1: (99.74%) (48640/48768)\n",
      "Epoch: 150 | Batch_idx: 390 |  Loss_1: (0.0080) | Acc_1: (99.74%) (49870/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5437) | Acc: (91.60%) (9160/10000)\n",
      "Epoch: 151 | Batch_idx: 0 |  Loss_1: (0.0023) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 151 | Batch_idx: 10 |  Loss_1: (0.0046) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 151 | Batch_idx: 20 |  Loss_1: (0.0099) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 151 | Batch_idx: 30 |  Loss_1: (0.0098) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 151 | Batch_idx: 40 |  Loss_1: (0.0103) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 151 | Batch_idx: 50 |  Loss_1: (0.0092) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 151 | Batch_idx: 60 |  Loss_1: (0.0098) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 151 | Batch_idx: 70 |  Loss_1: (0.0093) | Acc_1: (99.69%) (9060/9088)\n",
      "Epoch: 151 | Batch_idx: 80 |  Loss_1: (0.0097) | Acc_1: (99.68%) (10335/10368)\n",
      "Epoch: 151 | Batch_idx: 90 |  Loss_1: (0.0093) | Acc_1: (99.69%) (11612/11648)\n",
      "Epoch: 151 | Batch_idx: 100 |  Loss_1: (0.0095) | Acc_1: (99.68%) (12886/12928)\n",
      "Epoch: 151 | Batch_idx: 110 |  Loss_1: (0.0089) | Acc_1: (99.70%) (14165/14208)\n",
      "Epoch: 151 | Batch_idx: 120 |  Loss_1: (0.0084) | Acc_1: (99.72%) (15445/15488)\n",
      "Epoch: 151 | Batch_idx: 130 |  Loss_1: (0.0085) | Acc_1: (99.73%) (16722/16768)\n",
      "Epoch: 151 | Batch_idx: 140 |  Loss_1: (0.0082) | Acc_1: (99.74%) (18001/18048)\n",
      "Epoch: 151 | Batch_idx: 150 |  Loss_1: (0.0079) | Acc_1: (99.75%) (19279/19328)\n",
      "Epoch: 151 | Batch_idx: 160 |  Loss_1: (0.0076) | Acc_1: (99.76%) (20558/20608)\n",
      "Epoch: 151 | Batch_idx: 170 |  Loss_1: (0.0074) | Acc_1: (99.76%) (21836/21888)\n",
      "Epoch: 151 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.77%) (23115/23168)\n",
      "Epoch: 151 | Batch_idx: 190 |  Loss_1: (0.0069) | Acc_1: (99.78%) (24394/24448)\n",
      "Epoch: 151 | Batch_idx: 200 |  Loss_1: (0.0069) | Acc_1: (99.78%) (25672/25728)\n",
      "Epoch: 151 | Batch_idx: 210 |  Loss_1: (0.0068) | Acc_1: (99.78%) (26949/27008)\n",
      "Epoch: 151 | Batch_idx: 220 |  Loss_1: (0.0066) | Acc_1: (99.79%) (28228/28288)\n",
      "Epoch: 151 | Batch_idx: 230 |  Loss_1: (0.0066) | Acc_1: (99.79%) (29505/29568)\n",
      "Epoch: 151 | Batch_idx: 240 |  Loss_1: (0.0065) | Acc_1: (99.79%) (30782/30848)\n",
      "Epoch: 151 | Batch_idx: 250 |  Loss_1: (0.0065) | Acc_1: (99.79%) (32061/32128)\n",
      "Epoch: 151 | Batch_idx: 260 |  Loss_1: (0.0065) | Acc_1: (99.79%) (33338/33408)\n",
      "Epoch: 151 | Batch_idx: 270 |  Loss_1: (0.0065) | Acc_1: (99.79%) (34615/34688)\n",
      "Epoch: 151 | Batch_idx: 280 |  Loss_1: (0.0065) | Acc_1: (99.79%) (35893/35968)\n",
      "Epoch: 151 | Batch_idx: 290 |  Loss_1: (0.0066) | Acc_1: (99.79%) (37170/37248)\n",
      "Epoch: 151 | Batch_idx: 300 |  Loss_1: (0.0067) | Acc_1: (99.78%) (38445/38528)\n",
      "Epoch: 151 | Batch_idx: 310 |  Loss_1: (0.0068) | Acc_1: (99.78%) (39721/39808)\n",
      "Epoch: 151 | Batch_idx: 320 |  Loss_1: (0.0068) | Acc_1: (99.78%) (40999/41088)\n",
      "Epoch: 151 | Batch_idx: 330 |  Loss_1: (0.0068) | Acc_1: (99.78%) (42276/42368)\n",
      "Epoch: 151 | Batch_idx: 340 |  Loss_1: (0.0067) | Acc_1: (99.79%) (43555/43648)\n",
      "Epoch: 151 | Batch_idx: 350 |  Loss_1: (0.0068) | Acc_1: (99.78%) (44831/44928)\n",
      "Epoch: 151 | Batch_idx: 360 |  Loss_1: (0.0068) | Acc_1: (99.79%) (46109/46208)\n",
      "Epoch: 151 | Batch_idx: 370 |  Loss_1: (0.0070) | Acc_1: (99.78%) (47383/47488)\n",
      "Epoch: 151 | Batch_idx: 380 |  Loss_1: (0.0072) | Acc_1: (99.77%) (48658/48768)\n",
      "Epoch: 151 | Batch_idx: 390 |  Loss_1: (0.0071) | Acc_1: (99.78%) (49888/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5373) | Acc: (91.65%) (9165/10000)\n",
      "Epoch: 152 | Batch_idx: 0 |  Loss_1: (0.0036) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 152 | Batch_idx: 10 |  Loss_1: (0.0083) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 152 | Batch_idx: 20 |  Loss_1: (0.0057) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 152 | Batch_idx: 30 |  Loss_1: (0.0067) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 152 | Batch_idx: 40 |  Loss_1: (0.0060) | Acc_1: (99.75%) (5235/5248)\n",
      "Epoch: 152 | Batch_idx: 50 |  Loss_1: (0.0061) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 152 | Batch_idx: 60 |  Loss_1: (0.0067) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 152 | Batch_idx: 70 |  Loss_1: (0.0075) | Acc_1: (99.74%) (9064/9088)\n",
      "Epoch: 152 | Batch_idx: 80 |  Loss_1: (0.0072) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 152 | Batch_idx: 90 |  Loss_1: (0.0071) | Acc_1: (99.74%) (11618/11648)\n",
      "Epoch: 152 | Batch_idx: 100 |  Loss_1: (0.0071) | Acc_1: (99.74%) (12894/12928)\n",
      "Epoch: 152 | Batch_idx: 110 |  Loss_1: (0.0076) | Acc_1: (99.73%) (14169/14208)\n",
      "Epoch: 152 | Batch_idx: 120 |  Loss_1: (0.0076) | Acc_1: (99.72%) (15444/15488)\n",
      "Epoch: 152 | Batch_idx: 130 |  Loss_1: (0.0079) | Acc_1: (99.71%) (16720/16768)\n",
      "Epoch: 152 | Batch_idx: 140 |  Loss_1: (0.0077) | Acc_1: (99.72%) (17997/18048)\n",
      "Epoch: 152 | Batch_idx: 150 |  Loss_1: (0.0074) | Acc_1: (99.73%) (19275/19328)\n",
      "Epoch: 152 | Batch_idx: 160 |  Loss_1: (0.0073) | Acc_1: (99.73%) (20553/20608)\n",
      "Epoch: 152 | Batch_idx: 170 |  Loss_1: (0.0075) | Acc_1: (99.73%) (21828/21888)\n",
      "Epoch: 152 | Batch_idx: 180 |  Loss_1: (0.0074) | Acc_1: (99.73%) (23106/23168)\n",
      "Epoch: 152 | Batch_idx: 190 |  Loss_1: (0.0074) | Acc_1: (99.74%) (24384/24448)\n",
      "Epoch: 152 | Batch_idx: 200 |  Loss_1: (0.0076) | Acc_1: (99.73%) (25659/25728)\n",
      "Epoch: 152 | Batch_idx: 210 |  Loss_1: (0.0076) | Acc_1: (99.73%) (26936/27008)\n",
      "Epoch: 152 | Batch_idx: 220 |  Loss_1: (0.0085) | Acc_1: (99.71%) (28205/28288)\n",
      "Epoch: 152 | Batch_idx: 230 |  Loss_1: (0.0084) | Acc_1: (99.71%) (29483/29568)\n",
      "Epoch: 152 | Batch_idx: 240 |  Loss_1: (0.0085) | Acc_1: (99.70%) (30756/30848)\n",
      "Epoch: 152 | Batch_idx: 250 |  Loss_1: (0.0085) | Acc_1: (99.70%) (32032/32128)\n",
      "Epoch: 152 | Batch_idx: 260 |  Loss_1: (0.0087) | Acc_1: (99.70%) (33307/33408)\n",
      "Epoch: 152 | Batch_idx: 270 |  Loss_1: (0.0090) | Acc_1: (99.69%) (34580/34688)\n",
      "Epoch: 152 | Batch_idx: 280 |  Loss_1: (0.0089) | Acc_1: (99.69%) (35858/35968)\n",
      "Epoch: 152 | Batch_idx: 290 |  Loss_1: (0.0089) | Acc_1: (99.69%) (37134/37248)\n",
      "Epoch: 152 | Batch_idx: 300 |  Loss_1: (0.0088) | Acc_1: (99.69%) (38410/38528)\n",
      "Epoch: 152 | Batch_idx: 310 |  Loss_1: (0.0089) | Acc_1: (99.69%) (39683/39808)\n",
      "Epoch: 152 | Batch_idx: 320 |  Loss_1: (0.0088) | Acc_1: (99.69%) (40961/41088)\n",
      "Epoch: 152 | Batch_idx: 330 |  Loss_1: (0.0087) | Acc_1: (99.69%) (42237/42368)\n",
      "Epoch: 152 | Batch_idx: 340 |  Loss_1: (0.0092) | Acc_1: (99.68%) (43509/43648)\n",
      "Epoch: 152 | Batch_idx: 350 |  Loss_1: (0.0092) | Acc_1: (99.68%) (44786/44928)\n",
      "Epoch: 152 | Batch_idx: 360 |  Loss_1: (0.0094) | Acc_1: (99.68%) (46059/46208)\n",
      "Epoch: 152 | Batch_idx: 370 |  Loss_1: (0.0094) | Acc_1: (99.68%) (47334/47488)\n",
      "Epoch: 152 | Batch_idx: 380 |  Loss_1: (0.0094) | Acc_1: (99.68%) (48612/48768)\n",
      "Epoch: 152 | Batch_idx: 390 |  Loss_1: (0.0097) | Acc_1: (99.67%) (49834/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5441) | Acc: (91.49%) (9149/10000)\n",
      "Epoch: 153 | Batch_idx: 0 |  Loss_1: (0.0170) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 153 | Batch_idx: 10 |  Loss_1: (0.0168) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 153 | Batch_idx: 20 |  Loss_1: (0.0146) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 153 | Batch_idx: 30 |  Loss_1: (0.0115) | Acc_1: (99.57%) (3951/3968)\n",
      "Epoch: 153 | Batch_idx: 40 |  Loss_1: (0.0138) | Acc_1: (99.50%) (5222/5248)\n",
      "Epoch: 153 | Batch_idx: 50 |  Loss_1: (0.0138) | Acc_1: (99.51%) (6496/6528)\n",
      "Epoch: 153 | Batch_idx: 60 |  Loss_1: (0.0124) | Acc_1: (99.55%) (7773/7808)\n",
      "Epoch: 153 | Batch_idx: 70 |  Loss_1: (0.0123) | Acc_1: (99.56%) (9048/9088)\n",
      "Epoch: 153 | Batch_idx: 80 |  Loss_1: (0.0119) | Acc_1: (99.59%) (10325/10368)\n",
      "Epoch: 153 | Batch_idx: 90 |  Loss_1: (0.0114) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 153 | Batch_idx: 100 |  Loss_1: (0.0115) | Acc_1: (99.61%) (12877/12928)\n",
      "Epoch: 153 | Batch_idx: 110 |  Loss_1: (0.0115) | Acc_1: (99.61%) (14153/14208)\n",
      "Epoch: 153 | Batch_idx: 120 |  Loss_1: (0.0118) | Acc_1: (99.61%) (15427/15488)\n",
      "Epoch: 153 | Batch_idx: 130 |  Loss_1: (0.0117) | Acc_1: (99.61%) (16702/16768)\n",
      "Epoch: 153 | Batch_idx: 140 |  Loss_1: (0.0117) | Acc_1: (99.61%) (17977/18048)\n",
      "Epoch: 153 | Batch_idx: 150 |  Loss_1: (0.0119) | Acc_1: (99.60%) (19250/19328)\n",
      "Epoch: 153 | Batch_idx: 160 |  Loss_1: (0.0113) | Acc_1: (99.62%) (20529/20608)\n",
      "Epoch: 153 | Batch_idx: 170 |  Loss_1: (0.0113) | Acc_1: (99.63%) (21806/21888)\n",
      "Epoch: 153 | Batch_idx: 180 |  Loss_1: (0.0113) | Acc_1: (99.62%) (23079/23168)\n",
      "Epoch: 153 | Batch_idx: 190 |  Loss_1: (0.0114) | Acc_1: (99.62%) (24354/24448)\n",
      "Epoch: 153 | Batch_idx: 200 |  Loss_1: (0.0112) | Acc_1: (99.62%) (25630/25728)\n",
      "Epoch: 153 | Batch_idx: 210 |  Loss_1: (0.0109) | Acc_1: (99.63%) (26908/27008)\n",
      "Epoch: 153 | Batch_idx: 220 |  Loss_1: (0.0111) | Acc_1: (99.63%) (28182/28288)\n",
      "Epoch: 153 | Batch_idx: 230 |  Loss_1: (0.0109) | Acc_1: (99.63%) (29458/29568)\n",
      "Epoch: 153 | Batch_idx: 240 |  Loss_1: (0.0110) | Acc_1: (99.63%) (30733/30848)\n",
      "Epoch: 153 | Batch_idx: 250 |  Loss_1: (0.0110) | Acc_1: (99.63%) (32009/32128)\n",
      "Epoch: 153 | Batch_idx: 260 |  Loss_1: (0.0108) | Acc_1: (99.64%) (33287/33408)\n",
      "Epoch: 153 | Batch_idx: 270 |  Loss_1: (0.0108) | Acc_1: (99.64%) (34563/34688)\n",
      "Epoch: 153 | Batch_idx: 280 |  Loss_1: (0.0108) | Acc_1: (99.64%) (35837/35968)\n",
      "Epoch: 153 | Batch_idx: 290 |  Loss_1: (0.0107) | Acc_1: (99.65%) (37116/37248)\n",
      "Epoch: 153 | Batch_idx: 300 |  Loss_1: (0.0106) | Acc_1: (99.64%) (38391/38528)\n",
      "Epoch: 153 | Batch_idx: 310 |  Loss_1: (0.0108) | Acc_1: (99.64%) (39665/39808)\n",
      "Epoch: 153 | Batch_idx: 320 |  Loss_1: (0.0106) | Acc_1: (99.64%) (40941/41088)\n",
      "Epoch: 153 | Batch_idx: 330 |  Loss_1: (0.0105) | Acc_1: (99.65%) (42218/42368)\n",
      "Epoch: 153 | Batch_idx: 340 |  Loss_1: (0.0103) | Acc_1: (99.65%) (43496/43648)\n",
      "Epoch: 153 | Batch_idx: 350 |  Loss_1: (0.0102) | Acc_1: (99.65%) (44772/44928)\n",
      "Epoch: 153 | Batch_idx: 360 |  Loss_1: (0.0103) | Acc_1: (99.65%) (46044/46208)\n",
      "Epoch: 153 | Batch_idx: 370 |  Loss_1: (0.0104) | Acc_1: (99.64%) (47318/47488)\n",
      "Epoch: 153 | Batch_idx: 380 |  Loss_1: (0.0104) | Acc_1: (99.64%) (48592/48768)\n",
      "Epoch: 153 | Batch_idx: 390 |  Loss_1: (0.0104) | Acc_1: (99.64%) (49820/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5628) | Acc: (91.47%) (9147/10000)\n",
      "Epoch: 154 | Batch_idx: 0 |  Loss_1: (0.0024) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 154 | Batch_idx: 10 |  Loss_1: (0.0062) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 154 | Batch_idx: 20 |  Loss_1: (0.0077) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 154 | Batch_idx: 30 |  Loss_1: (0.0085) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 154 | Batch_idx: 40 |  Loss_1: (0.0089) | Acc_1: (99.64%) (5229/5248)\n",
      "Epoch: 154 | Batch_idx: 50 |  Loss_1: (0.0100) | Acc_1: (99.62%) (6503/6528)\n",
      "Epoch: 154 | Batch_idx: 60 |  Loss_1: (0.0091) | Acc_1: (99.67%) (7782/7808)\n",
      "Epoch: 154 | Batch_idx: 70 |  Loss_1: (0.0087) | Acc_1: (99.67%) (9058/9088)\n",
      "Epoch: 154 | Batch_idx: 80 |  Loss_1: (0.0085) | Acc_1: (99.68%) (10335/10368)\n",
      "Epoch: 154 | Batch_idx: 90 |  Loss_1: (0.0083) | Acc_1: (99.70%) (11613/11648)\n",
      "Epoch: 154 | Batch_idx: 100 |  Loss_1: (0.0082) | Acc_1: (99.71%) (12890/12928)\n",
      "Epoch: 154 | Batch_idx: 110 |  Loss_1: (0.0084) | Acc_1: (99.70%) (14166/14208)\n",
      "Epoch: 154 | Batch_idx: 120 |  Loss_1: (0.0084) | Acc_1: (99.70%) (15442/15488)\n",
      "Epoch: 154 | Batch_idx: 130 |  Loss_1: (0.0085) | Acc_1: (99.70%) (16718/16768)\n",
      "Epoch: 154 | Batch_idx: 140 |  Loss_1: (0.0084) | Acc_1: (99.71%) (17995/18048)\n",
      "Epoch: 154 | Batch_idx: 150 |  Loss_1: (0.0084) | Acc_1: (99.71%) (19271/19328)\n",
      "Epoch: 154 | Batch_idx: 160 |  Loss_1: (0.0085) | Acc_1: (99.71%) (20548/20608)\n",
      "Epoch: 154 | Batch_idx: 170 |  Loss_1: (0.0085) | Acc_1: (99.71%) (21825/21888)\n",
      "Epoch: 154 | Batch_idx: 180 |  Loss_1: (0.0088) | Acc_1: (99.70%) (23098/23168)\n",
      "Epoch: 154 | Batch_idx: 190 |  Loss_1: (0.0093) | Acc_1: (99.67%) (24368/24448)\n",
      "Epoch: 154 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.67%) (25644/25728)\n",
      "Epoch: 154 | Batch_idx: 210 |  Loss_1: (0.0094) | Acc_1: (99.65%) (26914/27008)\n",
      "Epoch: 154 | Batch_idx: 220 |  Loss_1: (0.0094) | Acc_1: (99.65%) (28190/28288)\n",
      "Epoch: 154 | Batch_idx: 230 |  Loss_1: (0.0095) | Acc_1: (99.65%) (29465/29568)\n",
      "Epoch: 154 | Batch_idx: 240 |  Loss_1: (0.0095) | Acc_1: (99.65%) (30739/30848)\n",
      "Epoch: 154 | Batch_idx: 250 |  Loss_1: (0.0097) | Acc_1: (99.64%) (32013/32128)\n",
      "Epoch: 154 | Batch_idx: 260 |  Loss_1: (0.0096) | Acc_1: (99.65%) (33290/33408)\n",
      "Epoch: 154 | Batch_idx: 270 |  Loss_1: (0.0094) | Acc_1: (99.66%) (34569/34688)\n",
      "Epoch: 154 | Batch_idx: 280 |  Loss_1: (0.0096) | Acc_1: (99.65%) (35842/35968)\n",
      "Epoch: 154 | Batch_idx: 290 |  Loss_1: (0.0096) | Acc_1: (99.65%) (37118/37248)\n",
      "Epoch: 154 | Batch_idx: 300 |  Loss_1: (0.0096) | Acc_1: (99.65%) (38395/38528)\n",
      "Epoch: 154 | Batch_idx: 310 |  Loss_1: (0.0097) | Acc_1: (99.65%) (39670/39808)\n",
      "Epoch: 154 | Batch_idx: 320 |  Loss_1: (0.0097) | Acc_1: (99.65%) (40943/41088)\n",
      "Epoch: 154 | Batch_idx: 330 |  Loss_1: (0.0095) | Acc_1: (99.65%) (42221/42368)\n",
      "Epoch: 154 | Batch_idx: 340 |  Loss_1: (0.0095) | Acc_1: (99.65%) (43496/43648)\n",
      "Epoch: 154 | Batch_idx: 350 |  Loss_1: (0.0095) | Acc_1: (99.65%) (44770/44928)\n",
      "Epoch: 154 | Batch_idx: 360 |  Loss_1: (0.0096) | Acc_1: (99.64%) (46042/46208)\n",
      "Epoch: 154 | Batch_idx: 370 |  Loss_1: (0.0097) | Acc_1: (99.64%) (47316/47488)\n",
      "Epoch: 154 | Batch_idx: 380 |  Loss_1: (0.0098) | Acc_1: (99.64%) (48590/48768)\n",
      "Epoch: 154 | Batch_idx: 390 |  Loss_1: (0.0098) | Acc_1: (99.63%) (49817/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5609) | Acc: (91.16%) (9116/10000)\n",
      "Epoch: 155 | Batch_idx: 0 |  Loss_1: (0.0159) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 155 | Batch_idx: 10 |  Loss_1: (0.0156) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 155 | Batch_idx: 20 |  Loss_1: (0.0104) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 155 | Batch_idx: 30 |  Loss_1: (0.0114) | Acc_1: (99.62%) (3953/3968)\n",
      "Epoch: 155 | Batch_idx: 40 |  Loss_1: (0.0096) | Acc_1: (99.68%) (5231/5248)\n",
      "Epoch: 155 | Batch_idx: 50 |  Loss_1: (0.0087) | Acc_1: (99.71%) (6509/6528)\n",
      "Epoch: 155 | Batch_idx: 60 |  Loss_1: (0.0103) | Acc_1: (99.67%) (7782/7808)\n",
      "Epoch: 155 | Batch_idx: 70 |  Loss_1: (0.0105) | Acc_1: (99.66%) (9057/9088)\n",
      "Epoch: 155 | Batch_idx: 80 |  Loss_1: (0.0097) | Acc_1: (99.68%) (10335/10368)\n",
      "Epoch: 155 | Batch_idx: 90 |  Loss_1: (0.0103) | Acc_1: (99.66%) (11608/11648)\n",
      "Epoch: 155 | Batch_idx: 100 |  Loss_1: (0.0107) | Acc_1: (99.63%) (12880/12928)\n",
      "Epoch: 155 | Batch_idx: 110 |  Loss_1: (0.0109) | Acc_1: (99.62%) (14154/14208)\n",
      "Epoch: 155 | Batch_idx: 120 |  Loss_1: (0.0103) | Acc_1: (99.64%) (15432/15488)\n",
      "Epoch: 155 | Batch_idx: 130 |  Loss_1: (0.0107) | Acc_1: (99.63%) (16706/16768)\n",
      "Epoch: 155 | Batch_idx: 140 |  Loss_1: (0.0107) | Acc_1: (99.63%) (17981/18048)\n",
      "Epoch: 155 | Batch_idx: 150 |  Loss_1: (0.0103) | Acc_1: (99.64%) (19259/19328)\n",
      "Epoch: 155 | Batch_idx: 160 |  Loss_1: (0.0101) | Acc_1: (99.66%) (20537/20608)\n",
      "Epoch: 155 | Batch_idx: 170 |  Loss_1: (0.0101) | Acc_1: (99.65%) (21812/21888)\n",
      "Epoch: 155 | Batch_idx: 180 |  Loss_1: (0.0100) | Acc_1: (99.67%) (23091/23168)\n",
      "Epoch: 155 | Batch_idx: 190 |  Loss_1: (0.0098) | Acc_1: (99.67%) (24368/24448)\n",
      "Epoch: 155 | Batch_idx: 200 |  Loss_1: (0.0099) | Acc_1: (99.67%) (25642/25728)\n",
      "Epoch: 155 | Batch_idx: 210 |  Loss_1: (0.0099) | Acc_1: (99.67%) (26918/27008)\n",
      "Epoch: 155 | Batch_idx: 220 |  Loss_1: (0.0098) | Acc_1: (99.67%) (28194/28288)\n",
      "Epoch: 155 | Batch_idx: 230 |  Loss_1: (0.0100) | Acc_1: (99.66%) (29467/29568)\n",
      "Epoch: 155 | Batch_idx: 240 |  Loss_1: (0.0101) | Acc_1: (99.66%) (30742/30848)\n",
      "Epoch: 155 | Batch_idx: 250 |  Loss_1: (0.0102) | Acc_1: (99.66%) (32018/32128)\n",
      "Epoch: 155 | Batch_idx: 260 |  Loss_1: (0.0100) | Acc_1: (99.66%) (33296/33408)\n",
      "Epoch: 155 | Batch_idx: 270 |  Loss_1: (0.0099) | Acc_1: (99.67%) (34575/34688)\n",
      "Epoch: 155 | Batch_idx: 280 |  Loss_1: (0.0100) | Acc_1: (99.67%) (35850/35968)\n",
      "Epoch: 155 | Batch_idx: 290 |  Loss_1: (0.0099) | Acc_1: (99.67%) (37126/37248)\n",
      "Epoch: 155 | Batch_idx: 300 |  Loss_1: (0.0098) | Acc_1: (99.67%) (38402/38528)\n",
      "Epoch: 155 | Batch_idx: 310 |  Loss_1: (0.0098) | Acc_1: (99.68%) (39679/39808)\n",
      "Epoch: 155 | Batch_idx: 320 |  Loss_1: (0.0097) | Acc_1: (99.67%) (40954/41088)\n",
      "Epoch: 155 | Batch_idx: 330 |  Loss_1: (0.0096) | Acc_1: (99.68%) (42232/42368)\n",
      "Epoch: 155 | Batch_idx: 340 |  Loss_1: (0.0098) | Acc_1: (99.67%) (43503/43648)\n",
      "Epoch: 155 | Batch_idx: 350 |  Loss_1: (0.0099) | Acc_1: (99.67%) (44778/44928)\n",
      "Epoch: 155 | Batch_idx: 360 |  Loss_1: (0.0099) | Acc_1: (99.66%) (46052/46208)\n",
      "Epoch: 155 | Batch_idx: 370 |  Loss_1: (0.0099) | Acc_1: (99.66%) (47325/47488)\n",
      "Epoch: 155 | Batch_idx: 380 |  Loss_1: (0.0100) | Acc_1: (99.65%) (48598/48768)\n",
      "Epoch: 155 | Batch_idx: 390 |  Loss_1: (0.0102) | Acc_1: (99.65%) (49825/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5897) | Acc: (91.09%) (9109/10000)\n",
      "Epoch: 156 | Batch_idx: 0 |  Loss_1: (0.0039) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 156 | Batch_idx: 10 |  Loss_1: (0.0067) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 156 | Batch_idx: 20 |  Loss_1: (0.0060) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 156 | Batch_idx: 30 |  Loss_1: (0.0056) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 156 | Batch_idx: 40 |  Loss_1: (0.0063) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 156 | Batch_idx: 50 |  Loss_1: (0.0073) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 156 | Batch_idx: 60 |  Loss_1: (0.0079) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 156 | Batch_idx: 70 |  Loss_1: (0.0076) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 156 | Batch_idx: 80 |  Loss_1: (0.0086) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 156 | Batch_idx: 90 |  Loss_1: (0.0091) | Acc_1: (99.71%) (11614/11648)\n",
      "Epoch: 156 | Batch_idx: 100 |  Loss_1: (0.0089) | Acc_1: (99.72%) (12892/12928)\n",
      "Epoch: 156 | Batch_idx: 110 |  Loss_1: (0.0089) | Acc_1: (99.73%) (14169/14208)\n",
      "Epoch: 156 | Batch_idx: 120 |  Loss_1: (0.0093) | Acc_1: (99.70%) (15441/15488)\n",
      "Epoch: 156 | Batch_idx: 130 |  Loss_1: (0.0091) | Acc_1: (99.69%) (16716/16768)\n",
      "Epoch: 156 | Batch_idx: 140 |  Loss_1: (0.0095) | Acc_1: (99.68%) (17990/18048)\n",
      "Epoch: 156 | Batch_idx: 150 |  Loss_1: (0.0095) | Acc_1: (99.66%) (19263/19328)\n",
      "Epoch: 156 | Batch_idx: 160 |  Loss_1: (0.0095) | Acc_1: (99.66%) (20538/20608)\n",
      "Epoch: 156 | Batch_idx: 170 |  Loss_1: (0.0097) | Acc_1: (99.66%) (21813/21888)\n",
      "Epoch: 156 | Batch_idx: 180 |  Loss_1: (0.0103) | Acc_1: (99.64%) (23084/23168)\n",
      "Epoch: 156 | Batch_idx: 190 |  Loss_1: (0.0106) | Acc_1: (99.63%) (24357/24448)\n",
      "Epoch: 156 | Batch_idx: 200 |  Loss_1: (0.0107) | Acc_1: (99.61%) (25628/25728)\n",
      "Epoch: 156 | Batch_idx: 210 |  Loss_1: (0.0107) | Acc_1: (99.61%) (26904/27008)\n",
      "Epoch: 156 | Batch_idx: 220 |  Loss_1: (0.0106) | Acc_1: (99.61%) (28179/28288)\n",
      "Epoch: 156 | Batch_idx: 230 |  Loss_1: (0.0107) | Acc_1: (99.61%) (29453/29568)\n",
      "Epoch: 156 | Batch_idx: 240 |  Loss_1: (0.0108) | Acc_1: (99.61%) (30727/30848)\n",
      "Epoch: 156 | Batch_idx: 250 |  Loss_1: (0.0108) | Acc_1: (99.61%) (32002/32128)\n",
      "Epoch: 156 | Batch_idx: 260 |  Loss_1: (0.0109) | Acc_1: (99.60%) (33275/33408)\n",
      "Epoch: 156 | Batch_idx: 270 |  Loss_1: (0.0109) | Acc_1: (99.61%) (34551/34688)\n",
      "Epoch: 156 | Batch_idx: 280 |  Loss_1: (0.0108) | Acc_1: (99.61%) (35827/35968)\n",
      "Epoch: 156 | Batch_idx: 290 |  Loss_1: (0.0107) | Acc_1: (99.61%) (37103/37248)\n",
      "Epoch: 156 | Batch_idx: 300 |  Loss_1: (0.0105) | Acc_1: (99.62%) (38382/38528)\n",
      "Epoch: 156 | Batch_idx: 310 |  Loss_1: (0.0107) | Acc_1: (99.61%) (39654/39808)\n",
      "Epoch: 156 | Batch_idx: 320 |  Loss_1: (0.0105) | Acc_1: (99.62%) (40932/41088)\n",
      "Epoch: 156 | Batch_idx: 330 |  Loss_1: (0.0104) | Acc_1: (99.62%) (42209/42368)\n",
      "Epoch: 156 | Batch_idx: 340 |  Loss_1: (0.0104) | Acc_1: (99.63%) (43485/43648)\n",
      "Epoch: 156 | Batch_idx: 350 |  Loss_1: (0.0103) | Acc_1: (99.63%) (44762/44928)\n",
      "Epoch: 156 | Batch_idx: 360 |  Loss_1: (0.0102) | Acc_1: (99.63%) (46039/46208)\n",
      "Epoch: 156 | Batch_idx: 370 |  Loss_1: (0.0102) | Acc_1: (99.63%) (47313/47488)\n",
      "Epoch: 156 | Batch_idx: 380 |  Loss_1: (0.0103) | Acc_1: (99.63%) (48588/48768)\n",
      "Epoch: 156 | Batch_idx: 390 |  Loss_1: (0.0105) | Acc_1: (99.63%) (49813/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5503) | Acc: (91.33%) (9133/10000)\n",
      "Epoch: 157 | Batch_idx: 0 |  Loss_1: (0.0459) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 157 | Batch_idx: 10 |  Loss_1: (0.0151) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 157 | Batch_idx: 20 |  Loss_1: (0.0146) | Acc_1: (99.48%) (2674/2688)\n",
      "Epoch: 157 | Batch_idx: 30 |  Loss_1: (0.0137) | Acc_1: (99.52%) (3949/3968)\n",
      "Epoch: 157 | Batch_idx: 40 |  Loss_1: (0.0132) | Acc_1: (99.54%) (5224/5248)\n",
      "Epoch: 157 | Batch_idx: 50 |  Loss_1: (0.0123) | Acc_1: (99.56%) (6499/6528)\n",
      "Epoch: 157 | Batch_idx: 60 |  Loss_1: (0.0118) | Acc_1: (99.58%) (7775/7808)\n",
      "Epoch: 157 | Batch_idx: 70 |  Loss_1: (0.0129) | Acc_1: (99.54%) (9046/9088)\n",
      "Epoch: 157 | Batch_idx: 80 |  Loss_1: (0.0124) | Acc_1: (99.57%) (10323/10368)\n",
      "Epoch: 157 | Batch_idx: 90 |  Loss_1: (0.0117) | Acc_1: (99.61%) (11602/11648)\n",
      "Epoch: 157 | Batch_idx: 100 |  Loss_1: (0.0119) | Acc_1: (99.57%) (12873/12928)\n",
      "Epoch: 157 | Batch_idx: 110 |  Loss_1: (0.0116) | Acc_1: (99.58%) (14149/14208)\n",
      "Epoch: 157 | Batch_idx: 120 |  Loss_1: (0.0126) | Acc_1: (99.57%) (15421/15488)\n",
      "Epoch: 157 | Batch_idx: 130 |  Loss_1: (0.0123) | Acc_1: (99.58%) (16698/16768)\n",
      "Epoch: 157 | Batch_idx: 140 |  Loss_1: (0.0123) | Acc_1: (99.58%) (17973/18048)\n",
      "Epoch: 157 | Batch_idx: 150 |  Loss_1: (0.0122) | Acc_1: (99.59%) (19249/19328)\n",
      "Epoch: 157 | Batch_idx: 160 |  Loss_1: (0.0119) | Acc_1: (99.60%) (20526/20608)\n",
      "Epoch: 157 | Batch_idx: 170 |  Loss_1: (0.0120) | Acc_1: (99.59%) (21799/21888)\n",
      "Epoch: 157 | Batch_idx: 180 |  Loss_1: (0.0122) | Acc_1: (99.60%) (23075/23168)\n",
      "Epoch: 157 | Batch_idx: 190 |  Loss_1: (0.0120) | Acc_1: (99.60%) (24349/24448)\n",
      "Epoch: 157 | Batch_idx: 200 |  Loss_1: (0.0117) | Acc_1: (99.60%) (25626/25728)\n",
      "Epoch: 157 | Batch_idx: 210 |  Loss_1: (0.0117) | Acc_1: (99.60%) (26900/27008)\n",
      "Epoch: 157 | Batch_idx: 220 |  Loss_1: (0.0116) | Acc_1: (99.60%) (28174/28288)\n",
      "Epoch: 157 | Batch_idx: 230 |  Loss_1: (0.0115) | Acc_1: (99.60%) (29451/29568)\n",
      "Epoch: 157 | Batch_idx: 240 |  Loss_1: (0.0115) | Acc_1: (99.60%) (30725/30848)\n",
      "Epoch: 157 | Batch_idx: 250 |  Loss_1: (0.0113) | Acc_1: (99.61%) (32002/32128)\n",
      "Epoch: 157 | Batch_idx: 260 |  Loss_1: (0.0113) | Acc_1: (99.60%) (33276/33408)\n",
      "Epoch: 157 | Batch_idx: 270 |  Loss_1: (0.0113) | Acc_1: (99.60%) (34550/34688)\n",
      "Epoch: 157 | Batch_idx: 280 |  Loss_1: (0.0112) | Acc_1: (99.60%) (35825/35968)\n",
      "Epoch: 157 | Batch_idx: 290 |  Loss_1: (0.0114) | Acc_1: (99.59%) (37097/37248)\n",
      "Epoch: 157 | Batch_idx: 300 |  Loss_1: (0.0112) | Acc_1: (99.60%) (38374/38528)\n",
      "Epoch: 157 | Batch_idx: 310 |  Loss_1: (0.0112) | Acc_1: (99.60%) (39650/39808)\n",
      "Epoch: 157 | Batch_idx: 320 |  Loss_1: (0.0113) | Acc_1: (99.60%) (40923/41088)\n",
      "Epoch: 157 | Batch_idx: 330 |  Loss_1: (0.0114) | Acc_1: (99.60%) (42199/42368)\n",
      "Epoch: 157 | Batch_idx: 340 |  Loss_1: (0.0112) | Acc_1: (99.61%) (43478/43648)\n",
      "Epoch: 157 | Batch_idx: 350 |  Loss_1: (0.0114) | Acc_1: (99.60%) (44749/44928)\n",
      "Epoch: 157 | Batch_idx: 360 |  Loss_1: (0.0113) | Acc_1: (99.60%) (46023/46208)\n",
      "Epoch: 157 | Batch_idx: 370 |  Loss_1: (0.0112) | Acc_1: (99.60%) (47300/47488)\n",
      "Epoch: 157 | Batch_idx: 380 |  Loss_1: (0.0111) | Acc_1: (99.61%) (48578/48768)\n",
      "Epoch: 157 | Batch_idx: 390 |  Loss_1: (0.0109) | Acc_1: (99.62%) (49809/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5369) | Acc: (91.65%) (9165/10000)\n",
      "Epoch: 158 | Batch_idx: 0 |  Loss_1: (0.0025) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 158 | Batch_idx: 10 |  Loss_1: (0.0074) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 158 | Batch_idx: 20 |  Loss_1: (0.0068) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 158 | Batch_idx: 30 |  Loss_1: (0.0079) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 158 | Batch_idx: 40 |  Loss_1: (0.0080) | Acc_1: (99.79%) (5237/5248)\n",
      "Epoch: 158 | Batch_idx: 50 |  Loss_1: (0.0080) | Acc_1: (99.79%) (6514/6528)\n",
      "Epoch: 158 | Batch_idx: 60 |  Loss_1: (0.0075) | Acc_1: (99.80%) (7792/7808)\n",
      "Epoch: 158 | Batch_idx: 70 |  Loss_1: (0.0071) | Acc_1: (99.81%) (9071/9088)\n",
      "Epoch: 158 | Batch_idx: 80 |  Loss_1: (0.0071) | Acc_1: (99.81%) (10348/10368)\n",
      "Epoch: 158 | Batch_idx: 90 |  Loss_1: (0.0071) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 158 | Batch_idx: 100 |  Loss_1: (0.0068) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 158 | Batch_idx: 110 |  Loss_1: (0.0069) | Acc_1: (99.80%) (14180/14208)\n",
      "Epoch: 158 | Batch_idx: 120 |  Loss_1: (0.0066) | Acc_1: (99.81%) (15459/15488)\n",
      "Epoch: 158 | Batch_idx: 130 |  Loss_1: (0.0067) | Acc_1: (99.80%) (16735/16768)\n",
      "Epoch: 158 | Batch_idx: 140 |  Loss_1: (0.0066) | Acc_1: (99.81%) (18013/18048)\n",
      "Epoch: 158 | Batch_idx: 150 |  Loss_1: (0.0066) | Acc_1: (99.81%) (19291/19328)\n",
      "Epoch: 158 | Batch_idx: 160 |  Loss_1: (0.0064) | Acc_1: (99.81%) (20569/20608)\n",
      "Epoch: 158 | Batch_idx: 170 |  Loss_1: (0.0067) | Acc_1: (99.80%) (21844/21888)\n",
      "Epoch: 158 | Batch_idx: 180 |  Loss_1: (0.0065) | Acc_1: (99.81%) (23123/23168)\n",
      "Epoch: 158 | Batch_idx: 190 |  Loss_1: (0.0064) | Acc_1: (99.81%) (24401/24448)\n",
      "Epoch: 158 | Batch_idx: 200 |  Loss_1: (0.0064) | Acc_1: (99.80%) (25677/25728)\n",
      "Epoch: 158 | Batch_idx: 210 |  Loss_1: (0.0063) | Acc_1: (99.81%) (26956/27008)\n",
      "Epoch: 158 | Batch_idx: 220 |  Loss_1: (0.0065) | Acc_1: (99.79%) (28230/28288)\n",
      "Epoch: 158 | Batch_idx: 230 |  Loss_1: (0.0064) | Acc_1: (99.80%) (29508/29568)\n",
      "Epoch: 158 | Batch_idx: 240 |  Loss_1: (0.0064) | Acc_1: (99.80%) (30785/30848)\n",
      "Epoch: 158 | Batch_idx: 250 |  Loss_1: (0.0064) | Acc_1: (99.80%) (32064/32128)\n",
      "Epoch: 158 | Batch_idx: 260 |  Loss_1: (0.0063) | Acc_1: (99.81%) (33343/33408)\n",
      "Epoch: 158 | Batch_idx: 270 |  Loss_1: (0.0066) | Acc_1: (99.79%) (34616/34688)\n",
      "Epoch: 158 | Batch_idx: 280 |  Loss_1: (0.0065) | Acc_1: (99.79%) (35894/35968)\n",
      "Epoch: 158 | Batch_idx: 290 |  Loss_1: (0.0066) | Acc_1: (99.79%) (37171/37248)\n",
      "Epoch: 158 | Batch_idx: 300 |  Loss_1: (0.0066) | Acc_1: (99.79%) (38447/38528)\n",
      "Epoch: 158 | Batch_idx: 310 |  Loss_1: (0.0067) | Acc_1: (99.79%) (39725/39808)\n",
      "Epoch: 158 | Batch_idx: 320 |  Loss_1: (0.0069) | Acc_1: (99.79%) (41002/41088)\n",
      "Epoch: 158 | Batch_idx: 330 |  Loss_1: (0.0068) | Acc_1: (99.79%) (42280/42368)\n",
      "Epoch: 158 | Batch_idx: 340 |  Loss_1: (0.0068) | Acc_1: (99.79%) (43557/43648)\n",
      "Epoch: 158 | Batch_idx: 350 |  Loss_1: (0.0069) | Acc_1: (99.79%) (44832/44928)\n",
      "Epoch: 158 | Batch_idx: 360 |  Loss_1: (0.0069) | Acc_1: (99.79%) (46111/46208)\n",
      "Epoch: 158 | Batch_idx: 370 |  Loss_1: (0.0068) | Acc_1: (99.79%) (47389/47488)\n",
      "Epoch: 158 | Batch_idx: 380 |  Loss_1: (0.0067) | Acc_1: (99.79%) (48668/48768)\n",
      "Epoch: 158 | Batch_idx: 390 |  Loss_1: (0.0066) | Acc_1: (99.80%) (49900/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5286) | Acc: (91.69%) (9169/10000)\n",
      "Epoch: 159 | Batch_idx: 0 |  Loss_1: (0.0021) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 159 | Batch_idx: 10 |  Loss_1: (0.0044) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 159 | Batch_idx: 20 |  Loss_1: (0.0041) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 159 | Batch_idx: 30 |  Loss_1: (0.0034) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 159 | Batch_idx: 40 |  Loss_1: (0.0033) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 159 | Batch_idx: 50 |  Loss_1: (0.0040) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 159 | Batch_idx: 60 |  Loss_1: (0.0044) | Acc_1: (99.82%) (7794/7808)\n",
      "Epoch: 159 | Batch_idx: 70 |  Loss_1: (0.0048) | Acc_1: (99.80%) (9070/9088)\n",
      "Epoch: 159 | Batch_idx: 80 |  Loss_1: (0.0052) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 159 | Batch_idx: 90 |  Loss_1: (0.0049) | Acc_1: (99.81%) (11626/11648)\n",
      "Epoch: 159 | Batch_idx: 100 |  Loss_1: (0.0048) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 159 | Batch_idx: 110 |  Loss_1: (0.0046) | Acc_1: (99.82%) (14182/14208)\n",
      "Epoch: 159 | Batch_idx: 120 |  Loss_1: (0.0045) | Acc_1: (99.83%) (15462/15488)\n",
      "Epoch: 159 | Batch_idx: 130 |  Loss_1: (0.0043) | Acc_1: (99.84%) (16742/16768)\n",
      "Epoch: 159 | Batch_idx: 140 |  Loss_1: (0.0041) | Acc_1: (99.85%) (18021/18048)\n",
      "Epoch: 159 | Batch_idx: 150 |  Loss_1: (0.0039) | Acc_1: (99.86%) (19301/19328)\n",
      "Epoch: 159 | Batch_idx: 160 |  Loss_1: (0.0039) | Acc_1: (99.86%) (20580/20608)\n",
      "Epoch: 159 | Batch_idx: 170 |  Loss_1: (0.0038) | Acc_1: (99.87%) (21859/21888)\n",
      "Epoch: 159 | Batch_idx: 180 |  Loss_1: (0.0039) | Acc_1: (99.87%) (23137/23168)\n",
      "Epoch: 159 | Batch_idx: 190 |  Loss_1: (0.0039) | Acc_1: (99.86%) (24413/24448)\n",
      "Epoch: 159 | Batch_idx: 200 |  Loss_1: (0.0040) | Acc_1: (99.86%) (25691/25728)\n",
      "Epoch: 159 | Batch_idx: 210 |  Loss_1: (0.0039) | Acc_1: (99.86%) (26971/27008)\n",
      "Epoch: 159 | Batch_idx: 220 |  Loss_1: (0.0039) | Acc_1: (99.87%) (28251/28288)\n",
      "Epoch: 159 | Batch_idx: 230 |  Loss_1: (0.0038) | Acc_1: (99.87%) (29530/29568)\n",
      "Epoch: 159 | Batch_idx: 240 |  Loss_1: (0.0039) | Acc_1: (99.86%) (30806/30848)\n",
      "Epoch: 159 | Batch_idx: 250 |  Loss_1: (0.0038) | Acc_1: (99.87%) (32085/32128)\n",
      "Epoch: 159 | Batch_idx: 260 |  Loss_1: (0.0038) | Acc_1: (99.87%) (33363/33408)\n",
      "Epoch: 159 | Batch_idx: 270 |  Loss_1: (0.0038) | Acc_1: (99.87%) (34643/34688)\n",
      "Epoch: 159 | Batch_idx: 280 |  Loss_1: (0.0040) | Acc_1: (99.87%) (35920/35968)\n",
      "Epoch: 159 | Batch_idx: 290 |  Loss_1: (0.0039) | Acc_1: (99.87%) (37199/37248)\n",
      "Epoch: 159 | Batch_idx: 300 |  Loss_1: (0.0040) | Acc_1: (99.87%) (38478/38528)\n",
      "Epoch: 159 | Batch_idx: 310 |  Loss_1: (0.0039) | Acc_1: (99.87%) (39757/39808)\n",
      "Epoch: 159 | Batch_idx: 320 |  Loss_1: (0.0039) | Acc_1: (99.87%) (41036/41088)\n",
      "Epoch: 159 | Batch_idx: 330 |  Loss_1: (0.0038) | Acc_1: (99.88%) (42316/42368)\n",
      "Epoch: 159 | Batch_idx: 340 |  Loss_1: (0.0038) | Acc_1: (99.88%) (43595/43648)\n",
      "Epoch: 159 | Batch_idx: 350 |  Loss_1: (0.0038) | Acc_1: (99.88%) (44872/44928)\n",
      "Epoch: 159 | Batch_idx: 360 |  Loss_1: (0.0040) | Acc_1: (99.86%) (46145/46208)\n",
      "Epoch: 159 | Batch_idx: 370 |  Loss_1: (0.0042) | Acc_1: (99.86%) (47420/47488)\n",
      "Epoch: 159 | Batch_idx: 380 |  Loss_1: (0.0041) | Acc_1: (99.86%) (48699/48768)\n",
      "Epoch: 159 | Batch_idx: 390 |  Loss_1: (0.0044) | Acc_1: (99.85%) (49925/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5528) | Acc: (91.54%) (9154/10000)\n",
      "Epoch: 160 | Batch_idx: 0 |  Loss_1: (0.0206) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 160 | Batch_idx: 10 |  Loss_1: (0.0141) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 160 | Batch_idx: 20 |  Loss_1: (0.0147) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 160 | Batch_idx: 30 |  Loss_1: (0.0120) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 160 | Batch_idx: 40 |  Loss_1: (0.0105) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 160 | Batch_idx: 50 |  Loss_1: (0.0107) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 160 | Batch_idx: 60 |  Loss_1: (0.0099) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 160 | Batch_idx: 70 |  Loss_1: (0.0095) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 160 | Batch_idx: 80 |  Loss_1: (0.0093) | Acc_1: (99.72%) (10339/10368)\n",
      "Epoch: 160 | Batch_idx: 90 |  Loss_1: (0.0102) | Acc_1: (99.72%) (11615/11648)\n",
      "Epoch: 160 | Batch_idx: 100 |  Loss_1: (0.0098) | Acc_1: (99.73%) (12893/12928)\n",
      "Epoch: 160 | Batch_idx: 110 |  Loss_1: (0.0102) | Acc_1: (99.73%) (14169/14208)\n",
      "Epoch: 160 | Batch_idx: 120 |  Loss_1: (0.0103) | Acc_1: (99.72%) (15445/15488)\n",
      "Epoch: 160 | Batch_idx: 130 |  Loss_1: (0.0104) | Acc_1: (99.70%) (16718/16768)\n",
      "Epoch: 160 | Batch_idx: 140 |  Loss_1: (0.0105) | Acc_1: (99.68%) (17991/18048)\n",
      "Epoch: 160 | Batch_idx: 150 |  Loss_1: (0.0100) | Acc_1: (99.69%) (19269/19328)\n",
      "Epoch: 160 | Batch_idx: 160 |  Loss_1: (0.0099) | Acc_1: (99.68%) (20543/20608)\n",
      "Epoch: 160 | Batch_idx: 170 |  Loss_1: (0.0094) | Acc_1: (99.70%) (21823/21888)\n",
      "Epoch: 160 | Batch_idx: 180 |  Loss_1: (0.0093) | Acc_1: (99.70%) (23098/23168)\n",
      "Epoch: 160 | Batch_idx: 190 |  Loss_1: (0.0092) | Acc_1: (99.71%) (24376/24448)\n",
      "Epoch: 160 | Batch_idx: 200 |  Loss_1: (0.0092) | Acc_1: (99.70%) (25651/25728)\n",
      "Epoch: 160 | Batch_idx: 210 |  Loss_1: (0.0090) | Acc_1: (99.71%) (26929/27008)\n",
      "Epoch: 160 | Batch_idx: 220 |  Loss_1: (0.0090) | Acc_1: (99.71%) (28206/28288)\n",
      "Epoch: 160 | Batch_idx: 230 |  Loss_1: (0.0088) | Acc_1: (99.72%) (29484/29568)\n",
      "Epoch: 160 | Batch_idx: 240 |  Loss_1: (0.0088) | Acc_1: (99.71%) (30758/30848)\n",
      "Epoch: 160 | Batch_idx: 250 |  Loss_1: (0.0086) | Acc_1: (99.71%) (32036/32128)\n",
      "Epoch: 160 | Batch_idx: 260 |  Loss_1: (0.0089) | Acc_1: (99.71%) (33310/33408)\n",
      "Epoch: 160 | Batch_idx: 270 |  Loss_1: (0.0092) | Acc_1: (99.70%) (34583/34688)\n",
      "Epoch: 160 | Batch_idx: 280 |  Loss_1: (0.0093) | Acc_1: (99.69%) (35857/35968)\n",
      "Epoch: 160 | Batch_idx: 290 |  Loss_1: (0.0090) | Acc_1: (99.70%) (37137/37248)\n",
      "Epoch: 160 | Batch_idx: 300 |  Loss_1: (0.0089) | Acc_1: (99.70%) (38414/38528)\n",
      "Epoch: 160 | Batch_idx: 310 |  Loss_1: (0.0088) | Acc_1: (99.71%) (39692/39808)\n",
      "Epoch: 160 | Batch_idx: 320 |  Loss_1: (0.0090) | Acc_1: (99.70%) (40964/41088)\n",
      "Epoch: 160 | Batch_idx: 330 |  Loss_1: (0.0089) | Acc_1: (99.70%) (42242/42368)\n",
      "Epoch: 160 | Batch_idx: 340 |  Loss_1: (0.0088) | Acc_1: (99.70%) (43518/43648)\n",
      "Epoch: 160 | Batch_idx: 350 |  Loss_1: (0.0089) | Acc_1: (99.69%) (44790/44928)\n",
      "Epoch: 160 | Batch_idx: 360 |  Loss_1: (0.0089) | Acc_1: (99.69%) (46063/46208)\n",
      "Epoch: 160 | Batch_idx: 370 |  Loss_1: (0.0090) | Acc_1: (99.68%) (47338/47488)\n",
      "Epoch: 160 | Batch_idx: 380 |  Loss_1: (0.0089) | Acc_1: (99.69%) (48615/48768)\n",
      "Epoch: 160 | Batch_idx: 390 |  Loss_1: (0.0089) | Acc_1: (99.69%) (49844/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5828) | Acc: (91.24%) (9124/10000)\n",
      "Epoch: 161 | Batch_idx: 0 |  Loss_1: (0.0144) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 161 | Batch_idx: 10 |  Loss_1: (0.0061) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 161 | Batch_idx: 20 |  Loss_1: (0.0100) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 161 | Batch_idx: 30 |  Loss_1: (0.0091) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 161 | Batch_idx: 40 |  Loss_1: (0.0101) | Acc_1: (99.66%) (5230/5248)\n",
      "Epoch: 161 | Batch_idx: 50 |  Loss_1: (0.0102) | Acc_1: (99.63%) (6504/6528)\n",
      "Epoch: 161 | Batch_idx: 60 |  Loss_1: (0.0102) | Acc_1: (99.60%) (7777/7808)\n",
      "Epoch: 161 | Batch_idx: 70 |  Loss_1: (0.0105) | Acc_1: (99.60%) (9052/9088)\n",
      "Epoch: 161 | Batch_idx: 80 |  Loss_1: (0.0102) | Acc_1: (99.61%) (10328/10368)\n",
      "Epoch: 161 | Batch_idx: 90 |  Loss_1: (0.0103) | Acc_1: (99.62%) (11604/11648)\n",
      "Epoch: 161 | Batch_idx: 100 |  Loss_1: (0.0103) | Acc_1: (99.61%) (12877/12928)\n",
      "Epoch: 161 | Batch_idx: 110 |  Loss_1: (0.0102) | Acc_1: (99.61%) (14153/14208)\n",
      "Epoch: 161 | Batch_idx: 120 |  Loss_1: (0.0098) | Acc_1: (99.63%) (15431/15488)\n",
      "Epoch: 161 | Batch_idx: 130 |  Loss_1: (0.0102) | Acc_1: (99.62%) (16705/16768)\n",
      "Epoch: 161 | Batch_idx: 140 |  Loss_1: (0.0104) | Acc_1: (99.61%) (17978/18048)\n",
      "Epoch: 161 | Batch_idx: 150 |  Loss_1: (0.0103) | Acc_1: (99.62%) (19254/19328)\n",
      "Epoch: 161 | Batch_idx: 160 |  Loss_1: (0.0101) | Acc_1: (99.63%) (20531/20608)\n",
      "Epoch: 161 | Batch_idx: 170 |  Loss_1: (0.0106) | Acc_1: (99.62%) (21805/21888)\n",
      "Epoch: 161 | Batch_idx: 180 |  Loss_1: (0.0106) | Acc_1: (99.62%) (23080/23168)\n",
      "Epoch: 161 | Batch_idx: 190 |  Loss_1: (0.0105) | Acc_1: (99.63%) (24357/24448)\n",
      "Epoch: 161 | Batch_idx: 200 |  Loss_1: (0.0103) | Acc_1: (99.64%) (25635/25728)\n",
      "Epoch: 161 | Batch_idx: 210 |  Loss_1: (0.0101) | Acc_1: (99.64%) (26912/27008)\n",
      "Epoch: 161 | Batch_idx: 220 |  Loss_1: (0.0100) | Acc_1: (99.65%) (28190/28288)\n",
      "Epoch: 161 | Batch_idx: 230 |  Loss_1: (0.0097) | Acc_1: (99.67%) (29469/29568)\n",
      "Epoch: 161 | Batch_idx: 240 |  Loss_1: (0.0095) | Acc_1: (99.67%) (30747/30848)\n",
      "Epoch: 161 | Batch_idx: 250 |  Loss_1: (0.0092) | Acc_1: (99.68%) (32026/32128)\n",
      "Epoch: 161 | Batch_idx: 260 |  Loss_1: (0.0090) | Acc_1: (99.69%) (33306/33408)\n",
      "Epoch: 161 | Batch_idx: 270 |  Loss_1: (0.0089) | Acc_1: (99.70%) (34583/34688)\n",
      "Epoch: 161 | Batch_idx: 280 |  Loss_1: (0.0088) | Acc_1: (99.70%) (35861/35968)\n",
      "Epoch: 161 | Batch_idx: 290 |  Loss_1: (0.0086) | Acc_1: (99.71%) (37139/37248)\n",
      "Epoch: 161 | Batch_idx: 300 |  Loss_1: (0.0086) | Acc_1: (99.70%) (38414/38528)\n",
      "Epoch: 161 | Batch_idx: 310 |  Loss_1: (0.0086) | Acc_1: (99.71%) (39692/39808)\n",
      "Epoch: 161 | Batch_idx: 320 |  Loss_1: (0.0087) | Acc_1: (99.70%) (40964/41088)\n",
      "Epoch: 161 | Batch_idx: 330 |  Loss_1: (0.0087) | Acc_1: (99.70%) (42239/42368)\n",
      "Epoch: 161 | Batch_idx: 340 |  Loss_1: (0.0085) | Acc_1: (99.70%) (43516/43648)\n",
      "Epoch: 161 | Batch_idx: 350 |  Loss_1: (0.0084) | Acc_1: (99.70%) (44794/44928)\n",
      "Epoch: 161 | Batch_idx: 360 |  Loss_1: (0.0082) | Acc_1: (99.71%) (46073/46208)\n",
      "Epoch: 161 | Batch_idx: 370 |  Loss_1: (0.0082) | Acc_1: (99.71%) (47351/47488)\n",
      "Epoch: 161 | Batch_idx: 380 |  Loss_1: (0.0082) | Acc_1: (99.71%) (48628/48768)\n",
      "Epoch: 161 | Batch_idx: 390 |  Loss_1: (0.0081) | Acc_1: (99.71%) (49857/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5416) | Acc: (91.61%) (9161/10000)\n",
      "Epoch: 162 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 162 | Batch_idx: 10 |  Loss_1: (0.0026) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 162 | Batch_idx: 20 |  Loss_1: (0.0020) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 162 | Batch_idx: 30 |  Loss_1: (0.0037) | Acc_1: (99.90%) (3964/3968)\n",
      "Epoch: 162 | Batch_idx: 40 |  Loss_1: (0.0048) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 162 | Batch_idx: 50 |  Loss_1: (0.0050) | Acc_1: (99.82%) (6516/6528)\n",
      "Epoch: 162 | Batch_idx: 60 |  Loss_1: (0.0051) | Acc_1: (99.80%) (7792/7808)\n",
      "Epoch: 162 | Batch_idx: 70 |  Loss_1: (0.0056) | Acc_1: (99.79%) (9069/9088)\n",
      "Epoch: 162 | Batch_idx: 80 |  Loss_1: (0.0057) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 162 | Batch_idx: 90 |  Loss_1: (0.0058) | Acc_1: (99.79%) (11623/11648)\n",
      "Epoch: 162 | Batch_idx: 100 |  Loss_1: (0.0055) | Acc_1: (99.81%) (12903/12928)\n",
      "Epoch: 162 | Batch_idx: 110 |  Loss_1: (0.0052) | Acc_1: (99.82%) (14182/14208)\n",
      "Epoch: 162 | Batch_idx: 120 |  Loss_1: (0.0053) | Acc_1: (99.82%) (15460/15488)\n",
      "Epoch: 162 | Batch_idx: 130 |  Loss_1: (0.0052) | Acc_1: (99.83%) (16739/16768)\n",
      "Epoch: 162 | Batch_idx: 140 |  Loss_1: (0.0051) | Acc_1: (99.83%) (18018/18048)\n",
      "Epoch: 162 | Batch_idx: 150 |  Loss_1: (0.0053) | Acc_1: (99.83%) (19295/19328)\n",
      "Epoch: 162 | Batch_idx: 160 |  Loss_1: (0.0051) | Acc_1: (99.83%) (20573/20608)\n",
      "Epoch: 162 | Batch_idx: 170 |  Loss_1: (0.0054) | Acc_1: (99.82%) (21848/21888)\n",
      "Epoch: 162 | Batch_idx: 180 |  Loss_1: (0.0052) | Acc_1: (99.83%) (23128/23168)\n",
      "Epoch: 162 | Batch_idx: 190 |  Loss_1: (0.0053) | Acc_1: (99.82%) (24404/24448)\n",
      "Epoch: 162 | Batch_idx: 200 |  Loss_1: (0.0051) | Acc_1: (99.83%) (25683/25728)\n",
      "Epoch: 162 | Batch_idx: 210 |  Loss_1: (0.0054) | Acc_1: (99.83%) (26961/27008)\n",
      "Epoch: 162 | Batch_idx: 220 |  Loss_1: (0.0053) | Acc_1: (99.82%) (28238/28288)\n",
      "Epoch: 162 | Batch_idx: 230 |  Loss_1: (0.0054) | Acc_1: (99.82%) (29514/29568)\n",
      "Epoch: 162 | Batch_idx: 240 |  Loss_1: (0.0054) | Acc_1: (99.82%) (30792/30848)\n",
      "Epoch: 162 | Batch_idx: 250 |  Loss_1: (0.0055) | Acc_1: (99.81%) (32068/32128)\n",
      "Epoch: 162 | Batch_idx: 260 |  Loss_1: (0.0055) | Acc_1: (99.81%) (33344/33408)\n",
      "Epoch: 162 | Batch_idx: 270 |  Loss_1: (0.0055) | Acc_1: (99.81%) (34622/34688)\n",
      "Epoch: 162 | Batch_idx: 280 |  Loss_1: (0.0056) | Acc_1: (99.81%) (35899/35968)\n",
      "Epoch: 162 | Batch_idx: 290 |  Loss_1: (0.0055) | Acc_1: (99.81%) (37178/37248)\n",
      "Epoch: 162 | Batch_idx: 300 |  Loss_1: (0.0054) | Acc_1: (99.81%) (38456/38528)\n",
      "Epoch: 162 | Batch_idx: 310 |  Loss_1: (0.0054) | Acc_1: (99.81%) (39733/39808)\n",
      "Epoch: 162 | Batch_idx: 320 |  Loss_1: (0.0053) | Acc_1: (99.81%) (41011/41088)\n",
      "Epoch: 162 | Batch_idx: 330 |  Loss_1: (0.0052) | Acc_1: (99.82%) (42291/42368)\n",
      "Epoch: 162 | Batch_idx: 340 |  Loss_1: (0.0054) | Acc_1: (99.81%) (43566/43648)\n",
      "Epoch: 162 | Batch_idx: 350 |  Loss_1: (0.0055) | Acc_1: (99.81%) (44843/44928)\n",
      "Epoch: 162 | Batch_idx: 360 |  Loss_1: (0.0055) | Acc_1: (99.81%) (46120/46208)\n",
      "Epoch: 162 | Batch_idx: 370 |  Loss_1: (0.0055) | Acc_1: (99.81%) (47399/47488)\n",
      "Epoch: 162 | Batch_idx: 380 |  Loss_1: (0.0056) | Acc_1: (99.81%) (48675/48768)\n",
      "Epoch: 162 | Batch_idx: 390 |  Loss_1: (0.0055) | Acc_1: (99.81%) (49904/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5073) | Acc: (91.77%) (9177/10000)\n",
      "Epoch: 163 | Batch_idx: 0 |  Loss_1: (0.0045) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 163 | Batch_idx: 10 |  Loss_1: (0.0025) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 163 | Batch_idx: 20 |  Loss_1: (0.0073) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 163 | Batch_idx: 30 |  Loss_1: (0.0067) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 163 | Batch_idx: 40 |  Loss_1: (0.0068) | Acc_1: (99.81%) (5238/5248)\n",
      "Epoch: 163 | Batch_idx: 50 |  Loss_1: (0.0069) | Acc_1: (99.80%) (6515/6528)\n",
      "Epoch: 163 | Batch_idx: 60 |  Loss_1: (0.0077) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 163 | Batch_idx: 70 |  Loss_1: (0.0072) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 163 | Batch_idx: 80 |  Loss_1: (0.0074) | Acc_1: (99.76%) (10343/10368)\n",
      "Epoch: 163 | Batch_idx: 90 |  Loss_1: (0.0072) | Acc_1: (99.76%) (11620/11648)\n",
      "Epoch: 163 | Batch_idx: 100 |  Loss_1: (0.0067) | Acc_1: (99.78%) (12899/12928)\n",
      "Epoch: 163 | Batch_idx: 110 |  Loss_1: (0.0064) | Acc_1: (99.78%) (14177/14208)\n",
      "Epoch: 163 | Batch_idx: 120 |  Loss_1: (0.0063) | Acc_1: (99.77%) (15453/15488)\n",
      "Epoch: 163 | Batch_idx: 130 |  Loss_1: (0.0062) | Acc_1: (99.78%) (16731/16768)\n",
      "Epoch: 163 | Batch_idx: 140 |  Loss_1: (0.0060) | Acc_1: (99.78%) (18008/18048)\n",
      "Epoch: 163 | Batch_idx: 150 |  Loss_1: (0.0064) | Acc_1: (99.76%) (19282/19328)\n",
      "Epoch: 163 | Batch_idx: 160 |  Loss_1: (0.0062) | Acc_1: (99.78%) (20562/20608)\n",
      "Epoch: 163 | Batch_idx: 170 |  Loss_1: (0.0064) | Acc_1: (99.77%) (21838/21888)\n",
      "Epoch: 163 | Batch_idx: 180 |  Loss_1: (0.0063) | Acc_1: (99.78%) (23117/23168)\n",
      "Epoch: 163 | Batch_idx: 190 |  Loss_1: (0.0067) | Acc_1: (99.78%) (24394/24448)\n",
      "Epoch: 163 | Batch_idx: 200 |  Loss_1: (0.0068) | Acc_1: (99.78%) (25671/25728)\n",
      "Epoch: 163 | Batch_idx: 210 |  Loss_1: (0.0068) | Acc_1: (99.78%) (26948/27008)\n",
      "Epoch: 163 | Batch_idx: 220 |  Loss_1: (0.0071) | Acc_1: (99.77%) (28223/28288)\n",
      "Epoch: 163 | Batch_idx: 230 |  Loss_1: (0.0074) | Acc_1: (99.76%) (29498/29568)\n",
      "Epoch: 163 | Batch_idx: 240 |  Loss_1: (0.0074) | Acc_1: (99.76%) (30774/30848)\n",
      "Epoch: 163 | Batch_idx: 250 |  Loss_1: (0.0075) | Acc_1: (99.76%) (32050/32128)\n",
      "Epoch: 163 | Batch_idx: 260 |  Loss_1: (0.0076) | Acc_1: (99.75%) (33326/33408)\n",
      "Epoch: 163 | Batch_idx: 270 |  Loss_1: (0.0077) | Acc_1: (99.75%) (34600/34688)\n",
      "Epoch: 163 | Batch_idx: 280 |  Loss_1: (0.0076) | Acc_1: (99.75%) (35878/35968)\n",
      "Epoch: 163 | Batch_idx: 290 |  Loss_1: (0.0075) | Acc_1: (99.75%) (37154/37248)\n",
      "Epoch: 163 | Batch_idx: 300 |  Loss_1: (0.0075) | Acc_1: (99.75%) (38430/38528)\n",
      "Epoch: 163 | Batch_idx: 310 |  Loss_1: (0.0076) | Acc_1: (99.75%) (39707/39808)\n",
      "Epoch: 163 | Batch_idx: 320 |  Loss_1: (0.0076) | Acc_1: (99.74%) (40983/41088)\n",
      "Epoch: 163 | Batch_idx: 330 |  Loss_1: (0.0075) | Acc_1: (99.75%) (42262/42368)\n",
      "Epoch: 163 | Batch_idx: 340 |  Loss_1: (0.0074) | Acc_1: (99.75%) (43540/43648)\n",
      "Epoch: 163 | Batch_idx: 350 |  Loss_1: (0.0073) | Acc_1: (99.76%) (44818/44928)\n",
      "Epoch: 163 | Batch_idx: 360 |  Loss_1: (0.0073) | Acc_1: (99.76%) (46095/46208)\n",
      "Epoch: 163 | Batch_idx: 370 |  Loss_1: (0.0073) | Acc_1: (99.76%) (47372/47488)\n",
      "Epoch: 163 | Batch_idx: 380 |  Loss_1: (0.0073) | Acc_1: (99.75%) (48648/48768)\n",
      "Epoch: 163 | Batch_idx: 390 |  Loss_1: (0.0072) | Acc_1: (99.76%) (49879/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5073) | Acc: (91.97%) (9197/10000)\n",
      "Epoch: 164 | Batch_idx: 0 |  Loss_1: (0.0033) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 164 | Batch_idx: 10 |  Loss_1: (0.0026) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 164 | Batch_idx: 20 |  Loss_1: (0.0022) | Acc_1: (99.93%) (2686/2688)\n",
      "Epoch: 164 | Batch_idx: 30 |  Loss_1: (0.0029) | Acc_1: (99.90%) (3964/3968)\n",
      "Epoch: 164 | Batch_idx: 40 |  Loss_1: (0.0039) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 164 | Batch_idx: 50 |  Loss_1: (0.0047) | Acc_1: (99.79%) (6514/6528)\n",
      "Epoch: 164 | Batch_idx: 60 |  Loss_1: (0.0050) | Acc_1: (99.81%) (7793/7808)\n",
      "Epoch: 164 | Batch_idx: 70 |  Loss_1: (0.0046) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 164 | Batch_idx: 80 |  Loss_1: (0.0051) | Acc_1: (99.81%) (10348/10368)\n",
      "Epoch: 164 | Batch_idx: 90 |  Loss_1: (0.0053) | Acc_1: (99.79%) (11623/11648)\n",
      "Epoch: 164 | Batch_idx: 100 |  Loss_1: (0.0056) | Acc_1: (99.78%) (12900/12928)\n",
      "Epoch: 164 | Batch_idx: 110 |  Loss_1: (0.0056) | Acc_1: (99.77%) (14175/14208)\n",
      "Epoch: 164 | Batch_idx: 120 |  Loss_1: (0.0054) | Acc_1: (99.78%) (15454/15488)\n",
      "Epoch: 164 | Batch_idx: 130 |  Loss_1: (0.0057) | Acc_1: (99.77%) (16730/16768)\n",
      "Epoch: 164 | Batch_idx: 140 |  Loss_1: (0.0059) | Acc_1: (99.77%) (18007/18048)\n",
      "Epoch: 164 | Batch_idx: 150 |  Loss_1: (0.0057) | Acc_1: (99.79%) (19287/19328)\n",
      "Epoch: 164 | Batch_idx: 160 |  Loss_1: (0.0056) | Acc_1: (99.79%) (20565/20608)\n",
      "Epoch: 164 | Batch_idx: 170 |  Loss_1: (0.0056) | Acc_1: (99.79%) (21841/21888)\n",
      "Epoch: 164 | Batch_idx: 180 |  Loss_1: (0.0058) | Acc_1: (99.78%) (23116/23168)\n",
      "Epoch: 164 | Batch_idx: 190 |  Loss_1: (0.0058) | Acc_1: (99.78%) (24394/24448)\n",
      "Epoch: 164 | Batch_idx: 200 |  Loss_1: (0.0059) | Acc_1: (99.77%) (25669/25728)\n",
      "Epoch: 164 | Batch_idx: 210 |  Loss_1: (0.0059) | Acc_1: (99.77%) (26946/27008)\n",
      "Epoch: 164 | Batch_idx: 220 |  Loss_1: (0.0060) | Acc_1: (99.77%) (28222/28288)\n",
      "Epoch: 164 | Batch_idx: 230 |  Loss_1: (0.0061) | Acc_1: (99.77%) (29499/29568)\n",
      "Epoch: 164 | Batch_idx: 240 |  Loss_1: (0.0062) | Acc_1: (99.76%) (30774/30848)\n",
      "Epoch: 164 | Batch_idx: 250 |  Loss_1: (0.0062) | Acc_1: (99.76%) (32052/32128)\n",
      "Epoch: 164 | Batch_idx: 260 |  Loss_1: (0.0063) | Acc_1: (99.75%) (33326/33408)\n",
      "Epoch: 164 | Batch_idx: 270 |  Loss_1: (0.0062) | Acc_1: (99.76%) (34606/34688)\n",
      "Epoch: 164 | Batch_idx: 280 |  Loss_1: (0.0061) | Acc_1: (99.77%) (35885/35968)\n",
      "Epoch: 164 | Batch_idx: 290 |  Loss_1: (0.0063) | Acc_1: (99.76%) (37158/37248)\n",
      "Epoch: 164 | Batch_idx: 300 |  Loss_1: (0.0064) | Acc_1: (99.75%) (38433/38528)\n",
      "Epoch: 164 | Batch_idx: 310 |  Loss_1: (0.0065) | Acc_1: (99.75%) (39708/39808)\n",
      "Epoch: 164 | Batch_idx: 320 |  Loss_1: (0.0067) | Acc_1: (99.74%) (40983/41088)\n",
      "Epoch: 164 | Batch_idx: 330 |  Loss_1: (0.0067) | Acc_1: (99.75%) (42260/42368)\n",
      "Epoch: 164 | Batch_idx: 340 |  Loss_1: (0.0067) | Acc_1: (99.75%) (43538/43648)\n",
      "Epoch: 164 | Batch_idx: 350 |  Loss_1: (0.0068) | Acc_1: (99.75%) (44816/44928)\n",
      "Epoch: 164 | Batch_idx: 360 |  Loss_1: (0.0068) | Acc_1: (99.75%) (46093/46208)\n",
      "Epoch: 164 | Batch_idx: 370 |  Loss_1: (0.0068) | Acc_1: (99.75%) (47371/47488)\n",
      "Epoch: 164 | Batch_idx: 380 |  Loss_1: (0.0068) | Acc_1: (99.75%) (48648/48768)\n",
      "Epoch: 164 | Batch_idx: 390 |  Loss_1: (0.0068) | Acc_1: (99.75%) (49877/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5411) | Acc: (91.68%) (9168/10000)\n",
      "Epoch: 165 | Batch_idx: 0 |  Loss_1: (0.0006) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 165 | Batch_idx: 10 |  Loss_1: (0.0034) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 165 | Batch_idx: 20 |  Loss_1: (0.0069) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 165 | Batch_idx: 30 |  Loss_1: (0.0076) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 165 | Batch_idx: 40 |  Loss_1: (0.0070) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 165 | Batch_idx: 50 |  Loss_1: (0.0079) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 165 | Batch_idx: 60 |  Loss_1: (0.0080) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 165 | Batch_idx: 70 |  Loss_1: (0.0085) | Acc_1: (99.70%) (9061/9088)\n",
      "Epoch: 165 | Batch_idx: 80 |  Loss_1: (0.0098) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 165 | Batch_idx: 90 |  Loss_1: (0.0099) | Acc_1: (99.69%) (11612/11648)\n",
      "Epoch: 165 | Batch_idx: 100 |  Loss_1: (0.0104) | Acc_1: (99.67%) (12885/12928)\n",
      "Epoch: 165 | Batch_idx: 110 |  Loss_1: (0.0101) | Acc_1: (99.67%) (14161/14208)\n",
      "Epoch: 165 | Batch_idx: 120 |  Loss_1: (0.0098) | Acc_1: (99.68%) (15438/15488)\n",
      "Epoch: 165 | Batch_idx: 130 |  Loss_1: (0.0101) | Acc_1: (99.67%) (16712/16768)\n",
      "Epoch: 165 | Batch_idx: 140 |  Loss_1: (0.0097) | Acc_1: (99.68%) (17991/18048)\n",
      "Epoch: 165 | Batch_idx: 150 |  Loss_1: (0.0097) | Acc_1: (99.68%) (19267/19328)\n",
      "Epoch: 165 | Batch_idx: 160 |  Loss_1: (0.0097) | Acc_1: (99.68%) (20543/20608)\n",
      "Epoch: 165 | Batch_idx: 170 |  Loss_1: (0.0095) | Acc_1: (99.69%) (21821/21888)\n",
      "Epoch: 165 | Batch_idx: 180 |  Loss_1: (0.0096) | Acc_1: (99.68%) (23095/23168)\n",
      "Epoch: 165 | Batch_idx: 190 |  Loss_1: (0.0098) | Acc_1: (99.68%) (24369/24448)\n",
      "Epoch: 165 | Batch_idx: 200 |  Loss_1: (0.0095) | Acc_1: (99.69%) (25647/25728)\n",
      "Epoch: 165 | Batch_idx: 210 |  Loss_1: (0.0094) | Acc_1: (99.69%) (26924/27008)\n",
      "Epoch: 165 | Batch_idx: 220 |  Loss_1: (0.0092) | Acc_1: (99.70%) (28202/28288)\n",
      "Epoch: 165 | Batch_idx: 230 |  Loss_1: (0.0090) | Acc_1: (99.70%) (29480/29568)\n",
      "Epoch: 165 | Batch_idx: 240 |  Loss_1: (0.0087) | Acc_1: (99.71%) (30759/30848)\n",
      "Epoch: 165 | Batch_idx: 250 |  Loss_1: (0.0086) | Acc_1: (99.72%) (32038/32128)\n",
      "Epoch: 165 | Batch_idx: 260 |  Loss_1: (0.0085) | Acc_1: (99.72%) (33315/33408)\n",
      "Epoch: 165 | Batch_idx: 270 |  Loss_1: (0.0085) | Acc_1: (99.72%) (34590/34688)\n",
      "Epoch: 165 | Batch_idx: 280 |  Loss_1: (0.0085) | Acc_1: (99.71%) (35865/35968)\n",
      "Epoch: 165 | Batch_idx: 290 |  Loss_1: (0.0083) | Acc_1: (99.72%) (37144/37248)\n",
      "Epoch: 165 | Batch_idx: 300 |  Loss_1: (0.0083) | Acc_1: (99.71%) (38417/38528)\n",
      "Epoch: 165 | Batch_idx: 310 |  Loss_1: (0.0083) | Acc_1: (99.72%) (39695/39808)\n",
      "Epoch: 165 | Batch_idx: 320 |  Loss_1: (0.0083) | Acc_1: (99.72%) (40971/41088)\n",
      "Epoch: 165 | Batch_idx: 330 |  Loss_1: (0.0082) | Acc_1: (99.72%) (42250/42368)\n",
      "Epoch: 165 | Batch_idx: 340 |  Loss_1: (0.0081) | Acc_1: (99.72%) (43526/43648)\n",
      "Epoch: 165 | Batch_idx: 350 |  Loss_1: (0.0080) | Acc_1: (99.73%) (44806/44928)\n",
      "Epoch: 165 | Batch_idx: 360 |  Loss_1: (0.0079) | Acc_1: (99.73%) (46084/46208)\n",
      "Epoch: 165 | Batch_idx: 370 |  Loss_1: (0.0079) | Acc_1: (99.73%) (47361/47488)\n",
      "Epoch: 165 | Batch_idx: 380 |  Loss_1: (0.0079) | Acc_1: (99.73%) (48638/48768)\n",
      "Epoch: 165 | Batch_idx: 390 |  Loss_1: (0.0078) | Acc_1: (99.74%) (49869/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5127) | Acc: (91.93%) (9193/10000)\n",
      "Epoch: 166 | Batch_idx: 0 |  Loss_1: (0.0010) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 166 | Batch_idx: 10 |  Loss_1: (0.0077) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 166 | Batch_idx: 20 |  Loss_1: (0.0075) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 166 | Batch_idx: 30 |  Loss_1: (0.0067) | Acc_1: (99.82%) (3961/3968)\n",
      "Epoch: 166 | Batch_idx: 40 |  Loss_1: (0.0065) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 166 | Batch_idx: 50 |  Loss_1: (0.0058) | Acc_1: (99.83%) (6517/6528)\n",
      "Epoch: 166 | Batch_idx: 60 |  Loss_1: (0.0059) | Acc_1: (99.82%) (7794/7808)\n",
      "Epoch: 166 | Batch_idx: 70 |  Loss_1: (0.0053) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 166 | Batch_idx: 80 |  Loss_1: (0.0052) | Acc_1: (99.84%) (10351/10368)\n",
      "Epoch: 166 | Batch_idx: 90 |  Loss_1: (0.0052) | Acc_1: (99.85%) (11630/11648)\n",
      "Epoch: 166 | Batch_idx: 100 |  Loss_1: (0.0056) | Acc_1: (99.83%) (12906/12928)\n",
      "Epoch: 166 | Batch_idx: 110 |  Loss_1: (0.0059) | Acc_1: (99.83%) (14184/14208)\n",
      "Epoch: 166 | Batch_idx: 120 |  Loss_1: (0.0061) | Acc_1: (99.82%) (15460/15488)\n",
      "Epoch: 166 | Batch_idx: 130 |  Loss_1: (0.0060) | Acc_1: (99.82%) (16738/16768)\n",
      "Epoch: 166 | Batch_idx: 140 |  Loss_1: (0.0058) | Acc_1: (99.83%) (18017/18048)\n",
      "Epoch: 166 | Batch_idx: 150 |  Loss_1: (0.0058) | Acc_1: (99.82%) (19294/19328)\n",
      "Epoch: 166 | Batch_idx: 160 |  Loss_1: (0.0058) | Acc_1: (99.82%) (20570/20608)\n",
      "Epoch: 166 | Batch_idx: 170 |  Loss_1: (0.0057) | Acc_1: (99.81%) (21847/21888)\n",
      "Epoch: 166 | Batch_idx: 180 |  Loss_1: (0.0059) | Acc_1: (99.80%) (23121/23168)\n",
      "Epoch: 166 | Batch_idx: 190 |  Loss_1: (0.0059) | Acc_1: (99.80%) (24398/24448)\n",
      "Epoch: 166 | Batch_idx: 200 |  Loss_1: (0.0061) | Acc_1: (99.79%) (25674/25728)\n",
      "Epoch: 166 | Batch_idx: 210 |  Loss_1: (0.0059) | Acc_1: (99.80%) (26954/27008)\n",
      "Epoch: 166 | Batch_idx: 220 |  Loss_1: (0.0059) | Acc_1: (99.80%) (28231/28288)\n",
      "Epoch: 166 | Batch_idx: 230 |  Loss_1: (0.0061) | Acc_1: (99.79%) (29507/29568)\n",
      "Epoch: 166 | Batch_idx: 240 |  Loss_1: (0.0062) | Acc_1: (99.79%) (30783/30848)\n",
      "Epoch: 166 | Batch_idx: 250 |  Loss_1: (0.0062) | Acc_1: (99.79%) (32060/32128)\n",
      "Epoch: 166 | Batch_idx: 260 |  Loss_1: (0.0063) | Acc_1: (99.78%) (33334/33408)\n",
      "Epoch: 166 | Batch_idx: 270 |  Loss_1: (0.0063) | Acc_1: (99.77%) (34609/34688)\n",
      "Epoch: 166 | Batch_idx: 280 |  Loss_1: (0.0064) | Acc_1: (99.77%) (35886/35968)\n",
      "Epoch: 166 | Batch_idx: 290 |  Loss_1: (0.0065) | Acc_1: (99.76%) (37159/37248)\n",
      "Epoch: 166 | Batch_idx: 300 |  Loss_1: (0.0066) | Acc_1: (99.76%) (38435/38528)\n",
      "Epoch: 166 | Batch_idx: 310 |  Loss_1: (0.0068) | Acc_1: (99.75%) (39710/39808)\n",
      "Epoch: 166 | Batch_idx: 320 |  Loss_1: (0.0069) | Acc_1: (99.75%) (40985/41088)\n",
      "Epoch: 166 | Batch_idx: 330 |  Loss_1: (0.0070) | Acc_1: (99.75%) (42262/42368)\n",
      "Epoch: 166 | Batch_idx: 340 |  Loss_1: (0.0072) | Acc_1: (99.74%) (43534/43648)\n",
      "Epoch: 166 | Batch_idx: 350 |  Loss_1: (0.0073) | Acc_1: (99.74%) (44811/44928)\n",
      "Epoch: 166 | Batch_idx: 360 |  Loss_1: (0.0074) | Acc_1: (99.74%) (46087/46208)\n",
      "Epoch: 166 | Batch_idx: 370 |  Loss_1: (0.0075) | Acc_1: (99.74%) (47363/47488)\n",
      "Epoch: 166 | Batch_idx: 380 |  Loss_1: (0.0074) | Acc_1: (99.74%) (48640/48768)\n",
      "Epoch: 166 | Batch_idx: 390 |  Loss_1: (0.0075) | Acc_1: (99.73%) (49867/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5609) | Acc: (91.32%) (9132/10000)\n",
      "Epoch: 167 | Batch_idx: 0 |  Loss_1: (0.0180) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 167 | Batch_idx: 10 |  Loss_1: (0.0110) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 167 | Batch_idx: 20 |  Loss_1: (0.0073) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 167 | Batch_idx: 30 |  Loss_1: (0.0077) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 167 | Batch_idx: 40 |  Loss_1: (0.0083) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 167 | Batch_idx: 50 |  Loss_1: (0.0085) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 167 | Batch_idx: 60 |  Loss_1: (0.0082) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 167 | Batch_idx: 70 |  Loss_1: (0.0085) | Acc_1: (99.72%) (9063/9088)\n",
      "Epoch: 167 | Batch_idx: 80 |  Loss_1: (0.0081) | Acc_1: (99.72%) (10339/10368)\n",
      "Epoch: 167 | Batch_idx: 90 |  Loss_1: (0.0077) | Acc_1: (99.73%) (11617/11648)\n",
      "Epoch: 167 | Batch_idx: 100 |  Loss_1: (0.0075) | Acc_1: (99.74%) (12895/12928)\n",
      "Epoch: 167 | Batch_idx: 110 |  Loss_1: (0.0072) | Acc_1: (99.76%) (14174/14208)\n",
      "Epoch: 167 | Batch_idx: 120 |  Loss_1: (0.0071) | Acc_1: (99.77%) (15452/15488)\n",
      "Epoch: 167 | Batch_idx: 130 |  Loss_1: (0.0072) | Acc_1: (99.76%) (16728/16768)\n",
      "Epoch: 167 | Batch_idx: 140 |  Loss_1: (0.0073) | Acc_1: (99.76%) (18004/18048)\n",
      "Epoch: 167 | Batch_idx: 150 |  Loss_1: (0.0073) | Acc_1: (99.76%) (19281/19328)\n",
      "Epoch: 167 | Batch_idx: 160 |  Loss_1: (0.0072) | Acc_1: (99.76%) (20558/20608)\n",
      "Epoch: 167 | Batch_idx: 170 |  Loss_1: (0.0071) | Acc_1: (99.76%) (21836/21888)\n",
      "Epoch: 167 | Batch_idx: 180 |  Loss_1: (0.0072) | Acc_1: (99.76%) (23113/23168)\n",
      "Epoch: 167 | Batch_idx: 190 |  Loss_1: (0.0071) | Acc_1: (99.77%) (24392/24448)\n",
      "Epoch: 167 | Batch_idx: 200 |  Loss_1: (0.0069) | Acc_1: (99.77%) (25670/25728)\n",
      "Epoch: 167 | Batch_idx: 210 |  Loss_1: (0.0068) | Acc_1: (99.78%) (26948/27008)\n",
      "Epoch: 167 | Batch_idx: 220 |  Loss_1: (0.0067) | Acc_1: (99.78%) (28226/28288)\n",
      "Epoch: 167 | Batch_idx: 230 |  Loss_1: (0.0069) | Acc_1: (99.77%) (29501/29568)\n",
      "Epoch: 167 | Batch_idx: 240 |  Loss_1: (0.0071) | Acc_1: (99.76%) (30775/30848)\n",
      "Epoch: 167 | Batch_idx: 250 |  Loss_1: (0.0069) | Acc_1: (99.77%) (32055/32128)\n",
      "Epoch: 167 | Batch_idx: 260 |  Loss_1: (0.0070) | Acc_1: (99.77%) (33332/33408)\n",
      "Epoch: 167 | Batch_idx: 270 |  Loss_1: (0.0072) | Acc_1: (99.77%) (34607/34688)\n",
      "Epoch: 167 | Batch_idx: 280 |  Loss_1: (0.0071) | Acc_1: (99.77%) (35886/35968)\n",
      "Epoch: 167 | Batch_idx: 290 |  Loss_1: (0.0073) | Acc_1: (99.76%) (37160/37248)\n",
      "Epoch: 167 | Batch_idx: 300 |  Loss_1: (0.0074) | Acc_1: (99.76%) (38436/38528)\n",
      "Epoch: 167 | Batch_idx: 310 |  Loss_1: (0.0077) | Acc_1: (99.75%) (39710/39808)\n",
      "Epoch: 167 | Batch_idx: 320 |  Loss_1: (0.0081) | Acc_1: (99.74%) (40983/41088)\n",
      "Epoch: 167 | Batch_idx: 330 |  Loss_1: (0.0080) | Acc_1: (99.75%) (42260/42368)\n",
      "Epoch: 167 | Batch_idx: 340 |  Loss_1: (0.0079) | Acc_1: (99.75%) (43537/43648)\n",
      "Epoch: 167 | Batch_idx: 350 |  Loss_1: (0.0080) | Acc_1: (99.75%) (44814/44928)\n",
      "Epoch: 167 | Batch_idx: 360 |  Loss_1: (0.0082) | Acc_1: (99.74%) (46086/46208)\n",
      "Epoch: 167 | Batch_idx: 370 |  Loss_1: (0.0082) | Acc_1: (99.73%) (47362/47488)\n",
      "Epoch: 167 | Batch_idx: 380 |  Loss_1: (0.0083) | Acc_1: (99.73%) (48635/48768)\n",
      "Epoch: 167 | Batch_idx: 390 |  Loss_1: (0.0082) | Acc_1: (99.73%) (49865/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5353) | Acc: (91.52%) (9152/10000)\n",
      "Epoch: 168 | Batch_idx: 0 |  Loss_1: (0.0113) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 168 | Batch_idx: 10 |  Loss_1: (0.0119) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 168 | Batch_idx: 20 |  Loss_1: (0.0096) | Acc_1: (99.63%) (2678/2688)\n",
      "Epoch: 168 | Batch_idx: 30 |  Loss_1: (0.0096) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 168 | Batch_idx: 40 |  Loss_1: (0.0094) | Acc_1: (99.68%) (5231/5248)\n",
      "Epoch: 168 | Batch_idx: 50 |  Loss_1: (0.0092) | Acc_1: (99.68%) (6507/6528)\n",
      "Epoch: 168 | Batch_idx: 60 |  Loss_1: (0.0088) | Acc_1: (99.68%) (7783/7808)\n",
      "Epoch: 168 | Batch_idx: 70 |  Loss_1: (0.0091) | Acc_1: (99.67%) (9058/9088)\n",
      "Epoch: 168 | Batch_idx: 80 |  Loss_1: (0.0089) | Acc_1: (99.67%) (10334/10368)\n",
      "Epoch: 168 | Batch_idx: 90 |  Loss_1: (0.0091) | Acc_1: (99.66%) (11608/11648)\n",
      "Epoch: 168 | Batch_idx: 100 |  Loss_1: (0.0091) | Acc_1: (99.66%) (12884/12928)\n",
      "Epoch: 168 | Batch_idx: 110 |  Loss_1: (0.0088) | Acc_1: (99.68%) (14162/14208)\n",
      "Epoch: 168 | Batch_idx: 120 |  Loss_1: (0.0086) | Acc_1: (99.69%) (15440/15488)\n",
      "Epoch: 168 | Batch_idx: 130 |  Loss_1: (0.0088) | Acc_1: (99.68%) (16714/16768)\n",
      "Epoch: 168 | Batch_idx: 140 |  Loss_1: (0.0085) | Acc_1: (99.70%) (17993/18048)\n",
      "Epoch: 168 | Batch_idx: 150 |  Loss_1: (0.0084) | Acc_1: (99.69%) (19268/19328)\n",
      "Epoch: 168 | Batch_idx: 160 |  Loss_1: (0.0083) | Acc_1: (99.68%) (20543/20608)\n",
      "Epoch: 168 | Batch_idx: 170 |  Loss_1: (0.0080) | Acc_1: (99.69%) (21821/21888)\n",
      "Epoch: 168 | Batch_idx: 180 |  Loss_1: (0.0079) | Acc_1: (99.70%) (23098/23168)\n",
      "Epoch: 168 | Batch_idx: 190 |  Loss_1: (0.0076) | Acc_1: (99.71%) (24376/24448)\n",
      "Epoch: 168 | Batch_idx: 200 |  Loss_1: (0.0075) | Acc_1: (99.71%) (25653/25728)\n",
      "Epoch: 168 | Batch_idx: 210 |  Loss_1: (0.0073) | Acc_1: (99.71%) (26931/27008)\n",
      "Epoch: 168 | Batch_idx: 220 |  Loss_1: (0.0073) | Acc_1: (99.72%) (28208/28288)\n",
      "Epoch: 168 | Batch_idx: 230 |  Loss_1: (0.0074) | Acc_1: (99.71%) (29482/29568)\n",
      "Epoch: 168 | Batch_idx: 240 |  Loss_1: (0.0074) | Acc_1: (99.71%) (30758/30848)\n",
      "Epoch: 168 | Batch_idx: 250 |  Loss_1: (0.0072) | Acc_1: (99.72%) (32037/32128)\n",
      "Epoch: 168 | Batch_idx: 260 |  Loss_1: (0.0072) | Acc_1: (99.72%) (33315/33408)\n",
      "Epoch: 168 | Batch_idx: 270 |  Loss_1: (0.0071) | Acc_1: (99.72%) (34592/34688)\n",
      "Epoch: 168 | Batch_idx: 280 |  Loss_1: (0.0070) | Acc_1: (99.73%) (35871/35968)\n",
      "Epoch: 168 | Batch_idx: 290 |  Loss_1: (0.0069) | Acc_1: (99.73%) (37147/37248)\n",
      "Epoch: 168 | Batch_idx: 300 |  Loss_1: (0.0069) | Acc_1: (99.73%) (38424/38528)\n",
      "Epoch: 168 | Batch_idx: 310 |  Loss_1: (0.0067) | Acc_1: (99.74%) (39704/39808)\n",
      "Epoch: 168 | Batch_idx: 320 |  Loss_1: (0.0067) | Acc_1: (99.74%) (40982/41088)\n",
      "Epoch: 168 | Batch_idx: 330 |  Loss_1: (0.0067) | Acc_1: (99.74%) (42259/42368)\n",
      "Epoch: 168 | Batch_idx: 340 |  Loss_1: (0.0066) | Acc_1: (99.75%) (43538/43648)\n",
      "Epoch: 168 | Batch_idx: 350 |  Loss_1: (0.0065) | Acc_1: (99.75%) (44814/44928)\n",
      "Epoch: 168 | Batch_idx: 360 |  Loss_1: (0.0065) | Acc_1: (99.75%) (46092/46208)\n",
      "Epoch: 168 | Batch_idx: 370 |  Loss_1: (0.0064) | Acc_1: (99.75%) (47369/47488)\n",
      "Epoch: 168 | Batch_idx: 380 |  Loss_1: (0.0064) | Acc_1: (99.75%) (48646/48768)\n",
      "Epoch: 168 | Batch_idx: 390 |  Loss_1: (0.0063) | Acc_1: (99.75%) (49875/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5097) | Acc: (92.48%) (9248/10000)\n",
      "Epoch: 169 | Batch_idx: 0 |  Loss_1: (0.0007) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 169 | Batch_idx: 10 |  Loss_1: (0.0024) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 169 | Batch_idx: 20 |  Loss_1: (0.0072) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 169 | Batch_idx: 30 |  Loss_1: (0.0070) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 169 | Batch_idx: 40 |  Loss_1: (0.0075) | Acc_1: (99.71%) (5233/5248)\n",
      "Epoch: 169 | Batch_idx: 50 |  Loss_1: (0.0069) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 169 | Batch_idx: 60 |  Loss_1: (0.0064) | Acc_1: (99.78%) (7791/7808)\n",
      "Epoch: 169 | Batch_idx: 70 |  Loss_1: (0.0063) | Acc_1: (99.78%) (9068/9088)\n",
      "Epoch: 169 | Batch_idx: 80 |  Loss_1: (0.0059) | Acc_1: (99.78%) (10345/10368)\n",
      "Epoch: 169 | Batch_idx: 90 |  Loss_1: (0.0058) | Acc_1: (99.79%) (11623/11648)\n",
      "Epoch: 169 | Batch_idx: 100 |  Loss_1: (0.0058) | Acc_1: (99.78%) (12900/12928)\n",
      "Epoch: 169 | Batch_idx: 110 |  Loss_1: (0.0056) | Acc_1: (99.80%) (14179/14208)\n",
      "Epoch: 169 | Batch_idx: 120 |  Loss_1: (0.0054) | Acc_1: (99.80%) (15457/15488)\n",
      "Epoch: 169 | Batch_idx: 130 |  Loss_1: (0.0053) | Acc_1: (99.80%) (16734/16768)\n",
      "Epoch: 169 | Batch_idx: 140 |  Loss_1: (0.0055) | Acc_1: (99.79%) (18011/18048)\n",
      "Epoch: 169 | Batch_idx: 150 |  Loss_1: (0.0055) | Acc_1: (99.80%) (19289/19328)\n",
      "Epoch: 169 | Batch_idx: 160 |  Loss_1: (0.0054) | Acc_1: (99.80%) (20567/20608)\n",
      "Epoch: 169 | Batch_idx: 170 |  Loss_1: (0.0056) | Acc_1: (99.80%) (21845/21888)\n",
      "Epoch: 169 | Batch_idx: 180 |  Loss_1: (0.0054) | Acc_1: (99.81%) (23124/23168)\n",
      "Epoch: 169 | Batch_idx: 190 |  Loss_1: (0.0054) | Acc_1: (99.81%) (24402/24448)\n",
      "Epoch: 169 | Batch_idx: 200 |  Loss_1: (0.0055) | Acc_1: (99.81%) (25679/25728)\n",
      "Epoch: 169 | Batch_idx: 210 |  Loss_1: (0.0055) | Acc_1: (99.81%) (26957/27008)\n",
      "Epoch: 169 | Batch_idx: 220 |  Loss_1: (0.0055) | Acc_1: (99.81%) (28234/28288)\n",
      "Epoch: 169 | Batch_idx: 230 |  Loss_1: (0.0055) | Acc_1: (99.80%) (29510/29568)\n",
      "Epoch: 169 | Batch_idx: 240 |  Loss_1: (0.0057) | Acc_1: (99.80%) (30786/30848)\n",
      "Epoch: 169 | Batch_idx: 250 |  Loss_1: (0.0057) | Acc_1: (99.80%) (32064/32128)\n",
      "Epoch: 169 | Batch_idx: 260 |  Loss_1: (0.0056) | Acc_1: (99.81%) (33343/33408)\n",
      "Epoch: 169 | Batch_idx: 270 |  Loss_1: (0.0056) | Acc_1: (99.80%) (34620/34688)\n",
      "Epoch: 169 | Batch_idx: 280 |  Loss_1: (0.0057) | Acc_1: (99.80%) (35897/35968)\n",
      "Epoch: 169 | Batch_idx: 290 |  Loss_1: (0.0055) | Acc_1: (99.81%) (37177/37248)\n",
      "Epoch: 169 | Batch_idx: 300 |  Loss_1: (0.0056) | Acc_1: (99.81%) (38455/38528)\n",
      "Epoch: 169 | Batch_idx: 310 |  Loss_1: (0.0056) | Acc_1: (99.81%) (39732/39808)\n",
      "Epoch: 169 | Batch_idx: 320 |  Loss_1: (0.0055) | Acc_1: (99.82%) (41012/41088)\n",
      "Epoch: 169 | Batch_idx: 330 |  Loss_1: (0.0054) | Acc_1: (99.82%) (42290/42368)\n",
      "Epoch: 169 | Batch_idx: 340 |  Loss_1: (0.0054) | Acc_1: (99.81%) (43566/43648)\n",
      "Epoch: 169 | Batch_idx: 350 |  Loss_1: (0.0054) | Acc_1: (99.81%) (44843/44928)\n",
      "Epoch: 169 | Batch_idx: 360 |  Loss_1: (0.0055) | Acc_1: (99.81%) (46120/46208)\n",
      "Epoch: 169 | Batch_idx: 370 |  Loss_1: (0.0055) | Acc_1: (99.81%) (47396/47488)\n",
      "Epoch: 169 | Batch_idx: 380 |  Loss_1: (0.0055) | Acc_1: (99.81%) (48674/48768)\n",
      "Epoch: 169 | Batch_idx: 390 |  Loss_1: (0.0056) | Acc_1: (99.80%) (49901/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5207) | Acc: (91.99%) (9199/10000)\n",
      "Epoch: 170 | Batch_idx: 0 |  Loss_1: (0.0010) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 170 | Batch_idx: 10 |  Loss_1: (0.0056) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 170 | Batch_idx: 20 |  Loss_1: (0.0069) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 170 | Batch_idx: 30 |  Loss_1: (0.0078) | Acc_1: (99.65%) (3954/3968)\n",
      "Epoch: 170 | Batch_idx: 40 |  Loss_1: (0.0068) | Acc_1: (99.71%) (5233/5248)\n",
      "Epoch: 170 | Batch_idx: 50 |  Loss_1: (0.0083) | Acc_1: (99.66%) (6506/6528)\n",
      "Epoch: 170 | Batch_idx: 60 |  Loss_1: (0.0072) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 170 | Batch_idx: 70 |  Loss_1: (0.0070) | Acc_1: (99.74%) (9064/9088)\n",
      "Epoch: 170 | Batch_idx: 80 |  Loss_1: (0.0069) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 170 | Batch_idx: 90 |  Loss_1: (0.0064) | Acc_1: (99.77%) (11621/11648)\n",
      "Epoch: 170 | Batch_idx: 100 |  Loss_1: (0.0059) | Acc_1: (99.79%) (12901/12928)\n",
      "Epoch: 170 | Batch_idx: 110 |  Loss_1: (0.0058) | Acc_1: (99.79%) (14178/14208)\n",
      "Epoch: 170 | Batch_idx: 120 |  Loss_1: (0.0058) | Acc_1: (99.80%) (15457/15488)\n",
      "Epoch: 170 | Batch_idx: 130 |  Loss_1: (0.0056) | Acc_1: (99.80%) (16735/16768)\n",
      "Epoch: 170 | Batch_idx: 140 |  Loss_1: (0.0056) | Acc_1: (99.79%) (18011/18048)\n",
      "Epoch: 170 | Batch_idx: 150 |  Loss_1: (0.0054) | Acc_1: (99.80%) (19289/19328)\n",
      "Epoch: 170 | Batch_idx: 160 |  Loss_1: (0.0052) | Acc_1: (99.81%) (20569/20608)\n",
      "Epoch: 170 | Batch_idx: 170 |  Loss_1: (0.0051) | Acc_1: (99.82%) (21848/21888)\n",
      "Epoch: 170 | Batch_idx: 180 |  Loss_1: (0.0050) | Acc_1: (99.82%) (23127/23168)\n",
      "Epoch: 170 | Batch_idx: 190 |  Loss_1: (0.0049) | Acc_1: (99.83%) (24406/24448)\n",
      "Epoch: 170 | Batch_idx: 200 |  Loss_1: (0.0048) | Acc_1: (99.83%) (25684/25728)\n",
      "Epoch: 170 | Batch_idx: 210 |  Loss_1: (0.0048) | Acc_1: (99.83%) (26962/27008)\n",
      "Epoch: 170 | Batch_idx: 220 |  Loss_1: (0.0048) | Acc_1: (99.83%) (28239/28288)\n",
      "Epoch: 170 | Batch_idx: 230 |  Loss_1: (0.0048) | Acc_1: (99.82%) (29516/29568)\n",
      "Epoch: 170 | Batch_idx: 240 |  Loss_1: (0.0049) | Acc_1: (99.82%) (30793/30848)\n",
      "Epoch: 170 | Batch_idx: 250 |  Loss_1: (0.0050) | Acc_1: (99.82%) (32071/32128)\n",
      "Epoch: 170 | Batch_idx: 260 |  Loss_1: (0.0050) | Acc_1: (99.82%) (33348/33408)\n",
      "Epoch: 170 | Batch_idx: 270 |  Loss_1: (0.0052) | Acc_1: (99.82%) (34625/34688)\n",
      "Epoch: 170 | Batch_idx: 280 |  Loss_1: (0.0052) | Acc_1: (99.82%) (35902/35968)\n",
      "Epoch: 170 | Batch_idx: 290 |  Loss_1: (0.0051) | Acc_1: (99.81%) (37179/37248)\n",
      "Epoch: 170 | Batch_idx: 300 |  Loss_1: (0.0051) | Acc_1: (99.82%) (38458/38528)\n",
      "Epoch: 170 | Batch_idx: 310 |  Loss_1: (0.0050) | Acc_1: (99.82%) (39737/39808)\n",
      "Epoch: 170 | Batch_idx: 320 |  Loss_1: (0.0051) | Acc_1: (99.81%) (41011/41088)\n",
      "Epoch: 170 | Batch_idx: 330 |  Loss_1: (0.0050) | Acc_1: (99.81%) (42289/42368)\n",
      "Epoch: 170 | Batch_idx: 340 |  Loss_1: (0.0050) | Acc_1: (99.82%) (43569/43648)\n",
      "Epoch: 170 | Batch_idx: 350 |  Loss_1: (0.0049) | Acc_1: (99.82%) (44848/44928)\n",
      "Epoch: 170 | Batch_idx: 360 |  Loss_1: (0.0048) | Acc_1: (99.82%) (46126/46208)\n",
      "Epoch: 170 | Batch_idx: 370 |  Loss_1: (0.0047) | Acc_1: (99.83%) (47406/47488)\n",
      "Epoch: 170 | Batch_idx: 380 |  Loss_1: (0.0047) | Acc_1: (99.83%) (48685/48768)\n",
      "Epoch: 170 | Batch_idx: 390 |  Loss_1: (0.0047) | Acc_1: (99.83%) (49916/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5190) | Acc: (92.06%) (9206/10000)\n",
      "Epoch: 171 | Batch_idx: 0 |  Loss_1: (0.0034) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 171 | Batch_idx: 10 |  Loss_1: (0.0054) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 171 | Batch_idx: 20 |  Loss_1: (0.0055) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 171 | Batch_idx: 30 |  Loss_1: (0.0069) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 171 | Batch_idx: 40 |  Loss_1: (0.0063) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 171 | Batch_idx: 50 |  Loss_1: (0.0060) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 171 | Batch_idx: 60 |  Loss_1: (0.0064) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 171 | Batch_idx: 70 |  Loss_1: (0.0060) | Acc_1: (99.78%) (9068/9088)\n",
      "Epoch: 171 | Batch_idx: 80 |  Loss_1: (0.0063) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 171 | Batch_idx: 90 |  Loss_1: (0.0060) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 171 | Batch_idx: 100 |  Loss_1: (0.0058) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 171 | Batch_idx: 110 |  Loss_1: (0.0055) | Acc_1: (99.83%) (14184/14208)\n",
      "Epoch: 171 | Batch_idx: 120 |  Loss_1: (0.0057) | Acc_1: (99.83%) (15461/15488)\n",
      "Epoch: 171 | Batch_idx: 130 |  Loss_1: (0.0055) | Acc_1: (99.83%) (16740/16768)\n",
      "Epoch: 171 | Batch_idx: 140 |  Loss_1: (0.0053) | Acc_1: (99.83%) (18018/18048)\n",
      "Epoch: 171 | Batch_idx: 150 |  Loss_1: (0.0051) | Acc_1: (99.84%) (19297/19328)\n",
      "Epoch: 171 | Batch_idx: 160 |  Loss_1: (0.0052) | Acc_1: (99.84%) (20574/20608)\n",
      "Epoch: 171 | Batch_idx: 170 |  Loss_1: (0.0051) | Acc_1: (99.84%) (21853/21888)\n",
      "Epoch: 171 | Batch_idx: 180 |  Loss_1: (0.0052) | Acc_1: (99.84%) (23130/23168)\n",
      "Epoch: 171 | Batch_idx: 190 |  Loss_1: (0.0059) | Acc_1: (99.82%) (24405/24448)\n",
      "Epoch: 171 | Batch_idx: 200 |  Loss_1: (0.0061) | Acc_1: (99.81%) (25679/25728)\n",
      "Epoch: 171 | Batch_idx: 210 |  Loss_1: (0.0062) | Acc_1: (99.81%) (26956/27008)\n",
      "Epoch: 171 | Batch_idx: 220 |  Loss_1: (0.0061) | Acc_1: (99.81%) (28234/28288)\n",
      "Epoch: 171 | Batch_idx: 230 |  Loss_1: (0.0060) | Acc_1: (99.81%) (29512/29568)\n",
      "Epoch: 171 | Batch_idx: 240 |  Loss_1: (0.0062) | Acc_1: (99.80%) (30785/30848)\n",
      "Epoch: 171 | Batch_idx: 250 |  Loss_1: (0.0065) | Acc_1: (99.79%) (32062/32128)\n",
      "Epoch: 171 | Batch_idx: 260 |  Loss_1: (0.0063) | Acc_1: (99.80%) (33341/33408)\n",
      "Epoch: 171 | Batch_idx: 270 |  Loss_1: (0.0063) | Acc_1: (99.80%) (34620/34688)\n",
      "Epoch: 171 | Batch_idx: 280 |  Loss_1: (0.0063) | Acc_1: (99.80%) (35897/35968)\n",
      "Epoch: 171 | Batch_idx: 290 |  Loss_1: (0.0061) | Acc_1: (99.81%) (37177/37248)\n",
      "Epoch: 171 | Batch_idx: 300 |  Loss_1: (0.0062) | Acc_1: (99.80%) (38451/38528)\n",
      "Epoch: 171 | Batch_idx: 310 |  Loss_1: (0.0062) | Acc_1: (99.80%) (39730/39808)\n",
      "Epoch: 171 | Batch_idx: 320 |  Loss_1: (0.0062) | Acc_1: (99.81%) (41009/41088)\n",
      "Epoch: 171 | Batch_idx: 330 |  Loss_1: (0.0063) | Acc_1: (99.80%) (42284/42368)\n",
      "Epoch: 171 | Batch_idx: 340 |  Loss_1: (0.0063) | Acc_1: (99.80%) (43559/43648)\n",
      "Epoch: 171 | Batch_idx: 350 |  Loss_1: (0.0064) | Acc_1: (99.80%) (44836/44928)\n",
      "Epoch: 171 | Batch_idx: 360 |  Loss_1: (0.0063) | Acc_1: (99.79%) (46113/46208)\n",
      "Epoch: 171 | Batch_idx: 370 |  Loss_1: (0.0065) | Acc_1: (99.79%) (47388/47488)\n",
      "Epoch: 171 | Batch_idx: 380 |  Loss_1: (0.0065) | Acc_1: (99.79%) (48666/48768)\n",
      "Epoch: 171 | Batch_idx: 390 |  Loss_1: (0.0065) | Acc_1: (99.79%) (49895/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5440) | Acc: (91.74%) (9174/10000)\n",
      "Epoch: 172 | Batch_idx: 0 |  Loss_1: (0.0014) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 172 | Batch_idx: 10 |  Loss_1: (0.0056) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 172 | Batch_idx: 20 |  Loss_1: (0.0056) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 172 | Batch_idx: 30 |  Loss_1: (0.0054) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 172 | Batch_idx: 40 |  Loss_1: (0.0054) | Acc_1: (99.79%) (5237/5248)\n",
      "Epoch: 172 | Batch_idx: 50 |  Loss_1: (0.0064) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 172 | Batch_idx: 60 |  Loss_1: (0.0070) | Acc_1: (99.74%) (7788/7808)\n",
      "Epoch: 172 | Batch_idx: 70 |  Loss_1: (0.0069) | Acc_1: (99.76%) (9066/9088)\n",
      "Epoch: 172 | Batch_idx: 80 |  Loss_1: (0.0064) | Acc_1: (99.78%) (10345/10368)\n",
      "Epoch: 172 | Batch_idx: 90 |  Loss_1: (0.0061) | Acc_1: (99.79%) (11624/11648)\n",
      "Epoch: 172 | Batch_idx: 100 |  Loss_1: (0.0061) | Acc_1: (99.80%) (12902/12928)\n",
      "Epoch: 172 | Batch_idx: 110 |  Loss_1: (0.0058) | Acc_1: (99.81%) (14181/14208)\n",
      "Epoch: 172 | Batch_idx: 120 |  Loss_1: (0.0059) | Acc_1: (99.80%) (15457/15488)\n",
      "Epoch: 172 | Batch_idx: 130 |  Loss_1: (0.0057) | Acc_1: (99.81%) (16736/16768)\n",
      "Epoch: 172 | Batch_idx: 140 |  Loss_1: (0.0056) | Acc_1: (99.81%) (18014/18048)\n",
      "Epoch: 172 | Batch_idx: 150 |  Loss_1: (0.0054) | Acc_1: (99.82%) (19293/19328)\n",
      "Epoch: 172 | Batch_idx: 160 |  Loss_1: (0.0052) | Acc_1: (99.83%) (20572/20608)\n",
      "Epoch: 172 | Batch_idx: 170 |  Loss_1: (0.0052) | Acc_1: (99.83%) (21850/21888)\n",
      "Epoch: 172 | Batch_idx: 180 |  Loss_1: (0.0051) | Acc_1: (99.83%) (23129/23168)\n",
      "Epoch: 172 | Batch_idx: 190 |  Loss_1: (0.0053) | Acc_1: (99.83%) (24406/24448)\n",
      "Epoch: 172 | Batch_idx: 200 |  Loss_1: (0.0052) | Acc_1: (99.83%) (25684/25728)\n",
      "Epoch: 172 | Batch_idx: 210 |  Loss_1: (0.0051) | Acc_1: (99.83%) (26963/27008)\n",
      "Epoch: 172 | Batch_idx: 220 |  Loss_1: (0.0052) | Acc_1: (99.82%) (28238/28288)\n",
      "Epoch: 172 | Batch_idx: 230 |  Loss_1: (0.0052) | Acc_1: (99.82%) (29515/29568)\n",
      "Epoch: 172 | Batch_idx: 240 |  Loss_1: (0.0053) | Acc_1: (99.82%) (30791/30848)\n",
      "Epoch: 172 | Batch_idx: 250 |  Loss_1: (0.0052) | Acc_1: (99.82%) (32071/32128)\n",
      "Epoch: 172 | Batch_idx: 260 |  Loss_1: (0.0053) | Acc_1: (99.82%) (33349/33408)\n",
      "Epoch: 172 | Batch_idx: 270 |  Loss_1: (0.0053) | Acc_1: (99.82%) (34627/34688)\n",
      "Epoch: 172 | Batch_idx: 280 |  Loss_1: (0.0053) | Acc_1: (99.82%) (35903/35968)\n",
      "Epoch: 172 | Batch_idx: 290 |  Loss_1: (0.0052) | Acc_1: (99.82%) (37181/37248)\n",
      "Epoch: 172 | Batch_idx: 300 |  Loss_1: (0.0053) | Acc_1: (99.82%) (38460/38528)\n",
      "Epoch: 172 | Batch_idx: 310 |  Loss_1: (0.0054) | Acc_1: (99.82%) (39737/39808)\n",
      "Epoch: 172 | Batch_idx: 320 |  Loss_1: (0.0055) | Acc_1: (99.82%) (41014/41088)\n",
      "Epoch: 172 | Batch_idx: 330 |  Loss_1: (0.0055) | Acc_1: (99.82%) (42291/42368)\n",
      "Epoch: 172 | Batch_idx: 340 |  Loss_1: (0.0055) | Acc_1: (99.82%) (43570/43648)\n",
      "Epoch: 172 | Batch_idx: 350 |  Loss_1: (0.0054) | Acc_1: (99.82%) (44848/44928)\n",
      "Epoch: 172 | Batch_idx: 360 |  Loss_1: (0.0054) | Acc_1: (99.82%) (46126/46208)\n",
      "Epoch: 172 | Batch_idx: 370 |  Loss_1: (0.0055) | Acc_1: (99.82%) (47401/47488)\n",
      "Epoch: 172 | Batch_idx: 380 |  Loss_1: (0.0054) | Acc_1: (99.82%) (48680/48768)\n",
      "Epoch: 172 | Batch_idx: 390 |  Loss_1: (0.0053) | Acc_1: (99.82%) (49912/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5342) | Acc: (91.95%) (9195/10000)\n",
      "Epoch: 173 | Batch_idx: 0 |  Loss_1: (0.0144) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 173 | Batch_idx: 10 |  Loss_1: (0.0068) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 173 | Batch_idx: 20 |  Loss_1: (0.0050) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 173 | Batch_idx: 30 |  Loss_1: (0.0047) | Acc_1: (99.75%) (3958/3968)\n",
      "Epoch: 173 | Batch_idx: 40 |  Loss_1: (0.0058) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 173 | Batch_idx: 50 |  Loss_1: (0.0064) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 173 | Batch_idx: 60 |  Loss_1: (0.0055) | Acc_1: (99.78%) (7791/7808)\n",
      "Epoch: 173 | Batch_idx: 70 |  Loss_1: (0.0050) | Acc_1: (99.81%) (9071/9088)\n",
      "Epoch: 173 | Batch_idx: 80 |  Loss_1: (0.0055) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 173 | Batch_idx: 90 |  Loss_1: (0.0052) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 173 | Batch_idx: 100 |  Loss_1: (0.0050) | Acc_1: (99.80%) (12902/12928)\n",
      "Epoch: 173 | Batch_idx: 110 |  Loss_1: (0.0051) | Acc_1: (99.80%) (14180/14208)\n",
      "Epoch: 173 | Batch_idx: 120 |  Loss_1: (0.0051) | Acc_1: (99.81%) (15458/15488)\n",
      "Epoch: 173 | Batch_idx: 130 |  Loss_1: (0.0049) | Acc_1: (99.82%) (16738/16768)\n",
      "Epoch: 173 | Batch_idx: 140 |  Loss_1: (0.0052) | Acc_1: (99.82%) (18015/18048)\n",
      "Epoch: 173 | Batch_idx: 150 |  Loss_1: (0.0052) | Acc_1: (99.82%) (19293/19328)\n",
      "Epoch: 173 | Batch_idx: 160 |  Loss_1: (0.0050) | Acc_1: (99.83%) (20573/20608)\n",
      "Epoch: 173 | Batch_idx: 170 |  Loss_1: (0.0051) | Acc_1: (99.83%) (21850/21888)\n",
      "Epoch: 173 | Batch_idx: 180 |  Loss_1: (0.0050) | Acc_1: (99.84%) (23130/23168)\n",
      "Epoch: 173 | Batch_idx: 190 |  Loss_1: (0.0050) | Acc_1: (99.83%) (24406/24448)\n",
      "Epoch: 173 | Batch_idx: 200 |  Loss_1: (0.0052) | Acc_1: (99.83%) (25684/25728)\n",
      "Epoch: 173 | Batch_idx: 210 |  Loss_1: (0.0051) | Acc_1: (99.83%) (26961/27008)\n",
      "Epoch: 173 | Batch_idx: 220 |  Loss_1: (0.0053) | Acc_1: (99.82%) (28236/28288)\n",
      "Epoch: 173 | Batch_idx: 230 |  Loss_1: (0.0054) | Acc_1: (99.81%) (29511/29568)\n",
      "Epoch: 173 | Batch_idx: 240 |  Loss_1: (0.0055) | Acc_1: (99.81%) (30788/30848)\n",
      "Epoch: 173 | Batch_idx: 250 |  Loss_1: (0.0057) | Acc_1: (99.80%) (32064/32128)\n",
      "Epoch: 173 | Batch_idx: 260 |  Loss_1: (0.0059) | Acc_1: (99.80%) (33340/33408)\n",
      "Epoch: 173 | Batch_idx: 270 |  Loss_1: (0.0060) | Acc_1: (99.79%) (34616/34688)\n",
      "Epoch: 173 | Batch_idx: 280 |  Loss_1: (0.0059) | Acc_1: (99.79%) (35893/35968)\n",
      "Epoch: 173 | Batch_idx: 290 |  Loss_1: (0.0060) | Acc_1: (99.79%) (37170/37248)\n",
      "Epoch: 173 | Batch_idx: 300 |  Loss_1: (0.0060) | Acc_1: (99.79%) (38448/38528)\n",
      "Epoch: 173 | Batch_idx: 310 |  Loss_1: (0.0062) | Acc_1: (99.78%) (39721/39808)\n",
      "Epoch: 173 | Batch_idx: 320 |  Loss_1: (0.0062) | Acc_1: (99.78%) (40998/41088)\n",
      "Epoch: 173 | Batch_idx: 330 |  Loss_1: (0.0061) | Acc_1: (99.79%) (42278/42368)\n",
      "Epoch: 173 | Batch_idx: 340 |  Loss_1: (0.0064) | Acc_1: (99.78%) (43553/43648)\n",
      "Epoch: 173 | Batch_idx: 350 |  Loss_1: (0.0063) | Acc_1: (99.79%) (44832/44928)\n",
      "Epoch: 173 | Batch_idx: 360 |  Loss_1: (0.0065) | Acc_1: (99.78%) (46108/46208)\n",
      "Epoch: 173 | Batch_idx: 370 |  Loss_1: (0.0066) | Acc_1: (99.78%) (47384/47488)\n",
      "Epoch: 173 | Batch_idx: 380 |  Loss_1: (0.0069) | Acc_1: (99.76%) (48653/48768)\n",
      "Epoch: 173 | Batch_idx: 390 |  Loss_1: (0.0069) | Acc_1: (99.76%) (49882/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5869) | Acc: (91.24%) (9124/10000)\n",
      "Epoch: 174 | Batch_idx: 0 |  Loss_1: (0.0085) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 174 | Batch_idx: 10 |  Loss_1: (0.0080) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 174 | Batch_idx: 20 |  Loss_1: (0.0112) | Acc_1: (99.59%) (2677/2688)\n",
      "Epoch: 174 | Batch_idx: 30 |  Loss_1: (0.0104) | Acc_1: (99.60%) (3952/3968)\n",
      "Epoch: 174 | Batch_idx: 40 |  Loss_1: (0.0095) | Acc_1: (99.64%) (5229/5248)\n",
      "Epoch: 174 | Batch_idx: 50 |  Loss_1: (0.0095) | Acc_1: (99.65%) (6505/6528)\n",
      "Epoch: 174 | Batch_idx: 60 |  Loss_1: (0.0091) | Acc_1: (99.67%) (7782/7808)\n",
      "Epoch: 174 | Batch_idx: 70 |  Loss_1: (0.0080) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 174 | Batch_idx: 80 |  Loss_1: (0.0075) | Acc_1: (99.74%) (10341/10368)\n",
      "Epoch: 174 | Batch_idx: 90 |  Loss_1: (0.0075) | Acc_1: (99.74%) (11618/11648)\n",
      "Epoch: 174 | Batch_idx: 100 |  Loss_1: (0.0069) | Acc_1: (99.77%) (12898/12928)\n",
      "Epoch: 174 | Batch_idx: 110 |  Loss_1: (0.0065) | Acc_1: (99.79%) (14178/14208)\n",
      "Epoch: 174 | Batch_idx: 120 |  Loss_1: (0.0067) | Acc_1: (99.77%) (15453/15488)\n",
      "Epoch: 174 | Batch_idx: 130 |  Loss_1: (0.0064) | Acc_1: (99.79%) (16732/16768)\n",
      "Epoch: 174 | Batch_idx: 140 |  Loss_1: (0.0069) | Acc_1: (99.77%) (18007/18048)\n",
      "Epoch: 174 | Batch_idx: 150 |  Loss_1: (0.0069) | Acc_1: (99.77%) (19283/19328)\n",
      "Epoch: 174 | Batch_idx: 160 |  Loss_1: (0.0068) | Acc_1: (99.77%) (20560/20608)\n",
      "Epoch: 174 | Batch_idx: 170 |  Loss_1: (0.0071) | Acc_1: (99.74%) (21832/21888)\n",
      "Epoch: 174 | Batch_idx: 180 |  Loss_1: (0.0070) | Acc_1: (99.75%) (23109/23168)\n",
      "Epoch: 174 | Batch_idx: 190 |  Loss_1: (0.0070) | Acc_1: (99.75%) (24387/24448)\n",
      "Epoch: 174 | Batch_idx: 200 |  Loss_1: (0.0071) | Acc_1: (99.75%) (25664/25728)\n",
      "Epoch: 174 | Batch_idx: 210 |  Loss_1: (0.0071) | Acc_1: (99.75%) (26940/27008)\n",
      "Epoch: 174 | Batch_idx: 220 |  Loss_1: (0.0071) | Acc_1: (99.75%) (28218/28288)\n",
      "Epoch: 174 | Batch_idx: 230 |  Loss_1: (0.0070) | Acc_1: (99.75%) (29494/29568)\n",
      "Epoch: 174 | Batch_idx: 240 |  Loss_1: (0.0071) | Acc_1: (99.75%) (30771/30848)\n",
      "Epoch: 174 | Batch_idx: 250 |  Loss_1: (0.0070) | Acc_1: (99.75%) (32049/32128)\n",
      "Epoch: 174 | Batch_idx: 260 |  Loss_1: (0.0070) | Acc_1: (99.75%) (33325/33408)\n",
      "Epoch: 174 | Batch_idx: 270 |  Loss_1: (0.0070) | Acc_1: (99.75%) (34602/34688)\n",
      "Epoch: 174 | Batch_idx: 280 |  Loss_1: (0.0070) | Acc_1: (99.75%) (35878/35968)\n",
      "Epoch: 174 | Batch_idx: 290 |  Loss_1: (0.0071) | Acc_1: (99.75%) (37155/37248)\n",
      "Epoch: 174 | Batch_idx: 300 |  Loss_1: (0.0071) | Acc_1: (99.74%) (38429/38528)\n",
      "Epoch: 174 | Batch_idx: 310 |  Loss_1: (0.0074) | Acc_1: (99.74%) (39705/39808)\n",
      "Epoch: 174 | Batch_idx: 320 |  Loss_1: (0.0073) | Acc_1: (99.74%) (40981/41088)\n",
      "Epoch: 174 | Batch_idx: 330 |  Loss_1: (0.0074) | Acc_1: (99.74%) (42257/42368)\n",
      "Epoch: 174 | Batch_idx: 340 |  Loss_1: (0.0073) | Acc_1: (99.74%) (43534/43648)\n",
      "Epoch: 174 | Batch_idx: 350 |  Loss_1: (0.0074) | Acc_1: (99.74%) (44812/44928)\n",
      "Epoch: 174 | Batch_idx: 360 |  Loss_1: (0.0073) | Acc_1: (99.74%) (46090/46208)\n",
      "Epoch: 174 | Batch_idx: 370 |  Loss_1: (0.0075) | Acc_1: (99.74%) (47365/47488)\n",
      "Epoch: 174 | Batch_idx: 380 |  Loss_1: (0.0074) | Acc_1: (99.74%) (48643/48768)\n",
      "Epoch: 174 | Batch_idx: 390 |  Loss_1: (0.0074) | Acc_1: (99.75%) (49873/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5485) | Acc: (91.70%) (9170/10000)\n",
      "Epoch: 175 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 175 | Batch_idx: 10 |  Loss_1: (0.0092) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 175 | Batch_idx: 20 |  Loss_1: (0.0066) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 175 | Batch_idx: 30 |  Loss_1: (0.0065) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 175 | Batch_idx: 40 |  Loss_1: (0.0076) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 175 | Batch_idx: 50 |  Loss_1: (0.0078) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 175 | Batch_idx: 60 |  Loss_1: (0.0080) | Acc_1: (99.71%) (7785/7808)\n",
      "Epoch: 175 | Batch_idx: 70 |  Loss_1: (0.0075) | Acc_1: (99.74%) (9064/9088)\n",
      "Epoch: 175 | Batch_idx: 80 |  Loss_1: (0.0078) | Acc_1: (99.73%) (10340/10368)\n",
      "Epoch: 175 | Batch_idx: 90 |  Loss_1: (0.0071) | Acc_1: (99.76%) (11620/11648)\n",
      "Epoch: 175 | Batch_idx: 100 |  Loss_1: (0.0070) | Acc_1: (99.76%) (12897/12928)\n",
      "Epoch: 175 | Batch_idx: 110 |  Loss_1: (0.0072) | Acc_1: (99.76%) (14174/14208)\n",
      "Epoch: 175 | Batch_idx: 120 |  Loss_1: (0.0071) | Acc_1: (99.77%) (15452/15488)\n",
      "Epoch: 175 | Batch_idx: 130 |  Loss_1: (0.0070) | Acc_1: (99.77%) (16729/16768)\n",
      "Epoch: 175 | Batch_idx: 140 |  Loss_1: (0.0068) | Acc_1: (99.77%) (18007/18048)\n",
      "Epoch: 175 | Batch_idx: 150 |  Loss_1: (0.0066) | Acc_1: (99.79%) (19287/19328)\n",
      "Epoch: 175 | Batch_idx: 160 |  Loss_1: (0.0065) | Acc_1: (99.79%) (20564/20608)\n",
      "Epoch: 175 | Batch_idx: 170 |  Loss_1: (0.0064) | Acc_1: (99.79%) (21842/21888)\n",
      "Epoch: 175 | Batch_idx: 180 |  Loss_1: (0.0062) | Acc_1: (99.80%) (23122/23168)\n",
      "Epoch: 175 | Batch_idx: 190 |  Loss_1: (0.0064) | Acc_1: (99.80%) (24400/24448)\n",
      "Epoch: 175 | Batch_idx: 200 |  Loss_1: (0.0063) | Acc_1: (99.81%) (25678/25728)\n",
      "Epoch: 175 | Batch_idx: 210 |  Loss_1: (0.0062) | Acc_1: (99.81%) (26957/27008)\n",
      "Epoch: 175 | Batch_idx: 220 |  Loss_1: (0.0061) | Acc_1: (99.81%) (28235/28288)\n",
      "Epoch: 175 | Batch_idx: 230 |  Loss_1: (0.0061) | Acc_1: (99.81%) (29512/29568)\n",
      "Epoch: 175 | Batch_idx: 240 |  Loss_1: (0.0060) | Acc_1: (99.82%) (30792/30848)\n",
      "Epoch: 175 | Batch_idx: 250 |  Loss_1: (0.0058) | Acc_1: (99.82%) (32071/32128)\n",
      "Epoch: 175 | Batch_idx: 260 |  Loss_1: (0.0058) | Acc_1: (99.82%) (33349/33408)\n",
      "Epoch: 175 | Batch_idx: 270 |  Loss_1: (0.0057) | Acc_1: (99.82%) (34627/34688)\n",
      "Epoch: 175 | Batch_idx: 280 |  Loss_1: (0.0056) | Acc_1: (99.82%) (35905/35968)\n",
      "Epoch: 175 | Batch_idx: 290 |  Loss_1: (0.0057) | Acc_1: (99.83%) (37183/37248)\n",
      "Epoch: 175 | Batch_idx: 300 |  Loss_1: (0.0057) | Acc_1: (99.83%) (38462/38528)\n",
      "Epoch: 175 | Batch_idx: 310 |  Loss_1: (0.0059) | Acc_1: (99.82%) (39737/39808)\n",
      "Epoch: 175 | Batch_idx: 320 |  Loss_1: (0.0060) | Acc_1: (99.81%) (41011/41088)\n",
      "Epoch: 175 | Batch_idx: 330 |  Loss_1: (0.0063) | Acc_1: (99.80%) (42285/42368)\n",
      "Epoch: 175 | Batch_idx: 340 |  Loss_1: (0.0065) | Acc_1: (99.80%) (43559/43648)\n",
      "Epoch: 175 | Batch_idx: 350 |  Loss_1: (0.0065) | Acc_1: (99.79%) (44835/44928)\n",
      "Epoch: 175 | Batch_idx: 360 |  Loss_1: (0.0068) | Acc_1: (99.78%) (46107/46208)\n",
      "Epoch: 175 | Batch_idx: 370 |  Loss_1: (0.0067) | Acc_1: (99.79%) (47386/47488)\n",
      "Epoch: 175 | Batch_idx: 380 |  Loss_1: (0.0068) | Acc_1: (99.78%) (48662/48768)\n",
      "Epoch: 175 | Batch_idx: 390 |  Loss_1: (0.0069) | Acc_1: (99.77%) (49887/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5378) | Acc: (91.65%) (9165/10000)\n",
      "Epoch: 176 | Batch_idx: 0 |  Loss_1: (0.0026) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 176 | Batch_idx: 10 |  Loss_1: (0.0096) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 176 | Batch_idx: 20 |  Loss_1: (0.0089) | Acc_1: (99.70%) (2680/2688)\n",
      "Epoch: 176 | Batch_idx: 30 |  Loss_1: (0.0083) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 176 | Batch_idx: 40 |  Loss_1: (0.0078) | Acc_1: (99.73%) (5234/5248)\n",
      "Epoch: 176 | Batch_idx: 50 |  Loss_1: (0.0086) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 176 | Batch_idx: 60 |  Loss_1: (0.0088) | Acc_1: (99.71%) (7785/7808)\n",
      "Epoch: 176 | Batch_idx: 70 |  Loss_1: (0.0096) | Acc_1: (99.66%) (9057/9088)\n",
      "Epoch: 176 | Batch_idx: 80 |  Loss_1: (0.0090) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 176 | Batch_idx: 90 |  Loss_1: (0.0095) | Acc_1: (99.67%) (11610/11648)\n",
      "Epoch: 176 | Batch_idx: 100 |  Loss_1: (0.0091) | Acc_1: (99.69%) (12888/12928)\n",
      "Epoch: 176 | Batch_idx: 110 |  Loss_1: (0.0087) | Acc_1: (99.70%) (14166/14208)\n",
      "Epoch: 176 | Batch_idx: 120 |  Loss_1: (0.0086) | Acc_1: (99.71%) (15443/15488)\n",
      "Epoch: 176 | Batch_idx: 130 |  Loss_1: (0.0086) | Acc_1: (99.71%) (16719/16768)\n",
      "Epoch: 176 | Batch_idx: 140 |  Loss_1: (0.0084) | Acc_1: (99.72%) (17997/18048)\n",
      "Epoch: 176 | Batch_idx: 150 |  Loss_1: (0.0083) | Acc_1: (99.73%) (19275/19328)\n",
      "Epoch: 176 | Batch_idx: 160 |  Loss_1: (0.0081) | Acc_1: (99.73%) (20553/20608)\n",
      "Epoch: 176 | Batch_idx: 170 |  Loss_1: (0.0080) | Acc_1: (99.73%) (21829/21888)\n",
      "Epoch: 176 | Batch_idx: 180 |  Loss_1: (0.0081) | Acc_1: (99.73%) (23105/23168)\n",
      "Epoch: 176 | Batch_idx: 190 |  Loss_1: (0.0080) | Acc_1: (99.73%) (24382/24448)\n",
      "Epoch: 176 | Batch_idx: 200 |  Loss_1: (0.0080) | Acc_1: (99.74%) (25660/25728)\n",
      "Epoch: 176 | Batch_idx: 210 |  Loss_1: (0.0079) | Acc_1: (99.74%) (26937/27008)\n",
      "Epoch: 176 | Batch_idx: 220 |  Loss_1: (0.0078) | Acc_1: (99.73%) (28213/28288)\n",
      "Epoch: 176 | Batch_idx: 230 |  Loss_1: (0.0077) | Acc_1: (99.73%) (29489/29568)\n",
      "Epoch: 176 | Batch_idx: 240 |  Loss_1: (0.0082) | Acc_1: (99.72%) (30761/30848)\n",
      "Epoch: 176 | Batch_idx: 250 |  Loss_1: (0.0082) | Acc_1: (99.72%) (32038/32128)\n",
      "Epoch: 176 | Batch_idx: 260 |  Loss_1: (0.0083) | Acc_1: (99.71%) (33312/33408)\n",
      "Epoch: 176 | Batch_idx: 270 |  Loss_1: (0.0084) | Acc_1: (99.71%) (34589/34688)\n",
      "Epoch: 176 | Batch_idx: 280 |  Loss_1: (0.0082) | Acc_1: (99.72%) (35868/35968)\n",
      "Epoch: 176 | Batch_idx: 290 |  Loss_1: (0.0082) | Acc_1: (99.72%) (37144/37248)\n",
      "Epoch: 176 | Batch_idx: 300 |  Loss_1: (0.0081) | Acc_1: (99.72%) (38422/38528)\n",
      "Epoch: 176 | Batch_idx: 310 |  Loss_1: (0.0080) | Acc_1: (99.73%) (39702/39808)\n",
      "Epoch: 176 | Batch_idx: 320 |  Loss_1: (0.0080) | Acc_1: (99.73%) (40977/41088)\n",
      "Epoch: 176 | Batch_idx: 330 |  Loss_1: (0.0083) | Acc_1: (99.73%) (42252/42368)\n",
      "Epoch: 176 | Batch_idx: 340 |  Loss_1: (0.0083) | Acc_1: (99.73%) (43531/43648)\n",
      "Epoch: 176 | Batch_idx: 350 |  Loss_1: (0.0085) | Acc_1: (99.73%) (44805/44928)\n",
      "Epoch: 176 | Batch_idx: 360 |  Loss_1: (0.0083) | Acc_1: (99.73%) (46084/46208)\n",
      "Epoch: 176 | Batch_idx: 370 |  Loss_1: (0.0085) | Acc_1: (99.73%) (47359/47488)\n",
      "Epoch: 176 | Batch_idx: 380 |  Loss_1: (0.0084) | Acc_1: (99.73%) (48637/48768)\n",
      "Epoch: 176 | Batch_idx: 390 |  Loss_1: (0.0082) | Acc_1: (99.74%) (49869/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5507) | Acc: (91.67%) (9167/10000)\n",
      "Epoch: 177 | Batch_idx: 0 |  Loss_1: (0.0060) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 177 | Batch_idx: 10 |  Loss_1: (0.0032) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 177 | Batch_idx: 20 |  Loss_1: (0.0052) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 177 | Batch_idx: 30 |  Loss_1: (0.0054) | Acc_1: (99.82%) (3961/3968)\n",
      "Epoch: 177 | Batch_idx: 40 |  Loss_1: (0.0050) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 177 | Batch_idx: 50 |  Loss_1: (0.0044) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 177 | Batch_idx: 60 |  Loss_1: (0.0042) | Acc_1: (99.88%) (7799/7808)\n",
      "Epoch: 177 | Batch_idx: 70 |  Loss_1: (0.0046) | Acc_1: (99.87%) (9076/9088)\n",
      "Epoch: 177 | Batch_idx: 80 |  Loss_1: (0.0043) | Acc_1: (99.87%) (10355/10368)\n",
      "Epoch: 177 | Batch_idx: 90 |  Loss_1: (0.0041) | Acc_1: (99.88%) (11634/11648)\n",
      "Epoch: 177 | Batch_idx: 100 |  Loss_1: (0.0038) | Acc_1: (99.89%) (12914/12928)\n",
      "Epoch: 177 | Batch_idx: 110 |  Loss_1: (0.0038) | Acc_1: (99.88%) (14191/14208)\n",
      "Epoch: 177 | Batch_idx: 120 |  Loss_1: (0.0037) | Acc_1: (99.88%) (15469/15488)\n",
      "Epoch: 177 | Batch_idx: 130 |  Loss_1: (0.0036) | Acc_1: (99.88%) (16748/16768)\n",
      "Epoch: 177 | Batch_idx: 140 |  Loss_1: (0.0036) | Acc_1: (99.88%) (18027/18048)\n",
      "Epoch: 177 | Batch_idx: 150 |  Loss_1: (0.0042) | Acc_1: (99.87%) (19303/19328)\n",
      "Epoch: 177 | Batch_idx: 160 |  Loss_1: (0.0042) | Acc_1: (99.86%) (20580/20608)\n",
      "Epoch: 177 | Batch_idx: 170 |  Loss_1: (0.0041) | Acc_1: (99.87%) (21859/21888)\n",
      "Epoch: 177 | Batch_idx: 180 |  Loss_1: (0.0040) | Acc_1: (99.86%) (23136/23168)\n",
      "Epoch: 177 | Batch_idx: 190 |  Loss_1: (0.0040) | Acc_1: (99.87%) (24415/24448)\n",
      "Epoch: 177 | Batch_idx: 200 |  Loss_1: (0.0042) | Acc_1: (99.85%) (25690/25728)\n",
      "Epoch: 177 | Batch_idx: 210 |  Loss_1: (0.0047) | Acc_1: (99.85%) (26967/27008)\n",
      "Epoch: 177 | Batch_idx: 220 |  Loss_1: (0.0047) | Acc_1: (99.85%) (28246/28288)\n",
      "Epoch: 177 | Batch_idx: 230 |  Loss_1: (0.0046) | Acc_1: (99.85%) (29524/29568)\n",
      "Epoch: 177 | Batch_idx: 240 |  Loss_1: (0.0047) | Acc_1: (99.84%) (30800/30848)\n",
      "Epoch: 177 | Batch_idx: 250 |  Loss_1: (0.0047) | Acc_1: (99.84%) (32077/32128)\n",
      "Epoch: 177 | Batch_idx: 260 |  Loss_1: (0.0047) | Acc_1: (99.84%) (33354/33408)\n",
      "Epoch: 177 | Batch_idx: 270 |  Loss_1: (0.0049) | Acc_1: (99.83%) (34629/34688)\n",
      "Epoch: 177 | Batch_idx: 280 |  Loss_1: (0.0050) | Acc_1: (99.82%) (35904/35968)\n",
      "Epoch: 177 | Batch_idx: 290 |  Loss_1: (0.0050) | Acc_1: (99.82%) (37180/37248)\n",
      "Epoch: 177 | Batch_idx: 300 |  Loss_1: (0.0051) | Acc_1: (99.82%) (38458/38528)\n",
      "Epoch: 177 | Batch_idx: 310 |  Loss_1: (0.0051) | Acc_1: (99.82%) (39736/39808)\n",
      "Epoch: 177 | Batch_idx: 320 |  Loss_1: (0.0051) | Acc_1: (99.82%) (41013/41088)\n",
      "Epoch: 177 | Batch_idx: 330 |  Loss_1: (0.0050) | Acc_1: (99.82%) (42293/42368)\n",
      "Epoch: 177 | Batch_idx: 340 |  Loss_1: (0.0051) | Acc_1: (99.82%) (43568/43648)\n",
      "Epoch: 177 | Batch_idx: 350 |  Loss_1: (0.0050) | Acc_1: (99.82%) (44848/44928)\n",
      "Epoch: 177 | Batch_idx: 360 |  Loss_1: (0.0050) | Acc_1: (99.82%) (46126/46208)\n",
      "Epoch: 177 | Batch_idx: 370 |  Loss_1: (0.0052) | Acc_1: (99.82%) (47403/47488)\n",
      "Epoch: 177 | Batch_idx: 380 |  Loss_1: (0.0052) | Acc_1: (99.82%) (48680/48768)\n",
      "Epoch: 177 | Batch_idx: 390 |  Loss_1: (0.0053) | Acc_1: (99.82%) (49909/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5274) | Acc: (91.81%) (9181/10000)\n",
      "Epoch: 178 | Batch_idx: 0 |  Loss_1: (0.0006) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 178 | Batch_idx: 10 |  Loss_1: (0.0100) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 178 | Batch_idx: 20 |  Loss_1: (0.0071) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 178 | Batch_idx: 30 |  Loss_1: (0.0083) | Acc_1: (99.70%) (3956/3968)\n",
      "Epoch: 178 | Batch_idx: 40 |  Loss_1: (0.0070) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 178 | Batch_idx: 50 |  Loss_1: (0.0062) | Acc_1: (99.80%) (6515/6528)\n",
      "Epoch: 178 | Batch_idx: 60 |  Loss_1: (0.0057) | Acc_1: (99.82%) (7794/7808)\n",
      "Epoch: 178 | Batch_idx: 70 |  Loss_1: (0.0055) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 178 | Batch_idx: 80 |  Loss_1: (0.0053) | Acc_1: (99.85%) (10352/10368)\n",
      "Epoch: 178 | Batch_idx: 90 |  Loss_1: (0.0051) | Acc_1: (99.85%) (11631/11648)\n",
      "Epoch: 178 | Batch_idx: 100 |  Loss_1: (0.0047) | Acc_1: (99.86%) (12910/12928)\n",
      "Epoch: 178 | Batch_idx: 110 |  Loss_1: (0.0045) | Acc_1: (99.87%) (14189/14208)\n",
      "Epoch: 178 | Batch_idx: 120 |  Loss_1: (0.0044) | Acc_1: (99.87%) (15468/15488)\n",
      "Epoch: 178 | Batch_idx: 130 |  Loss_1: (0.0043) | Acc_1: (99.87%) (16747/16768)\n",
      "Epoch: 178 | Batch_idx: 140 |  Loss_1: (0.0047) | Acc_1: (99.87%) (18024/18048)\n",
      "Epoch: 178 | Batch_idx: 150 |  Loss_1: (0.0045) | Acc_1: (99.87%) (19303/19328)\n",
      "Epoch: 178 | Batch_idx: 160 |  Loss_1: (0.0045) | Acc_1: (99.87%) (20581/20608)\n",
      "Epoch: 178 | Batch_idx: 170 |  Loss_1: (0.0047) | Acc_1: (99.86%) (21857/21888)\n",
      "Epoch: 178 | Batch_idx: 180 |  Loss_1: (0.0049) | Acc_1: (99.85%) (23134/23168)\n",
      "Epoch: 178 | Batch_idx: 190 |  Loss_1: (0.0050) | Acc_1: (99.85%) (24412/24448)\n",
      "Epoch: 178 | Batch_idx: 200 |  Loss_1: (0.0050) | Acc_1: (99.85%) (25689/25728)\n",
      "Epoch: 178 | Batch_idx: 210 |  Loss_1: (0.0049) | Acc_1: (99.85%) (26968/27008)\n",
      "Epoch: 178 | Batch_idx: 220 |  Loss_1: (0.0052) | Acc_1: (99.84%) (28243/28288)\n",
      "Epoch: 178 | Batch_idx: 230 |  Loss_1: (0.0051) | Acc_1: (99.84%) (29522/29568)\n",
      "Epoch: 178 | Batch_idx: 240 |  Loss_1: (0.0052) | Acc_1: (99.84%) (30800/30848)\n",
      "Epoch: 178 | Batch_idx: 250 |  Loss_1: (0.0052) | Acc_1: (99.84%) (32078/32128)\n",
      "Epoch: 178 | Batch_idx: 260 |  Loss_1: (0.0054) | Acc_1: (99.84%) (33355/33408)\n",
      "Epoch: 178 | Batch_idx: 270 |  Loss_1: (0.0056) | Acc_1: (99.84%) (34632/34688)\n",
      "Epoch: 178 | Batch_idx: 280 |  Loss_1: (0.0059) | Acc_1: (99.82%) (35903/35968)\n",
      "Epoch: 178 | Batch_idx: 290 |  Loss_1: (0.0063) | Acc_1: (99.80%) (37173/37248)\n",
      "Epoch: 178 | Batch_idx: 300 |  Loss_1: (0.0063) | Acc_1: (99.79%) (38449/38528)\n",
      "Epoch: 178 | Batch_idx: 310 |  Loss_1: (0.0064) | Acc_1: (99.79%) (39723/39808)\n",
      "Epoch: 178 | Batch_idx: 320 |  Loss_1: (0.0067) | Acc_1: (99.78%) (40997/41088)\n",
      "Epoch: 178 | Batch_idx: 330 |  Loss_1: (0.0071) | Acc_1: (99.77%) (42270/42368)\n",
      "Epoch: 178 | Batch_idx: 340 |  Loss_1: (0.0074) | Acc_1: (99.75%) (43541/43648)\n",
      "Epoch: 178 | Batch_idx: 350 |  Loss_1: (0.0072) | Acc_1: (99.76%) (44821/44928)\n",
      "Epoch: 178 | Batch_idx: 360 |  Loss_1: (0.0072) | Acc_1: (99.76%) (46099/46208)\n",
      "Epoch: 178 | Batch_idx: 370 |  Loss_1: (0.0071) | Acc_1: (99.77%) (47377/47488)\n",
      "Epoch: 178 | Batch_idx: 380 |  Loss_1: (0.0072) | Acc_1: (99.76%) (48652/48768)\n",
      "Epoch: 178 | Batch_idx: 390 |  Loss_1: (0.0072) | Acc_1: (99.76%) (49881/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5554) | Acc: (91.36%) (9136/10000)\n",
      "Epoch: 179 | Batch_idx: 0 |  Loss_1: (0.0147) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 179 | Batch_idx: 10 |  Loss_1: (0.0064) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 179 | Batch_idx: 20 |  Loss_1: (0.0072) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 179 | Batch_idx: 30 |  Loss_1: (0.0071) | Acc_1: (99.80%) (3960/3968)\n",
      "Epoch: 179 | Batch_idx: 40 |  Loss_1: (0.0070) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 179 | Batch_idx: 50 |  Loss_1: (0.0075) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 179 | Batch_idx: 60 |  Loss_1: (0.0068) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 179 | Batch_idx: 70 |  Loss_1: (0.0065) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 179 | Batch_idx: 80 |  Loss_1: (0.0067) | Acc_1: (99.75%) (10342/10368)\n",
      "Epoch: 179 | Batch_idx: 90 |  Loss_1: (0.0067) | Acc_1: (99.75%) (11619/11648)\n",
      "Epoch: 179 | Batch_idx: 100 |  Loss_1: (0.0064) | Acc_1: (99.75%) (12896/12928)\n",
      "Epoch: 179 | Batch_idx: 110 |  Loss_1: (0.0062) | Acc_1: (99.76%) (14174/14208)\n",
      "Epoch: 179 | Batch_idx: 120 |  Loss_1: (0.0062) | Acc_1: (99.76%) (15451/15488)\n",
      "Epoch: 179 | Batch_idx: 130 |  Loss_1: (0.0060) | Acc_1: (99.77%) (16730/16768)\n",
      "Epoch: 179 | Batch_idx: 140 |  Loss_1: (0.0061) | Acc_1: (99.78%) (18009/18048)\n",
      "Epoch: 179 | Batch_idx: 150 |  Loss_1: (0.0064) | Acc_1: (99.77%) (19284/19328)\n",
      "Epoch: 179 | Batch_idx: 160 |  Loss_1: (0.0063) | Acc_1: (99.78%) (20562/20608)\n",
      "Epoch: 179 | Batch_idx: 170 |  Loss_1: (0.0064) | Acc_1: (99.78%) (21840/21888)\n",
      "Epoch: 179 | Batch_idx: 180 |  Loss_1: (0.0065) | Acc_1: (99.78%) (23117/23168)\n",
      "Epoch: 179 | Batch_idx: 190 |  Loss_1: (0.0070) | Acc_1: (99.77%) (24392/24448)\n",
      "Epoch: 179 | Batch_idx: 200 |  Loss_1: (0.0071) | Acc_1: (99.76%) (25667/25728)\n",
      "Epoch: 179 | Batch_idx: 210 |  Loss_1: (0.0072) | Acc_1: (99.76%) (26943/27008)\n",
      "Epoch: 179 | Batch_idx: 220 |  Loss_1: (0.0075) | Acc_1: (99.75%) (28218/28288)\n",
      "Epoch: 179 | Batch_idx: 230 |  Loss_1: (0.0073) | Acc_1: (99.76%) (29498/29568)\n",
      "Epoch: 179 | Batch_idx: 240 |  Loss_1: (0.0074) | Acc_1: (99.75%) (30772/30848)\n",
      "Epoch: 179 | Batch_idx: 250 |  Loss_1: (0.0074) | Acc_1: (99.75%) (32047/32128)\n",
      "Epoch: 179 | Batch_idx: 260 |  Loss_1: (0.0074) | Acc_1: (99.75%) (33325/33408)\n",
      "Epoch: 179 | Batch_idx: 270 |  Loss_1: (0.0073) | Acc_1: (99.76%) (34604/34688)\n",
      "Epoch: 179 | Batch_idx: 280 |  Loss_1: (0.0074) | Acc_1: (99.75%) (35879/35968)\n",
      "Epoch: 179 | Batch_idx: 290 |  Loss_1: (0.0075) | Acc_1: (99.75%) (37156/37248)\n",
      "Epoch: 179 | Batch_idx: 300 |  Loss_1: (0.0075) | Acc_1: (99.76%) (38435/38528)\n",
      "Epoch: 179 | Batch_idx: 310 |  Loss_1: (0.0075) | Acc_1: (99.76%) (39713/39808)\n",
      "Epoch: 179 | Batch_idx: 320 |  Loss_1: (0.0075) | Acc_1: (99.76%) (40991/41088)\n",
      "Epoch: 179 | Batch_idx: 330 |  Loss_1: (0.0074) | Acc_1: (99.77%) (42270/42368)\n",
      "Epoch: 179 | Batch_idx: 340 |  Loss_1: (0.0073) | Acc_1: (99.77%) (43548/43648)\n",
      "Epoch: 179 | Batch_idx: 350 |  Loss_1: (0.0074) | Acc_1: (99.77%) (44823/44928)\n",
      "Epoch: 179 | Batch_idx: 360 |  Loss_1: (0.0074) | Acc_1: (99.76%) (46099/46208)\n",
      "Epoch: 179 | Batch_idx: 370 |  Loss_1: (0.0075) | Acc_1: (99.76%) (47375/47488)\n",
      "Epoch: 179 | Batch_idx: 380 |  Loss_1: (0.0075) | Acc_1: (99.76%) (48651/48768)\n",
      "Epoch: 179 | Batch_idx: 390 |  Loss_1: (0.0076) | Acc_1: (99.76%) (49878/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5458) | Acc: (91.59%) (9159/10000)\n",
      "Epoch: 180 | Batch_idx: 0 |  Loss_1: (0.0005) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 180 | Batch_idx: 10 |  Loss_1: (0.0124) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 180 | Batch_idx: 20 |  Loss_1: (0.0085) | Acc_1: (99.74%) (2681/2688)\n",
      "Epoch: 180 | Batch_idx: 30 |  Loss_1: (0.0074) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 180 | Batch_idx: 40 |  Loss_1: (0.0070) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 180 | Batch_idx: 50 |  Loss_1: (0.0076) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 180 | Batch_idx: 60 |  Loss_1: (0.0073) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 180 | Batch_idx: 70 |  Loss_1: (0.0076) | Acc_1: (99.75%) (9065/9088)\n",
      "Epoch: 180 | Batch_idx: 80 |  Loss_1: (0.0070) | Acc_1: (99.76%) (10343/10368)\n",
      "Epoch: 180 | Batch_idx: 90 |  Loss_1: (0.0071) | Acc_1: (99.76%) (11620/11648)\n",
      "Epoch: 180 | Batch_idx: 100 |  Loss_1: (0.0073) | Acc_1: (99.75%) (12896/12928)\n",
      "Epoch: 180 | Batch_idx: 110 |  Loss_1: (0.0071) | Acc_1: (99.75%) (14173/14208)\n",
      "Epoch: 180 | Batch_idx: 120 |  Loss_1: (0.0069) | Acc_1: (99.75%) (15450/15488)\n",
      "Epoch: 180 | Batch_idx: 130 |  Loss_1: (0.0067) | Acc_1: (99.77%) (16729/16768)\n",
      "Epoch: 180 | Batch_idx: 140 |  Loss_1: (0.0066) | Acc_1: (99.78%) (18008/18048)\n",
      "Epoch: 180 | Batch_idx: 150 |  Loss_1: (0.0064) | Acc_1: (99.79%) (19287/19328)\n",
      "Epoch: 180 | Batch_idx: 160 |  Loss_1: (0.0064) | Acc_1: (99.79%) (20564/20608)\n",
      "Epoch: 180 | Batch_idx: 170 |  Loss_1: (0.0062) | Acc_1: (99.79%) (21843/21888)\n",
      "Epoch: 180 | Batch_idx: 180 |  Loss_1: (0.0061) | Acc_1: (99.80%) (23122/23168)\n",
      "Epoch: 180 | Batch_idx: 190 |  Loss_1: (0.0060) | Acc_1: (99.80%) (24398/24448)\n",
      "Epoch: 180 | Batch_idx: 200 |  Loss_1: (0.0061) | Acc_1: (99.79%) (25675/25728)\n",
      "Epoch: 180 | Batch_idx: 210 |  Loss_1: (0.0060) | Acc_1: (99.80%) (26954/27008)\n",
      "Epoch: 180 | Batch_idx: 220 |  Loss_1: (0.0059) | Acc_1: (99.81%) (28233/28288)\n",
      "Epoch: 180 | Batch_idx: 230 |  Loss_1: (0.0058) | Acc_1: (99.81%) (29512/29568)\n",
      "Epoch: 180 | Batch_idx: 240 |  Loss_1: (0.0057) | Acc_1: (99.81%) (30790/30848)\n",
      "Epoch: 180 | Batch_idx: 250 |  Loss_1: (0.0056) | Acc_1: (99.81%) (32068/32128)\n",
      "Epoch: 180 | Batch_idx: 260 |  Loss_1: (0.0056) | Acc_1: (99.81%) (33346/33408)\n",
      "Epoch: 180 | Batch_idx: 270 |  Loss_1: (0.0056) | Acc_1: (99.81%) (34623/34688)\n",
      "Epoch: 180 | Batch_idx: 280 |  Loss_1: (0.0057) | Acc_1: (99.81%) (35899/35968)\n",
      "Epoch: 180 | Batch_idx: 290 |  Loss_1: (0.0058) | Acc_1: (99.80%) (37173/37248)\n",
      "Epoch: 180 | Batch_idx: 300 |  Loss_1: (0.0057) | Acc_1: (99.81%) (38453/38528)\n",
      "Epoch: 180 | Batch_idx: 310 |  Loss_1: (0.0058) | Acc_1: (99.80%) (39729/39808)\n",
      "Epoch: 180 | Batch_idx: 320 |  Loss_1: (0.0060) | Acc_1: (99.80%) (41005/41088)\n",
      "Epoch: 180 | Batch_idx: 330 |  Loss_1: (0.0059) | Acc_1: (99.80%) (42285/42368)\n",
      "Epoch: 180 | Batch_idx: 340 |  Loss_1: (0.0059) | Acc_1: (99.81%) (43564/43648)\n",
      "Epoch: 180 | Batch_idx: 350 |  Loss_1: (0.0059) | Acc_1: (99.81%) (44841/44928)\n",
      "Epoch: 180 | Batch_idx: 360 |  Loss_1: (0.0059) | Acc_1: (99.81%) (46119/46208)\n",
      "Epoch: 180 | Batch_idx: 370 |  Loss_1: (0.0059) | Acc_1: (99.81%) (47397/47488)\n",
      "Epoch: 180 | Batch_idx: 380 |  Loss_1: (0.0058) | Acc_1: (99.81%) (48675/48768)\n",
      "Epoch: 180 | Batch_idx: 390 |  Loss_1: (0.0059) | Acc_1: (99.81%) (49903/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5177) | Acc: (91.94%) (9194/10000)\n",
      "Epoch: 181 | Batch_idx: 0 |  Loss_1: (0.0009) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 181 | Batch_idx: 10 |  Loss_1: (0.0049) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 181 | Batch_idx: 20 |  Loss_1: (0.0068) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 181 | Batch_idx: 30 |  Loss_1: (0.0065) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 181 | Batch_idx: 40 |  Loss_1: (0.0061) | Acc_1: (99.81%) (5238/5248)\n",
      "Epoch: 181 | Batch_idx: 50 |  Loss_1: (0.0076) | Acc_1: (99.77%) (6513/6528)\n",
      "Epoch: 181 | Batch_idx: 60 |  Loss_1: (0.0071) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 181 | Batch_idx: 70 |  Loss_1: (0.0066) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 181 | Batch_idx: 80 |  Loss_1: (0.0064) | Acc_1: (99.78%) (10345/10368)\n",
      "Epoch: 181 | Batch_idx: 90 |  Loss_1: (0.0061) | Acc_1: (99.79%) (11623/11648)\n",
      "Epoch: 181 | Batch_idx: 100 |  Loss_1: (0.0061) | Acc_1: (99.78%) (12899/12928)\n",
      "Epoch: 181 | Batch_idx: 110 |  Loss_1: (0.0058) | Acc_1: (99.80%) (14179/14208)\n",
      "Epoch: 181 | Batch_idx: 120 |  Loss_1: (0.0059) | Acc_1: (99.79%) (15455/15488)\n",
      "Epoch: 181 | Batch_idx: 130 |  Loss_1: (0.0060) | Acc_1: (99.79%) (16732/16768)\n",
      "Epoch: 181 | Batch_idx: 140 |  Loss_1: (0.0059) | Acc_1: (99.78%) (18009/18048)\n",
      "Epoch: 181 | Batch_idx: 150 |  Loss_1: (0.0061) | Acc_1: (99.77%) (19284/19328)\n",
      "Epoch: 181 | Batch_idx: 160 |  Loss_1: (0.0059) | Acc_1: (99.78%) (20563/20608)\n",
      "Epoch: 181 | Batch_idx: 170 |  Loss_1: (0.0059) | Acc_1: (99.79%) (21842/21888)\n",
      "Epoch: 181 | Batch_idx: 180 |  Loss_1: (0.0059) | Acc_1: (99.79%) (23119/23168)\n",
      "Epoch: 181 | Batch_idx: 190 |  Loss_1: (0.0057) | Acc_1: (99.80%) (24398/24448)\n",
      "Epoch: 181 | Batch_idx: 200 |  Loss_1: (0.0056) | Acc_1: (99.80%) (25676/25728)\n",
      "Epoch: 181 | Batch_idx: 210 |  Loss_1: (0.0059) | Acc_1: (99.78%) (26948/27008)\n",
      "Epoch: 181 | Batch_idx: 220 |  Loss_1: (0.0057) | Acc_1: (99.78%) (28227/28288)\n",
      "Epoch: 181 | Batch_idx: 230 |  Loss_1: (0.0055) | Acc_1: (99.79%) (29507/29568)\n",
      "Epoch: 181 | Batch_idx: 240 |  Loss_1: (0.0054) | Acc_1: (99.80%) (30786/30848)\n",
      "Epoch: 181 | Batch_idx: 250 |  Loss_1: (0.0053) | Acc_1: (99.81%) (32066/32128)\n",
      "Epoch: 181 | Batch_idx: 260 |  Loss_1: (0.0052) | Acc_1: (99.81%) (33345/33408)\n",
      "Epoch: 181 | Batch_idx: 270 |  Loss_1: (0.0052) | Acc_1: (99.81%) (34622/34688)\n",
      "Epoch: 181 | Batch_idx: 280 |  Loss_1: (0.0052) | Acc_1: (99.81%) (35900/35968)\n",
      "Epoch: 181 | Batch_idx: 290 |  Loss_1: (0.0051) | Acc_1: (99.82%) (37180/37248)\n",
      "Epoch: 181 | Batch_idx: 300 |  Loss_1: (0.0051) | Acc_1: (99.82%) (38459/38528)\n",
      "Epoch: 181 | Batch_idx: 310 |  Loss_1: (0.0051) | Acc_1: (99.82%) (39735/39808)\n",
      "Epoch: 181 | Batch_idx: 320 |  Loss_1: (0.0052) | Acc_1: (99.82%) (41012/41088)\n",
      "Epoch: 181 | Batch_idx: 330 |  Loss_1: (0.0052) | Acc_1: (99.82%) (42290/42368)\n",
      "Epoch: 181 | Batch_idx: 340 |  Loss_1: (0.0053) | Acc_1: (99.81%) (43564/43648)\n",
      "Epoch: 181 | Batch_idx: 350 |  Loss_1: (0.0053) | Acc_1: (99.81%) (44841/44928)\n",
      "Epoch: 181 | Batch_idx: 360 |  Loss_1: (0.0053) | Acc_1: (99.80%) (46117/46208)\n",
      "Epoch: 181 | Batch_idx: 370 |  Loss_1: (0.0053) | Acc_1: (99.80%) (47395/47488)\n",
      "Epoch: 181 | Batch_idx: 380 |  Loss_1: (0.0053) | Acc_1: (99.80%) (48672/48768)\n",
      "Epoch: 181 | Batch_idx: 390 |  Loss_1: (0.0055) | Acc_1: (99.80%) (49900/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5271) | Acc: (91.96%) (9196/10000)\n",
      "Epoch: 182 | Batch_idx: 0 |  Loss_1: (0.0013) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 182 | Batch_idx: 10 |  Loss_1: (0.0043) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 182 | Batch_idx: 20 |  Loss_1: (0.0049) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 182 | Batch_idx: 30 |  Loss_1: (0.0052) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 182 | Batch_idx: 40 |  Loss_1: (0.0046) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 182 | Batch_idx: 50 |  Loss_1: (0.0047) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 182 | Batch_idx: 60 |  Loss_1: (0.0054) | Acc_1: (99.85%) (7796/7808)\n",
      "Epoch: 182 | Batch_idx: 70 |  Loss_1: (0.0053) | Acc_1: (99.85%) (9074/9088)\n",
      "Epoch: 182 | Batch_idx: 80 |  Loss_1: (0.0058) | Acc_1: (99.84%) (10351/10368)\n",
      "Epoch: 182 | Batch_idx: 90 |  Loss_1: (0.0054) | Acc_1: (99.85%) (11630/11648)\n",
      "Epoch: 182 | Batch_idx: 100 |  Loss_1: (0.0053) | Acc_1: (99.85%) (12908/12928)\n",
      "Epoch: 182 | Batch_idx: 110 |  Loss_1: (0.0052) | Acc_1: (99.85%) (14186/14208)\n",
      "Epoch: 182 | Batch_idx: 120 |  Loss_1: (0.0053) | Acc_1: (99.84%) (15463/15488)\n",
      "Epoch: 182 | Batch_idx: 130 |  Loss_1: (0.0052) | Acc_1: (99.83%) (16740/16768)\n",
      "Epoch: 182 | Batch_idx: 140 |  Loss_1: (0.0049) | Acc_1: (99.84%) (18020/18048)\n",
      "Epoch: 182 | Batch_idx: 150 |  Loss_1: (0.0050) | Acc_1: (99.83%) (19296/19328)\n",
      "Epoch: 182 | Batch_idx: 160 |  Loss_1: (0.0055) | Acc_1: (99.83%) (20572/20608)\n",
      "Epoch: 182 | Batch_idx: 170 |  Loss_1: (0.0054) | Acc_1: (99.83%) (21850/21888)\n",
      "Epoch: 182 | Batch_idx: 180 |  Loss_1: (0.0057) | Acc_1: (99.81%) (23125/23168)\n",
      "Epoch: 182 | Batch_idx: 190 |  Loss_1: (0.0061) | Acc_1: (99.81%) (24402/24448)\n",
      "Epoch: 182 | Batch_idx: 200 |  Loss_1: (0.0059) | Acc_1: (99.82%) (25682/25728)\n",
      "Epoch: 182 | Batch_idx: 210 |  Loss_1: (0.0063) | Acc_1: (99.81%) (26956/27008)\n",
      "Epoch: 182 | Batch_idx: 220 |  Loss_1: (0.0063) | Acc_1: (99.81%) (28233/28288)\n",
      "Epoch: 182 | Batch_idx: 230 |  Loss_1: (0.0064) | Acc_1: (99.80%) (29509/29568)\n",
      "Epoch: 182 | Batch_idx: 240 |  Loss_1: (0.0063) | Acc_1: (99.81%) (30788/30848)\n",
      "Epoch: 182 | Batch_idx: 250 |  Loss_1: (0.0062) | Acc_1: (99.81%) (32067/32128)\n",
      "Epoch: 182 | Batch_idx: 260 |  Loss_1: (0.0062) | Acc_1: (99.81%) (33345/33408)\n",
      "Epoch: 182 | Batch_idx: 270 |  Loss_1: (0.0061) | Acc_1: (99.82%) (34624/34688)\n",
      "Epoch: 182 | Batch_idx: 280 |  Loss_1: (0.0060) | Acc_1: (99.82%) (35902/35968)\n",
      "Epoch: 182 | Batch_idx: 290 |  Loss_1: (0.0062) | Acc_1: (99.81%) (37178/37248)\n",
      "Epoch: 182 | Batch_idx: 300 |  Loss_1: (0.0064) | Acc_1: (99.81%) (38453/38528)\n",
      "Epoch: 182 | Batch_idx: 310 |  Loss_1: (0.0065) | Acc_1: (99.80%) (39730/39808)\n",
      "Epoch: 182 | Batch_idx: 320 |  Loss_1: (0.0065) | Acc_1: (99.80%) (41007/41088)\n",
      "Epoch: 182 | Batch_idx: 330 |  Loss_1: (0.0064) | Acc_1: (99.80%) (42283/42368)\n",
      "Epoch: 182 | Batch_idx: 340 |  Loss_1: (0.0064) | Acc_1: (99.80%) (43561/43648)\n",
      "Epoch: 182 | Batch_idx: 350 |  Loss_1: (0.0065) | Acc_1: (99.80%) (44838/44928)\n",
      "Epoch: 182 | Batch_idx: 360 |  Loss_1: (0.0064) | Acc_1: (99.81%) (46118/46208)\n",
      "Epoch: 182 | Batch_idx: 370 |  Loss_1: (0.0063) | Acc_1: (99.80%) (47395/47488)\n",
      "Epoch: 182 | Batch_idx: 380 |  Loss_1: (0.0062) | Acc_1: (99.81%) (48674/48768)\n",
      "Epoch: 182 | Batch_idx: 390 |  Loss_1: (0.0062) | Acc_1: (99.81%) (49903/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5258) | Acc: (91.79%) (9179/10000)\n",
      "Epoch: 183 | Batch_idx: 0 |  Loss_1: (0.0005) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 183 | Batch_idx: 10 |  Loss_1: (0.0023) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 183 | Batch_idx: 20 |  Loss_1: (0.0035) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 183 | Batch_idx: 30 |  Loss_1: (0.0051) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 183 | Batch_idx: 40 |  Loss_1: (0.0051) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 183 | Batch_idx: 50 |  Loss_1: (0.0043) | Acc_1: (99.88%) (6520/6528)\n",
      "Epoch: 183 | Batch_idx: 60 |  Loss_1: (0.0056) | Acc_1: (99.83%) (7795/7808)\n",
      "Epoch: 183 | Batch_idx: 70 |  Loss_1: (0.0054) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 183 | Batch_idx: 80 |  Loss_1: (0.0057) | Acc_1: (99.82%) (10349/10368)\n",
      "Epoch: 183 | Batch_idx: 90 |  Loss_1: (0.0055) | Acc_1: (99.82%) (11627/11648)\n",
      "Epoch: 183 | Batch_idx: 100 |  Loss_1: (0.0052) | Acc_1: (99.84%) (12907/12928)\n",
      "Epoch: 183 | Batch_idx: 110 |  Loss_1: (0.0056) | Acc_1: (99.84%) (14185/14208)\n",
      "Epoch: 183 | Batch_idx: 120 |  Loss_1: (0.0061) | Acc_1: (99.83%) (15461/15488)\n",
      "Epoch: 183 | Batch_idx: 130 |  Loss_1: (0.0062) | Acc_1: (99.82%) (16737/16768)\n",
      "Epoch: 183 | Batch_idx: 140 |  Loss_1: (0.0065) | Acc_1: (99.81%) (18013/18048)\n",
      "Epoch: 183 | Batch_idx: 150 |  Loss_1: (0.0074) | Acc_1: (99.78%) (19285/19328)\n",
      "Epoch: 183 | Batch_idx: 160 |  Loss_1: (0.0074) | Acc_1: (99.78%) (20562/20608)\n",
      "Epoch: 183 | Batch_idx: 170 |  Loss_1: (0.0078) | Acc_1: (99.77%) (21837/21888)\n",
      "Epoch: 183 | Batch_idx: 180 |  Loss_1: (0.0084) | Acc_1: (99.73%) (23106/23168)\n",
      "Epoch: 183 | Batch_idx: 190 |  Loss_1: (0.0083) | Acc_1: (99.73%) (24383/24448)\n",
      "Epoch: 183 | Batch_idx: 200 |  Loss_1: (0.0087) | Acc_1: (99.73%) (25658/25728)\n",
      "Epoch: 183 | Batch_idx: 210 |  Loss_1: (0.0085) | Acc_1: (99.73%) (26936/27008)\n",
      "Epoch: 183 | Batch_idx: 220 |  Loss_1: (0.0083) | Acc_1: (99.74%) (28214/28288)\n",
      "Epoch: 183 | Batch_idx: 230 |  Loss_1: (0.0082) | Acc_1: (99.74%) (29491/29568)\n",
      "Epoch: 183 | Batch_idx: 240 |  Loss_1: (0.0081) | Acc_1: (99.74%) (30768/30848)\n",
      "Epoch: 183 | Batch_idx: 250 |  Loss_1: (0.0081) | Acc_1: (99.73%) (32042/32128)\n",
      "Epoch: 183 | Batch_idx: 260 |  Loss_1: (0.0080) | Acc_1: (99.74%) (33321/33408)\n",
      "Epoch: 183 | Batch_idx: 270 |  Loss_1: (0.0079) | Acc_1: (99.74%) (34597/34688)\n",
      "Epoch: 183 | Batch_idx: 280 |  Loss_1: (0.0079) | Acc_1: (99.74%) (35874/35968)\n",
      "Epoch: 183 | Batch_idx: 290 |  Loss_1: (0.0079) | Acc_1: (99.74%) (37152/37248)\n",
      "Epoch: 183 | Batch_idx: 300 |  Loss_1: (0.0077) | Acc_1: (99.75%) (38431/38528)\n",
      "Epoch: 183 | Batch_idx: 310 |  Loss_1: (0.0077) | Acc_1: (99.75%) (39708/39808)\n",
      "Epoch: 183 | Batch_idx: 320 |  Loss_1: (0.0077) | Acc_1: (99.75%) (40985/41088)\n",
      "Epoch: 183 | Batch_idx: 330 |  Loss_1: (0.0076) | Acc_1: (99.75%) (42262/42368)\n",
      "Epoch: 183 | Batch_idx: 340 |  Loss_1: (0.0075) | Acc_1: (99.75%) (43541/43648)\n",
      "Epoch: 183 | Batch_idx: 350 |  Loss_1: (0.0074) | Acc_1: (99.76%) (44818/44928)\n",
      "Epoch: 183 | Batch_idx: 360 |  Loss_1: (0.0077) | Acc_1: (99.75%) (46091/46208)\n",
      "Epoch: 183 | Batch_idx: 370 |  Loss_1: (0.0078) | Acc_1: (99.75%) (47368/47488)\n",
      "Epoch: 183 | Batch_idx: 380 |  Loss_1: (0.0078) | Acc_1: (99.74%) (48643/48768)\n",
      "Epoch: 183 | Batch_idx: 390 |  Loss_1: (0.0078) | Acc_1: (99.74%) (49869/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5188) | Acc: (91.75%) (9175/10000)\n",
      "Epoch: 184 | Batch_idx: 0 |  Loss_1: (0.0003) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 184 | Batch_idx: 10 |  Loss_1: (0.0052) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 184 | Batch_idx: 20 |  Loss_1: (0.0066) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 184 | Batch_idx: 30 |  Loss_1: (0.0088) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 184 | Batch_idx: 40 |  Loss_1: (0.0084) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 184 | Batch_idx: 50 |  Loss_1: (0.0077) | Acc_1: (99.72%) (6510/6528)\n",
      "Epoch: 184 | Batch_idx: 60 |  Loss_1: (0.0071) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 184 | Batch_idx: 70 |  Loss_1: (0.0066) | Acc_1: (99.77%) (9067/9088)\n",
      "Epoch: 184 | Batch_idx: 80 |  Loss_1: (0.0061) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 184 | Batch_idx: 90 |  Loss_1: (0.0067) | Acc_1: (99.78%) (11622/11648)\n",
      "Epoch: 184 | Batch_idx: 100 |  Loss_1: (0.0063) | Acc_1: (99.79%) (12901/12928)\n",
      "Epoch: 184 | Batch_idx: 110 |  Loss_1: (0.0062) | Acc_1: (99.80%) (14179/14208)\n",
      "Epoch: 184 | Batch_idx: 120 |  Loss_1: (0.0061) | Acc_1: (99.80%) (15457/15488)\n",
      "Epoch: 184 | Batch_idx: 130 |  Loss_1: (0.0062) | Acc_1: (99.79%) (16733/16768)\n",
      "Epoch: 184 | Batch_idx: 140 |  Loss_1: (0.0059) | Acc_1: (99.81%) (18013/18048)\n",
      "Epoch: 184 | Batch_idx: 150 |  Loss_1: (0.0059) | Acc_1: (99.81%) (19291/19328)\n",
      "Epoch: 184 | Batch_idx: 160 |  Loss_1: (0.0058) | Acc_1: (99.82%) (20570/20608)\n",
      "Epoch: 184 | Batch_idx: 170 |  Loss_1: (0.0057) | Acc_1: (99.82%) (21849/21888)\n",
      "Epoch: 184 | Batch_idx: 180 |  Loss_1: (0.0058) | Acc_1: (99.82%) (23127/23168)\n",
      "Epoch: 184 | Batch_idx: 190 |  Loss_1: (0.0057) | Acc_1: (99.82%) (24405/24448)\n",
      "Epoch: 184 | Batch_idx: 200 |  Loss_1: (0.0057) | Acc_1: (99.83%) (25683/25728)\n",
      "Epoch: 184 | Batch_idx: 210 |  Loss_1: (0.0058) | Acc_1: (99.82%) (26959/27008)\n",
      "Epoch: 184 | Batch_idx: 220 |  Loss_1: (0.0058) | Acc_1: (99.82%) (28236/28288)\n",
      "Epoch: 184 | Batch_idx: 230 |  Loss_1: (0.0060) | Acc_1: (99.81%) (29511/29568)\n",
      "Epoch: 184 | Batch_idx: 240 |  Loss_1: (0.0062) | Acc_1: (99.79%) (30782/30848)\n",
      "Epoch: 184 | Batch_idx: 250 |  Loss_1: (0.0064) | Acc_1: (99.78%) (32058/32128)\n",
      "Epoch: 184 | Batch_idx: 260 |  Loss_1: (0.0066) | Acc_1: (99.78%) (33333/33408)\n",
      "Epoch: 184 | Batch_idx: 270 |  Loss_1: (0.0067) | Acc_1: (99.77%) (34609/34688)\n",
      "Epoch: 184 | Batch_idx: 280 |  Loss_1: (0.0070) | Acc_1: (99.76%) (35880/35968)\n",
      "Epoch: 184 | Batch_idx: 290 |  Loss_1: (0.0071) | Acc_1: (99.76%) (37157/37248)\n",
      "Epoch: 184 | Batch_idx: 300 |  Loss_1: (0.0071) | Acc_1: (99.76%) (38435/38528)\n",
      "Epoch: 184 | Batch_idx: 310 |  Loss_1: (0.0071) | Acc_1: (99.76%) (39712/39808)\n",
      "Epoch: 184 | Batch_idx: 320 |  Loss_1: (0.0071) | Acc_1: (99.76%) (40988/41088)\n",
      "Epoch: 184 | Batch_idx: 330 |  Loss_1: (0.0072) | Acc_1: (99.75%) (42261/42368)\n",
      "Epoch: 184 | Batch_idx: 340 |  Loss_1: (0.0073) | Acc_1: (99.75%) (43538/43648)\n",
      "Epoch: 184 | Batch_idx: 350 |  Loss_1: (0.0073) | Acc_1: (99.74%) (44812/44928)\n",
      "Epoch: 184 | Batch_idx: 360 |  Loss_1: (0.0074) | Acc_1: (99.74%) (46086/46208)\n",
      "Epoch: 184 | Batch_idx: 370 |  Loss_1: (0.0075) | Acc_1: (99.73%) (47360/47488)\n",
      "Epoch: 184 | Batch_idx: 380 |  Loss_1: (0.0075) | Acc_1: (99.73%) (48638/48768)\n",
      "Epoch: 184 | Batch_idx: 390 |  Loss_1: (0.0075) | Acc_1: (99.73%) (49867/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5230) | Acc: (91.41%) (9141/10000)\n",
      "Epoch: 185 | Batch_idx: 0 |  Loss_1: (0.0014) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 185 | Batch_idx: 10 |  Loss_1: (0.0074) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 185 | Batch_idx: 20 |  Loss_1: (0.0101) | Acc_1: (99.67%) (2679/2688)\n",
      "Epoch: 185 | Batch_idx: 30 |  Loss_1: (0.0107) | Acc_1: (99.67%) (3955/3968)\n",
      "Epoch: 185 | Batch_idx: 40 |  Loss_1: (0.0093) | Acc_1: (99.70%) (5232/5248)\n",
      "Epoch: 185 | Batch_idx: 50 |  Loss_1: (0.0088) | Acc_1: (99.71%) (6509/6528)\n",
      "Epoch: 185 | Batch_idx: 60 |  Loss_1: (0.0077) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 185 | Batch_idx: 70 |  Loss_1: (0.0090) | Acc_1: (99.71%) (9062/9088)\n",
      "Epoch: 185 | Batch_idx: 80 |  Loss_1: (0.0093) | Acc_1: (99.69%) (10336/10368)\n",
      "Epoch: 185 | Batch_idx: 90 |  Loss_1: (0.0092) | Acc_1: (99.67%) (11610/11648)\n",
      "Epoch: 185 | Batch_idx: 100 |  Loss_1: (0.0090) | Acc_1: (99.68%) (12887/12928)\n",
      "Epoch: 185 | Batch_idx: 110 |  Loss_1: (0.0085) | Acc_1: (99.70%) (14166/14208)\n",
      "Epoch: 185 | Batch_idx: 120 |  Loss_1: (0.0083) | Acc_1: (99.71%) (15443/15488)\n",
      "Epoch: 185 | Batch_idx: 130 |  Loss_1: (0.0081) | Acc_1: (99.71%) (16720/16768)\n",
      "Epoch: 185 | Batch_idx: 140 |  Loss_1: (0.0080) | Acc_1: (99.72%) (17997/18048)\n",
      "Epoch: 185 | Batch_idx: 150 |  Loss_1: (0.0079) | Acc_1: (99.72%) (19273/19328)\n",
      "Epoch: 185 | Batch_idx: 160 |  Loss_1: (0.0078) | Acc_1: (99.72%) (20551/20608)\n",
      "Epoch: 185 | Batch_idx: 170 |  Loss_1: (0.0078) | Acc_1: (99.73%) (21828/21888)\n",
      "Epoch: 185 | Batch_idx: 180 |  Loss_1: (0.0077) | Acc_1: (99.73%) (23106/23168)\n",
      "Epoch: 185 | Batch_idx: 190 |  Loss_1: (0.0074) | Acc_1: (99.75%) (24386/24448)\n",
      "Epoch: 185 | Batch_idx: 200 |  Loss_1: (0.0072) | Acc_1: (99.76%) (25665/25728)\n",
      "Epoch: 185 | Batch_idx: 210 |  Loss_1: (0.0070) | Acc_1: (99.76%) (26944/27008)\n",
      "Epoch: 185 | Batch_idx: 220 |  Loss_1: (0.0069) | Acc_1: (99.77%) (28223/28288)\n",
      "Epoch: 185 | Batch_idx: 230 |  Loss_1: (0.0068) | Acc_1: (99.77%) (29501/29568)\n",
      "Epoch: 185 | Batch_idx: 240 |  Loss_1: (0.0067) | Acc_1: (99.78%) (30779/30848)\n",
      "Epoch: 185 | Batch_idx: 250 |  Loss_1: (0.0066) | Acc_1: (99.78%) (32058/32128)\n",
      "Epoch: 185 | Batch_idx: 260 |  Loss_1: (0.0064) | Acc_1: (99.78%) (33336/33408)\n",
      "Epoch: 185 | Batch_idx: 270 |  Loss_1: (0.0064) | Acc_1: (99.79%) (34614/34688)\n",
      "Epoch: 185 | Batch_idx: 280 |  Loss_1: (0.0064) | Acc_1: (99.79%) (35891/35968)\n",
      "Epoch: 185 | Batch_idx: 290 |  Loss_1: (0.0062) | Acc_1: (99.79%) (37170/37248)\n",
      "Epoch: 185 | Batch_idx: 300 |  Loss_1: (0.0063) | Acc_1: (99.79%) (38448/38528)\n",
      "Epoch: 185 | Batch_idx: 310 |  Loss_1: (0.0062) | Acc_1: (99.79%) (39726/39808)\n",
      "Epoch: 185 | Batch_idx: 320 |  Loss_1: (0.0063) | Acc_1: (99.79%) (41003/41088)\n",
      "Epoch: 185 | Batch_idx: 330 |  Loss_1: (0.0063) | Acc_1: (99.79%) (42280/42368)\n",
      "Epoch: 185 | Batch_idx: 340 |  Loss_1: (0.0062) | Acc_1: (99.80%) (43559/43648)\n",
      "Epoch: 185 | Batch_idx: 350 |  Loss_1: (0.0061) | Acc_1: (99.80%) (44839/44928)\n",
      "Epoch: 185 | Batch_idx: 360 |  Loss_1: (0.0062) | Acc_1: (99.80%) (46115/46208)\n",
      "Epoch: 185 | Batch_idx: 370 |  Loss_1: (0.0063) | Acc_1: (99.80%) (47391/47488)\n",
      "Epoch: 185 | Batch_idx: 380 |  Loss_1: (0.0064) | Acc_1: (99.80%) (48669/48768)\n",
      "Epoch: 185 | Batch_idx: 390 |  Loss_1: (0.0065) | Acc_1: (99.80%) (49898/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5330) | Acc: (91.61%) (9161/10000)\n",
      "Epoch: 186 | Batch_idx: 0 |  Loss_1: (0.0018) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 186 | Batch_idx: 10 |  Loss_1: (0.0100) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 186 | Batch_idx: 20 |  Loss_1: (0.0080) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 186 | Batch_idx: 30 |  Loss_1: (0.0079) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 186 | Batch_idx: 40 |  Loss_1: (0.0083) | Acc_1: (99.71%) (5233/5248)\n",
      "Epoch: 186 | Batch_idx: 50 |  Loss_1: (0.0086) | Acc_1: (99.69%) (6508/6528)\n",
      "Epoch: 186 | Batch_idx: 60 |  Loss_1: (0.0079) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 186 | Batch_idx: 70 |  Loss_1: (0.0073) | Acc_1: (99.74%) (9064/9088)\n",
      "Epoch: 186 | Batch_idx: 80 |  Loss_1: (0.0072) | Acc_1: (99.76%) (10343/10368)\n",
      "Epoch: 186 | Batch_idx: 90 |  Loss_1: (0.0072) | Acc_1: (99.77%) (11621/11648)\n",
      "Epoch: 186 | Batch_idx: 100 |  Loss_1: (0.0072) | Acc_1: (99.77%) (12898/12928)\n",
      "Epoch: 186 | Batch_idx: 110 |  Loss_1: (0.0071) | Acc_1: (99.77%) (14175/14208)\n",
      "Epoch: 186 | Batch_idx: 120 |  Loss_1: (0.0076) | Acc_1: (99.75%) (15450/15488)\n",
      "Epoch: 186 | Batch_idx: 130 |  Loss_1: (0.0072) | Acc_1: (99.77%) (16729/16768)\n",
      "Epoch: 186 | Batch_idx: 140 |  Loss_1: (0.0077) | Acc_1: (99.75%) (18002/18048)\n",
      "Epoch: 186 | Batch_idx: 150 |  Loss_1: (0.0077) | Acc_1: (99.74%) (19278/19328)\n",
      "Epoch: 186 | Batch_idx: 160 |  Loss_1: (0.0076) | Acc_1: (99.74%) (20555/20608)\n",
      "Epoch: 186 | Batch_idx: 170 |  Loss_1: (0.0077) | Acc_1: (99.74%) (21832/21888)\n",
      "Epoch: 186 | Batch_idx: 180 |  Loss_1: (0.0075) | Acc_1: (99.75%) (23109/23168)\n",
      "Epoch: 186 | Batch_idx: 190 |  Loss_1: (0.0073) | Acc_1: (99.75%) (24387/24448)\n",
      "Epoch: 186 | Batch_idx: 200 |  Loss_1: (0.0072) | Acc_1: (99.75%) (25664/25728)\n",
      "Epoch: 186 | Batch_idx: 210 |  Loss_1: (0.0070) | Acc_1: (99.76%) (26944/27008)\n",
      "Epoch: 186 | Batch_idx: 220 |  Loss_1: (0.0069) | Acc_1: (99.77%) (28222/28288)\n",
      "Epoch: 186 | Batch_idx: 230 |  Loss_1: (0.0067) | Acc_1: (99.78%) (29502/29568)\n",
      "Epoch: 186 | Batch_idx: 240 |  Loss_1: (0.0068) | Acc_1: (99.78%) (30780/30848)\n",
      "Epoch: 186 | Batch_idx: 250 |  Loss_1: (0.0067) | Acc_1: (99.78%) (32058/32128)\n",
      "Epoch: 186 | Batch_idx: 260 |  Loss_1: (0.0067) | Acc_1: (99.79%) (33337/33408)\n",
      "Epoch: 186 | Batch_idx: 270 |  Loss_1: (0.0067) | Acc_1: (99.78%) (34613/34688)\n",
      "Epoch: 186 | Batch_idx: 280 |  Loss_1: (0.0066) | Acc_1: (99.79%) (35891/35968)\n",
      "Epoch: 186 | Batch_idx: 290 |  Loss_1: (0.0065) | Acc_1: (99.79%) (37168/37248)\n",
      "Epoch: 186 | Batch_idx: 300 |  Loss_1: (0.0064) | Acc_1: (99.79%) (38447/38528)\n",
      "Epoch: 186 | Batch_idx: 310 |  Loss_1: (0.0063) | Acc_1: (99.79%) (39725/39808)\n",
      "Epoch: 186 | Batch_idx: 320 |  Loss_1: (0.0062) | Acc_1: (99.79%) (41003/41088)\n",
      "Epoch: 186 | Batch_idx: 330 |  Loss_1: (0.0061) | Acc_1: (99.79%) (42281/42368)\n",
      "Epoch: 186 | Batch_idx: 340 |  Loss_1: (0.0060) | Acc_1: (99.80%) (43560/43648)\n",
      "Epoch: 186 | Batch_idx: 350 |  Loss_1: (0.0061) | Acc_1: (99.79%) (44835/44928)\n",
      "Epoch: 186 | Batch_idx: 360 |  Loss_1: (0.0061) | Acc_1: (99.79%) (46111/46208)\n",
      "Epoch: 186 | Batch_idx: 370 |  Loss_1: (0.0061) | Acc_1: (99.79%) (47388/47488)\n",
      "Epoch: 186 | Batch_idx: 380 |  Loss_1: (0.0061) | Acc_1: (99.79%) (48667/48768)\n",
      "Epoch: 186 | Batch_idx: 390 |  Loss_1: (0.0062) | Acc_1: (99.79%) (49896/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5076) | Acc: (92.24%) (9224/10000)\n",
      "Epoch: 187 | Batch_idx: 0 |  Loss_1: (0.0009) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 187 | Batch_idx: 10 |  Loss_1: (0.0062) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 187 | Batch_idx: 20 |  Loss_1: (0.0046) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 187 | Batch_idx: 30 |  Loss_1: (0.0058) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 187 | Batch_idx: 40 |  Loss_1: (0.0065) | Acc_1: (99.83%) (5239/5248)\n",
      "Epoch: 187 | Batch_idx: 50 |  Loss_1: (0.0058) | Acc_1: (99.85%) (6518/6528)\n",
      "Epoch: 187 | Batch_idx: 60 |  Loss_1: (0.0055) | Acc_1: (99.86%) (7797/7808)\n",
      "Epoch: 187 | Batch_idx: 70 |  Loss_1: (0.0050) | Acc_1: (99.88%) (9077/9088)\n",
      "Epoch: 187 | Batch_idx: 80 |  Loss_1: (0.0047) | Acc_1: (99.89%) (10357/10368)\n",
      "Epoch: 187 | Batch_idx: 90 |  Loss_1: (0.0047) | Acc_1: (99.89%) (11635/11648)\n",
      "Epoch: 187 | Batch_idx: 100 |  Loss_1: (0.0046) | Acc_1: (99.89%) (12914/12928)\n",
      "Epoch: 187 | Batch_idx: 110 |  Loss_1: (0.0046) | Acc_1: (99.88%) (14191/14208)\n",
      "Epoch: 187 | Batch_idx: 120 |  Loss_1: (0.0046) | Acc_1: (99.87%) (15468/15488)\n",
      "Epoch: 187 | Batch_idx: 130 |  Loss_1: (0.0045) | Acc_1: (99.87%) (16746/16768)\n",
      "Epoch: 187 | Batch_idx: 140 |  Loss_1: (0.0046) | Acc_1: (99.87%) (18024/18048)\n",
      "Epoch: 187 | Batch_idx: 150 |  Loss_1: (0.0047) | Acc_1: (99.86%) (19301/19328)\n",
      "Epoch: 187 | Batch_idx: 160 |  Loss_1: (0.0048) | Acc_1: (99.86%) (20579/20608)\n",
      "Epoch: 187 | Batch_idx: 170 |  Loss_1: (0.0047) | Acc_1: (99.85%) (21856/21888)\n",
      "Epoch: 187 | Batch_idx: 180 |  Loss_1: (0.0047) | Acc_1: (99.85%) (23134/23168)\n",
      "Epoch: 187 | Batch_idx: 190 |  Loss_1: (0.0045) | Acc_1: (99.86%) (24414/24448)\n",
      "Epoch: 187 | Batch_idx: 200 |  Loss_1: (0.0046) | Acc_1: (99.86%) (25692/25728)\n",
      "Epoch: 187 | Batch_idx: 210 |  Loss_1: (0.0046) | Acc_1: (99.86%) (26970/27008)\n",
      "Epoch: 187 | Batch_idx: 220 |  Loss_1: (0.0046) | Acc_1: (99.86%) (28248/28288)\n",
      "Epoch: 187 | Batch_idx: 230 |  Loss_1: (0.0045) | Acc_1: (99.86%) (29528/29568)\n",
      "Epoch: 187 | Batch_idx: 240 |  Loss_1: (0.0044) | Acc_1: (99.86%) (30806/30848)\n",
      "Epoch: 187 | Batch_idx: 250 |  Loss_1: (0.0045) | Acc_1: (99.87%) (32085/32128)\n",
      "Epoch: 187 | Batch_idx: 260 |  Loss_1: (0.0045) | Acc_1: (99.86%) (33362/33408)\n",
      "Epoch: 187 | Batch_idx: 270 |  Loss_1: (0.0044) | Acc_1: (99.86%) (34641/34688)\n",
      "Epoch: 187 | Batch_idx: 280 |  Loss_1: (0.0044) | Acc_1: (99.87%) (35920/35968)\n",
      "Epoch: 187 | Batch_idx: 290 |  Loss_1: (0.0043) | Acc_1: (99.87%) (37199/37248)\n",
      "Epoch: 187 | Batch_idx: 300 |  Loss_1: (0.0043) | Acc_1: (99.87%) (38477/38528)\n",
      "Epoch: 187 | Batch_idx: 310 |  Loss_1: (0.0043) | Acc_1: (99.87%) (39756/39808)\n",
      "Epoch: 187 | Batch_idx: 320 |  Loss_1: (0.0043) | Acc_1: (99.87%) (41034/41088)\n",
      "Epoch: 187 | Batch_idx: 330 |  Loss_1: (0.0043) | Acc_1: (99.87%) (42312/42368)\n",
      "Epoch: 187 | Batch_idx: 340 |  Loss_1: (0.0043) | Acc_1: (99.87%) (43592/43648)\n",
      "Epoch: 187 | Batch_idx: 350 |  Loss_1: (0.0042) | Acc_1: (99.87%) (44871/44928)\n",
      "Epoch: 187 | Batch_idx: 360 |  Loss_1: (0.0043) | Acc_1: (99.87%) (46149/46208)\n",
      "Epoch: 187 | Batch_idx: 370 |  Loss_1: (0.0042) | Acc_1: (99.87%) (47428/47488)\n",
      "Epoch: 187 | Batch_idx: 380 |  Loss_1: (0.0042) | Acc_1: (99.87%) (48707/48768)\n",
      "Epoch: 187 | Batch_idx: 390 |  Loss_1: (0.0043) | Acc_1: (99.87%) (49935/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5285) | Acc: (91.67%) (9167/10000)\n",
      "Epoch: 188 | Batch_idx: 0 |  Loss_1: (0.0004) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 188 | Batch_idx: 10 |  Loss_1: (0.0049) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 188 | Batch_idx: 20 |  Loss_1: (0.0061) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 188 | Batch_idx: 30 |  Loss_1: (0.0068) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 188 | Batch_idx: 40 |  Loss_1: (0.0058) | Acc_1: (99.81%) (5238/5248)\n",
      "Epoch: 188 | Batch_idx: 50 |  Loss_1: (0.0065) | Acc_1: (99.79%) (6514/6528)\n",
      "Epoch: 188 | Batch_idx: 60 |  Loss_1: (0.0060) | Acc_1: (99.78%) (7791/7808)\n",
      "Epoch: 188 | Batch_idx: 70 |  Loss_1: (0.0058) | Acc_1: (99.80%) (9070/9088)\n",
      "Epoch: 188 | Batch_idx: 80 |  Loss_1: (0.0053) | Acc_1: (99.82%) (10349/10368)\n",
      "Epoch: 188 | Batch_idx: 90 |  Loss_1: (0.0053) | Acc_1: (99.81%) (11626/11648)\n",
      "Epoch: 188 | Batch_idx: 100 |  Loss_1: (0.0051) | Acc_1: (99.83%) (12906/12928)\n",
      "Epoch: 188 | Batch_idx: 110 |  Loss_1: (0.0051) | Acc_1: (99.83%) (14184/14208)\n",
      "Epoch: 188 | Batch_idx: 120 |  Loss_1: (0.0053) | Acc_1: (99.82%) (15460/15488)\n",
      "Epoch: 188 | Batch_idx: 130 |  Loss_1: (0.0052) | Acc_1: (99.83%) (16739/16768)\n",
      "Epoch: 188 | Batch_idx: 140 |  Loss_1: (0.0055) | Acc_1: (99.83%) (18017/18048)\n",
      "Epoch: 188 | Batch_idx: 150 |  Loss_1: (0.0053) | Acc_1: (99.83%) (19296/19328)\n",
      "Epoch: 188 | Batch_idx: 160 |  Loss_1: (0.0052) | Acc_1: (99.84%) (20574/20608)\n",
      "Epoch: 188 | Batch_idx: 170 |  Loss_1: (0.0053) | Acc_1: (99.84%) (21852/21888)\n",
      "Epoch: 188 | Batch_idx: 180 |  Loss_1: (0.0051) | Acc_1: (99.84%) (23131/23168)\n",
      "Epoch: 188 | Batch_idx: 190 |  Loss_1: (0.0051) | Acc_1: (99.84%) (24409/24448)\n",
      "Epoch: 188 | Batch_idx: 200 |  Loss_1: (0.0050) | Acc_1: (99.84%) (25688/25728)\n",
      "Epoch: 188 | Batch_idx: 210 |  Loss_1: (0.0049) | Acc_1: (99.84%) (26966/27008)\n",
      "Epoch: 188 | Batch_idx: 220 |  Loss_1: (0.0049) | Acc_1: (99.84%) (28244/28288)\n",
      "Epoch: 188 | Batch_idx: 230 |  Loss_1: (0.0048) | Acc_1: (99.84%) (29522/29568)\n",
      "Epoch: 188 | Batch_idx: 240 |  Loss_1: (0.0048) | Acc_1: (99.84%) (30800/30848)\n",
      "Epoch: 188 | Batch_idx: 250 |  Loss_1: (0.0047) | Acc_1: (99.85%) (32079/32128)\n",
      "Epoch: 188 | Batch_idx: 260 |  Loss_1: (0.0047) | Acc_1: (99.85%) (33357/33408)\n",
      "Epoch: 188 | Batch_idx: 270 |  Loss_1: (0.0047) | Acc_1: (99.85%) (34635/34688)\n",
      "Epoch: 188 | Batch_idx: 280 |  Loss_1: (0.0046) | Acc_1: (99.85%) (35913/35968)\n",
      "Epoch: 188 | Batch_idx: 290 |  Loss_1: (0.0046) | Acc_1: (99.85%) (37192/37248)\n",
      "Epoch: 188 | Batch_idx: 300 |  Loss_1: (0.0045) | Acc_1: (99.85%) (38470/38528)\n",
      "Epoch: 188 | Batch_idx: 310 |  Loss_1: (0.0044) | Acc_1: (99.85%) (39749/39808)\n",
      "Epoch: 188 | Batch_idx: 320 |  Loss_1: (0.0044) | Acc_1: (99.85%) (41027/41088)\n",
      "Epoch: 188 | Batch_idx: 330 |  Loss_1: (0.0043) | Acc_1: (99.86%) (42307/42368)\n",
      "Epoch: 188 | Batch_idx: 340 |  Loss_1: (0.0045) | Acc_1: (99.85%) (43583/43648)\n",
      "Epoch: 188 | Batch_idx: 350 |  Loss_1: (0.0044) | Acc_1: (99.85%) (44862/44928)\n",
      "Epoch: 188 | Batch_idx: 360 |  Loss_1: (0.0043) | Acc_1: (99.86%) (46141/46208)\n",
      "Epoch: 188 | Batch_idx: 370 |  Loss_1: (0.0043) | Acc_1: (99.86%) (47420/47488)\n",
      "Epoch: 188 | Batch_idx: 380 |  Loss_1: (0.0043) | Acc_1: (99.85%) (48697/48768)\n",
      "Epoch: 188 | Batch_idx: 390 |  Loss_1: (0.0043) | Acc_1: (99.85%) (49927/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5148) | Acc: (91.87%) (9187/10000)\n",
      "Epoch: 189 | Batch_idx: 0 |  Loss_1: (0.0004) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 189 | Batch_idx: 10 |  Loss_1: (0.0022) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 189 | Batch_idx: 20 |  Loss_1: (0.0043) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 189 | Batch_idx: 30 |  Loss_1: (0.0033) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 189 | Batch_idx: 40 |  Loss_1: (0.0031) | Acc_1: (99.89%) (5242/5248)\n",
      "Epoch: 189 | Batch_idx: 50 |  Loss_1: (0.0027) | Acc_1: (99.91%) (6522/6528)\n",
      "Epoch: 189 | Batch_idx: 60 |  Loss_1: (0.0025) | Acc_1: (99.92%) (7802/7808)\n",
      "Epoch: 189 | Batch_idx: 70 |  Loss_1: (0.0023) | Acc_1: (99.93%) (9082/9088)\n",
      "Epoch: 189 | Batch_idx: 80 |  Loss_1: (0.0021) | Acc_1: (99.94%) (10362/10368)\n",
      "Epoch: 189 | Batch_idx: 90 |  Loss_1: (0.0021) | Acc_1: (99.95%) (11642/11648)\n",
      "Epoch: 189 | Batch_idx: 100 |  Loss_1: (0.0021) | Acc_1: (99.95%) (12922/12928)\n",
      "Epoch: 189 | Batch_idx: 110 |  Loss_1: (0.0022) | Acc_1: (99.95%) (14201/14208)\n",
      "Epoch: 189 | Batch_idx: 120 |  Loss_1: (0.0022) | Acc_1: (99.94%) (15478/15488)\n",
      "Epoch: 189 | Batch_idx: 130 |  Loss_1: (0.0026) | Acc_1: (99.93%) (16756/16768)\n",
      "Epoch: 189 | Batch_idx: 140 |  Loss_1: (0.0025) | Acc_1: (99.93%) (18036/18048)\n",
      "Epoch: 189 | Batch_idx: 150 |  Loss_1: (0.0024) | Acc_1: (99.93%) (19315/19328)\n",
      "Epoch: 189 | Batch_idx: 160 |  Loss_1: (0.0024) | Acc_1: (99.94%) (20595/20608)\n",
      "Epoch: 189 | Batch_idx: 170 |  Loss_1: (0.0026) | Acc_1: (99.93%) (21872/21888)\n",
      "Epoch: 189 | Batch_idx: 180 |  Loss_1: (0.0025) | Acc_1: (99.93%) (23152/23168)\n",
      "Epoch: 189 | Batch_idx: 190 |  Loss_1: (0.0025) | Acc_1: (99.93%) (24430/24448)\n",
      "Epoch: 189 | Batch_idx: 200 |  Loss_1: (0.0025) | Acc_1: (99.92%) (25708/25728)\n",
      "Epoch: 189 | Batch_idx: 210 |  Loss_1: (0.0026) | Acc_1: (99.92%) (26986/27008)\n",
      "Epoch: 189 | Batch_idx: 220 |  Loss_1: (0.0027) | Acc_1: (99.92%) (28264/28288)\n",
      "Epoch: 189 | Batch_idx: 230 |  Loss_1: (0.0030) | Acc_1: (99.91%) (29541/29568)\n",
      "Epoch: 189 | Batch_idx: 240 |  Loss_1: (0.0032) | Acc_1: (99.90%) (30818/30848)\n",
      "Epoch: 189 | Batch_idx: 250 |  Loss_1: (0.0033) | Acc_1: (99.89%) (32094/32128)\n",
      "Epoch: 189 | Batch_idx: 260 |  Loss_1: (0.0033) | Acc_1: (99.90%) (33373/33408)\n",
      "Epoch: 189 | Batch_idx: 270 |  Loss_1: (0.0035) | Acc_1: (99.88%) (34646/34688)\n",
      "Epoch: 189 | Batch_idx: 280 |  Loss_1: (0.0035) | Acc_1: (99.87%) (35923/35968)\n",
      "Epoch: 189 | Batch_idx: 290 |  Loss_1: (0.0035) | Acc_1: (99.88%) (37202/37248)\n",
      "Epoch: 189 | Batch_idx: 300 |  Loss_1: (0.0036) | Acc_1: (99.87%) (38477/38528)\n",
      "Epoch: 189 | Batch_idx: 310 |  Loss_1: (0.0038) | Acc_1: (99.86%) (39753/39808)\n",
      "Epoch: 189 | Batch_idx: 320 |  Loss_1: (0.0039) | Acc_1: (99.85%) (41028/41088)\n",
      "Epoch: 189 | Batch_idx: 330 |  Loss_1: (0.0040) | Acc_1: (99.85%) (42305/42368)\n",
      "Epoch: 189 | Batch_idx: 340 |  Loss_1: (0.0044) | Acc_1: (99.84%) (43577/43648)\n",
      "Epoch: 189 | Batch_idx: 350 |  Loss_1: (0.0044) | Acc_1: (99.84%) (44856/44928)\n",
      "Epoch: 189 | Batch_idx: 360 |  Loss_1: (0.0044) | Acc_1: (99.84%) (46134/46208)\n",
      "Epoch: 189 | Batch_idx: 370 |  Loss_1: (0.0044) | Acc_1: (99.84%) (47412/47488)\n",
      "Epoch: 189 | Batch_idx: 380 |  Loss_1: (0.0045) | Acc_1: (99.84%) (48688/48768)\n",
      "Epoch: 189 | Batch_idx: 390 |  Loss_1: (0.0047) | Acc_1: (99.83%) (49915/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5316) | Acc: (91.88%) (9188/10000)\n",
      "Epoch: 190 | Batch_idx: 0 |  Loss_1: (0.0030) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 190 | Batch_idx: 10 |  Loss_1: (0.0016) | Acc_1: (100.00%) (1408/1408)\n",
      "Epoch: 190 | Batch_idx: 20 |  Loss_1: (0.0026) | Acc_1: (99.96%) (2687/2688)\n",
      "Epoch: 190 | Batch_idx: 30 |  Loss_1: (0.0025) | Acc_1: (99.97%) (3967/3968)\n",
      "Epoch: 190 | Batch_idx: 40 |  Loss_1: (0.0026) | Acc_1: (99.96%) (5246/5248)\n",
      "Epoch: 190 | Batch_idx: 50 |  Loss_1: (0.0036) | Acc_1: (99.94%) (6524/6528)\n",
      "Epoch: 190 | Batch_idx: 60 |  Loss_1: (0.0034) | Acc_1: (99.94%) (7803/7808)\n",
      "Epoch: 190 | Batch_idx: 70 |  Loss_1: (0.0036) | Acc_1: (99.92%) (9081/9088)\n",
      "Epoch: 190 | Batch_idx: 80 |  Loss_1: (0.0034) | Acc_1: (99.93%) (10361/10368)\n",
      "Epoch: 190 | Batch_idx: 90 |  Loss_1: (0.0033) | Acc_1: (99.93%) (11640/11648)\n",
      "Epoch: 190 | Batch_idx: 100 |  Loss_1: (0.0033) | Acc_1: (99.93%) (12919/12928)\n",
      "Epoch: 190 | Batch_idx: 110 |  Loss_1: (0.0036) | Acc_1: (99.91%) (14195/14208)\n",
      "Epoch: 190 | Batch_idx: 120 |  Loss_1: (0.0040) | Acc_1: (99.90%) (15472/15488)\n",
      "Epoch: 190 | Batch_idx: 130 |  Loss_1: (0.0039) | Acc_1: (99.90%) (16751/16768)\n",
      "Epoch: 190 | Batch_idx: 140 |  Loss_1: (0.0039) | Acc_1: (99.89%) (18029/18048)\n",
      "Epoch: 190 | Batch_idx: 150 |  Loss_1: (0.0039) | Acc_1: (99.88%) (19304/19328)\n",
      "Epoch: 190 | Batch_idx: 160 |  Loss_1: (0.0041) | Acc_1: (99.88%) (20583/20608)\n",
      "Epoch: 190 | Batch_idx: 170 |  Loss_1: (0.0039) | Acc_1: (99.89%) (21863/21888)\n",
      "Epoch: 190 | Batch_idx: 180 |  Loss_1: (0.0044) | Acc_1: (99.87%) (23138/23168)\n",
      "Epoch: 190 | Batch_idx: 190 |  Loss_1: (0.0046) | Acc_1: (99.87%) (24415/24448)\n",
      "Epoch: 190 | Batch_idx: 200 |  Loss_1: (0.0046) | Acc_1: (99.86%) (25693/25728)\n",
      "Epoch: 190 | Batch_idx: 210 |  Loss_1: (0.0045) | Acc_1: (99.87%) (26973/27008)\n",
      "Epoch: 190 | Batch_idx: 220 |  Loss_1: (0.0046) | Acc_1: (99.87%) (28251/28288)\n",
      "Epoch: 190 | Batch_idx: 230 |  Loss_1: (0.0048) | Acc_1: (99.86%) (29527/29568)\n",
      "Epoch: 190 | Batch_idx: 240 |  Loss_1: (0.0047) | Acc_1: (99.86%) (30806/30848)\n",
      "Epoch: 190 | Batch_idx: 250 |  Loss_1: (0.0048) | Acc_1: (99.86%) (32082/32128)\n",
      "Epoch: 190 | Batch_idx: 260 |  Loss_1: (0.0050) | Acc_1: (99.85%) (33359/33408)\n",
      "Epoch: 190 | Batch_idx: 270 |  Loss_1: (0.0052) | Acc_1: (99.85%) (34635/34688)\n",
      "Epoch: 190 | Batch_idx: 280 |  Loss_1: (0.0052) | Acc_1: (99.84%) (35912/35968)\n",
      "Epoch: 190 | Batch_idx: 290 |  Loss_1: (0.0052) | Acc_1: (99.85%) (37191/37248)\n",
      "Epoch: 190 | Batch_idx: 300 |  Loss_1: (0.0054) | Acc_1: (99.84%) (38468/38528)\n",
      "Epoch: 190 | Batch_idx: 310 |  Loss_1: (0.0054) | Acc_1: (99.84%) (39746/39808)\n",
      "Epoch: 190 | Batch_idx: 320 |  Loss_1: (0.0055) | Acc_1: (99.84%) (41022/41088)\n",
      "Epoch: 190 | Batch_idx: 330 |  Loss_1: (0.0054) | Acc_1: (99.84%) (42301/42368)\n",
      "Epoch: 190 | Batch_idx: 340 |  Loss_1: (0.0054) | Acc_1: (99.84%) (43579/43648)\n",
      "Epoch: 190 | Batch_idx: 350 |  Loss_1: (0.0054) | Acc_1: (99.84%) (44856/44928)\n",
      "Epoch: 190 | Batch_idx: 360 |  Loss_1: (0.0054) | Acc_1: (99.84%) (46133/46208)\n",
      "Epoch: 190 | Batch_idx: 370 |  Loss_1: (0.0054) | Acc_1: (99.84%) (47412/47488)\n",
      "Epoch: 190 | Batch_idx: 380 |  Loss_1: (0.0054) | Acc_1: (99.84%) (48691/48768)\n",
      "Epoch: 190 | Batch_idx: 390 |  Loss_1: (0.0054) | Acc_1: (99.84%) (49921/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5280) | Acc: (91.58%) (9158/10000)\n",
      "Epoch: 191 | Batch_idx: 0 |  Loss_1: (0.0071) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 191 | Batch_idx: 10 |  Loss_1: (0.0094) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 191 | Batch_idx: 20 |  Loss_1: (0.0056) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 191 | Batch_idx: 30 |  Loss_1: (0.0052) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 191 | Batch_idx: 40 |  Loss_1: (0.0046) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 191 | Batch_idx: 50 |  Loss_1: (0.0051) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 191 | Batch_idx: 60 |  Loss_1: (0.0055) | Acc_1: (99.83%) (7795/7808)\n",
      "Epoch: 191 | Batch_idx: 70 |  Loss_1: (0.0054) | Acc_1: (99.83%) (9073/9088)\n",
      "Epoch: 191 | Batch_idx: 80 |  Loss_1: (0.0066) | Acc_1: (99.78%) (10345/10368)\n",
      "Epoch: 191 | Batch_idx: 90 |  Loss_1: (0.0068) | Acc_1: (99.78%) (11622/11648)\n",
      "Epoch: 191 | Batch_idx: 100 |  Loss_1: (0.0076) | Acc_1: (99.77%) (12898/12928)\n",
      "Epoch: 191 | Batch_idx: 110 |  Loss_1: (0.0079) | Acc_1: (99.77%) (14175/14208)\n",
      "Epoch: 191 | Batch_idx: 120 |  Loss_1: (0.0075) | Acc_1: (99.78%) (15454/15488)\n",
      "Epoch: 191 | Batch_idx: 130 |  Loss_1: (0.0077) | Acc_1: (99.77%) (16729/16768)\n",
      "Epoch: 191 | Batch_idx: 140 |  Loss_1: (0.0077) | Acc_1: (99.77%) (18006/18048)\n",
      "Epoch: 191 | Batch_idx: 150 |  Loss_1: (0.0075) | Acc_1: (99.77%) (19284/19328)\n",
      "Epoch: 191 | Batch_idx: 160 |  Loss_1: (0.0073) | Acc_1: (99.78%) (20562/20608)\n",
      "Epoch: 191 | Batch_idx: 170 |  Loss_1: (0.0071) | Acc_1: (99.79%) (21841/21888)\n",
      "Epoch: 191 | Batch_idx: 180 |  Loss_1: (0.0071) | Acc_1: (99.78%) (23116/23168)\n",
      "Epoch: 191 | Batch_idx: 190 |  Loss_1: (0.0073) | Acc_1: (99.76%) (24390/24448)\n",
      "Epoch: 191 | Batch_idx: 200 |  Loss_1: (0.0072) | Acc_1: (99.77%) (25669/25728)\n",
      "Epoch: 191 | Batch_idx: 210 |  Loss_1: (0.0072) | Acc_1: (99.77%) (26945/27008)\n",
      "Epoch: 191 | Batch_idx: 220 |  Loss_1: (0.0072) | Acc_1: (99.77%) (28223/28288)\n",
      "Epoch: 191 | Batch_idx: 230 |  Loss_1: (0.0071) | Acc_1: (99.77%) (29501/29568)\n",
      "Epoch: 191 | Batch_idx: 240 |  Loss_1: (0.0072) | Acc_1: (99.77%) (30777/30848)\n",
      "Epoch: 191 | Batch_idx: 250 |  Loss_1: (0.0073) | Acc_1: (99.76%) (32052/32128)\n",
      "Epoch: 191 | Batch_idx: 260 |  Loss_1: (0.0076) | Acc_1: (99.75%) (33326/33408)\n",
      "Epoch: 191 | Batch_idx: 270 |  Loss_1: (0.0076) | Acc_1: (99.75%) (34603/34688)\n",
      "Epoch: 191 | Batch_idx: 280 |  Loss_1: (0.0074) | Acc_1: (99.76%) (35883/35968)\n",
      "Epoch: 191 | Batch_idx: 290 |  Loss_1: (0.0073) | Acc_1: (99.77%) (37163/37248)\n",
      "Epoch: 191 | Batch_idx: 300 |  Loss_1: (0.0071) | Acc_1: (99.78%) (38442/38528)\n",
      "Epoch: 191 | Batch_idx: 310 |  Loss_1: (0.0072) | Acc_1: (99.77%) (39716/39808)\n",
      "Epoch: 191 | Batch_idx: 320 |  Loss_1: (0.0071) | Acc_1: (99.77%) (40995/41088)\n",
      "Epoch: 191 | Batch_idx: 330 |  Loss_1: (0.0072) | Acc_1: (99.78%) (42273/42368)\n",
      "Epoch: 191 | Batch_idx: 340 |  Loss_1: (0.0072) | Acc_1: (99.78%) (43551/43648)\n",
      "Epoch: 191 | Batch_idx: 350 |  Loss_1: (0.0074) | Acc_1: (99.78%) (44827/44928)\n",
      "Epoch: 191 | Batch_idx: 360 |  Loss_1: (0.0074) | Acc_1: (99.77%) (46103/46208)\n",
      "Epoch: 191 | Batch_idx: 370 |  Loss_1: (0.0073) | Acc_1: (99.78%) (47382/47488)\n",
      "Epoch: 191 | Batch_idx: 380 |  Loss_1: (0.0073) | Acc_1: (99.78%) (48660/48768)\n",
      "Epoch: 191 | Batch_idx: 390 |  Loss_1: (0.0073) | Acc_1: (99.78%) (49888/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5533) | Acc: (91.38%) (9138/10000)\n",
      "Epoch: 192 | Batch_idx: 0 |  Loss_1: (0.0171) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 192 | Batch_idx: 10 |  Loss_1: (0.0082) | Acc_1: (99.72%) (1404/1408)\n",
      "Epoch: 192 | Batch_idx: 20 |  Loss_1: (0.0070) | Acc_1: (99.78%) (2682/2688)\n",
      "Epoch: 192 | Batch_idx: 30 |  Loss_1: (0.0066) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 192 | Batch_idx: 40 |  Loss_1: (0.0058) | Acc_1: (99.81%) (5238/5248)\n",
      "Epoch: 192 | Batch_idx: 50 |  Loss_1: (0.0062) | Acc_1: (99.75%) (6512/6528)\n",
      "Epoch: 192 | Batch_idx: 60 |  Loss_1: (0.0064) | Acc_1: (99.76%) (7789/7808)\n",
      "Epoch: 192 | Batch_idx: 70 |  Loss_1: (0.0060) | Acc_1: (99.78%) (9068/9088)\n",
      "Epoch: 192 | Batch_idx: 80 |  Loss_1: (0.0057) | Acc_1: (99.79%) (10346/10368)\n",
      "Epoch: 192 | Batch_idx: 90 |  Loss_1: (0.0061) | Acc_1: (99.80%) (11625/11648)\n",
      "Epoch: 192 | Batch_idx: 100 |  Loss_1: (0.0065) | Acc_1: (99.78%) (12900/12928)\n",
      "Epoch: 192 | Batch_idx: 110 |  Loss_1: (0.0065) | Acc_1: (99.79%) (14178/14208)\n",
      "Epoch: 192 | Batch_idx: 120 |  Loss_1: (0.0065) | Acc_1: (99.78%) (15454/15488)\n",
      "Epoch: 192 | Batch_idx: 130 |  Loss_1: (0.0063) | Acc_1: (99.79%) (16733/16768)\n",
      "Epoch: 192 | Batch_idx: 140 |  Loss_1: (0.0060) | Acc_1: (99.80%) (18012/18048)\n",
      "Epoch: 192 | Batch_idx: 150 |  Loss_1: (0.0059) | Acc_1: (99.80%) (19289/19328)\n",
      "Epoch: 192 | Batch_idx: 160 |  Loss_1: (0.0059) | Acc_1: (99.80%) (20567/20608)\n",
      "Epoch: 192 | Batch_idx: 170 |  Loss_1: (0.0061) | Acc_1: (99.80%) (21844/21888)\n",
      "Epoch: 192 | Batch_idx: 180 |  Loss_1: (0.0061) | Acc_1: (99.79%) (23119/23168)\n",
      "Epoch: 192 | Batch_idx: 190 |  Loss_1: (0.0062) | Acc_1: (99.78%) (24393/24448)\n",
      "Epoch: 192 | Batch_idx: 200 |  Loss_1: (0.0062) | Acc_1: (99.77%) (25669/25728)\n",
      "Epoch: 192 | Batch_idx: 210 |  Loss_1: (0.0060) | Acc_1: (99.78%) (26948/27008)\n",
      "Epoch: 192 | Batch_idx: 220 |  Loss_1: (0.0061) | Acc_1: (99.77%) (28222/28288)\n",
      "Epoch: 192 | Batch_idx: 230 |  Loss_1: (0.0060) | Acc_1: (99.77%) (29501/29568)\n",
      "Epoch: 192 | Batch_idx: 240 |  Loss_1: (0.0061) | Acc_1: (99.77%) (30778/30848)\n",
      "Epoch: 192 | Batch_idx: 250 |  Loss_1: (0.0061) | Acc_1: (99.77%) (32054/32128)\n",
      "Epoch: 192 | Batch_idx: 260 |  Loss_1: (0.0062) | Acc_1: (99.77%) (33332/33408)\n",
      "Epoch: 192 | Batch_idx: 270 |  Loss_1: (0.0062) | Acc_1: (99.77%) (34609/34688)\n",
      "Epoch: 192 | Batch_idx: 280 |  Loss_1: (0.0062) | Acc_1: (99.77%) (35885/35968)\n",
      "Epoch: 192 | Batch_idx: 290 |  Loss_1: (0.0062) | Acc_1: (99.77%) (37161/37248)\n",
      "Epoch: 192 | Batch_idx: 300 |  Loss_1: (0.0061) | Acc_1: (99.77%) (38441/38528)\n",
      "Epoch: 192 | Batch_idx: 310 |  Loss_1: (0.0060) | Acc_1: (99.78%) (39719/39808)\n",
      "Epoch: 192 | Batch_idx: 320 |  Loss_1: (0.0059) | Acc_1: (99.78%) (40998/41088)\n",
      "Epoch: 192 | Batch_idx: 330 |  Loss_1: (0.0058) | Acc_1: (99.78%) (42276/42368)\n",
      "Epoch: 192 | Batch_idx: 340 |  Loss_1: (0.0058) | Acc_1: (99.78%) (43553/43648)\n",
      "Epoch: 192 | Batch_idx: 350 |  Loss_1: (0.0058) | Acc_1: (99.78%) (44828/44928)\n",
      "Epoch: 192 | Batch_idx: 360 |  Loss_1: (0.0058) | Acc_1: (99.78%) (46106/46208)\n",
      "Epoch: 192 | Batch_idx: 370 |  Loss_1: (0.0057) | Acc_1: (99.78%) (47384/47488)\n",
      "Epoch: 192 | Batch_idx: 380 |  Loss_1: (0.0056) | Acc_1: (99.79%) (48664/48768)\n",
      "Epoch: 192 | Batch_idx: 390 |  Loss_1: (0.0056) | Acc_1: (99.79%) (49894/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4931) | Acc: (92.15%) (9215/10000)\n",
      "Epoch: 193 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 193 | Batch_idx: 10 |  Loss_1: (0.0049) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 193 | Batch_idx: 20 |  Loss_1: (0.0040) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 193 | Batch_idx: 30 |  Loss_1: (0.0045) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 193 | Batch_idx: 40 |  Loss_1: (0.0044) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 193 | Batch_idx: 50 |  Loss_1: (0.0047) | Acc_1: (99.82%) (6516/6528)\n",
      "Epoch: 193 | Batch_idx: 60 |  Loss_1: (0.0042) | Acc_1: (99.85%) (7796/7808)\n",
      "Epoch: 193 | Batch_idx: 70 |  Loss_1: (0.0043) | Acc_1: (99.85%) (9074/9088)\n",
      "Epoch: 193 | Batch_idx: 80 |  Loss_1: (0.0040) | Acc_1: (99.86%) (10353/10368)\n",
      "Epoch: 193 | Batch_idx: 90 |  Loss_1: (0.0039) | Acc_1: (99.86%) (11632/11648)\n",
      "Epoch: 193 | Batch_idx: 100 |  Loss_1: (0.0040) | Acc_1: (99.86%) (12910/12928)\n",
      "Epoch: 193 | Batch_idx: 110 |  Loss_1: (0.0041) | Acc_1: (99.85%) (14187/14208)\n",
      "Epoch: 193 | Batch_idx: 120 |  Loss_1: (0.0039) | Acc_1: (99.86%) (15467/15488)\n",
      "Epoch: 193 | Batch_idx: 130 |  Loss_1: (0.0038) | Acc_1: (99.87%) (16746/16768)\n",
      "Epoch: 193 | Batch_idx: 140 |  Loss_1: (0.0041) | Acc_1: (99.86%) (18023/18048)\n",
      "Epoch: 193 | Batch_idx: 150 |  Loss_1: (0.0043) | Acc_1: (99.86%) (19301/19328)\n",
      "Epoch: 193 | Batch_idx: 160 |  Loss_1: (0.0043) | Acc_1: (99.86%) (20579/20608)\n",
      "Epoch: 193 | Batch_idx: 170 |  Loss_1: (0.0046) | Acc_1: (99.85%) (21855/21888)\n",
      "Epoch: 193 | Batch_idx: 180 |  Loss_1: (0.0045) | Acc_1: (99.86%) (23135/23168)\n",
      "Epoch: 193 | Batch_idx: 190 |  Loss_1: (0.0046) | Acc_1: (99.86%) (24413/24448)\n",
      "Epoch: 193 | Batch_idx: 200 |  Loss_1: (0.0044) | Acc_1: (99.86%) (25693/25728)\n",
      "Epoch: 193 | Batch_idx: 210 |  Loss_1: (0.0045) | Acc_1: (99.86%) (26970/27008)\n",
      "Epoch: 193 | Batch_idx: 220 |  Loss_1: (0.0045) | Acc_1: (99.86%) (28248/28288)\n",
      "Epoch: 193 | Batch_idx: 230 |  Loss_1: (0.0046) | Acc_1: (99.85%) (29525/29568)\n",
      "Epoch: 193 | Batch_idx: 240 |  Loss_1: (0.0048) | Acc_1: (99.85%) (30801/30848)\n",
      "Epoch: 193 | Batch_idx: 250 |  Loss_1: (0.0050) | Acc_1: (99.84%) (32076/32128)\n",
      "Epoch: 193 | Batch_idx: 260 |  Loss_1: (0.0050) | Acc_1: (99.84%) (33354/33408)\n",
      "Epoch: 193 | Batch_idx: 270 |  Loss_1: (0.0051) | Acc_1: (99.83%) (34629/34688)\n",
      "Epoch: 193 | Batch_idx: 280 |  Loss_1: (0.0052) | Acc_1: (99.83%) (35906/35968)\n",
      "Epoch: 193 | Batch_idx: 290 |  Loss_1: (0.0053) | Acc_1: (99.83%) (37184/37248)\n",
      "Epoch: 193 | Batch_idx: 300 |  Loss_1: (0.0053) | Acc_1: (99.83%) (38461/38528)\n",
      "Epoch: 193 | Batch_idx: 310 |  Loss_1: (0.0053) | Acc_1: (99.83%) (39739/39808)\n",
      "Epoch: 193 | Batch_idx: 320 |  Loss_1: (0.0053) | Acc_1: (99.83%) (41017/41088)\n",
      "Epoch: 193 | Batch_idx: 330 |  Loss_1: (0.0052) | Acc_1: (99.83%) (42296/42368)\n",
      "Epoch: 193 | Batch_idx: 340 |  Loss_1: (0.0052) | Acc_1: (99.83%) (43574/43648)\n",
      "Epoch: 193 | Batch_idx: 350 |  Loss_1: (0.0051) | Acc_1: (99.83%) (44853/44928)\n",
      "Epoch: 193 | Batch_idx: 360 |  Loss_1: (0.0051) | Acc_1: (99.84%) (46132/46208)\n",
      "Epoch: 193 | Batch_idx: 370 |  Loss_1: (0.0052) | Acc_1: (99.83%) (47409/47488)\n",
      "Epoch: 193 | Batch_idx: 380 |  Loss_1: (0.0054) | Acc_1: (99.83%) (48684/48768)\n",
      "Epoch: 193 | Batch_idx: 390 |  Loss_1: (0.0054) | Acc_1: (99.83%) (49914/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5447) | Acc: (91.33%) (9133/10000)\n",
      "Epoch: 194 | Batch_idx: 0 |  Loss_1: (0.0043) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 194 | Batch_idx: 10 |  Loss_1: (0.0083) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 194 | Batch_idx: 20 |  Loss_1: (0.0079) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 194 | Batch_idx: 30 |  Loss_1: (0.0095) | Acc_1: (99.72%) (3957/3968)\n",
      "Epoch: 194 | Batch_idx: 40 |  Loss_1: (0.0082) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 194 | Batch_idx: 50 |  Loss_1: (0.0073) | Acc_1: (99.79%) (6514/6528)\n",
      "Epoch: 194 | Batch_idx: 60 |  Loss_1: (0.0073) | Acc_1: (99.77%) (7790/7808)\n",
      "Epoch: 194 | Batch_idx: 70 |  Loss_1: (0.0074) | Acc_1: (99.76%) (9066/9088)\n",
      "Epoch: 194 | Batch_idx: 80 |  Loss_1: (0.0082) | Acc_1: (99.73%) (10340/10368)\n",
      "Epoch: 194 | Batch_idx: 90 |  Loss_1: (0.0080) | Acc_1: (99.73%) (11617/11648)\n",
      "Epoch: 194 | Batch_idx: 100 |  Loss_1: (0.0079) | Acc_1: (99.73%) (12893/12928)\n",
      "Epoch: 194 | Batch_idx: 110 |  Loss_1: (0.0076) | Acc_1: (99.73%) (14169/14208)\n",
      "Epoch: 194 | Batch_idx: 120 |  Loss_1: (0.0075) | Acc_1: (99.74%) (15448/15488)\n",
      "Epoch: 194 | Batch_idx: 130 |  Loss_1: (0.0073) | Acc_1: (99.76%) (16727/16768)\n",
      "Epoch: 194 | Batch_idx: 140 |  Loss_1: (0.0070) | Acc_1: (99.77%) (18006/18048)\n",
      "Epoch: 194 | Batch_idx: 150 |  Loss_1: (0.0068) | Acc_1: (99.77%) (19283/19328)\n",
      "Epoch: 194 | Batch_idx: 160 |  Loss_1: (0.0069) | Acc_1: (99.76%) (20559/20608)\n",
      "Epoch: 194 | Batch_idx: 170 |  Loss_1: (0.0066) | Acc_1: (99.77%) (21838/21888)\n",
      "Epoch: 194 | Batch_idx: 180 |  Loss_1: (0.0065) | Acc_1: (99.78%) (23118/23168)\n",
      "Epoch: 194 | Batch_idx: 190 |  Loss_1: (0.0063) | Acc_1: (99.79%) (24397/24448)\n",
      "Epoch: 194 | Batch_idx: 200 |  Loss_1: (0.0065) | Acc_1: (99.78%) (25672/25728)\n",
      "Epoch: 194 | Batch_idx: 210 |  Loss_1: (0.0067) | Acc_1: (99.78%) (26948/27008)\n",
      "Epoch: 194 | Batch_idx: 220 |  Loss_1: (0.0066) | Acc_1: (99.78%) (28226/28288)\n",
      "Epoch: 194 | Batch_idx: 230 |  Loss_1: (0.0064) | Acc_1: (99.78%) (29504/29568)\n",
      "Epoch: 194 | Batch_idx: 240 |  Loss_1: (0.0063) | Acc_1: (99.79%) (30783/30848)\n",
      "Epoch: 194 | Batch_idx: 250 |  Loss_1: (0.0063) | Acc_1: (99.79%) (32061/32128)\n",
      "Epoch: 194 | Batch_idx: 260 |  Loss_1: (0.0065) | Acc_1: (99.78%) (33336/33408)\n",
      "Epoch: 194 | Batch_idx: 270 |  Loss_1: (0.0063) | Acc_1: (99.79%) (34615/34688)\n",
      "Epoch: 194 | Batch_idx: 280 |  Loss_1: (0.0066) | Acc_1: (99.78%) (35888/35968)\n",
      "Epoch: 194 | Batch_idx: 290 |  Loss_1: (0.0067) | Acc_1: (99.77%) (37164/37248)\n",
      "Epoch: 194 | Batch_idx: 300 |  Loss_1: (0.0068) | Acc_1: (99.77%) (38438/38528)\n",
      "Epoch: 194 | Batch_idx: 310 |  Loss_1: (0.0067) | Acc_1: (99.77%) (39717/39808)\n",
      "Epoch: 194 | Batch_idx: 320 |  Loss_1: (0.0067) | Acc_1: (99.77%) (40993/41088)\n",
      "Epoch: 194 | Batch_idx: 330 |  Loss_1: (0.0066) | Acc_1: (99.77%) (42271/42368)\n",
      "Epoch: 194 | Batch_idx: 340 |  Loss_1: (0.0066) | Acc_1: (99.78%) (43550/43648)\n",
      "Epoch: 194 | Batch_idx: 350 |  Loss_1: (0.0064) | Acc_1: (99.78%) (44830/44928)\n",
      "Epoch: 194 | Batch_idx: 360 |  Loss_1: (0.0063) | Acc_1: (99.78%) (46108/46208)\n",
      "Epoch: 194 | Batch_idx: 370 |  Loss_1: (0.0062) | Acc_1: (99.79%) (47387/47488)\n",
      "Epoch: 194 | Batch_idx: 380 |  Loss_1: (0.0062) | Acc_1: (99.79%) (48664/48768)\n",
      "Epoch: 194 | Batch_idx: 390 |  Loss_1: (0.0062) | Acc_1: (99.79%) (49894/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5133) | Acc: (91.99%) (9199/10000)\n",
      "Epoch: 195 | Batch_idx: 0 |  Loss_1: (0.0120) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 195 | Batch_idx: 10 |  Loss_1: (0.0024) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 195 | Batch_idx: 20 |  Loss_1: (0.0048) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 195 | Batch_idx: 30 |  Loss_1: (0.0043) | Acc_1: (99.82%) (3961/3968)\n",
      "Epoch: 195 | Batch_idx: 40 |  Loss_1: (0.0039) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 195 | Batch_idx: 50 |  Loss_1: (0.0040) | Acc_1: (99.83%) (6517/6528)\n",
      "Epoch: 195 | Batch_idx: 60 |  Loss_1: (0.0039) | Acc_1: (99.85%) (7796/7808)\n",
      "Epoch: 195 | Batch_idx: 70 |  Loss_1: (0.0035) | Acc_1: (99.87%) (9076/9088)\n",
      "Epoch: 195 | Batch_idx: 80 |  Loss_1: (0.0045) | Acc_1: (99.83%) (10350/10368)\n",
      "Epoch: 195 | Batch_idx: 90 |  Loss_1: (0.0045) | Acc_1: (99.83%) (11628/11648)\n",
      "Epoch: 195 | Batch_idx: 100 |  Loss_1: (0.0049) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 195 | Batch_idx: 110 |  Loss_1: (0.0052) | Acc_1: (99.80%) (14179/14208)\n",
      "Epoch: 195 | Batch_idx: 120 |  Loss_1: (0.0050) | Acc_1: (99.81%) (15458/15488)\n",
      "Epoch: 195 | Batch_idx: 130 |  Loss_1: (0.0049) | Acc_1: (99.82%) (16737/16768)\n",
      "Epoch: 195 | Batch_idx: 140 |  Loss_1: (0.0048) | Acc_1: (99.81%) (18014/18048)\n",
      "Epoch: 195 | Batch_idx: 150 |  Loss_1: (0.0048) | Acc_1: (99.82%) (19293/19328)\n",
      "Epoch: 195 | Batch_idx: 160 |  Loss_1: (0.0051) | Acc_1: (99.80%) (20567/20608)\n",
      "Epoch: 195 | Batch_idx: 170 |  Loss_1: (0.0053) | Acc_1: (99.79%) (21843/21888)\n",
      "Epoch: 195 | Batch_idx: 180 |  Loss_1: (0.0052) | Acc_1: (99.80%) (23121/23168)\n",
      "Epoch: 195 | Batch_idx: 190 |  Loss_1: (0.0052) | Acc_1: (99.80%) (24398/24448)\n",
      "Epoch: 195 | Batch_idx: 200 |  Loss_1: (0.0054) | Acc_1: (99.79%) (25675/25728)\n",
      "Epoch: 195 | Batch_idx: 210 |  Loss_1: (0.0053) | Acc_1: (99.80%) (26954/27008)\n",
      "Epoch: 195 | Batch_idx: 220 |  Loss_1: (0.0056) | Acc_1: (99.79%) (28229/28288)\n",
      "Epoch: 195 | Batch_idx: 230 |  Loss_1: (0.0057) | Acc_1: (99.79%) (29506/29568)\n",
      "Epoch: 195 | Batch_idx: 240 |  Loss_1: (0.0056) | Acc_1: (99.79%) (30784/30848)\n",
      "Epoch: 195 | Batch_idx: 250 |  Loss_1: (0.0056) | Acc_1: (99.79%) (32062/32128)\n",
      "Epoch: 195 | Batch_idx: 260 |  Loss_1: (0.0056) | Acc_1: (99.80%) (33342/33408)\n",
      "Epoch: 195 | Batch_idx: 270 |  Loss_1: (0.0057) | Acc_1: (99.80%) (34617/34688)\n",
      "Epoch: 195 | Batch_idx: 280 |  Loss_1: (0.0056) | Acc_1: (99.80%) (35895/35968)\n",
      "Epoch: 195 | Batch_idx: 290 |  Loss_1: (0.0056) | Acc_1: (99.79%) (37171/37248)\n",
      "Epoch: 195 | Batch_idx: 300 |  Loss_1: (0.0058) | Acc_1: (99.79%) (38448/38528)\n",
      "Epoch: 195 | Batch_idx: 310 |  Loss_1: (0.0059) | Acc_1: (99.79%) (39724/39808)\n",
      "Epoch: 195 | Batch_idx: 320 |  Loss_1: (0.0058) | Acc_1: (99.80%) (41004/41088)\n",
      "Epoch: 195 | Batch_idx: 330 |  Loss_1: (0.0057) | Acc_1: (99.80%) (42283/42368)\n",
      "Epoch: 195 | Batch_idx: 340 |  Loss_1: (0.0056) | Acc_1: (99.80%) (43562/43648)\n",
      "Epoch: 195 | Batch_idx: 350 |  Loss_1: (0.0056) | Acc_1: (99.81%) (44841/44928)\n",
      "Epoch: 195 | Batch_idx: 360 |  Loss_1: (0.0056) | Acc_1: (99.81%) (46119/46208)\n",
      "Epoch: 195 | Batch_idx: 370 |  Loss_1: (0.0055) | Acc_1: (99.80%) (47395/47488)\n",
      "Epoch: 195 | Batch_idx: 380 |  Loss_1: (0.0056) | Acc_1: (99.80%) (48671/48768)\n",
      "Epoch: 195 | Batch_idx: 390 |  Loss_1: (0.0056) | Acc_1: (99.80%) (49902/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5278) | Acc: (91.89%) (9189/10000)\n",
      "Epoch: 196 | Batch_idx: 0 |  Loss_1: (0.0002) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 196 | Batch_idx: 10 |  Loss_1: (0.0046) | Acc_1: (99.86%) (1406/1408)\n",
      "Epoch: 196 | Batch_idx: 20 |  Loss_1: (0.0041) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 196 | Batch_idx: 30 |  Loss_1: (0.0042) | Acc_1: (99.85%) (3962/3968)\n",
      "Epoch: 196 | Batch_idx: 40 |  Loss_1: (0.0042) | Acc_1: (99.85%) (5240/5248)\n",
      "Epoch: 196 | Batch_idx: 50 |  Loss_1: (0.0046) | Acc_1: (99.85%) (6518/6528)\n",
      "Epoch: 196 | Batch_idx: 60 |  Loss_1: (0.0052) | Acc_1: (99.82%) (7794/7808)\n",
      "Epoch: 196 | Batch_idx: 70 |  Loss_1: (0.0055) | Acc_1: (99.80%) (9070/9088)\n",
      "Epoch: 196 | Batch_idx: 80 |  Loss_1: (0.0053) | Acc_1: (99.81%) (10348/10368)\n",
      "Epoch: 196 | Batch_idx: 90 |  Loss_1: (0.0050) | Acc_1: (99.82%) (11627/11648)\n",
      "Epoch: 196 | Batch_idx: 100 |  Loss_1: (0.0047) | Acc_1: (99.84%) (12907/12928)\n",
      "Epoch: 196 | Batch_idx: 110 |  Loss_1: (0.0043) | Acc_1: (99.85%) (14187/14208)\n",
      "Epoch: 196 | Batch_idx: 120 |  Loss_1: (0.0043) | Acc_1: (99.86%) (15467/15488)\n",
      "Epoch: 196 | Batch_idx: 130 |  Loss_1: (0.0042) | Acc_1: (99.87%) (16746/16768)\n",
      "Epoch: 196 | Batch_idx: 140 |  Loss_1: (0.0043) | Acc_1: (99.87%) (18024/18048)\n",
      "Epoch: 196 | Batch_idx: 150 |  Loss_1: (0.0043) | Acc_1: (99.87%) (19302/19328)\n",
      "Epoch: 196 | Batch_idx: 160 |  Loss_1: (0.0042) | Acc_1: (99.87%) (20582/20608)\n",
      "Epoch: 196 | Batch_idx: 170 |  Loss_1: (0.0041) | Acc_1: (99.88%) (21861/21888)\n",
      "Epoch: 196 | Batch_idx: 180 |  Loss_1: (0.0040) | Acc_1: (99.88%) (23140/23168)\n",
      "Epoch: 196 | Batch_idx: 190 |  Loss_1: (0.0039) | Acc_1: (99.89%) (24420/24448)\n",
      "Epoch: 196 | Batch_idx: 200 |  Loss_1: (0.0038) | Acc_1: (99.89%) (25699/25728)\n",
      "Epoch: 196 | Batch_idx: 210 |  Loss_1: (0.0042) | Acc_1: (99.88%) (26975/27008)\n",
      "Epoch: 196 | Batch_idx: 220 |  Loss_1: (0.0042) | Acc_1: (99.88%) (28253/28288)\n",
      "Epoch: 196 | Batch_idx: 230 |  Loss_1: (0.0043) | Acc_1: (99.87%) (29530/29568)\n",
      "Epoch: 196 | Batch_idx: 240 |  Loss_1: (0.0045) | Acc_1: (99.86%) (30806/30848)\n",
      "Epoch: 196 | Batch_idx: 250 |  Loss_1: (0.0047) | Acc_1: (99.86%) (32083/32128)\n",
      "Epoch: 196 | Batch_idx: 260 |  Loss_1: (0.0046) | Acc_1: (99.86%) (33360/33408)\n",
      "Epoch: 196 | Batch_idx: 270 |  Loss_1: (0.0048) | Acc_1: (99.85%) (34636/34688)\n",
      "Epoch: 196 | Batch_idx: 280 |  Loss_1: (0.0049) | Acc_1: (99.85%) (35913/35968)\n",
      "Epoch: 196 | Batch_idx: 290 |  Loss_1: (0.0048) | Acc_1: (99.85%) (37192/37248)\n",
      "Epoch: 196 | Batch_idx: 300 |  Loss_1: (0.0050) | Acc_1: (99.84%) (38466/38528)\n",
      "Epoch: 196 | Batch_idx: 310 |  Loss_1: (0.0050) | Acc_1: (99.84%) (39743/39808)\n",
      "Epoch: 196 | Batch_idx: 320 |  Loss_1: (0.0050) | Acc_1: (99.83%) (41018/41088)\n",
      "Epoch: 196 | Batch_idx: 330 |  Loss_1: (0.0052) | Acc_1: (99.82%) (42293/42368)\n",
      "Epoch: 196 | Batch_idx: 340 |  Loss_1: (0.0053) | Acc_1: (99.82%) (43570/43648)\n",
      "Epoch: 196 | Batch_idx: 350 |  Loss_1: (0.0052) | Acc_1: (99.83%) (44850/44928)\n",
      "Epoch: 196 | Batch_idx: 360 |  Loss_1: (0.0055) | Acc_1: (99.82%) (46126/46208)\n",
      "Epoch: 196 | Batch_idx: 370 |  Loss_1: (0.0054) | Acc_1: (99.82%) (47404/47488)\n",
      "Epoch: 196 | Batch_idx: 380 |  Loss_1: (0.0054) | Acc_1: (99.82%) (48681/48768)\n",
      "Epoch: 196 | Batch_idx: 390 |  Loss_1: (0.0055) | Acc_1: (99.82%) (49909/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5219) | Acc: (91.80%) (9180/10000)\n",
      "Epoch: 197 | Batch_idx: 0 |  Loss_1: (0.0005) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 197 | Batch_idx: 10 |  Loss_1: (0.0069) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 197 | Batch_idx: 20 |  Loss_1: (0.0059) | Acc_1: (99.85%) (2684/2688)\n",
      "Epoch: 197 | Batch_idx: 30 |  Loss_1: (0.0050) | Acc_1: (99.87%) (3963/3968)\n",
      "Epoch: 197 | Batch_idx: 40 |  Loss_1: (0.0046) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 197 | Batch_idx: 50 |  Loss_1: (0.0054) | Acc_1: (99.86%) (6519/6528)\n",
      "Epoch: 197 | Batch_idx: 60 |  Loss_1: (0.0049) | Acc_1: (99.87%) (7798/7808)\n",
      "Epoch: 197 | Batch_idx: 70 |  Loss_1: (0.0046) | Acc_1: (99.88%) (9077/9088)\n",
      "Epoch: 197 | Batch_idx: 80 |  Loss_1: (0.0047) | Acc_1: (99.86%) (10354/10368)\n",
      "Epoch: 197 | Batch_idx: 90 |  Loss_1: (0.0049) | Acc_1: (99.85%) (11631/11648)\n",
      "Epoch: 197 | Batch_idx: 100 |  Loss_1: (0.0055) | Acc_1: (99.81%) (12904/12928)\n",
      "Epoch: 197 | Batch_idx: 110 |  Loss_1: (0.0057) | Acc_1: (99.80%) (14180/14208)\n",
      "Epoch: 197 | Batch_idx: 120 |  Loss_1: (0.0056) | Acc_1: (99.80%) (15457/15488)\n",
      "Epoch: 197 | Batch_idx: 130 |  Loss_1: (0.0054) | Acc_1: (99.82%) (16737/16768)\n",
      "Epoch: 197 | Batch_idx: 140 |  Loss_1: (0.0057) | Acc_1: (99.80%) (18012/18048)\n",
      "Epoch: 197 | Batch_idx: 150 |  Loss_1: (0.0062) | Acc_1: (99.78%) (19286/19328)\n",
      "Epoch: 197 | Batch_idx: 160 |  Loss_1: (0.0066) | Acc_1: (99.79%) (20564/20608)\n",
      "Epoch: 197 | Batch_idx: 170 |  Loss_1: (0.0064) | Acc_1: (99.79%) (21842/21888)\n",
      "Epoch: 197 | Batch_idx: 180 |  Loss_1: (0.0067) | Acc_1: (99.78%) (23116/23168)\n",
      "Epoch: 197 | Batch_idx: 190 |  Loss_1: (0.0065) | Acc_1: (99.78%) (24395/24448)\n",
      "Epoch: 197 | Batch_idx: 200 |  Loss_1: (0.0069) | Acc_1: (99.77%) (25669/25728)\n",
      "Epoch: 197 | Batch_idx: 210 |  Loss_1: (0.0071) | Acc_1: (99.76%) (26942/27008)\n",
      "Epoch: 197 | Batch_idx: 220 |  Loss_1: (0.0071) | Acc_1: (99.75%) (28218/28288)\n",
      "Epoch: 197 | Batch_idx: 230 |  Loss_1: (0.0069) | Acc_1: (99.76%) (29497/29568)\n",
      "Epoch: 197 | Batch_idx: 240 |  Loss_1: (0.0068) | Acc_1: (99.76%) (30775/30848)\n",
      "Epoch: 197 | Batch_idx: 250 |  Loss_1: (0.0068) | Acc_1: (99.77%) (32053/32128)\n",
      "Epoch: 197 | Batch_idx: 260 |  Loss_1: (0.0066) | Acc_1: (99.78%) (33333/33408)\n",
      "Epoch: 197 | Batch_idx: 270 |  Loss_1: (0.0065) | Acc_1: (99.78%) (34613/34688)\n",
      "Epoch: 197 | Batch_idx: 280 |  Loss_1: (0.0063) | Acc_1: (99.79%) (35892/35968)\n",
      "Epoch: 197 | Batch_idx: 290 |  Loss_1: (0.0065) | Acc_1: (99.79%) (37168/37248)\n",
      "Epoch: 197 | Batch_idx: 300 |  Loss_1: (0.0064) | Acc_1: (99.79%) (38446/38528)\n",
      "Epoch: 197 | Batch_idx: 310 |  Loss_1: (0.0064) | Acc_1: (99.79%) (39723/39808)\n",
      "Epoch: 197 | Batch_idx: 320 |  Loss_1: (0.0063) | Acc_1: (99.79%) (41001/41088)\n",
      "Epoch: 197 | Batch_idx: 330 |  Loss_1: (0.0064) | Acc_1: (99.78%) (42276/42368)\n",
      "Epoch: 197 | Batch_idx: 340 |  Loss_1: (0.0065) | Acc_1: (99.78%) (43552/43648)\n",
      "Epoch: 197 | Batch_idx: 350 |  Loss_1: (0.0066) | Acc_1: (99.78%) (44827/44928)\n",
      "Epoch: 197 | Batch_idx: 360 |  Loss_1: (0.0066) | Acc_1: (99.77%) (46104/46208)\n",
      "Epoch: 197 | Batch_idx: 370 |  Loss_1: (0.0067) | Acc_1: (99.77%) (47381/47488)\n",
      "Epoch: 197 | Batch_idx: 380 |  Loss_1: (0.0068) | Acc_1: (99.78%) (48660/48768)\n",
      "Epoch: 197 | Batch_idx: 390 |  Loss_1: (0.0068) | Acc_1: (99.78%) (49889/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5075) | Acc: (91.93%) (9193/10000)\n",
      "Epoch: 198 | Batch_idx: 0 |  Loss_1: (0.0017) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 198 | Batch_idx: 10 |  Loss_1: (0.0069) | Acc_1: (99.79%) (1405/1408)\n",
      "Epoch: 198 | Batch_idx: 20 |  Loss_1: (0.0055) | Acc_1: (99.81%) (2683/2688)\n",
      "Epoch: 198 | Batch_idx: 30 |  Loss_1: (0.0057) | Acc_1: (99.77%) (3959/3968)\n",
      "Epoch: 198 | Batch_idx: 40 |  Loss_1: (0.0058) | Acc_1: (99.77%) (5236/5248)\n",
      "Epoch: 198 | Batch_idx: 50 |  Loss_1: (0.0061) | Acc_1: (99.74%) (6511/6528)\n",
      "Epoch: 198 | Batch_idx: 60 |  Loss_1: (0.0080) | Acc_1: (99.72%) (7786/7808)\n",
      "Epoch: 198 | Batch_idx: 70 |  Loss_1: (0.0071) | Acc_1: (99.75%) (9065/9088)\n",
      "Epoch: 198 | Batch_idx: 80 |  Loss_1: (0.0071) | Acc_1: (99.75%) (10342/10368)\n",
      "Epoch: 198 | Batch_idx: 90 |  Loss_1: (0.0066) | Acc_1: (99.76%) (11620/11648)\n",
      "Epoch: 198 | Batch_idx: 100 |  Loss_1: (0.0064) | Acc_1: (99.77%) (12898/12928)\n",
      "Epoch: 198 | Batch_idx: 110 |  Loss_1: (0.0066) | Acc_1: (99.77%) (14175/14208)\n",
      "Epoch: 198 | Batch_idx: 120 |  Loss_1: (0.0063) | Acc_1: (99.78%) (15454/15488)\n",
      "Epoch: 198 | Batch_idx: 130 |  Loss_1: (0.0064) | Acc_1: (99.79%) (16732/16768)\n",
      "Epoch: 198 | Batch_idx: 140 |  Loss_1: (0.0066) | Acc_1: (99.79%) (18011/18048)\n",
      "Epoch: 198 | Batch_idx: 150 |  Loss_1: (0.0064) | Acc_1: (99.80%) (19289/19328)\n",
      "Epoch: 198 | Batch_idx: 160 |  Loss_1: (0.0068) | Acc_1: (99.79%) (20564/20608)\n",
      "Epoch: 198 | Batch_idx: 170 |  Loss_1: (0.0074) | Acc_1: (99.78%) (21839/21888)\n",
      "Epoch: 198 | Batch_idx: 180 |  Loss_1: (0.0074) | Acc_1: (99.78%) (23117/23168)\n",
      "Epoch: 198 | Batch_idx: 190 |  Loss_1: (0.0079) | Acc_1: (99.78%) (24393/24448)\n",
      "Epoch: 198 | Batch_idx: 200 |  Loss_1: (0.0079) | Acc_1: (99.77%) (25668/25728)\n",
      "Epoch: 198 | Batch_idx: 210 |  Loss_1: (0.0078) | Acc_1: (99.77%) (26945/27008)\n",
      "Epoch: 198 | Batch_idx: 220 |  Loss_1: (0.0079) | Acc_1: (99.77%) (28222/28288)\n",
      "Epoch: 198 | Batch_idx: 230 |  Loss_1: (0.0078) | Acc_1: (99.77%) (29501/29568)\n",
      "Epoch: 198 | Batch_idx: 240 |  Loss_1: (0.0080) | Acc_1: (99.77%) (30776/30848)\n",
      "Epoch: 198 | Batch_idx: 250 |  Loss_1: (0.0081) | Acc_1: (99.75%) (32049/32128)\n",
      "Epoch: 198 | Batch_idx: 260 |  Loss_1: (0.0083) | Acc_1: (99.75%) (33326/33408)\n",
      "Epoch: 198 | Batch_idx: 270 |  Loss_1: (0.0084) | Acc_1: (99.75%) (34600/34688)\n",
      "Epoch: 198 | Batch_idx: 280 |  Loss_1: (0.0083) | Acc_1: (99.75%) (35878/35968)\n",
      "Epoch: 198 | Batch_idx: 290 |  Loss_1: (0.0081) | Acc_1: (99.76%) (37158/37248)\n",
      "Epoch: 198 | Batch_idx: 300 |  Loss_1: (0.0080) | Acc_1: (99.76%) (38436/38528)\n",
      "Epoch: 198 | Batch_idx: 310 |  Loss_1: (0.0079) | Acc_1: (99.76%) (39712/39808)\n",
      "Epoch: 198 | Batch_idx: 320 |  Loss_1: (0.0079) | Acc_1: (99.76%) (40989/41088)\n",
      "Epoch: 198 | Batch_idx: 330 |  Loss_1: (0.0079) | Acc_1: (99.76%) (42266/42368)\n",
      "Epoch: 198 | Batch_idx: 340 |  Loss_1: (0.0078) | Acc_1: (99.76%) (43544/43648)\n",
      "Epoch: 198 | Batch_idx: 350 |  Loss_1: (0.0078) | Acc_1: (99.76%) (44820/44928)\n",
      "Epoch: 198 | Batch_idx: 360 |  Loss_1: (0.0077) | Acc_1: (99.76%) (46098/46208)\n",
      "Epoch: 198 | Batch_idx: 370 |  Loss_1: (0.0076) | Acc_1: (99.76%) (47376/47488)\n",
      "Epoch: 198 | Batch_idx: 380 |  Loss_1: (0.0075) | Acc_1: (99.77%) (48655/48768)\n",
      "Epoch: 198 | Batch_idx: 390 |  Loss_1: (0.0075) | Acc_1: (99.77%) (49884/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5107) | Acc: (91.82%) (9182/10000)\n",
      "Epoch: 199 | Batch_idx: 0 |  Loss_1: (0.0001) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 199 | Batch_idx: 10 |  Loss_1: (0.0037) | Acc_1: (99.93%) (1407/1408)\n",
      "Epoch: 199 | Batch_idx: 20 |  Loss_1: (0.0044) | Acc_1: (99.89%) (2685/2688)\n",
      "Epoch: 199 | Batch_idx: 30 |  Loss_1: (0.0051) | Acc_1: (99.82%) (3961/3968)\n",
      "Epoch: 199 | Batch_idx: 40 |  Loss_1: (0.0041) | Acc_1: (99.87%) (5241/5248)\n",
      "Epoch: 199 | Batch_idx: 50 |  Loss_1: (0.0040) | Acc_1: (99.88%) (6520/6528)\n",
      "Epoch: 199 | Batch_idx: 60 |  Loss_1: (0.0037) | Acc_1: (99.87%) (7798/7808)\n",
      "Epoch: 199 | Batch_idx: 70 |  Loss_1: (0.0042) | Acc_1: (99.86%) (9075/9088)\n",
      "Epoch: 199 | Batch_idx: 80 |  Loss_1: (0.0043) | Acc_1: (99.85%) (10352/10368)\n",
      "Epoch: 199 | Batch_idx: 90 |  Loss_1: (0.0046) | Acc_1: (99.84%) (11629/11648)\n",
      "Epoch: 199 | Batch_idx: 100 |  Loss_1: (0.0044) | Acc_1: (99.84%) (12907/12928)\n",
      "Epoch: 199 | Batch_idx: 110 |  Loss_1: (0.0044) | Acc_1: (99.84%) (14185/14208)\n",
      "Epoch: 199 | Batch_idx: 120 |  Loss_1: (0.0045) | Acc_1: (99.83%) (15461/15488)\n",
      "Epoch: 199 | Batch_idx: 130 |  Loss_1: (0.0043) | Acc_1: (99.84%) (16741/16768)\n",
      "Epoch: 199 | Batch_idx: 140 |  Loss_1: (0.0043) | Acc_1: (99.83%) (18018/18048)\n",
      "Epoch: 199 | Batch_idx: 150 |  Loss_1: (0.0043) | Acc_1: (99.84%) (19297/19328)\n",
      "Epoch: 199 | Batch_idx: 160 |  Loss_1: (0.0043) | Acc_1: (99.84%) (20575/20608)\n",
      "Epoch: 199 | Batch_idx: 170 |  Loss_1: (0.0043) | Acc_1: (99.84%) (21853/21888)\n",
      "Epoch: 199 | Batch_idx: 180 |  Loss_1: (0.0042) | Acc_1: (99.84%) (23132/23168)\n",
      "Epoch: 199 | Batch_idx: 190 |  Loss_1: (0.0041) | Acc_1: (99.84%) (24410/24448)\n",
      "Epoch: 199 | Batch_idx: 200 |  Loss_1: (0.0041) | Acc_1: (99.85%) (25689/25728)\n",
      "Epoch: 199 | Batch_idx: 210 |  Loss_1: (0.0041) | Acc_1: (99.84%) (26966/27008)\n",
      "Epoch: 199 | Batch_idx: 220 |  Loss_1: (0.0041) | Acc_1: (99.84%) (28243/28288)\n",
      "Epoch: 199 | Batch_idx: 230 |  Loss_1: (0.0041) | Acc_1: (99.84%) (29522/29568)\n",
      "Epoch: 199 | Batch_idx: 240 |  Loss_1: (0.0041) | Acc_1: (99.85%) (30801/30848)\n",
      "Epoch: 199 | Batch_idx: 250 |  Loss_1: (0.0041) | Acc_1: (99.85%) (32080/32128)\n",
      "Epoch: 199 | Batch_idx: 260 |  Loss_1: (0.0040) | Acc_1: (99.85%) (33359/33408)\n",
      "Epoch: 199 | Batch_idx: 270 |  Loss_1: (0.0043) | Acc_1: (99.85%) (34636/34688)\n",
      "Epoch: 199 | Batch_idx: 280 |  Loss_1: (0.0042) | Acc_1: (99.85%) (35914/35968)\n",
      "Epoch: 199 | Batch_idx: 290 |  Loss_1: (0.0043) | Acc_1: (99.84%) (37190/37248)\n",
      "Epoch: 199 | Batch_idx: 300 |  Loss_1: (0.0044) | Acc_1: (99.84%) (38468/38528)\n",
      "Epoch: 199 | Batch_idx: 310 |  Loss_1: (0.0044) | Acc_1: (99.84%) (39745/39808)\n",
      "Epoch: 199 | Batch_idx: 320 |  Loss_1: (0.0044) | Acc_1: (99.84%) (41023/41088)\n",
      "Epoch: 199 | Batch_idx: 330 |  Loss_1: (0.0044) | Acc_1: (99.84%) (42301/42368)\n",
      "Epoch: 199 | Batch_idx: 340 |  Loss_1: (0.0044) | Acc_1: (99.85%) (43581/43648)\n",
      "Epoch: 199 | Batch_idx: 350 |  Loss_1: (0.0043) | Acc_1: (99.85%) (44860/44928)\n",
      "Epoch: 199 | Batch_idx: 360 |  Loss_1: (0.0044) | Acc_1: (99.85%) (46138/46208)\n",
      "Epoch: 199 | Batch_idx: 370 |  Loss_1: (0.0045) | Acc_1: (99.85%) (47416/47488)\n",
      "Epoch: 199 | Batch_idx: 380 |  Loss_1: (0.0044) | Acc_1: (99.85%) (48694/48768)\n",
      "Epoch: 199 | Batch_idx: 390 |  Loss_1: (0.0045) | Acc_1: (99.85%) (49924/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5421) | Acc: (92.08%) (9208/10000)\n",
      "3 hours 40 mins 3 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet_50_origin.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 200):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet_50_origin.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8766a17f2584f17ae9875767170f3464b2a051bfe2b6423fb227ac503acbc200"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hw2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
