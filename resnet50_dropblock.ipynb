{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.004\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet_50_dropblock') #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformer_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    #transforms.ColorJitter(brightness=(0.2, 2), contrast=(0.3, 2), saturation=(0.2, 2), hue=(-0.3, 0.3)),\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_2 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_3 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "validation_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_2 = torch.utils.data.DataLoader(dataset=training_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_3 = torch.utils.data.DataLoader(dataset=training_dataset_3, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlock2D(nn.Module):\n",
    "    r\"\"\"Randomly zeroes 2D spatial blocks of the input tensor.\n",
    "    As described in the paper\n",
    "    `DropBlock: A regularization method for convolutional networks`_ ,\n",
    "    dropping whole blocks of feature map allows to remove semantic\n",
    "    information as compared to regular dropout.\n",
    "    Args:\n",
    "        drop_prob (float): probability of an element to be dropped.\n",
    "        block_size (int): size of the block to drop\n",
    "    Shape:\n",
    "        - Input: `(N, C, H, W)`\n",
    "        - Output: `(N, C, H, W)`\n",
    "    .. _DropBlock: A regularization method for convolutional networks:\n",
    "       https://arxiv.org/abs/1810.12890\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob, block_size):\n",
    "        super(DropBlock2D, self).__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        assert x.dim() == 4, \\\n",
    "            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n",
    "\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        else:\n",
    "            # get gamma value\n",
    "            gamma = self._compute_gamma(x)\n",
    "\n",
    "            # sample mask\n",
    "            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n",
    "\n",
    "            # place mask on input device\n",
    "            mask = mask.to(x.device)\n",
    "\n",
    "            # compute block mask\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "\n",
    "            # apply block mask\n",
    "            out = x * block_mask[:, None, :, :]\n",
    "\n",
    "            # scale output\n",
    "            out = out * block_mask.numel() / block_mask.sum()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n",
    "                                  kernel_size=(self.block_size, self.block_size),\n",
    "                                  stride=(1, 1),\n",
    "                                  padding=self.block_size // 2)\n",
    "\n",
    "        if self.block_size % 2 == 0:\n",
    "            block_mask = block_mask[:, :, :-1, :-1]\n",
    "\n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "\n",
    "        return block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob / (self.block_size ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(nn.Module):\n",
    "    def __init__(self, dropblock, start_value, stop_value, nr_steps):\n",
    "        super(LinearScheduler, self).__init__()\n",
    "        self.dropblock = dropblock\n",
    "        self.i = 0\n",
    "        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=int(nr_steps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropblock(x)\n",
    "\n",
    "    def step(self):\n",
    "        if self.i < len(self.drop_values):\n",
    "            self.dropblock.drop_prob = self.drop_values[self.i]\n",
    "\n",
    "        self.i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, start_prob, stop_prob, block_size, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "       \n",
    "        self.start_prob = start_prob\n",
    "        self.stop_prob = stop_prob\n",
    "        self.block_size = block_size\n",
    "        self.dropblock = LinearScheduler(DropBlock2D(drop_prob=self.start_prob, block_size=self.block_size), start_value=self.start_prob, stop_value=self.start_prob, nr_steps=60000)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropblock.step()\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.dropblock(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.dropblock(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BottleNeck, [3, 4, 6, 3], 0.6, 0.8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 1 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
      "             ReLU-13          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-14          [-1, 256, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "           Conv2d-18           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
      "             ReLU-20           [-1, 64, 32, 32]               0\n",
      "           Conv2d-21          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-22          [-1, 256, 32, 32]             512\n",
      "             ReLU-23          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
      "             ReLU-27           [-1, 64, 32, 32]               0\n",
      "           Conv2d-28           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 32, 32]             128\n",
      "             ReLU-30           [-1, 64, 32, 32]               0\n",
      "           Conv2d-31          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
      "             ReLU-33          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "             ReLU-40          [-1, 128, 16, 16]               0\n",
      "           Conv2d-41          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-42          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-43          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-45          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-46          [-1, 512, 16, 16]               0\n",
      "           Conv2d-47          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-48          [-1, 128, 16, 16]             256\n",
      "             ReLU-49          [-1, 128, 16, 16]               0\n",
      "           Conv2d-50          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-51          [-1, 128, 16, 16]             256\n",
      "             ReLU-52          [-1, 128, 16, 16]               0\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-55          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-56          [-1, 512, 16, 16]               0\n",
      "           Conv2d-57          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-58          [-1, 128, 16, 16]             256\n",
      "             ReLU-59          [-1, 128, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-66          [-1, 512, 16, 16]               0\n",
      "           Conv2d-67          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-68          [-1, 128, 16, 16]             256\n",
      "             ReLU-69          [-1, 128, 16, 16]               0\n",
      "           Conv2d-70          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-71          [-1, 128, 16, 16]             256\n",
      "             ReLU-72          [-1, 128, 16, 16]               0\n",
      "           Conv2d-73          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-74          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-75          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-76          [-1, 512, 16, 16]               0\n",
      "      DropBlock2D-77          [-1, 512, 16, 16]               0\n",
      "  LinearScheduler-78          [-1, 512, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
      "             ReLU-81          [-1, 256, 16, 16]               0\n",
      "           Conv2d-82            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 8, 8]             512\n",
      "             ReLU-84            [-1, 256, 8, 8]               0\n",
      "           Conv2d-85           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-87           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-89           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-90           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-91            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 8, 8]             512\n",
      "             ReLU-93            [-1, 256, 8, 8]               0\n",
      "           Conv2d-94            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 8, 8]             512\n",
      "             ReLU-96            [-1, 256, 8, 8]               0\n",
      "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-99           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-100           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-101            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 8, 8]             512\n",
      "            ReLU-103            [-1, 256, 8, 8]               0\n",
      "          Conv2d-104            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 8, 8]             512\n",
      "            ReLU-106            [-1, 256, 8, 8]               0\n",
      "          Conv2d-107           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-109           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-110           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-111            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 8, 8]             512\n",
      "            ReLU-113            [-1, 256, 8, 8]               0\n",
      "          Conv2d-114            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 8, 8]             512\n",
      "            ReLU-116            [-1, 256, 8, 8]               0\n",
      "          Conv2d-117           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-119           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-120           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-121            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 8, 8]             512\n",
      "            ReLU-123            [-1, 256, 8, 8]               0\n",
      "          Conv2d-124            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 8, 8]             512\n",
      "            ReLU-126            [-1, 256, 8, 8]               0\n",
      "          Conv2d-127           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-129           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-130           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-131            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 8, 8]             512\n",
      "            ReLU-133            [-1, 256, 8, 8]               0\n",
      "          Conv2d-134            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 8, 8]             512\n",
      "            ReLU-136            [-1, 256, 8, 8]               0\n",
      "          Conv2d-137           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-139           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-140           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-141            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-143            [-1, 512, 8, 8]               0\n",
      "          Conv2d-144            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-146            [-1, 512, 4, 4]               0\n",
      "          Conv2d-147           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-149           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-151           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-152           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-153            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-155            [-1, 512, 4, 4]               0\n",
      "          Conv2d-156            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-158            [-1, 512, 4, 4]               0\n",
      "          Conv2d-159           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-161           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-162           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-163            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-165            [-1, 512, 4, 4]               0\n",
      "          Conv2d-166            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-168            [-1, 512, 4, 4]               0\n",
      "          Conv2d-169           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-171           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-172           [-1, 2048, 4, 4]               0\n",
      "     DropBlock2D-173           [-1, 2048, 4, 4]               0\n",
      " LinearScheduler-174           [-1, 2048, 4, 4]               0\n",
      "          Linear-175                   [-1, 10]          20,490\n",
      "          ResNet-176                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 23,520,842\n",
      "Trainable params: 23,520,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 90.56\n",
      "Params size (MB): 89.72\n",
      "Estimated Total Size (MB): 180.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(training_loader)\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(training_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(validation_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(validation_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (2.5891) | Acc_1: (7.81%) (10/128)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (2.9513) | Acc_1: (10.65%) (150/1408)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (2.7738) | Acc_1: (12.57%) (338/2688)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (2.7075) | Acc_1: (12.63%) (501/3968)\n",
      "Epoch: 0 | Batch_idx: 40 |  Loss_1: (2.6318) | Acc_1: (13.45%) (706/5248)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss_1: (2.5706) | Acc_1: (14.09%) (920/6528)\n",
      "Epoch: 0 | Batch_idx: 60 |  Loss_1: (2.5080) | Acc_1: (14.97%) (1169/7808)\n",
      "Epoch: 0 | Batch_idx: 70 |  Loss_1: (2.4571) | Acc_1: (15.72%) (1429/9088)\n",
      "Epoch: 0 | Batch_idx: 80 |  Loss_1: (2.4190) | Acc_1: (16.80%) (1742/10368)\n",
      "Epoch: 0 | Batch_idx: 90 |  Loss_1: (2.3894) | Acc_1: (17.33%) (2019/11648)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss_1: (2.3623) | Acc_1: (18.06%) (2335/12928)\n",
      "Epoch: 0 | Batch_idx: 110 |  Loss_1: (2.3394) | Acc_1: (18.47%) (2624/14208)\n",
      "Epoch: 0 | Batch_idx: 120 |  Loss_1: (2.3154) | Acc_1: (19.00%) (2942/15488)\n",
      "Epoch: 0 | Batch_idx: 130 |  Loss_1: (2.2934) | Acc_1: (19.63%) (3291/16768)\n",
      "Epoch: 0 | Batch_idx: 140 |  Loss_1: (2.2747) | Acc_1: (20.01%) (3612/18048)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss_1: (2.2537) | Acc_1: (20.55%) (3972/19328)\n",
      "Epoch: 0 | Batch_idx: 160 |  Loss_1: (2.2338) | Acc_1: (21.14%) (4356/20608)\n",
      "Epoch: 0 | Batch_idx: 170 |  Loss_1: (2.2174) | Acc_1: (21.64%) (4736/21888)\n",
      "Epoch: 0 | Batch_idx: 180 |  Loss_1: (2.2035) | Acc_1: (22.04%) (5107/23168)\n",
      "Epoch: 0 | Batch_idx: 190 |  Loss_1: (2.1877) | Acc_1: (22.44%) (5486/24448)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss_1: (2.1759) | Acc_1: (22.75%) (5854/25728)\n",
      "Epoch: 0 | Batch_idx: 210 |  Loss_1: (2.1625) | Acc_1: (23.27%) (6286/27008)\n",
      "Epoch: 0 | Batch_idx: 220 |  Loss_1: (2.1509) | Acc_1: (23.57%) (6668/28288)\n",
      "Epoch: 0 | Batch_idx: 230 |  Loss_1: (2.1386) | Acc_1: (23.93%) (7076/29568)\n",
      "Epoch: 0 | Batch_idx: 240 |  Loss_1: (2.1267) | Acc_1: (24.37%) (7517/30848)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss_1: (2.1158) | Acc_1: (24.73%) (7946/32128)\n",
      "Epoch: 0 | Batch_idx: 260 |  Loss_1: (2.1050) | Acc_1: (25.10%) (8385/33408)\n",
      "Epoch: 0 | Batch_idx: 270 |  Loss_1: (2.0942) | Acc_1: (25.43%) (8822/34688)\n",
      "Epoch: 0 | Batch_idx: 280 |  Loss_1: (2.0854) | Acc_1: (25.68%) (9236/35968)\n",
      "Epoch: 0 | Batch_idx: 290 |  Loss_1: (2.0750) | Acc_1: (25.97%) (9673/37248)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss_1: (2.0659) | Acc_1: (26.25%) (10114/38528)\n",
      "Epoch: 0 | Batch_idx: 310 |  Loss_1: (2.0548) | Acc_1: (26.61%) (10594/39808)\n",
      "Epoch: 0 | Batch_idx: 320 |  Loss_1: (2.0475) | Acc_1: (26.92%) (11060/41088)\n",
      "Epoch: 0 | Batch_idx: 330 |  Loss_1: (2.0397) | Acc_1: (27.25%) (11544/42368)\n",
      "Epoch: 0 | Batch_idx: 340 |  Loss_1: (2.0322) | Acc_1: (27.49%) (12001/43648)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss_1: (2.0237) | Acc_1: (27.74%) (12461/44928)\n",
      "Epoch: 0 | Batch_idx: 360 |  Loss_1: (2.0167) | Acc_1: (28.03%) (12953/46208)\n",
      "Epoch: 0 | Batch_idx: 370 |  Loss_1: (2.0094) | Acc_1: (28.29%) (13433/47488)\n",
      "Epoch: 0 | Batch_idx: 380 |  Loss_1: (2.0003) | Acc_1: (28.56%) (13927/48768)\n",
      "Epoch: 0 | Batch_idx: 390 |  Loss_1: (1.9928) | Acc_1: (28.77%) (14386/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.6819) | Acc: (39.92%) (3992/10000)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.7412) | Acc_1: (36.72%) (47/128)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.7187) | Acc_1: (38.07%) (536/1408)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.7084) | Acc_1: (38.13%) (1025/2688)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.7004) | Acc_1: (37.98%) (1507/3968)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss_1: (1.6890) | Acc_1: (38.55%) (2023/5248)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss_1: (1.6930) | Acc_1: (38.25%) (2497/6528)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss_1: (1.6925) | Acc_1: (38.49%) (3005/7808)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss_1: (1.6860) | Acc_1: (38.94%) (3539/9088)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss_1: (1.6796) | Acc_1: (38.96%) (4039/10368)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss_1: (1.6782) | Acc_1: (39.11%) (4556/11648)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss_1: (1.6776) | Acc_1: (38.98%) (5039/12928)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss_1: (1.6737) | Acc_1: (39.34%) (5589/14208)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss_1: (1.6641) | Acc_1: (39.56%) (6127/15488)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss_1: (1.6550) | Acc_1: (39.94%) (6697/16768)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss_1: (1.6496) | Acc_1: (40.13%) (7243/18048)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss_1: (1.6472) | Acc_1: (40.29%) (7788/19328)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss_1: (1.6444) | Acc_1: (40.45%) (8336/20608)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss_1: (1.6405) | Acc_1: (40.58%) (8882/21888)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss_1: (1.6401) | Acc_1: (40.61%) (9409/23168)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss_1: (1.6359) | Acc_1: (40.74%) (9960/24448)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss_1: (1.6315) | Acc_1: (40.95%) (10536/25728)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss_1: (1.6274) | Acc_1: (41.08%) (11095/27008)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss_1: (1.6257) | Acc_1: (41.15%) (11640/28288)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss_1: (1.6234) | Acc_1: (41.26%) (12199/29568)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss_1: (1.6207) | Acc_1: (41.36%) (12758/30848)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss_1: (1.6173) | Acc_1: (41.55%) (13349/32128)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss_1: (1.6149) | Acc_1: (41.72%) (13939/33408)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss_1: (1.6108) | Acc_1: (41.82%) (14506/34688)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss_1: (1.6083) | Acc_1: (41.91%) (15075/35968)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss_1: (1.6054) | Acc_1: (42.06%) (15665/37248)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss_1: (1.6059) | Acc_1: (42.09%) (16215/38528)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss_1: (1.6031) | Acc_1: (42.15%) (16778/39808)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss_1: (1.6005) | Acc_1: (42.25%) (17360/41088)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss_1: (1.5977) | Acc_1: (42.39%) (17960/42368)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss_1: (1.5939) | Acc_1: (42.52%) (18561/43648)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss_1: (1.5924) | Acc_1: (42.59%) (19134/44928)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss_1: (1.5887) | Acc_1: (42.74%) (19749/46208)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss_1: (1.5860) | Acc_1: (42.89%) (20367/47488)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss_1: (1.5841) | Acc_1: (42.97%) (20955/48768)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss_1: (1.5804) | Acc_1: (43.09%) (21547/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.3755) | Acc: (51.07%) (5107/10000)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.4116) | Acc_1: (49.22%) (63/128)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.4526) | Acc_1: (48.51%) (683/1408)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.4753) | Acc_1: (48.07%) (1292/2688)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.4787) | Acc_1: (46.98%) (1864/3968)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss_1: (1.4787) | Acc_1: (46.72%) (2452/5248)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss_1: (1.4736) | Acc_1: (46.81%) (3056/6528)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss_1: (1.4728) | Acc_1: (47.14%) (3681/7808)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss_1: (1.4752) | Acc_1: (47.21%) (4290/9088)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss_1: (1.4686) | Acc_1: (47.58%) (4933/10368)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss_1: (1.4675) | Acc_1: (47.60%) (5545/11648)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss_1: (1.4645) | Acc_1: (47.59%) (6152/12928)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss_1: (1.4632) | Acc_1: (47.72%) (6780/14208)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss_1: (1.4571) | Acc_1: (47.87%) (7414/15488)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss_1: (1.4569) | Acc_1: (47.83%) (8020/16768)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss_1: (1.4541) | Acc_1: (47.90%) (8645/18048)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss_1: (1.4519) | Acc_1: (48.04%) (9286/19328)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss_1: (1.4495) | Acc_1: (48.13%) (9919/20608)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss_1: (1.4443) | Acc_1: (48.33%) (10579/21888)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss_1: (1.4401) | Acc_1: (48.52%) (11240/23168)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss_1: (1.4394) | Acc_1: (48.73%) (11913/24448)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss_1: (1.4365) | Acc_1: (48.83%) (12563/25728)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss_1: (1.4366) | Acc_1: (48.79%) (13177/27008)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss_1: (1.4311) | Acc_1: (49.01%) (13865/28288)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss_1: (1.4247) | Acc_1: (49.25%) (14563/29568)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss_1: (1.4246) | Acc_1: (49.29%) (15205/30848)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss_1: (1.4226) | Acc_1: (49.30%) (15839/32128)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss_1: (1.4216) | Acc_1: (49.40%) (16502/33408)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss_1: (1.4191) | Acc_1: (49.56%) (17193/34688)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss_1: (1.4169) | Acc_1: (49.68%) (17869/35968)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss_1: (1.4141) | Acc_1: (49.80%) (18548/37248)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss_1: (1.4111) | Acc_1: (49.89%) (19220/38528)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss_1: (1.4093) | Acc_1: (49.93%) (19877/39808)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss_1: (1.4069) | Acc_1: (50.00%) (20545/41088)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss_1: (1.4031) | Acc_1: (50.11%) (21230/42368)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss_1: (1.4015) | Acc_1: (50.16%) (21896/43648)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss_1: (1.3997) | Acc_1: (50.24%) (22571/44928)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss_1: (1.3966) | Acc_1: (50.34%) (23263/46208)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss_1: (1.3925) | Acc_1: (50.49%) (23977/47488)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss_1: (1.3902) | Acc_1: (50.57%) (24662/48768)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss_1: (1.3878) | Acc_1: (50.67%) (25337/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.2493) | Acc: (55.07%) (5507/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.0365) | Acc_1: (64.84%) (83/128)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.3064) | Acc_1: (55.54%) (782/1408)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.3093) | Acc_1: (54.20%) (1457/2688)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.3111) | Acc_1: (54.11%) (2147/3968)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss_1: (1.3064) | Acc_1: (54.00%) (2834/5248)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss_1: (1.3097) | Acc_1: (53.60%) (3499/6528)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss_1: (1.3097) | Acc_1: (53.59%) (4184/7808)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss_1: (1.3119) | Acc_1: (53.58%) (4869/9088)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss_1: (1.3064) | Acc_1: (53.72%) (5570/10368)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss_1: (1.2993) | Acc_1: (53.82%) (6269/11648)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss_1: (1.2955) | Acc_1: (54.05%) (6988/12928)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss_1: (1.2914) | Acc_1: (54.17%) (7697/14208)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss_1: (1.2885) | Acc_1: (54.20%) (8394/15488)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss_1: (1.2875) | Acc_1: (54.29%) (9103/16768)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss_1: (1.2831) | Acc_1: (54.45%) (9827/18048)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss_1: (1.2798) | Acc_1: (54.58%) (10549/19328)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss_1: (1.2781) | Acc_1: (54.70%) (11272/20608)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss_1: (1.2780) | Acc_1: (54.82%) (12000/21888)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss_1: (1.2777) | Acc_1: (54.90%) (12719/23168)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss_1: (1.2759) | Acc_1: (55.08%) (13465/24448)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss_1: (1.2700) | Acc_1: (55.32%) (14233/25728)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss_1: (1.2680) | Acc_1: (55.41%) (14964/27008)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss_1: (1.2637) | Acc_1: (55.56%) (15716/28288)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss_1: (1.2653) | Acc_1: (55.59%) (16436/29568)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss_1: (1.2643) | Acc_1: (55.70%) (17183/30848)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss_1: (1.2629) | Acc_1: (55.79%) (17924/32128)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss_1: (1.2616) | Acc_1: (55.86%) (18663/33408)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss_1: (1.2576) | Acc_1: (56.02%) (19433/34688)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss_1: (1.2548) | Acc_1: (56.14%) (20194/35968)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss_1: (1.2543) | Acc_1: (56.14%) (20911/37248)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss_1: (1.2530) | Acc_1: (56.19%) (21648/38528)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss_1: (1.2515) | Acc_1: (56.18%) (22363/39808)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss_1: (1.2484) | Acc_1: (56.32%) (23139/41088)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss_1: (1.2456) | Acc_1: (56.40%) (23896/42368)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss_1: (1.2445) | Acc_1: (56.47%) (24649/43648)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss_1: (1.2436) | Acc_1: (56.49%) (25382/44928)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss_1: (1.2415) | Acc_1: (56.51%) (26111/46208)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss_1: (1.2403) | Acc_1: (56.58%) (26868/47488)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss_1: (1.2386) | Acc_1: (56.64%) (27624/48768)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss_1: (1.2371) | Acc_1: (56.69%) (28346/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0356) | Acc: (63.26%) (6326/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.3493) | Acc_1: (56.25%) (72/128)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.1815) | Acc_1: (59.87%) (843/1408)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.1474) | Acc_1: (60.64%) (1630/2688)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.1525) | Acc_1: (60.66%) (2407/3968)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss_1: (1.1516) | Acc_1: (60.31%) (3165/5248)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss_1: (1.1516) | Acc_1: (60.25%) (3933/6528)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss_1: (1.1543) | Acc_1: (60.23%) (4703/7808)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss_1: (1.1599) | Acc_1: (59.73%) (5428/9088)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss_1: (1.1626) | Acc_1: (59.70%) (6190/10368)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss_1: (1.1665) | Acc_1: (59.46%) (6926/11648)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss_1: (1.1637) | Acc_1: (59.61%) (7707/12928)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss_1: (1.1589) | Acc_1: (59.73%) (8487/14208)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss_1: (1.1555) | Acc_1: (59.81%) (9264/15488)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss_1: (1.1547) | Acc_1: (59.83%) (10032/16768)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss_1: (1.1524) | Acc_1: (59.83%) (10798/18048)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss_1: (1.1513) | Acc_1: (59.78%) (11555/19328)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss_1: (1.1503) | Acc_1: (59.80%) (12323/20608)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss_1: (1.1497) | Acc_1: (59.80%) (13089/21888)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss_1: (1.1513) | Acc_1: (59.83%) (13861/23168)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss_1: (1.1490) | Acc_1: (59.78%) (14614/24448)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss_1: (1.1457) | Acc_1: (59.94%) (15422/25728)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss_1: (1.1445) | Acc_1: (59.98%) (16199/27008)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss_1: (1.1415) | Acc_1: (60.08%) (16995/28288)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss_1: (1.1400) | Acc_1: (60.16%) (17788/29568)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss_1: (1.1387) | Acc_1: (60.20%) (18572/30848)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss_1: (1.1398) | Acc_1: (60.21%) (19343/32128)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss_1: (1.1393) | Acc_1: (60.19%) (20109/33408)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss_1: (1.1402) | Acc_1: (60.19%) (20878/34688)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss_1: (1.1386) | Acc_1: (60.26%) (21675/35968)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss_1: (1.1361) | Acc_1: (60.34%) (22476/37248)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss_1: (1.1344) | Acc_1: (60.36%) (23256/38528)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss_1: (1.1331) | Acc_1: (60.45%) (24063/39808)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss_1: (1.1316) | Acc_1: (60.51%) (24864/41088)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss_1: (1.1301) | Acc_1: (60.58%) (25666/42368)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss_1: (1.1282) | Acc_1: (60.67%) (26481/43648)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss_1: (1.1259) | Acc_1: (60.71%) (27276/44928)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss_1: (1.1253) | Acc_1: (60.69%) (28043/46208)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss_1: (1.1247) | Acc_1: (60.70%) (28826/47488)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss_1: (1.1249) | Acc_1: (60.71%) (29607/48768)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss_1: (1.1253) | Acc_1: (60.70%) (30351/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0575) | Acc: (64.52%) (6452/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (0.8749) | Acc_1: (69.53%) (89/128)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (0.9836) | Acc_1: (66.55%) (937/1408)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (1.0177) | Acc_1: (64.40%) (1731/2688)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (1.0276) | Acc_1: (64.01%) (2540/3968)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss_1: (1.0398) | Acc_1: (63.66%) (3341/5248)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss_1: (1.0545) | Acc_1: (63.30%) (4132/6528)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss_1: (1.0586) | Acc_1: (62.90%) (4911/7808)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss_1: (1.0583) | Acc_1: (62.73%) (5701/9088)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss_1: (1.0607) | Acc_1: (62.90%) (6521/10368)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss_1: (1.0612) | Acc_1: (62.95%) (7332/11648)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss_1: (1.0586) | Acc_1: (63.16%) (8165/12928)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss_1: (1.0578) | Acc_1: (63.20%) (8979/14208)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss_1: (1.0620) | Acc_1: (62.96%) (9751/15488)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss_1: (1.0657) | Acc_1: (62.89%) (10546/16768)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss_1: (1.0614) | Acc_1: (63.02%) (11373/18048)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss_1: (1.0587) | Acc_1: (63.08%) (12193/19328)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss_1: (1.0562) | Acc_1: (63.21%) (13027/20608)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss_1: (1.0509) | Acc_1: (63.37%) (13870/21888)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss_1: (1.0466) | Acc_1: (63.50%) (14712/23168)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss_1: (1.0454) | Acc_1: (63.54%) (15535/24448)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss_1: (1.0428) | Acc_1: (63.67%) (16381/25728)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss_1: (1.0422) | Acc_1: (63.66%) (17193/27008)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss_1: (1.0444) | Acc_1: (63.62%) (17997/28288)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss_1: (1.0456) | Acc_1: (63.59%) (18802/29568)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss_1: (1.0473) | Acc_1: (63.53%) (19598/30848)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss_1: (1.0477) | Acc_1: (63.51%) (20406/32128)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss_1: (1.0449) | Acc_1: (63.53%) (21225/33408)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss_1: (1.0454) | Acc_1: (63.52%) (22033/34688)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss_1: (1.0447) | Acc_1: (63.56%) (22861/35968)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss_1: (1.0449) | Acc_1: (63.56%) (23673/37248)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss_1: (1.0439) | Acc_1: (63.55%) (24483/38528)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss_1: (1.0434) | Acc_1: (63.59%) (25315/39808)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss_1: (1.0436) | Acc_1: (63.59%) (26129/41088)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss_1: (1.0424) | Acc_1: (63.63%) (26960/42368)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss_1: (1.0428) | Acc_1: (63.64%) (27779/43648)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss_1: (1.0415) | Acc_1: (63.69%) (28615/44928)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss_1: (1.0412) | Acc_1: (63.73%) (29449/46208)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss_1: (1.0409) | Acc_1: (63.73%) (30265/47488)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss_1: (1.0398) | Acc_1: (63.80%) (31113/48768)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss_1: (1.0387) | Acc_1: (63.87%) (31935/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8836) | Acc: (70.10%) (7010/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (1.0171) | Acc_1: (61.72%) (79/128)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (1.0030) | Acc_1: (64.20%) (904/1408)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (0.9865) | Acc_1: (65.07%) (1749/2688)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (0.9653) | Acc_1: (65.90%) (2615/3968)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss_1: (0.9699) | Acc_1: (65.97%) (3462/5248)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss_1: (0.9697) | Acc_1: (65.93%) (4304/6528)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss_1: (0.9571) | Acc_1: (66.55%) (5196/7808)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss_1: (0.9691) | Acc_1: (66.20%) (6016/9088)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss_1: (0.9700) | Acc_1: (66.05%) (6848/10368)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss_1: (0.9682) | Acc_1: (66.09%) (7698/11648)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss_1: (0.9694) | Acc_1: (65.99%) (8531/12928)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss_1: (0.9691) | Acc_1: (66.05%) (9385/14208)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss_1: (0.9725) | Acc_1: (65.98%) (10219/15488)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss_1: (0.9752) | Acc_1: (65.94%) (11056/16768)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss_1: (0.9740) | Acc_1: (65.92%) (11898/18048)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss_1: (0.9739) | Acc_1: (65.87%) (12731/19328)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss_1: (0.9746) | Acc_1: (65.83%) (13567/20608)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss_1: (0.9756) | Acc_1: (65.95%) (14435/21888)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss_1: (0.9737) | Acc_1: (66.06%) (15304/23168)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss_1: (0.9741) | Acc_1: (66.09%) (16157/24448)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss_1: (0.9754) | Acc_1: (66.10%) (17007/25728)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss_1: (0.9734) | Acc_1: (66.19%) (17876/27008)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss_1: (0.9735) | Acc_1: (66.25%) (18740/28288)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss_1: (0.9743) | Acc_1: (66.24%) (19586/29568)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss_1: (0.9728) | Acc_1: (66.28%) (20447/30848)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss_1: (0.9710) | Acc_1: (66.31%) (21303/32128)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss_1: (0.9720) | Acc_1: (66.30%) (22149/33408)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss_1: (0.9703) | Acc_1: (66.31%) (23001/34688)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss_1: (0.9710) | Acc_1: (66.27%) (23837/35968)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss_1: (0.9683) | Acc_1: (66.41%) (24735/37248)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss_1: (0.9669) | Acc_1: (66.46%) (25606/38528)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss_1: (0.9654) | Acc_1: (66.52%) (26480/39808)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss_1: (0.9651) | Acc_1: (66.54%) (27340/41088)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss_1: (0.9646) | Acc_1: (66.54%) (28190/42368)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss_1: (0.9635) | Acc_1: (66.51%) (29031/43648)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss_1: (0.9642) | Acc_1: (66.47%) (29862/44928)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss_1: (0.9642) | Acc_1: (66.43%) (30696/46208)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss_1: (0.9638) | Acc_1: (66.48%) (31570/47488)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss_1: (0.9640) | Acc_1: (66.52%) (32439/48768)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss_1: (0.9650) | Acc_1: (66.50%) (33248/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8498) | Acc: (71.01%) (7101/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (0.9530) | Acc_1: (63.28%) (81/128)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (0.9210) | Acc_1: (67.47%) (950/1408)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (0.9230) | Acc_1: (68.01%) (1828/2688)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (0.9137) | Acc_1: (68.40%) (2714/3968)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss_1: (0.9098) | Acc_1: (68.45%) (3592/5248)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss_1: (0.8962) | Acc_1: (68.67%) (4483/6528)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss_1: (0.9040) | Acc_1: (68.33%) (5335/7808)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss_1: (0.9095) | Acc_1: (68.34%) (6211/9088)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss_1: (0.9078) | Acc_1: (68.43%) (7095/10368)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss_1: (0.9054) | Acc_1: (68.47%) (7975/11648)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss_1: (0.9099) | Acc_1: (68.39%) (8841/12928)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss_1: (0.9105) | Acc_1: (68.31%) (9705/14208)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss_1: (0.9110) | Acc_1: (68.30%) (10578/15488)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss_1: (0.9126) | Acc_1: (68.18%) (11432/16768)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss_1: (0.9156) | Acc_1: (68.02%) (12276/18048)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss_1: (0.9162) | Acc_1: (68.04%) (13150/19328)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss_1: (0.9167) | Acc_1: (68.03%) (14019/20608)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss_1: (0.9196) | Acc_1: (67.95%) (14872/21888)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss_1: (0.9197) | Acc_1: (67.90%) (15732/23168)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss_1: (0.9163) | Acc_1: (68.02%) (16629/24448)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss_1: (0.9164) | Acc_1: (68.08%) (17515/25728)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss_1: (0.9161) | Acc_1: (68.06%) (18381/27008)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss_1: (0.9150) | Acc_1: (68.15%) (19278/28288)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss_1: (0.9139) | Acc_1: (68.22%) (20170/29568)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss_1: (0.9143) | Acc_1: (68.24%) (21050/30848)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss_1: (0.9133) | Acc_1: (68.29%) (21940/32128)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss_1: (0.9127) | Acc_1: (68.36%) (22837/33408)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss_1: (0.9128) | Acc_1: (68.34%) (23705/34688)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss_1: (0.9090) | Acc_1: (68.46%) (24625/35968)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss_1: (0.9094) | Acc_1: (68.51%) (25517/37248)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss_1: (0.9092) | Acc_1: (68.50%) (26391/38528)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss_1: (0.9086) | Acc_1: (68.56%) (27292/39808)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss_1: (0.9073) | Acc_1: (68.64%) (28202/41088)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss_1: (0.9067) | Acc_1: (68.68%) (29098/42368)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss_1: (0.9071) | Acc_1: (68.59%) (29936/43648)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss_1: (0.9051) | Acc_1: (68.65%) (30841/44928)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss_1: (0.9024) | Acc_1: (68.73%) (31758/46208)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss_1: (0.9025) | Acc_1: (68.78%) (32660/47488)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss_1: (0.9011) | Acc_1: (68.83%) (33567/48768)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss_1: (0.9011) | Acc_1: (68.83%) (34413/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8307) | Acc: (71.10%) (7110/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (0.9008) | Acc_1: (67.97%) (87/128)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (0.8443) | Acc_1: (71.24%) (1003/1408)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (0.8522) | Acc_1: (71.17%) (1913/2688)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.8468) | Acc_1: (71.27%) (2828/3968)\n",
      "Epoch: 8 | Batch_idx: 40 |  Loss_1: (0.8509) | Acc_1: (70.66%) (3708/5248)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss_1: (0.8598) | Acc_1: (70.39%) (4595/6528)\n",
      "Epoch: 8 | Batch_idx: 60 |  Loss_1: (0.8651) | Acc_1: (70.33%) (5491/7808)\n",
      "Epoch: 8 | Batch_idx: 70 |  Loss_1: (0.8615) | Acc_1: (70.37%) (6395/9088)\n",
      "Epoch: 8 | Batch_idx: 80 |  Loss_1: (0.8663) | Acc_1: (70.10%) (7268/10368)\n",
      "Epoch: 8 | Batch_idx: 90 |  Loss_1: (0.8675) | Acc_1: (70.05%) (8159/11648)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss_1: (0.8683) | Acc_1: (70.06%) (9057/12928)\n",
      "Epoch: 8 | Batch_idx: 110 |  Loss_1: (0.8702) | Acc_1: (70.00%) (9945/14208)\n",
      "Epoch: 8 | Batch_idx: 120 |  Loss_1: (0.8658) | Acc_1: (70.16%) (10867/15488)\n",
      "Epoch: 8 | Batch_idx: 130 |  Loss_1: (0.8656) | Acc_1: (70.09%) (11753/16768)\n",
      "Epoch: 8 | Batch_idx: 140 |  Loss_1: (0.8617) | Acc_1: (70.20%) (12669/18048)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss_1: (0.8599) | Acc_1: (70.30%) (13587/19328)\n",
      "Epoch: 8 | Batch_idx: 160 |  Loss_1: (0.8574) | Acc_1: (70.52%) (14533/20608)\n",
      "Epoch: 8 | Batch_idx: 170 |  Loss_1: (0.8569) | Acc_1: (70.50%) (15431/21888)\n",
      "Epoch: 8 | Batch_idx: 180 |  Loss_1: (0.8547) | Acc_1: (70.60%) (16356/23168)\n",
      "Epoch: 8 | Batch_idx: 190 |  Loss_1: (0.8533) | Acc_1: (70.62%) (17264/24448)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss_1: (0.8550) | Acc_1: (70.49%) (18136/25728)\n",
      "Epoch: 8 | Batch_idx: 210 |  Loss_1: (0.8531) | Acc_1: (70.60%) (19067/27008)\n",
      "Epoch: 8 | Batch_idx: 220 |  Loss_1: (0.8533) | Acc_1: (70.57%) (19962/28288)\n",
      "Epoch: 8 | Batch_idx: 230 |  Loss_1: (0.8553) | Acc_1: (70.51%) (20848/29568)\n",
      "Epoch: 8 | Batch_idx: 240 |  Loss_1: (0.8525) | Acc_1: (70.59%) (21776/30848)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss_1: (0.8506) | Acc_1: (70.67%) (22705/32128)\n",
      "Epoch: 8 | Batch_idx: 260 |  Loss_1: (0.8493) | Acc_1: (70.70%) (23619/33408)\n",
      "Epoch: 8 | Batch_idx: 270 |  Loss_1: (0.8498) | Acc_1: (70.66%) (24509/34688)\n",
      "Epoch: 8 | Batch_idx: 280 |  Loss_1: (0.8493) | Acc_1: (70.67%) (25417/35968)\n",
      "Epoch: 8 | Batch_idx: 290 |  Loss_1: (0.8497) | Acc_1: (70.60%) (26298/37248)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss_1: (0.8484) | Acc_1: (70.64%) (27217/38528)\n",
      "Epoch: 8 | Batch_idx: 310 |  Loss_1: (0.8483) | Acc_1: (70.64%) (28119/39808)\n",
      "Epoch: 8 | Batch_idx: 320 |  Loss_1: (0.8486) | Acc_1: (70.68%) (29039/41088)\n",
      "Epoch: 8 | Batch_idx: 330 |  Loss_1: (0.8501) | Acc_1: (70.64%) (29929/42368)\n",
      "Epoch: 8 | Batch_idx: 340 |  Loss_1: (0.8486) | Acc_1: (70.68%) (30849/43648)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss_1: (0.8490) | Acc_1: (70.69%) (31760/44928)\n",
      "Epoch: 8 | Batch_idx: 360 |  Loss_1: (0.8488) | Acc_1: (70.68%) (32660/46208)\n",
      "Epoch: 8 | Batch_idx: 370 |  Loss_1: (0.8466) | Acc_1: (70.76%) (33601/47488)\n",
      "Epoch: 8 | Batch_idx: 380 |  Loss_1: (0.8458) | Acc_1: (70.81%) (34535/48768)\n",
      "Epoch: 8 | Batch_idx: 390 |  Loss_1: (0.8444) | Acc_1: (70.86%) (35430/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8277) | Acc: (71.90%) (7190/10000)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (0.8021) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (0.8228) | Acc_1: (72.23%) (1017/1408)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (0.8196) | Acc_1: (71.69%) (1927/2688)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (0.8105) | Acc_1: (71.95%) (2855/3968)\n",
      "Epoch: 9 | Batch_idx: 40 |  Loss_1: (0.8081) | Acc_1: (71.53%) (3754/5248)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss_1: (0.8126) | Acc_1: (71.65%) (4677/6528)\n",
      "Epoch: 9 | Batch_idx: 60 |  Loss_1: (0.8068) | Acc_1: (71.77%) (5604/7808)\n",
      "Epoch: 9 | Batch_idx: 70 |  Loss_1: (0.7994) | Acc_1: (72.22%) (6563/9088)\n",
      "Epoch: 9 | Batch_idx: 80 |  Loss_1: (0.7986) | Acc_1: (72.18%) (7484/10368)\n",
      "Epoch: 9 | Batch_idx: 90 |  Loss_1: (0.7961) | Acc_1: (72.34%) (8426/11648)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss_1: (0.7952) | Acc_1: (72.32%) (9350/12928)\n",
      "Epoch: 9 | Batch_idx: 110 |  Loss_1: (0.7977) | Acc_1: (72.28%) (10270/14208)\n",
      "Epoch: 9 | Batch_idx: 120 |  Loss_1: (0.7962) | Acc_1: (72.22%) (11186/15488)\n",
      "Epoch: 9 | Batch_idx: 130 |  Loss_1: (0.7983) | Acc_1: (72.19%) (12105/16768)\n",
      "Epoch: 9 | Batch_idx: 140 |  Loss_1: (0.7982) | Acc_1: (72.22%) (13034/18048)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss_1: (0.7965) | Acc_1: (72.27%) (13969/19328)\n",
      "Epoch: 9 | Batch_idx: 160 |  Loss_1: (0.7941) | Acc_1: (72.37%) (14913/20608)\n",
      "Epoch: 9 | Batch_idx: 170 |  Loss_1: (0.7918) | Acc_1: (72.44%) (15855/21888)\n",
      "Epoch: 9 | Batch_idx: 180 |  Loss_1: (0.7941) | Acc_1: (72.41%) (16777/23168)\n",
      "Epoch: 9 | Batch_idx: 190 |  Loss_1: (0.7985) | Acc_1: (72.27%) (17669/24448)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss_1: (0.7984) | Acc_1: (72.31%) (18603/25728)\n",
      "Epoch: 9 | Batch_idx: 210 |  Loss_1: (0.7982) | Acc_1: (72.32%) (19533/27008)\n",
      "Epoch: 9 | Batch_idx: 220 |  Loss_1: (0.7997) | Acc_1: (72.30%) (20452/28288)\n",
      "Epoch: 9 | Batch_idx: 230 |  Loss_1: (0.7994) | Acc_1: (72.27%) (21369/29568)\n",
      "Epoch: 9 | Batch_idx: 240 |  Loss_1: (0.7982) | Acc_1: (72.33%) (22311/30848)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss_1: (0.7974) | Acc_1: (72.37%) (23251/32128)\n",
      "Epoch: 9 | Batch_idx: 260 |  Loss_1: (0.7982) | Acc_1: (72.39%) (24184/33408)\n",
      "Epoch: 9 | Batch_idx: 270 |  Loss_1: (0.7970) | Acc_1: (72.47%) (25138/34688)\n",
      "Epoch: 9 | Batch_idx: 280 |  Loss_1: (0.7949) | Acc_1: (72.52%) (26084/35968)\n",
      "Epoch: 9 | Batch_idx: 290 |  Loss_1: (0.7937) | Acc_1: (72.54%) (27020/37248)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss_1: (0.7931) | Acc_1: (72.56%) (27956/38528)\n",
      "Epoch: 9 | Batch_idx: 310 |  Loss_1: (0.7950) | Acc_1: (72.49%) (28858/39808)\n",
      "Epoch: 9 | Batch_idx: 320 |  Loss_1: (0.7943) | Acc_1: (72.53%) (29801/41088)\n",
      "Epoch: 9 | Batch_idx: 330 |  Loss_1: (0.7953) | Acc_1: (72.48%) (30710/42368)\n",
      "Epoch: 9 | Batch_idx: 340 |  Loss_1: (0.7958) | Acc_1: (72.47%) (31632/43648)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss_1: (0.7959) | Acc_1: (72.46%) (32555/44928)\n",
      "Epoch: 9 | Batch_idx: 360 |  Loss_1: (0.7946) | Acc_1: (72.55%) (33525/46208)\n",
      "Epoch: 9 | Batch_idx: 370 |  Loss_1: (0.7939) | Acc_1: (72.58%) (34469/47488)\n",
      "Epoch: 9 | Batch_idx: 380 |  Loss_1: (0.7943) | Acc_1: (72.60%) (35406/48768)\n",
      "Epoch: 9 | Batch_idx: 390 |  Loss_1: (0.7924) | Acc_1: (72.62%) (36312/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6012) | Acc: (79.01%) (7901/10000)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (0.5465) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.7472) | Acc_1: (74.43%) (1048/1408)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.7526) | Acc_1: (73.70%) (1981/2688)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.7483) | Acc_1: (74.70%) (2964/3968)\n",
      "Epoch: 10 | Batch_idx: 40 |  Loss_1: (0.7469) | Acc_1: (74.43%) (3906/5248)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss_1: (0.7459) | Acc_1: (74.56%) (4867/6528)\n",
      "Epoch: 10 | Batch_idx: 60 |  Loss_1: (0.7497) | Acc_1: (74.45%) (5813/7808)\n",
      "Epoch: 10 | Batch_idx: 70 |  Loss_1: (0.7558) | Acc_1: (74.06%) (6731/9088)\n",
      "Epoch: 10 | Batch_idx: 80 |  Loss_1: (0.7532) | Acc_1: (74.01%) (7673/10368)\n",
      "Epoch: 10 | Batch_idx: 90 |  Loss_1: (0.7507) | Acc_1: (74.08%) (8629/11648)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss_1: (0.7570) | Acc_1: (73.77%) (9537/12928)\n",
      "Epoch: 10 | Batch_idx: 110 |  Loss_1: (0.7578) | Acc_1: (73.87%) (10495/14208)\n",
      "Epoch: 10 | Batch_idx: 120 |  Loss_1: (0.7547) | Acc_1: (73.98%) (11458/15488)\n",
      "Epoch: 10 | Batch_idx: 130 |  Loss_1: (0.7546) | Acc_1: (73.99%) (12407/16768)\n",
      "Epoch: 10 | Batch_idx: 140 |  Loss_1: (0.7564) | Acc_1: (73.89%) (13335/18048)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss_1: (0.7565) | Acc_1: (73.89%) (14281/19328)\n",
      "Epoch: 10 | Batch_idx: 160 |  Loss_1: (0.7556) | Acc_1: (73.92%) (15234/20608)\n",
      "Epoch: 10 | Batch_idx: 170 |  Loss_1: (0.7526) | Acc_1: (74.07%) (16212/21888)\n",
      "Epoch: 10 | Batch_idx: 180 |  Loss_1: (0.7531) | Acc_1: (74.04%) (17154/23168)\n",
      "Epoch: 10 | Batch_idx: 190 |  Loss_1: (0.7518) | Acc_1: (74.17%) (18132/24448)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss_1: (0.7504) | Acc_1: (74.19%) (19087/25728)\n",
      "Epoch: 10 | Batch_idx: 210 |  Loss_1: (0.7492) | Acc_1: (74.21%) (20043/27008)\n",
      "Epoch: 10 | Batch_idx: 220 |  Loss_1: (0.7481) | Acc_1: (74.22%) (20996/28288)\n",
      "Epoch: 10 | Batch_idx: 230 |  Loss_1: (0.7490) | Acc_1: (74.12%) (21917/29568)\n",
      "Epoch: 10 | Batch_idx: 240 |  Loss_1: (0.7488) | Acc_1: (74.17%) (22880/30848)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss_1: (0.7487) | Acc_1: (74.20%) (23839/32128)\n",
      "Epoch: 10 | Batch_idx: 260 |  Loss_1: (0.7492) | Acc_1: (74.16%) (24775/33408)\n",
      "Epoch: 10 | Batch_idx: 270 |  Loss_1: (0.7506) | Acc_1: (74.08%) (25697/34688)\n",
      "Epoch: 10 | Batch_idx: 280 |  Loss_1: (0.7520) | Acc_1: (74.07%) (26641/35968)\n",
      "Epoch: 10 | Batch_idx: 290 |  Loss_1: (0.7514) | Acc_1: (74.11%) (27603/37248)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss_1: (0.7505) | Acc_1: (74.12%) (28557/38528)\n",
      "Epoch: 10 | Batch_idx: 310 |  Loss_1: (0.7511) | Acc_1: (74.12%) (29507/39808)\n",
      "Epoch: 10 | Batch_idx: 320 |  Loss_1: (0.7507) | Acc_1: (74.12%) (30456/41088)\n",
      "Epoch: 10 | Batch_idx: 330 |  Loss_1: (0.7511) | Acc_1: (74.10%) (31395/42368)\n",
      "Epoch: 10 | Batch_idx: 340 |  Loss_1: (0.7510) | Acc_1: (74.10%) (32345/43648)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss_1: (0.7506) | Acc_1: (74.11%) (33295/44928)\n",
      "Epoch: 10 | Batch_idx: 360 |  Loss_1: (0.7497) | Acc_1: (74.13%) (34255/46208)\n",
      "Epoch: 10 | Batch_idx: 370 |  Loss_1: (0.7501) | Acc_1: (74.10%) (35189/47488)\n",
      "Epoch: 10 | Batch_idx: 380 |  Loss_1: (0.7503) | Acc_1: (74.09%) (36131/48768)\n",
      "Epoch: 10 | Batch_idx: 390 |  Loss_1: (0.7504) | Acc_1: (74.05%) (37025/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6482) | Acc: (77.48%) (7748/10000)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.8354) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.7246) | Acc_1: (74.79%) (1053/1408)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.7261) | Acc_1: (75.07%) (2018/2688)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.7341) | Acc_1: (74.55%) (2958/3968)\n",
      "Epoch: 11 | Batch_idx: 40 |  Loss_1: (0.7258) | Acc_1: (74.98%) (3935/5248)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss_1: (0.7311) | Acc_1: (74.83%) (4885/6528)\n",
      "Epoch: 11 | Batch_idx: 60 |  Loss_1: (0.7253) | Acc_1: (75.12%) (5865/7808)\n",
      "Epoch: 11 | Batch_idx: 70 |  Loss_1: (0.7231) | Acc_1: (75.14%) (6829/9088)\n",
      "Epoch: 11 | Batch_idx: 80 |  Loss_1: (0.7221) | Acc_1: (75.14%) (7790/10368)\n",
      "Epoch: 11 | Batch_idx: 90 |  Loss_1: (0.7236) | Acc_1: (75.15%) (8753/11648)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss_1: (0.7251) | Acc_1: (75.04%) (9701/12928)\n",
      "Epoch: 11 | Batch_idx: 110 |  Loss_1: (0.7287) | Acc_1: (74.94%) (10648/14208)\n",
      "Epoch: 11 | Batch_idx: 120 |  Loss_1: (0.7297) | Acc_1: (74.92%) (11604/15488)\n",
      "Epoch: 11 | Batch_idx: 130 |  Loss_1: (0.7263) | Acc_1: (74.98%) (12573/16768)\n",
      "Epoch: 11 | Batch_idx: 140 |  Loss_1: (0.7244) | Acc_1: (75.08%) (13551/18048)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss_1: (0.7253) | Acc_1: (75.07%) (14509/19328)\n",
      "Epoch: 11 | Batch_idx: 160 |  Loss_1: (0.7262) | Acc_1: (75.04%) (15465/20608)\n",
      "Epoch: 11 | Batch_idx: 170 |  Loss_1: (0.7247) | Acc_1: (75.08%) (16434/21888)\n",
      "Epoch: 11 | Batch_idx: 180 |  Loss_1: (0.7222) | Acc_1: (75.10%) (17399/23168)\n",
      "Epoch: 11 | Batch_idx: 190 |  Loss_1: (0.7229) | Acc_1: (75.16%) (18374/24448)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss_1: (0.7252) | Acc_1: (75.16%) (19338/25728)\n",
      "Epoch: 11 | Batch_idx: 210 |  Loss_1: (0.7267) | Acc_1: (75.07%) (20274/27008)\n",
      "Epoch: 11 | Batch_idx: 220 |  Loss_1: (0.7262) | Acc_1: (75.08%) (21240/28288)\n",
      "Epoch: 11 | Batch_idx: 230 |  Loss_1: (0.7263) | Acc_1: (75.05%) (22192/29568)\n",
      "Epoch: 11 | Batch_idx: 240 |  Loss_1: (0.7254) | Acc_1: (75.12%) (23173/30848)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss_1: (0.7243) | Acc_1: (75.14%) (24142/32128)\n",
      "Epoch: 11 | Batch_idx: 260 |  Loss_1: (0.7236) | Acc_1: (75.11%) (25092/33408)\n",
      "Epoch: 11 | Batch_idx: 270 |  Loss_1: (0.7235) | Acc_1: (75.10%) (26049/34688)\n",
      "Epoch: 11 | Batch_idx: 280 |  Loss_1: (0.7226) | Acc_1: (75.13%) (27024/35968)\n",
      "Epoch: 11 | Batch_idx: 290 |  Loss_1: (0.7210) | Acc_1: (75.18%) (28003/37248)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss_1: (0.7199) | Acc_1: (75.27%) (29001/38528)\n",
      "Epoch: 11 | Batch_idx: 310 |  Loss_1: (0.7195) | Acc_1: (75.28%) (29967/39808)\n",
      "Epoch: 11 | Batch_idx: 320 |  Loss_1: (0.7197) | Acc_1: (75.28%) (30933/41088)\n",
      "Epoch: 11 | Batch_idx: 330 |  Loss_1: (0.7188) | Acc_1: (75.33%) (31917/42368)\n",
      "Epoch: 11 | Batch_idx: 340 |  Loss_1: (0.7188) | Acc_1: (75.35%) (32887/43648)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss_1: (0.7187) | Acc_1: (75.33%) (33843/44928)\n",
      "Epoch: 11 | Batch_idx: 360 |  Loss_1: (0.7174) | Acc_1: (75.38%) (34831/46208)\n",
      "Epoch: 11 | Batch_idx: 370 |  Loss_1: (0.7166) | Acc_1: (75.43%) (35820/47488)\n",
      "Epoch: 11 | Batch_idx: 380 |  Loss_1: (0.7156) | Acc_1: (75.46%) (36801/48768)\n",
      "Epoch: 11 | Batch_idx: 390 |  Loss_1: (0.7146) | Acc_1: (75.50%) (37751/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5871) | Acc: (79.61%) (7961/10000)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.8169) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.6656) | Acc_1: (77.49%) (1091/1408)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.6629) | Acc_1: (77.46%) (2082/2688)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.6566) | Acc_1: (77.80%) (3087/3968)\n",
      "Epoch: 12 | Batch_idx: 40 |  Loss_1: (0.6622) | Acc_1: (77.59%) (4072/5248)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss_1: (0.6561) | Acc_1: (77.76%) (5076/6528)\n",
      "Epoch: 12 | Batch_idx: 60 |  Loss_1: (0.6672) | Acc_1: (77.11%) (6021/7808)\n",
      "Epoch: 12 | Batch_idx: 70 |  Loss_1: (0.6682) | Acc_1: (77.17%) (7013/9088)\n",
      "Epoch: 12 | Batch_idx: 80 |  Loss_1: (0.6713) | Acc_1: (76.85%) (7968/10368)\n",
      "Epoch: 12 | Batch_idx: 90 |  Loss_1: (0.6775) | Acc_1: (76.57%) (8919/11648)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss_1: (0.6725) | Acc_1: (76.90%) (9941/12928)\n",
      "Epoch: 12 | Batch_idx: 110 |  Loss_1: (0.6676) | Acc_1: (77.09%) (10953/14208)\n",
      "Epoch: 12 | Batch_idx: 120 |  Loss_1: (0.6666) | Acc_1: (77.12%) (11944/15488)\n",
      "Epoch: 12 | Batch_idx: 130 |  Loss_1: (0.6696) | Acc_1: (77.03%) (12916/16768)\n",
      "Epoch: 12 | Batch_idx: 140 |  Loss_1: (0.6712) | Acc_1: (77.01%) (13899/18048)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss_1: (0.6746) | Acc_1: (76.92%) (14868/19328)\n",
      "Epoch: 12 | Batch_idx: 160 |  Loss_1: (0.6794) | Acc_1: (76.78%) (15823/20608)\n",
      "Epoch: 12 | Batch_idx: 170 |  Loss_1: (0.6776) | Acc_1: (76.84%) (16818/21888)\n",
      "Epoch: 12 | Batch_idx: 180 |  Loss_1: (0.6752) | Acc_1: (76.86%) (17806/23168)\n",
      "Epoch: 12 | Batch_idx: 190 |  Loss_1: (0.6753) | Acc_1: (76.82%) (18780/24448)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss_1: (0.6757) | Acc_1: (76.78%) (19755/25728)\n",
      "Epoch: 12 | Batch_idx: 210 |  Loss_1: (0.6786) | Acc_1: (76.72%) (20720/27008)\n",
      "Epoch: 12 | Batch_idx: 220 |  Loss_1: (0.6794) | Acc_1: (76.65%) (21682/28288)\n",
      "Epoch: 12 | Batch_idx: 230 |  Loss_1: (0.6781) | Acc_1: (76.76%) (22695/29568)\n",
      "Epoch: 12 | Batch_idx: 240 |  Loss_1: (0.6763) | Acc_1: (76.82%) (23696/30848)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss_1: (0.6747) | Acc_1: (76.78%) (24668/32128)\n",
      "Epoch: 12 | Batch_idx: 260 |  Loss_1: (0.6754) | Acc_1: (76.70%) (25625/33408)\n",
      "Epoch: 12 | Batch_idx: 270 |  Loss_1: (0.6734) | Acc_1: (76.76%) (26625/34688)\n",
      "Epoch: 12 | Batch_idx: 280 |  Loss_1: (0.6729) | Acc_1: (76.80%) (27622/35968)\n",
      "Epoch: 12 | Batch_idx: 290 |  Loss_1: (0.6732) | Acc_1: (76.79%) (28601/37248)\n",
      "Epoch: 12 | Batch_idx: 300 |  Loss_1: (0.6732) | Acc_1: (76.79%) (29587/38528)\n",
      "Epoch: 12 | Batch_idx: 310 |  Loss_1: (0.6726) | Acc_1: (76.83%) (30583/39808)\n",
      "Epoch: 12 | Batch_idx: 320 |  Loss_1: (0.6735) | Acc_1: (76.81%) (31558/41088)\n",
      "Epoch: 12 | Batch_idx: 330 |  Loss_1: (0.6746) | Acc_1: (76.74%) (32515/42368)\n",
      "Epoch: 12 | Batch_idx: 340 |  Loss_1: (0.6747) | Acc_1: (76.72%) (33488/43648)\n",
      "Epoch: 12 | Batch_idx: 350 |  Loss_1: (0.6762) | Acc_1: (76.68%) (34452/44928)\n",
      "Epoch: 12 | Batch_idx: 360 |  Loss_1: (0.6760) | Acc_1: (76.70%) (35440/46208)\n",
      "Epoch: 12 | Batch_idx: 370 |  Loss_1: (0.6767) | Acc_1: (76.67%) (36407/47488)\n",
      "Epoch: 12 | Batch_idx: 380 |  Loss_1: (0.6767) | Acc_1: (76.69%) (37398/48768)\n",
      "Epoch: 12 | Batch_idx: 390 |  Loss_1: (0.6767) | Acc_1: (76.69%) (38347/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5940) | Acc: (79.73%) (7973/10000)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.6482) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.6533) | Acc_1: (78.34%) (1103/1408)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.6188) | Acc_1: (79.09%) (2126/2688)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.6179) | Acc_1: (78.88%) (3130/3968)\n",
      "Epoch: 13 | Batch_idx: 40 |  Loss_1: (0.6270) | Acc_1: (78.62%) (4126/5248)\n",
      "Epoch: 13 | Batch_idx: 50 |  Loss_1: (0.6441) | Acc_1: (78.11%) (5099/6528)\n",
      "Epoch: 13 | Batch_idx: 60 |  Loss_1: (0.6462) | Acc_1: (77.80%) (6075/7808)\n",
      "Epoch: 13 | Batch_idx: 70 |  Loss_1: (0.6542) | Acc_1: (77.42%) (7036/9088)\n",
      "Epoch: 13 | Batch_idx: 80 |  Loss_1: (0.6534) | Acc_1: (77.47%) (8032/10368)\n",
      "Epoch: 13 | Batch_idx: 90 |  Loss_1: (0.6497) | Acc_1: (77.54%) (9032/11648)\n",
      "Epoch: 13 | Batch_idx: 100 |  Loss_1: (0.6497) | Acc_1: (77.61%) (10034/12928)\n",
      "Epoch: 13 | Batch_idx: 110 |  Loss_1: (0.6465) | Acc_1: (77.71%) (11041/14208)\n",
      "Epoch: 13 | Batch_idx: 120 |  Loss_1: (0.6510) | Acc_1: (77.60%) (12019/15488)\n",
      "Epoch: 13 | Batch_idx: 130 |  Loss_1: (0.6536) | Acc_1: (77.58%) (13009/16768)\n",
      "Epoch: 13 | Batch_idx: 140 |  Loss_1: (0.6549) | Acc_1: (77.52%) (13991/18048)\n",
      "Epoch: 13 | Batch_idx: 150 |  Loss_1: (0.6519) | Acc_1: (77.64%) (15007/19328)\n",
      "Epoch: 13 | Batch_idx: 160 |  Loss_1: (0.6489) | Acc_1: (77.72%) (16016/20608)\n",
      "Epoch: 13 | Batch_idx: 170 |  Loss_1: (0.6474) | Acc_1: (77.77%) (17023/21888)\n",
      "Epoch: 13 | Batch_idx: 180 |  Loss_1: (0.6493) | Acc_1: (77.75%) (18013/23168)\n",
      "Epoch: 13 | Batch_idx: 190 |  Loss_1: (0.6491) | Acc_1: (77.73%) (19004/24448)\n",
      "Epoch: 13 | Batch_idx: 200 |  Loss_1: (0.6493) | Acc_1: (77.76%) (20007/25728)\n",
      "Epoch: 13 | Batch_idx: 210 |  Loss_1: (0.6487) | Acc_1: (77.76%) (21002/27008)\n",
      "Epoch: 13 | Batch_idx: 220 |  Loss_1: (0.6491) | Acc_1: (77.72%) (21985/28288)\n",
      "Epoch: 13 | Batch_idx: 230 |  Loss_1: (0.6488) | Acc_1: (77.63%) (22955/29568)\n",
      "Epoch: 13 | Batch_idx: 240 |  Loss_1: (0.6469) | Acc_1: (77.71%) (23973/30848)\n",
      "Epoch: 13 | Batch_idx: 250 |  Loss_1: (0.6476) | Acc_1: (77.71%) (24967/32128)\n",
      "Epoch: 13 | Batch_idx: 260 |  Loss_1: (0.6458) | Acc_1: (77.78%) (25984/33408)\n",
      "Epoch: 13 | Batch_idx: 270 |  Loss_1: (0.6464) | Acc_1: (77.75%) (26969/34688)\n",
      "Epoch: 13 | Batch_idx: 280 |  Loss_1: (0.6463) | Acc_1: (77.79%) (27981/35968)\n",
      "Epoch: 13 | Batch_idx: 290 |  Loss_1: (0.6451) | Acc_1: (77.83%) (28990/37248)\n",
      "Epoch: 13 | Batch_idx: 300 |  Loss_1: (0.6445) | Acc_1: (77.87%) (30003/38528)\n",
      "Epoch: 13 | Batch_idx: 310 |  Loss_1: (0.6439) | Acc_1: (77.89%) (31005/39808)\n",
      "Epoch: 13 | Batch_idx: 320 |  Loss_1: (0.6439) | Acc_1: (77.91%) (32013/41088)\n",
      "Epoch: 13 | Batch_idx: 330 |  Loss_1: (0.6450) | Acc_1: (77.94%) (33020/42368)\n",
      "Epoch: 13 | Batch_idx: 340 |  Loss_1: (0.6440) | Acc_1: (78.00%) (34045/43648)\n",
      "Epoch: 13 | Batch_idx: 350 |  Loss_1: (0.6438) | Acc_1: (77.99%) (35041/44928)\n",
      "Epoch: 13 | Batch_idx: 360 |  Loss_1: (0.6450) | Acc_1: (77.98%) (36032/46208)\n",
      "Epoch: 13 | Batch_idx: 370 |  Loss_1: (0.6455) | Acc_1: (77.91%) (36999/47488)\n",
      "Epoch: 13 | Batch_idx: 380 |  Loss_1: (0.6457) | Acc_1: (77.90%) (37991/48768)\n",
      "Epoch: 13 | Batch_idx: 390 |  Loss_1: (0.6458) | Acc_1: (77.88%) (38938/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5645) | Acc: (80.90%) (8090/10000)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.5418) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.6540) | Acc_1: (76.85%) (1082/1408)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.6388) | Acc_1: (77.38%) (2080/2688)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.6330) | Acc_1: (78.15%) (3101/3968)\n",
      "Epoch: 14 | Batch_idx: 40 |  Loss_1: (0.6355) | Acc_1: (78.14%) (4101/5248)\n",
      "Epoch: 14 | Batch_idx: 50 |  Loss_1: (0.6355) | Acc_1: (78.20%) (5105/6528)\n",
      "Epoch: 14 | Batch_idx: 60 |  Loss_1: (0.6274) | Acc_1: (78.50%) (6129/7808)\n",
      "Epoch: 14 | Batch_idx: 70 |  Loss_1: (0.6256) | Acc_1: (78.64%) (7147/9088)\n",
      "Epoch: 14 | Batch_idx: 80 |  Loss_1: (0.6245) | Acc_1: (78.66%) (8155/10368)\n",
      "Epoch: 14 | Batch_idx: 90 |  Loss_1: (0.6254) | Acc_1: (78.61%) (9156/11648)\n",
      "Epoch: 14 | Batch_idx: 100 |  Loss_1: (0.6285) | Acc_1: (78.50%) (10149/12928)\n",
      "Epoch: 14 | Batch_idx: 110 |  Loss_1: (0.6214) | Acc_1: (78.77%) (11191/14208)\n",
      "Epoch: 14 | Batch_idx: 120 |  Loss_1: (0.6288) | Acc_1: (78.56%) (12167/15488)\n",
      "Epoch: 14 | Batch_idx: 130 |  Loss_1: (0.6282) | Acc_1: (78.57%) (13175/16768)\n",
      "Epoch: 14 | Batch_idx: 140 |  Loss_1: (0.6274) | Acc_1: (78.54%) (14174/18048)\n",
      "Epoch: 14 | Batch_idx: 150 |  Loss_1: (0.6278) | Acc_1: (78.53%) (15179/19328)\n",
      "Epoch: 14 | Batch_idx: 160 |  Loss_1: (0.6238) | Acc_1: (78.59%) (16195/20608)\n",
      "Epoch: 14 | Batch_idx: 170 |  Loss_1: (0.6231) | Acc_1: (78.62%) (17209/21888)\n",
      "Epoch: 14 | Batch_idx: 180 |  Loss_1: (0.6231) | Acc_1: (78.60%) (18209/23168)\n",
      "Epoch: 14 | Batch_idx: 190 |  Loss_1: (0.6230) | Acc_1: (78.62%) (19220/24448)\n",
      "Epoch: 14 | Batch_idx: 200 |  Loss_1: (0.6204) | Acc_1: (78.73%) (20255/25728)\n",
      "Epoch: 14 | Batch_idx: 210 |  Loss_1: (0.6205) | Acc_1: (78.76%) (21272/27008)\n",
      "Epoch: 14 | Batch_idx: 220 |  Loss_1: (0.6229) | Acc_1: (78.72%) (22267/28288)\n",
      "Epoch: 14 | Batch_idx: 230 |  Loss_1: (0.6224) | Acc_1: (78.76%) (23289/29568)\n",
      "Epoch: 14 | Batch_idx: 240 |  Loss_1: (0.6210) | Acc_1: (78.83%) (24318/30848)\n",
      "Epoch: 14 | Batch_idx: 250 |  Loss_1: (0.6204) | Acc_1: (78.83%) (25328/32128)\n",
      "Epoch: 14 | Batch_idx: 260 |  Loss_1: (0.6212) | Acc_1: (78.79%) (26322/33408)\n",
      "Epoch: 14 | Batch_idx: 270 |  Loss_1: (0.6201) | Acc_1: (78.83%) (27344/34688)\n",
      "Epoch: 14 | Batch_idx: 280 |  Loss_1: (0.6197) | Acc_1: (78.85%) (28361/35968)\n",
      "Epoch: 14 | Batch_idx: 290 |  Loss_1: (0.6171) | Acc_1: (78.99%) (29422/37248)\n",
      "Epoch: 14 | Batch_idx: 300 |  Loss_1: (0.6154) | Acc_1: (79.05%) (30458/38528)\n",
      "Epoch: 14 | Batch_idx: 310 |  Loss_1: (0.6141) | Acc_1: (79.13%) (31500/39808)\n",
      "Epoch: 14 | Batch_idx: 320 |  Loss_1: (0.6145) | Acc_1: (79.10%) (32501/41088)\n",
      "Epoch: 14 | Batch_idx: 330 |  Loss_1: (0.6163) | Acc_1: (79.05%) (33491/42368)\n",
      "Epoch: 14 | Batch_idx: 340 |  Loss_1: (0.6165) | Acc_1: (79.01%) (34488/43648)\n",
      "Epoch: 14 | Batch_idx: 350 |  Loss_1: (0.6170) | Acc_1: (78.99%) (35489/44928)\n",
      "Epoch: 14 | Batch_idx: 360 |  Loss_1: (0.6178) | Acc_1: (78.94%) (36477/46208)\n",
      "Epoch: 14 | Batch_idx: 370 |  Loss_1: (0.6188) | Acc_1: (78.88%) (37459/47488)\n",
      "Epoch: 14 | Batch_idx: 380 |  Loss_1: (0.6182) | Acc_1: (78.90%) (38480/48768)\n",
      "Epoch: 14 | Batch_idx: 390 |  Loss_1: (0.6188) | Acc_1: (78.89%) (39447/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4965) | Acc: (82.62%) (8262/10000)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.6066) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.6087) | Acc_1: (79.33%) (1117/1408)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.6245) | Acc_1: (78.76%) (2117/2688)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.6203) | Acc_1: (79.06%) (3137/3968)\n",
      "Epoch: 15 | Batch_idx: 40 |  Loss_1: (0.6111) | Acc_1: (79.21%) (4157/5248)\n",
      "Epoch: 15 | Batch_idx: 50 |  Loss_1: (0.6069) | Acc_1: (79.47%) (5188/6528)\n",
      "Epoch: 15 | Batch_idx: 60 |  Loss_1: (0.6052) | Acc_1: (79.43%) (6202/7808)\n",
      "Epoch: 15 | Batch_idx: 70 |  Loss_1: (0.6088) | Acc_1: (79.40%) (7216/9088)\n",
      "Epoch: 15 | Batch_idx: 80 |  Loss_1: (0.6059) | Acc_1: (79.51%) (8244/10368)\n",
      "Epoch: 15 | Batch_idx: 90 |  Loss_1: (0.6060) | Acc_1: (79.57%) (9268/11648)\n",
      "Epoch: 15 | Batch_idx: 100 |  Loss_1: (0.6073) | Acc_1: (79.51%) (10279/12928)\n",
      "Epoch: 15 | Batch_idx: 110 |  Loss_1: (0.6088) | Acc_1: (79.44%) (11287/14208)\n",
      "Epoch: 15 | Batch_idx: 120 |  Loss_1: (0.6113) | Acc_1: (79.24%) (12273/15488)\n",
      "Epoch: 15 | Batch_idx: 130 |  Loss_1: (0.6116) | Acc_1: (79.23%) (13285/16768)\n",
      "Epoch: 15 | Batch_idx: 140 |  Loss_1: (0.6100) | Acc_1: (79.27%) (14306/18048)\n",
      "Epoch: 15 | Batch_idx: 150 |  Loss_1: (0.6073) | Acc_1: (79.33%) (15332/19328)\n",
      "Epoch: 15 | Batch_idx: 160 |  Loss_1: (0.6076) | Acc_1: (79.33%) (16349/20608)\n",
      "Epoch: 15 | Batch_idx: 170 |  Loss_1: (0.6031) | Acc_1: (79.50%) (17401/21888)\n",
      "Epoch: 15 | Batch_idx: 180 |  Loss_1: (0.6019) | Acc_1: (79.56%) (18433/23168)\n",
      "Epoch: 15 | Batch_idx: 190 |  Loss_1: (0.6005) | Acc_1: (79.62%) (19465/24448)\n",
      "Epoch: 15 | Batch_idx: 200 |  Loss_1: (0.5998) | Acc_1: (79.62%) (20485/25728)\n",
      "Epoch: 15 | Batch_idx: 210 |  Loss_1: (0.5995) | Acc_1: (79.62%) (21505/27008)\n",
      "Epoch: 15 | Batch_idx: 220 |  Loss_1: (0.5980) | Acc_1: (79.71%) (22548/28288)\n",
      "Epoch: 15 | Batch_idx: 230 |  Loss_1: (0.5963) | Acc_1: (79.75%) (23581/29568)\n",
      "Epoch: 15 | Batch_idx: 240 |  Loss_1: (0.5958) | Acc_1: (79.76%) (24605/30848)\n",
      "Epoch: 15 | Batch_idx: 250 |  Loss_1: (0.5956) | Acc_1: (79.77%) (25627/32128)\n",
      "Epoch: 15 | Batch_idx: 260 |  Loss_1: (0.5957) | Acc_1: (79.74%) (26639/33408)\n",
      "Epoch: 15 | Batch_idx: 270 |  Loss_1: (0.5964) | Acc_1: (79.73%) (27657/34688)\n",
      "Epoch: 15 | Batch_idx: 280 |  Loss_1: (0.5968) | Acc_1: (79.71%) (28670/35968)\n",
      "Epoch: 15 | Batch_idx: 290 |  Loss_1: (0.5970) | Acc_1: (79.70%) (29688/37248)\n",
      "Epoch: 15 | Batch_idx: 300 |  Loss_1: (0.5966) | Acc_1: (79.71%) (30712/38528)\n",
      "Epoch: 15 | Batch_idx: 310 |  Loss_1: (0.5968) | Acc_1: (79.71%) (31732/39808)\n",
      "Epoch: 15 | Batch_idx: 320 |  Loss_1: (0.5972) | Acc_1: (79.70%) (32747/41088)\n",
      "Epoch: 15 | Batch_idx: 330 |  Loss_1: (0.5978) | Acc_1: (79.68%) (33757/42368)\n",
      "Epoch: 15 | Batch_idx: 340 |  Loss_1: (0.5966) | Acc_1: (79.69%) (34783/43648)\n",
      "Epoch: 15 | Batch_idx: 350 |  Loss_1: (0.5959) | Acc_1: (79.71%) (35813/44928)\n",
      "Epoch: 15 | Batch_idx: 360 |  Loss_1: (0.5937) | Acc_1: (79.77%) (36862/46208)\n",
      "Epoch: 15 | Batch_idx: 370 |  Loss_1: (0.5945) | Acc_1: (79.74%) (37867/47488)\n",
      "Epoch: 15 | Batch_idx: 380 |  Loss_1: (0.5952) | Acc_1: (79.73%) (38882/48768)\n",
      "Epoch: 15 | Batch_idx: 390 |  Loss_1: (0.5943) | Acc_1: (79.76%) (39882/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5224) | Acc: (82.21%) (8221/10000)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.5956) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.5626) | Acc_1: (80.40%) (1132/1408)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.5730) | Acc_1: (80.21%) (2156/2688)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.5771) | Acc_1: (80.22%) (3183/3968)\n",
      "Epoch: 16 | Batch_idx: 40 |  Loss_1: (0.5685) | Acc_1: (80.51%) (4225/5248)\n",
      "Epoch: 16 | Batch_idx: 50 |  Loss_1: (0.5693) | Acc_1: (80.62%) (5263/6528)\n",
      "Epoch: 16 | Batch_idx: 60 |  Loss_1: (0.5682) | Acc_1: (80.66%) (6298/7808)\n",
      "Epoch: 16 | Batch_idx: 70 |  Loss_1: (0.5748) | Acc_1: (80.40%) (7307/9088)\n",
      "Epoch: 16 | Batch_idx: 80 |  Loss_1: (0.5723) | Acc_1: (80.46%) (8342/10368)\n",
      "Epoch: 16 | Batch_idx: 90 |  Loss_1: (0.5720) | Acc_1: (80.55%) (9383/11648)\n",
      "Epoch: 16 | Batch_idx: 100 |  Loss_1: (0.5723) | Acc_1: (80.50%) (10407/12928)\n",
      "Epoch: 16 | Batch_idx: 110 |  Loss_1: (0.5734) | Acc_1: (80.35%) (11416/14208)\n",
      "Epoch: 16 | Batch_idx: 120 |  Loss_1: (0.5749) | Acc_1: (80.32%) (12440/15488)\n",
      "Epoch: 16 | Batch_idx: 130 |  Loss_1: (0.5707) | Acc_1: (80.41%) (13483/16768)\n",
      "Epoch: 16 | Batch_idx: 140 |  Loss_1: (0.5710) | Acc_1: (80.46%) (14522/18048)\n",
      "Epoch: 16 | Batch_idx: 150 |  Loss_1: (0.5667) | Acc_1: (80.60%) (15578/19328)\n",
      "Epoch: 16 | Batch_idx: 160 |  Loss_1: (0.5656) | Acc_1: (80.58%) (16606/20608)\n",
      "Epoch: 16 | Batch_idx: 170 |  Loss_1: (0.5649) | Acc_1: (80.60%) (17641/21888)\n",
      "Epoch: 16 | Batch_idx: 180 |  Loss_1: (0.5642) | Acc_1: (80.66%) (18687/23168)\n",
      "Epoch: 16 | Batch_idx: 190 |  Loss_1: (0.5649) | Acc_1: (80.56%) (19696/24448)\n",
      "Epoch: 16 | Batch_idx: 200 |  Loss_1: (0.5680) | Acc_1: (80.45%) (20699/25728)\n",
      "Epoch: 16 | Batch_idx: 210 |  Loss_1: (0.5691) | Acc_1: (80.45%) (21727/27008)\n",
      "Epoch: 16 | Batch_idx: 220 |  Loss_1: (0.5699) | Acc_1: (80.46%) (22760/28288)\n",
      "Epoch: 16 | Batch_idx: 230 |  Loss_1: (0.5692) | Acc_1: (80.51%) (23806/29568)\n",
      "Epoch: 16 | Batch_idx: 240 |  Loss_1: (0.5674) | Acc_1: (80.58%) (24856/30848)\n",
      "Epoch: 16 | Batch_idx: 250 |  Loss_1: (0.5654) | Acc_1: (80.64%) (25907/32128)\n",
      "Epoch: 16 | Batch_idx: 260 |  Loss_1: (0.5657) | Acc_1: (80.64%) (26940/33408)\n",
      "Epoch: 16 | Batch_idx: 270 |  Loss_1: (0.5661) | Acc_1: (80.62%) (27965/34688)\n",
      "Epoch: 16 | Batch_idx: 280 |  Loss_1: (0.5671) | Acc_1: (80.58%) (28983/35968)\n",
      "Epoch: 16 | Batch_idx: 290 |  Loss_1: (0.5687) | Acc_1: (80.51%) (29987/37248)\n",
      "Epoch: 16 | Batch_idx: 300 |  Loss_1: (0.5698) | Acc_1: (80.45%) (30994/38528)\n",
      "Epoch: 16 | Batch_idx: 310 |  Loss_1: (0.5708) | Acc_1: (80.41%) (32010/39808)\n",
      "Epoch: 16 | Batch_idx: 320 |  Loss_1: (0.5717) | Acc_1: (80.38%) (33026/41088)\n",
      "Epoch: 16 | Batch_idx: 330 |  Loss_1: (0.5714) | Acc_1: (80.38%) (34057/42368)\n",
      "Epoch: 16 | Batch_idx: 340 |  Loss_1: (0.5715) | Acc_1: (80.40%) (35093/43648)\n",
      "Epoch: 16 | Batch_idx: 350 |  Loss_1: (0.5726) | Acc_1: (80.34%) (36097/44928)\n",
      "Epoch: 16 | Batch_idx: 360 |  Loss_1: (0.5721) | Acc_1: (80.36%) (37131/46208)\n",
      "Epoch: 16 | Batch_idx: 370 |  Loss_1: (0.5713) | Acc_1: (80.39%) (38177/47488)\n",
      "Epoch: 16 | Batch_idx: 380 |  Loss_1: (0.5709) | Acc_1: (80.40%) (39208/48768)\n",
      "Epoch: 16 | Batch_idx: 390 |  Loss_1: (0.5707) | Acc_1: (80.39%) (40197/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5250) | Acc: (82.14%) (8214/10000)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.5601) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.5894) | Acc_1: (80.75%) (1137/1408)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.5650) | Acc_1: (80.80%) (2172/2688)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.5607) | Acc_1: (80.65%) (3200/3968)\n",
      "Epoch: 17 | Batch_idx: 40 |  Loss_1: (0.5716) | Acc_1: (80.09%) (4203/5248)\n",
      "Epoch: 17 | Batch_idx: 50 |  Loss_1: (0.5738) | Acc_1: (79.96%) (5220/6528)\n",
      "Epoch: 17 | Batch_idx: 60 |  Loss_1: (0.5761) | Acc_1: (80.03%) (6249/7808)\n",
      "Epoch: 17 | Batch_idx: 70 |  Loss_1: (0.5719) | Acc_1: (80.24%) (7292/9088)\n",
      "Epoch: 17 | Batch_idx: 80 |  Loss_1: (0.5711) | Acc_1: (80.23%) (8318/10368)\n",
      "Epoch: 17 | Batch_idx: 90 |  Loss_1: (0.5614) | Acc_1: (80.67%) (9397/11648)\n",
      "Epoch: 17 | Batch_idx: 100 |  Loss_1: (0.5559) | Acc_1: (80.87%) (10455/12928)\n",
      "Epoch: 17 | Batch_idx: 110 |  Loss_1: (0.5566) | Acc_1: (80.91%) (11495/14208)\n",
      "Epoch: 17 | Batch_idx: 120 |  Loss_1: (0.5538) | Acc_1: (80.95%) (12537/15488)\n",
      "Epoch: 17 | Batch_idx: 130 |  Loss_1: (0.5513) | Acc_1: (80.95%) (13573/16768)\n",
      "Epoch: 17 | Batch_idx: 140 |  Loss_1: (0.5522) | Acc_1: (80.90%) (14600/18048)\n",
      "Epoch: 17 | Batch_idx: 150 |  Loss_1: (0.5482) | Acc_1: (81.03%) (15661/19328)\n",
      "Epoch: 17 | Batch_idx: 160 |  Loss_1: (0.5454) | Acc_1: (81.10%) (16714/20608)\n",
      "Epoch: 17 | Batch_idx: 170 |  Loss_1: (0.5481) | Acc_1: (81.04%) (17738/21888)\n",
      "Epoch: 17 | Batch_idx: 180 |  Loss_1: (0.5505) | Acc_1: (80.97%) (18760/23168)\n",
      "Epoch: 17 | Batch_idx: 190 |  Loss_1: (0.5484) | Acc_1: (81.09%) (19825/24448)\n",
      "Epoch: 17 | Batch_idx: 200 |  Loss_1: (0.5504) | Acc_1: (81.06%) (20854/25728)\n",
      "Epoch: 17 | Batch_idx: 210 |  Loss_1: (0.5507) | Acc_1: (80.98%) (21870/27008)\n",
      "Epoch: 17 | Batch_idx: 220 |  Loss_1: (0.5525) | Acc_1: (80.92%) (22892/28288)\n",
      "Epoch: 17 | Batch_idx: 230 |  Loss_1: (0.5531) | Acc_1: (80.90%) (23920/29568)\n",
      "Epoch: 17 | Batch_idx: 240 |  Loss_1: (0.5553) | Acc_1: (80.83%) (24933/30848)\n",
      "Epoch: 17 | Batch_idx: 250 |  Loss_1: (0.5573) | Acc_1: (80.75%) (25944/32128)\n",
      "Epoch: 17 | Batch_idx: 260 |  Loss_1: (0.5557) | Acc_1: (80.83%) (27005/33408)\n",
      "Epoch: 17 | Batch_idx: 270 |  Loss_1: (0.5573) | Acc_1: (80.80%) (28028/34688)\n",
      "Epoch: 17 | Batch_idx: 280 |  Loss_1: (0.5566) | Acc_1: (80.84%) (29078/35968)\n",
      "Epoch: 17 | Batch_idx: 290 |  Loss_1: (0.5556) | Acc_1: (80.85%) (30116/37248)\n",
      "Epoch: 17 | Batch_idx: 300 |  Loss_1: (0.5558) | Acc_1: (80.83%) (31142/38528)\n",
      "Epoch: 17 | Batch_idx: 310 |  Loss_1: (0.5575) | Acc_1: (80.75%) (32146/39808)\n",
      "Epoch: 17 | Batch_idx: 320 |  Loss_1: (0.5571) | Acc_1: (80.81%) (33205/41088)\n",
      "Epoch: 17 | Batch_idx: 330 |  Loss_1: (0.5574) | Acc_1: (80.80%) (34233/42368)\n",
      "Epoch: 17 | Batch_idx: 340 |  Loss_1: (0.5569) | Acc_1: (80.82%) (35275/43648)\n",
      "Epoch: 17 | Batch_idx: 350 |  Loss_1: (0.5560) | Acc_1: (80.83%) (36315/44928)\n",
      "Epoch: 17 | Batch_idx: 360 |  Loss_1: (0.5561) | Acc_1: (80.82%) (37347/46208)\n",
      "Epoch: 17 | Batch_idx: 370 |  Loss_1: (0.5561) | Acc_1: (80.85%) (38395/47488)\n",
      "Epoch: 17 | Batch_idx: 380 |  Loss_1: (0.5547) | Acc_1: (80.89%) (39448/48768)\n",
      "Epoch: 17 | Batch_idx: 390 |  Loss_1: (0.5574) | Acc_1: (80.83%) (40417/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4718) | Acc: (83.84%) (8384/10000)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.4295) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.5387) | Acc_1: (81.18%) (1143/1408)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.5343) | Acc_1: (81.14%) (2181/2688)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.5374) | Acc_1: (81.05%) (3216/3968)\n",
      "Epoch: 18 | Batch_idx: 40 |  Loss_1: (0.5389) | Acc_1: (81.14%) (4258/5248)\n",
      "Epoch: 18 | Batch_idx: 50 |  Loss_1: (0.5419) | Acc_1: (81.20%) (5301/6528)\n",
      "Epoch: 18 | Batch_idx: 60 |  Loss_1: (0.5398) | Acc_1: (81.38%) (6354/7808)\n",
      "Epoch: 18 | Batch_idx: 70 |  Loss_1: (0.5478) | Acc_1: (81.01%) (7362/9088)\n",
      "Epoch: 18 | Batch_idx: 80 |  Loss_1: (0.5448) | Acc_1: (81.07%) (8405/10368)\n",
      "Epoch: 18 | Batch_idx: 90 |  Loss_1: (0.5480) | Acc_1: (80.86%) (9418/11648)\n",
      "Epoch: 18 | Batch_idx: 100 |  Loss_1: (0.5460) | Acc_1: (80.93%) (10462/12928)\n",
      "Epoch: 18 | Batch_idx: 110 |  Loss_1: (0.5440) | Acc_1: (81.11%) (11524/14208)\n",
      "Epoch: 18 | Batch_idx: 120 |  Loss_1: (0.5436) | Acc_1: (81.14%) (12567/15488)\n",
      "Epoch: 18 | Batch_idx: 130 |  Loss_1: (0.5453) | Acc_1: (81.11%) (13601/16768)\n",
      "Epoch: 18 | Batch_idx: 140 |  Loss_1: (0.5441) | Acc_1: (81.19%) (14653/18048)\n",
      "Epoch: 18 | Batch_idx: 150 |  Loss_1: (0.5424) | Acc_1: (81.28%) (15709/19328)\n",
      "Epoch: 18 | Batch_idx: 160 |  Loss_1: (0.5431) | Acc_1: (81.31%) (16757/20608)\n",
      "Epoch: 18 | Batch_idx: 170 |  Loss_1: (0.5448) | Acc_1: (81.25%) (17783/21888)\n",
      "Epoch: 18 | Batch_idx: 180 |  Loss_1: (0.5427) | Acc_1: (81.29%) (18834/23168)\n",
      "Epoch: 18 | Batch_idx: 190 |  Loss_1: (0.5410) | Acc_1: (81.32%) (19881/24448)\n",
      "Epoch: 18 | Batch_idx: 200 |  Loss_1: (0.5402) | Acc_1: (81.37%) (20934/25728)\n",
      "Epoch: 18 | Batch_idx: 210 |  Loss_1: (0.5415) | Acc_1: (81.42%) (21990/27008)\n",
      "Epoch: 18 | Batch_idx: 220 |  Loss_1: (0.5404) | Acc_1: (81.44%) (23037/28288)\n",
      "Epoch: 18 | Batch_idx: 230 |  Loss_1: (0.5393) | Acc_1: (81.46%) (24087/29568)\n",
      "Epoch: 18 | Batch_idx: 240 |  Loss_1: (0.5402) | Acc_1: (81.43%) (25120/30848)\n",
      "Epoch: 18 | Batch_idx: 250 |  Loss_1: (0.5405) | Acc_1: (81.42%) (26158/32128)\n",
      "Epoch: 18 | Batch_idx: 260 |  Loss_1: (0.5407) | Acc_1: (81.42%) (27202/33408)\n",
      "Epoch: 18 | Batch_idx: 270 |  Loss_1: (0.5410) | Acc_1: (81.45%) (28253/34688)\n",
      "Epoch: 18 | Batch_idx: 280 |  Loss_1: (0.5411) | Acc_1: (81.44%) (29291/35968)\n",
      "Epoch: 18 | Batch_idx: 290 |  Loss_1: (0.5391) | Acc_1: (81.53%) (30369/37248)\n",
      "Epoch: 18 | Batch_idx: 300 |  Loss_1: (0.5388) | Acc_1: (81.53%) (31412/38528)\n",
      "Epoch: 18 | Batch_idx: 310 |  Loss_1: (0.5395) | Acc_1: (81.48%) (32435/39808)\n",
      "Epoch: 18 | Batch_idx: 320 |  Loss_1: (0.5393) | Acc_1: (81.48%) (33478/41088)\n",
      "Epoch: 18 | Batch_idx: 330 |  Loss_1: (0.5380) | Acc_1: (81.50%) (34529/42368)\n",
      "Epoch: 18 | Batch_idx: 340 |  Loss_1: (0.5364) | Acc_1: (81.57%) (35603/43648)\n",
      "Epoch: 18 | Batch_idx: 350 |  Loss_1: (0.5375) | Acc_1: (81.54%) (36635/44928)\n",
      "Epoch: 18 | Batch_idx: 360 |  Loss_1: (0.5374) | Acc_1: (81.54%) (37679/46208)\n",
      "Epoch: 18 | Batch_idx: 370 |  Loss_1: (0.5372) | Acc_1: (81.52%) (38714/47488)\n",
      "Epoch: 18 | Batch_idx: 380 |  Loss_1: (0.5370) | Acc_1: (81.53%) (39760/48768)\n",
      "Epoch: 18 | Batch_idx: 390 |  Loss_1: (0.5375) | Acc_1: (81.53%) (40764/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5074) | Acc: (82.59%) (8259/10000)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.5182) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.5079) | Acc_1: (82.81%) (1166/1408)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.4936) | Acc_1: (82.37%) (2214/2688)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.5094) | Acc_1: (81.65%) (3240/3968)\n",
      "Epoch: 19 | Batch_idx: 40 |  Loss_1: (0.5112) | Acc_1: (81.57%) (4281/5248)\n",
      "Epoch: 19 | Batch_idx: 50 |  Loss_1: (0.5084) | Acc_1: (82.00%) (5353/6528)\n",
      "Epoch: 19 | Batch_idx: 60 |  Loss_1: (0.5043) | Acc_1: (82.34%) (6429/7808)\n",
      "Epoch: 19 | Batch_idx: 70 |  Loss_1: (0.5029) | Acc_1: (82.39%) (7488/9088)\n",
      "Epoch: 19 | Batch_idx: 80 |  Loss_1: (0.4952) | Acc_1: (82.70%) (8574/10368)\n",
      "Epoch: 19 | Batch_idx: 90 |  Loss_1: (0.4988) | Acc_1: (82.68%) (9631/11648)\n",
      "Epoch: 19 | Batch_idx: 100 |  Loss_1: (0.5020) | Acc_1: (82.66%) (10686/12928)\n",
      "Epoch: 19 | Batch_idx: 110 |  Loss_1: (0.5051) | Acc_1: (82.57%) (11731/14208)\n",
      "Epoch: 19 | Batch_idx: 120 |  Loss_1: (0.5068) | Acc_1: (82.63%) (12797/15488)\n",
      "Epoch: 19 | Batch_idx: 130 |  Loss_1: (0.5057) | Acc_1: (82.68%) (13864/16768)\n",
      "Epoch: 19 | Batch_idx: 140 |  Loss_1: (0.5050) | Acc_1: (82.69%) (14924/18048)\n",
      "Epoch: 19 | Batch_idx: 150 |  Loss_1: (0.5032) | Acc_1: (82.76%) (15995/19328)\n",
      "Epoch: 19 | Batch_idx: 160 |  Loss_1: (0.5063) | Acc_1: (82.65%) (17032/20608)\n",
      "Epoch: 19 | Batch_idx: 170 |  Loss_1: (0.5053) | Acc_1: (82.67%) (18095/21888)\n",
      "Epoch: 19 | Batch_idx: 180 |  Loss_1: (0.5057) | Acc_1: (82.61%) (19140/23168)\n",
      "Epoch: 19 | Batch_idx: 190 |  Loss_1: (0.5048) | Acc_1: (82.67%) (20210/24448)\n",
      "Epoch: 19 | Batch_idx: 200 |  Loss_1: (0.5052) | Acc_1: (82.70%) (21277/25728)\n",
      "Epoch: 19 | Batch_idx: 210 |  Loss_1: (0.5037) | Acc_1: (82.75%) (22348/27008)\n",
      "Epoch: 19 | Batch_idx: 220 |  Loss_1: (0.5040) | Acc_1: (82.76%) (23410/28288)\n",
      "Epoch: 19 | Batch_idx: 230 |  Loss_1: (0.5067) | Acc_1: (82.66%) (24442/29568)\n",
      "Epoch: 19 | Batch_idx: 240 |  Loss_1: (0.5072) | Acc_1: (82.61%) (25485/30848)\n",
      "Epoch: 19 | Batch_idx: 250 |  Loss_1: (0.5074) | Acc_1: (82.58%) (26530/32128)\n",
      "Epoch: 19 | Batch_idx: 260 |  Loss_1: (0.5073) | Acc_1: (82.57%) (27585/33408)\n",
      "Epoch: 19 | Batch_idx: 270 |  Loss_1: (0.5071) | Acc_1: (82.62%) (28658/34688)\n",
      "Epoch: 19 | Batch_idx: 280 |  Loss_1: (0.5082) | Acc_1: (82.60%) (29708/35968)\n",
      "Epoch: 19 | Batch_idx: 290 |  Loss_1: (0.5087) | Acc_1: (82.58%) (30759/37248)\n",
      "Epoch: 19 | Batch_idx: 300 |  Loss_1: (0.5091) | Acc_1: (82.58%) (31816/38528)\n",
      "Epoch: 19 | Batch_idx: 310 |  Loss_1: (0.5089) | Acc_1: (82.58%) (32875/39808)\n",
      "Epoch: 19 | Batch_idx: 320 |  Loss_1: (0.5088) | Acc_1: (82.58%) (33930/41088)\n",
      "Epoch: 19 | Batch_idx: 330 |  Loss_1: (0.5082) | Acc_1: (82.63%) (35008/42368)\n",
      "Epoch: 19 | Batch_idx: 340 |  Loss_1: (0.5098) | Acc_1: (82.57%) (36041/43648)\n",
      "Epoch: 19 | Batch_idx: 350 |  Loss_1: (0.5103) | Acc_1: (82.58%) (37103/44928)\n",
      "Epoch: 19 | Batch_idx: 360 |  Loss_1: (0.5098) | Acc_1: (82.57%) (38155/46208)\n",
      "Epoch: 19 | Batch_idx: 370 |  Loss_1: (0.5103) | Acc_1: (82.55%) (39202/47488)\n",
      "Epoch: 19 | Batch_idx: 380 |  Loss_1: (0.5120) | Acc_1: (82.48%) (40226/48768)\n",
      "Epoch: 19 | Batch_idx: 390 |  Loss_1: (0.5118) | Acc_1: (82.48%) (41240/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4732) | Acc: (84.46%) (8446/10000)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.3985) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.4850) | Acc_1: (83.03%) (1169/1408)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.4810) | Acc_1: (83.11%) (2234/2688)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.4779) | Acc_1: (83.14%) (3299/3968)\n",
      "Epoch: 20 | Batch_idx: 40 |  Loss_1: (0.4811) | Acc_1: (83.19%) (4366/5248)\n",
      "Epoch: 20 | Batch_idx: 50 |  Loss_1: (0.4797) | Acc_1: (83.35%) (5441/6528)\n",
      "Epoch: 20 | Batch_idx: 60 |  Loss_1: (0.4972) | Acc_1: (83.07%) (6486/7808)\n",
      "Epoch: 20 | Batch_idx: 70 |  Loss_1: (0.4969) | Acc_1: (83.03%) (7546/9088)\n",
      "Epoch: 20 | Batch_idx: 80 |  Loss_1: (0.4958) | Acc_1: (83.18%) (8624/10368)\n",
      "Epoch: 20 | Batch_idx: 90 |  Loss_1: (0.4944) | Acc_1: (83.16%) (9686/11648)\n",
      "Epoch: 20 | Batch_idx: 100 |  Loss_1: (0.4960) | Acc_1: (83.08%) (10740/12928)\n",
      "Epoch: 20 | Batch_idx: 110 |  Loss_1: (0.4977) | Acc_1: (83.04%) (11799/14208)\n",
      "Epoch: 20 | Batch_idx: 120 |  Loss_1: (0.4954) | Acc_1: (83.06%) (12864/15488)\n",
      "Epoch: 20 | Batch_idx: 130 |  Loss_1: (0.4962) | Acc_1: (82.94%) (13908/16768)\n",
      "Epoch: 20 | Batch_idx: 140 |  Loss_1: (0.4947) | Acc_1: (82.89%) (14960/18048)\n",
      "Epoch: 20 | Batch_idx: 150 |  Loss_1: (0.4938) | Acc_1: (82.91%) (16024/19328)\n",
      "Epoch: 20 | Batch_idx: 160 |  Loss_1: (0.4962) | Acc_1: (82.89%) (17083/20608)\n",
      "Epoch: 20 | Batch_idx: 170 |  Loss_1: (0.4966) | Acc_1: (82.88%) (18141/21888)\n",
      "Epoch: 20 | Batch_idx: 180 |  Loss_1: (0.4971) | Acc_1: (82.90%) (19207/23168)\n",
      "Epoch: 20 | Batch_idx: 190 |  Loss_1: (0.4965) | Acc_1: (82.89%) (20266/24448)\n",
      "Epoch: 20 | Batch_idx: 200 |  Loss_1: (0.4968) | Acc_1: (82.89%) (21327/25728)\n",
      "Epoch: 20 | Batch_idx: 210 |  Loss_1: (0.4965) | Acc_1: (82.87%) (22382/27008)\n",
      "Epoch: 20 | Batch_idx: 220 |  Loss_1: (0.4987) | Acc_1: (82.80%) (23423/28288)\n",
      "Epoch: 20 | Batch_idx: 230 |  Loss_1: (0.4980) | Acc_1: (82.87%) (24502/29568)\n",
      "Epoch: 20 | Batch_idx: 240 |  Loss_1: (0.4997) | Acc_1: (82.81%) (25544/30848)\n",
      "Epoch: 20 | Batch_idx: 250 |  Loss_1: (0.5007) | Acc_1: (82.84%) (26614/32128)\n",
      "Epoch: 20 | Batch_idx: 260 |  Loss_1: (0.5013) | Acc_1: (82.81%) (27666/33408)\n",
      "Epoch: 20 | Batch_idx: 270 |  Loss_1: (0.5023) | Acc_1: (82.80%) (28722/34688)\n",
      "Epoch: 20 | Batch_idx: 280 |  Loss_1: (0.5027) | Acc_1: (82.79%) (29779/35968)\n",
      "Epoch: 20 | Batch_idx: 290 |  Loss_1: (0.5013) | Acc_1: (82.83%) (30854/37248)\n",
      "Epoch: 20 | Batch_idx: 300 |  Loss_1: (0.5014) | Acc_1: (82.85%) (31922/38528)\n",
      "Epoch: 20 | Batch_idx: 310 |  Loss_1: (0.5014) | Acc_1: (82.81%) (32965/39808)\n",
      "Epoch: 20 | Batch_idx: 320 |  Loss_1: (0.5020) | Acc_1: (82.81%) (34027/41088)\n",
      "Epoch: 20 | Batch_idx: 330 |  Loss_1: (0.5023) | Acc_1: (82.79%) (35077/42368)\n",
      "Epoch: 20 | Batch_idx: 340 |  Loss_1: (0.5025) | Acc_1: (82.78%) (36133/43648)\n",
      "Epoch: 20 | Batch_idx: 350 |  Loss_1: (0.5014) | Acc_1: (82.81%) (37204/44928)\n",
      "Epoch: 20 | Batch_idx: 360 |  Loss_1: (0.5016) | Acc_1: (82.79%) (38255/46208)\n",
      "Epoch: 20 | Batch_idx: 370 |  Loss_1: (0.5023) | Acc_1: (82.76%) (39303/47488)\n",
      "Epoch: 20 | Batch_idx: 380 |  Loss_1: (0.5020) | Acc_1: (82.76%) (40359/48768)\n",
      "Epoch: 20 | Batch_idx: 390 |  Loss_1: (0.5023) | Acc_1: (82.75%) (41374/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4169) | Acc: (85.93%) (8593/10000)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.5547) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.4644) | Acc_1: (83.45%) (1175/1408)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.4646) | Acc_1: (84.26%) (2265/2688)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.4907) | Acc_1: (83.44%) (3311/3968)\n",
      "Epoch: 21 | Batch_idx: 40 |  Loss_1: (0.4922) | Acc_1: (83.19%) (4366/5248)\n",
      "Epoch: 21 | Batch_idx: 50 |  Loss_1: (0.4855) | Acc_1: (83.41%) (5445/6528)\n",
      "Epoch: 21 | Batch_idx: 60 |  Loss_1: (0.4867) | Acc_1: (83.48%) (6518/7808)\n",
      "Epoch: 21 | Batch_idx: 70 |  Loss_1: (0.4816) | Acc_1: (83.46%) (7585/9088)\n",
      "Epoch: 21 | Batch_idx: 80 |  Loss_1: (0.4776) | Acc_1: (83.54%) (8661/10368)\n",
      "Epoch: 21 | Batch_idx: 90 |  Loss_1: (0.4770) | Acc_1: (83.55%) (9732/11648)\n",
      "Epoch: 21 | Batch_idx: 100 |  Loss_1: (0.4744) | Acc_1: (83.60%) (10808/12928)\n",
      "Epoch: 21 | Batch_idx: 110 |  Loss_1: (0.4793) | Acc_1: (83.44%) (11855/14208)\n",
      "Epoch: 21 | Batch_idx: 120 |  Loss_1: (0.4794) | Acc_1: (83.43%) (12921/15488)\n",
      "Epoch: 21 | Batch_idx: 130 |  Loss_1: (0.4815) | Acc_1: (83.31%) (13969/16768)\n",
      "Epoch: 21 | Batch_idx: 140 |  Loss_1: (0.4809) | Acc_1: (83.31%) (15036/18048)\n",
      "Epoch: 21 | Batch_idx: 150 |  Loss_1: (0.4792) | Acc_1: (83.42%) (16124/19328)\n",
      "Epoch: 21 | Batch_idx: 160 |  Loss_1: (0.4791) | Acc_1: (83.43%) (17194/20608)\n",
      "Epoch: 21 | Batch_idx: 170 |  Loss_1: (0.4780) | Acc_1: (83.45%) (18266/21888)\n",
      "Epoch: 21 | Batch_idx: 180 |  Loss_1: (0.4783) | Acc_1: (83.44%) (19331/23168)\n",
      "Epoch: 21 | Batch_idx: 190 |  Loss_1: (0.4810) | Acc_1: (83.32%) (20371/24448)\n",
      "Epoch: 21 | Batch_idx: 200 |  Loss_1: (0.4825) | Acc_1: (83.26%) (21420/25728)\n",
      "Epoch: 21 | Batch_idx: 210 |  Loss_1: (0.4844) | Acc_1: (83.21%) (22474/27008)\n",
      "Epoch: 21 | Batch_idx: 220 |  Loss_1: (0.4843) | Acc_1: (83.22%) (23541/28288)\n",
      "Epoch: 21 | Batch_idx: 230 |  Loss_1: (0.4833) | Acc_1: (83.27%) (24620/29568)\n",
      "Epoch: 21 | Batch_idx: 240 |  Loss_1: (0.4826) | Acc_1: (83.31%) (25701/30848)\n",
      "Epoch: 21 | Batch_idx: 250 |  Loss_1: (0.4828) | Acc_1: (83.27%) (26754/32128)\n",
      "Epoch: 21 | Batch_idx: 260 |  Loss_1: (0.4809) | Acc_1: (83.33%) (27838/33408)\n",
      "Epoch: 21 | Batch_idx: 270 |  Loss_1: (0.4794) | Acc_1: (83.38%) (28924/34688)\n",
      "Epoch: 21 | Batch_idx: 280 |  Loss_1: (0.4798) | Acc_1: (83.36%) (29983/35968)\n",
      "Epoch: 21 | Batch_idx: 290 |  Loss_1: (0.4806) | Acc_1: (83.35%) (31048/37248)\n",
      "Epoch: 21 | Batch_idx: 300 |  Loss_1: (0.4815) | Acc_1: (83.32%) (32102/38528)\n",
      "Epoch: 21 | Batch_idx: 310 |  Loss_1: (0.4813) | Acc_1: (83.36%) (33182/39808)\n",
      "Epoch: 21 | Batch_idx: 320 |  Loss_1: (0.4823) | Acc_1: (83.29%) (34223/41088)\n",
      "Epoch: 21 | Batch_idx: 330 |  Loss_1: (0.4819) | Acc_1: (83.30%) (35292/42368)\n",
      "Epoch: 21 | Batch_idx: 340 |  Loss_1: (0.4816) | Acc_1: (83.31%) (36361/43648)\n",
      "Epoch: 21 | Batch_idx: 350 |  Loss_1: (0.4820) | Acc_1: (83.29%) (37420/44928)\n",
      "Epoch: 21 | Batch_idx: 360 |  Loss_1: (0.4825) | Acc_1: (83.30%) (38489/46208)\n",
      "Epoch: 21 | Batch_idx: 370 |  Loss_1: (0.4832) | Acc_1: (83.26%) (39539/47488)\n",
      "Epoch: 21 | Batch_idx: 380 |  Loss_1: (0.4844) | Acc_1: (83.24%) (40596/48768)\n",
      "Epoch: 21 | Batch_idx: 390 |  Loss_1: (0.4864) | Acc_1: (83.19%) (41593/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4726) | Acc: (83.90%) (8390/10000)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.4203) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.4913) | Acc_1: (83.24%) (1172/1408)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.4825) | Acc_1: (83.04%) (2232/2688)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.4861) | Acc_1: (83.09%) (3297/3968)\n",
      "Epoch: 22 | Batch_idx: 40 |  Loss_1: (0.4850) | Acc_1: (83.42%) (4378/5248)\n",
      "Epoch: 22 | Batch_idx: 50 |  Loss_1: (0.4824) | Acc_1: (83.55%) (5454/6528)\n",
      "Epoch: 22 | Batch_idx: 60 |  Loss_1: (0.4854) | Acc_1: (83.34%) (6507/7808)\n",
      "Epoch: 22 | Batch_idx: 70 |  Loss_1: (0.4828) | Acc_1: (83.44%) (7583/9088)\n",
      "Epoch: 22 | Batch_idx: 80 |  Loss_1: (0.4831) | Acc_1: (83.35%) (8642/10368)\n",
      "Epoch: 22 | Batch_idx: 90 |  Loss_1: (0.4802) | Acc_1: (83.45%) (9720/11648)\n",
      "Epoch: 22 | Batch_idx: 100 |  Loss_1: (0.4772) | Acc_1: (83.55%) (10801/12928)\n",
      "Epoch: 22 | Batch_idx: 110 |  Loss_1: (0.4750) | Acc_1: (83.68%) (11889/14208)\n",
      "Epoch: 22 | Batch_idx: 120 |  Loss_1: (0.4765) | Acc_1: (83.62%) (12951/15488)\n",
      "Epoch: 22 | Batch_idx: 130 |  Loss_1: (0.4756) | Acc_1: (83.67%) (14029/16768)\n",
      "Epoch: 22 | Batch_idx: 140 |  Loss_1: (0.4745) | Acc_1: (83.71%) (15108/18048)\n",
      "Epoch: 22 | Batch_idx: 150 |  Loss_1: (0.4754) | Acc_1: (83.66%) (16169/19328)\n",
      "Epoch: 22 | Batch_idx: 160 |  Loss_1: (0.4752) | Acc_1: (83.70%) (17249/20608)\n",
      "Epoch: 22 | Batch_idx: 170 |  Loss_1: (0.4767) | Acc_1: (83.60%) (18299/21888)\n",
      "Epoch: 22 | Batch_idx: 180 |  Loss_1: (0.4735) | Acc_1: (83.67%) (19385/23168)\n",
      "Epoch: 22 | Batch_idx: 190 |  Loss_1: (0.4732) | Acc_1: (83.68%) (20458/24448)\n",
      "Epoch: 22 | Batch_idx: 200 |  Loss_1: (0.4719) | Acc_1: (83.76%) (21549/25728)\n",
      "Epoch: 22 | Batch_idx: 210 |  Loss_1: (0.4712) | Acc_1: (83.79%) (22629/27008)\n",
      "Epoch: 22 | Batch_idx: 220 |  Loss_1: (0.4717) | Acc_1: (83.76%) (23693/28288)\n",
      "Epoch: 22 | Batch_idx: 230 |  Loss_1: (0.4722) | Acc_1: (83.74%) (24761/29568)\n",
      "Epoch: 22 | Batch_idx: 240 |  Loss_1: (0.4734) | Acc_1: (83.73%) (25828/30848)\n",
      "Epoch: 22 | Batch_idx: 250 |  Loss_1: (0.4738) | Acc_1: (83.72%) (26897/32128)\n",
      "Epoch: 22 | Batch_idx: 260 |  Loss_1: (0.4735) | Acc_1: (83.73%) (27974/33408)\n",
      "Epoch: 22 | Batch_idx: 270 |  Loss_1: (0.4736) | Acc_1: (83.71%) (29037/34688)\n",
      "Epoch: 22 | Batch_idx: 280 |  Loss_1: (0.4745) | Acc_1: (83.66%) (30092/35968)\n",
      "Epoch: 22 | Batch_idx: 290 |  Loss_1: (0.4737) | Acc_1: (83.69%) (31171/37248)\n",
      "Epoch: 22 | Batch_idx: 300 |  Loss_1: (0.4730) | Acc_1: (83.69%) (32245/38528)\n",
      "Epoch: 22 | Batch_idx: 310 |  Loss_1: (0.4730) | Acc_1: (83.69%) (33314/39808)\n",
      "Epoch: 22 | Batch_idx: 320 |  Loss_1: (0.4730) | Acc_1: (83.69%) (34386/41088)\n",
      "Epoch: 22 | Batch_idx: 330 |  Loss_1: (0.4730) | Acc_1: (83.71%) (35466/42368)\n",
      "Epoch: 22 | Batch_idx: 340 |  Loss_1: (0.4738) | Acc_1: (83.69%) (36528/43648)\n",
      "Epoch: 22 | Batch_idx: 350 |  Loss_1: (0.4731) | Acc_1: (83.70%) (37605/44928)\n",
      "Epoch: 22 | Batch_idx: 360 |  Loss_1: (0.4730) | Acc_1: (83.71%) (38681/46208)\n",
      "Epoch: 22 | Batch_idx: 370 |  Loss_1: (0.4725) | Acc_1: (83.73%) (39763/47488)\n",
      "Epoch: 22 | Batch_idx: 380 |  Loss_1: (0.4721) | Acc_1: (83.74%) (40836/48768)\n",
      "Epoch: 22 | Batch_idx: 390 |  Loss_1: (0.4733) | Acc_1: (83.70%) (41850/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4558) | Acc: (84.36%) (8436/10000)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.4747) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.4371) | Acc_1: (85.01%) (1197/1408)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.4196) | Acc_1: (85.71%) (2304/2688)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.4061) | Acc_1: (85.96%) (3411/3968)\n",
      "Epoch: 23 | Batch_idx: 40 |  Loss_1: (0.4130) | Acc_1: (85.88%) (4507/5248)\n",
      "Epoch: 23 | Batch_idx: 50 |  Loss_1: (0.4242) | Acc_1: (85.55%) (5585/6528)\n",
      "Epoch: 23 | Batch_idx: 60 |  Loss_1: (0.4279) | Acc_1: (85.26%) (6657/7808)\n",
      "Epoch: 23 | Batch_idx: 70 |  Loss_1: (0.4286) | Acc_1: (85.17%) (7740/9088)\n",
      "Epoch: 23 | Batch_idx: 80 |  Loss_1: (0.4345) | Acc_1: (85.10%) (8823/10368)\n",
      "Epoch: 23 | Batch_idx: 90 |  Loss_1: (0.4378) | Acc_1: (84.94%) (9894/11648)\n",
      "Epoch: 23 | Batch_idx: 100 |  Loss_1: (0.4386) | Acc_1: (84.96%) (10983/12928)\n",
      "Epoch: 23 | Batch_idx: 110 |  Loss_1: (0.4402) | Acc_1: (84.88%) (12060/14208)\n",
      "Epoch: 23 | Batch_idx: 120 |  Loss_1: (0.4458) | Acc_1: (84.72%) (13122/15488)\n",
      "Epoch: 23 | Batch_idx: 130 |  Loss_1: (0.4466) | Acc_1: (84.63%) (14190/16768)\n",
      "Epoch: 23 | Batch_idx: 140 |  Loss_1: (0.4480) | Acc_1: (84.56%) (15262/18048)\n",
      "Epoch: 23 | Batch_idx: 150 |  Loss_1: (0.4491) | Acc_1: (84.56%) (16343/19328)\n",
      "Epoch: 23 | Batch_idx: 160 |  Loss_1: (0.4490) | Acc_1: (84.57%) (17429/20608)\n",
      "Epoch: 23 | Batch_idx: 170 |  Loss_1: (0.4490) | Acc_1: (84.56%) (18508/21888)\n",
      "Epoch: 23 | Batch_idx: 180 |  Loss_1: (0.4514) | Acc_1: (84.48%) (19572/23168)\n",
      "Epoch: 23 | Batch_idx: 190 |  Loss_1: (0.4506) | Acc_1: (84.53%) (20667/24448)\n",
      "Epoch: 23 | Batch_idx: 200 |  Loss_1: (0.4522) | Acc_1: (84.49%) (21737/25728)\n",
      "Epoch: 23 | Batch_idx: 210 |  Loss_1: (0.4519) | Acc_1: (84.52%) (22826/27008)\n",
      "Epoch: 23 | Batch_idx: 220 |  Loss_1: (0.4522) | Acc_1: (84.52%) (23908/28288)\n",
      "Epoch: 23 | Batch_idx: 230 |  Loss_1: (0.4523) | Acc_1: (84.53%) (24994/29568)\n",
      "Epoch: 23 | Batch_idx: 240 |  Loss_1: (0.4543) | Acc_1: (84.45%) (26052/30848)\n",
      "Epoch: 23 | Batch_idx: 250 |  Loss_1: (0.4578) | Acc_1: (84.32%) (27091/32128)\n",
      "Epoch: 23 | Batch_idx: 260 |  Loss_1: (0.4589) | Acc_1: (84.31%) (28166/33408)\n",
      "Epoch: 23 | Batch_idx: 270 |  Loss_1: (0.4594) | Acc_1: (84.32%) (29249/34688)\n",
      "Epoch: 23 | Batch_idx: 280 |  Loss_1: (0.4591) | Acc_1: (84.31%) (30323/35968)\n",
      "Epoch: 23 | Batch_idx: 290 |  Loss_1: (0.4605) | Acc_1: (84.27%) (31388/37248)\n",
      "Epoch: 23 | Batch_idx: 300 |  Loss_1: (0.4611) | Acc_1: (84.26%) (32464/38528)\n",
      "Epoch: 23 | Batch_idx: 310 |  Loss_1: (0.4605) | Acc_1: (84.29%) (33554/39808)\n",
      "Epoch: 23 | Batch_idx: 320 |  Loss_1: (0.4601) | Acc_1: (84.32%) (34644/41088)\n",
      "Epoch: 23 | Batch_idx: 330 |  Loss_1: (0.4600) | Acc_1: (84.31%) (35720/42368)\n",
      "Epoch: 23 | Batch_idx: 340 |  Loss_1: (0.4596) | Acc_1: (84.31%) (36800/43648)\n",
      "Epoch: 23 | Batch_idx: 350 |  Loss_1: (0.4600) | Acc_1: (84.31%) (37878/44928)\n",
      "Epoch: 23 | Batch_idx: 360 |  Loss_1: (0.4603) | Acc_1: (84.32%) (38963/46208)\n",
      "Epoch: 23 | Batch_idx: 370 |  Loss_1: (0.4598) | Acc_1: (84.31%) (40037/47488)\n",
      "Epoch: 23 | Batch_idx: 380 |  Loss_1: (0.4607) | Acc_1: (84.25%) (41089/48768)\n",
      "Epoch: 23 | Batch_idx: 390 |  Loss_1: (0.4612) | Acc_1: (84.23%) (42113/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4077) | Acc: (86.07%) (8607/10000)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.4977) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.4367) | Acc_1: (84.66%) (1192/1408)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.4618) | Acc_1: (84.34%) (2267/2688)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.4504) | Acc_1: (84.85%) (3367/3968)\n",
      "Epoch: 24 | Batch_idx: 40 |  Loss_1: (0.4397) | Acc_1: (85.27%) (4475/5248)\n",
      "Epoch: 24 | Batch_idx: 50 |  Loss_1: (0.4490) | Acc_1: (85.00%) (5549/6528)\n",
      "Epoch: 24 | Batch_idx: 60 |  Loss_1: (0.4472) | Acc_1: (84.95%) (6633/7808)\n",
      "Epoch: 24 | Batch_idx: 70 |  Loss_1: (0.4459) | Acc_1: (84.91%) (7717/9088)\n",
      "Epoch: 24 | Batch_idx: 80 |  Loss_1: (0.4469) | Acc_1: (84.89%) (8801/10368)\n",
      "Epoch: 24 | Batch_idx: 90 |  Loss_1: (0.4475) | Acc_1: (84.88%) (9887/11648)\n",
      "Epoch: 24 | Batch_idx: 100 |  Loss_1: (0.4489) | Acc_1: (84.78%) (10961/12928)\n",
      "Epoch: 24 | Batch_idx: 110 |  Loss_1: (0.4459) | Acc_1: (84.83%) (12052/14208)\n",
      "Epoch: 24 | Batch_idx: 120 |  Loss_1: (0.4415) | Acc_1: (85.06%) (13174/15488)\n",
      "Epoch: 24 | Batch_idx: 130 |  Loss_1: (0.4404) | Acc_1: (85.17%) (14281/16768)\n",
      "Epoch: 24 | Batch_idx: 140 |  Loss_1: (0.4406) | Acc_1: (85.18%) (15374/18048)\n",
      "Epoch: 24 | Batch_idx: 150 |  Loss_1: (0.4403) | Acc_1: (85.18%) (16464/19328)\n",
      "Epoch: 24 | Batch_idx: 160 |  Loss_1: (0.4420) | Acc_1: (85.08%) (17533/20608)\n",
      "Epoch: 24 | Batch_idx: 170 |  Loss_1: (0.4418) | Acc_1: (85.03%) (18611/21888)\n",
      "Epoch: 24 | Batch_idx: 180 |  Loss_1: (0.4434) | Acc_1: (84.90%) (19670/23168)\n",
      "Epoch: 24 | Batch_idx: 190 |  Loss_1: (0.4432) | Acc_1: (84.94%) (20767/24448)\n",
      "Epoch: 24 | Batch_idx: 200 |  Loss_1: (0.4421) | Acc_1: (84.97%) (21860/25728)\n",
      "Epoch: 24 | Batch_idx: 210 |  Loss_1: (0.4414) | Acc_1: (85.04%) (22967/27008)\n",
      "Epoch: 24 | Batch_idx: 220 |  Loss_1: (0.4428) | Acc_1: (84.94%) (24029/28288)\n",
      "Epoch: 24 | Batch_idx: 230 |  Loss_1: (0.4446) | Acc_1: (84.86%) (25091/29568)\n",
      "Epoch: 24 | Batch_idx: 240 |  Loss_1: (0.4474) | Acc_1: (84.74%) (26140/30848)\n",
      "Epoch: 24 | Batch_idx: 250 |  Loss_1: (0.4473) | Acc_1: (84.74%) (27224/32128)\n",
      "Epoch: 24 | Batch_idx: 260 |  Loss_1: (0.4485) | Acc_1: (84.67%) (28287/33408)\n",
      "Epoch: 24 | Batch_idx: 270 |  Loss_1: (0.4480) | Acc_1: (84.65%) (29362/34688)\n",
      "Epoch: 24 | Batch_idx: 280 |  Loss_1: (0.4466) | Acc_1: (84.72%) (30473/35968)\n",
      "Epoch: 24 | Batch_idx: 290 |  Loss_1: (0.4453) | Acc_1: (84.77%) (31574/37248)\n",
      "Epoch: 24 | Batch_idx: 300 |  Loss_1: (0.4454) | Acc_1: (84.75%) (32653/38528)\n",
      "Epoch: 24 | Batch_idx: 310 |  Loss_1: (0.4451) | Acc_1: (84.71%) (33720/39808)\n",
      "Epoch: 24 | Batch_idx: 320 |  Loss_1: (0.4451) | Acc_1: (84.71%) (34807/41088)\n",
      "Epoch: 24 | Batch_idx: 330 |  Loss_1: (0.4439) | Acc_1: (84.75%) (35905/42368)\n",
      "Epoch: 24 | Batch_idx: 340 |  Loss_1: (0.4445) | Acc_1: (84.78%) (37003/43648)\n",
      "Epoch: 24 | Batch_idx: 350 |  Loss_1: (0.4441) | Acc_1: (84.78%) (38088/44928)\n",
      "Epoch: 24 | Batch_idx: 360 |  Loss_1: (0.4442) | Acc_1: (84.78%) (39173/46208)\n",
      "Epoch: 24 | Batch_idx: 370 |  Loss_1: (0.4452) | Acc_1: (84.73%) (40238/47488)\n",
      "Epoch: 24 | Batch_idx: 380 |  Loss_1: (0.4456) | Acc_1: (84.74%) (41326/48768)\n",
      "Epoch: 24 | Batch_idx: 390 |  Loss_1: (0.4464) | Acc_1: (84.71%) (42355/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4124) | Acc: (85.75%) (8575/10000)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.4758) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.4414) | Acc_1: (84.87%) (1195/1408)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.4450) | Acc_1: (85.08%) (2287/2688)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.4353) | Acc_1: (85.48%) (3392/3968)\n",
      "Epoch: 25 | Batch_idx: 40 |  Loss_1: (0.4292) | Acc_1: (85.65%) (4495/5248)\n",
      "Epoch: 25 | Batch_idx: 50 |  Loss_1: (0.4260) | Acc_1: (85.75%) (5598/6528)\n",
      "Epoch: 25 | Batch_idx: 60 |  Loss_1: (0.4282) | Acc_1: (85.55%) (6680/7808)\n",
      "Epoch: 25 | Batch_idx: 70 |  Loss_1: (0.4288) | Acc_1: (85.53%) (7773/9088)\n",
      "Epoch: 25 | Batch_idx: 80 |  Loss_1: (0.4262) | Acc_1: (85.65%) (8880/10368)\n",
      "Epoch: 25 | Batch_idx: 90 |  Loss_1: (0.4239) | Acc_1: (85.69%) (9981/11648)\n",
      "Epoch: 25 | Batch_idx: 100 |  Loss_1: (0.4207) | Acc_1: (85.73%) (11083/12928)\n",
      "Epoch: 25 | Batch_idx: 110 |  Loss_1: (0.4184) | Acc_1: (85.80%) (12190/14208)\n",
      "Epoch: 25 | Batch_idx: 120 |  Loss_1: (0.4160) | Acc_1: (85.86%) (13298/15488)\n",
      "Epoch: 25 | Batch_idx: 130 |  Loss_1: (0.4206) | Acc_1: (85.62%) (14356/16768)\n",
      "Epoch: 25 | Batch_idx: 140 |  Loss_1: (0.4193) | Acc_1: (85.69%) (15465/18048)\n",
      "Epoch: 25 | Batch_idx: 150 |  Loss_1: (0.4215) | Acc_1: (85.64%) (16552/19328)\n",
      "Epoch: 25 | Batch_idx: 160 |  Loss_1: (0.4189) | Acc_1: (85.68%) (17657/20608)\n",
      "Epoch: 25 | Batch_idx: 170 |  Loss_1: (0.4219) | Acc_1: (85.57%) (18729/21888)\n",
      "Epoch: 25 | Batch_idx: 180 |  Loss_1: (0.4226) | Acc_1: (85.57%) (19826/23168)\n",
      "Epoch: 25 | Batch_idx: 190 |  Loss_1: (0.4236) | Acc_1: (85.51%) (20906/24448)\n",
      "Epoch: 25 | Batch_idx: 200 |  Loss_1: (0.4226) | Acc_1: (85.56%) (22012/25728)\n",
      "Epoch: 25 | Batch_idx: 210 |  Loss_1: (0.4232) | Acc_1: (85.50%) (23093/27008)\n",
      "Epoch: 25 | Batch_idx: 220 |  Loss_1: (0.4266) | Acc_1: (85.41%) (24162/28288)\n",
      "Epoch: 25 | Batch_idx: 230 |  Loss_1: (0.4280) | Acc_1: (85.40%) (25250/29568)\n",
      "Epoch: 25 | Batch_idx: 240 |  Loss_1: (0.4287) | Acc_1: (85.40%) (26344/30848)\n",
      "Epoch: 25 | Batch_idx: 250 |  Loss_1: (0.4314) | Acc_1: (85.29%) (27403/32128)\n",
      "Epoch: 25 | Batch_idx: 260 |  Loss_1: (0.4329) | Acc_1: (85.22%) (28470/33408)\n",
      "Epoch: 25 | Batch_idx: 270 |  Loss_1: (0.4319) | Acc_1: (85.21%) (29559/34688)\n",
      "Epoch: 25 | Batch_idx: 280 |  Loss_1: (0.4320) | Acc_1: (85.23%) (30654/35968)\n",
      "Epoch: 25 | Batch_idx: 290 |  Loss_1: (0.4319) | Acc_1: (85.23%) (31745/37248)\n",
      "Epoch: 25 | Batch_idx: 300 |  Loss_1: (0.4328) | Acc_1: (85.18%) (32818/38528)\n",
      "Epoch: 25 | Batch_idx: 310 |  Loss_1: (0.4326) | Acc_1: (85.18%) (33907/39808)\n",
      "Epoch: 25 | Batch_idx: 320 |  Loss_1: (0.4325) | Acc_1: (85.19%) (35002/41088)\n",
      "Epoch: 25 | Batch_idx: 330 |  Loss_1: (0.4321) | Acc_1: (85.21%) (36100/42368)\n",
      "Epoch: 25 | Batch_idx: 340 |  Loss_1: (0.4331) | Acc_1: (85.14%) (37161/43648)\n",
      "Epoch: 25 | Batch_idx: 350 |  Loss_1: (0.4339) | Acc_1: (85.10%) (38234/44928)\n",
      "Epoch: 25 | Batch_idx: 360 |  Loss_1: (0.4346) | Acc_1: (85.04%) (39296/46208)\n",
      "Epoch: 25 | Batch_idx: 370 |  Loss_1: (0.4345) | Acc_1: (85.07%) (40399/47488)\n",
      "Epoch: 25 | Batch_idx: 380 |  Loss_1: (0.4345) | Acc_1: (85.05%) (41478/48768)\n",
      "Epoch: 25 | Batch_idx: 390 |  Loss_1: (0.4337) | Acc_1: (85.07%) (42533/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4075) | Acc: (86.39%) (8639/10000)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.4526) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.3961) | Acc_1: (86.72%) (1221/1408)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.3962) | Acc_1: (86.31%) (2320/2688)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.3894) | Acc_1: (86.49%) (3432/3968)\n",
      "Epoch: 26 | Batch_idx: 40 |  Loss_1: (0.3995) | Acc_1: (86.39%) (4534/5248)\n",
      "Epoch: 26 | Batch_idx: 50 |  Loss_1: (0.4087) | Acc_1: (86.04%) (5617/6528)\n",
      "Epoch: 26 | Batch_idx: 60 |  Loss_1: (0.4078) | Acc_1: (86.12%) (6724/7808)\n",
      "Epoch: 26 | Batch_idx: 70 |  Loss_1: (0.4067) | Acc_1: (86.11%) (7826/9088)\n",
      "Epoch: 26 | Batch_idx: 80 |  Loss_1: (0.4042) | Acc_1: (86.10%) (8927/10368)\n",
      "Epoch: 26 | Batch_idx: 90 |  Loss_1: (0.4067) | Acc_1: (86.01%) (10018/11648)\n",
      "Epoch: 26 | Batch_idx: 100 |  Loss_1: (0.4081) | Acc_1: (85.91%) (11107/12928)\n",
      "Epoch: 26 | Batch_idx: 110 |  Loss_1: (0.4064) | Acc_1: (85.94%) (12211/14208)\n",
      "Epoch: 26 | Batch_idx: 120 |  Loss_1: (0.4086) | Acc_1: (85.85%) (13297/15488)\n",
      "Epoch: 26 | Batch_idx: 130 |  Loss_1: (0.4145) | Acc_1: (85.63%) (14358/16768)\n",
      "Epoch: 26 | Batch_idx: 140 |  Loss_1: (0.4173) | Acc_1: (85.59%) (15448/18048)\n",
      "Epoch: 26 | Batch_idx: 150 |  Loss_1: (0.4195) | Acc_1: (85.48%) (16521/19328)\n",
      "Epoch: 26 | Batch_idx: 160 |  Loss_1: (0.4202) | Acc_1: (85.42%) (17604/20608)\n",
      "Epoch: 26 | Batch_idx: 170 |  Loss_1: (0.4178) | Acc_1: (85.50%) (18714/21888)\n",
      "Epoch: 26 | Batch_idx: 180 |  Loss_1: (0.4176) | Acc_1: (85.51%) (19811/23168)\n",
      "Epoch: 26 | Batch_idx: 190 |  Loss_1: (0.4174) | Acc_1: (85.52%) (20909/24448)\n",
      "Epoch: 26 | Batch_idx: 200 |  Loss_1: (0.4207) | Acc_1: (85.43%) (21979/25728)\n",
      "Epoch: 26 | Batch_idx: 210 |  Loss_1: (0.4198) | Acc_1: (85.46%) (23082/27008)\n",
      "Epoch: 26 | Batch_idx: 220 |  Loss_1: (0.4181) | Acc_1: (85.51%) (24189/28288)\n",
      "Epoch: 26 | Batch_idx: 230 |  Loss_1: (0.4190) | Acc_1: (85.51%) (25285/29568)\n",
      "Epoch: 26 | Batch_idx: 240 |  Loss_1: (0.4197) | Acc_1: (85.50%) (26376/30848)\n",
      "Epoch: 26 | Batch_idx: 250 |  Loss_1: (0.4185) | Acc_1: (85.57%) (27491/32128)\n",
      "Epoch: 26 | Batch_idx: 260 |  Loss_1: (0.4185) | Acc_1: (85.55%) (28581/33408)\n",
      "Epoch: 26 | Batch_idx: 270 |  Loss_1: (0.4177) | Acc_1: (85.57%) (29682/34688)\n",
      "Epoch: 26 | Batch_idx: 280 |  Loss_1: (0.4188) | Acc_1: (85.53%) (30763/35968)\n",
      "Epoch: 26 | Batch_idx: 290 |  Loss_1: (0.4184) | Acc_1: (85.54%) (31861/37248)\n",
      "Epoch: 26 | Batch_idx: 300 |  Loss_1: (0.4186) | Acc_1: (85.51%) (32944/38528)\n",
      "Epoch: 26 | Batch_idx: 310 |  Loss_1: (0.4203) | Acc_1: (85.44%) (34013/39808)\n",
      "Epoch: 26 | Batch_idx: 320 |  Loss_1: (0.4210) | Acc_1: (85.41%) (35093/41088)\n",
      "Epoch: 26 | Batch_idx: 330 |  Loss_1: (0.4221) | Acc_1: (85.37%) (36170/42368)\n",
      "Epoch: 26 | Batch_idx: 340 |  Loss_1: (0.4210) | Acc_1: (85.39%) (37271/43648)\n",
      "Epoch: 26 | Batch_idx: 350 |  Loss_1: (0.4224) | Acc_1: (85.33%) (38335/44928)\n",
      "Epoch: 26 | Batch_idx: 360 |  Loss_1: (0.4230) | Acc_1: (85.31%) (39421/46208)\n",
      "Epoch: 26 | Batch_idx: 370 |  Loss_1: (0.4231) | Acc_1: (85.30%) (40509/47488)\n",
      "Epoch: 26 | Batch_idx: 380 |  Loss_1: (0.4237) | Acc_1: (85.29%) (41592/48768)\n",
      "Epoch: 26 | Batch_idx: 390 |  Loss_1: (0.4242) | Acc_1: (85.26%) (42630/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3837) | Acc: (87.04%) (8704/10000)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.4699) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.4441) | Acc_1: (84.52%) (1190/1408)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.4339) | Acc_1: (84.90%) (2282/2688)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.4161) | Acc_1: (85.38%) (3388/3968)\n",
      "Epoch: 27 | Batch_idx: 40 |  Loss_1: (0.4170) | Acc_1: (85.50%) (4487/5248)\n",
      "Epoch: 27 | Batch_idx: 50 |  Loss_1: (0.4143) | Acc_1: (85.52%) (5583/6528)\n",
      "Epoch: 27 | Batch_idx: 60 |  Loss_1: (0.4113) | Acc_1: (85.54%) (6679/7808)\n",
      "Epoch: 27 | Batch_idx: 70 |  Loss_1: (0.4173) | Acc_1: (85.35%) (7757/9088)\n",
      "Epoch: 27 | Batch_idx: 80 |  Loss_1: (0.4179) | Acc_1: (85.40%) (8854/10368)\n",
      "Epoch: 27 | Batch_idx: 90 |  Loss_1: (0.4156) | Acc_1: (85.58%) (9968/11648)\n",
      "Epoch: 27 | Batch_idx: 100 |  Loss_1: (0.4135) | Acc_1: (85.62%) (11069/12928)\n",
      "Epoch: 27 | Batch_idx: 110 |  Loss_1: (0.4155) | Acc_1: (85.62%) (12165/14208)\n",
      "Epoch: 27 | Batch_idx: 120 |  Loss_1: (0.4142) | Acc_1: (85.61%) (13260/15488)\n",
      "Epoch: 27 | Batch_idx: 130 |  Loss_1: (0.4112) | Acc_1: (85.72%) (14374/16768)\n",
      "Epoch: 27 | Batch_idx: 140 |  Loss_1: (0.4121) | Acc_1: (85.70%) (15468/18048)\n",
      "Epoch: 27 | Batch_idx: 150 |  Loss_1: (0.4121) | Acc_1: (85.67%) (16559/19328)\n",
      "Epoch: 27 | Batch_idx: 160 |  Loss_1: (0.4109) | Acc_1: (85.70%) (17661/20608)\n",
      "Epoch: 27 | Batch_idx: 170 |  Loss_1: (0.4108) | Acc_1: (85.66%) (18750/21888)\n",
      "Epoch: 27 | Batch_idx: 180 |  Loss_1: (0.4115) | Acc_1: (85.64%) (19841/23168)\n",
      "Epoch: 27 | Batch_idx: 190 |  Loss_1: (0.4124) | Acc_1: (85.59%) (20926/24448)\n",
      "Epoch: 27 | Batch_idx: 200 |  Loss_1: (0.4118) | Acc_1: (85.55%) (22010/25728)\n",
      "Epoch: 27 | Batch_idx: 210 |  Loss_1: (0.4122) | Acc_1: (85.57%) (23110/27008)\n",
      "Epoch: 27 | Batch_idx: 220 |  Loss_1: (0.4135) | Acc_1: (85.57%) (24205/28288)\n",
      "Epoch: 27 | Batch_idx: 230 |  Loss_1: (0.4125) | Acc_1: (85.60%) (25310/29568)\n",
      "Epoch: 27 | Batch_idx: 240 |  Loss_1: (0.4135) | Acc_1: (85.54%) (26386/30848)\n",
      "Epoch: 27 | Batch_idx: 250 |  Loss_1: (0.4132) | Acc_1: (85.56%) (27488/32128)\n",
      "Epoch: 27 | Batch_idx: 260 |  Loss_1: (0.4111) | Acc_1: (85.63%) (28607/33408)\n",
      "Epoch: 27 | Batch_idx: 270 |  Loss_1: (0.4129) | Acc_1: (85.55%) (29674/34688)\n",
      "Epoch: 27 | Batch_idx: 280 |  Loss_1: (0.4118) | Acc_1: (85.58%) (30781/35968)\n",
      "Epoch: 27 | Batch_idx: 290 |  Loss_1: (0.4119) | Acc_1: (85.58%) (31878/37248)\n",
      "Epoch: 27 | Batch_idx: 300 |  Loss_1: (0.4124) | Acc_1: (85.58%) (32974/38528)\n",
      "Epoch: 27 | Batch_idx: 310 |  Loss_1: (0.4117) | Acc_1: (85.62%) (34083/39808)\n",
      "Epoch: 27 | Batch_idx: 320 |  Loss_1: (0.4114) | Acc_1: (85.62%) (35181/41088)\n",
      "Epoch: 27 | Batch_idx: 330 |  Loss_1: (0.4102) | Acc_1: (85.68%) (36300/42368)\n",
      "Epoch: 27 | Batch_idx: 340 |  Loss_1: (0.4114) | Acc_1: (85.63%) (37376/43648)\n",
      "Epoch: 27 | Batch_idx: 350 |  Loss_1: (0.4114) | Acc_1: (85.62%) (38466/44928)\n",
      "Epoch: 27 | Batch_idx: 360 |  Loss_1: (0.4117) | Acc_1: (85.60%) (39552/46208)\n",
      "Epoch: 27 | Batch_idx: 370 |  Loss_1: (0.4118) | Acc_1: (85.63%) (40663/47488)\n",
      "Epoch: 27 | Batch_idx: 380 |  Loss_1: (0.4131) | Acc_1: (85.60%) (41745/48768)\n",
      "Epoch: 27 | Batch_idx: 390 |  Loss_1: (0.4133) | Acc_1: (85.59%) (42796/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3793) | Acc: (87.41%) (8741/10000)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.4479) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.4033) | Acc_1: (85.80%) (1208/1408)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.4103) | Acc_1: (86.01%) (2312/2688)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.4027) | Acc_1: (86.37%) (3427/3968)\n",
      "Epoch: 28 | Batch_idx: 40 |  Loss_1: (0.3990) | Acc_1: (86.34%) (4531/5248)\n",
      "Epoch: 28 | Batch_idx: 50 |  Loss_1: (0.4022) | Acc_1: (86.34%) (5636/6528)\n",
      "Epoch: 28 | Batch_idx: 60 |  Loss_1: (0.3979) | Acc_1: (86.48%) (6752/7808)\n",
      "Epoch: 28 | Batch_idx: 70 |  Loss_1: (0.4002) | Acc_1: (86.38%) (7850/9088)\n",
      "Epoch: 28 | Batch_idx: 80 |  Loss_1: (0.3965) | Acc_1: (86.52%) (8970/10368)\n",
      "Epoch: 28 | Batch_idx: 90 |  Loss_1: (0.3993) | Acc_1: (86.42%) (10066/11648)\n",
      "Epoch: 28 | Batch_idx: 100 |  Loss_1: (0.4009) | Acc_1: (86.34%) (11162/12928)\n",
      "Epoch: 28 | Batch_idx: 110 |  Loss_1: (0.4046) | Acc_1: (86.16%) (12242/14208)\n",
      "Epoch: 28 | Batch_idx: 120 |  Loss_1: (0.4014) | Acc_1: (86.23%) (13356/15488)\n",
      "Epoch: 28 | Batch_idx: 130 |  Loss_1: (0.4011) | Acc_1: (86.24%) (14461/16768)\n",
      "Epoch: 28 | Batch_idx: 140 |  Loss_1: (0.4006) | Acc_1: (86.27%) (15570/18048)\n",
      "Epoch: 28 | Batch_idx: 150 |  Loss_1: (0.4012) | Acc_1: (86.21%) (16663/19328)\n",
      "Epoch: 28 | Batch_idx: 160 |  Loss_1: (0.4006) | Acc_1: (86.23%) (17771/20608)\n",
      "Epoch: 28 | Batch_idx: 170 |  Loss_1: (0.4032) | Acc_1: (86.10%) (18846/21888)\n",
      "Epoch: 28 | Batch_idx: 180 |  Loss_1: (0.4038) | Acc_1: (86.11%) (19949/23168)\n",
      "Epoch: 28 | Batch_idx: 190 |  Loss_1: (0.4034) | Acc_1: (86.10%) (21050/24448)\n",
      "Epoch: 28 | Batch_idx: 200 |  Loss_1: (0.4038) | Acc_1: (86.12%) (22157/25728)\n",
      "Epoch: 28 | Batch_idx: 210 |  Loss_1: (0.4045) | Acc_1: (86.10%) (23255/27008)\n",
      "Epoch: 28 | Batch_idx: 220 |  Loss_1: (0.4043) | Acc_1: (86.12%) (24362/28288)\n",
      "Epoch: 28 | Batch_idx: 230 |  Loss_1: (0.4038) | Acc_1: (86.13%) (25466/29568)\n",
      "Epoch: 28 | Batch_idx: 240 |  Loss_1: (0.4018) | Acc_1: (86.17%) (26582/30848)\n",
      "Epoch: 28 | Batch_idx: 250 |  Loss_1: (0.4009) | Acc_1: (86.21%) (27696/32128)\n",
      "Epoch: 28 | Batch_idx: 260 |  Loss_1: (0.3998) | Acc_1: (86.25%) (28815/33408)\n",
      "Epoch: 28 | Batch_idx: 270 |  Loss_1: (0.4009) | Acc_1: (86.21%) (29903/34688)\n",
      "Epoch: 28 | Batch_idx: 280 |  Loss_1: (0.4001) | Acc_1: (86.22%) (31013/35968)\n",
      "Epoch: 28 | Batch_idx: 290 |  Loss_1: (0.4014) | Acc_1: (86.19%) (32104/37248)\n",
      "Epoch: 28 | Batch_idx: 300 |  Loss_1: (0.4008) | Acc_1: (86.22%) (33219/38528)\n",
      "Epoch: 28 | Batch_idx: 310 |  Loss_1: (0.4011) | Acc_1: (86.17%) (34302/39808)\n",
      "Epoch: 28 | Batch_idx: 320 |  Loss_1: (0.4005) | Acc_1: (86.20%) (35417/41088)\n",
      "Epoch: 28 | Batch_idx: 330 |  Loss_1: (0.4004) | Acc_1: (86.19%) (36518/42368)\n",
      "Epoch: 28 | Batch_idx: 340 |  Loss_1: (0.3997) | Acc_1: (86.20%) (37626/43648)\n",
      "Epoch: 28 | Batch_idx: 350 |  Loss_1: (0.3989) | Acc_1: (86.24%) (38746/44928)\n",
      "Epoch: 28 | Batch_idx: 360 |  Loss_1: (0.4003) | Acc_1: (86.18%) (39822/46208)\n",
      "Epoch: 28 | Batch_idx: 370 |  Loss_1: (0.4000) | Acc_1: (86.22%) (40943/47488)\n",
      "Epoch: 28 | Batch_idx: 380 |  Loss_1: (0.4011) | Acc_1: (86.17%) (42025/48768)\n",
      "Epoch: 28 | Batch_idx: 390 |  Loss_1: (0.4017) | Acc_1: (86.15%) (43074/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3683) | Acc: (87.34%) (8734/10000)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.5807) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.4302) | Acc_1: (85.23%) (1200/1408)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.3944) | Acc_1: (86.38%) (2322/2688)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.3838) | Acc_1: (87.02%) (3453/3968)\n",
      "Epoch: 29 | Batch_idx: 40 |  Loss_1: (0.3844) | Acc_1: (86.81%) (4556/5248)\n",
      "Epoch: 29 | Batch_idx: 50 |  Loss_1: (0.3808) | Acc_1: (87.01%) (5680/6528)\n",
      "Epoch: 29 | Batch_idx: 60 |  Loss_1: (0.3812) | Acc_1: (86.95%) (6789/7808)\n",
      "Epoch: 29 | Batch_idx: 70 |  Loss_1: (0.3856) | Acc_1: (86.75%) (7884/9088)\n",
      "Epoch: 29 | Batch_idx: 80 |  Loss_1: (0.3862) | Acc_1: (86.64%) (8983/10368)\n",
      "Epoch: 29 | Batch_idx: 90 |  Loss_1: (0.3838) | Acc_1: (86.74%) (10103/11648)\n",
      "Epoch: 29 | Batch_idx: 100 |  Loss_1: (0.3893) | Acc_1: (86.51%) (11184/12928)\n",
      "Epoch: 29 | Batch_idx: 110 |  Loss_1: (0.3922) | Acc_1: (86.44%) (12281/14208)\n",
      "Epoch: 29 | Batch_idx: 120 |  Loss_1: (0.3923) | Acc_1: (86.42%) (13384/15488)\n",
      "Epoch: 29 | Batch_idx: 130 |  Loss_1: (0.3932) | Acc_1: (86.35%) (14480/16768)\n",
      "Epoch: 29 | Batch_idx: 140 |  Loss_1: (0.3910) | Acc_1: (86.48%) (15608/18048)\n",
      "Epoch: 29 | Batch_idx: 150 |  Loss_1: (0.3916) | Acc_1: (86.50%) (16718/19328)\n",
      "Epoch: 29 | Batch_idx: 160 |  Loss_1: (0.3921) | Acc_1: (86.41%) (17807/20608)\n",
      "Epoch: 29 | Batch_idx: 170 |  Loss_1: (0.3903) | Acc_1: (86.44%) (18921/21888)\n",
      "Epoch: 29 | Batch_idx: 180 |  Loss_1: (0.3904) | Acc_1: (86.44%) (20026/23168)\n",
      "Epoch: 29 | Batch_idx: 190 |  Loss_1: (0.3912) | Acc_1: (86.46%) (21137/24448)\n",
      "Epoch: 29 | Batch_idx: 200 |  Loss_1: (0.3914) | Acc_1: (86.44%) (22239/25728)\n",
      "Epoch: 29 | Batch_idx: 210 |  Loss_1: (0.3896) | Acc_1: (86.53%) (23370/27008)\n",
      "Epoch: 29 | Batch_idx: 220 |  Loss_1: (0.3901) | Acc_1: (86.52%) (24474/28288)\n",
      "Epoch: 29 | Batch_idx: 230 |  Loss_1: (0.3884) | Acc_1: (86.55%) (25591/29568)\n",
      "Epoch: 29 | Batch_idx: 240 |  Loss_1: (0.3896) | Acc_1: (86.52%) (26690/30848)\n",
      "Epoch: 29 | Batch_idx: 250 |  Loss_1: (0.3895) | Acc_1: (86.54%) (27804/32128)\n",
      "Epoch: 29 | Batch_idx: 260 |  Loss_1: (0.3917) | Acc_1: (86.46%) (28883/33408)\n",
      "Epoch: 29 | Batch_idx: 270 |  Loss_1: (0.3918) | Acc_1: (86.44%) (29986/34688)\n",
      "Epoch: 29 | Batch_idx: 280 |  Loss_1: (0.3919) | Acc_1: (86.46%) (31097/35968)\n",
      "Epoch: 29 | Batch_idx: 290 |  Loss_1: (0.3915) | Acc_1: (86.49%) (32216/37248)\n",
      "Epoch: 29 | Batch_idx: 300 |  Loss_1: (0.3926) | Acc_1: (86.47%) (33317/38528)\n",
      "Epoch: 29 | Batch_idx: 310 |  Loss_1: (0.3923) | Acc_1: (86.47%) (34423/39808)\n",
      "Epoch: 29 | Batch_idx: 320 |  Loss_1: (0.3925) | Acc_1: (86.48%) (35532/41088)\n",
      "Epoch: 29 | Batch_idx: 330 |  Loss_1: (0.3935) | Acc_1: (86.44%) (36624/42368)\n",
      "Epoch: 29 | Batch_idx: 340 |  Loss_1: (0.3923) | Acc_1: (86.49%) (37750/43648)\n",
      "Epoch: 29 | Batch_idx: 350 |  Loss_1: (0.3924) | Acc_1: (86.49%) (38857/44928)\n",
      "Epoch: 29 | Batch_idx: 360 |  Loss_1: (0.3925) | Acc_1: (86.50%) (39970/46208)\n",
      "Epoch: 29 | Batch_idx: 370 |  Loss_1: (0.3929) | Acc_1: (86.48%) (41066/47488)\n",
      "Epoch: 29 | Batch_idx: 380 |  Loss_1: (0.3935) | Acc_1: (86.46%) (42165/48768)\n",
      "Epoch: 29 | Batch_idx: 390 |  Loss_1: (0.3935) | Acc_1: (86.47%) (43235/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4130) | Acc: (86.12%) (8612/10000)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.4657) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.3755) | Acc_1: (86.72%) (1221/1408)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.3901) | Acc_1: (86.09%) (2314/2688)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.3835) | Acc_1: (86.27%) (3423/3968)\n",
      "Epoch: 30 | Batch_idx: 40 |  Loss_1: (0.3792) | Acc_1: (86.59%) (4544/5248)\n",
      "Epoch: 30 | Batch_idx: 50 |  Loss_1: (0.3873) | Acc_1: (86.47%) (5645/6528)\n",
      "Epoch: 30 | Batch_idx: 60 |  Loss_1: (0.3873) | Acc_1: (86.55%) (6758/7808)\n",
      "Epoch: 30 | Batch_idx: 70 |  Loss_1: (0.3884) | Acc_1: (86.63%) (7873/9088)\n",
      "Epoch: 30 | Batch_idx: 80 |  Loss_1: (0.3967) | Acc_1: (86.28%) (8945/10368)\n",
      "Epoch: 30 | Batch_idx: 90 |  Loss_1: (0.3947) | Acc_1: (86.26%) (10048/11648)\n",
      "Epoch: 30 | Batch_idx: 100 |  Loss_1: (0.3917) | Acc_1: (86.39%) (11168/12928)\n",
      "Epoch: 30 | Batch_idx: 110 |  Loss_1: (0.3964) | Acc_1: (86.14%) (12239/14208)\n",
      "Epoch: 30 | Batch_idx: 120 |  Loss_1: (0.3939) | Acc_1: (86.22%) (13353/15488)\n",
      "Epoch: 30 | Batch_idx: 130 |  Loss_1: (0.3920) | Acc_1: (86.28%) (14468/16768)\n",
      "Epoch: 30 | Batch_idx: 140 |  Loss_1: (0.3881) | Acc_1: (86.40%) (15593/18048)\n",
      "Epoch: 30 | Batch_idx: 150 |  Loss_1: (0.3884) | Acc_1: (86.39%) (16697/19328)\n",
      "Epoch: 30 | Batch_idx: 160 |  Loss_1: (0.3896) | Acc_1: (86.36%) (17798/20608)\n",
      "Epoch: 30 | Batch_idx: 170 |  Loss_1: (0.3902) | Acc_1: (86.32%) (18893/21888)\n",
      "Epoch: 30 | Batch_idx: 180 |  Loss_1: (0.3874) | Acc_1: (86.40%) (20017/23168)\n",
      "Epoch: 30 | Batch_idx: 190 |  Loss_1: (0.3870) | Acc_1: (86.42%) (21127/24448)\n",
      "Epoch: 30 | Batch_idx: 200 |  Loss_1: (0.3866) | Acc_1: (86.45%) (22241/25728)\n",
      "Epoch: 30 | Batch_idx: 210 |  Loss_1: (0.3859) | Acc_1: (86.49%) (23360/27008)\n",
      "Epoch: 30 | Batch_idx: 220 |  Loss_1: (0.3860) | Acc_1: (86.44%) (24452/28288)\n",
      "Epoch: 30 | Batch_idx: 230 |  Loss_1: (0.3851) | Acc_1: (86.44%) (25559/29568)\n",
      "Epoch: 30 | Batch_idx: 240 |  Loss_1: (0.3838) | Acc_1: (86.50%) (26684/30848)\n",
      "Epoch: 30 | Batch_idx: 250 |  Loss_1: (0.3843) | Acc_1: (86.48%) (27783/32128)\n",
      "Epoch: 30 | Batch_idx: 260 |  Loss_1: (0.3840) | Acc_1: (86.49%) (28896/33408)\n",
      "Epoch: 30 | Batch_idx: 270 |  Loss_1: (0.3849) | Acc_1: (86.45%) (29989/34688)\n",
      "Epoch: 30 | Batch_idx: 280 |  Loss_1: (0.3851) | Acc_1: (86.44%) (31091/35968)\n",
      "Epoch: 30 | Batch_idx: 290 |  Loss_1: (0.3853) | Acc_1: (86.45%) (32202/37248)\n",
      "Epoch: 30 | Batch_idx: 300 |  Loss_1: (0.3845) | Acc_1: (86.49%) (33321/38528)\n",
      "Epoch: 30 | Batch_idx: 310 |  Loss_1: (0.3867) | Acc_1: (86.44%) (34411/39808)\n",
      "Epoch: 30 | Batch_idx: 320 |  Loss_1: (0.3874) | Acc_1: (86.44%) (35516/41088)\n",
      "Epoch: 30 | Batch_idx: 330 |  Loss_1: (0.3874) | Acc_1: (86.43%) (36618/42368)\n",
      "Epoch: 30 | Batch_idx: 340 |  Loss_1: (0.3878) | Acc_1: (86.44%) (37731/43648)\n",
      "Epoch: 30 | Batch_idx: 350 |  Loss_1: (0.3882) | Acc_1: (86.43%) (38830/44928)\n",
      "Epoch: 30 | Batch_idx: 360 |  Loss_1: (0.3887) | Acc_1: (86.42%) (39933/46208)\n",
      "Epoch: 30 | Batch_idx: 370 |  Loss_1: (0.3888) | Acc_1: (86.43%) (41042/47488)\n",
      "Epoch: 30 | Batch_idx: 380 |  Loss_1: (0.3894) | Acc_1: (86.42%) (42147/48768)\n",
      "Epoch: 30 | Batch_idx: 390 |  Loss_1: (0.3890) | Acc_1: (86.44%) (43222/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3706) | Acc: (87.45%) (8745/10000)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.3716) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.3542) | Acc_1: (87.71%) (1235/1408)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.3680) | Acc_1: (87.28%) (2346/2688)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.3667) | Acc_1: (87.80%) (3484/3968)\n",
      "Epoch: 31 | Batch_idx: 40 |  Loss_1: (0.3661) | Acc_1: (87.54%) (4594/5248)\n",
      "Epoch: 31 | Batch_idx: 50 |  Loss_1: (0.3629) | Acc_1: (87.55%) (5715/6528)\n",
      "Epoch: 31 | Batch_idx: 60 |  Loss_1: (0.3610) | Acc_1: (87.67%) (6845/7808)\n",
      "Epoch: 31 | Batch_idx: 70 |  Loss_1: (0.3648) | Acc_1: (87.52%) (7954/9088)\n",
      "Epoch: 31 | Batch_idx: 80 |  Loss_1: (0.3661) | Acc_1: (87.39%) (9061/10368)\n",
      "Epoch: 31 | Batch_idx: 90 |  Loss_1: (0.3715) | Acc_1: (87.11%) (10146/11648)\n",
      "Epoch: 31 | Batch_idx: 100 |  Loss_1: (0.3700) | Acc_1: (87.14%) (11266/12928)\n",
      "Epoch: 31 | Batch_idx: 110 |  Loss_1: (0.3697) | Acc_1: (87.12%) (12378/14208)\n",
      "Epoch: 31 | Batch_idx: 120 |  Loss_1: (0.3688) | Acc_1: (87.12%) (13493/15488)\n",
      "Epoch: 31 | Batch_idx: 130 |  Loss_1: (0.3685) | Acc_1: (87.21%) (14623/16768)\n",
      "Epoch: 31 | Batch_idx: 140 |  Loss_1: (0.3689) | Acc_1: (87.15%) (15729/18048)\n",
      "Epoch: 31 | Batch_idx: 150 |  Loss_1: (0.3698) | Acc_1: (87.11%) (16836/19328)\n",
      "Epoch: 31 | Batch_idx: 160 |  Loss_1: (0.3705) | Acc_1: (87.11%) (17951/20608)\n",
      "Epoch: 31 | Batch_idx: 170 |  Loss_1: (0.3708) | Acc_1: (87.05%) (19053/21888)\n",
      "Epoch: 31 | Batch_idx: 180 |  Loss_1: (0.3710) | Acc_1: (87.07%) (20172/23168)\n",
      "Epoch: 31 | Batch_idx: 190 |  Loss_1: (0.3718) | Acc_1: (87.07%) (21287/24448)\n",
      "Epoch: 31 | Batch_idx: 200 |  Loss_1: (0.3740) | Acc_1: (86.94%) (22367/25728)\n",
      "Epoch: 31 | Batch_idx: 210 |  Loss_1: (0.3754) | Acc_1: (86.90%) (23471/27008)\n",
      "Epoch: 31 | Batch_idx: 220 |  Loss_1: (0.3742) | Acc_1: (86.94%) (24595/28288)\n",
      "Epoch: 31 | Batch_idx: 230 |  Loss_1: (0.3759) | Acc_1: (86.84%) (25678/29568)\n",
      "Epoch: 31 | Batch_idx: 240 |  Loss_1: (0.3768) | Acc_1: (86.81%) (26778/30848)\n",
      "Epoch: 31 | Batch_idx: 250 |  Loss_1: (0.3773) | Acc_1: (86.80%) (27887/32128)\n",
      "Epoch: 31 | Batch_idx: 260 |  Loss_1: (0.3767) | Acc_1: (86.80%) (28998/33408)\n",
      "Epoch: 31 | Batch_idx: 270 |  Loss_1: (0.3767) | Acc_1: (86.81%) (30111/34688)\n",
      "Epoch: 31 | Batch_idx: 280 |  Loss_1: (0.3785) | Acc_1: (86.76%) (31205/35968)\n",
      "Epoch: 31 | Batch_idx: 290 |  Loss_1: (0.3778) | Acc_1: (86.77%) (32320/37248)\n",
      "Epoch: 31 | Batch_idx: 300 |  Loss_1: (0.3786) | Acc_1: (86.73%) (33414/38528)\n",
      "Epoch: 31 | Batch_idx: 310 |  Loss_1: (0.3791) | Acc_1: (86.73%) (34525/39808)\n",
      "Epoch: 31 | Batch_idx: 320 |  Loss_1: (0.3794) | Acc_1: (86.74%) (35641/41088)\n",
      "Epoch: 31 | Batch_idx: 330 |  Loss_1: (0.3790) | Acc_1: (86.74%) (36748/42368)\n",
      "Epoch: 31 | Batch_idx: 340 |  Loss_1: (0.3789) | Acc_1: (86.75%) (37864/43648)\n",
      "Epoch: 31 | Batch_idx: 350 |  Loss_1: (0.3783) | Acc_1: (86.80%) (38999/44928)\n",
      "Epoch: 31 | Batch_idx: 360 |  Loss_1: (0.3783) | Acc_1: (86.80%) (40108/46208)\n",
      "Epoch: 31 | Batch_idx: 370 |  Loss_1: (0.3777) | Acc_1: (86.82%) (41227/47488)\n",
      "Epoch: 31 | Batch_idx: 380 |  Loss_1: (0.3772) | Acc_1: (86.81%) (42335/48768)\n",
      "Epoch: 31 | Batch_idx: 390 |  Loss_1: (0.3777) | Acc_1: (86.79%) (43397/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3588) | Acc: (87.92%) (8792/10000)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.4392) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.3357) | Acc_1: (88.14%) (1241/1408)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.3457) | Acc_1: (87.91%) (2363/2688)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.3575) | Acc_1: (87.32%) (3465/3968)\n",
      "Epoch: 32 | Batch_idx: 40 |  Loss_1: (0.3565) | Acc_1: (87.44%) (4589/5248)\n",
      "Epoch: 32 | Batch_idx: 50 |  Loss_1: (0.3591) | Acc_1: (87.38%) (5704/6528)\n",
      "Epoch: 32 | Batch_idx: 60 |  Loss_1: (0.3665) | Acc_1: (87.30%) (6816/7808)\n",
      "Epoch: 32 | Batch_idx: 70 |  Loss_1: (0.3641) | Acc_1: (87.39%) (7942/9088)\n",
      "Epoch: 32 | Batch_idx: 80 |  Loss_1: (0.3598) | Acc_1: (87.53%) (9075/10368)\n",
      "Epoch: 32 | Batch_idx: 90 |  Loss_1: (0.3594) | Acc_1: (87.47%) (10188/11648)\n",
      "Epoch: 32 | Batch_idx: 100 |  Loss_1: (0.3602) | Acc_1: (87.50%) (11312/12928)\n",
      "Epoch: 32 | Batch_idx: 110 |  Loss_1: (0.3605) | Acc_1: (87.47%) (12428/14208)\n",
      "Epoch: 32 | Batch_idx: 120 |  Loss_1: (0.3605) | Acc_1: (87.47%) (13548/15488)\n",
      "Epoch: 32 | Batch_idx: 130 |  Loss_1: (0.3636) | Acc_1: (87.51%) (14674/16768)\n",
      "Epoch: 32 | Batch_idx: 140 |  Loss_1: (0.3628) | Acc_1: (87.53%) (15798/18048)\n",
      "Epoch: 32 | Batch_idx: 150 |  Loss_1: (0.3622) | Acc_1: (87.51%) (16914/19328)\n",
      "Epoch: 32 | Batch_idx: 160 |  Loss_1: (0.3599) | Acc_1: (87.59%) (18051/20608)\n",
      "Epoch: 32 | Batch_idx: 170 |  Loss_1: (0.3601) | Acc_1: (87.58%) (19169/21888)\n",
      "Epoch: 32 | Batch_idx: 180 |  Loss_1: (0.3596) | Acc_1: (87.57%) (20289/23168)\n",
      "Epoch: 32 | Batch_idx: 190 |  Loss_1: (0.3571) | Acc_1: (87.65%) (21428/24448)\n",
      "Epoch: 32 | Batch_idx: 200 |  Loss_1: (0.3557) | Acc_1: (87.64%) (22549/25728)\n",
      "Epoch: 32 | Batch_idx: 210 |  Loss_1: (0.3543) | Acc_1: (87.71%) (23690/27008)\n",
      "Epoch: 32 | Batch_idx: 220 |  Loss_1: (0.3551) | Acc_1: (87.66%) (24797/28288)\n",
      "Epoch: 32 | Batch_idx: 230 |  Loss_1: (0.3562) | Acc_1: (87.60%) (25902/29568)\n",
      "Epoch: 32 | Batch_idx: 240 |  Loss_1: (0.3567) | Acc_1: (87.60%) (27024/30848)\n",
      "Epoch: 32 | Batch_idx: 250 |  Loss_1: (0.3572) | Acc_1: (87.56%) (28131/32128)\n",
      "Epoch: 32 | Batch_idx: 260 |  Loss_1: (0.3594) | Acc_1: (87.49%) (29228/33408)\n",
      "Epoch: 32 | Batch_idx: 270 |  Loss_1: (0.3597) | Acc_1: (87.47%) (30342/34688)\n",
      "Epoch: 32 | Batch_idx: 280 |  Loss_1: (0.3612) | Acc_1: (87.38%) (31430/35968)\n",
      "Epoch: 32 | Batch_idx: 290 |  Loss_1: (0.3616) | Acc_1: (87.36%) (32540/37248)\n",
      "Epoch: 32 | Batch_idx: 300 |  Loss_1: (0.3620) | Acc_1: (87.34%) (33651/38528)\n",
      "Epoch: 32 | Batch_idx: 310 |  Loss_1: (0.3632) | Acc_1: (87.31%) (34757/39808)\n",
      "Epoch: 32 | Batch_idx: 320 |  Loss_1: (0.3643) | Acc_1: (87.27%) (35858/41088)\n",
      "Epoch: 32 | Batch_idx: 330 |  Loss_1: (0.3642) | Acc_1: (87.26%) (36972/42368)\n",
      "Epoch: 32 | Batch_idx: 340 |  Loss_1: (0.3645) | Acc_1: (87.25%) (38084/43648)\n",
      "Epoch: 32 | Batch_idx: 350 |  Loss_1: (0.3653) | Acc_1: (87.23%) (39192/44928)\n",
      "Epoch: 32 | Batch_idx: 360 |  Loss_1: (0.3657) | Acc_1: (87.23%) (40309/46208)\n",
      "Epoch: 32 | Batch_idx: 370 |  Loss_1: (0.3650) | Acc_1: (87.26%) (41437/47488)\n",
      "Epoch: 32 | Batch_idx: 380 |  Loss_1: (0.3645) | Acc_1: (87.26%) (42556/48768)\n",
      "Epoch: 32 | Batch_idx: 390 |  Loss_1: (0.3640) | Acc_1: (87.26%) (43630/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3803) | Acc: (86.96%) (8696/10000)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.5513) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.4025) | Acc_1: (86.01%) (1211/1408)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.3754) | Acc_1: (86.64%) (2329/2688)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.3648) | Acc_1: (87.10%) (3456/3968)\n",
      "Epoch: 33 | Batch_idx: 40 |  Loss_1: (0.3590) | Acc_1: (87.37%) (4585/5248)\n",
      "Epoch: 33 | Batch_idx: 50 |  Loss_1: (0.3645) | Acc_1: (87.30%) (5699/6528)\n",
      "Epoch: 33 | Batch_idx: 60 |  Loss_1: (0.3675) | Acc_1: (87.24%) (6812/7808)\n",
      "Epoch: 33 | Batch_idx: 70 |  Loss_1: (0.3674) | Acc_1: (87.26%) (7930/9088)\n",
      "Epoch: 33 | Batch_idx: 80 |  Loss_1: (0.3650) | Acc_1: (87.32%) (9053/10368)\n",
      "Epoch: 33 | Batch_idx: 90 |  Loss_1: (0.3627) | Acc_1: (87.41%) (10181/11648)\n",
      "Epoch: 33 | Batch_idx: 100 |  Loss_1: (0.3602) | Acc_1: (87.51%) (11313/12928)\n",
      "Epoch: 33 | Batch_idx: 110 |  Loss_1: (0.3602) | Acc_1: (87.58%) (12443/14208)\n",
      "Epoch: 33 | Batch_idx: 120 |  Loss_1: (0.3632) | Acc_1: (87.51%) (13553/15488)\n",
      "Epoch: 33 | Batch_idx: 130 |  Loss_1: (0.3635) | Acc_1: (87.44%) (14662/16768)\n",
      "Epoch: 33 | Batch_idx: 140 |  Loss_1: (0.3622) | Acc_1: (87.44%) (15781/18048)\n",
      "Epoch: 33 | Batch_idx: 150 |  Loss_1: (0.3625) | Acc_1: (87.44%) (16900/19328)\n",
      "Epoch: 33 | Batch_idx: 160 |  Loss_1: (0.3632) | Acc_1: (87.37%) (18005/20608)\n",
      "Epoch: 33 | Batch_idx: 170 |  Loss_1: (0.3609) | Acc_1: (87.39%) (19129/21888)\n",
      "Epoch: 33 | Batch_idx: 180 |  Loss_1: (0.3602) | Acc_1: (87.43%) (20256/23168)\n",
      "Epoch: 33 | Batch_idx: 190 |  Loss_1: (0.3599) | Acc_1: (87.45%) (21380/24448)\n",
      "Epoch: 33 | Batch_idx: 200 |  Loss_1: (0.3616) | Acc_1: (87.40%) (22487/25728)\n",
      "Epoch: 33 | Batch_idx: 210 |  Loss_1: (0.3625) | Acc_1: (87.36%) (23594/27008)\n",
      "Epoch: 33 | Batch_idx: 220 |  Loss_1: (0.3607) | Acc_1: (87.42%) (24728/28288)\n",
      "Epoch: 33 | Batch_idx: 230 |  Loss_1: (0.3624) | Acc_1: (87.33%) (25822/29568)\n",
      "Epoch: 33 | Batch_idx: 240 |  Loss_1: (0.3645) | Acc_1: (87.28%) (26923/30848)\n",
      "Epoch: 33 | Batch_idx: 250 |  Loss_1: (0.3636) | Acc_1: (87.29%) (28045/32128)\n",
      "Epoch: 33 | Batch_idx: 260 |  Loss_1: (0.3632) | Acc_1: (87.28%) (29159/33408)\n",
      "Epoch: 33 | Batch_idx: 270 |  Loss_1: (0.3631) | Acc_1: (87.29%) (30278/34688)\n",
      "Epoch: 33 | Batch_idx: 280 |  Loss_1: (0.3627) | Acc_1: (87.32%) (31406/35968)\n",
      "Epoch: 33 | Batch_idx: 290 |  Loss_1: (0.3633) | Acc_1: (87.27%) (32508/37248)\n",
      "Epoch: 33 | Batch_idx: 300 |  Loss_1: (0.3627) | Acc_1: (87.28%) (33629/38528)\n",
      "Epoch: 33 | Batch_idx: 310 |  Loss_1: (0.3617) | Acc_1: (87.33%) (34766/39808)\n",
      "Epoch: 33 | Batch_idx: 320 |  Loss_1: (0.3623) | Acc_1: (87.33%) (35881/41088)\n",
      "Epoch: 33 | Batch_idx: 330 |  Loss_1: (0.3623) | Acc_1: (87.31%) (36993/42368)\n",
      "Epoch: 33 | Batch_idx: 340 |  Loss_1: (0.3629) | Acc_1: (87.31%) (38109/43648)\n",
      "Epoch: 33 | Batch_idx: 350 |  Loss_1: (0.3627) | Acc_1: (87.30%) (39222/44928)\n",
      "Epoch: 33 | Batch_idx: 360 |  Loss_1: (0.3620) | Acc_1: (87.34%) (40357/46208)\n",
      "Epoch: 33 | Batch_idx: 370 |  Loss_1: (0.3620) | Acc_1: (87.35%) (41483/47488)\n",
      "Epoch: 33 | Batch_idx: 380 |  Loss_1: (0.3624) | Acc_1: (87.35%) (42600/48768)\n",
      "Epoch: 33 | Batch_idx: 390 |  Loss_1: (0.3618) | Acc_1: (87.35%) (43675/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3282) | Acc: (89.09%) (8909/10000)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.3162) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.3213) | Acc_1: (89.63%) (1262/1408)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.3437) | Acc_1: (87.98%) (2365/2688)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.3440) | Acc_1: (87.83%) (3485/3968)\n",
      "Epoch: 34 | Batch_idx: 40 |  Loss_1: (0.3484) | Acc_1: (87.58%) (4596/5248)\n",
      "Epoch: 34 | Batch_idx: 50 |  Loss_1: (0.3489) | Acc_1: (87.50%) (5712/6528)\n",
      "Epoch: 34 | Batch_idx: 60 |  Loss_1: (0.3546) | Acc_1: (87.36%) (6821/7808)\n",
      "Epoch: 34 | Batch_idx: 70 |  Loss_1: (0.3566) | Acc_1: (87.44%) (7947/9088)\n",
      "Epoch: 34 | Batch_idx: 80 |  Loss_1: (0.3583) | Acc_1: (87.40%) (9062/10368)\n",
      "Epoch: 34 | Batch_idx: 90 |  Loss_1: (0.3562) | Acc_1: (87.48%) (10190/11648)\n",
      "Epoch: 34 | Batch_idx: 100 |  Loss_1: (0.3552) | Acc_1: (87.45%) (11305/12928)\n",
      "Epoch: 34 | Batch_idx: 110 |  Loss_1: (0.3541) | Acc_1: (87.48%) (12429/14208)\n",
      "Epoch: 34 | Batch_idx: 120 |  Loss_1: (0.3517) | Acc_1: (87.53%) (13557/15488)\n",
      "Epoch: 34 | Batch_idx: 130 |  Loss_1: (0.3493) | Acc_1: (87.67%) (14700/16768)\n",
      "Epoch: 34 | Batch_idx: 140 |  Loss_1: (0.3455) | Acc_1: (87.78%) (15842/18048)\n",
      "Epoch: 34 | Batch_idx: 150 |  Loss_1: (0.3444) | Acc_1: (87.85%) (16980/19328)\n",
      "Epoch: 34 | Batch_idx: 160 |  Loss_1: (0.3433) | Acc_1: (87.88%) (18110/20608)\n",
      "Epoch: 34 | Batch_idx: 170 |  Loss_1: (0.3433) | Acc_1: (87.97%) (19255/21888)\n",
      "Epoch: 34 | Batch_idx: 180 |  Loss_1: (0.3463) | Acc_1: (87.87%) (20358/23168)\n",
      "Epoch: 34 | Batch_idx: 190 |  Loss_1: (0.3464) | Acc_1: (87.91%) (21492/24448)\n",
      "Epoch: 34 | Batch_idx: 200 |  Loss_1: (0.3473) | Acc_1: (87.88%) (22610/25728)\n",
      "Epoch: 34 | Batch_idx: 210 |  Loss_1: (0.3466) | Acc_1: (87.87%) (23733/27008)\n",
      "Epoch: 34 | Batch_idx: 220 |  Loss_1: (0.3470) | Acc_1: (87.84%) (24848/28288)\n",
      "Epoch: 34 | Batch_idx: 230 |  Loss_1: (0.3508) | Acc_1: (87.70%) (25932/29568)\n",
      "Epoch: 34 | Batch_idx: 240 |  Loss_1: (0.3511) | Acc_1: (87.70%) (27055/30848)\n",
      "Epoch: 34 | Batch_idx: 250 |  Loss_1: (0.3521) | Acc_1: (87.71%) (28181/32128)\n",
      "Epoch: 34 | Batch_idx: 260 |  Loss_1: (0.3512) | Acc_1: (87.75%) (29317/33408)\n",
      "Epoch: 34 | Batch_idx: 270 |  Loss_1: (0.3516) | Acc_1: (87.74%) (30436/34688)\n",
      "Epoch: 34 | Batch_idx: 280 |  Loss_1: (0.3519) | Acc_1: (87.71%) (31549/35968)\n",
      "Epoch: 34 | Batch_idx: 290 |  Loss_1: (0.3516) | Acc_1: (87.74%) (32683/37248)\n",
      "Epoch: 34 | Batch_idx: 300 |  Loss_1: (0.3506) | Acc_1: (87.75%) (33808/38528)\n",
      "Epoch: 34 | Batch_idx: 310 |  Loss_1: (0.3514) | Acc_1: (87.72%) (34918/39808)\n",
      "Epoch: 34 | Batch_idx: 320 |  Loss_1: (0.3510) | Acc_1: (87.73%) (36046/41088)\n",
      "Epoch: 34 | Batch_idx: 330 |  Loss_1: (0.3505) | Acc_1: (87.74%) (37174/42368)\n",
      "Epoch: 34 | Batch_idx: 340 |  Loss_1: (0.3507) | Acc_1: (87.72%) (38287/43648)\n",
      "Epoch: 34 | Batch_idx: 350 |  Loss_1: (0.3504) | Acc_1: (87.71%) (39406/44928)\n",
      "Epoch: 34 | Batch_idx: 360 |  Loss_1: (0.3488) | Acc_1: (87.77%) (40555/46208)\n",
      "Epoch: 34 | Batch_idx: 370 |  Loss_1: (0.3493) | Acc_1: (87.75%) (41671/47488)\n",
      "Epoch: 34 | Batch_idx: 380 |  Loss_1: (0.3493) | Acc_1: (87.74%) (42791/48768)\n",
      "Epoch: 34 | Batch_idx: 390 |  Loss_1: (0.3496) | Acc_1: (87.73%) (43867/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3573) | Acc: (88.02%) (8802/10000)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.3180) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.3419) | Acc_1: (87.78%) (1236/1408)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.3486) | Acc_1: (87.69%) (2357/2688)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.3503) | Acc_1: (87.53%) (3473/3968)\n",
      "Epoch: 35 | Batch_idx: 40 |  Loss_1: (0.3407) | Acc_1: (87.88%) (4612/5248)\n",
      "Epoch: 35 | Batch_idx: 50 |  Loss_1: (0.3316) | Acc_1: (88.43%) (5773/6528)\n",
      "Epoch: 35 | Batch_idx: 60 |  Loss_1: (0.3352) | Acc_1: (88.29%) (6894/7808)\n",
      "Epoch: 35 | Batch_idx: 70 |  Loss_1: (0.3362) | Acc_1: (88.17%) (8013/9088)\n",
      "Epoch: 35 | Batch_idx: 80 |  Loss_1: (0.3400) | Acc_1: (87.96%) (9120/10368)\n",
      "Epoch: 35 | Batch_idx: 90 |  Loss_1: (0.3379) | Acc_1: (88.03%) (10254/11648)\n",
      "Epoch: 35 | Batch_idx: 100 |  Loss_1: (0.3383) | Acc_1: (87.97%) (11373/12928)\n",
      "Epoch: 35 | Batch_idx: 110 |  Loss_1: (0.3389) | Acc_1: (87.97%) (12499/14208)\n",
      "Epoch: 35 | Batch_idx: 120 |  Loss_1: (0.3401) | Acc_1: (87.97%) (13625/15488)\n",
      "Epoch: 35 | Batch_idx: 130 |  Loss_1: (0.3403) | Acc_1: (87.98%) (14752/16768)\n",
      "Epoch: 35 | Batch_idx: 140 |  Loss_1: (0.3402) | Acc_1: (88.00%) (15882/18048)\n",
      "Epoch: 35 | Batch_idx: 150 |  Loss_1: (0.3425) | Acc_1: (87.94%) (16997/19328)\n",
      "Epoch: 35 | Batch_idx: 160 |  Loss_1: (0.3418) | Acc_1: (87.97%) (18128/20608)\n",
      "Epoch: 35 | Batch_idx: 170 |  Loss_1: (0.3387) | Acc_1: (88.10%) (19284/21888)\n",
      "Epoch: 35 | Batch_idx: 180 |  Loss_1: (0.3397) | Acc_1: (88.05%) (20399/23168)\n",
      "Epoch: 35 | Batch_idx: 190 |  Loss_1: (0.3376) | Acc_1: (88.09%) (21536/24448)\n",
      "Epoch: 35 | Batch_idx: 200 |  Loss_1: (0.3387) | Acc_1: (88.08%) (22661/25728)\n",
      "Epoch: 35 | Batch_idx: 210 |  Loss_1: (0.3396) | Acc_1: (88.05%) (23781/27008)\n",
      "Epoch: 35 | Batch_idx: 220 |  Loss_1: (0.3393) | Acc_1: (88.07%) (24912/28288)\n",
      "Epoch: 35 | Batch_idx: 230 |  Loss_1: (0.3387) | Acc_1: (88.07%) (26041/29568)\n",
      "Epoch: 35 | Batch_idx: 240 |  Loss_1: (0.3416) | Acc_1: (87.97%) (27138/30848)\n",
      "Epoch: 35 | Batch_idx: 250 |  Loss_1: (0.3408) | Acc_1: (87.99%) (28270/32128)\n",
      "Epoch: 35 | Batch_idx: 260 |  Loss_1: (0.3403) | Acc_1: (88.00%) (29398/33408)\n",
      "Epoch: 35 | Batch_idx: 270 |  Loss_1: (0.3394) | Acc_1: (88.04%) (30539/34688)\n",
      "Epoch: 35 | Batch_idx: 280 |  Loss_1: (0.3393) | Acc_1: (88.04%) (31668/35968)\n",
      "Epoch: 35 | Batch_idx: 290 |  Loss_1: (0.3398) | Acc_1: (88.03%) (32789/37248)\n",
      "Epoch: 35 | Batch_idx: 300 |  Loss_1: (0.3402) | Acc_1: (88.00%) (33904/38528)\n",
      "Epoch: 35 | Batch_idx: 310 |  Loss_1: (0.3420) | Acc_1: (87.95%) (35010/39808)\n",
      "Epoch: 35 | Batch_idx: 320 |  Loss_1: (0.3422) | Acc_1: (87.94%) (36133/41088)\n",
      "Epoch: 35 | Batch_idx: 330 |  Loss_1: (0.3435) | Acc_1: (87.89%) (37237/42368)\n",
      "Epoch: 35 | Batch_idx: 340 |  Loss_1: (0.3431) | Acc_1: (87.90%) (38367/43648)\n",
      "Epoch: 35 | Batch_idx: 350 |  Loss_1: (0.3440) | Acc_1: (87.89%) (39488/44928)\n",
      "Epoch: 35 | Batch_idx: 360 |  Loss_1: (0.3448) | Acc_1: (87.85%) (40593/46208)\n",
      "Epoch: 35 | Batch_idx: 370 |  Loss_1: (0.3465) | Acc_1: (87.78%) (41686/47488)\n",
      "Epoch: 35 | Batch_idx: 380 |  Loss_1: (0.3470) | Acc_1: (87.75%) (42796/48768)\n",
      "Epoch: 35 | Batch_idx: 390 |  Loss_1: (0.3482) | Acc_1: (87.73%) (43867/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3542) | Acc: (87.98%) (8798/10000)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.3151) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.3206) | Acc_1: (88.78%) (1250/1408)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.3113) | Acc_1: (89.17%) (2397/2688)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.3089) | Acc_1: (89.31%) (3544/3968)\n",
      "Epoch: 36 | Batch_idx: 40 |  Loss_1: (0.3063) | Acc_1: (89.31%) (4687/5248)\n",
      "Epoch: 36 | Batch_idx: 50 |  Loss_1: (0.3105) | Acc_1: (89.25%) (5826/6528)\n",
      "Epoch: 36 | Batch_idx: 60 |  Loss_1: (0.3129) | Acc_1: (89.19%) (6964/7808)\n",
      "Epoch: 36 | Batch_idx: 70 |  Loss_1: (0.3172) | Acc_1: (89.06%) (8094/9088)\n",
      "Epoch: 36 | Batch_idx: 80 |  Loss_1: (0.3261) | Acc_1: (88.81%) (9208/10368)\n",
      "Epoch: 36 | Batch_idx: 90 |  Loss_1: (0.3252) | Acc_1: (88.81%) (10345/11648)\n",
      "Epoch: 36 | Batch_idx: 100 |  Loss_1: (0.3286) | Acc_1: (88.58%) (11451/12928)\n",
      "Epoch: 36 | Batch_idx: 110 |  Loss_1: (0.3315) | Acc_1: (88.50%) (12574/14208)\n",
      "Epoch: 36 | Batch_idx: 120 |  Loss_1: (0.3337) | Acc_1: (88.44%) (13698/15488)\n",
      "Epoch: 36 | Batch_idx: 130 |  Loss_1: (0.3318) | Acc_1: (88.45%) (14832/16768)\n",
      "Epoch: 36 | Batch_idx: 140 |  Loss_1: (0.3356) | Acc_1: (88.39%) (15953/18048)\n",
      "Epoch: 36 | Batch_idx: 150 |  Loss_1: (0.3340) | Acc_1: (88.49%) (17103/19328)\n",
      "Epoch: 36 | Batch_idx: 160 |  Loss_1: (0.3317) | Acc_1: (88.48%) (18234/20608)\n",
      "Epoch: 36 | Batch_idx: 170 |  Loss_1: (0.3339) | Acc_1: (88.43%) (19356/21888)\n",
      "Epoch: 36 | Batch_idx: 180 |  Loss_1: (0.3341) | Acc_1: (88.41%) (20482/23168)\n",
      "Epoch: 36 | Batch_idx: 190 |  Loss_1: (0.3323) | Acc_1: (88.46%) (21626/24448)\n",
      "Epoch: 36 | Batch_idx: 200 |  Loss_1: (0.3326) | Acc_1: (88.43%) (22751/25728)\n",
      "Epoch: 36 | Batch_idx: 210 |  Loss_1: (0.3339) | Acc_1: (88.37%) (23867/27008)\n",
      "Epoch: 36 | Batch_idx: 220 |  Loss_1: (0.3345) | Acc_1: (88.38%) (25000/28288)\n",
      "Epoch: 36 | Batch_idx: 230 |  Loss_1: (0.3346) | Acc_1: (88.36%) (26125/29568)\n",
      "Epoch: 36 | Batch_idx: 240 |  Loss_1: (0.3351) | Acc_1: (88.33%) (27247/30848)\n",
      "Epoch: 36 | Batch_idx: 250 |  Loss_1: (0.3360) | Acc_1: (88.26%) (28356/32128)\n",
      "Epoch: 36 | Batch_idx: 260 |  Loss_1: (0.3345) | Acc_1: (88.30%) (29498/33408)\n",
      "Epoch: 36 | Batch_idx: 270 |  Loss_1: (0.3348) | Acc_1: (88.31%) (30633/34688)\n",
      "Epoch: 36 | Batch_idx: 280 |  Loss_1: (0.3359) | Acc_1: (88.30%) (31761/35968)\n",
      "Epoch: 36 | Batch_idx: 290 |  Loss_1: (0.3359) | Acc_1: (88.28%) (32884/37248)\n",
      "Epoch: 36 | Batch_idx: 300 |  Loss_1: (0.3360) | Acc_1: (88.27%) (34009/38528)\n",
      "Epoch: 36 | Batch_idx: 310 |  Loss_1: (0.3359) | Acc_1: (88.28%) (35141/39808)\n",
      "Epoch: 36 | Batch_idx: 320 |  Loss_1: (0.3358) | Acc_1: (88.27%) (36267/41088)\n",
      "Epoch: 36 | Batch_idx: 330 |  Loss_1: (0.3359) | Acc_1: (88.26%) (37396/42368)\n",
      "Epoch: 36 | Batch_idx: 340 |  Loss_1: (0.3352) | Acc_1: (88.30%) (38540/43648)\n",
      "Epoch: 36 | Batch_idx: 350 |  Loss_1: (0.3355) | Acc_1: (88.28%) (39662/44928)\n",
      "Epoch: 36 | Batch_idx: 360 |  Loss_1: (0.3354) | Acc_1: (88.28%) (40793/46208)\n",
      "Epoch: 36 | Batch_idx: 370 |  Loss_1: (0.3359) | Acc_1: (88.25%) (41909/47488)\n",
      "Epoch: 36 | Batch_idx: 380 |  Loss_1: (0.3366) | Acc_1: (88.18%) (43006/48768)\n",
      "Epoch: 36 | Batch_idx: 390 |  Loss_1: (0.3372) | Acc_1: (88.15%) (44075/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3419) | Acc: (88.66%) (8866/10000)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.3068) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.3459) | Acc_1: (88.07%) (1240/1408)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.3510) | Acc_1: (87.83%) (2361/2688)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.3455) | Acc_1: (88.10%) (3496/3968)\n",
      "Epoch: 37 | Batch_idx: 40 |  Loss_1: (0.3446) | Acc_1: (88.01%) (4619/5248)\n",
      "Epoch: 37 | Batch_idx: 50 |  Loss_1: (0.3508) | Acc_1: (87.67%) (5723/6528)\n",
      "Epoch: 37 | Batch_idx: 60 |  Loss_1: (0.3437) | Acc_1: (87.92%) (6865/7808)\n",
      "Epoch: 37 | Batch_idx: 70 |  Loss_1: (0.3370) | Acc_1: (88.19%) (8015/9088)\n",
      "Epoch: 37 | Batch_idx: 80 |  Loss_1: (0.3373) | Acc_1: (88.10%) (9134/10368)\n",
      "Epoch: 37 | Batch_idx: 90 |  Loss_1: (0.3388) | Acc_1: (88.01%) (10251/11648)\n",
      "Epoch: 37 | Batch_idx: 100 |  Loss_1: (0.3361) | Acc_1: (88.17%) (11399/12928)\n",
      "Epoch: 37 | Batch_idx: 110 |  Loss_1: (0.3365) | Acc_1: (88.10%) (12517/14208)\n",
      "Epoch: 37 | Batch_idx: 120 |  Loss_1: (0.3356) | Acc_1: (88.16%) (13654/15488)\n",
      "Epoch: 37 | Batch_idx: 130 |  Loss_1: (0.3360) | Acc_1: (88.15%) (14781/16768)\n",
      "Epoch: 37 | Batch_idx: 140 |  Loss_1: (0.3366) | Acc_1: (88.16%) (15912/18048)\n",
      "Epoch: 37 | Batch_idx: 150 |  Loss_1: (0.3358) | Acc_1: (88.19%) (17046/19328)\n",
      "Epoch: 37 | Batch_idx: 160 |  Loss_1: (0.3340) | Acc_1: (88.25%) (18187/20608)\n",
      "Epoch: 37 | Batch_idx: 170 |  Loss_1: (0.3344) | Acc_1: (88.27%) (19320/21888)\n",
      "Epoch: 37 | Batch_idx: 180 |  Loss_1: (0.3339) | Acc_1: (88.28%) (20453/23168)\n",
      "Epoch: 37 | Batch_idx: 190 |  Loss_1: (0.3362) | Acc_1: (88.24%) (21573/24448)\n",
      "Epoch: 37 | Batch_idx: 200 |  Loss_1: (0.3372) | Acc_1: (88.20%) (22693/25728)\n",
      "Epoch: 37 | Batch_idx: 210 |  Loss_1: (0.3396) | Acc_1: (88.12%) (23800/27008)\n",
      "Epoch: 37 | Batch_idx: 220 |  Loss_1: (0.3392) | Acc_1: (88.14%) (24932/28288)\n",
      "Epoch: 37 | Batch_idx: 230 |  Loss_1: (0.3392) | Acc_1: (88.13%) (26058/29568)\n",
      "Epoch: 37 | Batch_idx: 240 |  Loss_1: (0.3371) | Acc_1: (88.22%) (27214/30848)\n",
      "Epoch: 37 | Batch_idx: 250 |  Loss_1: (0.3372) | Acc_1: (88.24%) (28349/32128)\n",
      "Epoch: 37 | Batch_idx: 260 |  Loss_1: (0.3371) | Acc_1: (88.22%) (29474/33408)\n",
      "Epoch: 37 | Batch_idx: 270 |  Loss_1: (0.3361) | Acc_1: (88.24%) (30608/34688)\n",
      "Epoch: 37 | Batch_idx: 280 |  Loss_1: (0.3363) | Acc_1: (88.24%) (31738/35968)\n",
      "Epoch: 37 | Batch_idx: 290 |  Loss_1: (0.3353) | Acc_1: (88.28%) (32881/37248)\n",
      "Epoch: 37 | Batch_idx: 300 |  Loss_1: (0.3356) | Acc_1: (88.27%) (34010/38528)\n",
      "Epoch: 37 | Batch_idx: 310 |  Loss_1: (0.3349) | Acc_1: (88.29%) (35145/39808)\n",
      "Epoch: 37 | Batch_idx: 320 |  Loss_1: (0.3342) | Acc_1: (88.32%) (36287/41088)\n",
      "Epoch: 37 | Batch_idx: 330 |  Loss_1: (0.3349) | Acc_1: (88.33%) (37422/42368)\n",
      "Epoch: 37 | Batch_idx: 340 |  Loss_1: (0.3350) | Acc_1: (88.33%) (38554/43648)\n",
      "Epoch: 37 | Batch_idx: 350 |  Loss_1: (0.3347) | Acc_1: (88.35%) (39695/44928)\n",
      "Epoch: 37 | Batch_idx: 360 |  Loss_1: (0.3337) | Acc_1: (88.38%) (40839/46208)\n",
      "Epoch: 37 | Batch_idx: 370 |  Loss_1: (0.3333) | Acc_1: (88.40%) (41978/47488)\n",
      "Epoch: 37 | Batch_idx: 380 |  Loss_1: (0.3334) | Acc_1: (88.39%) (43105/48768)\n",
      "Epoch: 37 | Batch_idx: 390 |  Loss_1: (0.3332) | Acc_1: (88.39%) (44195/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3853) | Acc: (87.38%) (8738/10000)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.1926) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.3034) | Acc_1: (88.21%) (1242/1408)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.3151) | Acc_1: (88.17%) (2370/2688)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.3199) | Acc_1: (88.03%) (3493/3968)\n",
      "Epoch: 38 | Batch_idx: 40 |  Loss_1: (0.3208) | Acc_1: (88.15%) (4626/5248)\n",
      "Epoch: 38 | Batch_idx: 50 |  Loss_1: (0.3197) | Acc_1: (88.27%) (5762/6528)\n",
      "Epoch: 38 | Batch_idx: 60 |  Loss_1: (0.3211) | Acc_1: (88.31%) (6895/7808)\n",
      "Epoch: 38 | Batch_idx: 70 |  Loss_1: (0.3163) | Acc_1: (88.52%) (8045/9088)\n",
      "Epoch: 38 | Batch_idx: 80 |  Loss_1: (0.3165) | Acc_1: (88.66%) (9192/10368)\n",
      "Epoch: 38 | Batch_idx: 90 |  Loss_1: (0.3214) | Acc_1: (88.44%) (10302/11648)\n",
      "Epoch: 38 | Batch_idx: 100 |  Loss_1: (0.3257) | Acc_1: (88.37%) (11424/12928)\n",
      "Epoch: 38 | Batch_idx: 110 |  Loss_1: (0.3232) | Acc_1: (88.46%) (12568/14208)\n",
      "Epoch: 38 | Batch_idx: 120 |  Loss_1: (0.3198) | Acc_1: (88.58%) (13720/15488)\n",
      "Epoch: 38 | Batch_idx: 130 |  Loss_1: (0.3205) | Acc_1: (88.57%) (14852/16768)\n",
      "Epoch: 38 | Batch_idx: 140 |  Loss_1: (0.3212) | Acc_1: (88.60%) (15990/18048)\n",
      "Epoch: 38 | Batch_idx: 150 |  Loss_1: (0.3215) | Acc_1: (88.61%) (17126/19328)\n",
      "Epoch: 38 | Batch_idx: 160 |  Loss_1: (0.3230) | Acc_1: (88.53%) (18245/20608)\n",
      "Epoch: 38 | Batch_idx: 170 |  Loss_1: (0.3210) | Acc_1: (88.61%) (19394/21888)\n",
      "Epoch: 38 | Batch_idx: 180 |  Loss_1: (0.3215) | Acc_1: (88.64%) (20537/23168)\n",
      "Epoch: 38 | Batch_idx: 190 |  Loss_1: (0.3204) | Acc_1: (88.66%) (21676/24448)\n",
      "Epoch: 38 | Batch_idx: 200 |  Loss_1: (0.3210) | Acc_1: (88.66%) (22810/25728)\n",
      "Epoch: 38 | Batch_idx: 210 |  Loss_1: (0.3208) | Acc_1: (88.65%) (23943/27008)\n",
      "Epoch: 38 | Batch_idx: 220 |  Loss_1: (0.3203) | Acc_1: (88.69%) (25090/28288)\n",
      "Epoch: 38 | Batch_idx: 230 |  Loss_1: (0.3192) | Acc_1: (88.73%) (26237/29568)\n",
      "Epoch: 38 | Batch_idx: 240 |  Loss_1: (0.3202) | Acc_1: (88.71%) (27364/30848)\n",
      "Epoch: 38 | Batch_idx: 250 |  Loss_1: (0.3208) | Acc_1: (88.69%) (28493/32128)\n",
      "Epoch: 38 | Batch_idx: 260 |  Loss_1: (0.3207) | Acc_1: (88.69%) (29630/33408)\n",
      "Epoch: 38 | Batch_idx: 270 |  Loss_1: (0.3207) | Acc_1: (88.71%) (30773/34688)\n",
      "Epoch: 38 | Batch_idx: 280 |  Loss_1: (0.3205) | Acc_1: (88.71%) (31907/35968)\n",
      "Epoch: 38 | Batch_idx: 290 |  Loss_1: (0.3205) | Acc_1: (88.70%) (33038/37248)\n",
      "Epoch: 38 | Batch_idx: 300 |  Loss_1: (0.3209) | Acc_1: (88.68%) (34168/38528)\n",
      "Epoch: 38 | Batch_idx: 310 |  Loss_1: (0.3203) | Acc_1: (88.69%) (35304/39808)\n",
      "Epoch: 38 | Batch_idx: 320 |  Loss_1: (0.3208) | Acc_1: (88.63%) (36416/41088)\n",
      "Epoch: 38 | Batch_idx: 330 |  Loss_1: (0.3202) | Acc_1: (88.65%) (37559/42368)\n",
      "Epoch: 38 | Batch_idx: 340 |  Loss_1: (0.3205) | Acc_1: (88.63%) (38684/43648)\n",
      "Epoch: 38 | Batch_idx: 350 |  Loss_1: (0.3210) | Acc_1: (88.62%) (39816/44928)\n",
      "Epoch: 38 | Batch_idx: 360 |  Loss_1: (0.3209) | Acc_1: (88.64%) (40957/46208)\n",
      "Epoch: 38 | Batch_idx: 370 |  Loss_1: (0.3222) | Acc_1: (88.61%) (42078/47488)\n",
      "Epoch: 38 | Batch_idx: 380 |  Loss_1: (0.3226) | Acc_1: (88.59%) (43205/48768)\n",
      "Epoch: 38 | Batch_idx: 390 |  Loss_1: (0.3243) | Acc_1: (88.52%) (44259/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3498) | Acc: (88.38%) (8838/10000)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.5050) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.3296) | Acc_1: (88.64%) (1248/1408)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.3410) | Acc_1: (87.91%) (2363/2688)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.3288) | Acc_1: (88.46%) (3510/3968)\n",
      "Epoch: 39 | Batch_idx: 40 |  Loss_1: (0.3218) | Acc_1: (88.70%) (4655/5248)\n",
      "Epoch: 39 | Batch_idx: 50 |  Loss_1: (0.3204) | Acc_1: (88.80%) (5797/6528)\n",
      "Epoch: 39 | Batch_idx: 60 |  Loss_1: (0.3135) | Acc_1: (89.15%) (6961/7808)\n",
      "Epoch: 39 | Batch_idx: 70 |  Loss_1: (0.3136) | Acc_1: (89.23%) (8109/9088)\n",
      "Epoch: 39 | Batch_idx: 80 |  Loss_1: (0.3139) | Acc_1: (89.19%) (9247/10368)\n",
      "Epoch: 39 | Batch_idx: 90 |  Loss_1: (0.3125) | Acc_1: (89.23%) (10394/11648)\n",
      "Epoch: 39 | Batch_idx: 100 |  Loss_1: (0.3100) | Acc_1: (89.31%) (11546/12928)\n",
      "Epoch: 39 | Batch_idx: 110 |  Loss_1: (0.3130) | Acc_1: (89.16%) (12668/14208)\n",
      "Epoch: 39 | Batch_idx: 120 |  Loss_1: (0.3145) | Acc_1: (89.08%) (13797/15488)\n",
      "Epoch: 39 | Batch_idx: 130 |  Loss_1: (0.3168) | Acc_1: (88.94%) (14914/16768)\n",
      "Epoch: 39 | Batch_idx: 140 |  Loss_1: (0.3182) | Acc_1: (88.90%) (16045/18048)\n",
      "Epoch: 39 | Batch_idx: 150 |  Loss_1: (0.3193) | Acc_1: (88.85%) (17173/19328)\n",
      "Epoch: 39 | Batch_idx: 160 |  Loss_1: (0.3200) | Acc_1: (88.83%) (18306/20608)\n",
      "Epoch: 39 | Batch_idx: 170 |  Loss_1: (0.3204) | Acc_1: (88.79%) (19434/21888)\n",
      "Epoch: 39 | Batch_idx: 180 |  Loss_1: (0.3198) | Acc_1: (88.81%) (20575/23168)\n",
      "Epoch: 39 | Batch_idx: 190 |  Loss_1: (0.3206) | Acc_1: (88.75%) (21697/24448)\n",
      "Epoch: 39 | Batch_idx: 200 |  Loss_1: (0.3193) | Acc_1: (88.76%) (22837/25728)\n",
      "Epoch: 39 | Batch_idx: 210 |  Loss_1: (0.3200) | Acc_1: (88.76%) (23972/27008)\n",
      "Epoch: 39 | Batch_idx: 220 |  Loss_1: (0.3211) | Acc_1: (88.75%) (25106/28288)\n",
      "Epoch: 39 | Batch_idx: 230 |  Loss_1: (0.3216) | Acc_1: (88.72%) (26233/29568)\n",
      "Epoch: 39 | Batch_idx: 240 |  Loss_1: (0.3206) | Acc_1: (88.73%) (27372/30848)\n",
      "Epoch: 39 | Batch_idx: 250 |  Loss_1: (0.3192) | Acc_1: (88.80%) (28531/32128)\n",
      "Epoch: 39 | Batch_idx: 260 |  Loss_1: (0.3206) | Acc_1: (88.77%) (29655/33408)\n",
      "Epoch: 39 | Batch_idx: 270 |  Loss_1: (0.3209) | Acc_1: (88.77%) (30791/34688)\n",
      "Epoch: 39 | Batch_idx: 280 |  Loss_1: (0.3190) | Acc_1: (88.81%) (31942/35968)\n",
      "Epoch: 39 | Batch_idx: 290 |  Loss_1: (0.3198) | Acc_1: (88.78%) (33068/37248)\n",
      "Epoch: 39 | Batch_idx: 300 |  Loss_1: (0.3191) | Acc_1: (88.80%) (34212/38528)\n",
      "Epoch: 39 | Batch_idx: 310 |  Loss_1: (0.3196) | Acc_1: (88.77%) (35338/39808)\n",
      "Epoch: 39 | Batch_idx: 320 |  Loss_1: (0.3195) | Acc_1: (88.78%) (36479/41088)\n",
      "Epoch: 39 | Batch_idx: 330 |  Loss_1: (0.3204) | Acc_1: (88.77%) (37611/42368)\n",
      "Epoch: 39 | Batch_idx: 340 |  Loss_1: (0.3198) | Acc_1: (88.79%) (38755/43648)\n",
      "Epoch: 39 | Batch_idx: 350 |  Loss_1: (0.3202) | Acc_1: (88.78%) (39889/44928)\n",
      "Epoch: 39 | Batch_idx: 360 |  Loss_1: (0.3214) | Acc_1: (88.74%) (41007/46208)\n",
      "Epoch: 39 | Batch_idx: 370 |  Loss_1: (0.3207) | Acc_1: (88.76%) (42151/47488)\n",
      "Epoch: 39 | Batch_idx: 380 |  Loss_1: (0.3207) | Acc_1: (88.75%) (43283/48768)\n",
      "Epoch: 39 | Batch_idx: 390 |  Loss_1: (0.3202) | Acc_1: (88.77%) (44385/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3729) | Acc: (87.90%) (8790/10000)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.2164) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.3212) | Acc_1: (88.71%) (1249/1408)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.3083) | Acc_1: (89.03%) (2393/2688)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.2975) | Acc_1: (89.31%) (3544/3968)\n",
      "Epoch: 40 | Batch_idx: 40 |  Loss_1: (0.2928) | Acc_1: (89.58%) (4701/5248)\n",
      "Epoch: 40 | Batch_idx: 50 |  Loss_1: (0.2984) | Acc_1: (89.46%) (5840/6528)\n",
      "Epoch: 40 | Batch_idx: 60 |  Loss_1: (0.2993) | Acc_1: (89.54%) (6991/7808)\n",
      "Epoch: 40 | Batch_idx: 70 |  Loss_1: (0.3007) | Acc_1: (89.46%) (8130/9088)\n",
      "Epoch: 40 | Batch_idx: 80 |  Loss_1: (0.3006) | Acc_1: (89.45%) (9274/10368)\n",
      "Epoch: 40 | Batch_idx: 90 |  Loss_1: (0.3007) | Acc_1: (89.41%) (10415/11648)\n",
      "Epoch: 40 | Batch_idx: 100 |  Loss_1: (0.3023) | Acc_1: (89.33%) (11549/12928)\n",
      "Epoch: 40 | Batch_idx: 110 |  Loss_1: (0.3029) | Acc_1: (89.39%) (12701/14208)\n",
      "Epoch: 40 | Batch_idx: 120 |  Loss_1: (0.3037) | Acc_1: (89.30%) (13831/15488)\n",
      "Epoch: 40 | Batch_idx: 130 |  Loss_1: (0.3016) | Acc_1: (89.36%) (14984/16768)\n",
      "Epoch: 40 | Batch_idx: 140 |  Loss_1: (0.3023) | Acc_1: (89.30%) (16116/18048)\n",
      "Epoch: 40 | Batch_idx: 150 |  Loss_1: (0.3015) | Acc_1: (89.32%) (17264/19328)\n",
      "Epoch: 40 | Batch_idx: 160 |  Loss_1: (0.3043) | Acc_1: (89.21%) (18384/20608)\n",
      "Epoch: 40 | Batch_idx: 170 |  Loss_1: (0.3050) | Acc_1: (89.18%) (19519/21888)\n",
      "Epoch: 40 | Batch_idx: 180 |  Loss_1: (0.3037) | Acc_1: (89.24%) (20676/23168)\n",
      "Epoch: 40 | Batch_idx: 190 |  Loss_1: (0.3045) | Acc_1: (89.21%) (21809/24448)\n",
      "Epoch: 40 | Batch_idx: 200 |  Loss_1: (0.3048) | Acc_1: (89.21%) (22951/25728)\n",
      "Epoch: 40 | Batch_idx: 210 |  Loss_1: (0.3050) | Acc_1: (89.26%) (24108/27008)\n",
      "Epoch: 40 | Batch_idx: 220 |  Loss_1: (0.3035) | Acc_1: (89.32%) (25268/28288)\n",
      "Epoch: 40 | Batch_idx: 230 |  Loss_1: (0.3047) | Acc_1: (89.32%) (26410/29568)\n",
      "Epoch: 40 | Batch_idx: 240 |  Loss_1: (0.3059) | Acc_1: (89.30%) (27547/30848)\n",
      "Epoch: 40 | Batch_idx: 250 |  Loss_1: (0.3063) | Acc_1: (89.29%) (28687/32128)\n",
      "Epoch: 40 | Batch_idx: 260 |  Loss_1: (0.3065) | Acc_1: (89.30%) (29832/33408)\n",
      "Epoch: 40 | Batch_idx: 270 |  Loss_1: (0.3062) | Acc_1: (89.27%) (30966/34688)\n",
      "Epoch: 40 | Batch_idx: 280 |  Loss_1: (0.3061) | Acc_1: (89.30%) (32118/35968)\n",
      "Epoch: 40 | Batch_idx: 290 |  Loss_1: (0.3067) | Acc_1: (89.27%) (33252/37248)\n",
      "Epoch: 40 | Batch_idx: 300 |  Loss_1: (0.3078) | Acc_1: (89.22%) (34376/38528)\n",
      "Epoch: 40 | Batch_idx: 310 |  Loss_1: (0.3079) | Acc_1: (89.21%) (35512/39808)\n",
      "Epoch: 40 | Batch_idx: 320 |  Loss_1: (0.3089) | Acc_1: (89.17%) (36637/41088)\n",
      "Epoch: 40 | Batch_idx: 330 |  Loss_1: (0.3104) | Acc_1: (89.13%) (37761/42368)\n",
      "Epoch: 40 | Batch_idx: 340 |  Loss_1: (0.3115) | Acc_1: (89.08%) (38881/43648)\n",
      "Epoch: 40 | Batch_idx: 350 |  Loss_1: (0.3116) | Acc_1: (89.07%) (40016/44928)\n",
      "Epoch: 40 | Batch_idx: 360 |  Loss_1: (0.3119) | Acc_1: (89.04%) (41145/46208)\n",
      "Epoch: 40 | Batch_idx: 370 |  Loss_1: (0.3127) | Acc_1: (89.04%) (42282/47488)\n",
      "Epoch: 40 | Batch_idx: 380 |  Loss_1: (0.3137) | Acc_1: (89.00%) (43405/48768)\n",
      "Epoch: 40 | Batch_idx: 390 |  Loss_1: (0.3146) | Acc_1: (88.99%) (44495/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3461) | Acc: (88.88%) (8888/10000)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.2672) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.2967) | Acc_1: (89.06%) (1254/1408)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.3148) | Acc_1: (88.80%) (2387/2688)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.3200) | Acc_1: (88.36%) (3506/3968)\n",
      "Epoch: 41 | Batch_idx: 40 |  Loss_1: (0.3143) | Acc_1: (88.87%) (4664/5248)\n",
      "Epoch: 41 | Batch_idx: 50 |  Loss_1: (0.3052) | Acc_1: (89.25%) (5826/6528)\n",
      "Epoch: 41 | Batch_idx: 60 |  Loss_1: (0.3041) | Acc_1: (89.32%) (6974/7808)\n",
      "Epoch: 41 | Batch_idx: 70 |  Loss_1: (0.3096) | Acc_1: (89.11%) (8098/9088)\n",
      "Epoch: 41 | Batch_idx: 80 |  Loss_1: (0.3046) | Acc_1: (89.19%) (9247/10368)\n",
      "Epoch: 41 | Batch_idx: 90 |  Loss_1: (0.3066) | Acc_1: (89.13%) (10382/11648)\n",
      "Epoch: 41 | Batch_idx: 100 |  Loss_1: (0.3049) | Acc_1: (89.22%) (11535/12928)\n",
      "Epoch: 41 | Batch_idx: 110 |  Loss_1: (0.3062) | Acc_1: (89.25%) (12680/14208)\n",
      "Epoch: 41 | Batch_idx: 120 |  Loss_1: (0.3061) | Acc_1: (89.29%) (13829/15488)\n",
      "Epoch: 41 | Batch_idx: 130 |  Loss_1: (0.3063) | Acc_1: (89.34%) (14981/16768)\n",
      "Epoch: 41 | Batch_idx: 140 |  Loss_1: (0.3059) | Acc_1: (89.36%) (16127/18048)\n",
      "Epoch: 41 | Batch_idx: 150 |  Loss_1: (0.3070) | Acc_1: (89.30%) (17259/19328)\n",
      "Epoch: 41 | Batch_idx: 160 |  Loss_1: (0.3060) | Acc_1: (89.32%) (18408/20608)\n",
      "Epoch: 41 | Batch_idx: 170 |  Loss_1: (0.3064) | Acc_1: (89.29%) (19544/21888)\n",
      "Epoch: 41 | Batch_idx: 180 |  Loss_1: (0.3063) | Acc_1: (89.30%) (20688/23168)\n",
      "Epoch: 41 | Batch_idx: 190 |  Loss_1: (0.3055) | Acc_1: (89.31%) (21835/24448)\n",
      "Epoch: 41 | Batch_idx: 200 |  Loss_1: (0.3073) | Acc_1: (89.27%) (22967/25728)\n",
      "Epoch: 41 | Batch_idx: 210 |  Loss_1: (0.3069) | Acc_1: (89.25%) (24105/27008)\n",
      "Epoch: 41 | Batch_idx: 220 |  Loss_1: (0.3075) | Acc_1: (89.24%) (25244/28288)\n",
      "Epoch: 41 | Batch_idx: 230 |  Loss_1: (0.3088) | Acc_1: (89.17%) (26365/29568)\n",
      "Epoch: 41 | Batch_idx: 240 |  Loss_1: (0.3091) | Acc_1: (89.17%) (27508/30848)\n",
      "Epoch: 41 | Batch_idx: 250 |  Loss_1: (0.3091) | Acc_1: (89.17%) (28649/32128)\n",
      "Epoch: 41 | Batch_idx: 260 |  Loss_1: (0.3089) | Acc_1: (89.19%) (29795/33408)\n",
      "Epoch: 41 | Batch_idx: 270 |  Loss_1: (0.3079) | Acc_1: (89.21%) (30944/34688)\n",
      "Epoch: 41 | Batch_idx: 280 |  Loss_1: (0.3083) | Acc_1: (89.19%) (32080/35968)\n",
      "Epoch: 41 | Batch_idx: 290 |  Loss_1: (0.3076) | Acc_1: (89.21%) (33229/37248)\n",
      "Epoch: 41 | Batch_idx: 300 |  Loss_1: (0.3102) | Acc_1: (89.16%) (34353/38528)\n",
      "Epoch: 41 | Batch_idx: 310 |  Loss_1: (0.3106) | Acc_1: (89.16%) (35491/39808)\n",
      "Epoch: 41 | Batch_idx: 320 |  Loss_1: (0.3103) | Acc_1: (89.15%) (36631/41088)\n",
      "Epoch: 41 | Batch_idx: 330 |  Loss_1: (0.3097) | Acc_1: (89.15%) (37772/42368)\n",
      "Epoch: 41 | Batch_idx: 340 |  Loss_1: (0.3094) | Acc_1: (89.15%) (38912/43648)\n",
      "Epoch: 41 | Batch_idx: 350 |  Loss_1: (0.3098) | Acc_1: (89.14%) (40048/44928)\n",
      "Epoch: 41 | Batch_idx: 360 |  Loss_1: (0.3107) | Acc_1: (89.11%) (41174/46208)\n",
      "Epoch: 41 | Batch_idx: 370 |  Loss_1: (0.3107) | Acc_1: (89.10%) (42311/47488)\n",
      "Epoch: 41 | Batch_idx: 380 |  Loss_1: (0.3113) | Acc_1: (89.07%) (43440/48768)\n",
      "Epoch: 41 | Batch_idx: 390 |  Loss_1: (0.3115) | Acc_1: (89.06%) (44529/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3600) | Acc: (88.32%) (8832/10000)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.3134) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.3106) | Acc_1: (88.21%) (1242/1408)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.2974) | Acc_1: (89.06%) (2394/2688)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.3012) | Acc_1: (88.71%) (3520/3968)\n",
      "Epoch: 42 | Batch_idx: 40 |  Loss_1: (0.3016) | Acc_1: (89.04%) (4673/5248)\n",
      "Epoch: 42 | Batch_idx: 50 |  Loss_1: (0.2946) | Acc_1: (89.28%) (5828/6528)\n",
      "Epoch: 42 | Batch_idx: 60 |  Loss_1: (0.2907) | Acc_1: (89.36%) (6977/7808)\n",
      "Epoch: 42 | Batch_idx: 70 |  Loss_1: (0.2898) | Acc_1: (89.49%) (8133/9088)\n",
      "Epoch: 42 | Batch_idx: 80 |  Loss_1: (0.2953) | Acc_1: (89.39%) (9268/10368)\n",
      "Epoch: 42 | Batch_idx: 90 |  Loss_1: (0.2929) | Acc_1: (89.54%) (10430/11648)\n",
      "Epoch: 42 | Batch_idx: 100 |  Loss_1: (0.2914) | Acc_1: (89.67%) (11593/12928)\n",
      "Epoch: 42 | Batch_idx: 110 |  Loss_1: (0.2933) | Acc_1: (89.60%) (12730/14208)\n",
      "Epoch: 42 | Batch_idx: 120 |  Loss_1: (0.2985) | Acc_1: (89.43%) (13851/15488)\n",
      "Epoch: 42 | Batch_idx: 130 |  Loss_1: (0.3001) | Acc_1: (89.43%) (14995/16768)\n",
      "Epoch: 42 | Batch_idx: 140 |  Loss_1: (0.2975) | Acc_1: (89.54%) (16160/18048)\n",
      "Epoch: 42 | Batch_idx: 150 |  Loss_1: (0.2981) | Acc_1: (89.49%) (17297/19328)\n",
      "Epoch: 42 | Batch_idx: 160 |  Loss_1: (0.3016) | Acc_1: (89.41%) (18426/20608)\n",
      "Epoch: 42 | Batch_idx: 170 |  Loss_1: (0.3031) | Acc_1: (89.38%) (19563/21888)\n",
      "Epoch: 42 | Batch_idx: 180 |  Loss_1: (0.3039) | Acc_1: (89.39%) (20709/23168)\n",
      "Epoch: 42 | Batch_idx: 190 |  Loss_1: (0.3022) | Acc_1: (89.46%) (21870/24448)\n",
      "Epoch: 42 | Batch_idx: 200 |  Loss_1: (0.3047) | Acc_1: (89.38%) (22995/25728)\n",
      "Epoch: 42 | Batch_idx: 210 |  Loss_1: (0.3042) | Acc_1: (89.36%) (24135/27008)\n",
      "Epoch: 42 | Batch_idx: 220 |  Loss_1: (0.3050) | Acc_1: (89.34%) (25272/28288)\n",
      "Epoch: 42 | Batch_idx: 230 |  Loss_1: (0.3045) | Acc_1: (89.37%) (26426/29568)\n",
      "Epoch: 42 | Batch_idx: 240 |  Loss_1: (0.3065) | Acc_1: (89.32%) (27553/30848)\n",
      "Epoch: 42 | Batch_idx: 250 |  Loss_1: (0.3053) | Acc_1: (89.33%) (28701/32128)\n",
      "Epoch: 42 | Batch_idx: 260 |  Loss_1: (0.3050) | Acc_1: (89.34%) (29847/33408)\n",
      "Epoch: 42 | Batch_idx: 270 |  Loss_1: (0.3051) | Acc_1: (89.32%) (30982/34688)\n",
      "Epoch: 42 | Batch_idx: 280 |  Loss_1: (0.3043) | Acc_1: (89.35%) (32139/35968)\n",
      "Epoch: 42 | Batch_idx: 290 |  Loss_1: (0.3031) | Acc_1: (89.40%) (33300/37248)\n",
      "Epoch: 42 | Batch_idx: 300 |  Loss_1: (0.3028) | Acc_1: (89.40%) (34445/38528)\n",
      "Epoch: 42 | Batch_idx: 310 |  Loss_1: (0.3035) | Acc_1: (89.38%) (35582/39808)\n",
      "Epoch: 42 | Batch_idx: 320 |  Loss_1: (0.3028) | Acc_1: (89.41%) (36735/41088)\n",
      "Epoch: 42 | Batch_idx: 330 |  Loss_1: (0.3037) | Acc_1: (89.37%) (37865/42368)\n",
      "Epoch: 42 | Batch_idx: 340 |  Loss_1: (0.3041) | Acc_1: (89.36%) (39004/43648)\n",
      "Epoch: 42 | Batch_idx: 350 |  Loss_1: (0.3039) | Acc_1: (89.38%) (40156/44928)\n",
      "Epoch: 42 | Batch_idx: 360 |  Loss_1: (0.3039) | Acc_1: (89.36%) (41293/46208)\n",
      "Epoch: 42 | Batch_idx: 370 |  Loss_1: (0.3053) | Acc_1: (89.33%) (42419/47488)\n",
      "Epoch: 42 | Batch_idx: 380 |  Loss_1: (0.3051) | Acc_1: (89.32%) (43562/48768)\n",
      "Epoch: 42 | Batch_idx: 390 |  Loss_1: (0.3048) | Acc_1: (89.34%) (44668/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3077) | Acc: (90.18%) (9018/10000)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.3997) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.2982) | Acc_1: (89.63%) (1262/1408)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.2938) | Acc_1: (89.66%) (2410/2688)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.2905) | Acc_1: (89.54%) (3553/3968)\n",
      "Epoch: 43 | Batch_idx: 40 |  Loss_1: (0.2790) | Acc_1: (89.77%) (4711/5248)\n",
      "Epoch: 43 | Batch_idx: 50 |  Loss_1: (0.2721) | Acc_1: (90.10%) (5882/6528)\n",
      "Epoch: 43 | Batch_idx: 60 |  Loss_1: (0.2780) | Acc_1: (90.00%) (7027/7808)\n",
      "Epoch: 43 | Batch_idx: 70 |  Loss_1: (0.2763) | Acc_1: (90.06%) (8185/9088)\n",
      "Epoch: 43 | Batch_idx: 80 |  Loss_1: (0.2791) | Acc_1: (90.03%) (9334/10368)\n",
      "Epoch: 43 | Batch_idx: 90 |  Loss_1: (0.2827) | Acc_1: (89.96%) (10479/11648)\n",
      "Epoch: 43 | Batch_idx: 100 |  Loss_1: (0.2838) | Acc_1: (89.92%) (11625/12928)\n",
      "Epoch: 43 | Batch_idx: 110 |  Loss_1: (0.2855) | Acc_1: (89.82%) (12762/14208)\n",
      "Epoch: 43 | Batch_idx: 120 |  Loss_1: (0.2858) | Acc_1: (89.85%) (13916/15488)\n",
      "Epoch: 43 | Batch_idx: 130 |  Loss_1: (0.2884) | Acc_1: (89.75%) (15049/16768)\n",
      "Epoch: 43 | Batch_idx: 140 |  Loss_1: (0.2908) | Acc_1: (89.69%) (16187/18048)\n",
      "Epoch: 43 | Batch_idx: 150 |  Loss_1: (0.2936) | Acc_1: (89.57%) (17313/19328)\n",
      "Epoch: 43 | Batch_idx: 160 |  Loss_1: (0.2921) | Acc_1: (89.61%) (18467/20608)\n",
      "Epoch: 43 | Batch_idx: 170 |  Loss_1: (0.2914) | Acc_1: (89.67%) (19626/21888)\n",
      "Epoch: 43 | Batch_idx: 180 |  Loss_1: (0.2920) | Acc_1: (89.64%) (20767/23168)\n",
      "Epoch: 43 | Batch_idx: 190 |  Loss_1: (0.2925) | Acc_1: (89.60%) (21906/24448)\n",
      "Epoch: 43 | Batch_idx: 200 |  Loss_1: (0.2938) | Acc_1: (89.58%) (23047/25728)\n",
      "Epoch: 43 | Batch_idx: 210 |  Loss_1: (0.2949) | Acc_1: (89.56%) (24189/27008)\n",
      "Epoch: 43 | Batch_idx: 220 |  Loss_1: (0.2975) | Acc_1: (89.45%) (25304/28288)\n",
      "Epoch: 43 | Batch_idx: 230 |  Loss_1: (0.2975) | Acc_1: (89.45%) (26449/29568)\n",
      "Epoch: 43 | Batch_idx: 240 |  Loss_1: (0.2975) | Acc_1: (89.44%) (27591/30848)\n",
      "Epoch: 43 | Batch_idx: 250 |  Loss_1: (0.2981) | Acc_1: (89.43%) (28733/32128)\n",
      "Epoch: 43 | Batch_idx: 260 |  Loss_1: (0.2983) | Acc_1: (89.43%) (29878/33408)\n",
      "Epoch: 43 | Batch_idx: 270 |  Loss_1: (0.2987) | Acc_1: (89.41%) (31013/34688)\n",
      "Epoch: 43 | Batch_idx: 280 |  Loss_1: (0.2985) | Acc_1: (89.40%) (32155/35968)\n",
      "Epoch: 43 | Batch_idx: 290 |  Loss_1: (0.2974) | Acc_1: (89.46%) (33322/37248)\n",
      "Epoch: 43 | Batch_idx: 300 |  Loss_1: (0.2974) | Acc_1: (89.49%) (34478/38528)\n",
      "Epoch: 43 | Batch_idx: 310 |  Loss_1: (0.2965) | Acc_1: (89.55%) (35647/39808)\n",
      "Epoch: 43 | Batch_idx: 320 |  Loss_1: (0.2969) | Acc_1: (89.55%) (36795/41088)\n",
      "Epoch: 43 | Batch_idx: 330 |  Loss_1: (0.2980) | Acc_1: (89.51%) (37923/42368)\n",
      "Epoch: 43 | Batch_idx: 340 |  Loss_1: (0.2995) | Acc_1: (89.44%) (39039/43648)\n",
      "Epoch: 43 | Batch_idx: 350 |  Loss_1: (0.2999) | Acc_1: (89.43%) (40178/44928)\n",
      "Epoch: 43 | Batch_idx: 360 |  Loss_1: (0.3002) | Acc_1: (89.40%) (41311/46208)\n",
      "Epoch: 43 | Batch_idx: 370 |  Loss_1: (0.2997) | Acc_1: (89.41%) (42458/47488)\n",
      "Epoch: 43 | Batch_idx: 380 |  Loss_1: (0.2991) | Acc_1: (89.40%) (43597/48768)\n",
      "Epoch: 43 | Batch_idx: 390 |  Loss_1: (0.2991) | Acc_1: (89.41%) (44705/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3387) | Acc: (88.92%) (8892/10000)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.1959) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.2917) | Acc_1: (89.20%) (1256/1408)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.2860) | Acc_1: (89.51%) (2406/2688)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.2819) | Acc_1: (89.39%) (3547/3968)\n",
      "Epoch: 44 | Batch_idx: 40 |  Loss_1: (0.2881) | Acc_1: (89.44%) (4694/5248)\n",
      "Epoch: 44 | Batch_idx: 50 |  Loss_1: (0.2848) | Acc_1: (89.51%) (5843/6528)\n",
      "Epoch: 44 | Batch_idx: 60 |  Loss_1: (0.2935) | Acc_1: (89.37%) (6978/7808)\n",
      "Epoch: 44 | Batch_idx: 70 |  Loss_1: (0.2883) | Acc_1: (89.65%) (8147/9088)\n",
      "Epoch: 44 | Batch_idx: 80 |  Loss_1: (0.2901) | Acc_1: (89.64%) (9294/10368)\n",
      "Epoch: 44 | Batch_idx: 90 |  Loss_1: (0.2885) | Acc_1: (89.69%) (10447/11648)\n",
      "Epoch: 44 | Batch_idx: 100 |  Loss_1: (0.2875) | Acc_1: (89.77%) (11605/12928)\n",
      "Epoch: 44 | Batch_idx: 110 |  Loss_1: (0.2866) | Acc_1: (89.83%) (12763/14208)\n",
      "Epoch: 44 | Batch_idx: 120 |  Loss_1: (0.2865) | Acc_1: (89.84%) (13915/15488)\n",
      "Epoch: 44 | Batch_idx: 130 |  Loss_1: (0.2877) | Acc_1: (89.83%) (15063/16768)\n",
      "Epoch: 44 | Batch_idx: 140 |  Loss_1: (0.2880) | Acc_1: (89.84%) (16214/18048)\n",
      "Epoch: 44 | Batch_idx: 150 |  Loss_1: (0.2882) | Acc_1: (89.85%) (17366/19328)\n",
      "Epoch: 44 | Batch_idx: 160 |  Loss_1: (0.2865) | Acc_1: (89.87%) (18521/20608)\n",
      "Epoch: 44 | Batch_idx: 170 |  Loss_1: (0.2840) | Acc_1: (89.95%) (19689/21888)\n",
      "Epoch: 44 | Batch_idx: 180 |  Loss_1: (0.2834) | Acc_1: (90.01%) (20853/23168)\n",
      "Epoch: 44 | Batch_idx: 190 |  Loss_1: (0.2821) | Acc_1: (90.05%) (22016/24448)\n",
      "Epoch: 44 | Batch_idx: 200 |  Loss_1: (0.2835) | Acc_1: (89.98%) (23150/25728)\n",
      "Epoch: 44 | Batch_idx: 210 |  Loss_1: (0.2850) | Acc_1: (89.96%) (24297/27008)\n",
      "Epoch: 44 | Batch_idx: 220 |  Loss_1: (0.2845) | Acc_1: (89.97%) (25450/28288)\n",
      "Epoch: 44 | Batch_idx: 230 |  Loss_1: (0.2858) | Acc_1: (89.91%) (26585/29568)\n",
      "Epoch: 44 | Batch_idx: 240 |  Loss_1: (0.2855) | Acc_1: (89.93%) (27743/30848)\n",
      "Epoch: 44 | Batch_idx: 250 |  Loss_1: (0.2869) | Acc_1: (89.88%) (28876/32128)\n",
      "Epoch: 44 | Batch_idx: 260 |  Loss_1: (0.2855) | Acc_1: (89.95%) (30052/33408)\n",
      "Epoch: 44 | Batch_idx: 270 |  Loss_1: (0.2860) | Acc_1: (89.94%) (31199/34688)\n",
      "Epoch: 44 | Batch_idx: 280 |  Loss_1: (0.2866) | Acc_1: (89.93%) (32346/35968)\n",
      "Epoch: 44 | Batch_idx: 290 |  Loss_1: (0.2876) | Acc_1: (89.86%) (33472/37248)\n",
      "Epoch: 44 | Batch_idx: 300 |  Loss_1: (0.2889) | Acc_1: (89.82%) (34605/38528)\n",
      "Epoch: 44 | Batch_idx: 310 |  Loss_1: (0.2898) | Acc_1: (89.80%) (35749/39808)\n",
      "Epoch: 44 | Batch_idx: 320 |  Loss_1: (0.2892) | Acc_1: (89.87%) (36924/41088)\n",
      "Epoch: 44 | Batch_idx: 330 |  Loss_1: (0.2885) | Acc_1: (89.90%) (38090/42368)\n",
      "Epoch: 44 | Batch_idx: 340 |  Loss_1: (0.2884) | Acc_1: (89.92%) (39248/43648)\n",
      "Epoch: 44 | Batch_idx: 350 |  Loss_1: (0.2874) | Acc_1: (89.96%) (40416/44928)\n",
      "Epoch: 44 | Batch_idx: 360 |  Loss_1: (0.2874) | Acc_1: (89.95%) (41565/46208)\n",
      "Epoch: 44 | Batch_idx: 370 |  Loss_1: (0.2881) | Acc_1: (89.92%) (42702/47488)\n",
      "Epoch: 44 | Batch_idx: 380 |  Loss_1: (0.2880) | Acc_1: (89.92%) (43854/48768)\n",
      "Epoch: 44 | Batch_idx: 390 |  Loss_1: (0.2884) | Acc_1: (89.92%) (44961/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3109) | Acc: (90.00%) (9000/10000)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.2628) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.2519) | Acc_1: (90.98%) (1281/1408)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.2729) | Acc_1: (90.29%) (2427/2688)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.2721) | Acc_1: (90.25%) (3581/3968)\n",
      "Epoch: 45 | Batch_idx: 40 |  Loss_1: (0.2751) | Acc_1: (90.17%) (4732/5248)\n",
      "Epoch: 45 | Batch_idx: 50 |  Loss_1: (0.2765) | Acc_1: (90.29%) (5894/6528)\n",
      "Epoch: 45 | Batch_idx: 60 |  Loss_1: (0.2796) | Acc_1: (90.32%) (7052/7808)\n",
      "Epoch: 45 | Batch_idx: 70 |  Loss_1: (0.2772) | Acc_1: (90.38%) (8214/9088)\n",
      "Epoch: 45 | Batch_idx: 80 |  Loss_1: (0.2770) | Acc_1: (90.38%) (9371/10368)\n",
      "Epoch: 45 | Batch_idx: 90 |  Loss_1: (0.2800) | Acc_1: (90.27%) (10515/11648)\n",
      "Epoch: 45 | Batch_idx: 100 |  Loss_1: (0.2814) | Acc_1: (90.22%) (11663/12928)\n",
      "Epoch: 45 | Batch_idx: 110 |  Loss_1: (0.2814) | Acc_1: (90.22%) (12818/14208)\n",
      "Epoch: 45 | Batch_idx: 120 |  Loss_1: (0.2798) | Acc_1: (90.22%) (13973/15488)\n",
      "Epoch: 45 | Batch_idx: 130 |  Loss_1: (0.2815) | Acc_1: (90.12%) (15112/16768)\n",
      "Epoch: 45 | Batch_idx: 140 |  Loss_1: (0.2820) | Acc_1: (90.09%) (16259/18048)\n",
      "Epoch: 45 | Batch_idx: 150 |  Loss_1: (0.2832) | Acc_1: (90.02%) (17399/19328)\n",
      "Epoch: 45 | Batch_idx: 160 |  Loss_1: (0.2828) | Acc_1: (90.02%) (18551/20608)\n",
      "Epoch: 45 | Batch_idx: 170 |  Loss_1: (0.2839) | Acc_1: (89.96%) (19690/21888)\n",
      "Epoch: 45 | Batch_idx: 180 |  Loss_1: (0.2833) | Acc_1: (89.96%) (20842/23168)\n",
      "Epoch: 45 | Batch_idx: 190 |  Loss_1: (0.2846) | Acc_1: (89.89%) (21976/24448)\n",
      "Epoch: 45 | Batch_idx: 200 |  Loss_1: (0.2847) | Acc_1: (89.90%) (23129/25728)\n",
      "Epoch: 45 | Batch_idx: 210 |  Loss_1: (0.2842) | Acc_1: (89.93%) (24288/27008)\n",
      "Epoch: 45 | Batch_idx: 220 |  Loss_1: (0.2842) | Acc_1: (89.93%) (25439/28288)\n",
      "Epoch: 45 | Batch_idx: 230 |  Loss_1: (0.2842) | Acc_1: (89.92%) (26589/29568)\n",
      "Epoch: 45 | Batch_idx: 240 |  Loss_1: (0.2828) | Acc_1: (89.99%) (27759/30848)\n",
      "Epoch: 45 | Batch_idx: 250 |  Loss_1: (0.2834) | Acc_1: (89.98%) (28909/32128)\n",
      "Epoch: 45 | Batch_idx: 260 |  Loss_1: (0.2836) | Acc_1: (89.96%) (30054/33408)\n",
      "Epoch: 45 | Batch_idx: 270 |  Loss_1: (0.2842) | Acc_1: (89.93%) (31196/34688)\n",
      "Epoch: 45 | Batch_idx: 280 |  Loss_1: (0.2849) | Acc_1: (89.92%) (32343/35968)\n",
      "Epoch: 45 | Batch_idx: 290 |  Loss_1: (0.2865) | Acc_1: (89.88%) (33477/37248)\n",
      "Epoch: 45 | Batch_idx: 300 |  Loss_1: (0.2867) | Acc_1: (89.85%) (34618/38528)\n",
      "Epoch: 45 | Batch_idx: 310 |  Loss_1: (0.2864) | Acc_1: (89.85%) (35768/39808)\n",
      "Epoch: 45 | Batch_idx: 320 |  Loss_1: (0.2887) | Acc_1: (89.75%) (36877/41088)\n",
      "Epoch: 45 | Batch_idx: 330 |  Loss_1: (0.2906) | Acc_1: (89.71%) (38010/42368)\n",
      "Epoch: 45 | Batch_idx: 340 |  Loss_1: (0.2904) | Acc_1: (89.72%) (39161/43648)\n",
      "Epoch: 45 | Batch_idx: 350 |  Loss_1: (0.2906) | Acc_1: (89.70%) (40299/44928)\n",
      "Epoch: 45 | Batch_idx: 360 |  Loss_1: (0.2910) | Acc_1: (89.68%) (41441/46208)\n",
      "Epoch: 45 | Batch_idx: 370 |  Loss_1: (0.2910) | Acc_1: (89.68%) (42587/47488)\n",
      "Epoch: 45 | Batch_idx: 380 |  Loss_1: (0.2907) | Acc_1: (89.70%) (43746/48768)\n",
      "Epoch: 45 | Batch_idx: 390 |  Loss_1: (0.2913) | Acc_1: (89.67%) (44837/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3185) | Acc: (89.70%) (8970/10000)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.3179) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.2808) | Acc_1: (90.55%) (1275/1408)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.2567) | Acc_1: (91.07%) (2448/2688)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.2559) | Acc_1: (90.85%) (3605/3968)\n",
      "Epoch: 46 | Batch_idx: 40 |  Loss_1: (0.2685) | Acc_1: (90.53%) (4751/5248)\n",
      "Epoch: 46 | Batch_idx: 50 |  Loss_1: (0.2809) | Acc_1: (90.15%) (5885/6528)\n",
      "Epoch: 46 | Batch_idx: 60 |  Loss_1: (0.2764) | Acc_1: (90.27%) (7048/7808)\n",
      "Epoch: 46 | Batch_idx: 70 |  Loss_1: (0.2792) | Acc_1: (90.17%) (8195/9088)\n",
      "Epoch: 46 | Batch_idx: 80 |  Loss_1: (0.2784) | Acc_1: (90.22%) (9354/10368)\n",
      "Epoch: 46 | Batch_idx: 90 |  Loss_1: (0.2774) | Acc_1: (90.26%) (10514/11648)\n",
      "Epoch: 46 | Batch_idx: 100 |  Loss_1: (0.2786) | Acc_1: (90.25%) (11667/12928)\n",
      "Epoch: 46 | Batch_idx: 110 |  Loss_1: (0.2785) | Acc_1: (90.22%) (12819/14208)\n",
      "Epoch: 46 | Batch_idx: 120 |  Loss_1: (0.2772) | Acc_1: (90.32%) (13989/15488)\n",
      "Epoch: 46 | Batch_idx: 130 |  Loss_1: (0.2761) | Acc_1: (90.31%) (15144/16768)\n",
      "Epoch: 46 | Batch_idx: 140 |  Loss_1: (0.2750) | Acc_1: (90.36%) (16309/18048)\n",
      "Epoch: 46 | Batch_idx: 150 |  Loss_1: (0.2754) | Acc_1: (90.36%) (17464/19328)\n",
      "Epoch: 46 | Batch_idx: 160 |  Loss_1: (0.2766) | Acc_1: (90.29%) (18606/20608)\n",
      "Epoch: 46 | Batch_idx: 170 |  Loss_1: (0.2761) | Acc_1: (90.30%) (19765/21888)\n",
      "Epoch: 46 | Batch_idx: 180 |  Loss_1: (0.2765) | Acc_1: (90.28%) (20917/23168)\n",
      "Epoch: 46 | Batch_idx: 190 |  Loss_1: (0.2773) | Acc_1: (90.24%) (22061/24448)\n",
      "Epoch: 46 | Batch_idx: 200 |  Loss_1: (0.2758) | Acc_1: (90.30%) (23232/25728)\n",
      "Epoch: 46 | Batch_idx: 210 |  Loss_1: (0.2774) | Acc_1: (90.27%) (24381/27008)\n",
      "Epoch: 46 | Batch_idx: 220 |  Loss_1: (0.2791) | Acc_1: (90.26%) (25532/28288)\n",
      "Epoch: 46 | Batch_idx: 230 |  Loss_1: (0.2797) | Acc_1: (90.23%) (26679/29568)\n",
      "Epoch: 46 | Batch_idx: 240 |  Loss_1: (0.2782) | Acc_1: (90.27%) (27845/30848)\n",
      "Epoch: 46 | Batch_idx: 250 |  Loss_1: (0.2798) | Acc_1: (90.22%) (28986/32128)\n",
      "Epoch: 46 | Batch_idx: 260 |  Loss_1: (0.2803) | Acc_1: (90.21%) (30139/33408)\n",
      "Epoch: 46 | Batch_idx: 270 |  Loss_1: (0.2796) | Acc_1: (90.23%) (31300/34688)\n",
      "Epoch: 46 | Batch_idx: 280 |  Loss_1: (0.2794) | Acc_1: (90.24%) (32457/35968)\n",
      "Epoch: 46 | Batch_idx: 290 |  Loss_1: (0.2791) | Acc_1: (90.23%) (33610/37248)\n",
      "Epoch: 46 | Batch_idx: 300 |  Loss_1: (0.2805) | Acc_1: (90.17%) (34741/38528)\n",
      "Epoch: 46 | Batch_idx: 310 |  Loss_1: (0.2813) | Acc_1: (90.13%) (35879/39808)\n",
      "Epoch: 46 | Batch_idx: 320 |  Loss_1: (0.2818) | Acc_1: (90.10%) (37019/41088)\n",
      "Epoch: 46 | Batch_idx: 330 |  Loss_1: (0.2834) | Acc_1: (90.03%) (38143/42368)\n",
      "Epoch: 46 | Batch_idx: 340 |  Loss_1: (0.2839) | Acc_1: (90.01%) (39287/43648)\n",
      "Epoch: 46 | Batch_idx: 350 |  Loss_1: (0.2853) | Acc_1: (89.97%) (40421/44928)\n",
      "Epoch: 46 | Batch_idx: 360 |  Loss_1: (0.2847) | Acc_1: (89.98%) (41578/46208)\n",
      "Epoch: 46 | Batch_idx: 370 |  Loss_1: (0.2846) | Acc_1: (89.99%) (42734/47488)\n",
      "Epoch: 46 | Batch_idx: 380 |  Loss_1: (0.2859) | Acc_1: (89.94%) (43862/48768)\n",
      "Epoch: 46 | Batch_idx: 390 |  Loss_1: (0.2857) | Acc_1: (89.95%) (44975/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3160) | Acc: (90.22%) (9022/10000)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.3020) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.2672) | Acc_1: (89.99%) (1267/1408)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.2710) | Acc_1: (89.99%) (2419/2688)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.2809) | Acc_1: (89.59%) (3555/3968)\n",
      "Epoch: 47 | Batch_idx: 40 |  Loss_1: (0.2785) | Acc_1: (89.84%) (4715/5248)\n",
      "Epoch: 47 | Batch_idx: 50 |  Loss_1: (0.2733) | Acc_1: (90.01%) (5876/6528)\n",
      "Epoch: 47 | Batch_idx: 60 |  Loss_1: (0.2753) | Acc_1: (90.02%) (7029/7808)\n",
      "Epoch: 47 | Batch_idx: 70 |  Loss_1: (0.2809) | Acc_1: (89.80%) (8161/9088)\n",
      "Epoch: 47 | Batch_idx: 80 |  Loss_1: (0.2784) | Acc_1: (90.01%) (9332/10368)\n",
      "Epoch: 47 | Batch_idx: 90 |  Loss_1: (0.2824) | Acc_1: (89.96%) (10478/11648)\n",
      "Epoch: 47 | Batch_idx: 100 |  Loss_1: (0.2812) | Acc_1: (90.00%) (11635/12928)\n",
      "Epoch: 47 | Batch_idx: 110 |  Loss_1: (0.2848) | Acc_1: (89.85%) (12766/14208)\n",
      "Epoch: 47 | Batch_idx: 120 |  Loss_1: (0.2835) | Acc_1: (89.90%) (13924/15488)\n",
      "Epoch: 47 | Batch_idx: 130 |  Loss_1: (0.2829) | Acc_1: (89.96%) (15085/16768)\n",
      "Epoch: 47 | Batch_idx: 140 |  Loss_1: (0.2820) | Acc_1: (89.99%) (16242/18048)\n",
      "Epoch: 47 | Batch_idx: 150 |  Loss_1: (0.2818) | Acc_1: (90.05%) (17405/19328)\n",
      "Epoch: 47 | Batch_idx: 160 |  Loss_1: (0.2827) | Acc_1: (90.03%) (18554/20608)\n",
      "Epoch: 47 | Batch_idx: 170 |  Loss_1: (0.2823) | Acc_1: (90.03%) (19706/21888)\n",
      "Epoch: 47 | Batch_idx: 180 |  Loss_1: (0.2834) | Acc_1: (90.02%) (20856/23168)\n",
      "Epoch: 47 | Batch_idx: 190 |  Loss_1: (0.2853) | Acc_1: (89.96%) (21993/24448)\n",
      "Epoch: 47 | Batch_idx: 200 |  Loss_1: (0.2856) | Acc_1: (89.92%) (23135/25728)\n",
      "Epoch: 47 | Batch_idx: 210 |  Loss_1: (0.2863) | Acc_1: (89.90%) (24280/27008)\n",
      "Epoch: 47 | Batch_idx: 220 |  Loss_1: (0.2862) | Acc_1: (89.90%) (25432/28288)\n",
      "Epoch: 47 | Batch_idx: 230 |  Loss_1: (0.2856) | Acc_1: (89.92%) (26587/29568)\n",
      "Epoch: 47 | Batch_idx: 240 |  Loss_1: (0.2857) | Acc_1: (89.89%) (27730/30848)\n",
      "Epoch: 47 | Batch_idx: 250 |  Loss_1: (0.2843) | Acc_1: (89.92%) (28889/32128)\n",
      "Epoch: 47 | Batch_idx: 260 |  Loss_1: (0.2833) | Acc_1: (89.93%) (30045/33408)\n",
      "Epoch: 47 | Batch_idx: 270 |  Loss_1: (0.2840) | Acc_1: (89.91%) (31187/34688)\n",
      "Epoch: 47 | Batch_idx: 280 |  Loss_1: (0.2835) | Acc_1: (89.94%) (32348/35968)\n",
      "Epoch: 47 | Batch_idx: 290 |  Loss_1: (0.2832) | Acc_1: (89.94%) (33500/37248)\n",
      "Epoch: 47 | Batch_idx: 300 |  Loss_1: (0.2822) | Acc_1: (89.98%) (34669/38528)\n",
      "Epoch: 47 | Batch_idx: 310 |  Loss_1: (0.2826) | Acc_1: (89.97%) (35817/39808)\n",
      "Epoch: 47 | Batch_idx: 320 |  Loss_1: (0.2829) | Acc_1: (89.95%) (36957/41088)\n",
      "Epoch: 47 | Batch_idx: 330 |  Loss_1: (0.2825) | Acc_1: (89.94%) (38105/42368)\n",
      "Epoch: 47 | Batch_idx: 340 |  Loss_1: (0.2824) | Acc_1: (89.93%) (39251/43648)\n",
      "Epoch: 47 | Batch_idx: 350 |  Loss_1: (0.2823) | Acc_1: (89.93%) (40404/44928)\n",
      "Epoch: 47 | Batch_idx: 360 |  Loss_1: (0.2841) | Acc_1: (89.89%) (41535/46208)\n",
      "Epoch: 47 | Batch_idx: 370 |  Loss_1: (0.2838) | Acc_1: (89.91%) (42697/47488)\n",
      "Epoch: 47 | Batch_idx: 380 |  Loss_1: (0.2839) | Acc_1: (89.91%) (43848/48768)\n",
      "Epoch: 47 | Batch_idx: 390 |  Loss_1: (0.2838) | Acc_1: (89.89%) (44946/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3416) | Acc: (89.02%) (8902/10000)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.3687) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.2779) | Acc_1: (89.63%) (1262/1408)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.2745) | Acc_1: (90.10%) (2422/2688)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.2889) | Acc_1: (90.12%) (3576/3968)\n",
      "Epoch: 48 | Batch_idx: 40 |  Loss_1: (0.2768) | Acc_1: (90.45%) (4747/5248)\n",
      "Epoch: 48 | Batch_idx: 50 |  Loss_1: (0.2770) | Acc_1: (90.24%) (5891/6528)\n",
      "Epoch: 48 | Batch_idx: 60 |  Loss_1: (0.2752) | Acc_1: (90.28%) (7049/7808)\n",
      "Epoch: 48 | Batch_idx: 70 |  Loss_1: (0.2759) | Acc_1: (90.26%) (8203/9088)\n",
      "Epoch: 48 | Batch_idx: 80 |  Loss_1: (0.2805) | Acc_1: (90.01%) (9332/10368)\n",
      "Epoch: 48 | Batch_idx: 90 |  Loss_1: (0.2796) | Acc_1: (90.02%) (10486/11648)\n",
      "Epoch: 48 | Batch_idx: 100 |  Loss_1: (0.2760) | Acc_1: (90.17%) (11657/12928)\n",
      "Epoch: 48 | Batch_idx: 110 |  Loss_1: (0.2748) | Acc_1: (90.20%) (12816/14208)\n",
      "Epoch: 48 | Batch_idx: 120 |  Loss_1: (0.2712) | Acc_1: (90.35%) (13994/15488)\n",
      "Epoch: 48 | Batch_idx: 130 |  Loss_1: (0.2706) | Acc_1: (90.37%) (15153/16768)\n",
      "Epoch: 48 | Batch_idx: 140 |  Loss_1: (0.2709) | Acc_1: (90.35%) (16307/18048)\n",
      "Epoch: 48 | Batch_idx: 150 |  Loss_1: (0.2715) | Acc_1: (90.29%) (17452/19328)\n",
      "Epoch: 48 | Batch_idx: 160 |  Loss_1: (0.2707) | Acc_1: (90.30%) (18609/20608)\n",
      "Epoch: 48 | Batch_idx: 170 |  Loss_1: (0.2719) | Acc_1: (90.27%) (19759/21888)\n",
      "Epoch: 48 | Batch_idx: 180 |  Loss_1: (0.2732) | Acc_1: (90.29%) (20918/23168)\n",
      "Epoch: 48 | Batch_idx: 190 |  Loss_1: (0.2725) | Acc_1: (90.30%) (22077/24448)\n",
      "Epoch: 48 | Batch_idx: 200 |  Loss_1: (0.2728) | Acc_1: (90.32%) (23238/25728)\n",
      "Epoch: 48 | Batch_idx: 210 |  Loss_1: (0.2708) | Acc_1: (90.40%) (24414/27008)\n",
      "Epoch: 48 | Batch_idx: 220 |  Loss_1: (0.2694) | Acc_1: (90.42%) (25577/28288)\n",
      "Epoch: 48 | Batch_idx: 230 |  Loss_1: (0.2696) | Acc_1: (90.40%) (26729/29568)\n",
      "Epoch: 48 | Batch_idx: 240 |  Loss_1: (0.2700) | Acc_1: (90.41%) (27891/30848)\n",
      "Epoch: 48 | Batch_idx: 250 |  Loss_1: (0.2709) | Acc_1: (90.34%) (29025/32128)\n",
      "Epoch: 48 | Batch_idx: 260 |  Loss_1: (0.2694) | Acc_1: (90.39%) (30198/33408)\n",
      "Epoch: 48 | Batch_idx: 270 |  Loss_1: (0.2712) | Acc_1: (90.35%) (31339/34688)\n",
      "Epoch: 48 | Batch_idx: 280 |  Loss_1: (0.2707) | Acc_1: (90.37%) (32504/35968)\n",
      "Epoch: 48 | Batch_idx: 290 |  Loss_1: (0.2718) | Acc_1: (90.34%) (33651/37248)\n",
      "Epoch: 48 | Batch_idx: 300 |  Loss_1: (0.2731) | Acc_1: (90.30%) (34789/38528)\n",
      "Epoch: 48 | Batch_idx: 310 |  Loss_1: (0.2733) | Acc_1: (90.29%) (35941/39808)\n",
      "Epoch: 48 | Batch_idx: 320 |  Loss_1: (0.2732) | Acc_1: (90.28%) (37093/41088)\n",
      "Epoch: 48 | Batch_idx: 330 |  Loss_1: (0.2738) | Acc_1: (90.24%) (38231/42368)\n",
      "Epoch: 48 | Batch_idx: 340 |  Loss_1: (0.2735) | Acc_1: (90.23%) (39382/43648)\n",
      "Epoch: 48 | Batch_idx: 350 |  Loss_1: (0.2732) | Acc_1: (90.23%) (40539/44928)\n",
      "Epoch: 48 | Batch_idx: 360 |  Loss_1: (0.2739) | Acc_1: (90.22%) (41688/46208)\n",
      "Epoch: 48 | Batch_idx: 370 |  Loss_1: (0.2740) | Acc_1: (90.20%) (42834/47488)\n",
      "Epoch: 48 | Batch_idx: 380 |  Loss_1: (0.2752) | Acc_1: (90.17%) (43975/48768)\n",
      "Epoch: 48 | Batch_idx: 390 |  Loss_1: (0.2761) | Acc_1: (90.17%) (45086/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3535) | Acc: (88.62%) (8862/10000)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.2187) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.2754) | Acc_1: (90.41%) (1273/1408)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.2872) | Acc_1: (89.58%) (2408/2688)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.2711) | Acc_1: (90.37%) (3586/3968)\n",
      "Epoch: 49 | Batch_idx: 40 |  Loss_1: (0.2605) | Acc_1: (90.78%) (4764/5248)\n",
      "Epoch: 49 | Batch_idx: 50 |  Loss_1: (0.2562) | Acc_1: (90.87%) (5932/6528)\n",
      "Epoch: 49 | Batch_idx: 60 |  Loss_1: (0.2628) | Acc_1: (90.57%) (7072/7808)\n",
      "Epoch: 49 | Batch_idx: 70 |  Loss_1: (0.2612) | Acc_1: (90.68%) (8241/9088)\n",
      "Epoch: 49 | Batch_idx: 80 |  Loss_1: (0.2597) | Acc_1: (90.77%) (9411/10368)\n",
      "Epoch: 49 | Batch_idx: 90 |  Loss_1: (0.2567) | Acc_1: (90.94%) (10593/11648)\n",
      "Epoch: 49 | Batch_idx: 100 |  Loss_1: (0.2600) | Acc_1: (90.82%) (11741/12928)\n",
      "Epoch: 49 | Batch_idx: 110 |  Loss_1: (0.2597) | Acc_1: (90.84%) (12906/14208)\n",
      "Epoch: 49 | Batch_idx: 120 |  Loss_1: (0.2617) | Acc_1: (90.90%) (14078/15488)\n",
      "Epoch: 49 | Batch_idx: 130 |  Loss_1: (0.2594) | Acc_1: (90.98%) (15256/16768)\n",
      "Epoch: 49 | Batch_idx: 140 |  Loss_1: (0.2598) | Acc_1: (90.99%) (16421/18048)\n",
      "Epoch: 49 | Batch_idx: 150 |  Loss_1: (0.2594) | Acc_1: (90.99%) (17587/19328)\n",
      "Epoch: 49 | Batch_idx: 160 |  Loss_1: (0.2612) | Acc_1: (90.92%) (18737/20608)\n",
      "Epoch: 49 | Batch_idx: 170 |  Loss_1: (0.2623) | Acc_1: (90.87%) (19889/21888)\n",
      "Epoch: 49 | Batch_idx: 180 |  Loss_1: (0.2626) | Acc_1: (90.84%) (21045/23168)\n",
      "Epoch: 49 | Batch_idx: 190 |  Loss_1: (0.2632) | Acc_1: (90.83%) (22206/24448)\n",
      "Epoch: 49 | Batch_idx: 200 |  Loss_1: (0.2652) | Acc_1: (90.77%) (23354/25728)\n",
      "Epoch: 49 | Batch_idx: 210 |  Loss_1: (0.2660) | Acc_1: (90.75%) (24510/27008)\n",
      "Epoch: 49 | Batch_idx: 220 |  Loss_1: (0.2665) | Acc_1: (90.73%) (25666/28288)\n",
      "Epoch: 49 | Batch_idx: 230 |  Loss_1: (0.2662) | Acc_1: (90.70%) (26819/29568)\n",
      "Epoch: 49 | Batch_idx: 240 |  Loss_1: (0.2676) | Acc_1: (90.66%) (27968/30848)\n",
      "Epoch: 49 | Batch_idx: 250 |  Loss_1: (0.2692) | Acc_1: (90.60%) (29108/32128)\n",
      "Epoch: 49 | Batch_idx: 260 |  Loss_1: (0.2677) | Acc_1: (90.66%) (30287/33408)\n",
      "Epoch: 49 | Batch_idx: 270 |  Loss_1: (0.2667) | Acc_1: (90.69%) (31460/34688)\n",
      "Epoch: 49 | Batch_idx: 280 |  Loss_1: (0.2669) | Acc_1: (90.68%) (32615/35968)\n",
      "Epoch: 49 | Batch_idx: 290 |  Loss_1: (0.2657) | Acc_1: (90.73%) (33795/37248)\n",
      "Epoch: 49 | Batch_idx: 300 |  Loss_1: (0.2657) | Acc_1: (90.73%) (34955/38528)\n",
      "Epoch: 49 | Batch_idx: 310 |  Loss_1: (0.2670) | Acc_1: (90.69%) (36102/39808)\n",
      "Epoch: 49 | Batch_idx: 320 |  Loss_1: (0.2657) | Acc_1: (90.72%) (37274/41088)\n",
      "Epoch: 49 | Batch_idx: 330 |  Loss_1: (0.2645) | Acc_1: (90.76%) (38452/42368)\n",
      "Epoch: 49 | Batch_idx: 340 |  Loss_1: (0.2643) | Acc_1: (90.76%) (39614/43648)\n",
      "Epoch: 49 | Batch_idx: 350 |  Loss_1: (0.2645) | Acc_1: (90.74%) (40769/44928)\n",
      "Epoch: 49 | Batch_idx: 360 |  Loss_1: (0.2654) | Acc_1: (90.72%) (41921/46208)\n",
      "Epoch: 49 | Batch_idx: 370 |  Loss_1: (0.2664) | Acc_1: (90.69%) (43066/47488)\n",
      "Epoch: 49 | Batch_idx: 380 |  Loss_1: (0.2670) | Acc_1: (90.64%) (44201/48768)\n",
      "Epoch: 49 | Batch_idx: 390 |  Loss_1: (0.2674) | Acc_1: (90.60%) (45302/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3494) | Acc: (88.73%) (8873/10000)\n",
      "Epoch: 50 | Batch_idx: 0 |  Loss_1: (0.1636) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 50 | Batch_idx: 10 |  Loss_1: (0.2731) | Acc_1: (90.34%) (1272/1408)\n",
      "Epoch: 50 | Batch_idx: 20 |  Loss_1: (0.2506) | Acc_1: (91.11%) (2449/2688)\n",
      "Epoch: 50 | Batch_idx: 30 |  Loss_1: (0.2440) | Acc_1: (91.36%) (3625/3968)\n",
      "Epoch: 50 | Batch_idx: 40 |  Loss_1: (0.2504) | Acc_1: (91.03%) (4777/5248)\n",
      "Epoch: 50 | Batch_idx: 50 |  Loss_1: (0.2432) | Acc_1: (91.48%) (5972/6528)\n",
      "Epoch: 50 | Batch_idx: 60 |  Loss_1: (0.2485) | Acc_1: (91.29%) (7128/7808)\n",
      "Epoch: 50 | Batch_idx: 70 |  Loss_1: (0.2472) | Acc_1: (91.31%) (8298/9088)\n",
      "Epoch: 50 | Batch_idx: 80 |  Loss_1: (0.2496) | Acc_1: (91.16%) (9451/10368)\n",
      "Epoch: 50 | Batch_idx: 90 |  Loss_1: (0.2537) | Acc_1: (90.98%) (10597/11648)\n",
      "Epoch: 50 | Batch_idx: 100 |  Loss_1: (0.2543) | Acc_1: (90.98%) (11762/12928)\n",
      "Epoch: 50 | Batch_idx: 110 |  Loss_1: (0.2557) | Acc_1: (90.95%) (12922/14208)\n",
      "Epoch: 50 | Batch_idx: 120 |  Loss_1: (0.2559) | Acc_1: (90.88%) (14076/15488)\n",
      "Epoch: 50 | Batch_idx: 130 |  Loss_1: (0.2544) | Acc_1: (90.94%) (15248/16768)\n",
      "Epoch: 50 | Batch_idx: 140 |  Loss_1: (0.2540) | Acc_1: (90.92%) (16410/18048)\n",
      "Epoch: 50 | Batch_idx: 150 |  Loss_1: (0.2571) | Acc_1: (90.87%) (17564/19328)\n",
      "Epoch: 50 | Batch_idx: 160 |  Loss_1: (0.2590) | Acc_1: (90.80%) (18713/20608)\n",
      "Epoch: 50 | Batch_idx: 170 |  Loss_1: (0.2604) | Acc_1: (90.77%) (19867/21888)\n",
      "Epoch: 50 | Batch_idx: 180 |  Loss_1: (0.2615) | Acc_1: (90.74%) (21023/23168)\n",
      "Epoch: 50 | Batch_idx: 190 |  Loss_1: (0.2625) | Acc_1: (90.67%) (22166/24448)\n",
      "Epoch: 50 | Batch_idx: 200 |  Loss_1: (0.2636) | Acc_1: (90.59%) (23307/25728)\n",
      "Epoch: 50 | Batch_idx: 210 |  Loss_1: (0.2652) | Acc_1: (90.58%) (24464/27008)\n",
      "Epoch: 50 | Batch_idx: 220 |  Loss_1: (0.2655) | Acc_1: (90.56%) (25619/28288)\n",
      "Epoch: 50 | Batch_idx: 230 |  Loss_1: (0.2661) | Acc_1: (90.53%) (26769/29568)\n",
      "Epoch: 50 | Batch_idx: 240 |  Loss_1: (0.2679) | Acc_1: (90.45%) (27902/30848)\n",
      "Epoch: 50 | Batch_idx: 250 |  Loss_1: (0.2684) | Acc_1: (90.43%) (29052/32128)\n",
      "Epoch: 50 | Batch_idx: 260 |  Loss_1: (0.2689) | Acc_1: (90.41%) (30204/33408)\n",
      "Epoch: 50 | Batch_idx: 270 |  Loss_1: (0.2683) | Acc_1: (90.43%) (31368/34688)\n",
      "Epoch: 50 | Batch_idx: 280 |  Loss_1: (0.2683) | Acc_1: (90.45%) (32532/35968)\n",
      "Epoch: 50 | Batch_idx: 290 |  Loss_1: (0.2696) | Acc_1: (90.41%) (33675/37248)\n",
      "Epoch: 50 | Batch_idx: 300 |  Loss_1: (0.2700) | Acc_1: (90.41%) (34834/38528)\n",
      "Epoch: 50 | Batch_idx: 310 |  Loss_1: (0.2709) | Acc_1: (90.39%) (35984/39808)\n",
      "Epoch: 50 | Batch_idx: 320 |  Loss_1: (0.2708) | Acc_1: (90.40%) (37144/41088)\n",
      "Epoch: 50 | Batch_idx: 330 |  Loss_1: (0.2707) | Acc_1: (90.40%) (38300/42368)\n",
      "Epoch: 50 | Batch_idx: 340 |  Loss_1: (0.2714) | Acc_1: (90.39%) (39452/43648)\n",
      "Epoch: 50 | Batch_idx: 350 |  Loss_1: (0.2722) | Acc_1: (90.36%) (40597/44928)\n",
      "Epoch: 50 | Batch_idx: 360 |  Loss_1: (0.2739) | Acc_1: (90.30%) (41727/46208)\n",
      "Epoch: 50 | Batch_idx: 370 |  Loss_1: (0.2725) | Acc_1: (90.33%) (42898/47488)\n",
      "Epoch: 50 | Batch_idx: 380 |  Loss_1: (0.2725) | Acc_1: (90.34%) (44058/48768)\n",
      "Epoch: 50 | Batch_idx: 390 |  Loss_1: (0.2729) | Acc_1: (90.34%) (45168/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3325) | Acc: (89.30%) (8930/10000)\n",
      "Epoch: 51 | Batch_idx: 0 |  Loss_1: (0.3518) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 51 | Batch_idx: 10 |  Loss_1: (0.2806) | Acc_1: (90.13%) (1269/1408)\n",
      "Epoch: 51 | Batch_idx: 20 |  Loss_1: (0.2629) | Acc_1: (90.48%) (2432/2688)\n",
      "Epoch: 51 | Batch_idx: 30 |  Loss_1: (0.2642) | Acc_1: (90.32%) (3584/3968)\n",
      "Epoch: 51 | Batch_idx: 40 |  Loss_1: (0.2567) | Acc_1: (90.61%) (4755/5248)\n",
      "Epoch: 51 | Batch_idx: 50 |  Loss_1: (0.2621) | Acc_1: (90.36%) (5899/6528)\n",
      "Epoch: 51 | Batch_idx: 60 |  Loss_1: (0.2630) | Acc_1: (90.46%) (7063/7808)\n",
      "Epoch: 51 | Batch_idx: 70 |  Loss_1: (0.2577) | Acc_1: (90.69%) (8242/9088)\n",
      "Epoch: 51 | Batch_idx: 80 |  Loss_1: (0.2601) | Acc_1: (90.61%) (9394/10368)\n",
      "Epoch: 51 | Batch_idx: 90 |  Loss_1: (0.2605) | Acc_1: (90.63%) (10557/11648)\n",
      "Epoch: 51 | Batch_idx: 100 |  Loss_1: (0.2640) | Acc_1: (90.52%) (11703/12928)\n",
      "Epoch: 51 | Batch_idx: 110 |  Loss_1: (0.2634) | Acc_1: (90.52%) (12861/14208)\n",
      "Epoch: 51 | Batch_idx: 120 |  Loss_1: (0.2638) | Acc_1: (90.57%) (14027/15488)\n",
      "Epoch: 51 | Batch_idx: 130 |  Loss_1: (0.2626) | Acc_1: (90.70%) (15208/16768)\n",
      "Epoch: 51 | Batch_idx: 140 |  Loss_1: (0.2613) | Acc_1: (90.78%) (16384/18048)\n",
      "Epoch: 51 | Batch_idx: 150 |  Loss_1: (0.2648) | Acc_1: (90.67%) (17525/19328)\n",
      "Epoch: 51 | Batch_idx: 160 |  Loss_1: (0.2656) | Acc_1: (90.65%) (18682/20608)\n",
      "Epoch: 51 | Batch_idx: 170 |  Loss_1: (0.2691) | Acc_1: (90.57%) (19823/21888)\n",
      "Epoch: 51 | Batch_idx: 180 |  Loss_1: (0.2693) | Acc_1: (90.54%) (20977/23168)\n",
      "Epoch: 51 | Batch_idx: 190 |  Loss_1: (0.2713) | Acc_1: (90.53%) (22133/24448)\n",
      "Epoch: 51 | Batch_idx: 200 |  Loss_1: (0.2702) | Acc_1: (90.54%) (23295/25728)\n",
      "Epoch: 51 | Batch_idx: 210 |  Loss_1: (0.2721) | Acc_1: (90.48%) (24436/27008)\n",
      "Epoch: 51 | Batch_idx: 220 |  Loss_1: (0.2737) | Acc_1: (90.44%) (25585/28288)\n",
      "Epoch: 51 | Batch_idx: 230 |  Loss_1: (0.2744) | Acc_1: (90.40%) (26730/29568)\n",
      "Epoch: 51 | Batch_idx: 240 |  Loss_1: (0.2736) | Acc_1: (90.42%) (27892/30848)\n",
      "Epoch: 51 | Batch_idx: 250 |  Loss_1: (0.2735) | Acc_1: (90.44%) (29056/32128)\n",
      "Epoch: 51 | Batch_idx: 260 |  Loss_1: (0.2733) | Acc_1: (90.43%) (30212/33408)\n",
      "Epoch: 51 | Batch_idx: 270 |  Loss_1: (0.2720) | Acc_1: (90.47%) (31382/34688)\n",
      "Epoch: 51 | Batch_idx: 280 |  Loss_1: (0.2719) | Acc_1: (90.46%) (32535/35968)\n",
      "Epoch: 51 | Batch_idx: 290 |  Loss_1: (0.2705) | Acc_1: (90.50%) (33710/37248)\n",
      "Epoch: 51 | Batch_idx: 300 |  Loss_1: (0.2712) | Acc_1: (90.45%) (34848/38528)\n",
      "Epoch: 51 | Batch_idx: 310 |  Loss_1: (0.2711) | Acc_1: (90.47%) (36016/39808)\n",
      "Epoch: 51 | Batch_idx: 320 |  Loss_1: (0.2704) | Acc_1: (90.48%) (37177/41088)\n",
      "Epoch: 51 | Batch_idx: 330 |  Loss_1: (0.2708) | Acc_1: (90.47%) (38332/42368)\n",
      "Epoch: 51 | Batch_idx: 340 |  Loss_1: (0.2700) | Acc_1: (90.49%) (39496/43648)\n",
      "Epoch: 51 | Batch_idx: 350 |  Loss_1: (0.2696) | Acc_1: (90.50%) (40658/44928)\n",
      "Epoch: 51 | Batch_idx: 360 |  Loss_1: (0.2685) | Acc_1: (90.53%) (41834/46208)\n",
      "Epoch: 51 | Batch_idx: 370 |  Loss_1: (0.2689) | Acc_1: (90.53%) (42993/47488)\n",
      "Epoch: 51 | Batch_idx: 380 |  Loss_1: (0.2675) | Acc_1: (90.57%) (44170/48768)\n",
      "Epoch: 51 | Batch_idx: 390 |  Loss_1: (0.2674) | Acc_1: (90.57%) (45287/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2932) | Acc: (90.64%) (9064/10000)\n",
      "Epoch: 52 | Batch_idx: 0 |  Loss_1: (0.1880) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 52 | Batch_idx: 10 |  Loss_1: (0.2418) | Acc_1: (91.19%) (1284/1408)\n",
      "Epoch: 52 | Batch_idx: 20 |  Loss_1: (0.2364) | Acc_1: (91.56%) (2461/2688)\n",
      "Epoch: 52 | Batch_idx: 30 |  Loss_1: (0.2566) | Acc_1: (90.88%) (3606/3968)\n",
      "Epoch: 52 | Batch_idx: 40 |  Loss_1: (0.2576) | Acc_1: (90.74%) (4762/5248)\n",
      "Epoch: 52 | Batch_idx: 50 |  Loss_1: (0.2575) | Acc_1: (90.89%) (5933/6528)\n",
      "Epoch: 52 | Batch_idx: 60 |  Loss_1: (0.2613) | Acc_1: (90.75%) (7086/7808)\n",
      "Epoch: 52 | Batch_idx: 70 |  Loss_1: (0.2619) | Acc_1: (90.75%) (8247/9088)\n",
      "Epoch: 52 | Batch_idx: 80 |  Loss_1: (0.2606) | Acc_1: (90.72%) (9406/10368)\n",
      "Epoch: 52 | Batch_idx: 90 |  Loss_1: (0.2587) | Acc_1: (90.76%) (10572/11648)\n",
      "Epoch: 52 | Batch_idx: 100 |  Loss_1: (0.2620) | Acc_1: (90.66%) (11720/12928)\n",
      "Epoch: 52 | Batch_idx: 110 |  Loss_1: (0.2593) | Acc_1: (90.78%) (12898/14208)\n",
      "Epoch: 52 | Batch_idx: 120 |  Loss_1: (0.2603) | Acc_1: (90.81%) (14064/15488)\n",
      "Epoch: 52 | Batch_idx: 130 |  Loss_1: (0.2598) | Acc_1: (90.81%) (15227/16768)\n",
      "Epoch: 52 | Batch_idx: 140 |  Loss_1: (0.2592) | Acc_1: (90.82%) (16392/18048)\n",
      "Epoch: 52 | Batch_idx: 150 |  Loss_1: (0.2607) | Acc_1: (90.78%) (17546/19328)\n",
      "Epoch: 52 | Batch_idx: 160 |  Loss_1: (0.2598) | Acc_1: (90.79%) (18709/20608)\n",
      "Epoch: 52 | Batch_idx: 170 |  Loss_1: (0.2598) | Acc_1: (90.82%) (19878/21888)\n",
      "Epoch: 52 | Batch_idx: 180 |  Loss_1: (0.2585) | Acc_1: (90.86%) (21051/23168)\n",
      "Epoch: 52 | Batch_idx: 190 |  Loss_1: (0.2578) | Acc_1: (90.86%) (22213/24448)\n",
      "Epoch: 52 | Batch_idx: 200 |  Loss_1: (0.2586) | Acc_1: (90.80%) (23360/25728)\n",
      "Epoch: 52 | Batch_idx: 210 |  Loss_1: (0.2608) | Acc_1: (90.70%) (24497/27008)\n",
      "Epoch: 52 | Batch_idx: 220 |  Loss_1: (0.2612) | Acc_1: (90.71%) (25659/28288)\n",
      "Epoch: 52 | Batch_idx: 230 |  Loss_1: (0.2624) | Acc_1: (90.70%) (26817/29568)\n",
      "Epoch: 52 | Batch_idx: 240 |  Loss_1: (0.2618) | Acc_1: (90.70%) (27978/30848)\n",
      "Epoch: 52 | Batch_idx: 250 |  Loss_1: (0.2626) | Acc_1: (90.65%) (29125/32128)\n",
      "Epoch: 52 | Batch_idx: 260 |  Loss_1: (0.2627) | Acc_1: (90.66%) (30289/33408)\n",
      "Epoch: 52 | Batch_idx: 270 |  Loss_1: (0.2616) | Acc_1: (90.70%) (31462/34688)\n",
      "Epoch: 52 | Batch_idx: 280 |  Loss_1: (0.2619) | Acc_1: (90.67%) (32613/35968)\n",
      "Epoch: 52 | Batch_idx: 290 |  Loss_1: (0.2612) | Acc_1: (90.68%) (33778/37248)\n",
      "Epoch: 52 | Batch_idx: 300 |  Loss_1: (0.2607) | Acc_1: (90.72%) (34953/38528)\n",
      "Epoch: 52 | Batch_idx: 310 |  Loss_1: (0.2613) | Acc_1: (90.71%) (36108/39808)\n",
      "Epoch: 52 | Batch_idx: 320 |  Loss_1: (0.2612) | Acc_1: (90.71%) (37272/41088)\n",
      "Epoch: 52 | Batch_idx: 330 |  Loss_1: (0.2617) | Acc_1: (90.70%) (38427/42368)\n",
      "Epoch: 52 | Batch_idx: 340 |  Loss_1: (0.2617) | Acc_1: (90.69%) (39586/43648)\n",
      "Epoch: 52 | Batch_idx: 350 |  Loss_1: (0.2616) | Acc_1: (90.71%) (40752/44928)\n",
      "Epoch: 52 | Batch_idx: 360 |  Loss_1: (0.2618) | Acc_1: (90.70%) (41912/46208)\n",
      "Epoch: 52 | Batch_idx: 370 |  Loss_1: (0.2617) | Acc_1: (90.71%) (43078/47488)\n",
      "Epoch: 52 | Batch_idx: 380 |  Loss_1: (0.2631) | Acc_1: (90.66%) (44211/48768)\n",
      "Epoch: 52 | Batch_idx: 390 |  Loss_1: (0.2629) | Acc_1: (90.67%) (45333/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3357) | Acc: (89.23%) (8923/10000)\n",
      "Epoch: 53 | Batch_idx: 0 |  Loss_1: (0.1620) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 53 | Batch_idx: 10 |  Loss_1: (0.2241) | Acc_1: (92.40%) (1301/1408)\n",
      "Epoch: 53 | Batch_idx: 20 |  Loss_1: (0.2377) | Acc_1: (91.59%) (2462/2688)\n",
      "Epoch: 53 | Batch_idx: 30 |  Loss_1: (0.2420) | Acc_1: (91.31%) (3623/3968)\n",
      "Epoch: 53 | Batch_idx: 40 |  Loss_1: (0.2440) | Acc_1: (91.43%) (4798/5248)\n",
      "Epoch: 53 | Batch_idx: 50 |  Loss_1: (0.2466) | Acc_1: (91.39%) (5966/6528)\n",
      "Epoch: 53 | Batch_idx: 60 |  Loss_1: (0.2441) | Acc_1: (91.34%) (7132/7808)\n",
      "Epoch: 53 | Batch_idx: 70 |  Loss_1: (0.2465) | Acc_1: (91.24%) (8292/9088)\n",
      "Epoch: 53 | Batch_idx: 80 |  Loss_1: (0.2439) | Acc_1: (91.43%) (9479/10368)\n",
      "Epoch: 53 | Batch_idx: 90 |  Loss_1: (0.2437) | Acc_1: (91.44%) (10651/11648)\n",
      "Epoch: 53 | Batch_idx: 100 |  Loss_1: (0.2454) | Acc_1: (91.33%) (11807/12928)\n",
      "Epoch: 53 | Batch_idx: 110 |  Loss_1: (0.2470) | Acc_1: (91.24%) (12964/14208)\n",
      "Epoch: 53 | Batch_idx: 120 |  Loss_1: (0.2475) | Acc_1: (91.17%) (14120/15488)\n",
      "Epoch: 53 | Batch_idx: 130 |  Loss_1: (0.2489) | Acc_1: (91.09%) (15274/16768)\n",
      "Epoch: 53 | Batch_idx: 140 |  Loss_1: (0.2501) | Acc_1: (91.00%) (16423/18048)\n",
      "Epoch: 53 | Batch_idx: 150 |  Loss_1: (0.2511) | Acc_1: (90.97%) (17582/19328)\n",
      "Epoch: 53 | Batch_idx: 160 |  Loss_1: (0.2525) | Acc_1: (90.93%) (18738/20608)\n",
      "Epoch: 53 | Batch_idx: 170 |  Loss_1: (0.2527) | Acc_1: (90.96%) (19909/21888)\n",
      "Epoch: 53 | Batch_idx: 180 |  Loss_1: (0.2515) | Acc_1: (91.00%) (21083/23168)\n",
      "Epoch: 53 | Batch_idx: 190 |  Loss_1: (0.2517) | Acc_1: (90.97%) (22241/24448)\n",
      "Epoch: 53 | Batch_idx: 200 |  Loss_1: (0.2510) | Acc_1: (91.01%) (23414/25728)\n",
      "Epoch: 53 | Batch_idx: 210 |  Loss_1: (0.2489) | Acc_1: (91.11%) (24607/27008)\n",
      "Epoch: 53 | Batch_idx: 220 |  Loss_1: (0.2482) | Acc_1: (91.11%) (25773/28288)\n",
      "Epoch: 53 | Batch_idx: 230 |  Loss_1: (0.2493) | Acc_1: (91.06%) (26926/29568)\n",
      "Epoch: 53 | Batch_idx: 240 |  Loss_1: (0.2492) | Acc_1: (91.10%) (28101/30848)\n",
      "Epoch: 53 | Batch_idx: 250 |  Loss_1: (0.2481) | Acc_1: (91.14%) (29282/32128)\n",
      "Epoch: 53 | Batch_idx: 260 |  Loss_1: (0.2490) | Acc_1: (91.13%) (30446/33408)\n",
      "Epoch: 53 | Batch_idx: 270 |  Loss_1: (0.2512) | Acc_1: (91.07%) (31592/34688)\n",
      "Epoch: 53 | Batch_idx: 280 |  Loss_1: (0.2507) | Acc_1: (91.09%) (32762/35968)\n",
      "Epoch: 53 | Batch_idx: 290 |  Loss_1: (0.2512) | Acc_1: (91.06%) (33919/37248)\n",
      "Epoch: 53 | Batch_idx: 300 |  Loss_1: (0.2513) | Acc_1: (91.07%) (35087/38528)\n",
      "Epoch: 53 | Batch_idx: 310 |  Loss_1: (0.2519) | Acc_1: (91.03%) (36238/39808)\n",
      "Epoch: 53 | Batch_idx: 320 |  Loss_1: (0.2526) | Acc_1: (91.01%) (37393/41088)\n",
      "Epoch: 53 | Batch_idx: 330 |  Loss_1: (0.2521) | Acc_1: (91.05%) (38575/42368)\n",
      "Epoch: 53 | Batch_idx: 340 |  Loss_1: (0.2525) | Acc_1: (91.02%) (39729/43648)\n",
      "Epoch: 53 | Batch_idx: 350 |  Loss_1: (0.2528) | Acc_1: (91.01%) (40889/44928)\n",
      "Epoch: 53 | Batch_idx: 360 |  Loss_1: (0.2533) | Acc_1: (90.97%) (42037/46208)\n",
      "Epoch: 53 | Batch_idx: 370 |  Loss_1: (0.2539) | Acc_1: (90.96%) (43193/47488)\n",
      "Epoch: 53 | Batch_idx: 380 |  Loss_1: (0.2538) | Acc_1: (90.97%) (44362/48768)\n",
      "Epoch: 53 | Batch_idx: 390 |  Loss_1: (0.2536) | Acc_1: (90.98%) (45489/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3375) | Acc: (89.24%) (8924/10000)\n",
      "Epoch: 54 | Batch_idx: 0 |  Loss_1: (0.2978) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 54 | Batch_idx: 10 |  Loss_1: (0.2730) | Acc_1: (90.48%) (1274/1408)\n",
      "Epoch: 54 | Batch_idx: 20 |  Loss_1: (0.2907) | Acc_1: (89.96%) (2418/2688)\n",
      "Epoch: 54 | Batch_idx: 30 |  Loss_1: (0.2774) | Acc_1: (90.27%) (3582/3968)\n",
      "Epoch: 54 | Batch_idx: 40 |  Loss_1: (0.2753) | Acc_1: (90.21%) (4734/5248)\n",
      "Epoch: 54 | Batch_idx: 50 |  Loss_1: (0.2709) | Acc_1: (90.33%) (5897/6528)\n",
      "Epoch: 54 | Batch_idx: 60 |  Loss_1: (0.2751) | Acc_1: (90.28%) (7049/7808)\n",
      "Epoch: 54 | Batch_idx: 70 |  Loss_1: (0.2697) | Acc_1: (90.42%) (8217/9088)\n",
      "Epoch: 54 | Batch_idx: 80 |  Loss_1: (0.2674) | Acc_1: (90.46%) (9379/10368)\n",
      "Epoch: 54 | Batch_idx: 90 |  Loss_1: (0.2694) | Acc_1: (90.28%) (10516/11648)\n",
      "Epoch: 54 | Batch_idx: 100 |  Loss_1: (0.2672) | Acc_1: (90.41%) (11688/12928)\n",
      "Epoch: 54 | Batch_idx: 110 |  Loss_1: (0.2658) | Acc_1: (90.51%) (12860/14208)\n",
      "Epoch: 54 | Batch_idx: 120 |  Loss_1: (0.2638) | Acc_1: (90.61%) (14034/15488)\n",
      "Epoch: 54 | Batch_idx: 130 |  Loss_1: (0.2628) | Acc_1: (90.67%) (15203/16768)\n",
      "Epoch: 54 | Batch_idx: 140 |  Loss_1: (0.2607) | Acc_1: (90.71%) (16372/18048)\n",
      "Epoch: 54 | Batch_idx: 150 |  Loss_1: (0.2612) | Acc_1: (90.76%) (17543/19328)\n",
      "Epoch: 54 | Batch_idx: 160 |  Loss_1: (0.2618) | Acc_1: (90.74%) (18699/20608)\n",
      "Epoch: 54 | Batch_idx: 170 |  Loss_1: (0.2625) | Acc_1: (90.74%) (19861/21888)\n",
      "Epoch: 54 | Batch_idx: 180 |  Loss_1: (0.2611) | Acc_1: (90.77%) (21030/23168)\n",
      "Epoch: 54 | Batch_idx: 190 |  Loss_1: (0.2595) | Acc_1: (90.81%) (22202/24448)\n",
      "Epoch: 54 | Batch_idx: 200 |  Loss_1: (0.2576) | Acc_1: (90.87%) (23380/25728)\n",
      "Epoch: 54 | Batch_idx: 210 |  Loss_1: (0.2553) | Acc_1: (90.97%) (24570/27008)\n",
      "Epoch: 54 | Batch_idx: 220 |  Loss_1: (0.2541) | Acc_1: (90.99%) (25739/28288)\n",
      "Epoch: 54 | Batch_idx: 230 |  Loss_1: (0.2529) | Acc_1: (91.02%) (26914/29568)\n",
      "Epoch: 54 | Batch_idx: 240 |  Loss_1: (0.2521) | Acc_1: (91.04%) (28085/30848)\n",
      "Epoch: 54 | Batch_idx: 250 |  Loss_1: (0.2510) | Acc_1: (91.10%) (29270/32128)\n",
      "Epoch: 54 | Batch_idx: 260 |  Loss_1: (0.2514) | Acc_1: (91.10%) (30436/33408)\n",
      "Epoch: 54 | Batch_idx: 270 |  Loss_1: (0.2519) | Acc_1: (91.10%) (31601/34688)\n",
      "Epoch: 54 | Batch_idx: 280 |  Loss_1: (0.2529) | Acc_1: (91.05%) (32749/35968)\n",
      "Epoch: 54 | Batch_idx: 290 |  Loss_1: (0.2528) | Acc_1: (91.04%) (33909/37248)\n",
      "Epoch: 54 | Batch_idx: 300 |  Loss_1: (0.2516) | Acc_1: (91.05%) (35081/38528)\n",
      "Epoch: 54 | Batch_idx: 310 |  Loss_1: (0.2527) | Acc_1: (91.02%) (36235/39808)\n",
      "Epoch: 54 | Batch_idx: 320 |  Loss_1: (0.2533) | Acc_1: (91.02%) (37398/41088)\n",
      "Epoch: 54 | Batch_idx: 330 |  Loss_1: (0.2522) | Acc_1: (91.07%) (38583/42368)\n",
      "Epoch: 54 | Batch_idx: 340 |  Loss_1: (0.2529) | Acc_1: (91.03%) (39732/43648)\n",
      "Epoch: 54 | Batch_idx: 350 |  Loss_1: (0.2528) | Acc_1: (91.02%) (40893/44928)\n",
      "Epoch: 54 | Batch_idx: 360 |  Loss_1: (0.2526) | Acc_1: (91.01%) (42055/46208)\n",
      "Epoch: 54 | Batch_idx: 370 |  Loss_1: (0.2520) | Acc_1: (91.01%) (43219/47488)\n",
      "Epoch: 54 | Batch_idx: 380 |  Loss_1: (0.2526) | Acc_1: (91.00%) (44380/48768)\n",
      "Epoch: 54 | Batch_idx: 390 |  Loss_1: (0.2529) | Acc_1: (90.98%) (45491/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2987) | Acc: (90.46%) (9046/10000)\n",
      "Epoch: 55 | Batch_idx: 0 |  Loss_1: (0.1990) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 55 | Batch_idx: 10 |  Loss_1: (0.2244) | Acc_1: (91.41%) (1287/1408)\n",
      "Epoch: 55 | Batch_idx: 20 |  Loss_1: (0.2321) | Acc_1: (91.44%) (2458/2688)\n",
      "Epoch: 55 | Batch_idx: 30 |  Loss_1: (0.2265) | Acc_1: (91.63%) (3636/3968)\n",
      "Epoch: 55 | Batch_idx: 40 |  Loss_1: (0.2286) | Acc_1: (91.63%) (4809/5248)\n",
      "Epoch: 55 | Batch_idx: 50 |  Loss_1: (0.2258) | Acc_1: (91.73%) (5988/6528)\n",
      "Epoch: 55 | Batch_idx: 60 |  Loss_1: (0.2281) | Acc_1: (91.65%) (7156/7808)\n",
      "Epoch: 55 | Batch_idx: 70 |  Loss_1: (0.2291) | Acc_1: (91.64%) (8328/9088)\n",
      "Epoch: 55 | Batch_idx: 80 |  Loss_1: (0.2276) | Acc_1: (91.73%) (9511/10368)\n",
      "Epoch: 55 | Batch_idx: 90 |  Loss_1: (0.2266) | Acc_1: (91.78%) (10690/11648)\n",
      "Epoch: 55 | Batch_idx: 100 |  Loss_1: (0.2317) | Acc_1: (91.62%) (11844/12928)\n",
      "Epoch: 55 | Batch_idx: 110 |  Loss_1: (0.2328) | Acc_1: (91.56%) (13009/14208)\n",
      "Epoch: 55 | Batch_idx: 120 |  Loss_1: (0.2351) | Acc_1: (91.55%) (14179/15488)\n",
      "Epoch: 55 | Batch_idx: 130 |  Loss_1: (0.2370) | Acc_1: (91.44%) (15332/16768)\n",
      "Epoch: 55 | Batch_idx: 140 |  Loss_1: (0.2381) | Acc_1: (91.40%) (16495/18048)\n",
      "Epoch: 55 | Batch_idx: 150 |  Loss_1: (0.2392) | Acc_1: (91.37%) (17660/19328)\n",
      "Epoch: 55 | Batch_idx: 160 |  Loss_1: (0.2401) | Acc_1: (91.36%) (18827/20608)\n",
      "Epoch: 55 | Batch_idx: 170 |  Loss_1: (0.2378) | Acc_1: (91.47%) (20022/21888)\n",
      "Epoch: 55 | Batch_idx: 180 |  Loss_1: (0.2397) | Acc_1: (91.40%) (21176/23168)\n",
      "Epoch: 55 | Batch_idx: 190 |  Loss_1: (0.2417) | Acc_1: (91.33%) (22328/24448)\n",
      "Epoch: 55 | Batch_idx: 200 |  Loss_1: (0.2418) | Acc_1: (91.33%) (23498/25728)\n",
      "Epoch: 55 | Batch_idx: 210 |  Loss_1: (0.2417) | Acc_1: (91.34%) (24669/27008)\n",
      "Epoch: 55 | Batch_idx: 220 |  Loss_1: (0.2435) | Acc_1: (91.29%) (25825/28288)\n",
      "Epoch: 55 | Batch_idx: 230 |  Loss_1: (0.2433) | Acc_1: (91.27%) (26986/29568)\n",
      "Epoch: 55 | Batch_idx: 240 |  Loss_1: (0.2437) | Acc_1: (91.28%) (28158/30848)\n",
      "Epoch: 55 | Batch_idx: 250 |  Loss_1: (0.2440) | Acc_1: (91.27%) (29322/32128)\n",
      "Epoch: 55 | Batch_idx: 260 |  Loss_1: (0.2459) | Acc_1: (91.22%) (30476/33408)\n",
      "Epoch: 55 | Batch_idx: 270 |  Loss_1: (0.2477) | Acc_1: (91.16%) (31620/34688)\n",
      "Epoch: 55 | Batch_idx: 280 |  Loss_1: (0.2493) | Acc_1: (91.12%) (32773/35968)\n",
      "Epoch: 55 | Batch_idx: 290 |  Loss_1: (0.2494) | Acc_1: (91.11%) (33937/37248)\n",
      "Epoch: 55 | Batch_idx: 300 |  Loss_1: (0.2492) | Acc_1: (91.11%) (35104/38528)\n",
      "Epoch: 55 | Batch_idx: 310 |  Loss_1: (0.2494) | Acc_1: (91.12%) (36274/39808)\n",
      "Epoch: 55 | Batch_idx: 320 |  Loss_1: (0.2483) | Acc_1: (91.15%) (37450/41088)\n",
      "Epoch: 55 | Batch_idx: 330 |  Loss_1: (0.2504) | Acc_1: (91.09%) (38593/42368)\n",
      "Epoch: 55 | Batch_idx: 340 |  Loss_1: (0.2508) | Acc_1: (91.07%) (39752/43648)\n",
      "Epoch: 55 | Batch_idx: 350 |  Loss_1: (0.2509) | Acc_1: (91.08%) (40921/44928)\n",
      "Epoch: 55 | Batch_idx: 360 |  Loss_1: (0.2506) | Acc_1: (91.09%) (42092/46208)\n",
      "Epoch: 55 | Batch_idx: 370 |  Loss_1: (0.2501) | Acc_1: (91.10%) (43263/47488)\n",
      "Epoch: 55 | Batch_idx: 380 |  Loss_1: (0.2493) | Acc_1: (91.14%) (44445/48768)\n",
      "Epoch: 55 | Batch_idx: 390 |  Loss_1: (0.2482) | Acc_1: (91.18%) (45589/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2994) | Acc: (90.72%) (9072/10000)\n",
      "Epoch: 56 | Batch_idx: 0 |  Loss_1: (0.1932) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 56 | Batch_idx: 10 |  Loss_1: (0.2253) | Acc_1: (91.76%) (1292/1408)\n",
      "Epoch: 56 | Batch_idx: 20 |  Loss_1: (0.2266) | Acc_1: (91.67%) (2464/2688)\n",
      "Epoch: 56 | Batch_idx: 30 |  Loss_1: (0.2265) | Acc_1: (91.86%) (3645/3968)\n",
      "Epoch: 56 | Batch_idx: 40 |  Loss_1: (0.2257) | Acc_1: (91.81%) (4818/5248)\n",
      "Epoch: 56 | Batch_idx: 50 |  Loss_1: (0.2304) | Acc_1: (91.57%) (5978/6528)\n",
      "Epoch: 56 | Batch_idx: 60 |  Loss_1: (0.2321) | Acc_1: (91.56%) (7149/7808)\n",
      "Epoch: 56 | Batch_idx: 70 |  Loss_1: (0.2345) | Acc_1: (91.47%) (8313/9088)\n",
      "Epoch: 56 | Batch_idx: 80 |  Loss_1: (0.2462) | Acc_1: (91.11%) (9446/10368)\n",
      "Epoch: 56 | Batch_idx: 90 |  Loss_1: (0.2474) | Acc_1: (91.11%) (10612/11648)\n",
      "Epoch: 56 | Batch_idx: 100 |  Loss_1: (0.2514) | Acc_1: (90.98%) (11762/12928)\n",
      "Epoch: 56 | Batch_idx: 110 |  Loss_1: (0.2520) | Acc_1: (90.96%) (12923/14208)\n",
      "Epoch: 56 | Batch_idx: 120 |  Loss_1: (0.2527) | Acc_1: (90.95%) (14086/15488)\n",
      "Epoch: 56 | Batch_idx: 130 |  Loss_1: (0.2522) | Acc_1: (90.99%) (15258/16768)\n",
      "Epoch: 56 | Batch_idx: 140 |  Loss_1: (0.2512) | Acc_1: (91.05%) (16432/18048)\n",
      "Epoch: 56 | Batch_idx: 150 |  Loss_1: (0.2505) | Acc_1: (91.05%) (17599/19328)\n",
      "Epoch: 56 | Batch_idx: 160 |  Loss_1: (0.2485) | Acc_1: (91.15%) (18785/20608)\n",
      "Epoch: 56 | Batch_idx: 170 |  Loss_1: (0.2487) | Acc_1: (91.16%) (19953/21888)\n",
      "Epoch: 56 | Batch_idx: 180 |  Loss_1: (0.2477) | Acc_1: (91.15%) (21117/23168)\n",
      "Epoch: 56 | Batch_idx: 190 |  Loss_1: (0.2491) | Acc_1: (91.11%) (22275/24448)\n",
      "Epoch: 56 | Batch_idx: 200 |  Loss_1: (0.2475) | Acc_1: (91.18%) (23460/25728)\n",
      "Epoch: 56 | Batch_idx: 210 |  Loss_1: (0.2466) | Acc_1: (91.20%) (24632/27008)\n",
      "Epoch: 56 | Batch_idx: 220 |  Loss_1: (0.2463) | Acc_1: (91.20%) (25799/28288)\n",
      "Epoch: 56 | Batch_idx: 230 |  Loss_1: (0.2465) | Acc_1: (91.17%) (26957/29568)\n",
      "Epoch: 56 | Batch_idx: 240 |  Loss_1: (0.2475) | Acc_1: (91.14%) (28114/30848)\n",
      "Epoch: 56 | Batch_idx: 250 |  Loss_1: (0.2466) | Acc_1: (91.14%) (29282/32128)\n",
      "Epoch: 56 | Batch_idx: 260 |  Loss_1: (0.2475) | Acc_1: (91.12%) (30440/33408)\n",
      "Epoch: 56 | Batch_idx: 270 |  Loss_1: (0.2470) | Acc_1: (91.16%) (31620/34688)\n",
      "Epoch: 56 | Batch_idx: 280 |  Loss_1: (0.2473) | Acc_1: (91.17%) (32791/35968)\n",
      "Epoch: 56 | Batch_idx: 290 |  Loss_1: (0.2471) | Acc_1: (91.19%) (33966/37248)\n",
      "Epoch: 56 | Batch_idx: 300 |  Loss_1: (0.2474) | Acc_1: (91.19%) (35132/38528)\n",
      "Epoch: 56 | Batch_idx: 310 |  Loss_1: (0.2472) | Acc_1: (91.18%) (36298/39808)\n",
      "Epoch: 56 | Batch_idx: 320 |  Loss_1: (0.2477) | Acc_1: (91.17%) (37458/41088)\n",
      "Epoch: 56 | Batch_idx: 330 |  Loss_1: (0.2473) | Acc_1: (91.19%) (38637/42368)\n",
      "Epoch: 56 | Batch_idx: 340 |  Loss_1: (0.2475) | Acc_1: (91.18%) (39800/43648)\n",
      "Epoch: 56 | Batch_idx: 350 |  Loss_1: (0.2475) | Acc_1: (91.18%) (40966/44928)\n",
      "Epoch: 56 | Batch_idx: 360 |  Loss_1: (0.2486) | Acc_1: (91.16%) (42122/46208)\n",
      "Epoch: 56 | Batch_idx: 370 |  Loss_1: (0.2488) | Acc_1: (91.16%) (43289/47488)\n",
      "Epoch: 56 | Batch_idx: 380 |  Loss_1: (0.2490) | Acc_1: (91.16%) (44458/48768)\n",
      "Epoch: 56 | Batch_idx: 390 |  Loss_1: (0.2499) | Acc_1: (91.14%) (45572/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3106) | Acc: (90.48%) (9048/10000)\n",
      "Epoch: 57 | Batch_idx: 0 |  Loss_1: (0.2094) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 57 | Batch_idx: 10 |  Loss_1: (0.2344) | Acc_1: (91.69%) (1291/1408)\n",
      "Epoch: 57 | Batch_idx: 20 |  Loss_1: (0.2248) | Acc_1: (91.89%) (2470/2688)\n",
      "Epoch: 57 | Batch_idx: 30 |  Loss_1: (0.2270) | Acc_1: (91.83%) (3644/3968)\n",
      "Epoch: 57 | Batch_idx: 40 |  Loss_1: (0.2294) | Acc_1: (91.75%) (4815/5248)\n",
      "Epoch: 57 | Batch_idx: 50 |  Loss_1: (0.2320) | Acc_1: (91.51%) (5974/6528)\n",
      "Epoch: 57 | Batch_idx: 60 |  Loss_1: (0.2364) | Acc_1: (91.34%) (7132/7808)\n",
      "Epoch: 57 | Batch_idx: 70 |  Loss_1: (0.2372) | Acc_1: (91.36%) (8303/9088)\n",
      "Epoch: 57 | Batch_idx: 80 |  Loss_1: (0.2331) | Acc_1: (91.52%) (9489/10368)\n",
      "Epoch: 57 | Batch_idx: 90 |  Loss_1: (0.2381) | Acc_1: (91.35%) (10640/11648)\n",
      "Epoch: 57 | Batch_idx: 100 |  Loss_1: (0.2350) | Acc_1: (91.49%) (11828/12928)\n",
      "Epoch: 57 | Batch_idx: 110 |  Loss_1: (0.2369) | Acc_1: (91.34%) (12977/14208)\n",
      "Epoch: 57 | Batch_idx: 120 |  Loss_1: (0.2391) | Acc_1: (91.25%) (14133/15488)\n",
      "Epoch: 57 | Batch_idx: 130 |  Loss_1: (0.2391) | Acc_1: (91.21%) (15294/16768)\n",
      "Epoch: 57 | Batch_idx: 140 |  Loss_1: (0.2419) | Acc_1: (91.12%) (16446/18048)\n",
      "Epoch: 57 | Batch_idx: 150 |  Loss_1: (0.2423) | Acc_1: (91.10%) (17608/19328)\n",
      "Epoch: 57 | Batch_idx: 160 |  Loss_1: (0.2420) | Acc_1: (91.16%) (18787/20608)\n",
      "Epoch: 57 | Batch_idx: 170 |  Loss_1: (0.2420) | Acc_1: (91.16%) (19953/21888)\n",
      "Epoch: 57 | Batch_idx: 180 |  Loss_1: (0.2417) | Acc_1: (91.16%) (21120/23168)\n",
      "Epoch: 57 | Batch_idx: 190 |  Loss_1: (0.2430) | Acc_1: (91.13%) (22280/24448)\n",
      "Epoch: 57 | Batch_idx: 200 |  Loss_1: (0.2422) | Acc_1: (91.16%) (23453/25728)\n",
      "Epoch: 57 | Batch_idx: 210 |  Loss_1: (0.2415) | Acc_1: (91.17%) (24624/27008)\n",
      "Epoch: 57 | Batch_idx: 220 |  Loss_1: (0.2415) | Acc_1: (91.19%) (25795/28288)\n",
      "Epoch: 57 | Batch_idx: 230 |  Loss_1: (0.2423) | Acc_1: (91.15%) (26952/29568)\n",
      "Epoch: 57 | Batch_idx: 240 |  Loss_1: (0.2421) | Acc_1: (91.19%) (28129/30848)\n",
      "Epoch: 57 | Batch_idx: 250 |  Loss_1: (0.2431) | Acc_1: (91.18%) (29293/32128)\n",
      "Epoch: 57 | Batch_idx: 260 |  Loss_1: (0.2431) | Acc_1: (91.15%) (30451/33408)\n",
      "Epoch: 57 | Batch_idx: 270 |  Loss_1: (0.2416) | Acc_1: (91.25%) (31653/34688)\n",
      "Epoch: 57 | Batch_idx: 280 |  Loss_1: (0.2417) | Acc_1: (91.27%) (32829/35968)\n",
      "Epoch: 57 | Batch_idx: 290 |  Loss_1: (0.2418) | Acc_1: (91.27%) (33998/37248)\n",
      "Epoch: 57 | Batch_idx: 300 |  Loss_1: (0.2424) | Acc_1: (91.26%) (35160/38528)\n",
      "Epoch: 57 | Batch_idx: 310 |  Loss_1: (0.2422) | Acc_1: (91.27%) (36331/39808)\n",
      "Epoch: 57 | Batch_idx: 320 |  Loss_1: (0.2422) | Acc_1: (91.25%) (37494/41088)\n",
      "Epoch: 57 | Batch_idx: 330 |  Loss_1: (0.2431) | Acc_1: (91.23%) (38652/42368)\n",
      "Epoch: 57 | Batch_idx: 340 |  Loss_1: (0.2429) | Acc_1: (91.24%) (39826/43648)\n",
      "Epoch: 57 | Batch_idx: 350 |  Loss_1: (0.2430) | Acc_1: (91.23%) (40989/44928)\n",
      "Epoch: 57 | Batch_idx: 360 |  Loss_1: (0.2432) | Acc_1: (91.23%) (42154/46208)\n",
      "Epoch: 57 | Batch_idx: 370 |  Loss_1: (0.2433) | Acc_1: (91.24%) (43327/47488)\n",
      "Epoch: 57 | Batch_idx: 380 |  Loss_1: (0.2436) | Acc_1: (91.23%) (44489/48768)\n",
      "Epoch: 57 | Batch_idx: 390 |  Loss_1: (0.2436) | Acc_1: (91.21%) (45605/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3167) | Acc: (89.87%) (8987/10000)\n",
      "Epoch: 58 | Batch_idx: 0 |  Loss_1: (0.2434) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 58 | Batch_idx: 10 |  Loss_1: (0.2382) | Acc_1: (90.98%) (1281/1408)\n",
      "Epoch: 58 | Batch_idx: 20 |  Loss_1: (0.2251) | Acc_1: (91.78%) (2467/2688)\n",
      "Epoch: 58 | Batch_idx: 30 |  Loss_1: (0.2326) | Acc_1: (91.61%) (3635/3968)\n",
      "Epoch: 58 | Batch_idx: 40 |  Loss_1: (0.2257) | Acc_1: (91.86%) (4821/5248)\n",
      "Epoch: 58 | Batch_idx: 50 |  Loss_1: (0.2281) | Acc_1: (91.88%) (5998/6528)\n",
      "Epoch: 58 | Batch_idx: 60 |  Loss_1: (0.2300) | Acc_1: (91.71%) (7161/7808)\n",
      "Epoch: 58 | Batch_idx: 70 |  Loss_1: (0.2282) | Acc_1: (91.69%) (8333/9088)\n",
      "Epoch: 58 | Batch_idx: 80 |  Loss_1: (0.2318) | Acc_1: (91.54%) (9491/10368)\n",
      "Epoch: 58 | Batch_idx: 90 |  Loss_1: (0.2359) | Acc_1: (91.47%) (10654/11648)\n",
      "Epoch: 58 | Batch_idx: 100 |  Loss_1: (0.2378) | Acc_1: (91.45%) (11823/12928)\n",
      "Epoch: 58 | Batch_idx: 110 |  Loss_1: (0.2359) | Acc_1: (91.56%) (13009/14208)\n",
      "Epoch: 58 | Batch_idx: 120 |  Loss_1: (0.2341) | Acc_1: (91.57%) (14183/15488)\n",
      "Epoch: 58 | Batch_idx: 130 |  Loss_1: (0.2373) | Acc_1: (91.47%) (15338/16768)\n",
      "Epoch: 58 | Batch_idx: 140 |  Loss_1: (0.2384) | Acc_1: (91.43%) (16501/18048)\n",
      "Epoch: 58 | Batch_idx: 150 |  Loss_1: (0.2385) | Acc_1: (91.45%) (17675/19328)\n",
      "Epoch: 58 | Batch_idx: 160 |  Loss_1: (0.2377) | Acc_1: (91.47%) (18850/20608)\n",
      "Epoch: 58 | Batch_idx: 170 |  Loss_1: (0.2395) | Acc_1: (91.38%) (20002/21888)\n",
      "Epoch: 58 | Batch_idx: 180 |  Loss_1: (0.2410) | Acc_1: (91.35%) (21163/23168)\n",
      "Epoch: 58 | Batch_idx: 190 |  Loss_1: (0.2421) | Acc_1: (91.28%) (22316/24448)\n",
      "Epoch: 58 | Batch_idx: 200 |  Loss_1: (0.2421) | Acc_1: (91.27%) (23483/25728)\n",
      "Epoch: 58 | Batch_idx: 210 |  Loss_1: (0.2430) | Acc_1: (91.24%) (24643/27008)\n",
      "Epoch: 58 | Batch_idx: 220 |  Loss_1: (0.2429) | Acc_1: (91.26%) (25816/28288)\n",
      "Epoch: 58 | Batch_idx: 230 |  Loss_1: (0.2432) | Acc_1: (91.26%) (26983/29568)\n",
      "Epoch: 58 | Batch_idx: 240 |  Loss_1: (0.2420) | Acc_1: (91.29%) (28160/30848)\n",
      "Epoch: 58 | Batch_idx: 250 |  Loss_1: (0.2426) | Acc_1: (91.28%) (29327/32128)\n",
      "Epoch: 58 | Batch_idx: 260 |  Loss_1: (0.2424) | Acc_1: (91.28%) (30494/33408)\n",
      "Epoch: 58 | Batch_idx: 270 |  Loss_1: (0.2430) | Acc_1: (91.26%) (31657/34688)\n",
      "Epoch: 58 | Batch_idx: 280 |  Loss_1: (0.2444) | Acc_1: (91.24%) (32818/35968)\n",
      "Epoch: 58 | Batch_idx: 290 |  Loss_1: (0.2445) | Acc_1: (91.22%) (33977/37248)\n",
      "Epoch: 58 | Batch_idx: 300 |  Loss_1: (0.2453) | Acc_1: (91.20%) (35139/38528)\n",
      "Epoch: 58 | Batch_idx: 310 |  Loss_1: (0.2453) | Acc_1: (91.23%) (36315/39808)\n",
      "Epoch: 58 | Batch_idx: 320 |  Loss_1: (0.2443) | Acc_1: (91.26%) (37495/41088)\n",
      "Epoch: 58 | Batch_idx: 330 |  Loss_1: (0.2441) | Acc_1: (91.27%) (38669/42368)\n",
      "Epoch: 58 | Batch_idx: 340 |  Loss_1: (0.2446) | Acc_1: (91.23%) (39821/43648)\n",
      "Epoch: 58 | Batch_idx: 350 |  Loss_1: (0.2445) | Acc_1: (91.23%) (40989/44928)\n",
      "Epoch: 58 | Batch_idx: 360 |  Loss_1: (0.2443) | Acc_1: (91.22%) (42151/46208)\n",
      "Epoch: 58 | Batch_idx: 370 |  Loss_1: (0.2440) | Acc_1: (91.23%) (43325/47488)\n",
      "Epoch: 58 | Batch_idx: 380 |  Loss_1: (0.2447) | Acc_1: (91.23%) (44489/48768)\n",
      "Epoch: 58 | Batch_idx: 390 |  Loss_1: (0.2454) | Acc_1: (91.23%) (45617/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3092) | Acc: (90.39%) (9039/10000)\n",
      "Epoch: 59 | Batch_idx: 0 |  Loss_1: (0.3256) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 59 | Batch_idx: 10 |  Loss_1: (0.2037) | Acc_1: (92.26%) (1299/1408)\n",
      "Epoch: 59 | Batch_idx: 20 |  Loss_1: (0.2137) | Acc_1: (92.00%) (2473/2688)\n",
      "Epoch: 59 | Batch_idx: 30 |  Loss_1: (0.2130) | Acc_1: (92.24%) (3660/3968)\n",
      "Epoch: 59 | Batch_idx: 40 |  Loss_1: (0.2235) | Acc_1: (92.02%) (4829/5248)\n",
      "Epoch: 59 | Batch_idx: 50 |  Loss_1: (0.2238) | Acc_1: (91.94%) (6002/6528)\n",
      "Epoch: 59 | Batch_idx: 60 |  Loss_1: (0.2222) | Acc_1: (91.91%) (7176/7808)\n",
      "Epoch: 59 | Batch_idx: 70 |  Loss_1: (0.2257) | Acc_1: (91.77%) (8340/9088)\n",
      "Epoch: 59 | Batch_idx: 80 |  Loss_1: (0.2278) | Acc_1: (91.81%) (9519/10368)\n",
      "Epoch: 59 | Batch_idx: 90 |  Loss_1: (0.2273) | Acc_1: (91.79%) (10692/11648)\n",
      "Epoch: 59 | Batch_idx: 100 |  Loss_1: (0.2287) | Acc_1: (91.81%) (11869/12928)\n",
      "Epoch: 59 | Batch_idx: 110 |  Loss_1: (0.2287) | Acc_1: (91.79%) (13042/14208)\n",
      "Epoch: 59 | Batch_idx: 120 |  Loss_1: (0.2308) | Acc_1: (91.70%) (14203/15488)\n",
      "Epoch: 59 | Batch_idx: 130 |  Loss_1: (0.2296) | Acc_1: (91.72%) (15380/16768)\n",
      "Epoch: 59 | Batch_idx: 140 |  Loss_1: (0.2295) | Acc_1: (91.71%) (16552/18048)\n",
      "Epoch: 59 | Batch_idx: 150 |  Loss_1: (0.2303) | Acc_1: (91.68%) (17719/19328)\n",
      "Epoch: 59 | Batch_idx: 160 |  Loss_1: (0.2292) | Acc_1: (91.75%) (18907/20608)\n",
      "Epoch: 59 | Batch_idx: 170 |  Loss_1: (0.2298) | Acc_1: (91.76%) (20084/21888)\n",
      "Epoch: 59 | Batch_idx: 180 |  Loss_1: (0.2293) | Acc_1: (91.80%) (21268/23168)\n",
      "Epoch: 59 | Batch_idx: 190 |  Loss_1: (0.2301) | Acc_1: (91.80%) (22443/24448)\n",
      "Epoch: 59 | Batch_idx: 200 |  Loss_1: (0.2303) | Acc_1: (91.78%) (23612/25728)\n",
      "Epoch: 59 | Batch_idx: 210 |  Loss_1: (0.2302) | Acc_1: (91.78%) (24789/27008)\n",
      "Epoch: 59 | Batch_idx: 220 |  Loss_1: (0.2285) | Acc_1: (91.85%) (25982/28288)\n",
      "Epoch: 59 | Batch_idx: 230 |  Loss_1: (0.2292) | Acc_1: (91.84%) (27156/29568)\n",
      "Epoch: 59 | Batch_idx: 240 |  Loss_1: (0.2299) | Acc_1: (91.81%) (28321/30848)\n",
      "Epoch: 59 | Batch_idx: 250 |  Loss_1: (0.2304) | Acc_1: (91.80%) (29492/32128)\n",
      "Epoch: 59 | Batch_idx: 260 |  Loss_1: (0.2320) | Acc_1: (91.74%) (30649/33408)\n",
      "Epoch: 59 | Batch_idx: 270 |  Loss_1: (0.2327) | Acc_1: (91.71%) (31812/34688)\n",
      "Epoch: 59 | Batch_idx: 280 |  Loss_1: (0.2331) | Acc_1: (91.69%) (32978/35968)\n",
      "Epoch: 59 | Batch_idx: 290 |  Loss_1: (0.2315) | Acc_1: (91.75%) (34174/37248)\n",
      "Epoch: 59 | Batch_idx: 300 |  Loss_1: (0.2312) | Acc_1: (91.75%) (35348/38528)\n",
      "Epoch: 59 | Batch_idx: 310 |  Loss_1: (0.2330) | Acc_1: (91.68%) (36494/39808)\n",
      "Epoch: 59 | Batch_idx: 320 |  Loss_1: (0.2334) | Acc_1: (91.66%) (37662/41088)\n",
      "Epoch: 59 | Batch_idx: 330 |  Loss_1: (0.2326) | Acc_1: (91.69%) (38847/42368)\n",
      "Epoch: 59 | Batch_idx: 340 |  Loss_1: (0.2325) | Acc_1: (91.69%) (40023/43648)\n",
      "Epoch: 59 | Batch_idx: 350 |  Loss_1: (0.2333) | Acc_1: (91.65%) (41175/44928)\n",
      "Epoch: 59 | Batch_idx: 360 |  Loss_1: (0.2336) | Acc_1: (91.64%) (42343/46208)\n",
      "Epoch: 59 | Batch_idx: 370 |  Loss_1: (0.2340) | Acc_1: (91.64%) (43519/47488)\n",
      "Epoch: 59 | Batch_idx: 380 |  Loss_1: (0.2347) | Acc_1: (91.62%) (44681/48768)\n",
      "Epoch: 59 | Batch_idx: 390 |  Loss_1: (0.2352) | Acc_1: (91.61%) (45804/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3227) | Acc: (89.58%) (8958/10000)\n",
      "Epoch: 60 | Batch_idx: 0 |  Loss_1: (0.2617) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 60 | Batch_idx: 10 |  Loss_1: (0.2439) | Acc_1: (91.05%) (1282/1408)\n",
      "Epoch: 60 | Batch_idx: 20 |  Loss_1: (0.2492) | Acc_1: (91.07%) (2448/2688)\n",
      "Epoch: 60 | Batch_idx: 30 |  Loss_1: (0.2466) | Acc_1: (91.18%) (3618/3968)\n",
      "Epoch: 60 | Batch_idx: 40 |  Loss_1: (0.2430) | Acc_1: (91.33%) (4793/5248)\n",
      "Epoch: 60 | Batch_idx: 50 |  Loss_1: (0.2421) | Acc_1: (91.34%) (5963/6528)\n",
      "Epoch: 60 | Batch_idx: 60 |  Loss_1: (0.2321) | Acc_1: (91.62%) (7154/7808)\n",
      "Epoch: 60 | Batch_idx: 70 |  Loss_1: (0.2304) | Acc_1: (91.68%) (8332/9088)\n",
      "Epoch: 60 | Batch_idx: 80 |  Loss_1: (0.2331) | Acc_1: (91.61%) (9498/10368)\n",
      "Epoch: 60 | Batch_idx: 90 |  Loss_1: (0.2314) | Acc_1: (91.70%) (10681/11648)\n",
      "Epoch: 60 | Batch_idx: 100 |  Loss_1: (0.2343) | Acc_1: (91.60%) (11842/12928)\n",
      "Epoch: 60 | Batch_idx: 110 |  Loss_1: (0.2336) | Acc_1: (91.64%) (13020/14208)\n",
      "Epoch: 60 | Batch_idx: 120 |  Loss_1: (0.2309) | Acc_1: (91.70%) (14202/15488)\n",
      "Epoch: 60 | Batch_idx: 130 |  Loss_1: (0.2325) | Acc_1: (91.60%) (15360/16768)\n",
      "Epoch: 60 | Batch_idx: 140 |  Loss_1: (0.2331) | Acc_1: (91.54%) (16522/18048)\n",
      "Epoch: 60 | Batch_idx: 150 |  Loss_1: (0.2345) | Acc_1: (91.49%) (17684/19328)\n",
      "Epoch: 60 | Batch_idx: 160 |  Loss_1: (0.2331) | Acc_1: (91.53%) (18862/20608)\n",
      "Epoch: 60 | Batch_idx: 170 |  Loss_1: (0.2336) | Acc_1: (91.48%) (20024/21888)\n",
      "Epoch: 60 | Batch_idx: 180 |  Loss_1: (0.2327) | Acc_1: (91.53%) (21206/23168)\n",
      "Epoch: 60 | Batch_idx: 190 |  Loss_1: (0.2324) | Acc_1: (91.52%) (22375/24448)\n",
      "Epoch: 60 | Batch_idx: 200 |  Loss_1: (0.2324) | Acc_1: (91.53%) (23548/25728)\n",
      "Epoch: 60 | Batch_idx: 210 |  Loss_1: (0.2311) | Acc_1: (91.57%) (24730/27008)\n",
      "Epoch: 60 | Batch_idx: 220 |  Loss_1: (0.2323) | Acc_1: (91.53%) (25891/28288)\n",
      "Epoch: 60 | Batch_idx: 230 |  Loss_1: (0.2320) | Acc_1: (91.53%) (27064/29568)\n",
      "Epoch: 60 | Batch_idx: 240 |  Loss_1: (0.2308) | Acc_1: (91.57%) (28248/30848)\n",
      "Epoch: 60 | Batch_idx: 250 |  Loss_1: (0.2328) | Acc_1: (91.54%) (29410/32128)\n",
      "Epoch: 60 | Batch_idx: 260 |  Loss_1: (0.2330) | Acc_1: (91.56%) (30589/33408)\n",
      "Epoch: 60 | Batch_idx: 270 |  Loss_1: (0.2323) | Acc_1: (91.57%) (31764/34688)\n",
      "Epoch: 60 | Batch_idx: 280 |  Loss_1: (0.2323) | Acc_1: (91.57%) (32936/35968)\n",
      "Epoch: 60 | Batch_idx: 290 |  Loss_1: (0.2327) | Acc_1: (91.58%) (34112/37248)\n",
      "Epoch: 60 | Batch_idx: 300 |  Loss_1: (0.2333) | Acc_1: (91.55%) (35273/38528)\n",
      "Epoch: 60 | Batch_idx: 310 |  Loss_1: (0.2352) | Acc_1: (91.49%) (36420/39808)\n",
      "Epoch: 60 | Batch_idx: 320 |  Loss_1: (0.2369) | Acc_1: (91.43%) (37566/41088)\n",
      "Epoch: 60 | Batch_idx: 330 |  Loss_1: (0.2378) | Acc_1: (91.39%) (38722/42368)\n",
      "Epoch: 60 | Batch_idx: 340 |  Loss_1: (0.2379) | Acc_1: (91.40%) (39894/43648)\n",
      "Epoch: 60 | Batch_idx: 350 |  Loss_1: (0.2375) | Acc_1: (91.41%) (41067/44928)\n",
      "Epoch: 60 | Batch_idx: 360 |  Loss_1: (0.2369) | Acc_1: (91.45%) (42255/46208)\n",
      "Epoch: 60 | Batch_idx: 370 |  Loss_1: (0.2363) | Acc_1: (91.48%) (43441/47488)\n",
      "Epoch: 60 | Batch_idx: 380 |  Loss_1: (0.2370) | Acc_1: (91.48%) (44614/48768)\n",
      "Epoch: 60 | Batch_idx: 390 |  Loss_1: (0.2366) | Acc_1: (91.50%) (45750/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3065) | Acc: (90.49%) (9049/10000)\n",
      "Epoch: 61 | Batch_idx: 0 |  Loss_1: (0.1805) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 61 | Batch_idx: 10 |  Loss_1: (0.2257) | Acc_1: (91.48%) (1288/1408)\n",
      "Epoch: 61 | Batch_idx: 20 |  Loss_1: (0.2269) | Acc_1: (91.74%) (2466/2688)\n",
      "Epoch: 61 | Batch_idx: 30 |  Loss_1: (0.2317) | Acc_1: (91.63%) (3636/3968)\n",
      "Epoch: 61 | Batch_idx: 40 |  Loss_1: (0.2351) | Acc_1: (91.62%) (4808/5248)\n",
      "Epoch: 61 | Batch_idx: 50 |  Loss_1: (0.2340) | Acc_1: (91.56%) (5977/6528)\n",
      "Epoch: 61 | Batch_idx: 60 |  Loss_1: (0.2342) | Acc_1: (91.60%) (7152/7808)\n",
      "Epoch: 61 | Batch_idx: 70 |  Loss_1: (0.2346) | Acc_1: (91.57%) (8322/9088)\n",
      "Epoch: 61 | Batch_idx: 80 |  Loss_1: (0.2355) | Acc_1: (91.50%) (9487/10368)\n",
      "Epoch: 61 | Batch_idx: 90 |  Loss_1: (0.2358) | Acc_1: (91.54%) (10662/11648)\n",
      "Epoch: 61 | Batch_idx: 100 |  Loss_1: (0.2355) | Acc_1: (91.55%) (11836/12928)\n",
      "Epoch: 61 | Batch_idx: 110 |  Loss_1: (0.2361) | Acc_1: (91.48%) (12998/14208)\n",
      "Epoch: 61 | Batch_idx: 120 |  Loss_1: (0.2375) | Acc_1: (91.42%) (14159/15488)\n",
      "Epoch: 61 | Batch_idx: 130 |  Loss_1: (0.2348) | Acc_1: (91.51%) (15345/16768)\n",
      "Epoch: 61 | Batch_idx: 140 |  Loss_1: (0.2345) | Acc_1: (91.51%) (16515/18048)\n",
      "Epoch: 61 | Batch_idx: 150 |  Loss_1: (0.2332) | Acc_1: (91.58%) (17700/19328)\n",
      "Epoch: 61 | Batch_idx: 160 |  Loss_1: (0.2324) | Acc_1: (91.63%) (18883/20608)\n",
      "Epoch: 61 | Batch_idx: 170 |  Loss_1: (0.2318) | Acc_1: (91.62%) (20054/21888)\n",
      "Epoch: 61 | Batch_idx: 180 |  Loss_1: (0.2307) | Acc_1: (91.68%) (21240/23168)\n",
      "Epoch: 61 | Batch_idx: 190 |  Loss_1: (0.2310) | Acc_1: (91.67%) (22412/24448)\n",
      "Epoch: 61 | Batch_idx: 200 |  Loss_1: (0.2305) | Acc_1: (91.70%) (23593/25728)\n",
      "Epoch: 61 | Batch_idx: 210 |  Loss_1: (0.2321) | Acc_1: (91.67%) (24757/27008)\n",
      "Epoch: 61 | Batch_idx: 220 |  Loss_1: (0.2324) | Acc_1: (91.67%) (25933/28288)\n",
      "Epoch: 61 | Batch_idx: 230 |  Loss_1: (0.2331) | Acc_1: (91.64%) (27097/29568)\n",
      "Epoch: 61 | Batch_idx: 240 |  Loss_1: (0.2328) | Acc_1: (91.65%) (28272/30848)\n",
      "Epoch: 61 | Batch_idx: 250 |  Loss_1: (0.2314) | Acc_1: (91.69%) (29459/32128)\n",
      "Epoch: 61 | Batch_idx: 260 |  Loss_1: (0.2304) | Acc_1: (91.73%) (30644/33408)\n",
      "Epoch: 61 | Batch_idx: 270 |  Loss_1: (0.2308) | Acc_1: (91.71%) (31812/34688)\n",
      "Epoch: 61 | Batch_idx: 280 |  Loss_1: (0.2288) | Acc_1: (91.81%) (33022/35968)\n",
      "Epoch: 61 | Batch_idx: 290 |  Loss_1: (0.2300) | Acc_1: (91.79%) (34191/37248)\n",
      "Epoch: 61 | Batch_idx: 300 |  Loss_1: (0.2307) | Acc_1: (91.77%) (35359/38528)\n",
      "Epoch: 61 | Batch_idx: 310 |  Loss_1: (0.2316) | Acc_1: (91.76%) (36529/39808)\n",
      "Epoch: 61 | Batch_idx: 320 |  Loss_1: (0.2313) | Acc_1: (91.79%) (37716/41088)\n",
      "Epoch: 61 | Batch_idx: 330 |  Loss_1: (0.2309) | Acc_1: (91.78%) (38887/42368)\n",
      "Epoch: 61 | Batch_idx: 340 |  Loss_1: (0.2313) | Acc_1: (91.76%) (40051/43648)\n",
      "Epoch: 61 | Batch_idx: 350 |  Loss_1: (0.2330) | Acc_1: (91.71%) (41203/44928)\n",
      "Epoch: 61 | Batch_idx: 360 |  Loss_1: (0.2331) | Acc_1: (91.71%) (42378/46208)\n",
      "Epoch: 61 | Batch_idx: 370 |  Loss_1: (0.2333) | Acc_1: (91.70%) (43545/47488)\n",
      "Epoch: 61 | Batch_idx: 380 |  Loss_1: (0.2333) | Acc_1: (91.70%) (44719/48768)\n",
      "Epoch: 61 | Batch_idx: 390 |  Loss_1: (0.2335) | Acc_1: (91.69%) (45843/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2883) | Acc: (90.69%) (9069/10000)\n",
      "Epoch: 62 | Batch_idx: 0 |  Loss_1: (0.2723) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 62 | Batch_idx: 10 |  Loss_1: (0.2233) | Acc_1: (91.69%) (1291/1408)\n",
      "Epoch: 62 | Batch_idx: 20 |  Loss_1: (0.2350) | Acc_1: (91.74%) (2466/2688)\n",
      "Epoch: 62 | Batch_idx: 30 |  Loss_1: (0.2430) | Acc_1: (91.33%) (3624/3968)\n",
      "Epoch: 62 | Batch_idx: 40 |  Loss_1: (0.2386) | Acc_1: (91.43%) (4798/5248)\n",
      "Epoch: 62 | Batch_idx: 50 |  Loss_1: (0.2352) | Acc_1: (91.65%) (5983/6528)\n",
      "Epoch: 62 | Batch_idx: 60 |  Loss_1: (0.2348) | Acc_1: (91.65%) (7156/7808)\n",
      "Epoch: 62 | Batch_idx: 70 |  Loss_1: (0.2314) | Acc_1: (91.69%) (8333/9088)\n",
      "Epoch: 62 | Batch_idx: 80 |  Loss_1: (0.2287) | Acc_1: (91.80%) (9518/10368)\n",
      "Epoch: 62 | Batch_idx: 90 |  Loss_1: (0.2258) | Acc_1: (91.91%) (10706/11648)\n",
      "Epoch: 62 | Batch_idx: 100 |  Loss_1: (0.2250) | Acc_1: (91.98%) (11891/12928)\n",
      "Epoch: 62 | Batch_idx: 110 |  Loss_1: (0.2237) | Acc_1: (91.95%) (13064/14208)\n",
      "Epoch: 62 | Batch_idx: 120 |  Loss_1: (0.2226) | Acc_1: (91.98%) (14246/15488)\n",
      "Epoch: 62 | Batch_idx: 130 |  Loss_1: (0.2233) | Acc_1: (91.99%) (15425/16768)\n",
      "Epoch: 62 | Batch_idx: 140 |  Loss_1: (0.2257) | Acc_1: (91.89%) (16585/18048)\n",
      "Epoch: 62 | Batch_idx: 150 |  Loss_1: (0.2259) | Acc_1: (91.91%) (17765/19328)\n",
      "Epoch: 62 | Batch_idx: 160 |  Loss_1: (0.2269) | Acc_1: (91.89%) (18936/20608)\n",
      "Epoch: 62 | Batch_idx: 170 |  Loss_1: (0.2259) | Acc_1: (91.91%) (20117/21888)\n",
      "Epoch: 62 | Batch_idx: 180 |  Loss_1: (0.2249) | Acc_1: (91.94%) (21301/23168)\n",
      "Epoch: 62 | Batch_idx: 190 |  Loss_1: (0.2248) | Acc_1: (91.94%) (22477/24448)\n",
      "Epoch: 62 | Batch_idx: 200 |  Loss_1: (0.2260) | Acc_1: (91.86%) (23633/25728)\n",
      "Epoch: 62 | Batch_idx: 210 |  Loss_1: (0.2273) | Acc_1: (91.84%) (24805/27008)\n",
      "Epoch: 62 | Batch_idx: 220 |  Loss_1: (0.2270) | Acc_1: (91.82%) (25975/28288)\n",
      "Epoch: 62 | Batch_idx: 230 |  Loss_1: (0.2271) | Acc_1: (91.84%) (27154/29568)\n",
      "Epoch: 62 | Batch_idx: 240 |  Loss_1: (0.2268) | Acc_1: (91.84%) (28331/30848)\n",
      "Epoch: 62 | Batch_idx: 250 |  Loss_1: (0.2263) | Acc_1: (91.88%) (29518/32128)\n",
      "Epoch: 62 | Batch_idx: 260 |  Loss_1: (0.2265) | Acc_1: (91.83%) (30678/33408)\n",
      "Epoch: 62 | Batch_idx: 270 |  Loss_1: (0.2267) | Acc_1: (91.83%) (31853/34688)\n",
      "Epoch: 62 | Batch_idx: 280 |  Loss_1: (0.2274) | Acc_1: (91.84%) (33032/35968)\n",
      "Epoch: 62 | Batch_idx: 290 |  Loss_1: (0.2272) | Acc_1: (91.83%) (34206/37248)\n",
      "Epoch: 62 | Batch_idx: 300 |  Loss_1: (0.2264) | Acc_1: (91.87%) (35394/38528)\n",
      "Epoch: 62 | Batch_idx: 310 |  Loss_1: (0.2253) | Acc_1: (91.91%) (36588/39808)\n",
      "Epoch: 62 | Batch_idx: 320 |  Loss_1: (0.2248) | Acc_1: (91.91%) (37765/41088)\n",
      "Epoch: 62 | Batch_idx: 330 |  Loss_1: (0.2245) | Acc_1: (91.92%) (38943/42368)\n",
      "Epoch: 62 | Batch_idx: 340 |  Loss_1: (0.2250) | Acc_1: (91.92%) (40120/43648)\n",
      "Epoch: 62 | Batch_idx: 350 |  Loss_1: (0.2256) | Acc_1: (91.89%) (41286/44928)\n",
      "Epoch: 62 | Batch_idx: 360 |  Loss_1: (0.2246) | Acc_1: (91.92%) (42474/46208)\n",
      "Epoch: 62 | Batch_idx: 370 |  Loss_1: (0.2259) | Acc_1: (91.90%) (43641/47488)\n",
      "Epoch: 62 | Batch_idx: 380 |  Loss_1: (0.2258) | Acc_1: (91.92%) (44827/48768)\n",
      "Epoch: 62 | Batch_idx: 390 |  Loss_1: (0.2262) | Acc_1: (91.92%) (45958/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3170) | Acc: (90.13%) (9013/10000)\n",
      "Epoch: 63 | Batch_idx: 0 |  Loss_1: (0.2175) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 63 | Batch_idx: 10 |  Loss_1: (0.2169) | Acc_1: (91.83%) (1293/1408)\n",
      "Epoch: 63 | Batch_idx: 20 |  Loss_1: (0.2148) | Acc_1: (92.11%) (2476/2688)\n",
      "Epoch: 63 | Batch_idx: 30 |  Loss_1: (0.2271) | Acc_1: (91.78%) (3642/3968)\n",
      "Epoch: 63 | Batch_idx: 40 |  Loss_1: (0.2267) | Acc_1: (91.71%) (4813/5248)\n",
      "Epoch: 63 | Batch_idx: 50 |  Loss_1: (0.2220) | Acc_1: (91.87%) (5997/6528)\n",
      "Epoch: 63 | Batch_idx: 60 |  Loss_1: (0.2204) | Acc_1: (91.94%) (7179/7808)\n",
      "Epoch: 63 | Batch_idx: 70 |  Loss_1: (0.2169) | Acc_1: (92.12%) (8372/9088)\n",
      "Epoch: 63 | Batch_idx: 80 |  Loss_1: (0.2206) | Acc_1: (92.04%) (9543/10368)\n",
      "Epoch: 63 | Batch_idx: 90 |  Loss_1: (0.2248) | Acc_1: (91.92%) (10707/11648)\n",
      "Epoch: 63 | Batch_idx: 100 |  Loss_1: (0.2249) | Acc_1: (91.89%) (11880/12928)\n",
      "Epoch: 63 | Batch_idx: 110 |  Loss_1: (0.2227) | Acc_1: (91.95%) (13064/14208)\n",
      "Epoch: 63 | Batch_idx: 120 |  Loss_1: (0.2228) | Acc_1: (91.99%) (14247/15488)\n",
      "Epoch: 63 | Batch_idx: 130 |  Loss_1: (0.2236) | Acc_1: (91.95%) (15418/16768)\n",
      "Epoch: 63 | Batch_idx: 140 |  Loss_1: (0.2236) | Acc_1: (91.97%) (16598/18048)\n",
      "Epoch: 63 | Batch_idx: 150 |  Loss_1: (0.2241) | Acc_1: (91.94%) (17770/19328)\n",
      "Epoch: 63 | Batch_idx: 160 |  Loss_1: (0.2236) | Acc_1: (91.96%) (18951/20608)\n",
      "Epoch: 63 | Batch_idx: 170 |  Loss_1: (0.2244) | Acc_1: (91.96%) (20128/21888)\n",
      "Epoch: 63 | Batch_idx: 180 |  Loss_1: (0.2240) | Acc_1: (91.97%) (21307/23168)\n",
      "Epoch: 63 | Batch_idx: 190 |  Loss_1: (0.2252) | Acc_1: (91.93%) (22474/24448)\n",
      "Epoch: 63 | Batch_idx: 200 |  Loss_1: (0.2266) | Acc_1: (91.92%) (23648/25728)\n",
      "Epoch: 63 | Batch_idx: 210 |  Loss_1: (0.2271) | Acc_1: (91.91%) (24824/27008)\n",
      "Epoch: 63 | Batch_idx: 220 |  Loss_1: (0.2277) | Acc_1: (91.89%) (25993/28288)\n",
      "Epoch: 63 | Batch_idx: 230 |  Loss_1: (0.2278) | Acc_1: (91.84%) (27154/29568)\n",
      "Epoch: 63 | Batch_idx: 240 |  Loss_1: (0.2293) | Acc_1: (91.79%) (28315/30848)\n",
      "Epoch: 63 | Batch_idx: 250 |  Loss_1: (0.2289) | Acc_1: (91.78%) (29488/32128)\n",
      "Epoch: 63 | Batch_idx: 260 |  Loss_1: (0.2277) | Acc_1: (91.85%) (30684/33408)\n",
      "Epoch: 63 | Batch_idx: 270 |  Loss_1: (0.2270) | Acc_1: (91.88%) (31870/34688)\n",
      "Epoch: 63 | Batch_idx: 280 |  Loss_1: (0.2269) | Acc_1: (91.89%) (33050/35968)\n",
      "Epoch: 63 | Batch_idx: 290 |  Loss_1: (0.2277) | Acc_1: (91.86%) (34216/37248)\n",
      "Epoch: 63 | Batch_idx: 300 |  Loss_1: (0.2279) | Acc_1: (91.87%) (35395/38528)\n",
      "Epoch: 63 | Batch_idx: 310 |  Loss_1: (0.2272) | Acc_1: (91.91%) (36587/39808)\n",
      "Epoch: 63 | Batch_idx: 320 |  Loss_1: (0.2261) | Acc_1: (91.96%) (37784/41088)\n",
      "Epoch: 63 | Batch_idx: 330 |  Loss_1: (0.2273) | Acc_1: (91.93%) (38949/42368)\n",
      "Epoch: 63 | Batch_idx: 340 |  Loss_1: (0.2284) | Acc_1: (91.90%) (40113/43648)\n",
      "Epoch: 63 | Batch_idx: 350 |  Loss_1: (0.2279) | Acc_1: (91.93%) (41303/44928)\n",
      "Epoch: 63 | Batch_idx: 360 |  Loss_1: (0.2274) | Acc_1: (91.94%) (42482/46208)\n",
      "Epoch: 63 | Batch_idx: 370 |  Loss_1: (0.2274) | Acc_1: (91.93%) (43656/47488)\n",
      "Epoch: 63 | Batch_idx: 380 |  Loss_1: (0.2277) | Acc_1: (91.92%) (44827/48768)\n",
      "Epoch: 63 | Batch_idx: 390 |  Loss_1: (0.2279) | Acc_1: (91.91%) (45954/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3004) | Acc: (91.17%) (9117/10000)\n",
      "Epoch: 64 | Batch_idx: 0 |  Loss_1: (0.1948) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 64 | Batch_idx: 10 |  Loss_1: (0.1928) | Acc_1: (92.76%) (1306/1408)\n",
      "Epoch: 64 | Batch_idx: 20 |  Loss_1: (0.2008) | Acc_1: (92.82%) (2495/2688)\n",
      "Epoch: 64 | Batch_idx: 30 |  Loss_1: (0.2064) | Acc_1: (92.67%) (3677/3968)\n",
      "Epoch: 64 | Batch_idx: 40 |  Loss_1: (0.2090) | Acc_1: (92.51%) (4855/5248)\n",
      "Epoch: 64 | Batch_idx: 50 |  Loss_1: (0.2067) | Acc_1: (92.56%) (6042/6528)\n",
      "Epoch: 64 | Batch_idx: 60 |  Loss_1: (0.2099) | Acc_1: (92.33%) (7209/7808)\n",
      "Epoch: 64 | Batch_idx: 70 |  Loss_1: (0.2106) | Acc_1: (92.24%) (8383/9088)\n",
      "Epoch: 64 | Batch_idx: 80 |  Loss_1: (0.2089) | Acc_1: (92.32%) (9572/10368)\n",
      "Epoch: 64 | Batch_idx: 90 |  Loss_1: (0.2100) | Acc_1: (92.37%) (10759/11648)\n",
      "Epoch: 64 | Batch_idx: 100 |  Loss_1: (0.2097) | Acc_1: (92.39%) (11944/12928)\n",
      "Epoch: 64 | Batch_idx: 110 |  Loss_1: (0.2122) | Acc_1: (92.29%) (13112/14208)\n",
      "Epoch: 64 | Batch_idx: 120 |  Loss_1: (0.2119) | Acc_1: (92.28%) (14292/15488)\n",
      "Epoch: 64 | Batch_idx: 130 |  Loss_1: (0.2127) | Acc_1: (92.24%) (15467/16768)\n",
      "Epoch: 64 | Batch_idx: 140 |  Loss_1: (0.2121) | Acc_1: (92.29%) (16656/18048)\n",
      "Epoch: 64 | Batch_idx: 150 |  Loss_1: (0.2122) | Acc_1: (92.27%) (17834/19328)\n",
      "Epoch: 64 | Batch_idx: 160 |  Loss_1: (0.2140) | Acc_1: (92.22%) (19004/20608)\n",
      "Epoch: 64 | Batch_idx: 170 |  Loss_1: (0.2118) | Acc_1: (92.29%) (20201/21888)\n",
      "Epoch: 64 | Batch_idx: 180 |  Loss_1: (0.2134) | Acc_1: (92.25%) (21372/23168)\n",
      "Epoch: 64 | Batch_idx: 190 |  Loss_1: (0.2136) | Acc_1: (92.22%) (22545/24448)\n",
      "Epoch: 64 | Batch_idx: 200 |  Loss_1: (0.2135) | Acc_1: (92.23%) (23728/25728)\n",
      "Epoch: 64 | Batch_idx: 210 |  Loss_1: (0.2139) | Acc_1: (92.24%) (24913/27008)\n",
      "Epoch: 64 | Batch_idx: 220 |  Loss_1: (0.2139) | Acc_1: (92.25%) (26096/28288)\n",
      "Epoch: 64 | Batch_idx: 230 |  Loss_1: (0.2146) | Acc_1: (92.21%) (27264/29568)\n",
      "Epoch: 64 | Batch_idx: 240 |  Loss_1: (0.2158) | Acc_1: (92.16%) (28428/30848)\n",
      "Epoch: 64 | Batch_idx: 250 |  Loss_1: (0.2162) | Acc_1: (92.13%) (29601/32128)\n",
      "Epoch: 64 | Batch_idx: 260 |  Loss_1: (0.2171) | Acc_1: (92.12%) (30777/33408)\n",
      "Epoch: 64 | Batch_idx: 270 |  Loss_1: (0.2182) | Acc_1: (92.06%) (31935/34688)\n",
      "Epoch: 64 | Batch_idx: 280 |  Loss_1: (0.2185) | Acc_1: (92.08%) (33121/35968)\n",
      "Epoch: 64 | Batch_idx: 290 |  Loss_1: (0.2189) | Acc_1: (92.06%) (34290/37248)\n",
      "Epoch: 64 | Batch_idx: 300 |  Loss_1: (0.2195) | Acc_1: (92.05%) (35464/38528)\n",
      "Epoch: 64 | Batch_idx: 310 |  Loss_1: (0.2193) | Acc_1: (92.06%) (36647/39808)\n",
      "Epoch: 64 | Batch_idx: 320 |  Loss_1: (0.2204) | Acc_1: (92.04%) (37817/41088)\n",
      "Epoch: 64 | Batch_idx: 330 |  Loss_1: (0.2202) | Acc_1: (92.06%) (39004/42368)\n",
      "Epoch: 64 | Batch_idx: 340 |  Loss_1: (0.2200) | Acc_1: (92.09%) (40194/43648)\n",
      "Epoch: 64 | Batch_idx: 350 |  Loss_1: (0.2202) | Acc_1: (92.09%) (41372/44928)\n",
      "Epoch: 64 | Batch_idx: 360 |  Loss_1: (0.2209) | Acc_1: (92.05%) (42536/46208)\n",
      "Epoch: 64 | Batch_idx: 370 |  Loss_1: (0.2211) | Acc_1: (92.04%) (43708/47488)\n",
      "Epoch: 64 | Batch_idx: 380 |  Loss_1: (0.2220) | Acc_1: (92.03%) (44881/48768)\n",
      "Epoch: 64 | Batch_idx: 390 |  Loss_1: (0.2217) | Acc_1: (92.03%) (46015/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3069) | Acc: (91.05%) (9105/10000)\n",
      "Epoch: 65 | Batch_idx: 0 |  Loss_1: (0.1802) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 65 | Batch_idx: 10 |  Loss_1: (0.1983) | Acc_1: (93.04%) (1310/1408)\n",
      "Epoch: 65 | Batch_idx: 20 |  Loss_1: (0.2185) | Acc_1: (92.04%) (2474/2688)\n",
      "Epoch: 65 | Batch_idx: 30 |  Loss_1: (0.2191) | Acc_1: (91.91%) (3647/3968)\n",
      "Epoch: 65 | Batch_idx: 40 |  Loss_1: (0.2238) | Acc_1: (91.75%) (4815/5248)\n",
      "Epoch: 65 | Batch_idx: 50 |  Loss_1: (0.2285) | Acc_1: (91.71%) (5987/6528)\n",
      "Epoch: 65 | Batch_idx: 60 |  Loss_1: (0.2270) | Acc_1: (91.70%) (7160/7808)\n",
      "Epoch: 65 | Batch_idx: 70 |  Loss_1: (0.2265) | Acc_1: (91.74%) (8337/9088)\n",
      "Epoch: 65 | Batch_idx: 80 |  Loss_1: (0.2250) | Acc_1: (91.72%) (9510/10368)\n",
      "Epoch: 65 | Batch_idx: 90 |  Loss_1: (0.2247) | Acc_1: (91.66%) (10677/11648)\n",
      "Epoch: 65 | Batch_idx: 100 |  Loss_1: (0.2287) | Acc_1: (91.51%) (11830/12928)\n",
      "Epoch: 65 | Batch_idx: 110 |  Loss_1: (0.2259) | Acc_1: (91.60%) (13015/14208)\n",
      "Epoch: 65 | Batch_idx: 120 |  Loss_1: (0.2257) | Acc_1: (91.68%) (14199/15488)\n",
      "Epoch: 65 | Batch_idx: 130 |  Loss_1: (0.2255) | Acc_1: (91.68%) (15373/16768)\n",
      "Epoch: 65 | Batch_idx: 140 |  Loss_1: (0.2265) | Acc_1: (91.67%) (16544/18048)\n",
      "Epoch: 65 | Batch_idx: 150 |  Loss_1: (0.2263) | Acc_1: (91.70%) (17723/19328)\n",
      "Epoch: 65 | Batch_idx: 160 |  Loss_1: (0.2255) | Acc_1: (91.72%) (18901/20608)\n",
      "Epoch: 65 | Batch_idx: 170 |  Loss_1: (0.2229) | Acc_1: (91.79%) (20092/21888)\n",
      "Epoch: 65 | Batch_idx: 180 |  Loss_1: (0.2234) | Acc_1: (91.78%) (21264/23168)\n",
      "Epoch: 65 | Batch_idx: 190 |  Loss_1: (0.2242) | Acc_1: (91.76%) (22433/24448)\n",
      "Epoch: 65 | Batch_idx: 200 |  Loss_1: (0.2231) | Acc_1: (91.79%) (23616/25728)\n",
      "Epoch: 65 | Batch_idx: 210 |  Loss_1: (0.2232) | Acc_1: (91.80%) (24793/27008)\n",
      "Epoch: 65 | Batch_idx: 220 |  Loss_1: (0.2234) | Acc_1: (91.78%) (25964/28288)\n",
      "Epoch: 65 | Batch_idx: 230 |  Loss_1: (0.2233) | Acc_1: (91.78%) (27138/29568)\n",
      "Epoch: 65 | Batch_idx: 240 |  Loss_1: (0.2218) | Acc_1: (91.84%) (28332/30848)\n",
      "Epoch: 65 | Batch_idx: 250 |  Loss_1: (0.2214) | Acc_1: (91.85%) (29509/32128)\n",
      "Epoch: 65 | Batch_idx: 260 |  Loss_1: (0.2213) | Acc_1: (91.86%) (30690/33408)\n",
      "Epoch: 65 | Batch_idx: 270 |  Loss_1: (0.2211) | Acc_1: (91.89%) (31875/34688)\n",
      "Epoch: 65 | Batch_idx: 280 |  Loss_1: (0.2209) | Acc_1: (91.90%) (33054/35968)\n",
      "Epoch: 65 | Batch_idx: 290 |  Loss_1: (0.2209) | Acc_1: (91.89%) (34228/37248)\n",
      "Epoch: 65 | Batch_idx: 300 |  Loss_1: (0.2212) | Acc_1: (91.89%) (35403/38528)\n",
      "Epoch: 65 | Batch_idx: 310 |  Loss_1: (0.2212) | Acc_1: (91.91%) (36586/39808)\n",
      "Epoch: 65 | Batch_idx: 320 |  Loss_1: (0.2215) | Acc_1: (91.90%) (37759/41088)\n",
      "Epoch: 65 | Batch_idx: 330 |  Loss_1: (0.2220) | Acc_1: (91.88%) (38926/42368)\n",
      "Epoch: 65 | Batch_idx: 340 |  Loss_1: (0.2228) | Acc_1: (91.84%) (40088/43648)\n",
      "Epoch: 65 | Batch_idx: 350 |  Loss_1: (0.2232) | Acc_1: (91.85%) (41266/44928)\n",
      "Epoch: 65 | Batch_idx: 360 |  Loss_1: (0.2248) | Acc_1: (91.80%) (42419/46208)\n",
      "Epoch: 65 | Batch_idx: 370 |  Loss_1: (0.2258) | Acc_1: (91.76%) (43575/47488)\n",
      "Epoch: 65 | Batch_idx: 380 |  Loss_1: (0.2263) | Acc_1: (91.75%) (44747/48768)\n",
      "Epoch: 65 | Batch_idx: 390 |  Loss_1: (0.2260) | Acc_1: (91.78%) (45889/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3199) | Acc: (90.41%) (9041/10000)\n",
      "Epoch: 66 | Batch_idx: 0 |  Loss_1: (0.1725) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 66 | Batch_idx: 10 |  Loss_1: (0.2036) | Acc_1: (92.47%) (1302/1408)\n",
      "Epoch: 66 | Batch_idx: 20 |  Loss_1: (0.1956) | Acc_1: (92.71%) (2492/2688)\n",
      "Epoch: 66 | Batch_idx: 30 |  Loss_1: (0.2010) | Acc_1: (92.72%) (3679/3968)\n",
      "Epoch: 66 | Batch_idx: 40 |  Loss_1: (0.2040) | Acc_1: (92.74%) (4867/5248)\n",
      "Epoch: 66 | Batch_idx: 50 |  Loss_1: (0.1997) | Acc_1: (92.89%) (6064/6528)\n",
      "Epoch: 66 | Batch_idx: 60 |  Loss_1: (0.1994) | Acc_1: (92.93%) (7256/7808)\n",
      "Epoch: 66 | Batch_idx: 70 |  Loss_1: (0.2013) | Acc_1: (92.91%) (8444/9088)\n",
      "Epoch: 66 | Batch_idx: 80 |  Loss_1: (0.1984) | Acc_1: (92.99%) (9641/10368)\n",
      "Epoch: 66 | Batch_idx: 90 |  Loss_1: (0.2050) | Acc_1: (92.72%) (10800/11648)\n",
      "Epoch: 66 | Batch_idx: 100 |  Loss_1: (0.2059) | Acc_1: (92.64%) (11976/12928)\n",
      "Epoch: 66 | Batch_idx: 110 |  Loss_1: (0.2060) | Acc_1: (92.63%) (13161/14208)\n",
      "Epoch: 66 | Batch_idx: 120 |  Loss_1: (0.2075) | Acc_1: (92.56%) (14335/15488)\n",
      "Epoch: 66 | Batch_idx: 130 |  Loss_1: (0.2080) | Acc_1: (92.54%) (15517/16768)\n",
      "Epoch: 66 | Batch_idx: 140 |  Loss_1: (0.2104) | Acc_1: (92.46%) (16688/18048)\n",
      "Epoch: 66 | Batch_idx: 150 |  Loss_1: (0.2094) | Acc_1: (92.52%) (17883/19328)\n",
      "Epoch: 66 | Batch_idx: 160 |  Loss_1: (0.2104) | Acc_1: (92.51%) (19064/20608)\n",
      "Epoch: 66 | Batch_idx: 170 |  Loss_1: (0.2097) | Acc_1: (92.52%) (20251/21888)\n",
      "Epoch: 66 | Batch_idx: 180 |  Loss_1: (0.2102) | Acc_1: (92.47%) (21423/23168)\n",
      "Epoch: 66 | Batch_idx: 190 |  Loss_1: (0.2106) | Acc_1: (92.45%) (22603/24448)\n",
      "Epoch: 66 | Batch_idx: 200 |  Loss_1: (0.2127) | Acc_1: (92.39%) (23770/25728)\n",
      "Epoch: 66 | Batch_idx: 210 |  Loss_1: (0.2124) | Acc_1: (92.43%) (24964/27008)\n",
      "Epoch: 66 | Batch_idx: 220 |  Loss_1: (0.2123) | Acc_1: (92.44%) (26149/28288)\n",
      "Epoch: 66 | Batch_idx: 230 |  Loss_1: (0.2145) | Acc_1: (92.38%) (27316/29568)\n",
      "Epoch: 66 | Batch_idx: 240 |  Loss_1: (0.2165) | Acc_1: (92.32%) (28479/30848)\n",
      "Epoch: 66 | Batch_idx: 250 |  Loss_1: (0.2177) | Acc_1: (92.28%) (29648/32128)\n",
      "Epoch: 66 | Batch_idx: 260 |  Loss_1: (0.2184) | Acc_1: (92.26%) (30822/33408)\n",
      "Epoch: 66 | Batch_idx: 270 |  Loss_1: (0.2185) | Acc_1: (92.24%) (31995/34688)\n",
      "Epoch: 66 | Batch_idx: 280 |  Loss_1: (0.2187) | Acc_1: (92.24%) (33178/35968)\n",
      "Epoch: 66 | Batch_idx: 290 |  Loss_1: (0.2193) | Acc_1: (92.22%) (34349/37248)\n",
      "Epoch: 66 | Batch_idx: 300 |  Loss_1: (0.2187) | Acc_1: (92.23%) (35535/38528)\n",
      "Epoch: 66 | Batch_idx: 310 |  Loss_1: (0.2183) | Acc_1: (92.26%) (36725/39808)\n",
      "Epoch: 66 | Batch_idx: 320 |  Loss_1: (0.2185) | Acc_1: (92.24%) (37900/41088)\n",
      "Epoch: 66 | Batch_idx: 330 |  Loss_1: (0.2189) | Acc_1: (92.24%) (39081/42368)\n",
      "Epoch: 66 | Batch_idx: 340 |  Loss_1: (0.2195) | Acc_1: (92.23%) (40258/43648)\n",
      "Epoch: 66 | Batch_idx: 350 |  Loss_1: (0.2196) | Acc_1: (92.22%) (41432/44928)\n",
      "Epoch: 66 | Batch_idx: 360 |  Loss_1: (0.2208) | Acc_1: (92.17%) (42589/46208)\n",
      "Epoch: 66 | Batch_idx: 370 |  Loss_1: (0.2207) | Acc_1: (92.16%) (43765/47488)\n",
      "Epoch: 66 | Batch_idx: 380 |  Loss_1: (0.2207) | Acc_1: (92.15%) (44939/48768)\n",
      "Epoch: 66 | Batch_idx: 390 |  Loss_1: (0.2219) | Acc_1: (92.12%) (46061/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3137) | Acc: (90.91%) (9091/10000)\n",
      "Epoch: 67 | Batch_idx: 0 |  Loss_1: (0.3089) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 67 | Batch_idx: 10 |  Loss_1: (0.2340) | Acc_1: (91.83%) (1293/1408)\n",
      "Epoch: 67 | Batch_idx: 20 |  Loss_1: (0.2301) | Acc_1: (91.70%) (2465/2688)\n",
      "Epoch: 67 | Batch_idx: 30 |  Loss_1: (0.2271) | Acc_1: (91.71%) (3639/3968)\n",
      "Epoch: 67 | Batch_idx: 40 |  Loss_1: (0.2204) | Acc_1: (91.90%) (4823/5248)\n",
      "Epoch: 67 | Batch_idx: 50 |  Loss_1: (0.2274) | Acc_1: (91.73%) (5988/6528)\n",
      "Epoch: 67 | Batch_idx: 60 |  Loss_1: (0.2264) | Acc_1: (91.73%) (7162/7808)\n",
      "Epoch: 67 | Batch_idx: 70 |  Loss_1: (0.2205) | Acc_1: (91.93%) (8355/9088)\n",
      "Epoch: 67 | Batch_idx: 80 |  Loss_1: (0.2187) | Acc_1: (92.03%) (9542/10368)\n",
      "Epoch: 67 | Batch_idx: 90 |  Loss_1: (0.2189) | Acc_1: (92.07%) (10724/11648)\n",
      "Epoch: 67 | Batch_idx: 100 |  Loss_1: (0.2165) | Acc_1: (92.12%) (11909/12928)\n",
      "Epoch: 67 | Batch_idx: 110 |  Loss_1: (0.2149) | Acc_1: (92.22%) (13103/14208)\n",
      "Epoch: 67 | Batch_idx: 120 |  Loss_1: (0.2131) | Acc_1: (92.26%) (14289/15488)\n",
      "Epoch: 67 | Batch_idx: 130 |  Loss_1: (0.2137) | Acc_1: (92.24%) (15466/16768)\n",
      "Epoch: 67 | Batch_idx: 140 |  Loss_1: (0.2152) | Acc_1: (92.20%) (16641/18048)\n",
      "Epoch: 67 | Batch_idx: 150 |  Loss_1: (0.2158) | Acc_1: (92.24%) (17829/19328)\n",
      "Epoch: 67 | Batch_idx: 160 |  Loss_1: (0.2152) | Acc_1: (92.29%) (19019/20608)\n",
      "Epoch: 67 | Batch_idx: 170 |  Loss_1: (0.2149) | Acc_1: (92.32%) (20207/21888)\n",
      "Epoch: 67 | Batch_idx: 180 |  Loss_1: (0.2141) | Acc_1: (92.36%) (21398/23168)\n",
      "Epoch: 67 | Batch_idx: 190 |  Loss_1: (0.2153) | Acc_1: (92.31%) (22569/24448)\n",
      "Epoch: 67 | Batch_idx: 200 |  Loss_1: (0.2164) | Acc_1: (92.28%) (23741/25728)\n",
      "Epoch: 67 | Batch_idx: 210 |  Loss_1: (0.2177) | Acc_1: (92.23%) (24909/27008)\n",
      "Epoch: 67 | Batch_idx: 220 |  Loss_1: (0.2181) | Acc_1: (92.22%) (26088/28288)\n",
      "Epoch: 67 | Batch_idx: 230 |  Loss_1: (0.2179) | Acc_1: (92.23%) (27270/29568)\n",
      "Epoch: 67 | Batch_idx: 240 |  Loss_1: (0.2177) | Acc_1: (92.24%) (28453/30848)\n",
      "Epoch: 67 | Batch_idx: 250 |  Loss_1: (0.2180) | Acc_1: (92.23%) (29633/32128)\n",
      "Epoch: 67 | Batch_idx: 260 |  Loss_1: (0.2174) | Acc_1: (92.24%) (30815/33408)\n",
      "Epoch: 67 | Batch_idx: 270 |  Loss_1: (0.2174) | Acc_1: (92.24%) (31996/34688)\n",
      "Epoch: 67 | Batch_idx: 280 |  Loss_1: (0.2163) | Acc_1: (92.30%) (33197/35968)\n",
      "Epoch: 67 | Batch_idx: 290 |  Loss_1: (0.2161) | Acc_1: (92.30%) (34380/37248)\n",
      "Epoch: 67 | Batch_idx: 300 |  Loss_1: (0.2152) | Acc_1: (92.31%) (35567/38528)\n",
      "Epoch: 67 | Batch_idx: 310 |  Loss_1: (0.2152) | Acc_1: (92.31%) (36745/39808)\n",
      "Epoch: 67 | Batch_idx: 320 |  Loss_1: (0.2156) | Acc_1: (92.30%) (37925/41088)\n",
      "Epoch: 67 | Batch_idx: 330 |  Loss_1: (0.2162) | Acc_1: (92.27%) (39091/42368)\n",
      "Epoch: 67 | Batch_idx: 340 |  Loss_1: (0.2170) | Acc_1: (92.24%) (40261/43648)\n",
      "Epoch: 67 | Batch_idx: 350 |  Loss_1: (0.2171) | Acc_1: (92.24%) (41440/44928)\n",
      "Epoch: 67 | Batch_idx: 360 |  Loss_1: (0.2170) | Acc_1: (92.25%) (42628/46208)\n",
      "Epoch: 67 | Batch_idx: 370 |  Loss_1: (0.2169) | Acc_1: (92.26%) (43812/47488)\n",
      "Epoch: 67 | Batch_idx: 380 |  Loss_1: (0.2176) | Acc_1: (92.23%) (44977/48768)\n",
      "Epoch: 67 | Batch_idx: 390 |  Loss_1: (0.2190) | Acc_1: (92.18%) (46092/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2973) | Acc: (91.00%) (9100/10000)\n",
      "Epoch: 68 | Batch_idx: 0 |  Loss_1: (0.2251) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 68 | Batch_idx: 10 |  Loss_1: (0.2194) | Acc_1: (92.40%) (1301/1408)\n",
      "Epoch: 68 | Batch_idx: 20 |  Loss_1: (0.2259) | Acc_1: (92.04%) (2474/2688)\n",
      "Epoch: 68 | Batch_idx: 30 |  Loss_1: (0.2149) | Acc_1: (92.46%) (3669/3968)\n",
      "Epoch: 68 | Batch_idx: 40 |  Loss_1: (0.2207) | Acc_1: (92.21%) (4839/5248)\n",
      "Epoch: 68 | Batch_idx: 50 |  Loss_1: (0.2214) | Acc_1: (92.02%) (6007/6528)\n",
      "Epoch: 68 | Batch_idx: 60 |  Loss_1: (0.2192) | Acc_1: (92.14%) (7194/7808)\n",
      "Epoch: 68 | Batch_idx: 70 |  Loss_1: (0.2203) | Acc_1: (92.11%) (8371/9088)\n",
      "Epoch: 68 | Batch_idx: 80 |  Loss_1: (0.2215) | Acc_1: (92.07%) (9546/10368)\n",
      "Epoch: 68 | Batch_idx: 90 |  Loss_1: (0.2223) | Acc_1: (92.08%) (10725/11648)\n",
      "Epoch: 68 | Batch_idx: 100 |  Loss_1: (0.2193) | Acc_1: (92.16%) (11914/12928)\n",
      "Epoch: 68 | Batch_idx: 110 |  Loss_1: (0.2199) | Acc_1: (92.05%) (13078/14208)\n",
      "Epoch: 68 | Batch_idx: 120 |  Loss_1: (0.2211) | Acc_1: (92.03%) (14253/15488)\n",
      "Epoch: 68 | Batch_idx: 130 |  Loss_1: (0.2204) | Acc_1: (92.09%) (15441/16768)\n",
      "Epoch: 68 | Batch_idx: 140 |  Loss_1: (0.2218) | Acc_1: (92.00%) (16604/18048)\n",
      "Epoch: 68 | Batch_idx: 150 |  Loss_1: (0.2223) | Acc_1: (92.02%) (17785/19328)\n",
      "Epoch: 68 | Batch_idx: 160 |  Loss_1: (0.2232) | Acc_1: (92.04%) (18967/20608)\n",
      "Epoch: 68 | Batch_idx: 170 |  Loss_1: (0.2250) | Acc_1: (91.96%) (20128/21888)\n",
      "Epoch: 68 | Batch_idx: 180 |  Loss_1: (0.2233) | Acc_1: (92.01%) (21318/23168)\n",
      "Epoch: 68 | Batch_idx: 190 |  Loss_1: (0.2236) | Acc_1: (92.00%) (22493/24448)\n",
      "Epoch: 68 | Batch_idx: 200 |  Loss_1: (0.2241) | Acc_1: (91.98%) (23664/25728)\n",
      "Epoch: 68 | Batch_idx: 210 |  Loss_1: (0.2234) | Acc_1: (91.98%) (24842/27008)\n",
      "Epoch: 68 | Batch_idx: 220 |  Loss_1: (0.2232) | Acc_1: (91.99%) (26021/28288)\n",
      "Epoch: 68 | Batch_idx: 230 |  Loss_1: (0.2226) | Acc_1: (91.98%) (27198/29568)\n",
      "Epoch: 68 | Batch_idx: 240 |  Loss_1: (0.2228) | Acc_1: (91.99%) (28378/30848)\n",
      "Epoch: 68 | Batch_idx: 250 |  Loss_1: (0.2222) | Acc_1: (92.03%) (29568/32128)\n",
      "Epoch: 68 | Batch_idx: 260 |  Loss_1: (0.2219) | Acc_1: (92.03%) (30745/33408)\n",
      "Epoch: 68 | Batch_idx: 270 |  Loss_1: (0.2222) | Acc_1: (92.01%) (31917/34688)\n",
      "Epoch: 68 | Batch_idx: 280 |  Loss_1: (0.2216) | Acc_1: (92.03%) (33102/35968)\n",
      "Epoch: 68 | Batch_idx: 290 |  Loss_1: (0.2208) | Acc_1: (92.07%) (34293/37248)\n",
      "Epoch: 68 | Batch_idx: 300 |  Loss_1: (0.2208) | Acc_1: (92.05%) (35464/38528)\n",
      "Epoch: 68 | Batch_idx: 310 |  Loss_1: (0.2198) | Acc_1: (92.08%) (36655/39808)\n",
      "Epoch: 68 | Batch_idx: 320 |  Loss_1: (0.2194) | Acc_1: (92.10%) (37840/41088)\n",
      "Epoch: 68 | Batch_idx: 330 |  Loss_1: (0.2189) | Acc_1: (92.12%) (39029/42368)\n",
      "Epoch: 68 | Batch_idx: 340 |  Loss_1: (0.2193) | Acc_1: (92.12%) (40208/43648)\n",
      "Epoch: 68 | Batch_idx: 350 |  Loss_1: (0.2190) | Acc_1: (92.11%) (41385/44928)\n",
      "Epoch: 68 | Batch_idx: 360 |  Loss_1: (0.2187) | Acc_1: (92.13%) (42570/46208)\n",
      "Epoch: 68 | Batch_idx: 370 |  Loss_1: (0.2185) | Acc_1: (92.13%) (43751/47488)\n",
      "Epoch: 68 | Batch_idx: 380 |  Loss_1: (0.2186) | Acc_1: (92.12%) (44927/48768)\n",
      "Epoch: 68 | Batch_idx: 390 |  Loss_1: (0.2189) | Acc_1: (92.10%) (46051/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2758) | Acc: (91.38%) (9138/10000)\n",
      "Epoch: 69 | Batch_idx: 0 |  Loss_1: (0.2961) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 69 | Batch_idx: 10 |  Loss_1: (0.2107) | Acc_1: (92.97%) (1309/1408)\n",
      "Epoch: 69 | Batch_idx: 20 |  Loss_1: (0.1974) | Acc_1: (92.97%) (2499/2688)\n",
      "Epoch: 69 | Batch_idx: 30 |  Loss_1: (0.1914) | Acc_1: (93.37%) (3705/3968)\n",
      "Epoch: 69 | Batch_idx: 40 |  Loss_1: (0.1934) | Acc_1: (93.20%) (4891/5248)\n",
      "Epoch: 69 | Batch_idx: 50 |  Loss_1: (0.1934) | Acc_1: (93.17%) (6082/6528)\n",
      "Epoch: 69 | Batch_idx: 60 |  Loss_1: (0.2014) | Acc_1: (92.83%) (7248/7808)\n",
      "Epoch: 69 | Batch_idx: 70 |  Loss_1: (0.1956) | Acc_1: (92.97%) (8449/9088)\n",
      "Epoch: 69 | Batch_idx: 80 |  Loss_1: (0.1972) | Acc_1: (92.93%) (9635/10368)\n",
      "Epoch: 69 | Batch_idx: 90 |  Loss_1: (0.1979) | Acc_1: (92.87%) (10818/11648)\n",
      "Epoch: 69 | Batch_idx: 100 |  Loss_1: (0.2018) | Acc_1: (92.71%) (11986/12928)\n",
      "Epoch: 69 | Batch_idx: 110 |  Loss_1: (0.2033) | Acc_1: (92.68%) (13168/14208)\n",
      "Epoch: 69 | Batch_idx: 120 |  Loss_1: (0.2061) | Acc_1: (92.63%) (14346/15488)\n",
      "Epoch: 69 | Batch_idx: 130 |  Loss_1: (0.2065) | Acc_1: (92.61%) (15529/16768)\n",
      "Epoch: 69 | Batch_idx: 140 |  Loss_1: (0.2046) | Acc_1: (92.68%) (16727/18048)\n",
      "Epoch: 69 | Batch_idx: 150 |  Loss_1: (0.2044) | Acc_1: (92.73%) (17923/19328)\n",
      "Epoch: 69 | Batch_idx: 160 |  Loss_1: (0.2056) | Acc_1: (92.68%) (19099/20608)\n",
      "Epoch: 69 | Batch_idx: 170 |  Loss_1: (0.2065) | Acc_1: (92.64%) (20278/21888)\n",
      "Epoch: 69 | Batch_idx: 180 |  Loss_1: (0.2064) | Acc_1: (92.65%) (21466/23168)\n",
      "Epoch: 69 | Batch_idx: 190 |  Loss_1: (0.2095) | Acc_1: (92.54%) (22625/24448)\n",
      "Epoch: 69 | Batch_idx: 200 |  Loss_1: (0.2099) | Acc_1: (92.51%) (23801/25728)\n",
      "Epoch: 69 | Batch_idx: 210 |  Loss_1: (0.2099) | Acc_1: (92.51%) (24984/27008)\n",
      "Epoch: 69 | Batch_idx: 220 |  Loss_1: (0.2096) | Acc_1: (92.51%) (26170/28288)\n",
      "Epoch: 69 | Batch_idx: 230 |  Loss_1: (0.2104) | Acc_1: (92.47%) (27341/29568)\n",
      "Epoch: 69 | Batch_idx: 240 |  Loss_1: (0.2119) | Acc_1: (92.43%) (28513/30848)\n",
      "Epoch: 69 | Batch_idx: 250 |  Loss_1: (0.2126) | Acc_1: (92.42%) (29693/32128)\n",
      "Epoch: 69 | Batch_idx: 260 |  Loss_1: (0.2128) | Acc_1: (92.42%) (30876/33408)\n",
      "Epoch: 69 | Batch_idx: 270 |  Loss_1: (0.2134) | Acc_1: (92.40%) (32050/34688)\n",
      "Epoch: 69 | Batch_idx: 280 |  Loss_1: (0.2141) | Acc_1: (92.37%) (33224/35968)\n",
      "Epoch: 69 | Batch_idx: 290 |  Loss_1: (0.2140) | Acc_1: (92.38%) (34410/37248)\n",
      "Epoch: 69 | Batch_idx: 300 |  Loss_1: (0.2145) | Acc_1: (92.37%) (35588/38528)\n",
      "Epoch: 69 | Batch_idx: 310 |  Loss_1: (0.2144) | Acc_1: (92.36%) (36768/39808)\n",
      "Epoch: 69 | Batch_idx: 320 |  Loss_1: (0.2149) | Acc_1: (92.35%) (37943/41088)\n",
      "Epoch: 69 | Batch_idx: 330 |  Loss_1: (0.2142) | Acc_1: (92.36%) (39131/42368)\n",
      "Epoch: 69 | Batch_idx: 340 |  Loss_1: (0.2139) | Acc_1: (92.38%) (40321/43648)\n",
      "Epoch: 69 | Batch_idx: 350 |  Loss_1: (0.2142) | Acc_1: (92.37%) (41499/44928)\n",
      "Epoch: 69 | Batch_idx: 360 |  Loss_1: (0.2146) | Acc_1: (92.36%) (42677/46208)\n",
      "Epoch: 69 | Batch_idx: 370 |  Loss_1: (0.2143) | Acc_1: (92.39%) (43874/47488)\n",
      "Epoch: 69 | Batch_idx: 380 |  Loss_1: (0.2142) | Acc_1: (92.39%) (45057/48768)\n",
      "Epoch: 69 | Batch_idx: 390 |  Loss_1: (0.2141) | Acc_1: (92.39%) (46197/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2997) | Acc: (90.46%) (9046/10000)\n",
      "Epoch: 70 | Batch_idx: 0 |  Loss_1: (0.2018) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 70 | Batch_idx: 10 |  Loss_1: (0.1959) | Acc_1: (92.97%) (1309/1408)\n",
      "Epoch: 70 | Batch_idx: 20 |  Loss_1: (0.1886) | Acc_1: (93.27%) (2507/2688)\n",
      "Epoch: 70 | Batch_idx: 30 |  Loss_1: (0.2170) | Acc_1: (92.46%) (3669/3968)\n",
      "Epoch: 70 | Batch_idx: 40 |  Loss_1: (0.2120) | Acc_1: (92.42%) (4850/5248)\n",
      "Epoch: 70 | Batch_idx: 50 |  Loss_1: (0.2153) | Acc_1: (92.36%) (6029/6528)\n",
      "Epoch: 70 | Batch_idx: 60 |  Loss_1: (0.2149) | Acc_1: (92.30%) (7207/7808)\n",
      "Epoch: 70 | Batch_idx: 70 |  Loss_1: (0.2114) | Acc_1: (92.36%) (8394/9088)\n",
      "Epoch: 70 | Batch_idx: 80 |  Loss_1: (0.2134) | Acc_1: (92.21%) (9560/10368)\n",
      "Epoch: 70 | Batch_idx: 90 |  Loss_1: (0.2109) | Acc_1: (92.25%) (10745/11648)\n",
      "Epoch: 70 | Batch_idx: 100 |  Loss_1: (0.2132) | Acc_1: (92.13%) (11911/12928)\n",
      "Epoch: 70 | Batch_idx: 110 |  Loss_1: (0.2146) | Acc_1: (92.09%) (13084/14208)\n",
      "Epoch: 70 | Batch_idx: 120 |  Loss_1: (0.2151) | Acc_1: (92.10%) (14265/15488)\n",
      "Epoch: 70 | Batch_idx: 130 |  Loss_1: (0.2137) | Acc_1: (92.13%) (15448/16768)\n",
      "Epoch: 70 | Batch_idx: 140 |  Loss_1: (0.2163) | Acc_1: (92.02%) (16608/18048)\n",
      "Epoch: 70 | Batch_idx: 150 |  Loss_1: (0.2153) | Acc_1: (92.07%) (17796/19328)\n",
      "Epoch: 70 | Batch_idx: 160 |  Loss_1: (0.2154) | Acc_1: (92.09%) (18978/20608)\n",
      "Epoch: 70 | Batch_idx: 170 |  Loss_1: (0.2144) | Acc_1: (92.14%) (20168/21888)\n",
      "Epoch: 70 | Batch_idx: 180 |  Loss_1: (0.2136) | Acc_1: (92.19%) (21358/23168)\n",
      "Epoch: 70 | Batch_idx: 190 |  Loss_1: (0.2143) | Acc_1: (92.16%) (22531/24448)\n",
      "Epoch: 70 | Batch_idx: 200 |  Loss_1: (0.2155) | Acc_1: (92.14%) (23706/25728)\n",
      "Epoch: 70 | Batch_idx: 210 |  Loss_1: (0.2174) | Acc_1: (92.11%) (24877/27008)\n",
      "Epoch: 70 | Batch_idx: 220 |  Loss_1: (0.2170) | Acc_1: (92.12%) (26058/28288)\n",
      "Epoch: 70 | Batch_idx: 230 |  Loss_1: (0.2185) | Acc_1: (92.10%) (27232/29568)\n",
      "Epoch: 70 | Batch_idx: 240 |  Loss_1: (0.2181) | Acc_1: (92.13%) (28421/30848)\n",
      "Epoch: 70 | Batch_idx: 250 |  Loss_1: (0.2193) | Acc_1: (92.07%) (29580/32128)\n",
      "Epoch: 70 | Batch_idx: 260 |  Loss_1: (0.2209) | Acc_1: (92.03%) (30747/33408)\n",
      "Epoch: 70 | Batch_idx: 270 |  Loss_1: (0.2210) | Acc_1: (92.01%) (31915/34688)\n",
      "Epoch: 70 | Batch_idx: 280 |  Loss_1: (0.2209) | Acc_1: (92.02%) (33097/35968)\n",
      "Epoch: 70 | Batch_idx: 290 |  Loss_1: (0.2206) | Acc_1: (92.02%) (34275/37248)\n",
      "Epoch: 70 | Batch_idx: 300 |  Loss_1: (0.2207) | Acc_1: (92.02%) (35452/38528)\n",
      "Epoch: 70 | Batch_idx: 310 |  Loss_1: (0.2200) | Acc_1: (92.03%) (36634/39808)\n",
      "Epoch: 70 | Batch_idx: 320 |  Loss_1: (0.2195) | Acc_1: (92.06%) (37824/41088)\n",
      "Epoch: 70 | Batch_idx: 330 |  Loss_1: (0.2195) | Acc_1: (92.05%) (39001/42368)\n",
      "Epoch: 70 | Batch_idx: 340 |  Loss_1: (0.2194) | Acc_1: (92.06%) (40184/43648)\n",
      "Epoch: 70 | Batch_idx: 350 |  Loss_1: (0.2190) | Acc_1: (92.10%) (41377/44928)\n",
      "Epoch: 70 | Batch_idx: 360 |  Loss_1: (0.2193) | Acc_1: (92.08%) (42550/46208)\n",
      "Epoch: 70 | Batch_idx: 370 |  Loss_1: (0.2193) | Acc_1: (92.09%) (43733/47488)\n",
      "Epoch: 70 | Batch_idx: 380 |  Loss_1: (0.2187) | Acc_1: (92.11%) (44919/48768)\n",
      "Epoch: 70 | Batch_idx: 390 |  Loss_1: (0.2187) | Acc_1: (92.11%) (46053/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3002) | Acc: (90.83%) (9083/10000)\n",
      "Epoch: 71 | Batch_idx: 0 |  Loss_1: (0.1492) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 71 | Batch_idx: 10 |  Loss_1: (0.1793) | Acc_1: (93.75%) (1320/1408)\n",
      "Epoch: 71 | Batch_idx: 20 |  Loss_1: (0.1900) | Acc_1: (93.23%) (2506/2688)\n",
      "Epoch: 71 | Batch_idx: 30 |  Loss_1: (0.1930) | Acc_1: (93.02%) (3691/3968)\n",
      "Epoch: 71 | Batch_idx: 40 |  Loss_1: (0.1924) | Acc_1: (93.16%) (4889/5248)\n",
      "Epoch: 71 | Batch_idx: 50 |  Loss_1: (0.2021) | Acc_1: (92.83%) (6060/6528)\n",
      "Epoch: 71 | Batch_idx: 60 |  Loss_1: (0.2066) | Acc_1: (92.65%) (7234/7808)\n",
      "Epoch: 71 | Batch_idx: 70 |  Loss_1: (0.2095) | Acc_1: (92.48%) (8405/9088)\n",
      "Epoch: 71 | Batch_idx: 80 |  Loss_1: (0.2070) | Acc_1: (92.61%) (9602/10368)\n",
      "Epoch: 71 | Batch_idx: 90 |  Loss_1: (0.2083) | Acc_1: (92.59%) (10785/11648)\n",
      "Epoch: 71 | Batch_idx: 100 |  Loss_1: (0.2088) | Acc_1: (92.57%) (11967/12928)\n",
      "Epoch: 71 | Batch_idx: 110 |  Loss_1: (0.2085) | Acc_1: (92.51%) (13144/14208)\n",
      "Epoch: 71 | Batch_idx: 120 |  Loss_1: (0.2100) | Acc_1: (92.43%) (14316/15488)\n",
      "Epoch: 71 | Batch_idx: 130 |  Loss_1: (0.2110) | Acc_1: (92.45%) (15502/16768)\n",
      "Epoch: 71 | Batch_idx: 140 |  Loss_1: (0.2125) | Acc_1: (92.38%) (16673/18048)\n",
      "Epoch: 71 | Batch_idx: 150 |  Loss_1: (0.2114) | Acc_1: (92.44%) (17867/19328)\n",
      "Epoch: 71 | Batch_idx: 160 |  Loss_1: (0.2100) | Acc_1: (92.51%) (19065/20608)\n",
      "Epoch: 71 | Batch_idx: 170 |  Loss_1: (0.2112) | Acc_1: (92.49%) (20244/21888)\n",
      "Epoch: 71 | Batch_idx: 180 |  Loss_1: (0.2111) | Acc_1: (92.49%) (21428/23168)\n",
      "Epoch: 71 | Batch_idx: 190 |  Loss_1: (0.2117) | Acc_1: (92.49%) (22612/24448)\n",
      "Epoch: 71 | Batch_idx: 200 |  Loss_1: (0.2110) | Acc_1: (92.51%) (23801/25728)\n",
      "Epoch: 71 | Batch_idx: 210 |  Loss_1: (0.2107) | Acc_1: (92.52%) (24987/27008)\n",
      "Epoch: 71 | Batch_idx: 220 |  Loss_1: (0.2099) | Acc_1: (92.54%) (26178/28288)\n",
      "Epoch: 71 | Batch_idx: 230 |  Loss_1: (0.2082) | Acc_1: (92.59%) (27377/29568)\n",
      "Epoch: 71 | Batch_idx: 240 |  Loss_1: (0.2074) | Acc_1: (92.61%) (28567/30848)\n",
      "Epoch: 71 | Batch_idx: 250 |  Loss_1: (0.2078) | Acc_1: (92.58%) (29744/32128)\n",
      "Epoch: 71 | Batch_idx: 260 |  Loss_1: (0.2072) | Acc_1: (92.60%) (30937/33408)\n",
      "Epoch: 71 | Batch_idx: 270 |  Loss_1: (0.2056) | Acc_1: (92.63%) (32131/34688)\n",
      "Epoch: 71 | Batch_idx: 280 |  Loss_1: (0.2049) | Acc_1: (92.66%) (33329/35968)\n",
      "Epoch: 71 | Batch_idx: 290 |  Loss_1: (0.2056) | Acc_1: (92.64%) (34508/37248)\n",
      "Epoch: 71 | Batch_idx: 300 |  Loss_1: (0.2059) | Acc_1: (92.62%) (35685/38528)\n",
      "Epoch: 71 | Batch_idx: 310 |  Loss_1: (0.2065) | Acc_1: (92.59%) (36860/39808)\n",
      "Epoch: 71 | Batch_idx: 320 |  Loss_1: (0.2061) | Acc_1: (92.61%) (38052/41088)\n",
      "Epoch: 71 | Batch_idx: 330 |  Loss_1: (0.2076) | Acc_1: (92.56%) (39216/42368)\n",
      "Epoch: 71 | Batch_idx: 340 |  Loss_1: (0.2081) | Acc_1: (92.56%) (40402/43648)\n",
      "Epoch: 71 | Batch_idx: 350 |  Loss_1: (0.2091) | Acc_1: (92.53%) (41570/44928)\n",
      "Epoch: 71 | Batch_idx: 360 |  Loss_1: (0.2096) | Acc_1: (92.51%) (42748/46208)\n",
      "Epoch: 71 | Batch_idx: 370 |  Loss_1: (0.2091) | Acc_1: (92.53%) (43941/47488)\n",
      "Epoch: 71 | Batch_idx: 380 |  Loss_1: (0.2089) | Acc_1: (92.52%) (45121/48768)\n",
      "Epoch: 71 | Batch_idx: 390 |  Loss_1: (0.2094) | Acc_1: (92.50%) (46249/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3138) | Acc: (90.89%) (9089/10000)\n",
      "Epoch: 72 | Batch_idx: 0 |  Loss_1: (0.2506) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 72 | Batch_idx: 10 |  Loss_1: (0.2272) | Acc_1: (91.41%) (1287/1408)\n",
      "Epoch: 72 | Batch_idx: 20 |  Loss_1: (0.2195) | Acc_1: (91.74%) (2466/2688)\n",
      "Epoch: 72 | Batch_idx: 30 |  Loss_1: (0.2212) | Acc_1: (91.89%) (3646/3968)\n",
      "Epoch: 72 | Batch_idx: 40 |  Loss_1: (0.2200) | Acc_1: (91.90%) (4823/5248)\n",
      "Epoch: 72 | Batch_idx: 50 |  Loss_1: (0.2119) | Acc_1: (92.16%) (6016/6528)\n",
      "Epoch: 72 | Batch_idx: 60 |  Loss_1: (0.2113) | Acc_1: (92.21%) (7200/7808)\n",
      "Epoch: 72 | Batch_idx: 70 |  Loss_1: (0.2117) | Acc_1: (92.21%) (8380/9088)\n",
      "Epoch: 72 | Batch_idx: 80 |  Loss_1: (0.2128) | Acc_1: (92.22%) (9561/10368)\n",
      "Epoch: 72 | Batch_idx: 90 |  Loss_1: (0.2118) | Acc_1: (92.37%) (10759/11648)\n",
      "Epoch: 72 | Batch_idx: 100 |  Loss_1: (0.2101) | Acc_1: (92.41%) (11947/12928)\n",
      "Epoch: 72 | Batch_idx: 110 |  Loss_1: (0.2120) | Acc_1: (92.34%) (13120/14208)\n",
      "Epoch: 72 | Batch_idx: 120 |  Loss_1: (0.2117) | Acc_1: (92.34%) (14302/15488)\n",
      "Epoch: 72 | Batch_idx: 130 |  Loss_1: (0.2113) | Acc_1: (92.35%) (15485/16768)\n",
      "Epoch: 72 | Batch_idx: 140 |  Loss_1: (0.2123) | Acc_1: (92.33%) (16663/18048)\n",
      "Epoch: 72 | Batch_idx: 150 |  Loss_1: (0.2128) | Acc_1: (92.33%) (17846/19328)\n",
      "Epoch: 72 | Batch_idx: 160 |  Loss_1: (0.2123) | Acc_1: (92.38%) (19037/20608)\n",
      "Epoch: 72 | Batch_idx: 170 |  Loss_1: (0.2111) | Acc_1: (92.40%) (20224/21888)\n",
      "Epoch: 72 | Batch_idx: 180 |  Loss_1: (0.2096) | Acc_1: (92.45%) (21418/23168)\n",
      "Epoch: 72 | Batch_idx: 190 |  Loss_1: (0.2098) | Acc_1: (92.46%) (22604/24448)\n",
      "Epoch: 72 | Batch_idx: 200 |  Loss_1: (0.2095) | Acc_1: (92.44%) (23783/25728)\n",
      "Epoch: 72 | Batch_idx: 210 |  Loss_1: (0.2082) | Acc_1: (92.47%) (24973/27008)\n",
      "Epoch: 72 | Batch_idx: 220 |  Loss_1: (0.2084) | Acc_1: (92.45%) (26152/28288)\n",
      "Epoch: 72 | Batch_idx: 230 |  Loss_1: (0.2093) | Acc_1: (92.40%) (27320/29568)\n",
      "Epoch: 72 | Batch_idx: 240 |  Loss_1: (0.2099) | Acc_1: (92.37%) (28493/30848)\n",
      "Epoch: 72 | Batch_idx: 250 |  Loss_1: (0.2103) | Acc_1: (92.34%) (29667/32128)\n",
      "Epoch: 72 | Batch_idx: 260 |  Loss_1: (0.2109) | Acc_1: (92.33%) (30845/33408)\n",
      "Epoch: 72 | Batch_idx: 270 |  Loss_1: (0.2109) | Acc_1: (92.33%) (32028/34688)\n",
      "Epoch: 72 | Batch_idx: 280 |  Loss_1: (0.2100) | Acc_1: (92.37%) (33223/35968)\n",
      "Epoch: 72 | Batch_idx: 290 |  Loss_1: (0.2102) | Acc_1: (92.39%) (34414/37248)\n",
      "Epoch: 72 | Batch_idx: 300 |  Loss_1: (0.2113) | Acc_1: (92.33%) (35574/38528)\n",
      "Epoch: 72 | Batch_idx: 310 |  Loss_1: (0.2117) | Acc_1: (92.33%) (36756/39808)\n",
      "Epoch: 72 | Batch_idx: 320 |  Loss_1: (0.2118) | Acc_1: (92.33%) (37938/41088)\n",
      "Epoch: 72 | Batch_idx: 330 |  Loss_1: (0.2119) | Acc_1: (92.34%) (39123/42368)\n",
      "Epoch: 72 | Batch_idx: 340 |  Loss_1: (0.2117) | Acc_1: (92.35%) (40309/43648)\n",
      "Epoch: 72 | Batch_idx: 350 |  Loss_1: (0.2118) | Acc_1: (92.37%) (41501/44928)\n",
      "Epoch: 72 | Batch_idx: 360 |  Loss_1: (0.2115) | Acc_1: (92.39%) (42692/46208)\n",
      "Epoch: 72 | Batch_idx: 370 |  Loss_1: (0.2109) | Acc_1: (92.40%) (43877/47488)\n",
      "Epoch: 72 | Batch_idx: 380 |  Loss_1: (0.2113) | Acc_1: (92.39%) (45056/48768)\n",
      "Epoch: 72 | Batch_idx: 390 |  Loss_1: (0.2110) | Acc_1: (92.41%) (46203/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3210) | Acc: (90.38%) (9038/10000)\n",
      "Epoch: 73 | Batch_idx: 0 |  Loss_1: (0.1750) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 73 | Batch_idx: 10 |  Loss_1: (0.1843) | Acc_1: (93.54%) (1317/1408)\n",
      "Epoch: 73 | Batch_idx: 20 |  Loss_1: (0.1925) | Acc_1: (92.78%) (2494/2688)\n",
      "Epoch: 73 | Batch_idx: 30 |  Loss_1: (0.2042) | Acc_1: (92.24%) (3660/3968)\n",
      "Epoch: 73 | Batch_idx: 40 |  Loss_1: (0.1973) | Acc_1: (92.68%) (4864/5248)\n",
      "Epoch: 73 | Batch_idx: 50 |  Loss_1: (0.1949) | Acc_1: (92.83%) (6060/6528)\n",
      "Epoch: 73 | Batch_idx: 60 |  Loss_1: (0.1971) | Acc_1: (92.75%) (7242/7808)\n",
      "Epoch: 73 | Batch_idx: 70 |  Loss_1: (0.1989) | Acc_1: (92.69%) (8424/9088)\n",
      "Epoch: 73 | Batch_idx: 80 |  Loss_1: (0.1987) | Acc_1: (92.73%) (9614/10368)\n",
      "Epoch: 73 | Batch_idx: 90 |  Loss_1: (0.2010) | Acc_1: (92.64%) (10791/11648)\n",
      "Epoch: 73 | Batch_idx: 100 |  Loss_1: (0.1990) | Acc_1: (92.72%) (11987/12928)\n",
      "Epoch: 73 | Batch_idx: 110 |  Loss_1: (0.2003) | Acc_1: (92.67%) (13167/14208)\n",
      "Epoch: 73 | Batch_idx: 120 |  Loss_1: (0.1984) | Acc_1: (92.76%) (14366/15488)\n",
      "Epoch: 73 | Batch_idx: 130 |  Loss_1: (0.1988) | Acc_1: (92.78%) (15557/16768)\n",
      "Epoch: 73 | Batch_idx: 140 |  Loss_1: (0.2028) | Acc_1: (92.68%) (16727/18048)\n",
      "Epoch: 73 | Batch_idx: 150 |  Loss_1: (0.1995) | Acc_1: (92.81%) (17938/19328)\n",
      "Epoch: 73 | Batch_idx: 160 |  Loss_1: (0.1998) | Acc_1: (92.77%) (19118/20608)\n",
      "Epoch: 73 | Batch_idx: 170 |  Loss_1: (0.2020) | Acc_1: (92.71%) (20293/21888)\n",
      "Epoch: 73 | Batch_idx: 180 |  Loss_1: (0.2026) | Acc_1: (92.70%) (21476/23168)\n",
      "Epoch: 73 | Batch_idx: 190 |  Loss_1: (0.2020) | Acc_1: (92.73%) (22670/24448)\n",
      "Epoch: 73 | Batch_idx: 200 |  Loss_1: (0.2031) | Acc_1: (92.71%) (23853/25728)\n",
      "Epoch: 73 | Batch_idx: 210 |  Loss_1: (0.2031) | Acc_1: (92.71%) (25038/27008)\n",
      "Epoch: 73 | Batch_idx: 220 |  Loss_1: (0.2042) | Acc_1: (92.63%) (26203/28288)\n",
      "Epoch: 73 | Batch_idx: 230 |  Loss_1: (0.2046) | Acc_1: (92.61%) (27384/29568)\n",
      "Epoch: 73 | Batch_idx: 240 |  Loss_1: (0.2043) | Acc_1: (92.61%) (28569/30848)\n",
      "Epoch: 73 | Batch_idx: 250 |  Loss_1: (0.2034) | Acc_1: (92.66%) (29770/32128)\n",
      "Epoch: 73 | Batch_idx: 260 |  Loss_1: (0.2020) | Acc_1: (92.73%) (30978/33408)\n",
      "Epoch: 73 | Batch_idx: 270 |  Loss_1: (0.2020) | Acc_1: (92.72%) (32164/34688)\n",
      "Epoch: 73 | Batch_idx: 280 |  Loss_1: (0.2026) | Acc_1: (92.69%) (33337/35968)\n",
      "Epoch: 73 | Batch_idx: 290 |  Loss_1: (0.2033) | Acc_1: (92.67%) (34516/37248)\n",
      "Epoch: 73 | Batch_idx: 300 |  Loss_1: (0.2030) | Acc_1: (92.70%) (35715/38528)\n",
      "Epoch: 73 | Batch_idx: 310 |  Loss_1: (0.2032) | Acc_1: (92.69%) (36900/39808)\n",
      "Epoch: 73 | Batch_idx: 320 |  Loss_1: (0.2038) | Acc_1: (92.68%) (38079/41088)\n",
      "Epoch: 73 | Batch_idx: 330 |  Loss_1: (0.2043) | Acc_1: (92.66%) (39259/42368)\n",
      "Epoch: 73 | Batch_idx: 340 |  Loss_1: (0.2047) | Acc_1: (92.64%) (40436/43648)\n",
      "Epoch: 73 | Batch_idx: 350 |  Loss_1: (0.2056) | Acc_1: (92.61%) (41610/44928)\n",
      "Epoch: 73 | Batch_idx: 360 |  Loss_1: (0.2055) | Acc_1: (92.62%) (42798/46208)\n",
      "Epoch: 73 | Batch_idx: 370 |  Loss_1: (0.2054) | Acc_1: (92.62%) (43983/47488)\n",
      "Epoch: 73 | Batch_idx: 380 |  Loss_1: (0.2052) | Acc_1: (92.60%) (45160/48768)\n",
      "Epoch: 73 | Batch_idx: 390 |  Loss_1: (0.2060) | Acc_1: (92.58%) (46290/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3209) | Acc: (90.53%) (9053/10000)\n",
      "Epoch: 74 | Batch_idx: 0 |  Loss_1: (0.2408) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 74 | Batch_idx: 10 |  Loss_1: (0.1986) | Acc_1: (92.54%) (1303/1408)\n",
      "Epoch: 74 | Batch_idx: 20 |  Loss_1: (0.1903) | Acc_1: (92.82%) (2495/2688)\n",
      "Epoch: 74 | Batch_idx: 30 |  Loss_1: (0.1787) | Acc_1: (93.52%) (3711/3968)\n",
      "Epoch: 74 | Batch_idx: 40 |  Loss_1: (0.1818) | Acc_1: (93.43%) (4903/5248)\n",
      "Epoch: 74 | Batch_idx: 50 |  Loss_1: (0.1842) | Acc_1: (93.26%) (6088/6528)\n",
      "Epoch: 74 | Batch_idx: 60 |  Loss_1: (0.1832) | Acc_1: (93.25%) (7281/7808)\n",
      "Epoch: 74 | Batch_idx: 70 |  Loss_1: (0.1836) | Acc_1: (93.32%) (8481/9088)\n",
      "Epoch: 74 | Batch_idx: 80 |  Loss_1: (0.1857) | Acc_1: (93.31%) (9674/10368)\n",
      "Epoch: 74 | Batch_idx: 90 |  Loss_1: (0.1854) | Acc_1: (93.28%) (10865/11648)\n",
      "Epoch: 74 | Batch_idx: 100 |  Loss_1: (0.1869) | Acc_1: (93.24%) (12054/12928)\n",
      "Epoch: 74 | Batch_idx: 110 |  Loss_1: (0.1887) | Acc_1: (93.18%) (13239/14208)\n",
      "Epoch: 74 | Batch_idx: 120 |  Loss_1: (0.1889) | Acc_1: (93.19%) (14433/15488)\n",
      "Epoch: 74 | Batch_idx: 130 |  Loss_1: (0.1901) | Acc_1: (93.18%) (15624/16768)\n",
      "Epoch: 74 | Batch_idx: 140 |  Loss_1: (0.1900) | Acc_1: (93.18%) (16818/18048)\n",
      "Epoch: 74 | Batch_idx: 150 |  Loss_1: (0.1902) | Acc_1: (93.15%) (18004/19328)\n",
      "Epoch: 74 | Batch_idx: 160 |  Loss_1: (0.1906) | Acc_1: (93.16%) (19198/20608)\n",
      "Epoch: 74 | Batch_idx: 170 |  Loss_1: (0.1909) | Acc_1: (93.14%) (20387/21888)\n",
      "Epoch: 74 | Batch_idx: 180 |  Loss_1: (0.1904) | Acc_1: (93.16%) (21583/23168)\n",
      "Epoch: 74 | Batch_idx: 190 |  Loss_1: (0.1914) | Acc_1: (93.10%) (22761/24448)\n",
      "Epoch: 74 | Batch_idx: 200 |  Loss_1: (0.1918) | Acc_1: (93.08%) (23948/25728)\n",
      "Epoch: 74 | Batch_idx: 210 |  Loss_1: (0.1912) | Acc_1: (93.09%) (25143/27008)\n",
      "Epoch: 74 | Batch_idx: 220 |  Loss_1: (0.1908) | Acc_1: (93.12%) (26343/28288)\n",
      "Epoch: 74 | Batch_idx: 230 |  Loss_1: (0.1902) | Acc_1: (93.15%) (27542/29568)\n",
      "Epoch: 74 | Batch_idx: 240 |  Loss_1: (0.1911) | Acc_1: (93.13%) (28730/30848)\n",
      "Epoch: 74 | Batch_idx: 250 |  Loss_1: (0.1917) | Acc_1: (93.10%) (29912/32128)\n",
      "Epoch: 74 | Batch_idx: 260 |  Loss_1: (0.1937) | Acc_1: (93.07%) (31092/33408)\n",
      "Epoch: 74 | Batch_idx: 270 |  Loss_1: (0.1947) | Acc_1: (93.01%) (32265/34688)\n",
      "Epoch: 74 | Batch_idx: 280 |  Loss_1: (0.1949) | Acc_1: (93.02%) (33459/35968)\n",
      "Epoch: 74 | Batch_idx: 290 |  Loss_1: (0.1953) | Acc_1: (93.00%) (34640/37248)\n",
      "Epoch: 74 | Batch_idx: 300 |  Loss_1: (0.1958) | Acc_1: (93.00%) (35830/38528)\n",
      "Epoch: 74 | Batch_idx: 310 |  Loss_1: (0.1957) | Acc_1: (92.99%) (37018/39808)\n",
      "Epoch: 74 | Batch_idx: 320 |  Loss_1: (0.1957) | Acc_1: (92.99%) (38206/41088)\n",
      "Epoch: 74 | Batch_idx: 330 |  Loss_1: (0.1946) | Acc_1: (93.02%) (39410/42368)\n",
      "Epoch: 74 | Batch_idx: 340 |  Loss_1: (0.1945) | Acc_1: (93.00%) (40591/43648)\n",
      "Epoch: 74 | Batch_idx: 350 |  Loss_1: (0.1956) | Acc_1: (92.97%) (41770/44928)\n",
      "Epoch: 74 | Batch_idx: 360 |  Loss_1: (0.1949) | Acc_1: (92.98%) (42966/46208)\n",
      "Epoch: 74 | Batch_idx: 370 |  Loss_1: (0.1954) | Acc_1: (92.97%) (44149/47488)\n",
      "Epoch: 74 | Batch_idx: 380 |  Loss_1: (0.1957) | Acc_1: (92.98%) (45343/48768)\n",
      "Epoch: 74 | Batch_idx: 390 |  Loss_1: (0.1954) | Acc_1: (92.99%) (46495/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3146) | Acc: (90.90%) (9090/10000)\n",
      "Epoch: 75 | Batch_idx: 0 |  Loss_1: (0.2414) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 75 | Batch_idx: 10 |  Loss_1: (0.2005) | Acc_1: (91.83%) (1293/1408)\n",
      "Epoch: 75 | Batch_idx: 20 |  Loss_1: (0.1868) | Acc_1: (92.56%) (2488/2688)\n",
      "Epoch: 75 | Batch_idx: 30 |  Loss_1: (0.1885) | Acc_1: (92.74%) (3680/3968)\n",
      "Epoch: 75 | Batch_idx: 40 |  Loss_1: (0.1865) | Acc_1: (93.03%) (4882/5248)\n",
      "Epoch: 75 | Batch_idx: 50 |  Loss_1: (0.1883) | Acc_1: (92.97%) (6069/6528)\n",
      "Epoch: 75 | Batch_idx: 60 |  Loss_1: (0.1867) | Acc_1: (93.03%) (7264/7808)\n",
      "Epoch: 75 | Batch_idx: 70 |  Loss_1: (0.1911) | Acc_1: (92.91%) (8444/9088)\n",
      "Epoch: 75 | Batch_idx: 80 |  Loss_1: (0.1916) | Acc_1: (92.96%) (9638/10368)\n",
      "Epoch: 75 | Batch_idx: 90 |  Loss_1: (0.1925) | Acc_1: (92.87%) (10818/11648)\n",
      "Epoch: 75 | Batch_idx: 100 |  Loss_1: (0.1942) | Acc_1: (92.89%) (12009/12928)\n",
      "Epoch: 75 | Batch_idx: 110 |  Loss_1: (0.1968) | Acc_1: (92.88%) (13197/14208)\n",
      "Epoch: 75 | Batch_idx: 120 |  Loss_1: (0.1997) | Acc_1: (92.77%) (14368/15488)\n",
      "Epoch: 75 | Batch_idx: 130 |  Loss_1: (0.2001) | Acc_1: (92.74%) (15550/16768)\n",
      "Epoch: 75 | Batch_idx: 140 |  Loss_1: (0.2001) | Acc_1: (92.76%) (16741/18048)\n",
      "Epoch: 75 | Batch_idx: 150 |  Loss_1: (0.1998) | Acc_1: (92.79%) (17935/19328)\n",
      "Epoch: 75 | Batch_idx: 160 |  Loss_1: (0.1991) | Acc_1: (92.82%) (19128/20608)\n",
      "Epoch: 75 | Batch_idx: 170 |  Loss_1: (0.1989) | Acc_1: (92.79%) (20309/21888)\n",
      "Epoch: 75 | Batch_idx: 180 |  Loss_1: (0.1994) | Acc_1: (92.76%) (21491/23168)\n",
      "Epoch: 75 | Batch_idx: 190 |  Loss_1: (0.2007) | Acc_1: (92.69%) (22662/24448)\n",
      "Epoch: 75 | Batch_idx: 200 |  Loss_1: (0.2005) | Acc_1: (92.75%) (23862/25728)\n",
      "Epoch: 75 | Batch_idx: 210 |  Loss_1: (0.2025) | Acc_1: (92.68%) (25031/27008)\n",
      "Epoch: 75 | Batch_idx: 220 |  Loss_1: (0.2024) | Acc_1: (92.70%) (26222/28288)\n",
      "Epoch: 75 | Batch_idx: 230 |  Loss_1: (0.2015) | Acc_1: (92.73%) (27418/29568)\n",
      "Epoch: 75 | Batch_idx: 240 |  Loss_1: (0.2022) | Acc_1: (92.71%) (28599/30848)\n",
      "Epoch: 75 | Batch_idx: 250 |  Loss_1: (0.2011) | Acc_1: (92.74%) (29797/32128)\n",
      "Epoch: 75 | Batch_idx: 260 |  Loss_1: (0.2015) | Acc_1: (92.74%) (30982/33408)\n",
      "Epoch: 75 | Batch_idx: 270 |  Loss_1: (0.2013) | Acc_1: (92.74%) (32169/34688)\n",
      "Epoch: 75 | Batch_idx: 280 |  Loss_1: (0.2005) | Acc_1: (92.77%) (33366/35968)\n",
      "Epoch: 75 | Batch_idx: 290 |  Loss_1: (0.1999) | Acc_1: (92.79%) (34564/37248)\n",
      "Epoch: 75 | Batch_idx: 300 |  Loss_1: (0.1994) | Acc_1: (92.82%) (35760/38528)\n",
      "Epoch: 75 | Batch_idx: 310 |  Loss_1: (0.1995) | Acc_1: (92.82%) (36948/39808)\n",
      "Epoch: 75 | Batch_idx: 320 |  Loss_1: (0.1989) | Acc_1: (92.83%) (38141/41088)\n",
      "Epoch: 75 | Batch_idx: 330 |  Loss_1: (0.1989) | Acc_1: (92.82%) (39328/42368)\n",
      "Epoch: 75 | Batch_idx: 340 |  Loss_1: (0.1993) | Acc_1: (92.84%) (40522/43648)\n",
      "Epoch: 75 | Batch_idx: 350 |  Loss_1: (0.1990) | Acc_1: (92.83%) (41706/44928)\n",
      "Epoch: 75 | Batch_idx: 360 |  Loss_1: (0.1991) | Acc_1: (92.82%) (42892/46208)\n",
      "Epoch: 75 | Batch_idx: 370 |  Loss_1: (0.1996) | Acc_1: (92.81%) (44074/47488)\n",
      "Epoch: 75 | Batch_idx: 380 |  Loss_1: (0.2002) | Acc_1: (92.78%) (45248/48768)\n",
      "Epoch: 75 | Batch_idx: 390 |  Loss_1: (0.1998) | Acc_1: (92.78%) (46391/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3083) | Acc: (91.19%) (9119/10000)\n",
      "Epoch: 76 | Batch_idx: 0 |  Loss_1: (0.1391) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 76 | Batch_idx: 10 |  Loss_1: (0.1930) | Acc_1: (92.61%) (1304/1408)\n",
      "Epoch: 76 | Batch_idx: 20 |  Loss_1: (0.2050) | Acc_1: (92.34%) (2482/2688)\n",
      "Epoch: 76 | Batch_idx: 30 |  Loss_1: (0.2160) | Acc_1: (92.01%) (3651/3968)\n",
      "Epoch: 76 | Batch_idx: 40 |  Loss_1: (0.2110) | Acc_1: (92.15%) (4836/5248)\n",
      "Epoch: 76 | Batch_idx: 50 |  Loss_1: (0.2077) | Acc_1: (92.36%) (6029/6528)\n",
      "Epoch: 76 | Batch_idx: 60 |  Loss_1: (0.2024) | Acc_1: (92.60%) (7230/7808)\n",
      "Epoch: 76 | Batch_idx: 70 |  Loss_1: (0.2088) | Acc_1: (92.50%) (8406/9088)\n",
      "Epoch: 76 | Batch_idx: 80 |  Loss_1: (0.2039) | Acc_1: (92.72%) (9613/10368)\n",
      "Epoch: 76 | Batch_idx: 90 |  Loss_1: (0.1992) | Acc_1: (92.91%) (10822/11648)\n",
      "Epoch: 76 | Batch_idx: 100 |  Loss_1: (0.1964) | Acc_1: (93.02%) (12026/12928)\n",
      "Epoch: 76 | Batch_idx: 110 |  Loss_1: (0.1955) | Acc_1: (93.03%) (13217/14208)\n",
      "Epoch: 76 | Batch_idx: 120 |  Loss_1: (0.1926) | Acc_1: (93.18%) (14431/15488)\n",
      "Epoch: 76 | Batch_idx: 130 |  Loss_1: (0.1921) | Acc_1: (93.19%) (15626/16768)\n",
      "Epoch: 76 | Batch_idx: 140 |  Loss_1: (0.1922) | Acc_1: (93.16%) (16814/18048)\n",
      "Epoch: 76 | Batch_idx: 150 |  Loss_1: (0.1929) | Acc_1: (93.09%) (17992/19328)\n",
      "Epoch: 76 | Batch_idx: 160 |  Loss_1: (0.1912) | Acc_1: (93.14%) (19195/20608)\n",
      "Epoch: 76 | Batch_idx: 170 |  Loss_1: (0.1908) | Acc_1: (93.15%) (20389/21888)\n",
      "Epoch: 76 | Batch_idx: 180 |  Loss_1: (0.1901) | Acc_1: (93.17%) (21586/23168)\n",
      "Epoch: 76 | Batch_idx: 190 |  Loss_1: (0.1897) | Acc_1: (93.17%) (22778/24448)\n",
      "Epoch: 76 | Batch_idx: 200 |  Loss_1: (0.1905) | Acc_1: (93.15%) (23965/25728)\n",
      "Epoch: 76 | Batch_idx: 210 |  Loss_1: (0.1897) | Acc_1: (93.17%) (25164/27008)\n",
      "Epoch: 76 | Batch_idx: 220 |  Loss_1: (0.1908) | Acc_1: (93.12%) (26342/28288)\n",
      "Epoch: 76 | Batch_idx: 230 |  Loss_1: (0.1901) | Acc_1: (93.14%) (27540/29568)\n",
      "Epoch: 76 | Batch_idx: 240 |  Loss_1: (0.1903) | Acc_1: (93.14%) (28733/30848)\n",
      "Epoch: 76 | Batch_idx: 250 |  Loss_1: (0.1890) | Acc_1: (93.17%) (29935/32128)\n",
      "Epoch: 76 | Batch_idx: 260 |  Loss_1: (0.1902) | Acc_1: (93.12%) (31108/33408)\n",
      "Epoch: 76 | Batch_idx: 270 |  Loss_1: (0.1906) | Acc_1: (93.09%) (32292/34688)\n",
      "Epoch: 76 | Batch_idx: 280 |  Loss_1: (0.1902) | Acc_1: (93.10%) (33485/35968)\n",
      "Epoch: 76 | Batch_idx: 290 |  Loss_1: (0.1898) | Acc_1: (93.13%) (34688/37248)\n",
      "Epoch: 76 | Batch_idx: 300 |  Loss_1: (0.1904) | Acc_1: (93.13%) (35881/38528)\n",
      "Epoch: 76 | Batch_idx: 310 |  Loss_1: (0.1905) | Acc_1: (93.12%) (37068/39808)\n",
      "Epoch: 76 | Batch_idx: 320 |  Loss_1: (0.1900) | Acc_1: (93.13%) (38265/41088)\n",
      "Epoch: 76 | Batch_idx: 330 |  Loss_1: (0.1906) | Acc_1: (93.11%) (39448/42368)\n",
      "Epoch: 76 | Batch_idx: 340 |  Loss_1: (0.1907) | Acc_1: (93.12%) (40643/43648)\n",
      "Epoch: 76 | Batch_idx: 350 |  Loss_1: (0.1915) | Acc_1: (93.08%) (41820/44928)\n",
      "Epoch: 76 | Batch_idx: 360 |  Loss_1: (0.1925) | Acc_1: (93.05%) (42998/46208)\n",
      "Epoch: 76 | Batch_idx: 370 |  Loss_1: (0.1940) | Acc_1: (92.99%) (44159/47488)\n",
      "Epoch: 76 | Batch_idx: 380 |  Loss_1: (0.1951) | Acc_1: (92.94%) (45326/48768)\n",
      "Epoch: 76 | Batch_idx: 390 |  Loss_1: (0.1961) | Acc_1: (92.93%) (46465/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3872) | Acc: (89.25%) (8925/10000)\n",
      "Epoch: 77 | Batch_idx: 0 |  Loss_1: (0.1911) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 77 | Batch_idx: 10 |  Loss_1: (0.1837) | Acc_1: (93.61%) (1318/1408)\n",
      "Epoch: 77 | Batch_idx: 20 |  Loss_1: (0.1919) | Acc_1: (93.12%) (2503/2688)\n",
      "Epoch: 77 | Batch_idx: 30 |  Loss_1: (0.1912) | Acc_1: (93.22%) (3699/3968)\n",
      "Epoch: 77 | Batch_idx: 40 |  Loss_1: (0.1959) | Acc_1: (92.93%) (4877/5248)\n",
      "Epoch: 77 | Batch_idx: 50 |  Loss_1: (0.1923) | Acc_1: (93.17%) (6082/6528)\n",
      "Epoch: 77 | Batch_idx: 60 |  Loss_1: (0.1919) | Acc_1: (93.24%) (7280/7808)\n",
      "Epoch: 77 | Batch_idx: 70 |  Loss_1: (0.1890) | Acc_1: (93.30%) (8479/9088)\n",
      "Epoch: 77 | Batch_idx: 80 |  Loss_1: (0.1883) | Acc_1: (93.29%) (9672/10368)\n",
      "Epoch: 77 | Batch_idx: 90 |  Loss_1: (0.1905) | Acc_1: (93.18%) (10854/11648)\n",
      "Epoch: 77 | Batch_idx: 100 |  Loss_1: (0.1931) | Acc_1: (93.13%) (12040/12928)\n",
      "Epoch: 77 | Batch_idx: 110 |  Loss_1: (0.1935) | Acc_1: (93.16%) (13236/14208)\n",
      "Epoch: 77 | Batch_idx: 120 |  Loss_1: (0.1919) | Acc_1: (93.15%) (14427/15488)\n",
      "Epoch: 77 | Batch_idx: 130 |  Loss_1: (0.1937) | Acc_1: (93.07%) (15606/16768)\n",
      "Epoch: 77 | Batch_idx: 140 |  Loss_1: (0.1929) | Acc_1: (93.10%) (16802/18048)\n",
      "Epoch: 77 | Batch_idx: 150 |  Loss_1: (0.1942) | Acc_1: (93.02%) (17978/19328)\n",
      "Epoch: 77 | Batch_idx: 160 |  Loss_1: (0.1965) | Acc_1: (92.93%) (19152/20608)\n",
      "Epoch: 77 | Batch_idx: 170 |  Loss_1: (0.1976) | Acc_1: (92.90%) (20335/21888)\n",
      "Epoch: 77 | Batch_idx: 180 |  Loss_1: (0.1957) | Acc_1: (92.94%) (21533/23168)\n",
      "Epoch: 77 | Batch_idx: 190 |  Loss_1: (0.1958) | Acc_1: (92.91%) (22714/24448)\n",
      "Epoch: 77 | Batch_idx: 200 |  Loss_1: (0.1974) | Acc_1: (92.85%) (23889/25728)\n",
      "Epoch: 77 | Batch_idx: 210 |  Loss_1: (0.1968) | Acc_1: (92.87%) (25082/27008)\n",
      "Epoch: 77 | Batch_idx: 220 |  Loss_1: (0.1966) | Acc_1: (92.83%) (26261/28288)\n",
      "Epoch: 77 | Batch_idx: 230 |  Loss_1: (0.1965) | Acc_1: (92.83%) (27449/29568)\n",
      "Epoch: 77 | Batch_idx: 240 |  Loss_1: (0.1959) | Acc_1: (92.87%) (28648/30848)\n",
      "Epoch: 77 | Batch_idx: 250 |  Loss_1: (0.1969) | Acc_1: (92.83%) (29825/32128)\n",
      "Epoch: 77 | Batch_idx: 260 |  Loss_1: (0.1967) | Acc_1: (92.85%) (31018/33408)\n",
      "Epoch: 77 | Batch_idx: 270 |  Loss_1: (0.1982) | Acc_1: (92.78%) (32182/34688)\n",
      "Epoch: 77 | Batch_idx: 280 |  Loss_1: (0.1981) | Acc_1: (92.79%) (33374/35968)\n",
      "Epoch: 77 | Batch_idx: 290 |  Loss_1: (0.1992) | Acc_1: (92.74%) (34543/37248)\n",
      "Epoch: 77 | Batch_idx: 300 |  Loss_1: (0.1987) | Acc_1: (92.76%) (35738/38528)\n",
      "Epoch: 77 | Batch_idx: 310 |  Loss_1: (0.1995) | Acc_1: (92.74%) (36916/39808)\n",
      "Epoch: 77 | Batch_idx: 320 |  Loss_1: (0.1999) | Acc_1: (92.72%) (38098/41088)\n",
      "Epoch: 77 | Batch_idx: 330 |  Loss_1: (0.2002) | Acc_1: (92.69%) (39271/42368)\n",
      "Epoch: 77 | Batch_idx: 340 |  Loss_1: (0.2005) | Acc_1: (92.69%) (40457/43648)\n",
      "Epoch: 77 | Batch_idx: 350 |  Loss_1: (0.2008) | Acc_1: (92.67%) (41635/44928)\n",
      "Epoch: 77 | Batch_idx: 360 |  Loss_1: (0.2006) | Acc_1: (92.68%) (42826/46208)\n",
      "Epoch: 77 | Batch_idx: 370 |  Loss_1: (0.2009) | Acc_1: (92.66%) (44003/47488)\n",
      "Epoch: 77 | Batch_idx: 380 |  Loss_1: (0.2018) | Acc_1: (92.62%) (45170/48768)\n",
      "Epoch: 77 | Batch_idx: 390 |  Loss_1: (0.2024) | Acc_1: (92.61%) (46303/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3020) | Acc: (90.85%) (9085/10000)\n",
      "Epoch: 78 | Batch_idx: 0 |  Loss_1: (0.1927) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 78 | Batch_idx: 10 |  Loss_1: (0.1974) | Acc_1: (92.61%) (1304/1408)\n",
      "Epoch: 78 | Batch_idx: 20 |  Loss_1: (0.2150) | Acc_1: (92.37%) (2483/2688)\n",
      "Epoch: 78 | Batch_idx: 30 |  Loss_1: (0.2083) | Acc_1: (92.44%) (3668/3968)\n",
      "Epoch: 78 | Batch_idx: 40 |  Loss_1: (0.2068) | Acc_1: (92.61%) (4860/5248)\n",
      "Epoch: 78 | Batch_idx: 50 |  Loss_1: (0.2011) | Acc_1: (92.77%) (6056/6528)\n",
      "Epoch: 78 | Batch_idx: 60 |  Loss_1: (0.2026) | Acc_1: (92.74%) (7241/7808)\n",
      "Epoch: 78 | Batch_idx: 70 |  Loss_1: (0.2012) | Acc_1: (92.66%) (8421/9088)\n",
      "Epoch: 78 | Batch_idx: 80 |  Loss_1: (0.2029) | Acc_1: (92.53%) (9593/10368)\n",
      "Epoch: 78 | Batch_idx: 90 |  Loss_1: (0.2020) | Acc_1: (92.55%) (10780/11648)\n",
      "Epoch: 78 | Batch_idx: 100 |  Loss_1: (0.2008) | Acc_1: (92.64%) (11976/12928)\n",
      "Epoch: 78 | Batch_idx: 110 |  Loss_1: (0.1993) | Acc_1: (92.67%) (13166/14208)\n",
      "Epoch: 78 | Batch_idx: 120 |  Loss_1: (0.1976) | Acc_1: (92.72%) (14360/15488)\n",
      "Epoch: 78 | Batch_idx: 130 |  Loss_1: (0.1981) | Acc_1: (92.67%) (15539/16768)\n",
      "Epoch: 78 | Batch_idx: 140 |  Loss_1: (0.1988) | Acc_1: (92.66%) (16724/18048)\n",
      "Epoch: 78 | Batch_idx: 150 |  Loss_1: (0.1991) | Acc_1: (92.66%) (17910/19328)\n",
      "Epoch: 78 | Batch_idx: 160 |  Loss_1: (0.1983) | Acc_1: (92.66%) (19095/20608)\n",
      "Epoch: 78 | Batch_idx: 170 |  Loss_1: (0.1994) | Acc_1: (92.60%) (20268/21888)\n",
      "Epoch: 78 | Batch_idx: 180 |  Loss_1: (0.2002) | Acc_1: (92.56%) (21445/23168)\n",
      "Epoch: 78 | Batch_idx: 190 |  Loss_1: (0.2006) | Acc_1: (92.56%) (22630/24448)\n",
      "Epoch: 78 | Batch_idx: 200 |  Loss_1: (0.1998) | Acc_1: (92.59%) (23822/25728)\n",
      "Epoch: 78 | Batch_idx: 210 |  Loss_1: (0.1994) | Acc_1: (92.59%) (25008/27008)\n",
      "Epoch: 78 | Batch_idx: 220 |  Loss_1: (0.1986) | Acc_1: (92.62%) (26200/28288)\n",
      "Epoch: 78 | Batch_idx: 230 |  Loss_1: (0.2004) | Acc_1: (92.57%) (27372/29568)\n",
      "Epoch: 78 | Batch_idx: 240 |  Loss_1: (0.1997) | Acc_1: (92.63%) (28573/30848)\n",
      "Epoch: 78 | Batch_idx: 250 |  Loss_1: (0.1990) | Acc_1: (92.66%) (29771/32128)\n",
      "Epoch: 78 | Batch_idx: 260 |  Loss_1: (0.1987) | Acc_1: (92.67%) (30960/33408)\n",
      "Epoch: 78 | Batch_idx: 270 |  Loss_1: (0.1990) | Acc_1: (92.67%) (32144/34688)\n",
      "Epoch: 78 | Batch_idx: 280 |  Loss_1: (0.1991) | Acc_1: (92.66%) (33328/35968)\n",
      "Epoch: 78 | Batch_idx: 290 |  Loss_1: (0.2000) | Acc_1: (92.64%) (34505/37248)\n",
      "Epoch: 78 | Batch_idx: 300 |  Loss_1: (0.2001) | Acc_1: (92.61%) (35682/38528)\n",
      "Epoch: 78 | Batch_idx: 310 |  Loss_1: (0.2011) | Acc_1: (92.59%) (36859/39808)\n",
      "Epoch: 78 | Batch_idx: 320 |  Loss_1: (0.2018) | Acc_1: (92.59%) (38043/41088)\n",
      "Epoch: 78 | Batch_idx: 330 |  Loss_1: (0.2025) | Acc_1: (92.56%) (39217/42368)\n",
      "Epoch: 78 | Batch_idx: 340 |  Loss_1: (0.2022) | Acc_1: (92.58%) (40408/43648)\n",
      "Epoch: 78 | Batch_idx: 350 |  Loss_1: (0.2019) | Acc_1: (92.57%) (41592/44928)\n",
      "Epoch: 78 | Batch_idx: 360 |  Loss_1: (0.2020) | Acc_1: (92.59%) (42782/46208)\n",
      "Epoch: 78 | Batch_idx: 370 |  Loss_1: (0.2016) | Acc_1: (92.59%) (43968/47488)\n",
      "Epoch: 78 | Batch_idx: 380 |  Loss_1: (0.2009) | Acc_1: (92.62%) (45167/48768)\n",
      "Epoch: 78 | Batch_idx: 390 |  Loss_1: (0.2009) | Acc_1: (92.63%) (46314/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2906) | Acc: (91.44%) (9144/10000)\n",
      "Epoch: 79 | Batch_idx: 0 |  Loss_1: (0.1363) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 79 | Batch_idx: 10 |  Loss_1: (0.1781) | Acc_1: (93.25%) (1313/1408)\n",
      "Epoch: 79 | Batch_idx: 20 |  Loss_1: (0.1873) | Acc_1: (93.45%) (2512/2688)\n",
      "Epoch: 79 | Batch_idx: 30 |  Loss_1: (0.1795) | Acc_1: (93.70%) (3718/3968)\n",
      "Epoch: 79 | Batch_idx: 40 |  Loss_1: (0.1770) | Acc_1: (93.71%) (4918/5248)\n",
      "Epoch: 79 | Batch_idx: 50 |  Loss_1: (0.1808) | Acc_1: (93.60%) (6110/6528)\n",
      "Epoch: 79 | Batch_idx: 60 |  Loss_1: (0.1769) | Acc_1: (93.72%) (7318/7808)\n",
      "Epoch: 79 | Batch_idx: 70 |  Loss_1: (0.1803) | Acc_1: (93.57%) (8504/9088)\n",
      "Epoch: 79 | Batch_idx: 80 |  Loss_1: (0.1789) | Acc_1: (93.56%) (9700/10368)\n",
      "Epoch: 79 | Batch_idx: 90 |  Loss_1: (0.1809) | Acc_1: (93.51%) (10892/11648)\n",
      "Epoch: 79 | Batch_idx: 100 |  Loss_1: (0.1835) | Acc_1: (93.39%) (12074/12928)\n",
      "Epoch: 79 | Batch_idx: 110 |  Loss_1: (0.1859) | Acc_1: (93.33%) (13261/14208)\n",
      "Epoch: 79 | Batch_idx: 120 |  Loss_1: (0.1859) | Acc_1: (93.33%) (14455/15488)\n",
      "Epoch: 79 | Batch_idx: 130 |  Loss_1: (0.1865) | Acc_1: (93.31%) (15646/16768)\n",
      "Epoch: 79 | Batch_idx: 140 |  Loss_1: (0.1862) | Acc_1: (93.31%) (16841/18048)\n",
      "Epoch: 79 | Batch_idx: 150 |  Loss_1: (0.1863) | Acc_1: (93.28%) (18030/19328)\n",
      "Epoch: 79 | Batch_idx: 160 |  Loss_1: (0.1857) | Acc_1: (93.29%) (19225/20608)\n",
      "Epoch: 79 | Batch_idx: 170 |  Loss_1: (0.1874) | Acc_1: (93.19%) (20398/21888)\n",
      "Epoch: 79 | Batch_idx: 180 |  Loss_1: (0.1896) | Acc_1: (93.10%) (21570/23168)\n",
      "Epoch: 79 | Batch_idx: 190 |  Loss_1: (0.1900) | Acc_1: (93.09%) (22759/24448)\n",
      "Epoch: 79 | Batch_idx: 200 |  Loss_1: (0.1913) | Acc_1: (93.04%) (23937/25728)\n",
      "Epoch: 79 | Batch_idx: 210 |  Loss_1: (0.1913) | Acc_1: (93.04%) (25129/27008)\n",
      "Epoch: 79 | Batch_idx: 220 |  Loss_1: (0.1905) | Acc_1: (93.09%) (26333/28288)\n",
      "Epoch: 79 | Batch_idx: 230 |  Loss_1: (0.1902) | Acc_1: (93.09%) (27525/29568)\n",
      "Epoch: 79 | Batch_idx: 240 |  Loss_1: (0.1906) | Acc_1: (93.07%) (28710/30848)\n",
      "Epoch: 79 | Batch_idx: 250 |  Loss_1: (0.1907) | Acc_1: (93.07%) (29900/32128)\n",
      "Epoch: 79 | Batch_idx: 260 |  Loss_1: (0.1903) | Acc_1: (93.09%) (31101/33408)\n",
      "Epoch: 79 | Batch_idx: 270 |  Loss_1: (0.1910) | Acc_1: (93.09%) (32291/34688)\n",
      "Epoch: 79 | Batch_idx: 280 |  Loss_1: (0.1901) | Acc_1: (93.12%) (33495/35968)\n",
      "Epoch: 79 | Batch_idx: 290 |  Loss_1: (0.1905) | Acc_1: (93.11%) (34680/37248)\n",
      "Epoch: 79 | Batch_idx: 300 |  Loss_1: (0.1908) | Acc_1: (93.09%) (35866/38528)\n",
      "Epoch: 79 | Batch_idx: 310 |  Loss_1: (0.1917) | Acc_1: (93.06%) (37046/39808)\n",
      "Epoch: 79 | Batch_idx: 320 |  Loss_1: (0.1920) | Acc_1: (93.03%) (38226/41088)\n",
      "Epoch: 79 | Batch_idx: 330 |  Loss_1: (0.1924) | Acc_1: (93.03%) (39413/42368)\n",
      "Epoch: 79 | Batch_idx: 340 |  Loss_1: (0.1922) | Acc_1: (93.04%) (40611/43648)\n",
      "Epoch: 79 | Batch_idx: 350 |  Loss_1: (0.1924) | Acc_1: (93.04%) (41799/44928)\n",
      "Epoch: 79 | Batch_idx: 360 |  Loss_1: (0.1924) | Acc_1: (93.04%) (42993/46208)\n",
      "Epoch: 79 | Batch_idx: 370 |  Loss_1: (0.1922) | Acc_1: (93.05%) (44186/47488)\n",
      "Epoch: 79 | Batch_idx: 380 |  Loss_1: (0.1915) | Acc_1: (93.05%) (45380/48768)\n",
      "Epoch: 79 | Batch_idx: 390 |  Loss_1: (0.1923) | Acc_1: (93.04%) (46520/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3154) | Acc: (91.27%) (9127/10000)\n",
      "Epoch: 80 | Batch_idx: 0 |  Loss_1: (0.2747) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 80 | Batch_idx: 10 |  Loss_1: (0.2028) | Acc_1: (92.40%) (1301/1408)\n",
      "Epoch: 80 | Batch_idx: 20 |  Loss_1: (0.1941) | Acc_1: (92.71%) (2492/2688)\n",
      "Epoch: 80 | Batch_idx: 30 |  Loss_1: (0.2013) | Acc_1: (92.59%) (3674/3968)\n",
      "Epoch: 80 | Batch_idx: 40 |  Loss_1: (0.1992) | Acc_1: (92.70%) (4865/5248)\n",
      "Epoch: 80 | Batch_idx: 50 |  Loss_1: (0.1970) | Acc_1: (92.78%) (6057/6528)\n",
      "Epoch: 80 | Batch_idx: 60 |  Loss_1: (0.1935) | Acc_1: (92.99%) (7261/7808)\n",
      "Epoch: 80 | Batch_idx: 70 |  Loss_1: (0.1873) | Acc_1: (93.18%) (8468/9088)\n",
      "Epoch: 80 | Batch_idx: 80 |  Loss_1: (0.1865) | Acc_1: (93.25%) (9668/10368)\n",
      "Epoch: 80 | Batch_idx: 90 |  Loss_1: (0.1889) | Acc_1: (93.21%) (10857/11648)\n",
      "Epoch: 80 | Batch_idx: 100 |  Loss_1: (0.1896) | Acc_1: (93.16%) (12044/12928)\n",
      "Epoch: 80 | Batch_idx: 110 |  Loss_1: (0.1906) | Acc_1: (93.06%) (13222/14208)\n",
      "Epoch: 80 | Batch_idx: 120 |  Loss_1: (0.1941) | Acc_1: (92.93%) (14393/15488)\n",
      "Epoch: 80 | Batch_idx: 130 |  Loss_1: (0.1941) | Acc_1: (92.92%) (15581/16768)\n",
      "Epoch: 80 | Batch_idx: 140 |  Loss_1: (0.1950) | Acc_1: (92.90%) (16766/18048)\n",
      "Epoch: 80 | Batch_idx: 150 |  Loss_1: (0.1942) | Acc_1: (92.94%) (17964/19328)\n",
      "Epoch: 80 | Batch_idx: 160 |  Loss_1: (0.1938) | Acc_1: (92.96%) (19157/20608)\n",
      "Epoch: 80 | Batch_idx: 170 |  Loss_1: (0.1940) | Acc_1: (92.93%) (20340/21888)\n",
      "Epoch: 80 | Batch_idx: 180 |  Loss_1: (0.1939) | Acc_1: (92.93%) (21529/23168)\n",
      "Epoch: 80 | Batch_idx: 190 |  Loss_1: (0.1932) | Acc_1: (92.96%) (22726/24448)\n",
      "Epoch: 80 | Batch_idx: 200 |  Loss_1: (0.1916) | Acc_1: (93.03%) (23934/25728)\n",
      "Epoch: 80 | Batch_idx: 210 |  Loss_1: (0.1913) | Acc_1: (93.02%) (25123/27008)\n",
      "Epoch: 80 | Batch_idx: 220 |  Loss_1: (0.1921) | Acc_1: (93.02%) (26313/28288)\n",
      "Epoch: 80 | Batch_idx: 230 |  Loss_1: (0.1910) | Acc_1: (93.05%) (27513/29568)\n",
      "Epoch: 80 | Batch_idx: 240 |  Loss_1: (0.1930) | Acc_1: (92.97%) (28680/30848)\n",
      "Epoch: 80 | Batch_idx: 250 |  Loss_1: (0.1933) | Acc_1: (92.95%) (29863/32128)\n",
      "Epoch: 80 | Batch_idx: 260 |  Loss_1: (0.1939) | Acc_1: (92.95%) (31053/33408)\n",
      "Epoch: 80 | Batch_idx: 270 |  Loss_1: (0.1929) | Acc_1: (93.00%) (32261/34688)\n",
      "Epoch: 80 | Batch_idx: 280 |  Loss_1: (0.1935) | Acc_1: (92.99%) (33446/35968)\n",
      "Epoch: 80 | Batch_idx: 290 |  Loss_1: (0.1939) | Acc_1: (92.98%) (34632/37248)\n",
      "Epoch: 80 | Batch_idx: 300 |  Loss_1: (0.1946) | Acc_1: (92.96%) (35817/38528)\n",
      "Epoch: 80 | Batch_idx: 310 |  Loss_1: (0.1950) | Acc_1: (92.95%) (37001/39808)\n",
      "Epoch: 80 | Batch_idx: 320 |  Loss_1: (0.1949) | Acc_1: (92.95%) (38191/41088)\n",
      "Epoch: 80 | Batch_idx: 330 |  Loss_1: (0.1948) | Acc_1: (92.97%) (39389/42368)\n",
      "Epoch: 80 | Batch_idx: 340 |  Loss_1: (0.1949) | Acc_1: (92.96%) (40573/43648)\n",
      "Epoch: 80 | Batch_idx: 350 |  Loss_1: (0.1955) | Acc_1: (92.94%) (41754/44928)\n",
      "Epoch: 80 | Batch_idx: 360 |  Loss_1: (0.1948) | Acc_1: (92.96%) (42956/46208)\n",
      "Epoch: 80 | Batch_idx: 370 |  Loss_1: (0.1948) | Acc_1: (92.98%) (44156/47488)\n",
      "Epoch: 80 | Batch_idx: 380 |  Loss_1: (0.1952) | Acc_1: (92.98%) (45346/48768)\n",
      "Epoch: 80 | Batch_idx: 390 |  Loss_1: (0.1951) | Acc_1: (92.98%) (46492/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3375) | Acc: (90.80%) (9080/10000)\n",
      "Epoch: 81 | Batch_idx: 0 |  Loss_1: (0.2380) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 81 | Batch_idx: 10 |  Loss_1: (0.2093) | Acc_1: (92.83%) (1307/1408)\n",
      "Epoch: 81 | Batch_idx: 20 |  Loss_1: (0.1992) | Acc_1: (92.86%) (2496/2688)\n",
      "Epoch: 81 | Batch_idx: 30 |  Loss_1: (0.1973) | Acc_1: (92.99%) (3690/3968)\n",
      "Epoch: 81 | Batch_idx: 40 |  Loss_1: (0.2045) | Acc_1: (92.61%) (4860/5248)\n",
      "Epoch: 81 | Batch_idx: 50 |  Loss_1: (0.1987) | Acc_1: (92.82%) (6059/6528)\n",
      "Epoch: 81 | Batch_idx: 60 |  Loss_1: (0.1952) | Acc_1: (92.85%) (7250/7808)\n",
      "Epoch: 81 | Batch_idx: 70 |  Loss_1: (0.1963) | Acc_1: (92.87%) (8440/9088)\n",
      "Epoch: 81 | Batch_idx: 80 |  Loss_1: (0.1944) | Acc_1: (93.03%) (9645/10368)\n",
      "Epoch: 81 | Batch_idx: 90 |  Loss_1: (0.1948) | Acc_1: (92.99%) (10832/11648)\n",
      "Epoch: 81 | Batch_idx: 100 |  Loss_1: (0.1928) | Acc_1: (93.06%) (12031/12928)\n",
      "Epoch: 81 | Batch_idx: 110 |  Loss_1: (0.1894) | Acc_1: (93.17%) (13238/14208)\n",
      "Epoch: 81 | Batch_idx: 120 |  Loss_1: (0.1898) | Acc_1: (93.15%) (14427/15488)\n",
      "Epoch: 81 | Batch_idx: 130 |  Loss_1: (0.1882) | Acc_1: (93.23%) (15633/16768)\n",
      "Epoch: 81 | Batch_idx: 140 |  Loss_1: (0.1885) | Acc_1: (93.22%) (16824/18048)\n",
      "Epoch: 81 | Batch_idx: 150 |  Loss_1: (0.1871) | Acc_1: (93.27%) (18028/19328)\n",
      "Epoch: 81 | Batch_idx: 160 |  Loss_1: (0.1871) | Acc_1: (93.26%) (19220/20608)\n",
      "Epoch: 81 | Batch_idx: 170 |  Loss_1: (0.1884) | Acc_1: (93.20%) (20400/21888)\n",
      "Epoch: 81 | Batch_idx: 180 |  Loss_1: (0.1886) | Acc_1: (93.17%) (21586/23168)\n",
      "Epoch: 81 | Batch_idx: 190 |  Loss_1: (0.1879) | Acc_1: (93.17%) (22779/24448)\n",
      "Epoch: 81 | Batch_idx: 200 |  Loss_1: (0.1878) | Acc_1: (93.20%) (23978/25728)\n",
      "Epoch: 81 | Batch_idx: 210 |  Loss_1: (0.1891) | Acc_1: (93.15%) (25159/27008)\n",
      "Epoch: 81 | Batch_idx: 220 |  Loss_1: (0.1897) | Acc_1: (93.14%) (26347/28288)\n",
      "Epoch: 81 | Batch_idx: 230 |  Loss_1: (0.1895) | Acc_1: (93.13%) (27536/29568)\n",
      "Epoch: 81 | Batch_idx: 240 |  Loss_1: (0.1901) | Acc_1: (93.11%) (28723/30848)\n",
      "Epoch: 81 | Batch_idx: 250 |  Loss_1: (0.1903) | Acc_1: (93.10%) (29910/32128)\n",
      "Epoch: 81 | Batch_idx: 260 |  Loss_1: (0.1899) | Acc_1: (93.13%) (31113/33408)\n",
      "Epoch: 81 | Batch_idx: 270 |  Loss_1: (0.1906) | Acc_1: (93.12%) (32302/34688)\n",
      "Epoch: 81 | Batch_idx: 280 |  Loss_1: (0.1910) | Acc_1: (93.09%) (33484/35968)\n",
      "Epoch: 81 | Batch_idx: 290 |  Loss_1: (0.1913) | Acc_1: (93.09%) (34673/37248)\n",
      "Epoch: 81 | Batch_idx: 300 |  Loss_1: (0.1920) | Acc_1: (93.03%) (35843/38528)\n",
      "Epoch: 81 | Batch_idx: 310 |  Loss_1: (0.1929) | Acc_1: (92.99%) (37018/39808)\n",
      "Epoch: 81 | Batch_idx: 320 |  Loss_1: (0.1936) | Acc_1: (92.96%) (38197/41088)\n",
      "Epoch: 81 | Batch_idx: 330 |  Loss_1: (0.1944) | Acc_1: (92.96%) (39384/42368)\n",
      "Epoch: 81 | Batch_idx: 340 |  Loss_1: (0.1950) | Acc_1: (92.95%) (40570/43648)\n",
      "Epoch: 81 | Batch_idx: 350 |  Loss_1: (0.1949) | Acc_1: (92.96%) (41767/44928)\n",
      "Epoch: 81 | Batch_idx: 360 |  Loss_1: (0.1949) | Acc_1: (92.95%) (42952/46208)\n",
      "Epoch: 81 | Batch_idx: 370 |  Loss_1: (0.1952) | Acc_1: (92.94%) (44135/47488)\n",
      "Epoch: 81 | Batch_idx: 380 |  Loss_1: (0.1953) | Acc_1: (92.95%) (45330/48768)\n",
      "Epoch: 81 | Batch_idx: 390 |  Loss_1: (0.1961) | Acc_1: (92.91%) (46455/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3137) | Acc: (90.75%) (9075/10000)\n",
      "Epoch: 82 | Batch_idx: 0 |  Loss_1: (0.2141) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 82 | Batch_idx: 10 |  Loss_1: (0.1672) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 82 | Batch_idx: 20 |  Loss_1: (0.1727) | Acc_1: (93.23%) (2506/2688)\n",
      "Epoch: 82 | Batch_idx: 30 |  Loss_1: (0.1880) | Acc_1: (92.97%) (3689/3968)\n",
      "Epoch: 82 | Batch_idx: 40 |  Loss_1: (0.1933) | Acc_1: (92.97%) (4879/5248)\n",
      "Epoch: 82 | Batch_idx: 50 |  Loss_1: (0.1929) | Acc_1: (93.01%) (6072/6528)\n",
      "Epoch: 82 | Batch_idx: 60 |  Loss_1: (0.1925) | Acc_1: (93.03%) (7264/7808)\n",
      "Epoch: 82 | Batch_idx: 70 |  Loss_1: (0.1963) | Acc_1: (92.85%) (8438/9088)\n",
      "Epoch: 82 | Batch_idx: 80 |  Loss_1: (0.1910) | Acc_1: (93.02%) (9644/10368)\n",
      "Epoch: 82 | Batch_idx: 90 |  Loss_1: (0.1903) | Acc_1: (93.01%) (10834/11648)\n",
      "Epoch: 82 | Batch_idx: 100 |  Loss_1: (0.1894) | Acc_1: (93.05%) (12029/12928)\n",
      "Epoch: 82 | Batch_idx: 110 |  Loss_1: (0.1920) | Acc_1: (92.95%) (13206/14208)\n",
      "Epoch: 82 | Batch_idx: 120 |  Loss_1: (0.1921) | Acc_1: (92.97%) (14399/15488)\n",
      "Epoch: 82 | Batch_idx: 130 |  Loss_1: (0.1897) | Acc_1: (93.04%) (15601/16768)\n",
      "Epoch: 82 | Batch_idx: 140 |  Loss_1: (0.1895) | Acc_1: (93.08%) (16799/18048)\n",
      "Epoch: 82 | Batch_idx: 150 |  Loss_1: (0.1889) | Acc_1: (93.10%) (17995/19328)\n",
      "Epoch: 82 | Batch_idx: 160 |  Loss_1: (0.1858) | Acc_1: (93.22%) (19211/20608)\n",
      "Epoch: 82 | Batch_idx: 170 |  Loss_1: (0.1866) | Acc_1: (93.19%) (20398/21888)\n",
      "Epoch: 82 | Batch_idx: 180 |  Loss_1: (0.1845) | Acc_1: (93.26%) (21607/23168)\n",
      "Epoch: 82 | Batch_idx: 190 |  Loss_1: (0.1844) | Acc_1: (93.27%) (22802/24448)\n",
      "Epoch: 82 | Batch_idx: 200 |  Loss_1: (0.1842) | Acc_1: (93.27%) (23997/25728)\n",
      "Epoch: 82 | Batch_idx: 210 |  Loss_1: (0.1832) | Acc_1: (93.29%) (25195/27008)\n",
      "Epoch: 82 | Batch_idx: 220 |  Loss_1: (0.1827) | Acc_1: (93.32%) (26398/28288)\n",
      "Epoch: 82 | Batch_idx: 230 |  Loss_1: (0.1818) | Acc_1: (93.35%) (27601/29568)\n",
      "Epoch: 82 | Batch_idx: 240 |  Loss_1: (0.1824) | Acc_1: (93.32%) (28788/30848)\n",
      "Epoch: 82 | Batch_idx: 250 |  Loss_1: (0.1826) | Acc_1: (93.32%) (29982/32128)\n",
      "Epoch: 82 | Batch_idx: 260 |  Loss_1: (0.1823) | Acc_1: (93.33%) (31179/33408)\n",
      "Epoch: 82 | Batch_idx: 270 |  Loss_1: (0.1829) | Acc_1: (93.31%) (32369/34688)\n",
      "Epoch: 82 | Batch_idx: 280 |  Loss_1: (0.1833) | Acc_1: (93.30%) (33558/35968)\n",
      "Epoch: 82 | Batch_idx: 290 |  Loss_1: (0.1841) | Acc_1: (93.29%) (34748/37248)\n",
      "Epoch: 82 | Batch_idx: 300 |  Loss_1: (0.1845) | Acc_1: (93.25%) (35929/38528)\n",
      "Epoch: 82 | Batch_idx: 310 |  Loss_1: (0.1847) | Acc_1: (93.24%) (37115/39808)\n",
      "Epoch: 82 | Batch_idx: 320 |  Loss_1: (0.1856) | Acc_1: (93.22%) (38303/41088)\n",
      "Epoch: 82 | Batch_idx: 330 |  Loss_1: (0.1855) | Acc_1: (93.21%) (39492/42368)\n",
      "Epoch: 82 | Batch_idx: 340 |  Loss_1: (0.1855) | Acc_1: (93.21%) (40683/43648)\n",
      "Epoch: 82 | Batch_idx: 350 |  Loss_1: (0.1858) | Acc_1: (93.21%) (41877/44928)\n",
      "Epoch: 82 | Batch_idx: 360 |  Loss_1: (0.1862) | Acc_1: (93.20%) (43065/46208)\n",
      "Epoch: 82 | Batch_idx: 370 |  Loss_1: (0.1867) | Acc_1: (93.17%) (44246/47488)\n",
      "Epoch: 82 | Batch_idx: 380 |  Loss_1: (0.1867) | Acc_1: (93.17%) (45438/48768)\n",
      "Epoch: 82 | Batch_idx: 390 |  Loss_1: (0.1864) | Acc_1: (93.17%) (46584/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3311) | Acc: (90.82%) (9082/10000)\n",
      "Epoch: 83 | Batch_idx: 0 |  Loss_1: (0.2196) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 83 | Batch_idx: 10 |  Loss_1: (0.1958) | Acc_1: (93.25%) (1313/1408)\n",
      "Epoch: 83 | Batch_idx: 20 |  Loss_1: (0.1886) | Acc_1: (93.23%) (2506/2688)\n",
      "Epoch: 83 | Batch_idx: 30 |  Loss_1: (0.1947) | Acc_1: (92.99%) (3690/3968)\n",
      "Epoch: 83 | Batch_idx: 40 |  Loss_1: (0.1881) | Acc_1: (93.31%) (4897/5248)\n",
      "Epoch: 83 | Batch_idx: 50 |  Loss_1: (0.1837) | Acc_1: (93.43%) (6099/6528)\n",
      "Epoch: 83 | Batch_idx: 60 |  Loss_1: (0.1814) | Acc_1: (93.46%) (7297/7808)\n",
      "Epoch: 83 | Batch_idx: 70 |  Loss_1: (0.1805) | Acc_1: (93.43%) (8491/9088)\n",
      "Epoch: 83 | Batch_idx: 80 |  Loss_1: (0.1804) | Acc_1: (93.42%) (9686/10368)\n",
      "Epoch: 83 | Batch_idx: 90 |  Loss_1: (0.1854) | Acc_1: (93.25%) (10862/11648)\n",
      "Epoch: 83 | Batch_idx: 100 |  Loss_1: (0.1834) | Acc_1: (93.36%) (12069/12928)\n",
      "Epoch: 83 | Batch_idx: 110 |  Loss_1: (0.1828) | Acc_1: (93.36%) (13264/14208)\n",
      "Epoch: 83 | Batch_idx: 120 |  Loss_1: (0.1853) | Acc_1: (93.25%) (14443/15488)\n",
      "Epoch: 83 | Batch_idx: 130 |  Loss_1: (0.1832) | Acc_1: (93.33%) (15650/16768)\n",
      "Epoch: 83 | Batch_idx: 140 |  Loss_1: (0.1849) | Acc_1: (93.22%) (16825/18048)\n",
      "Epoch: 83 | Batch_idx: 150 |  Loss_1: (0.1888) | Acc_1: (93.08%) (17991/19328)\n",
      "Epoch: 83 | Batch_idx: 160 |  Loss_1: (0.1889) | Acc_1: (93.07%) (19180/20608)\n",
      "Epoch: 83 | Batch_idx: 170 |  Loss_1: (0.1886) | Acc_1: (93.09%) (20375/21888)\n",
      "Epoch: 83 | Batch_idx: 180 |  Loss_1: (0.1890) | Acc_1: (93.06%) (21561/23168)\n",
      "Epoch: 83 | Batch_idx: 190 |  Loss_1: (0.1897) | Acc_1: (93.08%) (22755/24448)\n",
      "Epoch: 83 | Batch_idx: 200 |  Loss_1: (0.1894) | Acc_1: (93.08%) (23947/25728)\n",
      "Epoch: 83 | Batch_idx: 210 |  Loss_1: (0.1902) | Acc_1: (93.04%) (25128/27008)\n",
      "Epoch: 83 | Batch_idx: 220 |  Loss_1: (0.1909) | Acc_1: (92.99%) (26305/28288)\n",
      "Epoch: 83 | Batch_idx: 230 |  Loss_1: (0.1904) | Acc_1: (92.99%) (27494/29568)\n",
      "Epoch: 83 | Batch_idx: 240 |  Loss_1: (0.1907) | Acc_1: (93.00%) (28688/30848)\n",
      "Epoch: 83 | Batch_idx: 250 |  Loss_1: (0.1911) | Acc_1: (92.98%) (29873/32128)\n",
      "Epoch: 83 | Batch_idx: 260 |  Loss_1: (0.1913) | Acc_1: (92.97%) (31058/33408)\n",
      "Epoch: 83 | Batch_idx: 270 |  Loss_1: (0.1908) | Acc_1: (92.99%) (32257/34688)\n",
      "Epoch: 83 | Batch_idx: 280 |  Loss_1: (0.1917) | Acc_1: (92.96%) (33435/35968)\n",
      "Epoch: 83 | Batch_idx: 290 |  Loss_1: (0.1921) | Acc_1: (92.95%) (34621/37248)\n",
      "Epoch: 83 | Batch_idx: 300 |  Loss_1: (0.1918) | Acc_1: (92.97%) (35820/38528)\n",
      "Epoch: 83 | Batch_idx: 310 |  Loss_1: (0.1919) | Acc_1: (92.99%) (37017/39808)\n",
      "Epoch: 83 | Batch_idx: 320 |  Loss_1: (0.1914) | Acc_1: (93.00%) (38212/41088)\n",
      "Epoch: 83 | Batch_idx: 330 |  Loss_1: (0.1914) | Acc_1: (92.99%) (39399/42368)\n",
      "Epoch: 83 | Batch_idx: 340 |  Loss_1: (0.1911) | Acc_1: (93.01%) (40599/43648)\n",
      "Epoch: 83 | Batch_idx: 350 |  Loss_1: (0.1914) | Acc_1: (93.02%) (41790/44928)\n",
      "Epoch: 83 | Batch_idx: 360 |  Loss_1: (0.1919) | Acc_1: (93.01%) (42977/46208)\n",
      "Epoch: 83 | Batch_idx: 370 |  Loss_1: (0.1931) | Acc_1: (92.98%) (44152/47488)\n",
      "Epoch: 83 | Batch_idx: 380 |  Loss_1: (0.1930) | Acc_1: (92.98%) (45343/48768)\n",
      "Epoch: 83 | Batch_idx: 390 |  Loss_1: (0.1928) | Acc_1: (92.99%) (46494/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3218) | Acc: (90.70%) (9070/10000)\n",
      "Epoch: 84 | Batch_idx: 0 |  Loss_1: (0.2624) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 84 | Batch_idx: 10 |  Loss_1: (0.1814) | Acc_1: (93.18%) (1312/1408)\n",
      "Epoch: 84 | Batch_idx: 20 |  Loss_1: (0.1822) | Acc_1: (93.08%) (2502/2688)\n",
      "Epoch: 84 | Batch_idx: 30 |  Loss_1: (0.1803) | Acc_1: (93.22%) (3699/3968)\n",
      "Epoch: 84 | Batch_idx: 40 |  Loss_1: (0.1890) | Acc_1: (93.14%) (4888/5248)\n",
      "Epoch: 84 | Batch_idx: 50 |  Loss_1: (0.1841) | Acc_1: (93.28%) (6089/6528)\n",
      "Epoch: 84 | Batch_idx: 60 |  Loss_1: (0.1849) | Acc_1: (93.26%) (7282/7808)\n",
      "Epoch: 84 | Batch_idx: 70 |  Loss_1: (0.1840) | Acc_1: (93.20%) (8470/9088)\n",
      "Epoch: 84 | Batch_idx: 80 |  Loss_1: (0.1818) | Acc_1: (93.27%) (9670/10368)\n",
      "Epoch: 84 | Batch_idx: 90 |  Loss_1: (0.1842) | Acc_1: (93.17%) (10852/11648)\n",
      "Epoch: 84 | Batch_idx: 100 |  Loss_1: (0.1824) | Acc_1: (93.20%) (12049/12928)\n",
      "Epoch: 84 | Batch_idx: 110 |  Loss_1: (0.1826) | Acc_1: (93.22%) (13245/14208)\n",
      "Epoch: 84 | Batch_idx: 120 |  Loss_1: (0.1809) | Acc_1: (93.30%) (14450/15488)\n",
      "Epoch: 84 | Batch_idx: 130 |  Loss_1: (0.1815) | Acc_1: (93.28%) (15642/16768)\n",
      "Epoch: 84 | Batch_idx: 140 |  Loss_1: (0.1802) | Acc_1: (93.32%) (16842/18048)\n",
      "Epoch: 84 | Batch_idx: 150 |  Loss_1: (0.1806) | Acc_1: (93.26%) (18026/19328)\n",
      "Epoch: 84 | Batch_idx: 160 |  Loss_1: (0.1809) | Acc_1: (93.31%) (19229/20608)\n",
      "Epoch: 84 | Batch_idx: 170 |  Loss_1: (0.1816) | Acc_1: (93.27%) (20414/21888)\n",
      "Epoch: 84 | Batch_idx: 180 |  Loss_1: (0.1824) | Acc_1: (93.21%) (21595/23168)\n",
      "Epoch: 84 | Batch_idx: 190 |  Loss_1: (0.1845) | Acc_1: (93.12%) (22766/24448)\n",
      "Epoch: 84 | Batch_idx: 200 |  Loss_1: (0.1843) | Acc_1: (93.13%) (23961/25728)\n",
      "Epoch: 84 | Batch_idx: 210 |  Loss_1: (0.1846) | Acc_1: (93.11%) (25147/27008)\n",
      "Epoch: 84 | Batch_idx: 220 |  Loss_1: (0.1844) | Acc_1: (93.16%) (26352/28288)\n",
      "Epoch: 84 | Batch_idx: 230 |  Loss_1: (0.1843) | Acc_1: (93.18%) (27552/29568)\n",
      "Epoch: 84 | Batch_idx: 240 |  Loss_1: (0.1846) | Acc_1: (93.17%) (28741/30848)\n",
      "Epoch: 84 | Batch_idx: 250 |  Loss_1: (0.1851) | Acc_1: (93.15%) (29927/32128)\n",
      "Epoch: 84 | Batch_idx: 260 |  Loss_1: (0.1855) | Acc_1: (93.14%) (31117/33408)\n",
      "Epoch: 84 | Batch_idx: 270 |  Loss_1: (0.1868) | Acc_1: (93.11%) (32298/34688)\n",
      "Epoch: 84 | Batch_idx: 280 |  Loss_1: (0.1862) | Acc_1: (93.12%) (33495/35968)\n",
      "Epoch: 84 | Batch_idx: 290 |  Loss_1: (0.1848) | Acc_1: (93.19%) (34710/37248)\n",
      "Epoch: 84 | Batch_idx: 300 |  Loss_1: (0.1855) | Acc_1: (93.15%) (35889/38528)\n",
      "Epoch: 84 | Batch_idx: 310 |  Loss_1: (0.1857) | Acc_1: (93.15%) (37083/39808)\n",
      "Epoch: 84 | Batch_idx: 320 |  Loss_1: (0.1862) | Acc_1: (93.15%) (38274/41088)\n",
      "Epoch: 84 | Batch_idx: 330 |  Loss_1: (0.1862) | Acc_1: (93.16%) (39469/42368)\n",
      "Epoch: 84 | Batch_idx: 340 |  Loss_1: (0.1862) | Acc_1: (93.17%) (40665/43648)\n",
      "Epoch: 84 | Batch_idx: 350 |  Loss_1: (0.1871) | Acc_1: (93.13%) (41842/44928)\n",
      "Epoch: 84 | Batch_idx: 360 |  Loss_1: (0.1875) | Acc_1: (93.10%) (43020/46208)\n",
      "Epoch: 84 | Batch_idx: 370 |  Loss_1: (0.1873) | Acc_1: (93.11%) (44216/47488)\n",
      "Epoch: 84 | Batch_idx: 380 |  Loss_1: (0.1874) | Acc_1: (93.11%) (45406/48768)\n",
      "Epoch: 84 | Batch_idx: 390 |  Loss_1: (0.1868) | Acc_1: (93.14%) (46570/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3210) | Acc: (91.37%) (9137/10000)\n",
      "Epoch: 85 | Batch_idx: 0 |  Loss_1: (0.1927) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 85 | Batch_idx: 10 |  Loss_1: (0.1982) | Acc_1: (93.04%) (1310/1408)\n",
      "Epoch: 85 | Batch_idx: 20 |  Loss_1: (0.1890) | Acc_1: (93.56%) (2515/2688)\n",
      "Epoch: 85 | Batch_idx: 30 |  Loss_1: (0.1895) | Acc_1: (93.47%) (3709/3968)\n",
      "Epoch: 85 | Batch_idx: 40 |  Loss_1: (0.1856) | Acc_1: (93.48%) (4906/5248)\n",
      "Epoch: 85 | Batch_idx: 50 |  Loss_1: (0.1841) | Acc_1: (93.43%) (6099/6528)\n",
      "Epoch: 85 | Batch_idx: 60 |  Loss_1: (0.1838) | Acc_1: (93.47%) (7298/7808)\n",
      "Epoch: 85 | Batch_idx: 70 |  Loss_1: (0.1832) | Acc_1: (93.47%) (8495/9088)\n",
      "Epoch: 85 | Batch_idx: 80 |  Loss_1: (0.1807) | Acc_1: (93.53%) (9697/10368)\n",
      "Epoch: 85 | Batch_idx: 90 |  Loss_1: (0.1839) | Acc_1: (93.34%) (10872/11648)\n",
      "Epoch: 85 | Batch_idx: 100 |  Loss_1: (0.1812) | Acc_1: (93.44%) (12080/12928)\n",
      "Epoch: 85 | Batch_idx: 110 |  Loss_1: (0.1828) | Acc_1: (93.39%) (13269/14208)\n",
      "Epoch: 85 | Batch_idx: 120 |  Loss_1: (0.1824) | Acc_1: (93.43%) (14470/15488)\n",
      "Epoch: 85 | Batch_idx: 130 |  Loss_1: (0.1797) | Acc_1: (93.54%) (15685/16768)\n",
      "Epoch: 85 | Batch_idx: 140 |  Loss_1: (0.1782) | Acc_1: (93.58%) (16890/18048)\n",
      "Epoch: 85 | Batch_idx: 150 |  Loss_1: (0.1818) | Acc_1: (93.51%) (18074/19328)\n",
      "Epoch: 85 | Batch_idx: 160 |  Loss_1: (0.1826) | Acc_1: (93.47%) (19263/20608)\n",
      "Epoch: 85 | Batch_idx: 170 |  Loss_1: (0.1859) | Acc_1: (93.39%) (20442/21888)\n",
      "Epoch: 85 | Batch_idx: 180 |  Loss_1: (0.1873) | Acc_1: (93.33%) (21623/23168)\n",
      "Epoch: 85 | Batch_idx: 190 |  Loss_1: (0.1873) | Acc_1: (93.35%) (22821/24448)\n",
      "Epoch: 85 | Batch_idx: 200 |  Loss_1: (0.1886) | Acc_1: (93.31%) (24008/25728)\n",
      "Epoch: 85 | Batch_idx: 210 |  Loss_1: (0.1884) | Acc_1: (93.30%) (25199/27008)\n",
      "Epoch: 85 | Batch_idx: 220 |  Loss_1: (0.1878) | Acc_1: (93.32%) (26397/28288)\n",
      "Epoch: 85 | Batch_idx: 230 |  Loss_1: (0.1870) | Acc_1: (93.33%) (27596/29568)\n",
      "Epoch: 85 | Batch_idx: 240 |  Loss_1: (0.1875) | Acc_1: (93.30%) (28781/30848)\n",
      "Epoch: 85 | Batch_idx: 250 |  Loss_1: (0.1885) | Acc_1: (93.27%) (29967/32128)\n",
      "Epoch: 85 | Batch_idx: 260 |  Loss_1: (0.1879) | Acc_1: (93.28%) (31162/33408)\n",
      "Epoch: 85 | Batch_idx: 270 |  Loss_1: (0.1885) | Acc_1: (93.26%) (32351/34688)\n",
      "Epoch: 85 | Batch_idx: 280 |  Loss_1: (0.1880) | Acc_1: (93.28%) (33552/35968)\n",
      "Epoch: 85 | Batch_idx: 290 |  Loss_1: (0.1879) | Acc_1: (93.27%) (34743/37248)\n",
      "Epoch: 85 | Batch_idx: 300 |  Loss_1: (0.1880) | Acc_1: (93.27%) (35935/38528)\n",
      "Epoch: 85 | Batch_idx: 310 |  Loss_1: (0.1879) | Acc_1: (93.27%) (37129/39808)\n",
      "Epoch: 85 | Batch_idx: 320 |  Loss_1: (0.1880) | Acc_1: (93.24%) (38312/41088)\n",
      "Epoch: 85 | Batch_idx: 330 |  Loss_1: (0.1875) | Acc_1: (93.24%) (39504/42368)\n",
      "Epoch: 85 | Batch_idx: 340 |  Loss_1: (0.1879) | Acc_1: (93.23%) (40691/43648)\n",
      "Epoch: 85 | Batch_idx: 350 |  Loss_1: (0.1870) | Acc_1: (93.26%) (41898/44928)\n",
      "Epoch: 85 | Batch_idx: 360 |  Loss_1: (0.1876) | Acc_1: (93.22%) (43074/46208)\n",
      "Epoch: 85 | Batch_idx: 370 |  Loss_1: (0.1879) | Acc_1: (93.22%) (44267/47488)\n",
      "Epoch: 85 | Batch_idx: 380 |  Loss_1: (0.1883) | Acc_1: (93.19%) (45449/48768)\n",
      "Epoch: 85 | Batch_idx: 390 |  Loss_1: (0.1878) | Acc_1: (93.21%) (46604/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3517) | Acc: (90.47%) (9047/10000)\n",
      "Epoch: 86 | Batch_idx: 0 |  Loss_1: (0.1367) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 86 | Batch_idx: 10 |  Loss_1: (0.1734) | Acc_1: (93.54%) (1317/1408)\n",
      "Epoch: 86 | Batch_idx: 20 |  Loss_1: (0.1791) | Acc_1: (93.60%) (2516/2688)\n",
      "Epoch: 86 | Batch_idx: 30 |  Loss_1: (0.1739) | Acc_1: (93.67%) (3717/3968)\n",
      "Epoch: 86 | Batch_idx: 40 |  Loss_1: (0.1826) | Acc_1: (93.52%) (4908/5248)\n",
      "Epoch: 86 | Batch_idx: 50 |  Loss_1: (0.1765) | Acc_1: (93.60%) (6110/6528)\n",
      "Epoch: 86 | Batch_idx: 60 |  Loss_1: (0.1716) | Acc_1: (93.80%) (7324/7808)\n",
      "Epoch: 86 | Batch_idx: 70 |  Loss_1: (0.1744) | Acc_1: (93.73%) (8518/9088)\n",
      "Epoch: 86 | Batch_idx: 80 |  Loss_1: (0.1739) | Acc_1: (93.74%) (9719/10368)\n",
      "Epoch: 86 | Batch_idx: 90 |  Loss_1: (0.1764) | Acc_1: (93.60%) (10903/11648)\n",
      "Epoch: 86 | Batch_idx: 100 |  Loss_1: (0.1768) | Acc_1: (93.58%) (12098/12928)\n",
      "Epoch: 86 | Batch_idx: 110 |  Loss_1: (0.1763) | Acc_1: (93.63%) (13303/14208)\n",
      "Epoch: 86 | Batch_idx: 120 |  Loss_1: (0.1777) | Acc_1: (93.58%) (14494/15488)\n",
      "Epoch: 86 | Batch_idx: 130 |  Loss_1: (0.1773) | Acc_1: (93.62%) (15699/16768)\n",
      "Epoch: 86 | Batch_idx: 140 |  Loss_1: (0.1761) | Acc_1: (93.64%) (16901/18048)\n",
      "Epoch: 86 | Batch_idx: 150 |  Loss_1: (0.1762) | Acc_1: (93.62%) (18095/19328)\n",
      "Epoch: 86 | Batch_idx: 160 |  Loss_1: (0.1764) | Acc_1: (93.60%) (19290/20608)\n",
      "Epoch: 86 | Batch_idx: 170 |  Loss_1: (0.1781) | Acc_1: (93.52%) (20469/21888)\n",
      "Epoch: 86 | Batch_idx: 180 |  Loss_1: (0.1794) | Acc_1: (93.47%) (21656/23168)\n",
      "Epoch: 86 | Batch_idx: 190 |  Loss_1: (0.1815) | Acc_1: (93.40%) (22835/24448)\n",
      "Epoch: 86 | Batch_idx: 200 |  Loss_1: (0.1807) | Acc_1: (93.41%) (24032/25728)\n",
      "Epoch: 86 | Batch_idx: 210 |  Loss_1: (0.1814) | Acc_1: (93.40%) (25225/27008)\n",
      "Epoch: 86 | Batch_idx: 220 |  Loss_1: (0.1823) | Acc_1: (93.36%) (26410/28288)\n",
      "Epoch: 86 | Batch_idx: 230 |  Loss_1: (0.1829) | Acc_1: (93.34%) (27600/29568)\n",
      "Epoch: 86 | Batch_idx: 240 |  Loss_1: (0.1831) | Acc_1: (93.35%) (28798/30848)\n",
      "Epoch: 86 | Batch_idx: 250 |  Loss_1: (0.1834) | Acc_1: (93.34%) (29988/32128)\n",
      "Epoch: 86 | Batch_idx: 260 |  Loss_1: (0.1832) | Acc_1: (93.34%) (31183/33408)\n",
      "Epoch: 86 | Batch_idx: 270 |  Loss_1: (0.1836) | Acc_1: (93.33%) (32375/34688)\n",
      "Epoch: 86 | Batch_idx: 280 |  Loss_1: (0.1839) | Acc_1: (93.31%) (33561/35968)\n",
      "Epoch: 86 | Batch_idx: 290 |  Loss_1: (0.1852) | Acc_1: (93.27%) (34743/37248)\n",
      "Epoch: 86 | Batch_idx: 300 |  Loss_1: (0.1869) | Acc_1: (93.23%) (35921/38528)\n",
      "Epoch: 86 | Batch_idx: 310 |  Loss_1: (0.1881) | Acc_1: (93.19%) (37097/39808)\n",
      "Epoch: 86 | Batch_idx: 320 |  Loss_1: (0.1882) | Acc_1: (93.17%) (38282/41088)\n",
      "Epoch: 86 | Batch_idx: 330 |  Loss_1: (0.1879) | Acc_1: (93.16%) (39470/42368)\n",
      "Epoch: 86 | Batch_idx: 340 |  Loss_1: (0.1890) | Acc_1: (93.11%) (40641/43648)\n",
      "Epoch: 86 | Batch_idx: 350 |  Loss_1: (0.1889) | Acc_1: (93.10%) (41830/44928)\n",
      "Epoch: 86 | Batch_idx: 360 |  Loss_1: (0.1888) | Acc_1: (93.11%) (43022/46208)\n",
      "Epoch: 86 | Batch_idx: 370 |  Loss_1: (0.1894) | Acc_1: (93.10%) (44213/47488)\n",
      "Epoch: 86 | Batch_idx: 380 |  Loss_1: (0.1894) | Acc_1: (93.08%) (45394/48768)\n",
      "Epoch: 86 | Batch_idx: 390 |  Loss_1: (0.1894) | Acc_1: (93.08%) (46542/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3212) | Acc: (91.04%) (9104/10000)\n",
      "Epoch: 87 | Batch_idx: 0 |  Loss_1: (0.1880) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 87 | Batch_idx: 10 |  Loss_1: (0.1623) | Acc_1: (94.32%) (1328/1408)\n",
      "Epoch: 87 | Batch_idx: 20 |  Loss_1: (0.1672) | Acc_1: (94.01%) (2527/2688)\n",
      "Epoch: 87 | Batch_idx: 30 |  Loss_1: (0.1637) | Acc_1: (94.13%) (3735/3968)\n",
      "Epoch: 87 | Batch_idx: 40 |  Loss_1: (0.1656) | Acc_1: (94.00%) (4933/5248)\n",
      "Epoch: 87 | Batch_idx: 50 |  Loss_1: (0.1678) | Acc_1: (93.73%) (6119/6528)\n",
      "Epoch: 87 | Batch_idx: 60 |  Loss_1: (0.1694) | Acc_1: (93.66%) (7313/7808)\n",
      "Epoch: 87 | Batch_idx: 70 |  Loss_1: (0.1752) | Acc_1: (93.49%) (8496/9088)\n",
      "Epoch: 87 | Batch_idx: 80 |  Loss_1: (0.1728) | Acc_1: (93.59%) (9703/10368)\n",
      "Epoch: 87 | Batch_idx: 90 |  Loss_1: (0.1714) | Acc_1: (93.63%) (10906/11648)\n",
      "Epoch: 87 | Batch_idx: 100 |  Loss_1: (0.1721) | Acc_1: (93.54%) (12093/12928)\n",
      "Epoch: 87 | Batch_idx: 110 |  Loss_1: (0.1721) | Acc_1: (93.56%) (13293/14208)\n",
      "Epoch: 87 | Batch_idx: 120 |  Loss_1: (0.1700) | Acc_1: (93.63%) (14502/15488)\n",
      "Epoch: 87 | Batch_idx: 130 |  Loss_1: (0.1703) | Acc_1: (93.64%) (15701/16768)\n",
      "Epoch: 87 | Batch_idx: 140 |  Loss_1: (0.1709) | Acc_1: (93.63%) (16899/18048)\n",
      "Epoch: 87 | Batch_idx: 150 |  Loss_1: (0.1727) | Acc_1: (93.58%) (18087/19328)\n",
      "Epoch: 87 | Batch_idx: 160 |  Loss_1: (0.1751) | Acc_1: (93.43%) (19255/20608)\n",
      "Epoch: 87 | Batch_idx: 170 |  Loss_1: (0.1762) | Acc_1: (93.41%) (20446/21888)\n",
      "Epoch: 87 | Batch_idx: 180 |  Loss_1: (0.1776) | Acc_1: (93.39%) (21636/23168)\n",
      "Epoch: 87 | Batch_idx: 190 |  Loss_1: (0.1767) | Acc_1: (93.44%) (22844/24448)\n",
      "Epoch: 87 | Batch_idx: 200 |  Loss_1: (0.1772) | Acc_1: (93.46%) (24046/25728)\n",
      "Epoch: 87 | Batch_idx: 210 |  Loss_1: (0.1778) | Acc_1: (93.43%) (25234/27008)\n",
      "Epoch: 87 | Batch_idx: 220 |  Loss_1: (0.1787) | Acc_1: (93.40%) (26421/28288)\n",
      "Epoch: 87 | Batch_idx: 230 |  Loss_1: (0.1803) | Acc_1: (93.34%) (27600/29568)\n",
      "Epoch: 87 | Batch_idx: 240 |  Loss_1: (0.1802) | Acc_1: (93.34%) (28794/30848)\n",
      "Epoch: 87 | Batch_idx: 250 |  Loss_1: (0.1812) | Acc_1: (93.31%) (29979/32128)\n",
      "Epoch: 87 | Batch_idx: 260 |  Loss_1: (0.1821) | Acc_1: (93.28%) (31163/33408)\n",
      "Epoch: 87 | Batch_idx: 270 |  Loss_1: (0.1821) | Acc_1: (93.29%) (32362/34688)\n",
      "Epoch: 87 | Batch_idx: 280 |  Loss_1: (0.1824) | Acc_1: (93.29%) (33553/35968)\n",
      "Epoch: 87 | Batch_idx: 290 |  Loss_1: (0.1826) | Acc_1: (93.27%) (34741/37248)\n",
      "Epoch: 87 | Batch_idx: 300 |  Loss_1: (0.1826) | Acc_1: (93.26%) (35931/38528)\n",
      "Epoch: 87 | Batch_idx: 310 |  Loss_1: (0.1834) | Acc_1: (93.23%) (37113/39808)\n",
      "Epoch: 87 | Batch_idx: 320 |  Loss_1: (0.1841) | Acc_1: (93.21%) (38297/41088)\n",
      "Epoch: 87 | Batch_idx: 330 |  Loss_1: (0.1845) | Acc_1: (93.20%) (39487/42368)\n",
      "Epoch: 87 | Batch_idx: 340 |  Loss_1: (0.1864) | Acc_1: (93.15%) (40660/43648)\n",
      "Epoch: 87 | Batch_idx: 350 |  Loss_1: (0.1865) | Acc_1: (93.16%) (41856/44928)\n",
      "Epoch: 87 | Batch_idx: 360 |  Loss_1: (0.1885) | Acc_1: (93.10%) (43018/46208)\n",
      "Epoch: 87 | Batch_idx: 370 |  Loss_1: (0.1883) | Acc_1: (93.11%) (44217/47488)\n",
      "Epoch: 87 | Batch_idx: 380 |  Loss_1: (0.1891) | Acc_1: (93.08%) (45393/48768)\n",
      "Epoch: 87 | Batch_idx: 390 |  Loss_1: (0.1895) | Acc_1: (93.05%) (46524/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3087) | Acc: (91.31%) (9131/10000)\n",
      "Epoch: 88 | Batch_idx: 0 |  Loss_1: (0.1586) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 88 | Batch_idx: 10 |  Loss_1: (0.1623) | Acc_1: (94.32%) (1328/1408)\n",
      "Epoch: 88 | Batch_idx: 20 |  Loss_1: (0.1853) | Acc_1: (93.19%) (2505/2688)\n",
      "Epoch: 88 | Batch_idx: 30 |  Loss_1: (0.1751) | Acc_1: (93.60%) (3714/3968)\n",
      "Epoch: 88 | Batch_idx: 40 |  Loss_1: (0.1707) | Acc_1: (93.75%) (4920/5248)\n",
      "Epoch: 88 | Batch_idx: 50 |  Loss_1: (0.1749) | Acc_1: (93.57%) (6108/6528)\n",
      "Epoch: 88 | Batch_idx: 60 |  Loss_1: (0.1729) | Acc_1: (93.56%) (7305/7808)\n",
      "Epoch: 88 | Batch_idx: 70 |  Loss_1: (0.1756) | Acc_1: (93.51%) (8498/9088)\n",
      "Epoch: 88 | Batch_idx: 80 |  Loss_1: (0.1774) | Acc_1: (93.47%) (9691/10368)\n",
      "Epoch: 88 | Batch_idx: 90 |  Loss_1: (0.1749) | Acc_1: (93.58%) (10900/11648)\n",
      "Epoch: 88 | Batch_idx: 100 |  Loss_1: (0.1733) | Acc_1: (93.60%) (12101/12928)\n",
      "Epoch: 88 | Batch_idx: 110 |  Loss_1: (0.1717) | Acc_1: (93.67%) (13308/14208)\n",
      "Epoch: 88 | Batch_idx: 120 |  Loss_1: (0.1707) | Acc_1: (93.72%) (14516/15488)\n",
      "Epoch: 88 | Batch_idx: 130 |  Loss_1: (0.1726) | Acc_1: (93.70%) (15712/16768)\n",
      "Epoch: 88 | Batch_idx: 140 |  Loss_1: (0.1718) | Acc_1: (93.72%) (16915/18048)\n",
      "Epoch: 88 | Batch_idx: 150 |  Loss_1: (0.1718) | Acc_1: (93.74%) (18119/19328)\n",
      "Epoch: 88 | Batch_idx: 160 |  Loss_1: (0.1727) | Acc_1: (93.69%) (19308/20608)\n",
      "Epoch: 88 | Batch_idx: 170 |  Loss_1: (0.1740) | Acc_1: (93.66%) (20501/21888)\n",
      "Epoch: 88 | Batch_idx: 180 |  Loss_1: (0.1739) | Acc_1: (93.63%) (21693/23168)\n",
      "Epoch: 88 | Batch_idx: 190 |  Loss_1: (0.1742) | Acc_1: (93.66%) (22898/24448)\n",
      "Epoch: 88 | Batch_idx: 200 |  Loss_1: (0.1760) | Acc_1: (93.61%) (24083/25728)\n",
      "Epoch: 88 | Batch_idx: 210 |  Loss_1: (0.1771) | Acc_1: (93.56%) (25269/27008)\n",
      "Epoch: 88 | Batch_idx: 220 |  Loss_1: (0.1772) | Acc_1: (93.54%) (26461/28288)\n",
      "Epoch: 88 | Batch_idx: 230 |  Loss_1: (0.1783) | Acc_1: (93.50%) (27647/29568)\n",
      "Epoch: 88 | Batch_idx: 240 |  Loss_1: (0.1784) | Acc_1: (93.52%) (28850/30848)\n",
      "Epoch: 88 | Batch_idx: 250 |  Loss_1: (0.1785) | Acc_1: (93.51%) (30044/32128)\n",
      "Epoch: 88 | Batch_idx: 260 |  Loss_1: (0.1790) | Acc_1: (93.50%) (31237/33408)\n",
      "Epoch: 88 | Batch_idx: 270 |  Loss_1: (0.1804) | Acc_1: (93.45%) (32415/34688)\n",
      "Epoch: 88 | Batch_idx: 280 |  Loss_1: (0.1799) | Acc_1: (93.46%) (33615/35968)\n",
      "Epoch: 88 | Batch_idx: 290 |  Loss_1: (0.1800) | Acc_1: (93.47%) (34814/37248)\n",
      "Epoch: 88 | Batch_idx: 300 |  Loss_1: (0.1803) | Acc_1: (93.45%) (36003/38528)\n",
      "Epoch: 88 | Batch_idx: 310 |  Loss_1: (0.1805) | Acc_1: (93.42%) (37190/39808)\n",
      "Epoch: 88 | Batch_idx: 320 |  Loss_1: (0.1812) | Acc_1: (93.39%) (38371/41088)\n",
      "Epoch: 88 | Batch_idx: 330 |  Loss_1: (0.1814) | Acc_1: (93.38%) (39562/42368)\n",
      "Epoch: 88 | Batch_idx: 340 |  Loss_1: (0.1810) | Acc_1: (93.41%) (40773/43648)\n",
      "Epoch: 88 | Batch_idx: 350 |  Loss_1: (0.1814) | Acc_1: (93.40%) (41961/44928)\n",
      "Epoch: 88 | Batch_idx: 360 |  Loss_1: (0.1812) | Acc_1: (93.40%) (43156/46208)\n",
      "Epoch: 88 | Batch_idx: 370 |  Loss_1: (0.1819) | Acc_1: (93.38%) (44346/47488)\n",
      "Epoch: 88 | Batch_idx: 380 |  Loss_1: (0.1815) | Acc_1: (93.40%) (45550/48768)\n",
      "Epoch: 88 | Batch_idx: 390 |  Loss_1: (0.1822) | Acc_1: (93.40%) (46699/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3137) | Acc: (91.39%) (9139/10000)\n",
      "Epoch: 89 | Batch_idx: 0 |  Loss_1: (0.1854) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 89 | Batch_idx: 10 |  Loss_1: (0.1914) | Acc_1: (93.32%) (1314/1408)\n",
      "Epoch: 89 | Batch_idx: 20 |  Loss_1: (0.1673) | Acc_1: (94.16%) (2531/2688)\n",
      "Epoch: 89 | Batch_idx: 30 |  Loss_1: (0.1611) | Acc_1: (94.35%) (3744/3968)\n",
      "Epoch: 89 | Batch_idx: 40 |  Loss_1: (0.1704) | Acc_1: (93.92%) (4929/5248)\n",
      "Epoch: 89 | Batch_idx: 50 |  Loss_1: (0.1653) | Acc_1: (94.04%) (6139/6528)\n",
      "Epoch: 89 | Batch_idx: 60 |  Loss_1: (0.1691) | Acc_1: (93.87%) (7329/7808)\n",
      "Epoch: 89 | Batch_idx: 70 |  Loss_1: (0.1691) | Acc_1: (93.86%) (8530/9088)\n",
      "Epoch: 89 | Batch_idx: 80 |  Loss_1: (0.1723) | Acc_1: (93.71%) (9716/10368)\n",
      "Epoch: 89 | Batch_idx: 90 |  Loss_1: (0.1725) | Acc_1: (93.70%) (10914/11648)\n",
      "Epoch: 89 | Batch_idx: 100 |  Loss_1: (0.1739) | Acc_1: (93.63%) (12105/12928)\n",
      "Epoch: 89 | Batch_idx: 110 |  Loss_1: (0.1778) | Acc_1: (93.54%) (13290/14208)\n",
      "Epoch: 89 | Batch_idx: 120 |  Loss_1: (0.1771) | Acc_1: (93.58%) (14493/15488)\n",
      "Epoch: 89 | Batch_idx: 130 |  Loss_1: (0.1784) | Acc_1: (93.48%) (15675/16768)\n",
      "Epoch: 89 | Batch_idx: 140 |  Loss_1: (0.1778) | Acc_1: (93.52%) (16879/18048)\n",
      "Epoch: 89 | Batch_idx: 150 |  Loss_1: (0.1781) | Acc_1: (93.53%) (18077/19328)\n",
      "Epoch: 89 | Batch_idx: 160 |  Loss_1: (0.1779) | Acc_1: (93.54%) (19277/20608)\n",
      "Epoch: 89 | Batch_idx: 170 |  Loss_1: (0.1785) | Acc_1: (93.54%) (20474/21888)\n",
      "Epoch: 89 | Batch_idx: 180 |  Loss_1: (0.1800) | Acc_1: (93.49%) (21659/23168)\n",
      "Epoch: 89 | Batch_idx: 190 |  Loss_1: (0.1795) | Acc_1: (93.52%) (22864/24448)\n",
      "Epoch: 89 | Batch_idx: 200 |  Loss_1: (0.1791) | Acc_1: (93.56%) (24071/25728)\n",
      "Epoch: 89 | Batch_idx: 210 |  Loss_1: (0.1796) | Acc_1: (93.56%) (25268/27008)\n",
      "Epoch: 89 | Batch_idx: 220 |  Loss_1: (0.1795) | Acc_1: (93.55%) (26463/28288)\n",
      "Epoch: 89 | Batch_idx: 230 |  Loss_1: (0.1789) | Acc_1: (93.55%) (27662/29568)\n",
      "Epoch: 89 | Batch_idx: 240 |  Loss_1: (0.1796) | Acc_1: (93.54%) (28855/30848)\n",
      "Epoch: 89 | Batch_idx: 250 |  Loss_1: (0.1797) | Acc_1: (93.54%) (30052/32128)\n",
      "Epoch: 89 | Batch_idx: 260 |  Loss_1: (0.1801) | Acc_1: (93.53%) (31248/33408)\n",
      "Epoch: 89 | Batch_idx: 270 |  Loss_1: (0.1793) | Acc_1: (93.56%) (32453/34688)\n",
      "Epoch: 89 | Batch_idx: 280 |  Loss_1: (0.1805) | Acc_1: (93.52%) (33639/35968)\n",
      "Epoch: 89 | Batch_idx: 290 |  Loss_1: (0.1795) | Acc_1: (93.56%) (34851/37248)\n",
      "Epoch: 89 | Batch_idx: 300 |  Loss_1: (0.1795) | Acc_1: (93.54%) (36040/38528)\n",
      "Epoch: 89 | Batch_idx: 310 |  Loss_1: (0.1788) | Acc_1: (93.58%) (37251/39808)\n",
      "Epoch: 89 | Batch_idx: 320 |  Loss_1: (0.1793) | Acc_1: (93.56%) (38440/41088)\n",
      "Epoch: 89 | Batch_idx: 330 |  Loss_1: (0.1794) | Acc_1: (93.55%) (39637/42368)\n",
      "Epoch: 89 | Batch_idx: 340 |  Loss_1: (0.1793) | Acc_1: (93.54%) (40828/43648)\n",
      "Epoch: 89 | Batch_idx: 350 |  Loss_1: (0.1797) | Acc_1: (93.52%) (42015/44928)\n",
      "Epoch: 89 | Batch_idx: 360 |  Loss_1: (0.1800) | Acc_1: (93.51%) (43208/46208)\n",
      "Epoch: 89 | Batch_idx: 370 |  Loss_1: (0.1806) | Acc_1: (93.46%) (44383/47488)\n",
      "Epoch: 89 | Batch_idx: 380 |  Loss_1: (0.1814) | Acc_1: (93.42%) (45560/48768)\n",
      "Epoch: 89 | Batch_idx: 390 |  Loss_1: (0.1822) | Acc_1: (93.40%) (46699/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2921) | Acc: (91.62%) (9162/10000)\n",
      "Epoch: 90 | Batch_idx: 0 |  Loss_1: (0.2066) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 90 | Batch_idx: 10 |  Loss_1: (0.2002) | Acc_1: (92.33%) (1300/1408)\n",
      "Epoch: 90 | Batch_idx: 20 |  Loss_1: (0.1808) | Acc_1: (93.34%) (2509/2688)\n",
      "Epoch: 90 | Batch_idx: 30 |  Loss_1: (0.1797) | Acc_1: (93.47%) (3709/3968)\n",
      "Epoch: 90 | Batch_idx: 40 |  Loss_1: (0.1814) | Acc_1: (93.41%) (4902/5248)\n",
      "Epoch: 90 | Batch_idx: 50 |  Loss_1: (0.1813) | Acc_1: (93.43%) (6099/6528)\n",
      "Epoch: 90 | Batch_idx: 60 |  Loss_1: (0.1812) | Acc_1: (93.49%) (7300/7808)\n",
      "Epoch: 90 | Batch_idx: 70 |  Loss_1: (0.1816) | Acc_1: (93.51%) (8498/9088)\n",
      "Epoch: 90 | Batch_idx: 80 |  Loss_1: (0.1830) | Acc_1: (93.37%) (9681/10368)\n",
      "Epoch: 90 | Batch_idx: 90 |  Loss_1: (0.1838) | Acc_1: (93.37%) (10876/11648)\n",
      "Epoch: 90 | Batch_idx: 100 |  Loss_1: (0.1834) | Acc_1: (93.35%) (12068/12928)\n",
      "Epoch: 90 | Batch_idx: 110 |  Loss_1: (0.1845) | Acc_1: (93.29%) (13255/14208)\n",
      "Epoch: 90 | Batch_idx: 120 |  Loss_1: (0.1845) | Acc_1: (93.29%) (14448/15488)\n",
      "Epoch: 90 | Batch_idx: 130 |  Loss_1: (0.1825) | Acc_1: (93.34%) (15651/16768)\n",
      "Epoch: 90 | Batch_idx: 140 |  Loss_1: (0.1863) | Acc_1: (93.24%) (16828/18048)\n",
      "Epoch: 90 | Batch_idx: 150 |  Loss_1: (0.1857) | Acc_1: (93.25%) (18023/19328)\n",
      "Epoch: 90 | Batch_idx: 160 |  Loss_1: (0.1855) | Acc_1: (93.24%) (19215/20608)\n",
      "Epoch: 90 | Batch_idx: 170 |  Loss_1: (0.1848) | Acc_1: (93.28%) (20418/21888)\n",
      "Epoch: 90 | Batch_idx: 180 |  Loss_1: (0.1857) | Acc_1: (93.27%) (21609/23168)\n",
      "Epoch: 90 | Batch_idx: 190 |  Loss_1: (0.1863) | Acc_1: (93.24%) (22796/24448)\n",
      "Epoch: 90 | Batch_idx: 200 |  Loss_1: (0.1856) | Acc_1: (93.26%) (23995/25728)\n",
      "Epoch: 90 | Batch_idx: 210 |  Loss_1: (0.1854) | Acc_1: (93.28%) (25192/27008)\n",
      "Epoch: 90 | Batch_idx: 220 |  Loss_1: (0.1839) | Acc_1: (93.34%) (26404/28288)\n",
      "Epoch: 90 | Batch_idx: 230 |  Loss_1: (0.1835) | Acc_1: (93.36%) (27605/29568)\n",
      "Epoch: 90 | Batch_idx: 240 |  Loss_1: (0.1837) | Acc_1: (93.35%) (28797/30848)\n",
      "Epoch: 90 | Batch_idx: 250 |  Loss_1: (0.1844) | Acc_1: (93.32%) (29983/32128)\n",
      "Epoch: 90 | Batch_idx: 260 |  Loss_1: (0.1845) | Acc_1: (93.35%) (31188/33408)\n",
      "Epoch: 90 | Batch_idx: 270 |  Loss_1: (0.1849) | Acc_1: (93.33%) (32374/34688)\n",
      "Epoch: 90 | Batch_idx: 280 |  Loss_1: (0.1851) | Acc_1: (93.31%) (33560/35968)\n",
      "Epoch: 90 | Batch_idx: 290 |  Loss_1: (0.1850) | Acc_1: (93.30%) (34753/37248)\n",
      "Epoch: 90 | Batch_idx: 300 |  Loss_1: (0.1840) | Acc_1: (93.33%) (35958/38528)\n",
      "Epoch: 90 | Batch_idx: 310 |  Loss_1: (0.1842) | Acc_1: (93.32%) (37148/39808)\n",
      "Epoch: 90 | Batch_idx: 320 |  Loss_1: (0.1848) | Acc_1: (93.30%) (38336/41088)\n",
      "Epoch: 90 | Batch_idx: 330 |  Loss_1: (0.1846) | Acc_1: (93.31%) (39532/42368)\n",
      "Epoch: 90 | Batch_idx: 340 |  Loss_1: (0.1850) | Acc_1: (93.28%) (40714/43648)\n",
      "Epoch: 90 | Batch_idx: 350 |  Loss_1: (0.1842) | Acc_1: (93.30%) (41919/44928)\n",
      "Epoch: 90 | Batch_idx: 360 |  Loss_1: (0.1835) | Acc_1: (93.32%) (43121/46208)\n",
      "Epoch: 90 | Batch_idx: 370 |  Loss_1: (0.1837) | Acc_1: (93.30%) (44308/47488)\n",
      "Epoch: 90 | Batch_idx: 380 |  Loss_1: (0.1836) | Acc_1: (93.31%) (45505/48768)\n",
      "Epoch: 90 | Batch_idx: 390 |  Loss_1: (0.1829) | Acc_1: (93.33%) (46667/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2846) | Acc: (91.89%) (9189/10000)\n",
      "Epoch: 91 | Batch_idx: 0 |  Loss_1: (0.1379) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 91 | Batch_idx: 10 |  Loss_1: (0.1986) | Acc_1: (92.68%) (1305/1408)\n",
      "Epoch: 91 | Batch_idx: 20 |  Loss_1: (0.1917) | Acc_1: (93.08%) (2502/2688)\n",
      "Epoch: 91 | Batch_idx: 30 |  Loss_1: (0.1849) | Acc_1: (93.22%) (3699/3968)\n",
      "Epoch: 91 | Batch_idx: 40 |  Loss_1: (0.1824) | Acc_1: (93.39%) (4901/5248)\n",
      "Epoch: 91 | Batch_idx: 50 |  Loss_1: (0.1782) | Acc_1: (93.47%) (6102/6528)\n",
      "Epoch: 91 | Batch_idx: 60 |  Loss_1: (0.1763) | Acc_1: (93.53%) (7303/7808)\n",
      "Epoch: 91 | Batch_idx: 70 |  Loss_1: (0.1732) | Acc_1: (93.68%) (8514/9088)\n",
      "Epoch: 91 | Batch_idx: 80 |  Loss_1: (0.1739) | Acc_1: (93.70%) (9715/10368)\n",
      "Epoch: 91 | Batch_idx: 90 |  Loss_1: (0.1748) | Acc_1: (93.70%) (10914/11648)\n",
      "Epoch: 91 | Batch_idx: 100 |  Loss_1: (0.1767) | Acc_1: (93.64%) (12106/12928)\n",
      "Epoch: 91 | Batch_idx: 110 |  Loss_1: (0.1773) | Acc_1: (93.57%) (13294/14208)\n",
      "Epoch: 91 | Batch_idx: 120 |  Loss_1: (0.1756) | Acc_1: (93.61%) (14499/15488)\n",
      "Epoch: 91 | Batch_idx: 130 |  Loss_1: (0.1766) | Acc_1: (93.57%) (15690/16768)\n",
      "Epoch: 91 | Batch_idx: 140 |  Loss_1: (0.1774) | Acc_1: (93.50%) (16874/18048)\n",
      "Epoch: 91 | Batch_idx: 150 |  Loss_1: (0.1784) | Acc_1: (93.42%) (18056/19328)\n",
      "Epoch: 91 | Batch_idx: 160 |  Loss_1: (0.1800) | Acc_1: (93.36%) (19240/20608)\n",
      "Epoch: 91 | Batch_idx: 170 |  Loss_1: (0.1786) | Acc_1: (93.40%) (20444/21888)\n",
      "Epoch: 91 | Batch_idx: 180 |  Loss_1: (0.1795) | Acc_1: (93.40%) (21638/23168)\n",
      "Epoch: 91 | Batch_idx: 190 |  Loss_1: (0.1793) | Acc_1: (93.41%) (22836/24448)\n",
      "Epoch: 91 | Batch_idx: 200 |  Loss_1: (0.1795) | Acc_1: (93.40%) (24029/25728)\n",
      "Epoch: 91 | Batch_idx: 210 |  Loss_1: (0.1796) | Acc_1: (93.39%) (25224/27008)\n",
      "Epoch: 91 | Batch_idx: 220 |  Loss_1: (0.1787) | Acc_1: (93.42%) (26427/28288)\n",
      "Epoch: 91 | Batch_idx: 230 |  Loss_1: (0.1787) | Acc_1: (93.44%) (27629/29568)\n",
      "Epoch: 91 | Batch_idx: 240 |  Loss_1: (0.1790) | Acc_1: (93.42%) (28818/30848)\n",
      "Epoch: 91 | Batch_idx: 250 |  Loss_1: (0.1784) | Acc_1: (93.43%) (30016/32128)\n",
      "Epoch: 91 | Batch_idx: 260 |  Loss_1: (0.1789) | Acc_1: (93.40%) (31202/33408)\n",
      "Epoch: 91 | Batch_idx: 270 |  Loss_1: (0.1800) | Acc_1: (93.38%) (32390/34688)\n",
      "Epoch: 91 | Batch_idx: 280 |  Loss_1: (0.1807) | Acc_1: (93.36%) (33579/35968)\n",
      "Epoch: 91 | Batch_idx: 290 |  Loss_1: (0.1801) | Acc_1: (93.36%) (34776/37248)\n",
      "Epoch: 91 | Batch_idx: 300 |  Loss_1: (0.1798) | Acc_1: (93.39%) (35983/38528)\n",
      "Epoch: 91 | Batch_idx: 310 |  Loss_1: (0.1797) | Acc_1: (93.39%) (37178/39808)\n",
      "Epoch: 91 | Batch_idx: 320 |  Loss_1: (0.1796) | Acc_1: (93.39%) (38374/41088)\n",
      "Epoch: 91 | Batch_idx: 330 |  Loss_1: (0.1786) | Acc_1: (93.43%) (39585/42368)\n",
      "Epoch: 91 | Batch_idx: 340 |  Loss_1: (0.1784) | Acc_1: (93.44%) (40784/43648)\n",
      "Epoch: 91 | Batch_idx: 350 |  Loss_1: (0.1778) | Acc_1: (93.45%) (41986/44928)\n",
      "Epoch: 91 | Batch_idx: 360 |  Loss_1: (0.1781) | Acc_1: (93.43%) (43172/46208)\n",
      "Epoch: 91 | Batch_idx: 370 |  Loss_1: (0.1780) | Acc_1: (93.42%) (44365/47488)\n",
      "Epoch: 91 | Batch_idx: 380 |  Loss_1: (0.1786) | Acc_1: (93.39%) (45546/48768)\n",
      "Epoch: 91 | Batch_idx: 390 |  Loss_1: (0.1789) | Acc_1: (93.39%) (46697/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3050) | Acc: (91.67%) (9167/10000)\n",
      "Epoch: 92 | Batch_idx: 0 |  Loss_1: (0.1991) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 92 | Batch_idx: 10 |  Loss_1: (0.1911) | Acc_1: (92.76%) (1306/1408)\n",
      "Epoch: 92 | Batch_idx: 20 |  Loss_1: (0.1968) | Acc_1: (92.97%) (2499/2688)\n",
      "Epoch: 92 | Batch_idx: 30 |  Loss_1: (0.1895) | Acc_1: (93.07%) (3693/3968)\n",
      "Epoch: 92 | Batch_idx: 40 |  Loss_1: (0.1881) | Acc_1: (93.08%) (4885/5248)\n",
      "Epoch: 92 | Batch_idx: 50 |  Loss_1: (0.1839) | Acc_1: (93.31%) (6091/6528)\n",
      "Epoch: 92 | Batch_idx: 60 |  Loss_1: (0.1825) | Acc_1: (93.37%) (7290/7808)\n",
      "Epoch: 92 | Batch_idx: 70 |  Loss_1: (0.1832) | Acc_1: (93.33%) (8482/9088)\n",
      "Epoch: 92 | Batch_idx: 80 |  Loss_1: (0.1827) | Acc_1: (93.28%) (9671/10368)\n",
      "Epoch: 92 | Batch_idx: 90 |  Loss_1: (0.1831) | Acc_1: (93.25%) (10862/11648)\n",
      "Epoch: 92 | Batch_idx: 100 |  Loss_1: (0.1822) | Acc_1: (93.24%) (12054/12928)\n",
      "Epoch: 92 | Batch_idx: 110 |  Loss_1: (0.1794) | Acc_1: (93.41%) (13271/14208)\n",
      "Epoch: 92 | Batch_idx: 120 |  Loss_1: (0.1789) | Acc_1: (93.44%) (14472/15488)\n",
      "Epoch: 92 | Batch_idx: 130 |  Loss_1: (0.1792) | Acc_1: (93.43%) (15667/16768)\n",
      "Epoch: 92 | Batch_idx: 140 |  Loss_1: (0.1799) | Acc_1: (93.42%) (16860/18048)\n",
      "Epoch: 92 | Batch_idx: 150 |  Loss_1: (0.1798) | Acc_1: (93.39%) (18050/19328)\n",
      "Epoch: 92 | Batch_idx: 160 |  Loss_1: (0.1800) | Acc_1: (93.39%) (19246/20608)\n",
      "Epoch: 92 | Batch_idx: 170 |  Loss_1: (0.1800) | Acc_1: (93.39%) (20442/21888)\n",
      "Epoch: 92 | Batch_idx: 180 |  Loss_1: (0.1811) | Acc_1: (93.34%) (21625/23168)\n",
      "Epoch: 92 | Batch_idx: 190 |  Loss_1: (0.1801) | Acc_1: (93.39%) (22832/24448)\n",
      "Epoch: 92 | Batch_idx: 200 |  Loss_1: (0.1793) | Acc_1: (93.41%) (24033/25728)\n",
      "Epoch: 92 | Batch_idx: 210 |  Loss_1: (0.1789) | Acc_1: (93.41%) (25228/27008)\n",
      "Epoch: 92 | Batch_idx: 220 |  Loss_1: (0.1791) | Acc_1: (93.39%) (26418/28288)\n",
      "Epoch: 92 | Batch_idx: 230 |  Loss_1: (0.1789) | Acc_1: (93.39%) (27613/29568)\n",
      "Epoch: 92 | Batch_idx: 240 |  Loss_1: (0.1801) | Acc_1: (93.36%) (28800/30848)\n",
      "Epoch: 92 | Batch_idx: 250 |  Loss_1: (0.1816) | Acc_1: (93.30%) (29975/32128)\n",
      "Epoch: 92 | Batch_idx: 260 |  Loss_1: (0.1814) | Acc_1: (93.31%) (31173/33408)\n",
      "Epoch: 92 | Batch_idx: 270 |  Loss_1: (0.1812) | Acc_1: (93.34%) (32378/34688)\n",
      "Epoch: 92 | Batch_idx: 280 |  Loss_1: (0.1821) | Acc_1: (93.29%) (33556/35968)\n",
      "Epoch: 92 | Batch_idx: 290 |  Loss_1: (0.1818) | Acc_1: (93.31%) (34757/37248)\n",
      "Epoch: 92 | Batch_idx: 300 |  Loss_1: (0.1821) | Acc_1: (93.30%) (35948/38528)\n",
      "Epoch: 92 | Batch_idx: 310 |  Loss_1: (0.1820) | Acc_1: (93.30%) (37139/39808)\n",
      "Epoch: 92 | Batch_idx: 320 |  Loss_1: (0.1825) | Acc_1: (93.30%) (38335/41088)\n",
      "Epoch: 92 | Batch_idx: 330 |  Loss_1: (0.1829) | Acc_1: (93.29%) (39526/42368)\n",
      "Epoch: 92 | Batch_idx: 340 |  Loss_1: (0.1826) | Acc_1: (93.31%) (40728/43648)\n",
      "Epoch: 92 | Batch_idx: 350 |  Loss_1: (0.1832) | Acc_1: (93.30%) (41917/44928)\n",
      "Epoch: 92 | Batch_idx: 360 |  Loss_1: (0.1832) | Acc_1: (93.29%) (43108/46208)\n",
      "Epoch: 92 | Batch_idx: 370 |  Loss_1: (0.1830) | Acc_1: (93.31%) (44311/47488)\n",
      "Epoch: 92 | Batch_idx: 380 |  Loss_1: (0.1824) | Acc_1: (93.33%) (45513/48768)\n",
      "Epoch: 92 | Batch_idx: 390 |  Loss_1: (0.1823) | Acc_1: (93.33%) (46664/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3387) | Acc: (90.99%) (9099/10000)\n",
      "Epoch: 93 | Batch_idx: 0 |  Loss_1: (0.1992) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 93 | Batch_idx: 10 |  Loss_1: (0.1441) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 93 | Batch_idx: 20 |  Loss_1: (0.1662) | Acc_1: (94.53%) (2541/2688)\n",
      "Epoch: 93 | Batch_idx: 30 |  Loss_1: (0.1727) | Acc_1: (94.13%) (3735/3968)\n",
      "Epoch: 93 | Batch_idx: 40 |  Loss_1: (0.1645) | Acc_1: (94.32%) (4950/5248)\n",
      "Epoch: 93 | Batch_idx: 50 |  Loss_1: (0.1644) | Acc_1: (94.26%) (6153/6528)\n",
      "Epoch: 93 | Batch_idx: 60 |  Loss_1: (0.1662) | Acc_1: (94.12%) (7349/7808)\n",
      "Epoch: 93 | Batch_idx: 70 |  Loss_1: (0.1654) | Acc_1: (94.20%) (8561/9088)\n",
      "Epoch: 93 | Batch_idx: 80 |  Loss_1: (0.1679) | Acc_1: (94.06%) (9752/10368)\n",
      "Epoch: 93 | Batch_idx: 90 |  Loss_1: (0.1678) | Acc_1: (94.02%) (10952/11648)\n",
      "Epoch: 93 | Batch_idx: 100 |  Loss_1: (0.1703) | Acc_1: (93.98%) (12150/12928)\n",
      "Epoch: 93 | Batch_idx: 110 |  Loss_1: (0.1719) | Acc_1: (93.97%) (13351/14208)\n",
      "Epoch: 93 | Batch_idx: 120 |  Loss_1: (0.1733) | Acc_1: (93.91%) (14545/15488)\n",
      "Epoch: 93 | Batch_idx: 130 |  Loss_1: (0.1753) | Acc_1: (93.80%) (15729/16768)\n",
      "Epoch: 93 | Batch_idx: 140 |  Loss_1: (0.1755) | Acc_1: (93.82%) (16933/18048)\n",
      "Epoch: 93 | Batch_idx: 150 |  Loss_1: (0.1758) | Acc_1: (93.80%) (18129/19328)\n",
      "Epoch: 93 | Batch_idx: 160 |  Loss_1: (0.1777) | Acc_1: (93.75%) (19319/20608)\n",
      "Epoch: 93 | Batch_idx: 170 |  Loss_1: (0.1775) | Acc_1: (93.74%) (20518/21888)\n",
      "Epoch: 93 | Batch_idx: 180 |  Loss_1: (0.1772) | Acc_1: (93.76%) (21722/23168)\n",
      "Epoch: 93 | Batch_idx: 190 |  Loss_1: (0.1773) | Acc_1: (93.71%) (22911/24448)\n",
      "Epoch: 93 | Batch_idx: 200 |  Loss_1: (0.1769) | Acc_1: (93.72%) (24113/25728)\n",
      "Epoch: 93 | Batch_idx: 210 |  Loss_1: (0.1773) | Acc_1: (93.68%) (25301/27008)\n",
      "Epoch: 93 | Batch_idx: 220 |  Loss_1: (0.1769) | Acc_1: (93.65%) (26493/28288)\n",
      "Epoch: 93 | Batch_idx: 230 |  Loss_1: (0.1752) | Acc_1: (93.70%) (27704/29568)\n",
      "Epoch: 93 | Batch_idx: 240 |  Loss_1: (0.1748) | Acc_1: (93.72%) (28911/30848)\n",
      "Epoch: 93 | Batch_idx: 250 |  Loss_1: (0.1750) | Acc_1: (93.70%) (30104/32128)\n",
      "Epoch: 93 | Batch_idx: 260 |  Loss_1: (0.1738) | Acc_1: (93.74%) (31318/33408)\n",
      "Epoch: 93 | Batch_idx: 270 |  Loss_1: (0.1738) | Acc_1: (93.75%) (32520/34688)\n",
      "Epoch: 93 | Batch_idx: 280 |  Loss_1: (0.1742) | Acc_1: (93.75%) (33719/35968)\n",
      "Epoch: 93 | Batch_idx: 290 |  Loss_1: (0.1743) | Acc_1: (93.74%) (34915/37248)\n",
      "Epoch: 93 | Batch_idx: 300 |  Loss_1: (0.1741) | Acc_1: (93.73%) (36113/38528)\n",
      "Epoch: 93 | Batch_idx: 310 |  Loss_1: (0.1740) | Acc_1: (93.73%) (37312/39808)\n",
      "Epoch: 93 | Batch_idx: 320 |  Loss_1: (0.1751) | Acc_1: (93.67%) (38489/41088)\n",
      "Epoch: 93 | Batch_idx: 330 |  Loss_1: (0.1746) | Acc_1: (93.69%) (39694/42368)\n",
      "Epoch: 93 | Batch_idx: 340 |  Loss_1: (0.1747) | Acc_1: (93.69%) (40894/43648)\n",
      "Epoch: 93 | Batch_idx: 350 |  Loss_1: (0.1740) | Acc_1: (93.72%) (42108/44928)\n",
      "Epoch: 93 | Batch_idx: 360 |  Loss_1: (0.1739) | Acc_1: (93.73%) (43309/46208)\n",
      "Epoch: 93 | Batch_idx: 370 |  Loss_1: (0.1744) | Acc_1: (93.70%) (44496/47488)\n",
      "Epoch: 93 | Batch_idx: 380 |  Loss_1: (0.1747) | Acc_1: (93.70%) (45698/48768)\n",
      "Epoch: 93 | Batch_idx: 390 |  Loss_1: (0.1748) | Acc_1: (93.69%) (46846/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2974) | Acc: (91.85%) (9185/10000)\n",
      "Epoch: 94 | Batch_idx: 0 |  Loss_1: (0.1957) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 94 | Batch_idx: 10 |  Loss_1: (0.1733) | Acc_1: (93.11%) (1311/1408)\n",
      "Epoch: 94 | Batch_idx: 20 |  Loss_1: (0.1714) | Acc_1: (93.56%) (2515/2688)\n",
      "Epoch: 94 | Batch_idx: 30 |  Loss_1: (0.1897) | Acc_1: (92.74%) (3680/3968)\n",
      "Epoch: 94 | Batch_idx: 40 |  Loss_1: (0.1799) | Acc_1: (93.18%) (4890/5248)\n",
      "Epoch: 94 | Batch_idx: 50 |  Loss_1: (0.1725) | Acc_1: (93.40%) (6097/6528)\n",
      "Epoch: 94 | Batch_idx: 60 |  Loss_1: (0.1732) | Acc_1: (93.47%) (7298/7808)\n",
      "Epoch: 94 | Batch_idx: 70 |  Loss_1: (0.1759) | Acc_1: (93.42%) (8490/9088)\n",
      "Epoch: 94 | Batch_idx: 80 |  Loss_1: (0.1751) | Acc_1: (93.48%) (9692/10368)\n",
      "Epoch: 94 | Batch_idx: 90 |  Loss_1: (0.1746) | Acc_1: (93.57%) (10899/11648)\n",
      "Epoch: 94 | Batch_idx: 100 |  Loss_1: (0.1765) | Acc_1: (93.50%) (12088/12928)\n",
      "Epoch: 94 | Batch_idx: 110 |  Loss_1: (0.1787) | Acc_1: (93.46%) (13279/14208)\n",
      "Epoch: 94 | Batch_idx: 120 |  Loss_1: (0.1757) | Acc_1: (93.57%) (14492/15488)\n",
      "Epoch: 94 | Batch_idx: 130 |  Loss_1: (0.1764) | Acc_1: (93.54%) (15685/16768)\n",
      "Epoch: 94 | Batch_idx: 140 |  Loss_1: (0.1765) | Acc_1: (93.52%) (16879/18048)\n",
      "Epoch: 94 | Batch_idx: 150 |  Loss_1: (0.1772) | Acc_1: (93.50%) (18071/19328)\n",
      "Epoch: 94 | Batch_idx: 160 |  Loss_1: (0.1778) | Acc_1: (93.45%) (19259/20608)\n",
      "Epoch: 94 | Batch_idx: 170 |  Loss_1: (0.1783) | Acc_1: (93.42%) (20448/21888)\n",
      "Epoch: 94 | Batch_idx: 180 |  Loss_1: (0.1783) | Acc_1: (93.46%) (21652/23168)\n",
      "Epoch: 94 | Batch_idx: 190 |  Loss_1: (0.1776) | Acc_1: (93.48%) (22854/24448)\n",
      "Epoch: 94 | Batch_idx: 200 |  Loss_1: (0.1767) | Acc_1: (93.54%) (24066/25728)\n",
      "Epoch: 94 | Batch_idx: 210 |  Loss_1: (0.1762) | Acc_1: (93.55%) (25267/27008)\n",
      "Epoch: 94 | Batch_idx: 220 |  Loss_1: (0.1747) | Acc_1: (93.59%) (26476/28288)\n",
      "Epoch: 94 | Batch_idx: 230 |  Loss_1: (0.1741) | Acc_1: (93.63%) (27685/29568)\n",
      "Epoch: 94 | Batch_idx: 240 |  Loss_1: (0.1743) | Acc_1: (93.61%) (28876/30848)\n",
      "Epoch: 94 | Batch_idx: 250 |  Loss_1: (0.1744) | Acc_1: (93.60%) (30072/32128)\n",
      "Epoch: 94 | Batch_idx: 260 |  Loss_1: (0.1755) | Acc_1: (93.56%) (31256/33408)\n",
      "Epoch: 94 | Batch_idx: 270 |  Loss_1: (0.1764) | Acc_1: (93.52%) (32439/34688)\n",
      "Epoch: 94 | Batch_idx: 280 |  Loss_1: (0.1766) | Acc_1: (93.50%) (33630/35968)\n",
      "Epoch: 94 | Batch_idx: 290 |  Loss_1: (0.1761) | Acc_1: (93.53%) (34839/37248)\n",
      "Epoch: 94 | Batch_idx: 300 |  Loss_1: (0.1760) | Acc_1: (93.53%) (36036/38528)\n",
      "Epoch: 94 | Batch_idx: 310 |  Loss_1: (0.1758) | Acc_1: (93.55%) (37239/39808)\n",
      "Epoch: 94 | Batch_idx: 320 |  Loss_1: (0.1756) | Acc_1: (93.57%) (38444/41088)\n",
      "Epoch: 94 | Batch_idx: 330 |  Loss_1: (0.1750) | Acc_1: (93.59%) (39652/42368)\n",
      "Epoch: 94 | Batch_idx: 340 |  Loss_1: (0.1754) | Acc_1: (93.56%) (40839/43648)\n",
      "Epoch: 94 | Batch_idx: 350 |  Loss_1: (0.1752) | Acc_1: (93.58%) (42044/44928)\n",
      "Epoch: 94 | Batch_idx: 360 |  Loss_1: (0.1754) | Acc_1: (93.57%) (43236/46208)\n",
      "Epoch: 94 | Batch_idx: 370 |  Loss_1: (0.1750) | Acc_1: (93.57%) (44433/47488)\n",
      "Epoch: 94 | Batch_idx: 380 |  Loss_1: (0.1748) | Acc_1: (93.57%) (45634/48768)\n",
      "Epoch: 94 | Batch_idx: 390 |  Loss_1: (0.1748) | Acc_1: (93.58%) (46788/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3107) | Acc: (91.51%) (9151/10000)\n",
      "Epoch: 95 | Batch_idx: 0 |  Loss_1: (0.1422) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 95 | Batch_idx: 10 |  Loss_1: (0.1793) | Acc_1: (93.75%) (1320/1408)\n",
      "Epoch: 95 | Batch_idx: 20 |  Loss_1: (0.1834) | Acc_1: (93.42%) (2511/2688)\n",
      "Epoch: 95 | Batch_idx: 30 |  Loss_1: (0.1891) | Acc_1: (93.20%) (3698/3968)\n",
      "Epoch: 95 | Batch_idx: 40 |  Loss_1: (0.1830) | Acc_1: (93.45%) (4904/5248)\n",
      "Epoch: 95 | Batch_idx: 50 |  Loss_1: (0.1847) | Acc_1: (93.35%) (6094/6528)\n",
      "Epoch: 95 | Batch_idx: 60 |  Loss_1: (0.1829) | Acc_1: (93.37%) (7290/7808)\n",
      "Epoch: 95 | Batch_idx: 70 |  Loss_1: (0.1812) | Acc_1: (93.44%) (8492/9088)\n",
      "Epoch: 95 | Batch_idx: 80 |  Loss_1: (0.1801) | Acc_1: (93.52%) (9696/10368)\n",
      "Epoch: 95 | Batch_idx: 90 |  Loss_1: (0.1787) | Acc_1: (93.54%) (10896/11648)\n",
      "Epoch: 95 | Batch_idx: 100 |  Loss_1: (0.1772) | Acc_1: (93.58%) (12098/12928)\n",
      "Epoch: 95 | Batch_idx: 110 |  Loss_1: (0.1771) | Acc_1: (93.57%) (13294/14208)\n",
      "Epoch: 95 | Batch_idx: 120 |  Loss_1: (0.1802) | Acc_1: (93.41%) (14467/15488)\n",
      "Epoch: 95 | Batch_idx: 130 |  Loss_1: (0.1803) | Acc_1: (93.41%) (15663/16768)\n",
      "Epoch: 95 | Batch_idx: 140 |  Loss_1: (0.1816) | Acc_1: (93.35%) (16848/18048)\n",
      "Epoch: 95 | Batch_idx: 150 |  Loss_1: (0.1808) | Acc_1: (93.35%) (18042/19328)\n",
      "Epoch: 95 | Batch_idx: 160 |  Loss_1: (0.1807) | Acc_1: (93.36%) (19240/20608)\n",
      "Epoch: 95 | Batch_idx: 170 |  Loss_1: (0.1813) | Acc_1: (93.33%) (20428/21888)\n",
      "Epoch: 95 | Batch_idx: 180 |  Loss_1: (0.1795) | Acc_1: (93.42%) (21643/23168)\n",
      "Epoch: 95 | Batch_idx: 190 |  Loss_1: (0.1797) | Acc_1: (93.42%) (22840/24448)\n",
      "Epoch: 95 | Batch_idx: 200 |  Loss_1: (0.1794) | Acc_1: (93.42%) (24034/25728)\n",
      "Epoch: 95 | Batch_idx: 210 |  Loss_1: (0.1792) | Acc_1: (93.41%) (25229/27008)\n",
      "Epoch: 95 | Batch_idx: 220 |  Loss_1: (0.1801) | Acc_1: (93.36%) (26409/28288)\n",
      "Epoch: 95 | Batch_idx: 230 |  Loss_1: (0.1809) | Acc_1: (93.33%) (27596/29568)\n",
      "Epoch: 95 | Batch_idx: 240 |  Loss_1: (0.1806) | Acc_1: (93.33%) (28790/30848)\n",
      "Epoch: 95 | Batch_idx: 250 |  Loss_1: (0.1801) | Acc_1: (93.33%) (29986/32128)\n",
      "Epoch: 95 | Batch_idx: 260 |  Loss_1: (0.1797) | Acc_1: (93.35%) (31188/33408)\n",
      "Epoch: 95 | Batch_idx: 270 |  Loss_1: (0.1787) | Acc_1: (93.39%) (32395/34688)\n",
      "Epoch: 95 | Batch_idx: 280 |  Loss_1: (0.1775) | Acc_1: (93.44%) (33610/35968)\n",
      "Epoch: 95 | Batch_idx: 290 |  Loss_1: (0.1778) | Acc_1: (93.43%) (34802/37248)\n",
      "Epoch: 95 | Batch_idx: 300 |  Loss_1: (0.1773) | Acc_1: (93.45%) (36003/38528)\n",
      "Epoch: 95 | Batch_idx: 310 |  Loss_1: (0.1780) | Acc_1: (93.41%) (37184/39808)\n",
      "Epoch: 95 | Batch_idx: 320 |  Loss_1: (0.1774) | Acc_1: (93.44%) (38394/41088)\n",
      "Epoch: 95 | Batch_idx: 330 |  Loss_1: (0.1772) | Acc_1: (93.44%) (39590/42368)\n",
      "Epoch: 95 | Batch_idx: 340 |  Loss_1: (0.1767) | Acc_1: (93.46%) (40795/43648)\n",
      "Epoch: 95 | Batch_idx: 350 |  Loss_1: (0.1766) | Acc_1: (93.47%) (41995/44928)\n",
      "Epoch: 95 | Batch_idx: 360 |  Loss_1: (0.1771) | Acc_1: (93.46%) (43188/46208)\n",
      "Epoch: 95 | Batch_idx: 370 |  Loss_1: (0.1775) | Acc_1: (93.45%) (44379/47488)\n",
      "Epoch: 95 | Batch_idx: 380 |  Loss_1: (0.1778) | Acc_1: (93.45%) (45573/48768)\n",
      "Epoch: 95 | Batch_idx: 390 |  Loss_1: (0.1785) | Acc_1: (93.41%) (46706/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2946) | Acc: (91.68%) (9168/10000)\n",
      "Epoch: 96 | Batch_idx: 0 |  Loss_1: (0.0826) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 96 | Batch_idx: 10 |  Loss_1: (0.1570) | Acc_1: (94.82%) (1335/1408)\n",
      "Epoch: 96 | Batch_idx: 20 |  Loss_1: (0.1730) | Acc_1: (93.79%) (2521/2688)\n",
      "Epoch: 96 | Batch_idx: 30 |  Loss_1: (0.1737) | Acc_1: (93.65%) (3716/3968)\n",
      "Epoch: 96 | Batch_idx: 40 |  Loss_1: (0.1755) | Acc_1: (93.52%) (4908/5248)\n",
      "Epoch: 96 | Batch_idx: 50 |  Loss_1: (0.1773) | Acc_1: (93.37%) (6095/6528)\n",
      "Epoch: 96 | Batch_idx: 60 |  Loss_1: (0.1789) | Acc_1: (93.25%) (7281/7808)\n",
      "Epoch: 96 | Batch_idx: 70 |  Loss_1: (0.1752) | Acc_1: (93.34%) (8483/9088)\n",
      "Epoch: 96 | Batch_idx: 80 |  Loss_1: (0.1723) | Acc_1: (93.46%) (9690/10368)\n",
      "Epoch: 96 | Batch_idx: 90 |  Loss_1: (0.1721) | Acc_1: (93.49%) (10890/11648)\n",
      "Epoch: 96 | Batch_idx: 100 |  Loss_1: (0.1714) | Acc_1: (93.55%) (12094/12928)\n",
      "Epoch: 96 | Batch_idx: 110 |  Loss_1: (0.1728) | Acc_1: (93.53%) (13289/14208)\n",
      "Epoch: 96 | Batch_idx: 120 |  Loss_1: (0.1732) | Acc_1: (93.50%) (14482/15488)\n",
      "Epoch: 96 | Batch_idx: 130 |  Loss_1: (0.1744) | Acc_1: (93.46%) (15671/16768)\n",
      "Epoch: 96 | Batch_idx: 140 |  Loss_1: (0.1758) | Acc_1: (93.43%) (16862/18048)\n",
      "Epoch: 96 | Batch_idx: 150 |  Loss_1: (0.1745) | Acc_1: (93.48%) (18068/19328)\n",
      "Epoch: 96 | Batch_idx: 160 |  Loss_1: (0.1746) | Acc_1: (93.49%) (19266/20608)\n",
      "Epoch: 96 | Batch_idx: 170 |  Loss_1: (0.1739) | Acc_1: (93.51%) (20468/21888)\n",
      "Epoch: 96 | Batch_idx: 180 |  Loss_1: (0.1744) | Acc_1: (93.51%) (21664/23168)\n",
      "Epoch: 96 | Batch_idx: 190 |  Loss_1: (0.1749) | Acc_1: (93.50%) (22860/24448)\n",
      "Epoch: 96 | Batch_idx: 200 |  Loss_1: (0.1750) | Acc_1: (93.51%) (24059/25728)\n",
      "Epoch: 96 | Batch_idx: 210 |  Loss_1: (0.1747) | Acc_1: (93.51%) (25255/27008)\n",
      "Epoch: 96 | Batch_idx: 220 |  Loss_1: (0.1750) | Acc_1: (93.52%) (26454/28288)\n",
      "Epoch: 96 | Batch_idx: 230 |  Loss_1: (0.1770) | Acc_1: (93.45%) (27630/29568)\n",
      "Epoch: 96 | Batch_idx: 240 |  Loss_1: (0.1769) | Acc_1: (93.49%) (28839/30848)\n",
      "Epoch: 96 | Batch_idx: 250 |  Loss_1: (0.1761) | Acc_1: (93.50%) (30040/32128)\n",
      "Epoch: 96 | Batch_idx: 260 |  Loss_1: (0.1760) | Acc_1: (93.52%) (31242/33408)\n",
      "Epoch: 96 | Batch_idx: 270 |  Loss_1: (0.1763) | Acc_1: (93.51%) (32438/34688)\n",
      "Epoch: 96 | Batch_idx: 280 |  Loss_1: (0.1761) | Acc_1: (93.52%) (33636/35968)\n",
      "Epoch: 96 | Batch_idx: 290 |  Loss_1: (0.1771) | Acc_1: (93.49%) (34824/37248)\n",
      "Epoch: 96 | Batch_idx: 300 |  Loss_1: (0.1776) | Acc_1: (93.48%) (36017/38528)\n",
      "Epoch: 96 | Batch_idx: 310 |  Loss_1: (0.1786) | Acc_1: (93.44%) (37198/39808)\n",
      "Epoch: 96 | Batch_idx: 320 |  Loss_1: (0.1781) | Acc_1: (93.46%) (38399/41088)\n",
      "Epoch: 96 | Batch_idx: 330 |  Loss_1: (0.1780) | Acc_1: (93.47%) (39602/42368)\n",
      "Epoch: 96 | Batch_idx: 340 |  Loss_1: (0.1782) | Acc_1: (93.45%) (40791/43648)\n",
      "Epoch: 96 | Batch_idx: 350 |  Loss_1: (0.1783) | Acc_1: (93.45%) (41987/44928)\n",
      "Epoch: 96 | Batch_idx: 360 |  Loss_1: (0.1779) | Acc_1: (93.48%) (43193/46208)\n",
      "Epoch: 96 | Batch_idx: 370 |  Loss_1: (0.1778) | Acc_1: (93.49%) (44396/47488)\n",
      "Epoch: 96 | Batch_idx: 380 |  Loss_1: (0.1778) | Acc_1: (93.49%) (45591/48768)\n",
      "Epoch: 96 | Batch_idx: 390 |  Loss_1: (0.1790) | Acc_1: (93.43%) (46717/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3217) | Acc: (91.38%) (9138/10000)\n",
      "Epoch: 97 | Batch_idx: 0 |  Loss_1: (0.2258) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 97 | Batch_idx: 10 |  Loss_1: (0.1946) | Acc_1: (92.19%) (1298/1408)\n",
      "Epoch: 97 | Batch_idx: 20 |  Loss_1: (0.1865) | Acc_1: (92.75%) (2493/2688)\n",
      "Epoch: 97 | Batch_idx: 30 |  Loss_1: (0.1759) | Acc_1: (93.27%) (3701/3968)\n",
      "Epoch: 97 | Batch_idx: 40 |  Loss_1: (0.1746) | Acc_1: (93.41%) (4902/5248)\n",
      "Epoch: 97 | Batch_idx: 50 |  Loss_1: (0.1759) | Acc_1: (93.31%) (6091/6528)\n",
      "Epoch: 97 | Batch_idx: 60 |  Loss_1: (0.1831) | Acc_1: (93.02%) (7263/7808)\n",
      "Epoch: 97 | Batch_idx: 70 |  Loss_1: (0.1818) | Acc_1: (93.05%) (8456/9088)\n",
      "Epoch: 97 | Batch_idx: 80 |  Loss_1: (0.1786) | Acc_1: (93.26%) (9669/10368)\n",
      "Epoch: 97 | Batch_idx: 90 |  Loss_1: (0.1799) | Acc_1: (93.29%) (10867/11648)\n",
      "Epoch: 97 | Batch_idx: 100 |  Loss_1: (0.1802) | Acc_1: (93.32%) (12064/12928)\n",
      "Epoch: 97 | Batch_idx: 110 |  Loss_1: (0.1784) | Acc_1: (93.36%) (13264/14208)\n",
      "Epoch: 97 | Batch_idx: 120 |  Loss_1: (0.1767) | Acc_1: (93.43%) (14470/15488)\n",
      "Epoch: 97 | Batch_idx: 130 |  Loss_1: (0.1757) | Acc_1: (93.46%) (15671/16768)\n",
      "Epoch: 97 | Batch_idx: 140 |  Loss_1: (0.1753) | Acc_1: (93.51%) (16877/18048)\n",
      "Epoch: 97 | Batch_idx: 150 |  Loss_1: (0.1754) | Acc_1: (93.51%) (18074/19328)\n",
      "Epoch: 97 | Batch_idx: 160 |  Loss_1: (0.1731) | Acc_1: (93.63%) (19295/20608)\n",
      "Epoch: 97 | Batch_idx: 170 |  Loss_1: (0.1730) | Acc_1: (93.66%) (20500/21888)\n",
      "Epoch: 97 | Batch_idx: 180 |  Loss_1: (0.1741) | Acc_1: (93.63%) (21692/23168)\n",
      "Epoch: 97 | Batch_idx: 190 |  Loss_1: (0.1735) | Acc_1: (93.66%) (22899/24448)\n",
      "Epoch: 97 | Batch_idx: 200 |  Loss_1: (0.1732) | Acc_1: (93.68%) (24101/25728)\n",
      "Epoch: 97 | Batch_idx: 210 |  Loss_1: (0.1724) | Acc_1: (93.70%) (25306/27008)\n",
      "Epoch: 97 | Batch_idx: 220 |  Loss_1: (0.1713) | Acc_1: (93.75%) (26519/28288)\n",
      "Epoch: 97 | Batch_idx: 230 |  Loss_1: (0.1712) | Acc_1: (93.73%) (27715/29568)\n",
      "Epoch: 97 | Batch_idx: 240 |  Loss_1: (0.1700) | Acc_1: (93.76%) (28924/30848)\n",
      "Epoch: 97 | Batch_idx: 250 |  Loss_1: (0.1698) | Acc_1: (93.77%) (30128/32128)\n",
      "Epoch: 97 | Batch_idx: 260 |  Loss_1: (0.1694) | Acc_1: (93.81%) (31339/33408)\n",
      "Epoch: 97 | Batch_idx: 270 |  Loss_1: (0.1690) | Acc_1: (93.80%) (32539/34688)\n",
      "Epoch: 97 | Batch_idx: 280 |  Loss_1: (0.1695) | Acc_1: (93.79%) (33736/35968)\n",
      "Epoch: 97 | Batch_idx: 290 |  Loss_1: (0.1689) | Acc_1: (93.81%) (34941/37248)\n",
      "Epoch: 97 | Batch_idx: 300 |  Loss_1: (0.1676) | Acc_1: (93.86%) (36161/38528)\n",
      "Epoch: 97 | Batch_idx: 310 |  Loss_1: (0.1687) | Acc_1: (93.82%) (37349/39808)\n",
      "Epoch: 97 | Batch_idx: 320 |  Loss_1: (0.1697) | Acc_1: (93.79%) (38537/41088)\n",
      "Epoch: 97 | Batch_idx: 330 |  Loss_1: (0.1710) | Acc_1: (93.75%) (39721/42368)\n",
      "Epoch: 97 | Batch_idx: 340 |  Loss_1: (0.1718) | Acc_1: (93.70%) (40896/43648)\n",
      "Epoch: 97 | Batch_idx: 350 |  Loss_1: (0.1723) | Acc_1: (93.68%) (42089/44928)\n",
      "Epoch: 97 | Batch_idx: 360 |  Loss_1: (0.1724) | Acc_1: (93.69%) (43290/46208)\n",
      "Epoch: 97 | Batch_idx: 370 |  Loss_1: (0.1721) | Acc_1: (93.71%) (44501/47488)\n",
      "Epoch: 97 | Batch_idx: 380 |  Loss_1: (0.1721) | Acc_1: (93.71%) (45702/48768)\n",
      "Epoch: 97 | Batch_idx: 390 |  Loss_1: (0.1725) | Acc_1: (93.70%) (46852/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3428) | Acc: (90.83%) (9083/10000)\n",
      "Epoch: 98 | Batch_idx: 0 |  Loss_1: (0.2356) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 98 | Batch_idx: 10 |  Loss_1: (0.1770) | Acc_1: (93.75%) (1320/1408)\n",
      "Epoch: 98 | Batch_idx: 20 |  Loss_1: (0.1752) | Acc_1: (93.60%) (2516/2688)\n",
      "Epoch: 98 | Batch_idx: 30 |  Loss_1: (0.1747) | Acc_1: (93.67%) (3717/3968)\n",
      "Epoch: 98 | Batch_idx: 40 |  Loss_1: (0.1716) | Acc_1: (93.77%) (4921/5248)\n",
      "Epoch: 98 | Batch_idx: 50 |  Loss_1: (0.1736) | Acc_1: (93.64%) (6113/6528)\n",
      "Epoch: 98 | Batch_idx: 60 |  Loss_1: (0.1665) | Acc_1: (93.93%) (7334/7808)\n",
      "Epoch: 98 | Batch_idx: 70 |  Loss_1: (0.1675) | Acc_1: (93.86%) (8530/9088)\n",
      "Epoch: 98 | Batch_idx: 80 |  Loss_1: (0.1702) | Acc_1: (93.77%) (9722/10368)\n",
      "Epoch: 98 | Batch_idx: 90 |  Loss_1: (0.1714) | Acc_1: (93.79%) (10925/11648)\n",
      "Epoch: 98 | Batch_idx: 100 |  Loss_1: (0.1693) | Acc_1: (93.81%) (12128/12928)\n",
      "Epoch: 98 | Batch_idx: 110 |  Loss_1: (0.1684) | Acc_1: (93.87%) (13337/14208)\n",
      "Epoch: 98 | Batch_idx: 120 |  Loss_1: (0.1679) | Acc_1: (93.89%) (14541/15488)\n",
      "Epoch: 98 | Batch_idx: 130 |  Loss_1: (0.1659) | Acc_1: (93.93%) (15751/16768)\n",
      "Epoch: 98 | Batch_idx: 140 |  Loss_1: (0.1647) | Acc_1: (93.96%) (16957/18048)\n",
      "Epoch: 98 | Batch_idx: 150 |  Loss_1: (0.1650) | Acc_1: (93.93%) (18155/19328)\n",
      "Epoch: 98 | Batch_idx: 160 |  Loss_1: (0.1656) | Acc_1: (93.87%) (19345/20608)\n",
      "Epoch: 98 | Batch_idx: 170 |  Loss_1: (0.1643) | Acc_1: (93.91%) (20556/21888)\n",
      "Epoch: 98 | Batch_idx: 180 |  Loss_1: (0.1640) | Acc_1: (93.91%) (21757/23168)\n",
      "Epoch: 98 | Batch_idx: 190 |  Loss_1: (0.1645) | Acc_1: (93.91%) (22958/24448)\n",
      "Epoch: 98 | Batch_idx: 200 |  Loss_1: (0.1660) | Acc_1: (93.88%) (24154/25728)\n",
      "Epoch: 98 | Batch_idx: 210 |  Loss_1: (0.1687) | Acc_1: (93.79%) (25332/27008)\n",
      "Epoch: 98 | Batch_idx: 220 |  Loss_1: (0.1699) | Acc_1: (93.78%) (26529/28288)\n",
      "Epoch: 98 | Batch_idx: 230 |  Loss_1: (0.1684) | Acc_1: (93.83%) (27744/29568)\n",
      "Epoch: 98 | Batch_idx: 240 |  Loss_1: (0.1688) | Acc_1: (93.82%) (28943/30848)\n",
      "Epoch: 98 | Batch_idx: 250 |  Loss_1: (0.1702) | Acc_1: (93.77%) (30127/32128)\n",
      "Epoch: 98 | Batch_idx: 260 |  Loss_1: (0.1701) | Acc_1: (93.79%) (31335/33408)\n",
      "Epoch: 98 | Batch_idx: 270 |  Loss_1: (0.1700) | Acc_1: (93.81%) (32541/34688)\n",
      "Epoch: 98 | Batch_idx: 280 |  Loss_1: (0.1692) | Acc_1: (93.83%) (33749/35968)\n",
      "Epoch: 98 | Batch_idx: 290 |  Loss_1: (0.1697) | Acc_1: (93.81%) (34944/37248)\n",
      "Epoch: 98 | Batch_idx: 300 |  Loss_1: (0.1706) | Acc_1: (93.77%) (36127/38528)\n",
      "Epoch: 98 | Batch_idx: 310 |  Loss_1: (0.1703) | Acc_1: (93.79%) (37334/39808)\n",
      "Epoch: 98 | Batch_idx: 320 |  Loss_1: (0.1705) | Acc_1: (93.78%) (38533/41088)\n",
      "Epoch: 98 | Batch_idx: 330 |  Loss_1: (0.1707) | Acc_1: (93.77%) (39730/42368)\n",
      "Epoch: 98 | Batch_idx: 340 |  Loss_1: (0.1706) | Acc_1: (93.77%) (40930/43648)\n",
      "Epoch: 98 | Batch_idx: 350 |  Loss_1: (0.1720) | Acc_1: (93.70%) (42097/44928)\n",
      "Epoch: 98 | Batch_idx: 360 |  Loss_1: (0.1718) | Acc_1: (93.70%) (43296/46208)\n",
      "Epoch: 98 | Batch_idx: 370 |  Loss_1: (0.1720) | Acc_1: (93.68%) (44489/47488)\n",
      "Epoch: 98 | Batch_idx: 380 |  Loss_1: (0.1721) | Acc_1: (93.68%) (45687/48768)\n",
      "Epoch: 98 | Batch_idx: 390 |  Loss_1: (0.1717) | Acc_1: (93.69%) (46843/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3142) | Acc: (91.51%) (9151/10000)\n",
      "Epoch: 99 | Batch_idx: 0 |  Loss_1: (0.2025) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 99 | Batch_idx: 10 |  Loss_1: (0.1636) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 99 | Batch_idx: 20 |  Loss_1: (0.1673) | Acc_1: (93.97%) (2526/2688)\n",
      "Epoch: 99 | Batch_idx: 30 |  Loss_1: (0.1773) | Acc_1: (93.67%) (3717/3968)\n",
      "Epoch: 99 | Batch_idx: 40 |  Loss_1: (0.1748) | Acc_1: (93.67%) (4916/5248)\n",
      "Epoch: 99 | Batch_idx: 50 |  Loss_1: (0.1720) | Acc_1: (93.81%) (6124/6528)\n",
      "Epoch: 99 | Batch_idx: 60 |  Loss_1: (0.1725) | Acc_1: (93.78%) (7322/7808)\n",
      "Epoch: 99 | Batch_idx: 70 |  Loss_1: (0.1680) | Acc_1: (93.92%) (8535/9088)\n",
      "Epoch: 99 | Batch_idx: 80 |  Loss_1: (0.1700) | Acc_1: (93.89%) (9735/10368)\n",
      "Epoch: 99 | Batch_idx: 90 |  Loss_1: (0.1684) | Acc_1: (93.90%) (10937/11648)\n",
      "Epoch: 99 | Batch_idx: 100 |  Loss_1: (0.1715) | Acc_1: (93.84%) (12131/12928)\n",
      "Epoch: 99 | Batch_idx: 110 |  Loss_1: (0.1751) | Acc_1: (93.67%) (13309/14208)\n",
      "Epoch: 99 | Batch_idx: 120 |  Loss_1: (0.1760) | Acc_1: (93.61%) (14498/15488)\n",
      "Epoch: 99 | Batch_idx: 130 |  Loss_1: (0.1741) | Acc_1: (93.71%) (15713/16768)\n",
      "Epoch: 99 | Batch_idx: 140 |  Loss_1: (0.1741) | Acc_1: (93.71%) (16913/18048)\n",
      "Epoch: 99 | Batch_idx: 150 |  Loss_1: (0.1740) | Acc_1: (93.72%) (18115/19328)\n",
      "Epoch: 99 | Batch_idx: 160 |  Loss_1: (0.1743) | Acc_1: (93.70%) (19310/20608)\n",
      "Epoch: 99 | Batch_idx: 170 |  Loss_1: (0.1732) | Acc_1: (93.75%) (20519/21888)\n",
      "Epoch: 99 | Batch_idx: 180 |  Loss_1: (0.1731) | Acc_1: (93.78%) (21728/23168)\n",
      "Epoch: 99 | Batch_idx: 190 |  Loss_1: (0.1720) | Acc_1: (93.82%) (22938/24448)\n",
      "Epoch: 99 | Batch_idx: 200 |  Loss_1: (0.1726) | Acc_1: (93.79%) (24131/25728)\n",
      "Epoch: 99 | Batch_idx: 210 |  Loss_1: (0.1723) | Acc_1: (93.79%) (25332/27008)\n",
      "Epoch: 99 | Batch_idx: 220 |  Loss_1: (0.1730) | Acc_1: (93.75%) (26520/28288)\n",
      "Epoch: 99 | Batch_idx: 230 |  Loss_1: (0.1724) | Acc_1: (93.77%) (27726/29568)\n",
      "Epoch: 99 | Batch_idx: 240 |  Loss_1: (0.1726) | Acc_1: (93.76%) (28922/30848)\n",
      "Epoch: 99 | Batch_idx: 250 |  Loss_1: (0.1722) | Acc_1: (93.79%) (30133/32128)\n",
      "Epoch: 99 | Batch_idx: 260 |  Loss_1: (0.1711) | Acc_1: (93.84%) (31350/33408)\n",
      "Epoch: 99 | Batch_idx: 270 |  Loss_1: (0.1711) | Acc_1: (93.81%) (32542/34688)\n",
      "Epoch: 99 | Batch_idx: 280 |  Loss_1: (0.1710) | Acc_1: (93.82%) (33745/35968)\n",
      "Epoch: 99 | Batch_idx: 290 |  Loss_1: (0.1708) | Acc_1: (93.84%) (34955/37248)\n",
      "Epoch: 99 | Batch_idx: 300 |  Loss_1: (0.1711) | Acc_1: (93.83%) (36149/38528)\n",
      "Epoch: 99 | Batch_idx: 310 |  Loss_1: (0.1715) | Acc_1: (93.81%) (37343/39808)\n",
      "Epoch: 99 | Batch_idx: 320 |  Loss_1: (0.1715) | Acc_1: (93.80%) (38540/41088)\n",
      "Epoch: 99 | Batch_idx: 330 |  Loss_1: (0.1732) | Acc_1: (93.72%) (39706/42368)\n",
      "Epoch: 99 | Batch_idx: 340 |  Loss_1: (0.1727) | Acc_1: (93.73%) (40910/43648)\n",
      "Epoch: 99 | Batch_idx: 350 |  Loss_1: (0.1735) | Acc_1: (93.70%) (42097/44928)\n",
      "Epoch: 99 | Batch_idx: 360 |  Loss_1: (0.1735) | Acc_1: (93.70%) (43299/46208)\n",
      "Epoch: 99 | Batch_idx: 370 |  Loss_1: (0.1741) | Acc_1: (93.69%) (44491/47488)\n",
      "Epoch: 99 | Batch_idx: 380 |  Loss_1: (0.1738) | Acc_1: (93.71%) (45701/48768)\n",
      "Epoch: 99 | Batch_idx: 390 |  Loss_1: (0.1735) | Acc_1: (93.71%) (46857/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2990) | Acc: (91.64%) (9164/10000)\n",
      "Epoch: 100 | Batch_idx: 0 |  Loss_1: (0.1538) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 100 | Batch_idx: 10 |  Loss_1: (0.1625) | Acc_1: (93.47%) (1316/1408)\n",
      "Epoch: 100 | Batch_idx: 20 |  Loss_1: (0.1629) | Acc_1: (93.75%) (2520/2688)\n",
      "Epoch: 100 | Batch_idx: 30 |  Loss_1: (0.1597) | Acc_1: (93.90%) (3726/3968)\n",
      "Epoch: 100 | Batch_idx: 40 |  Loss_1: (0.1591) | Acc_1: (94.00%) (4933/5248)\n",
      "Epoch: 100 | Batch_idx: 50 |  Loss_1: (0.1665) | Acc_1: (93.73%) (6119/6528)\n",
      "Epoch: 100 | Batch_idx: 60 |  Loss_1: (0.1657) | Acc_1: (93.83%) (7326/7808)\n",
      "Epoch: 100 | Batch_idx: 70 |  Loss_1: (0.1701) | Acc_1: (93.68%) (8514/9088)\n",
      "Epoch: 100 | Batch_idx: 80 |  Loss_1: (0.1736) | Acc_1: (93.53%) (9697/10368)\n",
      "Epoch: 100 | Batch_idx: 90 |  Loss_1: (0.1743) | Acc_1: (93.54%) (10896/11648)\n",
      "Epoch: 100 | Batch_idx: 100 |  Loss_1: (0.1753) | Acc_1: (93.48%) (12085/12928)\n",
      "Epoch: 100 | Batch_idx: 110 |  Loss_1: (0.1763) | Acc_1: (93.42%) (13273/14208)\n",
      "Epoch: 100 | Batch_idx: 120 |  Loss_1: (0.1744) | Acc_1: (93.54%) (14487/15488)\n",
      "Epoch: 100 | Batch_idx: 130 |  Loss_1: (0.1738) | Acc_1: (93.58%) (15691/16768)\n",
      "Epoch: 100 | Batch_idx: 140 |  Loss_1: (0.1744) | Acc_1: (93.57%) (16888/18048)\n",
      "Epoch: 100 | Batch_idx: 150 |  Loss_1: (0.1756) | Acc_1: (93.53%) (18078/19328)\n",
      "Epoch: 100 | Batch_idx: 160 |  Loss_1: (0.1760) | Acc_1: (93.51%) (19270/20608)\n",
      "Epoch: 100 | Batch_idx: 170 |  Loss_1: (0.1756) | Acc_1: (93.54%) (20474/21888)\n",
      "Epoch: 100 | Batch_idx: 180 |  Loss_1: (0.1764) | Acc_1: (93.55%) (21673/23168)\n",
      "Epoch: 100 | Batch_idx: 190 |  Loss_1: (0.1754) | Acc_1: (93.61%) (22885/24448)\n",
      "Epoch: 100 | Batch_idx: 200 |  Loss_1: (0.1748) | Acc_1: (93.63%) (24088/25728)\n",
      "Epoch: 100 | Batch_idx: 210 |  Loss_1: (0.1736) | Acc_1: (93.67%) (25298/27008)\n",
      "Epoch: 100 | Batch_idx: 220 |  Loss_1: (0.1734) | Acc_1: (93.69%) (26503/28288)\n",
      "Epoch: 100 | Batch_idx: 230 |  Loss_1: (0.1747) | Acc_1: (93.65%) (27689/29568)\n",
      "Epoch: 100 | Batch_idx: 240 |  Loss_1: (0.1745) | Acc_1: (93.66%) (28893/30848)\n",
      "Epoch: 100 | Batch_idx: 250 |  Loss_1: (0.1749) | Acc_1: (93.64%) (30086/32128)\n",
      "Epoch: 100 | Batch_idx: 260 |  Loss_1: (0.1738) | Acc_1: (93.68%) (31298/33408)\n",
      "Epoch: 100 | Batch_idx: 270 |  Loss_1: (0.1742) | Acc_1: (93.67%) (32493/34688)\n",
      "Epoch: 100 | Batch_idx: 280 |  Loss_1: (0.1740) | Acc_1: (93.68%) (33696/35968)\n",
      "Epoch: 100 | Batch_idx: 290 |  Loss_1: (0.1735) | Acc_1: (93.70%) (34902/37248)\n",
      "Epoch: 100 | Batch_idx: 300 |  Loss_1: (0.1739) | Acc_1: (93.68%) (36092/38528)\n",
      "Epoch: 100 | Batch_idx: 310 |  Loss_1: (0.1740) | Acc_1: (93.66%) (37284/39808)\n",
      "Epoch: 100 | Batch_idx: 320 |  Loss_1: (0.1740) | Acc_1: (93.66%) (38485/41088)\n",
      "Epoch: 100 | Batch_idx: 330 |  Loss_1: (0.1741) | Acc_1: (93.66%) (39682/42368)\n",
      "Epoch: 100 | Batch_idx: 340 |  Loss_1: (0.1742) | Acc_1: (93.66%) (40882/43648)\n",
      "Epoch: 100 | Batch_idx: 350 |  Loss_1: (0.1746) | Acc_1: (93.66%) (42078/44928)\n",
      "Epoch: 100 | Batch_idx: 360 |  Loss_1: (0.1760) | Acc_1: (93.61%) (43256/46208)\n",
      "Epoch: 100 | Batch_idx: 370 |  Loss_1: (0.1759) | Acc_1: (93.63%) (44465/47488)\n",
      "Epoch: 100 | Batch_idx: 380 |  Loss_1: (0.1759) | Acc_1: (93.63%) (45661/48768)\n",
      "Epoch: 100 | Batch_idx: 390 |  Loss_1: (0.1761) | Acc_1: (93.63%) (46815/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2968) | Acc: (91.91%) (9191/10000)\n",
      "Epoch: 101 | Batch_idx: 0 |  Loss_1: (0.1300) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 101 | Batch_idx: 10 |  Loss_1: (0.1631) | Acc_1: (94.46%) (1330/1408)\n",
      "Epoch: 101 | Batch_idx: 20 |  Loss_1: (0.1653) | Acc_1: (93.97%) (2526/2688)\n",
      "Epoch: 101 | Batch_idx: 30 |  Loss_1: (0.1616) | Acc_1: (94.20%) (3738/3968)\n",
      "Epoch: 101 | Batch_idx: 40 |  Loss_1: (0.1690) | Acc_1: (93.94%) (4930/5248)\n",
      "Epoch: 101 | Batch_idx: 50 |  Loss_1: (0.1665) | Acc_1: (94.10%) (6143/6528)\n",
      "Epoch: 101 | Batch_idx: 60 |  Loss_1: (0.1711) | Acc_1: (93.90%) (7332/7808)\n",
      "Epoch: 101 | Batch_idx: 70 |  Loss_1: (0.1726) | Acc_1: (93.86%) (8530/9088)\n",
      "Epoch: 101 | Batch_idx: 80 |  Loss_1: (0.1746) | Acc_1: (93.81%) (9726/10368)\n",
      "Epoch: 101 | Batch_idx: 90 |  Loss_1: (0.1724) | Acc_1: (93.88%) (10935/11648)\n",
      "Epoch: 101 | Batch_idx: 100 |  Loss_1: (0.1705) | Acc_1: (93.96%) (12147/12928)\n",
      "Epoch: 101 | Batch_idx: 110 |  Loss_1: (0.1709) | Acc_1: (93.91%) (13343/14208)\n",
      "Epoch: 101 | Batch_idx: 120 |  Loss_1: (0.1706) | Acc_1: (93.87%) (14538/15488)\n",
      "Epoch: 101 | Batch_idx: 130 |  Loss_1: (0.1695) | Acc_1: (93.91%) (15746/16768)\n",
      "Epoch: 101 | Batch_idx: 140 |  Loss_1: (0.1706) | Acc_1: (93.86%) (16939/18048)\n",
      "Epoch: 101 | Batch_idx: 150 |  Loss_1: (0.1723) | Acc_1: (93.81%) (18132/19328)\n",
      "Epoch: 101 | Batch_idx: 160 |  Loss_1: (0.1729) | Acc_1: (93.78%) (19327/20608)\n",
      "Epoch: 101 | Batch_idx: 170 |  Loss_1: (0.1717) | Acc_1: (93.81%) (20533/21888)\n",
      "Epoch: 101 | Batch_idx: 180 |  Loss_1: (0.1707) | Acc_1: (93.84%) (21740/23168)\n",
      "Epoch: 101 | Batch_idx: 190 |  Loss_1: (0.1701) | Acc_1: (93.84%) (22942/24448)\n",
      "Epoch: 101 | Batch_idx: 200 |  Loss_1: (0.1710) | Acc_1: (93.80%) (24132/25728)\n",
      "Epoch: 101 | Batch_idx: 210 |  Loss_1: (0.1712) | Acc_1: (93.80%) (25333/27008)\n",
      "Epoch: 101 | Batch_idx: 220 |  Loss_1: (0.1727) | Acc_1: (93.76%) (26522/28288)\n",
      "Epoch: 101 | Batch_idx: 230 |  Loss_1: (0.1713) | Acc_1: (93.80%) (27734/29568)\n",
      "Epoch: 101 | Batch_idx: 240 |  Loss_1: (0.1719) | Acc_1: (93.78%) (28929/30848)\n",
      "Epoch: 101 | Batch_idx: 250 |  Loss_1: (0.1723) | Acc_1: (93.73%) (30115/32128)\n",
      "Epoch: 101 | Batch_idx: 260 |  Loss_1: (0.1726) | Acc_1: (93.74%) (31317/33408)\n",
      "Epoch: 101 | Batch_idx: 270 |  Loss_1: (0.1724) | Acc_1: (93.74%) (32516/34688)\n",
      "Epoch: 101 | Batch_idx: 280 |  Loss_1: (0.1725) | Acc_1: (93.74%) (33717/35968)\n",
      "Epoch: 101 | Batch_idx: 290 |  Loss_1: (0.1731) | Acc_1: (93.69%) (34898/37248)\n",
      "Epoch: 101 | Batch_idx: 300 |  Loss_1: (0.1742) | Acc_1: (93.65%) (36081/38528)\n",
      "Epoch: 101 | Batch_idx: 310 |  Loss_1: (0.1741) | Acc_1: (93.65%) (37282/39808)\n",
      "Epoch: 101 | Batch_idx: 320 |  Loss_1: (0.1740) | Acc_1: (93.65%) (38478/41088)\n",
      "Epoch: 101 | Batch_idx: 330 |  Loss_1: (0.1736) | Acc_1: (93.66%) (39682/42368)\n",
      "Epoch: 101 | Batch_idx: 340 |  Loss_1: (0.1744) | Acc_1: (93.63%) (40868/43648)\n",
      "Epoch: 101 | Batch_idx: 350 |  Loss_1: (0.1739) | Acc_1: (93.66%) (42079/44928)\n",
      "Epoch: 101 | Batch_idx: 360 |  Loss_1: (0.1740) | Acc_1: (93.65%) (43275/46208)\n",
      "Epoch: 101 | Batch_idx: 370 |  Loss_1: (0.1738) | Acc_1: (93.65%) (44472/47488)\n",
      "Epoch: 101 | Batch_idx: 380 |  Loss_1: (0.1743) | Acc_1: (93.64%) (45667/48768)\n",
      "Epoch: 101 | Batch_idx: 390 |  Loss_1: (0.1745) | Acc_1: (93.63%) (46817/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3014) | Acc: (91.64%) (9164/10000)\n",
      "Epoch: 102 | Batch_idx: 0 |  Loss_1: (0.1796) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 102 | Batch_idx: 10 |  Loss_1: (0.1779) | Acc_1: (93.47%) (1316/1408)\n",
      "Epoch: 102 | Batch_idx: 20 |  Loss_1: (0.1780) | Acc_1: (93.42%) (2511/2688)\n",
      "Epoch: 102 | Batch_idx: 30 |  Loss_1: (0.1750) | Acc_1: (93.60%) (3714/3968)\n",
      "Epoch: 102 | Batch_idx: 40 |  Loss_1: (0.1754) | Acc_1: (93.62%) (4913/5248)\n",
      "Epoch: 102 | Batch_idx: 50 |  Loss_1: (0.1758) | Acc_1: (93.52%) (6105/6528)\n",
      "Epoch: 102 | Batch_idx: 60 |  Loss_1: (0.1744) | Acc_1: (93.65%) (7312/7808)\n",
      "Epoch: 102 | Batch_idx: 70 |  Loss_1: (0.1708) | Acc_1: (93.85%) (8529/9088)\n",
      "Epoch: 102 | Batch_idx: 80 |  Loss_1: (0.1675) | Acc_1: (93.98%) (9744/10368)\n",
      "Epoch: 102 | Batch_idx: 90 |  Loss_1: (0.1639) | Acc_1: (94.08%) (10959/11648)\n",
      "Epoch: 102 | Batch_idx: 100 |  Loss_1: (0.1672) | Acc_1: (93.94%) (12144/12928)\n",
      "Epoch: 102 | Batch_idx: 110 |  Loss_1: (0.1669) | Acc_1: (93.93%) (13345/14208)\n",
      "Epoch: 102 | Batch_idx: 120 |  Loss_1: (0.1674) | Acc_1: (93.89%) (14541/15488)\n",
      "Epoch: 102 | Batch_idx: 130 |  Loss_1: (0.1687) | Acc_1: (93.84%) (15735/16768)\n",
      "Epoch: 102 | Batch_idx: 140 |  Loss_1: (0.1694) | Acc_1: (93.81%) (16930/18048)\n",
      "Epoch: 102 | Batch_idx: 150 |  Loss_1: (0.1690) | Acc_1: (93.82%) (18133/19328)\n",
      "Epoch: 102 | Batch_idx: 160 |  Loss_1: (0.1698) | Acc_1: (93.80%) (19330/20608)\n",
      "Epoch: 102 | Batch_idx: 170 |  Loss_1: (0.1700) | Acc_1: (93.79%) (20529/21888)\n",
      "Epoch: 102 | Batch_idx: 180 |  Loss_1: (0.1690) | Acc_1: (93.84%) (21742/23168)\n",
      "Epoch: 102 | Batch_idx: 190 |  Loss_1: (0.1693) | Acc_1: (93.84%) (22942/24448)\n",
      "Epoch: 102 | Batch_idx: 200 |  Loss_1: (0.1690) | Acc_1: (93.85%) (24145/25728)\n",
      "Epoch: 102 | Batch_idx: 210 |  Loss_1: (0.1696) | Acc_1: (93.82%) (25340/27008)\n",
      "Epoch: 102 | Batch_idx: 220 |  Loss_1: (0.1696) | Acc_1: (93.84%) (26546/28288)\n",
      "Epoch: 102 | Batch_idx: 230 |  Loss_1: (0.1685) | Acc_1: (93.90%) (27763/29568)\n",
      "Epoch: 102 | Batch_idx: 240 |  Loss_1: (0.1694) | Acc_1: (93.86%) (28954/30848)\n",
      "Epoch: 102 | Batch_idx: 250 |  Loss_1: (0.1698) | Acc_1: (93.84%) (30148/32128)\n",
      "Epoch: 102 | Batch_idx: 260 |  Loss_1: (0.1707) | Acc_1: (93.80%) (31338/33408)\n",
      "Epoch: 102 | Batch_idx: 270 |  Loss_1: (0.1707) | Acc_1: (93.81%) (32540/34688)\n",
      "Epoch: 102 | Batch_idx: 280 |  Loss_1: (0.1714) | Acc_1: (93.77%) (33726/35968)\n",
      "Epoch: 102 | Batch_idx: 290 |  Loss_1: (0.1718) | Acc_1: (93.76%) (34923/37248)\n",
      "Epoch: 102 | Batch_idx: 300 |  Loss_1: (0.1718) | Acc_1: (93.74%) (36118/38528)\n",
      "Epoch: 102 | Batch_idx: 310 |  Loss_1: (0.1711) | Acc_1: (93.77%) (37329/39808)\n",
      "Epoch: 102 | Batch_idx: 320 |  Loss_1: (0.1722) | Acc_1: (93.74%) (38514/41088)\n",
      "Epoch: 102 | Batch_idx: 330 |  Loss_1: (0.1728) | Acc_1: (93.73%) (39710/42368)\n",
      "Epoch: 102 | Batch_idx: 340 |  Loss_1: (0.1734) | Acc_1: (93.69%) (40892/43648)\n",
      "Epoch: 102 | Batch_idx: 350 |  Loss_1: (0.1735) | Acc_1: (93.69%) (42092/44928)\n",
      "Epoch: 102 | Batch_idx: 360 |  Loss_1: (0.1730) | Acc_1: (93.70%) (43298/46208)\n",
      "Epoch: 102 | Batch_idx: 370 |  Loss_1: (0.1726) | Acc_1: (93.73%) (44510/47488)\n",
      "Epoch: 102 | Batch_idx: 380 |  Loss_1: (0.1723) | Acc_1: (93.73%) (45710/48768)\n",
      "Epoch: 102 | Batch_idx: 390 |  Loss_1: (0.1712) | Acc_1: (93.77%) (46886/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3045) | Acc: (91.70%) (9170/10000)\n",
      "Epoch: 103 | Batch_idx: 0 |  Loss_1: (0.1401) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 103 | Batch_idx: 10 |  Loss_1: (0.1564) | Acc_1: (94.03%) (1324/1408)\n",
      "Epoch: 103 | Batch_idx: 20 |  Loss_1: (0.1621) | Acc_1: (93.71%) (2519/2688)\n",
      "Epoch: 103 | Batch_idx: 30 |  Loss_1: (0.1640) | Acc_1: (93.80%) (3722/3968)\n",
      "Epoch: 103 | Batch_idx: 40 |  Loss_1: (0.1713) | Acc_1: (93.60%) (4912/5248)\n",
      "Epoch: 103 | Batch_idx: 50 |  Loss_1: (0.1715) | Acc_1: (93.66%) (6114/6528)\n",
      "Epoch: 103 | Batch_idx: 60 |  Loss_1: (0.1684) | Acc_1: (93.74%) (7319/7808)\n",
      "Epoch: 103 | Batch_idx: 70 |  Loss_1: (0.1634) | Acc_1: (93.97%) (8540/9088)\n",
      "Epoch: 103 | Batch_idx: 80 |  Loss_1: (0.1660) | Acc_1: (93.83%) (9728/10368)\n",
      "Epoch: 103 | Batch_idx: 90 |  Loss_1: (0.1663) | Acc_1: (93.84%) (10930/11648)\n",
      "Epoch: 103 | Batch_idx: 100 |  Loss_1: (0.1683) | Acc_1: (93.73%) (12118/12928)\n",
      "Epoch: 103 | Batch_idx: 110 |  Loss_1: (0.1682) | Acc_1: (93.75%) (13320/14208)\n",
      "Epoch: 103 | Batch_idx: 120 |  Loss_1: (0.1666) | Acc_1: (93.81%) (14530/15488)\n",
      "Epoch: 103 | Batch_idx: 130 |  Loss_1: (0.1650) | Acc_1: (93.85%) (15737/16768)\n",
      "Epoch: 103 | Batch_idx: 140 |  Loss_1: (0.1647) | Acc_1: (93.83%) (16934/18048)\n",
      "Epoch: 103 | Batch_idx: 150 |  Loss_1: (0.1645) | Acc_1: (93.86%) (18142/19328)\n",
      "Epoch: 103 | Batch_idx: 160 |  Loss_1: (0.1646) | Acc_1: (93.83%) (19337/20608)\n",
      "Epoch: 103 | Batch_idx: 170 |  Loss_1: (0.1644) | Acc_1: (93.87%) (20547/21888)\n",
      "Epoch: 103 | Batch_idx: 180 |  Loss_1: (0.1656) | Acc_1: (93.81%) (21735/23168)\n",
      "Epoch: 103 | Batch_idx: 190 |  Loss_1: (0.1649) | Acc_1: (93.84%) (22941/24448)\n",
      "Epoch: 103 | Batch_idx: 200 |  Loss_1: (0.1643) | Acc_1: (93.85%) (24146/25728)\n",
      "Epoch: 103 | Batch_idx: 210 |  Loss_1: (0.1637) | Acc_1: (93.85%) (25347/27008)\n",
      "Epoch: 103 | Batch_idx: 220 |  Loss_1: (0.1630) | Acc_1: (93.87%) (26554/28288)\n",
      "Epoch: 103 | Batch_idx: 230 |  Loss_1: (0.1635) | Acc_1: (93.86%) (27754/29568)\n",
      "Epoch: 103 | Batch_idx: 240 |  Loss_1: (0.1640) | Acc_1: (93.85%) (28951/30848)\n",
      "Epoch: 103 | Batch_idx: 250 |  Loss_1: (0.1644) | Acc_1: (93.82%) (30144/32128)\n",
      "Epoch: 103 | Batch_idx: 260 |  Loss_1: (0.1650) | Acc_1: (93.82%) (31343/33408)\n",
      "Epoch: 103 | Batch_idx: 270 |  Loss_1: (0.1646) | Acc_1: (93.82%) (32545/34688)\n",
      "Epoch: 103 | Batch_idx: 280 |  Loss_1: (0.1651) | Acc_1: (93.81%) (33740/35968)\n",
      "Epoch: 103 | Batch_idx: 290 |  Loss_1: (0.1655) | Acc_1: (93.80%) (34937/37248)\n",
      "Epoch: 103 | Batch_idx: 300 |  Loss_1: (0.1667) | Acc_1: (93.78%) (36131/38528)\n",
      "Epoch: 103 | Batch_idx: 310 |  Loss_1: (0.1662) | Acc_1: (93.80%) (37341/39808)\n",
      "Epoch: 103 | Batch_idx: 320 |  Loss_1: (0.1657) | Acc_1: (93.82%) (38550/41088)\n",
      "Epoch: 103 | Batch_idx: 330 |  Loss_1: (0.1661) | Acc_1: (93.83%) (39752/42368)\n",
      "Epoch: 103 | Batch_idx: 340 |  Loss_1: (0.1665) | Acc_1: (93.82%) (40949/43648)\n",
      "Epoch: 103 | Batch_idx: 350 |  Loss_1: (0.1672) | Acc_1: (93.80%) (42144/44928)\n",
      "Epoch: 103 | Batch_idx: 360 |  Loss_1: (0.1669) | Acc_1: (93.81%) (43350/46208)\n",
      "Epoch: 103 | Batch_idx: 370 |  Loss_1: (0.1668) | Acc_1: (93.83%) (44556/47488)\n",
      "Epoch: 103 | Batch_idx: 380 |  Loss_1: (0.1673) | Acc_1: (93.80%) (45745/48768)\n",
      "Epoch: 103 | Batch_idx: 390 |  Loss_1: (0.1676) | Acc_1: (93.80%) (46898/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3053) | Acc: (91.95%) (9195/10000)\n",
      "Epoch: 104 | Batch_idx: 0 |  Loss_1: (0.1386) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 104 | Batch_idx: 10 |  Loss_1: (0.1225) | Acc_1: (95.24%) (1341/1408)\n",
      "Epoch: 104 | Batch_idx: 20 |  Loss_1: (0.1463) | Acc_1: (94.35%) (2536/2688)\n",
      "Epoch: 104 | Batch_idx: 30 |  Loss_1: (0.1454) | Acc_1: (94.43%) (3747/3968)\n",
      "Epoch: 104 | Batch_idx: 40 |  Loss_1: (0.1490) | Acc_1: (94.38%) (4953/5248)\n",
      "Epoch: 104 | Batch_idx: 50 |  Loss_1: (0.1501) | Acc_1: (94.52%) (6170/6528)\n",
      "Epoch: 104 | Batch_idx: 60 |  Loss_1: (0.1452) | Acc_1: (94.66%) (7391/7808)\n",
      "Epoch: 104 | Batch_idx: 70 |  Loss_1: (0.1477) | Acc_1: (94.51%) (8589/9088)\n",
      "Epoch: 104 | Batch_idx: 80 |  Loss_1: (0.1526) | Acc_1: (94.36%) (9783/10368)\n",
      "Epoch: 104 | Batch_idx: 90 |  Loss_1: (0.1544) | Acc_1: (94.27%) (10980/11648)\n",
      "Epoch: 104 | Batch_idx: 100 |  Loss_1: (0.1565) | Acc_1: (94.19%) (12177/12928)\n",
      "Epoch: 104 | Batch_idx: 110 |  Loss_1: (0.1561) | Acc_1: (94.21%) (13386/14208)\n",
      "Epoch: 104 | Batch_idx: 120 |  Loss_1: (0.1539) | Acc_1: (94.30%) (14605/15488)\n",
      "Epoch: 104 | Batch_idx: 130 |  Loss_1: (0.1549) | Acc_1: (94.31%) (15814/16768)\n",
      "Epoch: 104 | Batch_idx: 140 |  Loss_1: (0.1556) | Acc_1: (94.27%) (17014/18048)\n",
      "Epoch: 104 | Batch_idx: 150 |  Loss_1: (0.1541) | Acc_1: (94.32%) (18231/19328)\n",
      "Epoch: 104 | Batch_idx: 160 |  Loss_1: (0.1555) | Acc_1: (94.26%) (19426/20608)\n",
      "Epoch: 104 | Batch_idx: 170 |  Loss_1: (0.1548) | Acc_1: (94.31%) (20642/21888)\n",
      "Epoch: 104 | Batch_idx: 180 |  Loss_1: (0.1549) | Acc_1: (94.29%) (21846/23168)\n",
      "Epoch: 104 | Batch_idx: 190 |  Loss_1: (0.1559) | Acc_1: (94.26%) (23045/24448)\n",
      "Epoch: 104 | Batch_idx: 200 |  Loss_1: (0.1559) | Acc_1: (94.26%) (24252/25728)\n",
      "Epoch: 104 | Batch_idx: 210 |  Loss_1: (0.1571) | Acc_1: (94.22%) (25447/27008)\n",
      "Epoch: 104 | Batch_idx: 220 |  Loss_1: (0.1577) | Acc_1: (94.20%) (26647/28288)\n",
      "Epoch: 104 | Batch_idx: 230 |  Loss_1: (0.1592) | Acc_1: (94.15%) (27839/29568)\n",
      "Epoch: 104 | Batch_idx: 240 |  Loss_1: (0.1611) | Acc_1: (94.10%) (29028/30848)\n",
      "Epoch: 104 | Batch_idx: 250 |  Loss_1: (0.1612) | Acc_1: (94.09%) (30230/32128)\n",
      "Epoch: 104 | Batch_idx: 260 |  Loss_1: (0.1611) | Acc_1: (94.11%) (31440/33408)\n",
      "Epoch: 104 | Batch_idx: 270 |  Loss_1: (0.1609) | Acc_1: (94.11%) (32646/34688)\n",
      "Epoch: 104 | Batch_idx: 280 |  Loss_1: (0.1613) | Acc_1: (94.09%) (33842/35968)\n",
      "Epoch: 104 | Batch_idx: 290 |  Loss_1: (0.1617) | Acc_1: (94.08%) (35044/37248)\n",
      "Epoch: 104 | Batch_idx: 300 |  Loss_1: (0.1625) | Acc_1: (94.07%) (36242/38528)\n",
      "Epoch: 104 | Batch_idx: 310 |  Loss_1: (0.1626) | Acc_1: (94.06%) (37445/39808)\n",
      "Epoch: 104 | Batch_idx: 320 |  Loss_1: (0.1642) | Acc_1: (94.00%) (38622/41088)\n",
      "Epoch: 104 | Batch_idx: 330 |  Loss_1: (0.1638) | Acc_1: (94.00%) (39824/42368)\n",
      "Epoch: 104 | Batch_idx: 340 |  Loss_1: (0.1635) | Acc_1: (94.02%) (41039/43648)\n",
      "Epoch: 104 | Batch_idx: 350 |  Loss_1: (0.1638) | Acc_1: (94.03%) (42247/44928)\n",
      "Epoch: 104 | Batch_idx: 360 |  Loss_1: (0.1642) | Acc_1: (94.03%) (43448/46208)\n",
      "Epoch: 104 | Batch_idx: 370 |  Loss_1: (0.1642) | Acc_1: (94.03%) (44653/47488)\n",
      "Epoch: 104 | Batch_idx: 380 |  Loss_1: (0.1638) | Acc_1: (94.03%) (45857/48768)\n",
      "Epoch: 104 | Batch_idx: 390 |  Loss_1: (0.1641) | Acc_1: (94.02%) (47010/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3456) | Acc: (91.02%) (9102/10000)\n",
      "Epoch: 105 | Batch_idx: 0 |  Loss_1: (0.2119) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 105 | Batch_idx: 10 |  Loss_1: (0.1805) | Acc_1: (92.97%) (1309/1408)\n",
      "Epoch: 105 | Batch_idx: 20 |  Loss_1: (0.1787) | Acc_1: (93.45%) (2512/2688)\n",
      "Epoch: 105 | Batch_idx: 30 |  Loss_1: (0.1702) | Acc_1: (93.78%) (3721/3968)\n",
      "Epoch: 105 | Batch_idx: 40 |  Loss_1: (0.1741) | Acc_1: (93.60%) (4912/5248)\n",
      "Epoch: 105 | Batch_idx: 50 |  Loss_1: (0.1730) | Acc_1: (93.63%) (6112/6528)\n",
      "Epoch: 105 | Batch_idx: 60 |  Loss_1: (0.1738) | Acc_1: (93.65%) (7312/7808)\n",
      "Epoch: 105 | Batch_idx: 70 |  Loss_1: (0.1737) | Acc_1: (93.68%) (8514/9088)\n",
      "Epoch: 105 | Batch_idx: 80 |  Loss_1: (0.1715) | Acc_1: (93.76%) (9721/10368)\n",
      "Epoch: 105 | Batch_idx: 90 |  Loss_1: (0.1716) | Acc_1: (93.75%) (10920/11648)\n",
      "Epoch: 105 | Batch_idx: 100 |  Loss_1: (0.1708) | Acc_1: (93.82%) (12129/12928)\n",
      "Epoch: 105 | Batch_idx: 110 |  Loss_1: (0.1696) | Acc_1: (93.85%) (13334/14208)\n",
      "Epoch: 105 | Batch_idx: 120 |  Loss_1: (0.1697) | Acc_1: (93.82%) (14531/15488)\n",
      "Epoch: 105 | Batch_idx: 130 |  Loss_1: (0.1733) | Acc_1: (93.67%) (15707/16768)\n",
      "Epoch: 105 | Batch_idx: 140 |  Loss_1: (0.1757) | Acc_1: (93.63%) (16898/18048)\n",
      "Epoch: 105 | Batch_idx: 150 |  Loss_1: (0.1749) | Acc_1: (93.66%) (18103/19328)\n",
      "Epoch: 105 | Batch_idx: 160 |  Loss_1: (0.1779) | Acc_1: (93.58%) (19284/20608)\n",
      "Epoch: 105 | Batch_idx: 170 |  Loss_1: (0.1767) | Acc_1: (93.60%) (20487/21888)\n",
      "Epoch: 105 | Batch_idx: 180 |  Loss_1: (0.1765) | Acc_1: (93.59%) (21684/23168)\n",
      "Epoch: 105 | Batch_idx: 190 |  Loss_1: (0.1774) | Acc_1: (93.55%) (22871/24448)\n",
      "Epoch: 105 | Batch_idx: 200 |  Loss_1: (0.1765) | Acc_1: (93.57%) (24073/25728)\n",
      "Epoch: 105 | Batch_idx: 210 |  Loss_1: (0.1769) | Acc_1: (93.55%) (25266/27008)\n",
      "Epoch: 105 | Batch_idx: 220 |  Loss_1: (0.1777) | Acc_1: (93.53%) (26459/28288)\n",
      "Epoch: 105 | Batch_idx: 230 |  Loss_1: (0.1767) | Acc_1: (93.55%) (27661/29568)\n",
      "Epoch: 105 | Batch_idx: 240 |  Loss_1: (0.1756) | Acc_1: (93.59%) (28871/30848)\n",
      "Epoch: 105 | Batch_idx: 250 |  Loss_1: (0.1761) | Acc_1: (93.57%) (30062/32128)\n",
      "Epoch: 105 | Batch_idx: 260 |  Loss_1: (0.1752) | Acc_1: (93.59%) (31265/33408)\n",
      "Epoch: 105 | Batch_idx: 270 |  Loss_1: (0.1746) | Acc_1: (93.61%) (32470/34688)\n",
      "Epoch: 105 | Batch_idx: 280 |  Loss_1: (0.1739) | Acc_1: (93.65%) (33684/35968)\n",
      "Epoch: 105 | Batch_idx: 290 |  Loss_1: (0.1728) | Acc_1: (93.70%) (34901/37248)\n",
      "Epoch: 105 | Batch_idx: 300 |  Loss_1: (0.1722) | Acc_1: (93.72%) (36107/38528)\n",
      "Epoch: 105 | Batch_idx: 310 |  Loss_1: (0.1709) | Acc_1: (93.77%) (37327/39808)\n",
      "Epoch: 105 | Batch_idx: 320 |  Loss_1: (0.1701) | Acc_1: (93.80%) (38540/41088)\n",
      "Epoch: 105 | Batch_idx: 330 |  Loss_1: (0.1706) | Acc_1: (93.80%) (39740/42368)\n",
      "Epoch: 105 | Batch_idx: 340 |  Loss_1: (0.1706) | Acc_1: (93.79%) (40937/43648)\n",
      "Epoch: 105 | Batch_idx: 350 |  Loss_1: (0.1701) | Acc_1: (93.81%) (42147/44928)\n",
      "Epoch: 105 | Batch_idx: 360 |  Loss_1: (0.1707) | Acc_1: (93.80%) (43344/46208)\n",
      "Epoch: 105 | Batch_idx: 370 |  Loss_1: (0.1710) | Acc_1: (93.80%) (44544/47488)\n",
      "Epoch: 105 | Batch_idx: 380 |  Loss_1: (0.1721) | Acc_1: (93.75%) (45720/48768)\n",
      "Epoch: 105 | Batch_idx: 390 |  Loss_1: (0.1718) | Acc_1: (93.78%) (46890/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2838) | Acc: (91.98%) (9198/10000)\n",
      "Epoch: 106 | Batch_idx: 0 |  Loss_1: (0.0788) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 106 | Batch_idx: 10 |  Loss_1: (0.1559) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 106 | Batch_idx: 20 |  Loss_1: (0.1523) | Acc_1: (94.57%) (2542/2688)\n",
      "Epoch: 106 | Batch_idx: 30 |  Loss_1: (0.1560) | Acc_1: (94.41%) (3746/3968)\n",
      "Epoch: 106 | Batch_idx: 40 |  Loss_1: (0.1549) | Acc_1: (94.47%) (4958/5248)\n",
      "Epoch: 106 | Batch_idx: 50 |  Loss_1: (0.1519) | Acc_1: (94.53%) (6171/6528)\n",
      "Epoch: 106 | Batch_idx: 60 |  Loss_1: (0.1554) | Acc_1: (94.34%) (7366/7808)\n",
      "Epoch: 106 | Batch_idx: 70 |  Loss_1: (0.1599) | Acc_1: (94.16%) (8557/9088)\n",
      "Epoch: 106 | Batch_idx: 80 |  Loss_1: (0.1604) | Acc_1: (94.11%) (9757/10368)\n",
      "Epoch: 106 | Batch_idx: 90 |  Loss_1: (0.1574) | Acc_1: (94.20%) (10972/11648)\n",
      "Epoch: 106 | Batch_idx: 100 |  Loss_1: (0.1565) | Acc_1: (94.25%) (12184/12928)\n",
      "Epoch: 106 | Batch_idx: 110 |  Loss_1: (0.1573) | Acc_1: (94.23%) (13388/14208)\n",
      "Epoch: 106 | Batch_idx: 120 |  Loss_1: (0.1579) | Acc_1: (94.20%) (14589/15488)\n",
      "Epoch: 106 | Batch_idx: 130 |  Loss_1: (0.1560) | Acc_1: (94.26%) (15806/16768)\n",
      "Epoch: 106 | Batch_idx: 140 |  Loss_1: (0.1582) | Acc_1: (94.17%) (16995/18048)\n",
      "Epoch: 106 | Batch_idx: 150 |  Loss_1: (0.1577) | Acc_1: (94.21%) (18208/19328)\n",
      "Epoch: 106 | Batch_idx: 160 |  Loss_1: (0.1617) | Acc_1: (94.10%) (19392/20608)\n",
      "Epoch: 106 | Batch_idx: 170 |  Loss_1: (0.1621) | Acc_1: (94.08%) (20593/21888)\n",
      "Epoch: 106 | Batch_idx: 180 |  Loss_1: (0.1626) | Acc_1: (94.06%) (21791/23168)\n",
      "Epoch: 106 | Batch_idx: 190 |  Loss_1: (0.1636) | Acc_1: (94.03%) (22988/24448)\n",
      "Epoch: 106 | Batch_idx: 200 |  Loss_1: (0.1654) | Acc_1: (93.96%) (24174/25728)\n",
      "Epoch: 106 | Batch_idx: 210 |  Loss_1: (0.1651) | Acc_1: (93.99%) (25384/27008)\n",
      "Epoch: 106 | Batch_idx: 220 |  Loss_1: (0.1652) | Acc_1: (94.00%) (26591/28288)\n",
      "Epoch: 106 | Batch_idx: 230 |  Loss_1: (0.1654) | Acc_1: (93.99%) (27792/29568)\n",
      "Epoch: 106 | Batch_idx: 240 |  Loss_1: (0.1650) | Acc_1: (94.01%) (28999/30848)\n",
      "Epoch: 106 | Batch_idx: 250 |  Loss_1: (0.1643) | Acc_1: (94.02%) (30208/32128)\n",
      "Epoch: 106 | Batch_idx: 260 |  Loss_1: (0.1634) | Acc_1: (94.06%) (31423/33408)\n",
      "Epoch: 106 | Batch_idx: 270 |  Loss_1: (0.1642) | Acc_1: (94.01%) (32609/34688)\n",
      "Epoch: 106 | Batch_idx: 280 |  Loss_1: (0.1648) | Acc_1: (93.99%) (33807/35968)\n",
      "Epoch: 106 | Batch_idx: 290 |  Loss_1: (0.1645) | Acc_1: (94.01%) (35015/37248)\n",
      "Epoch: 106 | Batch_idx: 300 |  Loss_1: (0.1634) | Acc_1: (94.05%) (36235/38528)\n",
      "Epoch: 106 | Batch_idx: 310 |  Loss_1: (0.1634) | Acc_1: (94.08%) (37450/39808)\n",
      "Epoch: 106 | Batch_idx: 320 |  Loss_1: (0.1635) | Acc_1: (94.07%) (38650/41088)\n",
      "Epoch: 106 | Batch_idx: 330 |  Loss_1: (0.1632) | Acc_1: (94.07%) (39855/42368)\n",
      "Epoch: 106 | Batch_idx: 340 |  Loss_1: (0.1633) | Acc_1: (94.05%) (41053/43648)\n",
      "Epoch: 106 | Batch_idx: 350 |  Loss_1: (0.1642) | Acc_1: (94.02%) (42243/44928)\n",
      "Epoch: 106 | Batch_idx: 360 |  Loss_1: (0.1644) | Acc_1: (94.02%) (43446/46208)\n",
      "Epoch: 106 | Batch_idx: 370 |  Loss_1: (0.1635) | Acc_1: (94.04%) (44660/47488)\n",
      "Epoch: 106 | Batch_idx: 380 |  Loss_1: (0.1631) | Acc_1: (94.06%) (45871/48768)\n",
      "Epoch: 106 | Batch_idx: 390 |  Loss_1: (0.1640) | Acc_1: (94.02%) (47011/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3055) | Acc: (92.02%) (9202/10000)\n",
      "Epoch: 107 | Batch_idx: 0 |  Loss_1: (0.2263) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 107 | Batch_idx: 10 |  Loss_1: (0.2090) | Acc_1: (92.47%) (1302/1408)\n",
      "Epoch: 107 | Batch_idx: 20 |  Loss_1: (0.1828) | Acc_1: (93.30%) (2508/2688)\n",
      "Epoch: 107 | Batch_idx: 30 |  Loss_1: (0.1806) | Acc_1: (93.45%) (3708/3968)\n",
      "Epoch: 107 | Batch_idx: 40 |  Loss_1: (0.1731) | Acc_1: (93.83%) (4924/5248)\n",
      "Epoch: 107 | Batch_idx: 50 |  Loss_1: (0.1725) | Acc_1: (93.80%) (6123/6528)\n",
      "Epoch: 107 | Batch_idx: 60 |  Loss_1: (0.1706) | Acc_1: (93.80%) (7324/7808)\n",
      "Epoch: 107 | Batch_idx: 70 |  Loss_1: (0.1698) | Acc_1: (93.85%) (8529/9088)\n",
      "Epoch: 107 | Batch_idx: 80 |  Loss_1: (0.1680) | Acc_1: (93.89%) (9735/10368)\n",
      "Epoch: 107 | Batch_idx: 90 |  Loss_1: (0.1674) | Acc_1: (93.96%) (10944/11648)\n",
      "Epoch: 107 | Batch_idx: 100 |  Loss_1: (0.1650) | Acc_1: (94.04%) (12158/12928)\n",
      "Epoch: 107 | Batch_idx: 110 |  Loss_1: (0.1668) | Acc_1: (93.95%) (13348/14208)\n",
      "Epoch: 107 | Batch_idx: 120 |  Loss_1: (0.1660) | Acc_1: (93.98%) (14555/15488)\n",
      "Epoch: 107 | Batch_idx: 130 |  Loss_1: (0.1638) | Acc_1: (94.06%) (15772/16768)\n",
      "Epoch: 107 | Batch_idx: 140 |  Loss_1: (0.1636) | Acc_1: (94.10%) (16984/18048)\n",
      "Epoch: 107 | Batch_idx: 150 |  Loss_1: (0.1644) | Acc_1: (94.06%) (18179/19328)\n",
      "Epoch: 107 | Batch_idx: 160 |  Loss_1: (0.1644) | Acc_1: (94.04%) (19380/20608)\n",
      "Epoch: 107 | Batch_idx: 170 |  Loss_1: (0.1645) | Acc_1: (94.06%) (20588/21888)\n",
      "Epoch: 107 | Batch_idx: 180 |  Loss_1: (0.1657) | Acc_1: (94.00%) (21777/23168)\n",
      "Epoch: 107 | Batch_idx: 190 |  Loss_1: (0.1654) | Acc_1: (94.00%) (22980/24448)\n",
      "Epoch: 107 | Batch_idx: 200 |  Loss_1: (0.1661) | Acc_1: (93.98%) (24178/25728)\n",
      "Epoch: 107 | Batch_idx: 210 |  Loss_1: (0.1647) | Acc_1: (94.01%) (25390/27008)\n",
      "Epoch: 107 | Batch_idx: 220 |  Loss_1: (0.1638) | Acc_1: (94.03%) (26598/28288)\n",
      "Epoch: 107 | Batch_idx: 230 |  Loss_1: (0.1640) | Acc_1: (94.01%) (27796/29568)\n",
      "Epoch: 107 | Batch_idx: 240 |  Loss_1: (0.1644) | Acc_1: (94.01%) (28999/30848)\n",
      "Epoch: 107 | Batch_idx: 250 |  Loss_1: (0.1652) | Acc_1: (94.00%) (30199/32128)\n",
      "Epoch: 107 | Batch_idx: 260 |  Loss_1: (0.1641) | Acc_1: (94.04%) (31416/33408)\n",
      "Epoch: 107 | Batch_idx: 270 |  Loss_1: (0.1649) | Acc_1: (93.99%) (32603/34688)\n",
      "Epoch: 107 | Batch_idx: 280 |  Loss_1: (0.1656) | Acc_1: (93.96%) (33795/35968)\n",
      "Epoch: 107 | Batch_idx: 290 |  Loss_1: (0.1666) | Acc_1: (93.93%) (34988/37248)\n",
      "Epoch: 107 | Batch_idx: 300 |  Loss_1: (0.1676) | Acc_1: (93.90%) (36177/38528)\n",
      "Epoch: 107 | Batch_idx: 310 |  Loss_1: (0.1673) | Acc_1: (93.91%) (37383/39808)\n",
      "Epoch: 107 | Batch_idx: 320 |  Loss_1: (0.1671) | Acc_1: (93.91%) (38585/41088)\n",
      "Epoch: 107 | Batch_idx: 330 |  Loss_1: (0.1671) | Acc_1: (93.90%) (39783/42368)\n",
      "Epoch: 107 | Batch_idx: 340 |  Loss_1: (0.1674) | Acc_1: (93.89%) (40980/43648)\n",
      "Epoch: 107 | Batch_idx: 350 |  Loss_1: (0.1680) | Acc_1: (93.87%) (42175/44928)\n",
      "Epoch: 107 | Batch_idx: 360 |  Loss_1: (0.1684) | Acc_1: (93.86%) (43369/46208)\n",
      "Epoch: 107 | Batch_idx: 370 |  Loss_1: (0.1679) | Acc_1: (93.87%) (44578/47488)\n",
      "Epoch: 107 | Batch_idx: 380 |  Loss_1: (0.1673) | Acc_1: (93.90%) (45793/48768)\n",
      "Epoch: 107 | Batch_idx: 390 |  Loss_1: (0.1671) | Acc_1: (93.90%) (46952/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3340) | Acc: (91.30%) (9130/10000)\n",
      "Epoch: 108 | Batch_idx: 0 |  Loss_1: (0.1597) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 108 | Batch_idx: 10 |  Loss_1: (0.1789) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 108 | Batch_idx: 20 |  Loss_1: (0.1697) | Acc_1: (93.79%) (2521/2688)\n",
      "Epoch: 108 | Batch_idx: 30 |  Loss_1: (0.1781) | Acc_1: (93.40%) (3706/3968)\n",
      "Epoch: 108 | Batch_idx: 40 |  Loss_1: (0.1742) | Acc_1: (93.58%) (4911/5248)\n",
      "Epoch: 108 | Batch_idx: 50 |  Loss_1: (0.1727) | Acc_1: (93.64%) (6113/6528)\n",
      "Epoch: 108 | Batch_idx: 60 |  Loss_1: (0.1692) | Acc_1: (93.76%) (7321/7808)\n",
      "Epoch: 108 | Batch_idx: 70 |  Loss_1: (0.1651) | Acc_1: (93.87%) (8531/9088)\n",
      "Epoch: 108 | Batch_idx: 80 |  Loss_1: (0.1643) | Acc_1: (93.92%) (9738/10368)\n",
      "Epoch: 108 | Batch_idx: 90 |  Loss_1: (0.1616) | Acc_1: (93.99%) (10948/11648)\n",
      "Epoch: 108 | Batch_idx: 100 |  Loss_1: (0.1614) | Acc_1: (94.01%) (12154/12928)\n",
      "Epoch: 108 | Batch_idx: 110 |  Loss_1: (0.1625) | Acc_1: (94.03%) (13360/14208)\n",
      "Epoch: 108 | Batch_idx: 120 |  Loss_1: (0.1636) | Acc_1: (94.01%) (14560/15488)\n",
      "Epoch: 108 | Batch_idx: 130 |  Loss_1: (0.1625) | Acc_1: (94.04%) (15769/16768)\n",
      "Epoch: 108 | Batch_idx: 140 |  Loss_1: (0.1611) | Acc_1: (94.07%) (16977/18048)\n",
      "Epoch: 108 | Batch_idx: 150 |  Loss_1: (0.1584) | Acc_1: (94.18%) (18204/19328)\n",
      "Epoch: 108 | Batch_idx: 160 |  Loss_1: (0.1588) | Acc_1: (94.17%) (19406/20608)\n",
      "Epoch: 108 | Batch_idx: 170 |  Loss_1: (0.1609) | Acc_1: (94.10%) (20597/21888)\n",
      "Epoch: 108 | Batch_idx: 180 |  Loss_1: (0.1605) | Acc_1: (94.10%) (21800/23168)\n",
      "Epoch: 108 | Batch_idx: 190 |  Loss_1: (0.1594) | Acc_1: (94.13%) (23013/24448)\n",
      "Epoch: 108 | Batch_idx: 200 |  Loss_1: (0.1591) | Acc_1: (94.14%) (24221/25728)\n",
      "Epoch: 108 | Batch_idx: 210 |  Loss_1: (0.1587) | Acc_1: (94.15%) (25427/27008)\n",
      "Epoch: 108 | Batch_idx: 220 |  Loss_1: (0.1591) | Acc_1: (94.14%) (26631/28288)\n",
      "Epoch: 108 | Batch_idx: 230 |  Loss_1: (0.1598) | Acc_1: (94.09%) (27822/29568)\n",
      "Epoch: 108 | Batch_idx: 240 |  Loss_1: (0.1605) | Acc_1: (94.05%) (29013/30848)\n",
      "Epoch: 108 | Batch_idx: 250 |  Loss_1: (0.1615) | Acc_1: (93.99%) (30196/32128)\n",
      "Epoch: 108 | Batch_idx: 260 |  Loss_1: (0.1607) | Acc_1: (94.03%) (31415/33408)\n",
      "Epoch: 108 | Batch_idx: 270 |  Loss_1: (0.1612) | Acc_1: (94.02%) (32612/34688)\n",
      "Epoch: 108 | Batch_idx: 280 |  Loss_1: (0.1612) | Acc_1: (94.00%) (33810/35968)\n",
      "Epoch: 108 | Batch_idx: 290 |  Loss_1: (0.1621) | Acc_1: (93.99%) (35011/37248)\n",
      "Epoch: 108 | Batch_idx: 300 |  Loss_1: (0.1621) | Acc_1: (94.00%) (36218/38528)\n",
      "Epoch: 108 | Batch_idx: 310 |  Loss_1: (0.1624) | Acc_1: (94.00%) (37418/39808)\n",
      "Epoch: 108 | Batch_idx: 320 |  Loss_1: (0.1624) | Acc_1: (93.99%) (38619/41088)\n",
      "Epoch: 108 | Batch_idx: 330 |  Loss_1: (0.1624) | Acc_1: (93.99%) (39823/42368)\n",
      "Epoch: 108 | Batch_idx: 340 |  Loss_1: (0.1624) | Acc_1: (94.01%) (41035/43648)\n",
      "Epoch: 108 | Batch_idx: 350 |  Loss_1: (0.1642) | Acc_1: (93.96%) (42215/44928)\n",
      "Epoch: 108 | Batch_idx: 360 |  Loss_1: (0.1643) | Acc_1: (93.95%) (43413/46208)\n",
      "Epoch: 108 | Batch_idx: 370 |  Loss_1: (0.1654) | Acc_1: (93.92%) (44602/47488)\n",
      "Epoch: 108 | Batch_idx: 380 |  Loss_1: (0.1657) | Acc_1: (93.92%) (45805/48768)\n",
      "Epoch: 108 | Batch_idx: 390 |  Loss_1: (0.1657) | Acc_1: (93.93%) (46966/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3168) | Acc: (91.42%) (9142/10000)\n",
      "Epoch: 109 | Batch_idx: 0 |  Loss_1: (0.1021) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 109 | Batch_idx: 10 |  Loss_1: (0.1802) | Acc_1: (92.97%) (1309/1408)\n",
      "Epoch: 109 | Batch_idx: 20 |  Loss_1: (0.1664) | Acc_1: (93.71%) (2519/2688)\n",
      "Epoch: 109 | Batch_idx: 30 |  Loss_1: (0.1763) | Acc_1: (93.37%) (3705/3968)\n",
      "Epoch: 109 | Batch_idx: 40 |  Loss_1: (0.1770) | Acc_1: (93.39%) (4901/5248)\n",
      "Epoch: 109 | Batch_idx: 50 |  Loss_1: (0.1723) | Acc_1: (93.57%) (6108/6528)\n",
      "Epoch: 109 | Batch_idx: 60 |  Loss_1: (0.1679) | Acc_1: (93.72%) (7318/7808)\n",
      "Epoch: 109 | Batch_idx: 70 |  Loss_1: (0.1645) | Acc_1: (93.94%) (8537/9088)\n",
      "Epoch: 109 | Batch_idx: 80 |  Loss_1: (0.1615) | Acc_1: (94.04%) (9750/10368)\n",
      "Epoch: 109 | Batch_idx: 90 |  Loss_1: (0.1597) | Acc_1: (94.11%) (10962/11648)\n",
      "Epoch: 109 | Batch_idx: 100 |  Loss_1: (0.1593) | Acc_1: (94.13%) (12169/12928)\n",
      "Epoch: 109 | Batch_idx: 110 |  Loss_1: (0.1617) | Acc_1: (94.06%) (13364/14208)\n",
      "Epoch: 109 | Batch_idx: 120 |  Loss_1: (0.1600) | Acc_1: (94.12%) (14577/15488)\n",
      "Epoch: 109 | Batch_idx: 130 |  Loss_1: (0.1597) | Acc_1: (94.13%) (15783/16768)\n",
      "Epoch: 109 | Batch_idx: 140 |  Loss_1: (0.1606) | Acc_1: (94.09%) (16982/18048)\n",
      "Epoch: 109 | Batch_idx: 150 |  Loss_1: (0.1604) | Acc_1: (94.14%) (18195/19328)\n",
      "Epoch: 109 | Batch_idx: 160 |  Loss_1: (0.1619) | Acc_1: (94.08%) (19389/20608)\n",
      "Epoch: 109 | Batch_idx: 170 |  Loss_1: (0.1623) | Acc_1: (94.11%) (20599/21888)\n",
      "Epoch: 109 | Batch_idx: 180 |  Loss_1: (0.1628) | Acc_1: (94.08%) (21797/23168)\n",
      "Epoch: 109 | Batch_idx: 190 |  Loss_1: (0.1630) | Acc_1: (94.07%) (22998/24448)\n",
      "Epoch: 109 | Batch_idx: 200 |  Loss_1: (0.1642) | Acc_1: (94.05%) (24196/25728)\n",
      "Epoch: 109 | Batch_idx: 210 |  Loss_1: (0.1652) | Acc_1: (94.04%) (25397/27008)\n",
      "Epoch: 109 | Batch_idx: 220 |  Loss_1: (0.1649) | Acc_1: (94.04%) (26601/28288)\n",
      "Epoch: 109 | Batch_idx: 230 |  Loss_1: (0.1635) | Acc_1: (94.08%) (27817/29568)\n",
      "Epoch: 109 | Batch_idx: 240 |  Loss_1: (0.1626) | Acc_1: (94.10%) (29027/30848)\n",
      "Epoch: 109 | Batch_idx: 250 |  Loss_1: (0.1619) | Acc_1: (94.12%) (30238/32128)\n",
      "Epoch: 109 | Batch_idx: 260 |  Loss_1: (0.1615) | Acc_1: (94.14%) (31449/33408)\n",
      "Epoch: 109 | Batch_idx: 270 |  Loss_1: (0.1608) | Acc_1: (94.17%) (32665/34688)\n",
      "Epoch: 109 | Batch_idx: 280 |  Loss_1: (0.1605) | Acc_1: (94.17%) (33872/35968)\n",
      "Epoch: 109 | Batch_idx: 290 |  Loss_1: (0.1604) | Acc_1: (94.17%) (35076/37248)\n",
      "Epoch: 109 | Batch_idx: 300 |  Loss_1: (0.1621) | Acc_1: (94.11%) (36257/38528)\n",
      "Epoch: 109 | Batch_idx: 310 |  Loss_1: (0.1636) | Acc_1: (94.09%) (37455/39808)\n",
      "Epoch: 109 | Batch_idx: 320 |  Loss_1: (0.1635) | Acc_1: (94.10%) (38664/41088)\n",
      "Epoch: 109 | Batch_idx: 330 |  Loss_1: (0.1646) | Acc_1: (94.06%) (39853/42368)\n",
      "Epoch: 109 | Batch_idx: 340 |  Loss_1: (0.1650) | Acc_1: (94.04%) (41048/43648)\n",
      "Epoch: 109 | Batch_idx: 350 |  Loss_1: (0.1659) | Acc_1: (94.00%) (42234/44928)\n",
      "Epoch: 109 | Batch_idx: 360 |  Loss_1: (0.1648) | Acc_1: (94.05%) (43458/46208)\n",
      "Epoch: 109 | Batch_idx: 370 |  Loss_1: (0.1651) | Acc_1: (94.03%) (44654/47488)\n",
      "Epoch: 109 | Batch_idx: 380 |  Loss_1: (0.1653) | Acc_1: (94.03%) (45856/48768)\n",
      "Epoch: 109 | Batch_idx: 390 |  Loss_1: (0.1647) | Acc_1: (94.05%) (47024/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3075) | Acc: (91.88%) (9188/10000)\n",
      "Epoch: 110 | Batch_idx: 0 |  Loss_1: (0.1166) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 110 | Batch_idx: 10 |  Loss_1: (0.1191) | Acc_1: (95.45%) (1344/1408)\n",
      "Epoch: 110 | Batch_idx: 20 |  Loss_1: (0.1254) | Acc_1: (95.42%) (2565/2688)\n",
      "Epoch: 110 | Batch_idx: 30 |  Loss_1: (0.1435) | Acc_1: (94.83%) (3763/3968)\n",
      "Epoch: 110 | Batch_idx: 40 |  Loss_1: (0.1449) | Acc_1: (94.61%) (4965/5248)\n",
      "Epoch: 110 | Batch_idx: 50 |  Loss_1: (0.1485) | Acc_1: (94.53%) (6171/6528)\n",
      "Epoch: 110 | Batch_idx: 60 |  Loss_1: (0.1479) | Acc_1: (94.54%) (7382/7808)\n",
      "Epoch: 110 | Batch_idx: 70 |  Loss_1: (0.1492) | Acc_1: (94.56%) (8594/9088)\n",
      "Epoch: 110 | Batch_idx: 80 |  Loss_1: (0.1518) | Acc_1: (94.53%) (9801/10368)\n",
      "Epoch: 110 | Batch_idx: 90 |  Loss_1: (0.1518) | Acc_1: (94.51%) (11008/11648)\n",
      "Epoch: 110 | Batch_idx: 100 |  Loss_1: (0.1496) | Acc_1: (94.59%) (12228/12928)\n",
      "Epoch: 110 | Batch_idx: 110 |  Loss_1: (0.1490) | Acc_1: (94.64%) (13446/14208)\n",
      "Epoch: 110 | Batch_idx: 120 |  Loss_1: (0.1494) | Acc_1: (94.58%) (14648/15488)\n",
      "Epoch: 110 | Batch_idx: 130 |  Loss_1: (0.1500) | Acc_1: (94.54%) (15852/16768)\n",
      "Epoch: 110 | Batch_idx: 140 |  Loss_1: (0.1524) | Acc_1: (94.43%) (17043/18048)\n",
      "Epoch: 110 | Batch_idx: 150 |  Loss_1: (0.1545) | Acc_1: (94.36%) (18237/19328)\n",
      "Epoch: 110 | Batch_idx: 160 |  Loss_1: (0.1528) | Acc_1: (94.42%) (19459/20608)\n",
      "Epoch: 110 | Batch_idx: 170 |  Loss_1: (0.1545) | Acc_1: (94.35%) (20652/21888)\n",
      "Epoch: 110 | Batch_idx: 180 |  Loss_1: (0.1538) | Acc_1: (94.38%) (21865/23168)\n",
      "Epoch: 110 | Batch_idx: 190 |  Loss_1: (0.1541) | Acc_1: (94.36%) (23068/24448)\n",
      "Epoch: 110 | Batch_idx: 200 |  Loss_1: (0.1547) | Acc_1: (94.31%) (24264/25728)\n",
      "Epoch: 110 | Batch_idx: 210 |  Loss_1: (0.1553) | Acc_1: (94.29%) (25467/27008)\n",
      "Epoch: 110 | Batch_idx: 220 |  Loss_1: (0.1545) | Acc_1: (94.36%) (26693/28288)\n",
      "Epoch: 110 | Batch_idx: 230 |  Loss_1: (0.1537) | Acc_1: (94.38%) (27905/29568)\n",
      "Epoch: 110 | Batch_idx: 240 |  Loss_1: (0.1536) | Acc_1: (94.39%) (29116/30848)\n",
      "Epoch: 110 | Batch_idx: 250 |  Loss_1: (0.1546) | Acc_1: (94.35%) (30312/32128)\n",
      "Epoch: 110 | Batch_idx: 260 |  Loss_1: (0.1552) | Acc_1: (94.32%) (31510/33408)\n",
      "Epoch: 110 | Batch_idx: 270 |  Loss_1: (0.1553) | Acc_1: (94.31%) (32715/34688)\n",
      "Epoch: 110 | Batch_idx: 280 |  Loss_1: (0.1556) | Acc_1: (94.31%) (33921/35968)\n",
      "Epoch: 110 | Batch_idx: 290 |  Loss_1: (0.1554) | Acc_1: (94.33%) (35136/37248)\n",
      "Epoch: 110 | Batch_idx: 300 |  Loss_1: (0.1563) | Acc_1: (94.32%) (36338/38528)\n",
      "Epoch: 110 | Batch_idx: 310 |  Loss_1: (0.1558) | Acc_1: (94.34%) (37554/39808)\n",
      "Epoch: 110 | Batch_idx: 320 |  Loss_1: (0.1563) | Acc_1: (94.30%) (38745/41088)\n",
      "Epoch: 110 | Batch_idx: 330 |  Loss_1: (0.1568) | Acc_1: (94.28%) (39943/42368)\n",
      "Epoch: 110 | Batch_idx: 340 |  Loss_1: (0.1570) | Acc_1: (94.27%) (41147/43648)\n",
      "Epoch: 110 | Batch_idx: 350 |  Loss_1: (0.1570) | Acc_1: (94.27%) (42353/44928)\n",
      "Epoch: 110 | Batch_idx: 360 |  Loss_1: (0.1576) | Acc_1: (94.24%) (43547/46208)\n",
      "Epoch: 110 | Batch_idx: 370 |  Loss_1: (0.1574) | Acc_1: (94.25%) (44759/47488)\n",
      "Epoch: 110 | Batch_idx: 380 |  Loss_1: (0.1578) | Acc_1: (94.24%) (45958/48768)\n",
      "Epoch: 110 | Batch_idx: 390 |  Loss_1: (0.1587) | Acc_1: (94.20%) (47100/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3457) | Acc: (91.25%) (9125/10000)\n",
      "Epoch: 111 | Batch_idx: 0 |  Loss_1: (0.1870) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 111 | Batch_idx: 10 |  Loss_1: (0.1937) | Acc_1: (93.11%) (1311/1408)\n",
      "Epoch: 111 | Batch_idx: 20 |  Loss_1: (0.1770) | Acc_1: (93.30%) (2508/2688)\n",
      "Epoch: 111 | Batch_idx: 30 |  Loss_1: (0.1698) | Acc_1: (93.55%) (3712/3968)\n",
      "Epoch: 111 | Batch_idx: 40 |  Loss_1: (0.1652) | Acc_1: (93.69%) (4917/5248)\n",
      "Epoch: 111 | Batch_idx: 50 |  Loss_1: (0.1659) | Acc_1: (93.70%) (6117/6528)\n",
      "Epoch: 111 | Batch_idx: 60 |  Loss_1: (0.1625) | Acc_1: (93.89%) (7331/7808)\n",
      "Epoch: 111 | Batch_idx: 70 |  Loss_1: (0.1642) | Acc_1: (93.87%) (8531/9088)\n",
      "Epoch: 111 | Batch_idx: 80 |  Loss_1: (0.1642) | Acc_1: (93.95%) (9741/10368)\n",
      "Epoch: 111 | Batch_idx: 90 |  Loss_1: (0.1649) | Acc_1: (93.93%) (10941/11648)\n",
      "Epoch: 111 | Batch_idx: 100 |  Loss_1: (0.1630) | Acc_1: (94.01%) (12154/12928)\n",
      "Epoch: 111 | Batch_idx: 110 |  Loss_1: (0.1611) | Acc_1: (94.07%) (13366/14208)\n",
      "Epoch: 111 | Batch_idx: 120 |  Loss_1: (0.1611) | Acc_1: (94.04%) (14565/15488)\n",
      "Epoch: 111 | Batch_idx: 130 |  Loss_1: (0.1601) | Acc_1: (94.05%) (15771/16768)\n",
      "Epoch: 111 | Batch_idx: 140 |  Loss_1: (0.1581) | Acc_1: (94.14%) (16990/18048)\n",
      "Epoch: 111 | Batch_idx: 150 |  Loss_1: (0.1577) | Acc_1: (94.15%) (18198/19328)\n",
      "Epoch: 111 | Batch_idx: 160 |  Loss_1: (0.1581) | Acc_1: (94.13%) (19398/20608)\n",
      "Epoch: 111 | Batch_idx: 170 |  Loss_1: (0.1607) | Acc_1: (94.02%) (20580/21888)\n",
      "Epoch: 111 | Batch_idx: 180 |  Loss_1: (0.1626) | Acc_1: (93.98%) (21774/23168)\n",
      "Epoch: 111 | Batch_idx: 190 |  Loss_1: (0.1628) | Acc_1: (93.98%) (22976/24448)\n",
      "Epoch: 111 | Batch_idx: 200 |  Loss_1: (0.1623) | Acc_1: (94.01%) (24187/25728)\n",
      "Epoch: 111 | Batch_idx: 210 |  Loss_1: (0.1620) | Acc_1: (94.02%) (25394/27008)\n",
      "Epoch: 111 | Batch_idx: 220 |  Loss_1: (0.1617) | Acc_1: (94.03%) (26600/28288)\n",
      "Epoch: 111 | Batch_idx: 230 |  Loss_1: (0.1629) | Acc_1: (94.00%) (27793/29568)\n",
      "Epoch: 111 | Batch_idx: 240 |  Loss_1: (0.1638) | Acc_1: (93.94%) (28980/30848)\n",
      "Epoch: 111 | Batch_idx: 250 |  Loss_1: (0.1637) | Acc_1: (93.94%) (30181/32128)\n",
      "Epoch: 111 | Batch_idx: 260 |  Loss_1: (0.1629) | Acc_1: (93.99%) (31399/33408)\n",
      "Epoch: 111 | Batch_idx: 270 |  Loss_1: (0.1612) | Acc_1: (94.05%) (32625/34688)\n",
      "Epoch: 111 | Batch_idx: 280 |  Loss_1: (0.1618) | Acc_1: (94.03%) (33821/35968)\n",
      "Epoch: 111 | Batch_idx: 290 |  Loss_1: (0.1613) | Acc_1: (94.03%) (35026/37248)\n",
      "Epoch: 111 | Batch_idx: 300 |  Loss_1: (0.1615) | Acc_1: (94.04%) (36231/38528)\n",
      "Epoch: 111 | Batch_idx: 310 |  Loss_1: (0.1622) | Acc_1: (94.02%) (37428/39808)\n",
      "Epoch: 111 | Batch_idx: 320 |  Loss_1: (0.1622) | Acc_1: (94.03%) (38635/41088)\n",
      "Epoch: 111 | Batch_idx: 330 |  Loss_1: (0.1622) | Acc_1: (94.04%) (39843/42368)\n",
      "Epoch: 111 | Batch_idx: 340 |  Loss_1: (0.1617) | Acc_1: (94.06%) (41055/43648)\n",
      "Epoch: 111 | Batch_idx: 350 |  Loss_1: (0.1618) | Acc_1: (94.06%) (42260/44928)\n",
      "Epoch: 111 | Batch_idx: 360 |  Loss_1: (0.1609) | Acc_1: (94.09%) (43478/46208)\n",
      "Epoch: 111 | Batch_idx: 370 |  Loss_1: (0.1611) | Acc_1: (94.07%) (44672/47488)\n",
      "Epoch: 111 | Batch_idx: 380 |  Loss_1: (0.1607) | Acc_1: (94.09%) (45887/48768)\n",
      "Epoch: 111 | Batch_idx: 390 |  Loss_1: (0.1602) | Acc_1: (94.11%) (47055/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3406) | Acc: (91.85%) (9185/10000)\n",
      "Epoch: 112 | Batch_idx: 0 |  Loss_1: (0.1617) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 112 | Batch_idx: 10 |  Loss_1: (0.1721) | Acc_1: (93.89%) (1322/1408)\n",
      "Epoch: 112 | Batch_idx: 20 |  Loss_1: (0.1675) | Acc_1: (93.75%) (2520/2688)\n",
      "Epoch: 112 | Batch_idx: 30 |  Loss_1: (0.1616) | Acc_1: (94.13%) (3735/3968)\n",
      "Epoch: 112 | Batch_idx: 40 |  Loss_1: (0.1600) | Acc_1: (94.17%) (4942/5248)\n",
      "Epoch: 112 | Batch_idx: 50 |  Loss_1: (0.1621) | Acc_1: (94.10%) (6143/6528)\n",
      "Epoch: 112 | Batch_idx: 60 |  Loss_1: (0.1660) | Acc_1: (93.99%) (7339/7808)\n",
      "Epoch: 112 | Batch_idx: 70 |  Loss_1: (0.1636) | Acc_1: (94.08%) (8550/9088)\n",
      "Epoch: 112 | Batch_idx: 80 |  Loss_1: (0.1659) | Acc_1: (93.94%) (9740/10368)\n",
      "Epoch: 112 | Batch_idx: 90 |  Loss_1: (0.1639) | Acc_1: (93.97%) (10946/11648)\n",
      "Epoch: 112 | Batch_idx: 100 |  Loss_1: (0.1645) | Acc_1: (93.97%) (12149/12928)\n",
      "Epoch: 112 | Batch_idx: 110 |  Loss_1: (0.1629) | Acc_1: (94.02%) (13358/14208)\n",
      "Epoch: 112 | Batch_idx: 120 |  Loss_1: (0.1636) | Acc_1: (94.03%) (14563/15488)\n",
      "Epoch: 112 | Batch_idx: 130 |  Loss_1: (0.1641) | Acc_1: (94.00%) (15762/16768)\n",
      "Epoch: 112 | Batch_idx: 140 |  Loss_1: (0.1642) | Acc_1: (93.99%) (16963/18048)\n",
      "Epoch: 112 | Batch_idx: 150 |  Loss_1: (0.1631) | Acc_1: (94.03%) (18175/19328)\n",
      "Epoch: 112 | Batch_idx: 160 |  Loss_1: (0.1633) | Acc_1: (94.02%) (19376/20608)\n",
      "Epoch: 112 | Batch_idx: 170 |  Loss_1: (0.1638) | Acc_1: (94.01%) (20576/21888)\n",
      "Epoch: 112 | Batch_idx: 180 |  Loss_1: (0.1627) | Acc_1: (94.10%) (21800/23168)\n",
      "Epoch: 112 | Batch_idx: 190 |  Loss_1: (0.1616) | Acc_1: (94.15%) (23017/24448)\n",
      "Epoch: 112 | Batch_idx: 200 |  Loss_1: (0.1614) | Acc_1: (94.14%) (24221/25728)\n",
      "Epoch: 112 | Batch_idx: 210 |  Loss_1: (0.1616) | Acc_1: (94.12%) (25419/27008)\n",
      "Epoch: 112 | Batch_idx: 220 |  Loss_1: (0.1609) | Acc_1: (94.17%) (26639/28288)\n",
      "Epoch: 112 | Batch_idx: 230 |  Loss_1: (0.1607) | Acc_1: (94.18%) (27848/29568)\n",
      "Epoch: 112 | Batch_idx: 240 |  Loss_1: (0.1615) | Acc_1: (94.13%) (29037/30848)\n",
      "Epoch: 112 | Batch_idx: 250 |  Loss_1: (0.1614) | Acc_1: (94.15%) (30247/32128)\n",
      "Epoch: 112 | Batch_idx: 260 |  Loss_1: (0.1608) | Acc_1: (94.16%) (31457/33408)\n",
      "Epoch: 112 | Batch_idx: 270 |  Loss_1: (0.1612) | Acc_1: (94.16%) (32661/34688)\n",
      "Epoch: 112 | Batch_idx: 280 |  Loss_1: (0.1609) | Acc_1: (94.17%) (33871/35968)\n",
      "Epoch: 112 | Batch_idx: 290 |  Loss_1: (0.1605) | Acc_1: (94.19%) (35084/37248)\n",
      "Epoch: 112 | Batch_idx: 300 |  Loss_1: (0.1609) | Acc_1: (94.20%) (36292/38528)\n",
      "Epoch: 112 | Batch_idx: 310 |  Loss_1: (0.1610) | Acc_1: (94.21%) (37505/39808)\n",
      "Epoch: 112 | Batch_idx: 320 |  Loss_1: (0.1616) | Acc_1: (94.19%) (38701/41088)\n",
      "Epoch: 112 | Batch_idx: 330 |  Loss_1: (0.1615) | Acc_1: (94.21%) (39915/42368)\n",
      "Epoch: 112 | Batch_idx: 340 |  Loss_1: (0.1614) | Acc_1: (94.21%) (41119/43648)\n",
      "Epoch: 112 | Batch_idx: 350 |  Loss_1: (0.1621) | Acc_1: (94.17%) (42308/44928)\n",
      "Epoch: 112 | Batch_idx: 360 |  Loss_1: (0.1617) | Acc_1: (94.19%) (43525/46208)\n",
      "Epoch: 112 | Batch_idx: 370 |  Loss_1: (0.1618) | Acc_1: (94.19%) (44730/47488)\n",
      "Epoch: 112 | Batch_idx: 380 |  Loss_1: (0.1625) | Acc_1: (94.14%) (45912/48768)\n",
      "Epoch: 112 | Batch_idx: 390 |  Loss_1: (0.1632) | Acc_1: (94.13%) (47065/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3572) | Acc: (91.34%) (9134/10000)\n",
      "Epoch: 113 | Batch_idx: 0 |  Loss_1: (0.2129) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 113 | Batch_idx: 10 |  Loss_1: (0.1539) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 113 | Batch_idx: 20 |  Loss_1: (0.1634) | Acc_1: (94.23%) (2533/2688)\n",
      "Epoch: 113 | Batch_idx: 30 |  Loss_1: (0.1736) | Acc_1: (93.70%) (3718/3968)\n",
      "Epoch: 113 | Batch_idx: 40 |  Loss_1: (0.1556) | Acc_1: (94.34%) (4951/5248)\n",
      "Epoch: 113 | Batch_idx: 50 |  Loss_1: (0.1530) | Acc_1: (94.44%) (6165/6528)\n",
      "Epoch: 113 | Batch_idx: 60 |  Loss_1: (0.1554) | Acc_1: (94.39%) (7370/7808)\n",
      "Epoch: 113 | Batch_idx: 70 |  Loss_1: (0.1589) | Acc_1: (94.23%) (8564/9088)\n",
      "Epoch: 113 | Batch_idx: 80 |  Loss_1: (0.1621) | Acc_1: (94.06%) (9752/10368)\n",
      "Epoch: 113 | Batch_idx: 90 |  Loss_1: (0.1595) | Acc_1: (94.14%) (10966/11648)\n",
      "Epoch: 113 | Batch_idx: 100 |  Loss_1: (0.1586) | Acc_1: (94.18%) (12175/12928)\n",
      "Epoch: 113 | Batch_idx: 110 |  Loss_1: (0.1584) | Acc_1: (94.17%) (13379/14208)\n",
      "Epoch: 113 | Batch_idx: 120 |  Loss_1: (0.1561) | Acc_1: (94.27%) (14601/15488)\n",
      "Epoch: 113 | Batch_idx: 130 |  Loss_1: (0.1583) | Acc_1: (94.16%) (15789/16768)\n",
      "Epoch: 113 | Batch_idx: 140 |  Loss_1: (0.1599) | Acc_1: (94.12%) (16987/18048)\n",
      "Epoch: 113 | Batch_idx: 150 |  Loss_1: (0.1621) | Acc_1: (94.01%) (18170/19328)\n",
      "Epoch: 113 | Batch_idx: 160 |  Loss_1: (0.1612) | Acc_1: (94.01%) (19374/20608)\n",
      "Epoch: 113 | Batch_idx: 170 |  Loss_1: (0.1609) | Acc_1: (94.05%) (20586/21888)\n",
      "Epoch: 113 | Batch_idx: 180 |  Loss_1: (0.1602) | Acc_1: (94.08%) (21797/23168)\n",
      "Epoch: 113 | Batch_idx: 190 |  Loss_1: (0.1593) | Acc_1: (94.14%) (23016/24448)\n",
      "Epoch: 113 | Batch_idx: 200 |  Loss_1: (0.1588) | Acc_1: (94.16%) (24225/25728)\n",
      "Epoch: 113 | Batch_idx: 210 |  Loss_1: (0.1602) | Acc_1: (94.11%) (25418/27008)\n",
      "Epoch: 113 | Batch_idx: 220 |  Loss_1: (0.1598) | Acc_1: (94.11%) (26623/28288)\n",
      "Epoch: 113 | Batch_idx: 230 |  Loss_1: (0.1600) | Acc_1: (94.12%) (27829/29568)\n",
      "Epoch: 113 | Batch_idx: 240 |  Loss_1: (0.1605) | Acc_1: (94.09%) (29026/30848)\n",
      "Epoch: 113 | Batch_idx: 250 |  Loss_1: (0.1614) | Acc_1: (94.06%) (30218/32128)\n",
      "Epoch: 113 | Batch_idx: 260 |  Loss_1: (0.1613) | Acc_1: (94.04%) (31418/33408)\n",
      "Epoch: 113 | Batch_idx: 270 |  Loss_1: (0.1611) | Acc_1: (94.06%) (32628/34688)\n",
      "Epoch: 113 | Batch_idx: 280 |  Loss_1: (0.1624) | Acc_1: (94.02%) (33818/35968)\n",
      "Epoch: 113 | Batch_idx: 290 |  Loss_1: (0.1620) | Acc_1: (94.04%) (35028/37248)\n",
      "Epoch: 113 | Batch_idx: 300 |  Loss_1: (0.1625) | Acc_1: (94.03%) (36226/38528)\n",
      "Epoch: 113 | Batch_idx: 310 |  Loss_1: (0.1625) | Acc_1: (94.01%) (37424/39808)\n",
      "Epoch: 113 | Batch_idx: 320 |  Loss_1: (0.1631) | Acc_1: (93.99%) (38618/41088)\n",
      "Epoch: 113 | Batch_idx: 330 |  Loss_1: (0.1631) | Acc_1: (93.98%) (39818/42368)\n",
      "Epoch: 113 | Batch_idx: 340 |  Loss_1: (0.1631) | Acc_1: (93.97%) (41016/43648)\n",
      "Epoch: 113 | Batch_idx: 350 |  Loss_1: (0.1632) | Acc_1: (93.96%) (42213/44928)\n",
      "Epoch: 113 | Batch_idx: 360 |  Loss_1: (0.1639) | Acc_1: (93.94%) (43407/46208)\n",
      "Epoch: 113 | Batch_idx: 370 |  Loss_1: (0.1636) | Acc_1: (93.96%) (44621/47488)\n",
      "Epoch: 113 | Batch_idx: 380 |  Loss_1: (0.1640) | Acc_1: (93.96%) (45821/48768)\n",
      "Epoch: 113 | Batch_idx: 390 |  Loss_1: (0.1642) | Acc_1: (93.96%) (46978/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3022) | Acc: (92.19%) (9219/10000)\n",
      "Epoch: 114 | Batch_idx: 0 |  Loss_1: (0.1794) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 114 | Batch_idx: 10 |  Loss_1: (0.1508) | Acc_1: (94.25%) (1327/1408)\n",
      "Epoch: 114 | Batch_idx: 20 |  Loss_1: (0.1462) | Acc_1: (94.42%) (2538/2688)\n",
      "Epoch: 114 | Batch_idx: 30 |  Loss_1: (0.1439) | Acc_1: (94.66%) (3756/3968)\n",
      "Epoch: 114 | Batch_idx: 40 |  Loss_1: (0.1547) | Acc_1: (94.26%) (4947/5248)\n",
      "Epoch: 114 | Batch_idx: 50 |  Loss_1: (0.1535) | Acc_1: (94.32%) (6157/6528)\n",
      "Epoch: 114 | Batch_idx: 60 |  Loss_1: (0.1557) | Acc_1: (94.25%) (7359/7808)\n",
      "Epoch: 114 | Batch_idx: 70 |  Loss_1: (0.1590) | Acc_1: (94.12%) (8554/9088)\n",
      "Epoch: 114 | Batch_idx: 80 |  Loss_1: (0.1627) | Acc_1: (94.12%) (9758/10368)\n",
      "Epoch: 114 | Batch_idx: 90 |  Loss_1: (0.1646) | Acc_1: (94.04%) (10954/11648)\n",
      "Epoch: 114 | Batch_idx: 100 |  Loss_1: (0.1663) | Acc_1: (93.91%) (12141/12928)\n",
      "Epoch: 114 | Batch_idx: 110 |  Loss_1: (0.1660) | Acc_1: (93.93%) (13346/14208)\n",
      "Epoch: 114 | Batch_idx: 120 |  Loss_1: (0.1670) | Acc_1: (93.89%) (14542/15488)\n",
      "Epoch: 114 | Batch_idx: 130 |  Loss_1: (0.1683) | Acc_1: (93.82%) (15731/16768)\n",
      "Epoch: 114 | Batch_idx: 140 |  Loss_1: (0.1676) | Acc_1: (93.83%) (16935/18048)\n",
      "Epoch: 114 | Batch_idx: 150 |  Loss_1: (0.1697) | Acc_1: (93.75%) (18120/19328)\n",
      "Epoch: 114 | Batch_idx: 160 |  Loss_1: (0.1698) | Acc_1: (93.76%) (19323/20608)\n",
      "Epoch: 114 | Batch_idx: 170 |  Loss_1: (0.1705) | Acc_1: (93.73%) (20515/21888)\n",
      "Epoch: 114 | Batch_idx: 180 |  Loss_1: (0.1696) | Acc_1: (93.74%) (21718/23168)\n",
      "Epoch: 114 | Batch_idx: 190 |  Loss_1: (0.1701) | Acc_1: (93.77%) (22926/24448)\n",
      "Epoch: 114 | Batch_idx: 200 |  Loss_1: (0.1687) | Acc_1: (93.83%) (24140/25728)\n",
      "Epoch: 114 | Batch_idx: 210 |  Loss_1: (0.1686) | Acc_1: (93.82%) (25338/27008)\n",
      "Epoch: 114 | Batch_idx: 220 |  Loss_1: (0.1683) | Acc_1: (93.83%) (26544/28288)\n",
      "Epoch: 114 | Batch_idx: 230 |  Loss_1: (0.1686) | Acc_1: (93.84%) (27747/29568)\n",
      "Epoch: 114 | Batch_idx: 240 |  Loss_1: (0.1679) | Acc_1: (93.86%) (28954/30848)\n",
      "Epoch: 114 | Batch_idx: 250 |  Loss_1: (0.1679) | Acc_1: (93.85%) (30153/32128)\n",
      "Epoch: 114 | Batch_idx: 260 |  Loss_1: (0.1673) | Acc_1: (93.89%) (31366/33408)\n",
      "Epoch: 114 | Batch_idx: 270 |  Loss_1: (0.1675) | Acc_1: (93.88%) (32565/34688)\n",
      "Epoch: 114 | Batch_idx: 280 |  Loss_1: (0.1681) | Acc_1: (93.86%) (33759/35968)\n",
      "Epoch: 114 | Batch_idx: 290 |  Loss_1: (0.1679) | Acc_1: (93.88%) (34970/37248)\n",
      "Epoch: 114 | Batch_idx: 300 |  Loss_1: (0.1672) | Acc_1: (93.91%) (36182/38528)\n",
      "Epoch: 114 | Batch_idx: 310 |  Loss_1: (0.1676) | Acc_1: (93.89%) (37375/39808)\n",
      "Epoch: 114 | Batch_idx: 320 |  Loss_1: (0.1672) | Acc_1: (93.89%) (38578/41088)\n",
      "Epoch: 114 | Batch_idx: 330 |  Loss_1: (0.1672) | Acc_1: (93.91%) (39787/42368)\n",
      "Epoch: 114 | Batch_idx: 340 |  Loss_1: (0.1662) | Acc_1: (93.94%) (41004/43648)\n",
      "Epoch: 114 | Batch_idx: 350 |  Loss_1: (0.1660) | Acc_1: (93.95%) (42209/44928)\n",
      "Epoch: 114 | Batch_idx: 360 |  Loss_1: (0.1657) | Acc_1: (93.96%) (43417/46208)\n",
      "Epoch: 114 | Batch_idx: 370 |  Loss_1: (0.1654) | Acc_1: (93.96%) (44618/47488)\n",
      "Epoch: 114 | Batch_idx: 380 |  Loss_1: (0.1649) | Acc_1: (93.98%) (45832/48768)\n",
      "Epoch: 114 | Batch_idx: 390 |  Loss_1: (0.1640) | Acc_1: (94.01%) (47006/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3357) | Acc: (91.52%) (9152/10000)\n",
      "Epoch: 115 | Batch_idx: 0 |  Loss_1: (0.1145) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 115 | Batch_idx: 10 |  Loss_1: (0.1788) | Acc_1: (93.61%) (1318/1408)\n",
      "Epoch: 115 | Batch_idx: 20 |  Loss_1: (0.1771) | Acc_1: (93.64%) (2517/2688)\n",
      "Epoch: 115 | Batch_idx: 30 |  Loss_1: (0.1932) | Acc_1: (93.02%) (3691/3968)\n",
      "Epoch: 115 | Batch_idx: 40 |  Loss_1: (0.1836) | Acc_1: (93.25%) (4894/5248)\n",
      "Epoch: 115 | Batch_idx: 50 |  Loss_1: (0.1767) | Acc_1: (93.46%) (6101/6528)\n",
      "Epoch: 115 | Batch_idx: 60 |  Loss_1: (0.1748) | Acc_1: (93.61%) (7309/7808)\n",
      "Epoch: 115 | Batch_idx: 70 |  Loss_1: (0.1750) | Acc_1: (93.63%) (8509/9088)\n",
      "Epoch: 115 | Batch_idx: 80 |  Loss_1: (0.1736) | Acc_1: (93.74%) (9719/10368)\n",
      "Epoch: 115 | Batch_idx: 90 |  Loss_1: (0.1731) | Acc_1: (93.78%) (10924/11648)\n",
      "Epoch: 115 | Batch_idx: 100 |  Loss_1: (0.1731) | Acc_1: (93.77%) (12123/12928)\n",
      "Epoch: 115 | Batch_idx: 110 |  Loss_1: (0.1722) | Acc_1: (93.76%) (13322/14208)\n",
      "Epoch: 115 | Batch_idx: 120 |  Loss_1: (0.1711) | Acc_1: (93.83%) (14532/15488)\n",
      "Epoch: 115 | Batch_idx: 130 |  Loss_1: (0.1687) | Acc_1: (93.91%) (15747/16768)\n",
      "Epoch: 115 | Batch_idx: 140 |  Loss_1: (0.1682) | Acc_1: (93.89%) (16946/18048)\n",
      "Epoch: 115 | Batch_idx: 150 |  Loss_1: (0.1681) | Acc_1: (93.91%) (18151/19328)\n",
      "Epoch: 115 | Batch_idx: 160 |  Loss_1: (0.1669) | Acc_1: (93.94%) (19360/20608)\n",
      "Epoch: 115 | Batch_idx: 170 |  Loss_1: (0.1670) | Acc_1: (93.97%) (20569/21888)\n",
      "Epoch: 115 | Batch_idx: 180 |  Loss_1: (0.1666) | Acc_1: (93.98%) (21773/23168)\n",
      "Epoch: 115 | Batch_idx: 190 |  Loss_1: (0.1660) | Acc_1: (94.01%) (22983/24448)\n",
      "Epoch: 115 | Batch_idx: 200 |  Loss_1: (0.1672) | Acc_1: (93.99%) (24182/25728)\n",
      "Epoch: 115 | Batch_idx: 210 |  Loss_1: (0.1689) | Acc_1: (93.92%) (25365/27008)\n",
      "Epoch: 115 | Batch_idx: 220 |  Loss_1: (0.1680) | Acc_1: (93.96%) (26578/28288)\n",
      "Epoch: 115 | Batch_idx: 230 |  Loss_1: (0.1674) | Acc_1: (93.95%) (27780/29568)\n",
      "Epoch: 115 | Batch_idx: 240 |  Loss_1: (0.1669) | Acc_1: (93.96%) (28986/30848)\n",
      "Epoch: 115 | Batch_idx: 250 |  Loss_1: (0.1665) | Acc_1: (93.97%) (30191/32128)\n",
      "Epoch: 115 | Batch_idx: 260 |  Loss_1: (0.1665) | Acc_1: (93.95%) (31386/33408)\n",
      "Epoch: 115 | Batch_idx: 270 |  Loss_1: (0.1662) | Acc_1: (93.97%) (32598/34688)\n",
      "Epoch: 115 | Batch_idx: 280 |  Loss_1: (0.1657) | Acc_1: (93.99%) (33806/35968)\n",
      "Epoch: 115 | Batch_idx: 290 |  Loss_1: (0.1659) | Acc_1: (93.98%) (35005/37248)\n",
      "Epoch: 115 | Batch_idx: 300 |  Loss_1: (0.1654) | Acc_1: (94.00%) (36216/38528)\n",
      "Epoch: 115 | Batch_idx: 310 |  Loss_1: (0.1666) | Acc_1: (93.94%) (37395/39808)\n",
      "Epoch: 115 | Batch_idx: 320 |  Loss_1: (0.1659) | Acc_1: (93.96%) (38608/41088)\n",
      "Epoch: 115 | Batch_idx: 330 |  Loss_1: (0.1661) | Acc_1: (93.97%) (39813/42368)\n",
      "Epoch: 115 | Batch_idx: 340 |  Loss_1: (0.1656) | Acc_1: (94.00%) (41029/43648)\n",
      "Epoch: 115 | Batch_idx: 350 |  Loss_1: (0.1658) | Acc_1: (93.99%) (42228/44928)\n",
      "Epoch: 115 | Batch_idx: 360 |  Loss_1: (0.1664) | Acc_1: (93.96%) (43418/46208)\n",
      "Epoch: 115 | Batch_idx: 370 |  Loss_1: (0.1665) | Acc_1: (93.97%) (44623/47488)\n",
      "Epoch: 115 | Batch_idx: 380 |  Loss_1: (0.1664) | Acc_1: (93.98%) (45832/48768)\n",
      "Epoch: 115 | Batch_idx: 390 |  Loss_1: (0.1662) | Acc_1: (93.97%) (46987/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3113) | Acc: (91.98%) (9198/10000)\n",
      "Epoch: 116 | Batch_idx: 0 |  Loss_1: (0.1147) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 116 | Batch_idx: 10 |  Loss_1: (0.1295) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 116 | Batch_idx: 20 |  Loss_1: (0.1488) | Acc_1: (94.90%) (2551/2688)\n",
      "Epoch: 116 | Batch_idx: 30 |  Loss_1: (0.1517) | Acc_1: (94.61%) (3754/3968)\n",
      "Epoch: 116 | Batch_idx: 40 |  Loss_1: (0.1581) | Acc_1: (94.23%) (4945/5248)\n",
      "Epoch: 116 | Batch_idx: 50 |  Loss_1: (0.1577) | Acc_1: (94.24%) (6152/6528)\n",
      "Epoch: 116 | Batch_idx: 60 |  Loss_1: (0.1545) | Acc_1: (94.39%) (7370/7808)\n",
      "Epoch: 116 | Batch_idx: 70 |  Loss_1: (0.1549) | Acc_1: (94.29%) (8569/9088)\n",
      "Epoch: 116 | Batch_idx: 80 |  Loss_1: (0.1540) | Acc_1: (94.33%) (9780/10368)\n",
      "Epoch: 116 | Batch_idx: 90 |  Loss_1: (0.1542) | Acc_1: (94.29%) (10983/11648)\n",
      "Epoch: 116 | Batch_idx: 100 |  Loss_1: (0.1545) | Acc_1: (94.30%) (12191/12928)\n",
      "Epoch: 116 | Batch_idx: 110 |  Loss_1: (0.1557) | Acc_1: (94.24%) (13389/14208)\n",
      "Epoch: 116 | Batch_idx: 120 |  Loss_1: (0.1558) | Acc_1: (94.27%) (14601/15488)\n",
      "Epoch: 116 | Batch_idx: 130 |  Loss_1: (0.1562) | Acc_1: (94.24%) (15803/16768)\n",
      "Epoch: 116 | Batch_idx: 140 |  Loss_1: (0.1558) | Acc_1: (94.22%) (17005/18048)\n",
      "Epoch: 116 | Batch_idx: 150 |  Loss_1: (0.1556) | Acc_1: (94.23%) (18212/19328)\n",
      "Epoch: 116 | Batch_idx: 160 |  Loss_1: (0.1562) | Acc_1: (94.20%) (19413/20608)\n",
      "Epoch: 116 | Batch_idx: 170 |  Loss_1: (0.1548) | Acc_1: (94.24%) (20628/21888)\n",
      "Epoch: 116 | Batch_idx: 180 |  Loss_1: (0.1538) | Acc_1: (94.27%) (21841/23168)\n",
      "Epoch: 116 | Batch_idx: 190 |  Loss_1: (0.1536) | Acc_1: (94.28%) (23050/24448)\n",
      "Epoch: 116 | Batch_idx: 200 |  Loss_1: (0.1523) | Acc_1: (94.34%) (24271/25728)\n",
      "Epoch: 116 | Batch_idx: 210 |  Loss_1: (0.1522) | Acc_1: (94.34%) (25480/27008)\n",
      "Epoch: 116 | Batch_idx: 220 |  Loss_1: (0.1504) | Acc_1: (94.40%) (26704/28288)\n",
      "Epoch: 116 | Batch_idx: 230 |  Loss_1: (0.1500) | Acc_1: (94.43%) (27920/29568)\n",
      "Epoch: 116 | Batch_idx: 240 |  Loss_1: (0.1508) | Acc_1: (94.40%) (29121/30848)\n",
      "Epoch: 116 | Batch_idx: 250 |  Loss_1: (0.1517) | Acc_1: (94.36%) (30315/32128)\n",
      "Epoch: 116 | Batch_idx: 260 |  Loss_1: (0.1514) | Acc_1: (94.35%) (31522/33408)\n",
      "Epoch: 116 | Batch_idx: 270 |  Loss_1: (0.1513) | Acc_1: (94.36%) (32731/34688)\n",
      "Epoch: 116 | Batch_idx: 280 |  Loss_1: (0.1527) | Acc_1: (94.31%) (33920/35968)\n",
      "Epoch: 116 | Batch_idx: 290 |  Loss_1: (0.1540) | Acc_1: (94.24%) (35104/37248)\n",
      "Epoch: 116 | Batch_idx: 300 |  Loss_1: (0.1541) | Acc_1: (94.26%) (36317/38528)\n",
      "Epoch: 116 | Batch_idx: 310 |  Loss_1: (0.1545) | Acc_1: (94.24%) (37515/39808)\n",
      "Epoch: 116 | Batch_idx: 320 |  Loss_1: (0.1542) | Acc_1: (94.26%) (38728/41088)\n",
      "Epoch: 116 | Batch_idx: 330 |  Loss_1: (0.1541) | Acc_1: (94.27%) (39939/42368)\n",
      "Epoch: 116 | Batch_idx: 340 |  Loss_1: (0.1541) | Acc_1: (94.27%) (41148/43648)\n",
      "Epoch: 116 | Batch_idx: 350 |  Loss_1: (0.1543) | Acc_1: (94.26%) (42351/44928)\n",
      "Epoch: 116 | Batch_idx: 360 |  Loss_1: (0.1559) | Acc_1: (94.23%) (43541/46208)\n",
      "Epoch: 116 | Batch_idx: 370 |  Loss_1: (0.1562) | Acc_1: (94.20%) (44736/47488)\n",
      "Epoch: 116 | Batch_idx: 380 |  Loss_1: (0.1566) | Acc_1: (94.21%) (45943/48768)\n",
      "Epoch: 116 | Batch_idx: 390 |  Loss_1: (0.1565) | Acc_1: (94.22%) (47108/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3530) | Acc: (91.09%) (9109/10000)\n",
      "Epoch: 117 | Batch_idx: 0 |  Loss_1: (0.1585) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 117 | Batch_idx: 10 |  Loss_1: (0.1692) | Acc_1: (93.47%) (1316/1408)\n",
      "Epoch: 117 | Batch_idx: 20 |  Loss_1: (0.1714) | Acc_1: (93.42%) (2511/2688)\n",
      "Epoch: 117 | Batch_idx: 30 |  Loss_1: (0.1585) | Acc_1: (93.98%) (3729/3968)\n",
      "Epoch: 117 | Batch_idx: 40 |  Loss_1: (0.1602) | Acc_1: (93.94%) (4930/5248)\n",
      "Epoch: 117 | Batch_idx: 50 |  Loss_1: (0.1590) | Acc_1: (94.06%) (6140/6528)\n",
      "Epoch: 117 | Batch_idx: 60 |  Loss_1: (0.1599) | Acc_1: (94.03%) (7342/7808)\n",
      "Epoch: 117 | Batch_idx: 70 |  Loss_1: (0.1550) | Acc_1: (94.25%) (8565/9088)\n",
      "Epoch: 117 | Batch_idx: 80 |  Loss_1: (0.1594) | Acc_1: (94.05%) (9751/10368)\n",
      "Epoch: 117 | Batch_idx: 90 |  Loss_1: (0.1569) | Acc_1: (94.12%) (10963/11648)\n",
      "Epoch: 117 | Batch_idx: 100 |  Loss_1: (0.1576) | Acc_1: (94.11%) (12167/12928)\n",
      "Epoch: 117 | Batch_idx: 110 |  Loss_1: (0.1573) | Acc_1: (94.09%) (13368/14208)\n",
      "Epoch: 117 | Batch_idx: 120 |  Loss_1: (0.1565) | Acc_1: (94.11%) (14575/15488)\n",
      "Epoch: 117 | Batch_idx: 130 |  Loss_1: (0.1578) | Acc_1: (94.05%) (15771/16768)\n",
      "Epoch: 117 | Batch_idx: 140 |  Loss_1: (0.1610) | Acc_1: (93.92%) (16951/18048)\n",
      "Epoch: 117 | Batch_idx: 150 |  Loss_1: (0.1605) | Acc_1: (93.95%) (18158/19328)\n",
      "Epoch: 117 | Batch_idx: 160 |  Loss_1: (0.1618) | Acc_1: (93.91%) (19352/20608)\n",
      "Epoch: 117 | Batch_idx: 170 |  Loss_1: (0.1623) | Acc_1: (93.90%) (20552/21888)\n",
      "Epoch: 117 | Batch_idx: 180 |  Loss_1: (0.1610) | Acc_1: (93.97%) (21770/23168)\n",
      "Epoch: 117 | Batch_idx: 190 |  Loss_1: (0.1621) | Acc_1: (93.93%) (22965/24448)\n",
      "Epoch: 117 | Batch_idx: 200 |  Loss_1: (0.1624) | Acc_1: (93.92%) (24163/25728)\n",
      "Epoch: 117 | Batch_idx: 210 |  Loss_1: (0.1632) | Acc_1: (93.89%) (25359/27008)\n",
      "Epoch: 117 | Batch_idx: 220 |  Loss_1: (0.1617) | Acc_1: (93.94%) (26573/28288)\n",
      "Epoch: 117 | Batch_idx: 230 |  Loss_1: (0.1615) | Acc_1: (93.94%) (27776/29568)\n",
      "Epoch: 117 | Batch_idx: 240 |  Loss_1: (0.1616) | Acc_1: (93.96%) (28984/30848)\n",
      "Epoch: 117 | Batch_idx: 250 |  Loss_1: (0.1610) | Acc_1: (93.97%) (30190/32128)\n",
      "Epoch: 117 | Batch_idx: 260 |  Loss_1: (0.1606) | Acc_1: (93.99%) (31399/33408)\n",
      "Epoch: 117 | Batch_idx: 270 |  Loss_1: (0.1608) | Acc_1: (94.00%) (32605/34688)\n",
      "Epoch: 117 | Batch_idx: 280 |  Loss_1: (0.1601) | Acc_1: (94.04%) (33823/35968)\n",
      "Epoch: 117 | Batch_idx: 290 |  Loss_1: (0.1600) | Acc_1: (94.05%) (35031/37248)\n",
      "Epoch: 117 | Batch_idx: 300 |  Loss_1: (0.1590) | Acc_1: (94.11%) (36258/38528)\n",
      "Epoch: 117 | Batch_idx: 310 |  Loss_1: (0.1595) | Acc_1: (94.11%) (37462/39808)\n",
      "Epoch: 117 | Batch_idx: 320 |  Loss_1: (0.1589) | Acc_1: (94.13%) (38678/41088)\n",
      "Epoch: 117 | Batch_idx: 330 |  Loss_1: (0.1592) | Acc_1: (94.12%) (39876/42368)\n",
      "Epoch: 117 | Batch_idx: 340 |  Loss_1: (0.1592) | Acc_1: (94.12%) (41083/43648)\n",
      "Epoch: 117 | Batch_idx: 350 |  Loss_1: (0.1590) | Acc_1: (94.14%) (42294/44928)\n",
      "Epoch: 117 | Batch_idx: 360 |  Loss_1: (0.1587) | Acc_1: (94.15%) (43504/46208)\n",
      "Epoch: 117 | Batch_idx: 370 |  Loss_1: (0.1582) | Acc_1: (94.18%) (44724/47488)\n",
      "Epoch: 117 | Batch_idx: 380 |  Loss_1: (0.1580) | Acc_1: (94.18%) (45929/48768)\n",
      "Epoch: 117 | Batch_idx: 390 |  Loss_1: (0.1574) | Acc_1: (94.21%) (47104/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3315) | Acc: (92.01%) (9201/10000)\n",
      "Epoch: 118 | Batch_idx: 0 |  Loss_1: (0.1153) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 118 | Batch_idx: 10 |  Loss_1: (0.1559) | Acc_1: (93.96%) (1323/1408)\n",
      "Epoch: 118 | Batch_idx: 20 |  Loss_1: (0.1648) | Acc_1: (93.79%) (2521/2688)\n",
      "Epoch: 118 | Batch_idx: 30 |  Loss_1: (0.1682) | Acc_1: (93.88%) (3725/3968)\n",
      "Epoch: 118 | Batch_idx: 40 |  Loss_1: (0.1617) | Acc_1: (94.02%) (4934/5248)\n",
      "Epoch: 118 | Batch_idx: 50 |  Loss_1: (0.1586) | Acc_1: (94.26%) (6153/6528)\n",
      "Epoch: 118 | Batch_idx: 60 |  Loss_1: (0.1588) | Acc_1: (94.22%) (7357/7808)\n",
      "Epoch: 118 | Batch_idx: 70 |  Loss_1: (0.1571) | Acc_1: (94.36%) (8575/9088)\n",
      "Epoch: 118 | Batch_idx: 80 |  Loss_1: (0.1549) | Acc_1: (94.43%) (9790/10368)\n",
      "Epoch: 118 | Batch_idx: 90 |  Loss_1: (0.1555) | Acc_1: (94.39%) (10995/11648)\n",
      "Epoch: 118 | Batch_idx: 100 |  Loss_1: (0.1555) | Acc_1: (94.42%) (12206/12928)\n",
      "Epoch: 118 | Batch_idx: 110 |  Loss_1: (0.1558) | Acc_1: (94.38%) (13409/14208)\n",
      "Epoch: 118 | Batch_idx: 120 |  Loss_1: (0.1542) | Acc_1: (94.40%) (14620/15488)\n",
      "Epoch: 118 | Batch_idx: 130 |  Loss_1: (0.1555) | Acc_1: (94.40%) (15829/16768)\n",
      "Epoch: 118 | Batch_idx: 140 |  Loss_1: (0.1557) | Acc_1: (94.39%) (17036/18048)\n",
      "Epoch: 118 | Batch_idx: 150 |  Loss_1: (0.1553) | Acc_1: (94.42%) (18250/19328)\n",
      "Epoch: 118 | Batch_idx: 160 |  Loss_1: (0.1556) | Acc_1: (94.39%) (19451/20608)\n",
      "Epoch: 118 | Batch_idx: 170 |  Loss_1: (0.1573) | Acc_1: (94.33%) (20646/21888)\n",
      "Epoch: 118 | Batch_idx: 180 |  Loss_1: (0.1588) | Acc_1: (94.28%) (21842/23168)\n",
      "Epoch: 118 | Batch_idx: 190 |  Loss_1: (0.1585) | Acc_1: (94.27%) (23048/24448)\n",
      "Epoch: 118 | Batch_idx: 200 |  Loss_1: (0.1571) | Acc_1: (94.32%) (24267/25728)\n",
      "Epoch: 118 | Batch_idx: 210 |  Loss_1: (0.1568) | Acc_1: (94.34%) (25480/27008)\n",
      "Epoch: 118 | Batch_idx: 220 |  Loss_1: (0.1566) | Acc_1: (94.34%) (26688/28288)\n",
      "Epoch: 118 | Batch_idx: 230 |  Loss_1: (0.1565) | Acc_1: (94.32%) (27890/29568)\n",
      "Epoch: 118 | Batch_idx: 240 |  Loss_1: (0.1570) | Acc_1: (94.32%) (29095/30848)\n",
      "Epoch: 118 | Batch_idx: 250 |  Loss_1: (0.1572) | Acc_1: (94.31%) (30299/32128)\n",
      "Epoch: 118 | Batch_idx: 260 |  Loss_1: (0.1572) | Acc_1: (94.32%) (31509/33408)\n",
      "Epoch: 118 | Batch_idx: 270 |  Loss_1: (0.1573) | Acc_1: (94.31%) (32714/34688)\n",
      "Epoch: 118 | Batch_idx: 280 |  Loss_1: (0.1569) | Acc_1: (94.33%) (33928/35968)\n",
      "Epoch: 118 | Batch_idx: 290 |  Loss_1: (0.1565) | Acc_1: (94.34%) (35141/37248)\n",
      "Epoch: 118 | Batch_idx: 300 |  Loss_1: (0.1568) | Acc_1: (94.34%) (36349/38528)\n",
      "Epoch: 118 | Batch_idx: 310 |  Loss_1: (0.1569) | Acc_1: (94.34%) (37555/39808)\n",
      "Epoch: 118 | Batch_idx: 320 |  Loss_1: (0.1576) | Acc_1: (94.31%) (38749/41088)\n",
      "Epoch: 118 | Batch_idx: 330 |  Loss_1: (0.1568) | Acc_1: (94.34%) (39969/42368)\n",
      "Epoch: 118 | Batch_idx: 340 |  Loss_1: (0.1567) | Acc_1: (94.34%) (41177/43648)\n",
      "Epoch: 118 | Batch_idx: 350 |  Loss_1: (0.1569) | Acc_1: (94.32%) (42375/44928)\n",
      "Epoch: 118 | Batch_idx: 360 |  Loss_1: (0.1565) | Acc_1: (94.33%) (43586/46208)\n",
      "Epoch: 118 | Batch_idx: 370 |  Loss_1: (0.1567) | Acc_1: (94.32%) (44792/47488)\n",
      "Epoch: 118 | Batch_idx: 380 |  Loss_1: (0.1566) | Acc_1: (94.32%) (45996/48768)\n",
      "Epoch: 118 | Batch_idx: 390 |  Loss_1: (0.1571) | Acc_1: (94.30%) (47150/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3242) | Acc: (91.88%) (9188/10000)\n",
      "Epoch: 119 | Batch_idx: 0 |  Loss_1: (0.2896) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 119 | Batch_idx: 10 |  Loss_1: (0.1702) | Acc_1: (93.82%) (1321/1408)\n",
      "Epoch: 119 | Batch_idx: 20 |  Loss_1: (0.1592) | Acc_1: (94.27%) (2534/2688)\n",
      "Epoch: 119 | Batch_idx: 30 |  Loss_1: (0.1708) | Acc_1: (93.78%) (3721/3968)\n",
      "Epoch: 119 | Batch_idx: 40 |  Loss_1: (0.1671) | Acc_1: (93.90%) (4928/5248)\n",
      "Epoch: 119 | Batch_idx: 50 |  Loss_1: (0.1657) | Acc_1: (93.90%) (6130/6528)\n",
      "Epoch: 119 | Batch_idx: 60 |  Loss_1: (0.1661) | Acc_1: (93.89%) (7331/7808)\n",
      "Epoch: 119 | Batch_idx: 70 |  Loss_1: (0.1662) | Acc_1: (93.89%) (8533/9088)\n",
      "Epoch: 119 | Batch_idx: 80 |  Loss_1: (0.1642) | Acc_1: (93.97%) (9743/10368)\n",
      "Epoch: 119 | Batch_idx: 90 |  Loss_1: (0.1619) | Acc_1: (94.06%) (10956/11648)\n",
      "Epoch: 119 | Batch_idx: 100 |  Loss_1: (0.1596) | Acc_1: (94.14%) (12170/12928)\n",
      "Epoch: 119 | Batch_idx: 110 |  Loss_1: (0.1596) | Acc_1: (94.14%) (13375/14208)\n",
      "Epoch: 119 | Batch_idx: 120 |  Loss_1: (0.1569) | Acc_1: (94.23%) (14594/15488)\n",
      "Epoch: 119 | Batch_idx: 130 |  Loss_1: (0.1573) | Acc_1: (94.20%) (15796/16768)\n",
      "Epoch: 119 | Batch_idx: 140 |  Loss_1: (0.1575) | Acc_1: (94.22%) (17005/18048)\n",
      "Epoch: 119 | Batch_idx: 150 |  Loss_1: (0.1570) | Acc_1: (94.23%) (18212/19328)\n",
      "Epoch: 119 | Batch_idx: 160 |  Loss_1: (0.1562) | Acc_1: (94.26%) (19425/20608)\n",
      "Epoch: 119 | Batch_idx: 170 |  Loss_1: (0.1560) | Acc_1: (94.27%) (20634/21888)\n",
      "Epoch: 119 | Batch_idx: 180 |  Loss_1: (0.1561) | Acc_1: (94.29%) (21845/23168)\n",
      "Epoch: 119 | Batch_idx: 190 |  Loss_1: (0.1555) | Acc_1: (94.32%) (23059/24448)\n",
      "Epoch: 119 | Batch_idx: 200 |  Loss_1: (0.1555) | Acc_1: (94.30%) (24262/25728)\n",
      "Epoch: 119 | Batch_idx: 210 |  Loss_1: (0.1546) | Acc_1: (94.34%) (25479/27008)\n",
      "Epoch: 119 | Batch_idx: 220 |  Loss_1: (0.1545) | Acc_1: (94.33%) (26685/28288)\n",
      "Epoch: 119 | Batch_idx: 230 |  Loss_1: (0.1537) | Acc_1: (94.36%) (27900/29568)\n",
      "Epoch: 119 | Batch_idx: 240 |  Loss_1: (0.1537) | Acc_1: (94.35%) (29105/30848)\n",
      "Epoch: 119 | Batch_idx: 250 |  Loss_1: (0.1542) | Acc_1: (94.34%) (30310/32128)\n",
      "Epoch: 119 | Batch_idx: 260 |  Loss_1: (0.1549) | Acc_1: (94.33%) (31514/33408)\n",
      "Epoch: 119 | Batch_idx: 270 |  Loss_1: (0.1547) | Acc_1: (94.32%) (32719/34688)\n",
      "Epoch: 119 | Batch_idx: 280 |  Loss_1: (0.1535) | Acc_1: (94.36%) (33939/35968)\n",
      "Epoch: 119 | Batch_idx: 290 |  Loss_1: (0.1541) | Acc_1: (94.34%) (35138/37248)\n",
      "Epoch: 119 | Batch_idx: 300 |  Loss_1: (0.1552) | Acc_1: (94.31%) (36336/38528)\n",
      "Epoch: 119 | Batch_idx: 310 |  Loss_1: (0.1561) | Acc_1: (94.28%) (37531/39808)\n",
      "Epoch: 119 | Batch_idx: 320 |  Loss_1: (0.1553) | Acc_1: (94.32%) (38753/41088)\n",
      "Epoch: 119 | Batch_idx: 330 |  Loss_1: (0.1548) | Acc_1: (94.33%) (39966/42368)\n",
      "Epoch: 119 | Batch_idx: 340 |  Loss_1: (0.1543) | Acc_1: (94.35%) (41184/43648)\n",
      "Epoch: 119 | Batch_idx: 350 |  Loss_1: (0.1545) | Acc_1: (94.33%) (42382/44928)\n",
      "Epoch: 119 | Batch_idx: 360 |  Loss_1: (0.1548) | Acc_1: (94.32%) (43585/46208)\n",
      "Epoch: 119 | Batch_idx: 370 |  Loss_1: (0.1550) | Acc_1: (94.31%) (44787/47488)\n",
      "Epoch: 119 | Batch_idx: 380 |  Loss_1: (0.1559) | Acc_1: (94.28%) (45978/48768)\n",
      "Epoch: 119 | Batch_idx: 390 |  Loss_1: (0.1554) | Acc_1: (94.30%) (47149/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3131) | Acc: (92.20%) (9220/10000)\n",
      "Epoch: 120 | Batch_idx: 0 |  Loss_1: (0.1080) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 120 | Batch_idx: 10 |  Loss_1: (0.1535) | Acc_1: (94.32%) (1328/1408)\n",
      "Epoch: 120 | Batch_idx: 20 |  Loss_1: (0.1542) | Acc_1: (94.35%) (2536/2688)\n",
      "Epoch: 120 | Batch_idx: 30 |  Loss_1: (0.1560) | Acc_1: (94.18%) (3737/3968)\n",
      "Epoch: 120 | Batch_idx: 40 |  Loss_1: (0.1514) | Acc_1: (94.36%) (4952/5248)\n",
      "Epoch: 120 | Batch_idx: 50 |  Loss_1: (0.1525) | Acc_1: (94.32%) (6157/6528)\n",
      "Epoch: 120 | Batch_idx: 60 |  Loss_1: (0.1470) | Acc_1: (94.54%) (7382/7808)\n",
      "Epoch: 120 | Batch_idx: 70 |  Loss_1: (0.1468) | Acc_1: (94.58%) (8595/9088)\n",
      "Epoch: 120 | Batch_idx: 80 |  Loss_1: (0.1506) | Acc_1: (94.49%) (9797/10368)\n",
      "Epoch: 120 | Batch_idx: 90 |  Loss_1: (0.1543) | Acc_1: (94.34%) (10989/11648)\n",
      "Epoch: 120 | Batch_idx: 100 |  Loss_1: (0.1527) | Acc_1: (94.41%) (12205/12928)\n",
      "Epoch: 120 | Batch_idx: 110 |  Loss_1: (0.1538) | Acc_1: (94.30%) (13398/14208)\n",
      "Epoch: 120 | Batch_idx: 120 |  Loss_1: (0.1546) | Acc_1: (94.31%) (14606/15488)\n",
      "Epoch: 120 | Batch_idx: 130 |  Loss_1: (0.1529) | Acc_1: (94.39%) (15827/16768)\n",
      "Epoch: 120 | Batch_idx: 140 |  Loss_1: (0.1520) | Acc_1: (94.39%) (17036/18048)\n",
      "Epoch: 120 | Batch_idx: 150 |  Loss_1: (0.1519) | Acc_1: (94.43%) (18251/19328)\n",
      "Epoch: 120 | Batch_idx: 160 |  Loss_1: (0.1502) | Acc_1: (94.51%) (19476/20608)\n",
      "Epoch: 120 | Batch_idx: 170 |  Loss_1: (0.1501) | Acc_1: (94.52%) (20689/21888)\n",
      "Epoch: 120 | Batch_idx: 180 |  Loss_1: (0.1503) | Acc_1: (94.51%) (21896/23168)\n",
      "Epoch: 120 | Batch_idx: 190 |  Loss_1: (0.1515) | Acc_1: (94.45%) (23091/24448)\n",
      "Epoch: 120 | Batch_idx: 200 |  Loss_1: (0.1511) | Acc_1: (94.48%) (24307/25728)\n",
      "Epoch: 120 | Batch_idx: 210 |  Loss_1: (0.1528) | Acc_1: (94.42%) (25500/27008)\n",
      "Epoch: 120 | Batch_idx: 220 |  Loss_1: (0.1526) | Acc_1: (94.42%) (26710/28288)\n",
      "Epoch: 120 | Batch_idx: 230 |  Loss_1: (0.1524) | Acc_1: (94.43%) (27920/29568)\n",
      "Epoch: 120 | Batch_idx: 240 |  Loss_1: (0.1525) | Acc_1: (94.41%) (29124/30848)\n",
      "Epoch: 120 | Batch_idx: 250 |  Loss_1: (0.1520) | Acc_1: (94.44%) (30342/32128)\n",
      "Epoch: 120 | Batch_idx: 260 |  Loss_1: (0.1528) | Acc_1: (94.40%) (31538/33408)\n",
      "Epoch: 120 | Batch_idx: 270 |  Loss_1: (0.1537) | Acc_1: (94.40%) (32744/34688)\n",
      "Epoch: 120 | Batch_idx: 280 |  Loss_1: (0.1537) | Acc_1: (94.38%) (33948/35968)\n",
      "Epoch: 120 | Batch_idx: 290 |  Loss_1: (0.1534) | Acc_1: (94.40%) (35163/37248)\n",
      "Epoch: 120 | Batch_idx: 300 |  Loss_1: (0.1532) | Acc_1: (94.40%) (36372/38528)\n",
      "Epoch: 120 | Batch_idx: 310 |  Loss_1: (0.1553) | Acc_1: (94.32%) (37545/39808)\n",
      "Epoch: 120 | Batch_idx: 320 |  Loss_1: (0.1553) | Acc_1: (94.31%) (38751/41088)\n",
      "Epoch: 120 | Batch_idx: 330 |  Loss_1: (0.1556) | Acc_1: (94.30%) (39954/42368)\n",
      "Epoch: 120 | Batch_idx: 340 |  Loss_1: (0.1554) | Acc_1: (94.32%) (41167/43648)\n",
      "Epoch: 120 | Batch_idx: 350 |  Loss_1: (0.1553) | Acc_1: (94.32%) (42376/44928)\n",
      "Epoch: 120 | Batch_idx: 360 |  Loss_1: (0.1554) | Acc_1: (94.31%) (43579/46208)\n",
      "Epoch: 120 | Batch_idx: 370 |  Loss_1: (0.1552) | Acc_1: (94.31%) (44785/47488)\n",
      "Epoch: 120 | Batch_idx: 380 |  Loss_1: (0.1547) | Acc_1: (94.32%) (45999/48768)\n",
      "Epoch: 120 | Batch_idx: 390 |  Loss_1: (0.1551) | Acc_1: (94.32%) (47161/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3319) | Acc: (91.86%) (9186/10000)\n",
      "Epoch: 121 | Batch_idx: 0 |  Loss_1: (0.1222) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 121 | Batch_idx: 10 |  Loss_1: (0.1438) | Acc_1: (94.89%) (1336/1408)\n",
      "Epoch: 121 | Batch_idx: 20 |  Loss_1: (0.1409) | Acc_1: (95.09%) (2556/2688)\n",
      "Epoch: 121 | Batch_idx: 30 |  Loss_1: (0.1437) | Acc_1: (94.63%) (3755/3968)\n",
      "Epoch: 121 | Batch_idx: 40 |  Loss_1: (0.1440) | Acc_1: (94.63%) (4966/5248)\n",
      "Epoch: 121 | Batch_idx: 50 |  Loss_1: (0.1467) | Acc_1: (94.52%) (6170/6528)\n",
      "Epoch: 121 | Batch_idx: 60 |  Loss_1: (0.1463) | Acc_1: (94.54%) (7382/7808)\n",
      "Epoch: 121 | Batch_idx: 70 |  Loss_1: (0.1479) | Acc_1: (94.50%) (8588/9088)\n",
      "Epoch: 121 | Batch_idx: 80 |  Loss_1: (0.1514) | Acc_1: (94.39%) (9786/10368)\n",
      "Epoch: 121 | Batch_idx: 90 |  Loss_1: (0.1517) | Acc_1: (94.37%) (10992/11648)\n",
      "Epoch: 121 | Batch_idx: 100 |  Loss_1: (0.1530) | Acc_1: (94.34%) (12196/12928)\n",
      "Epoch: 121 | Batch_idx: 110 |  Loss_1: (0.1545) | Acc_1: (94.30%) (13398/14208)\n",
      "Epoch: 121 | Batch_idx: 120 |  Loss_1: (0.1569) | Acc_1: (94.24%) (14596/15488)\n",
      "Epoch: 121 | Batch_idx: 130 |  Loss_1: (0.1594) | Acc_1: (94.12%) (15782/16768)\n",
      "Epoch: 121 | Batch_idx: 140 |  Loss_1: (0.1602) | Acc_1: (94.11%) (16985/18048)\n",
      "Epoch: 121 | Batch_idx: 150 |  Loss_1: (0.1607) | Acc_1: (94.09%) (18186/19328)\n",
      "Epoch: 121 | Batch_idx: 160 |  Loss_1: (0.1617) | Acc_1: (94.04%) (19380/20608)\n",
      "Epoch: 121 | Batch_idx: 170 |  Loss_1: (0.1628) | Acc_1: (94.01%) (20577/21888)\n",
      "Epoch: 121 | Batch_idx: 180 |  Loss_1: (0.1628) | Acc_1: (94.03%) (21784/23168)\n",
      "Epoch: 121 | Batch_idx: 190 |  Loss_1: (0.1622) | Acc_1: (94.03%) (22988/24448)\n",
      "Epoch: 121 | Batch_idx: 200 |  Loss_1: (0.1606) | Acc_1: (94.09%) (24207/25728)\n",
      "Epoch: 121 | Batch_idx: 210 |  Loss_1: (0.1604) | Acc_1: (94.09%) (25411/27008)\n",
      "Epoch: 121 | Batch_idx: 220 |  Loss_1: (0.1596) | Acc_1: (94.11%) (26621/28288)\n",
      "Epoch: 121 | Batch_idx: 230 |  Loss_1: (0.1607) | Acc_1: (94.08%) (27818/29568)\n",
      "Epoch: 121 | Batch_idx: 240 |  Loss_1: (0.1601) | Acc_1: (94.10%) (29028/30848)\n",
      "Epoch: 121 | Batch_idx: 250 |  Loss_1: (0.1620) | Acc_1: (94.03%) (30209/32128)\n",
      "Epoch: 121 | Batch_idx: 260 |  Loss_1: (0.1628) | Acc_1: (93.99%) (31401/33408)\n",
      "Epoch: 121 | Batch_idx: 270 |  Loss_1: (0.1636) | Acc_1: (93.98%) (32599/34688)\n",
      "Epoch: 121 | Batch_idx: 280 |  Loss_1: (0.1639) | Acc_1: (93.96%) (33796/35968)\n",
      "Epoch: 121 | Batch_idx: 290 |  Loss_1: (0.1640) | Acc_1: (93.96%) (34999/37248)\n",
      "Epoch: 121 | Batch_idx: 300 |  Loss_1: (0.1634) | Acc_1: (93.98%) (36209/38528)\n",
      "Epoch: 121 | Batch_idx: 310 |  Loss_1: (0.1629) | Acc_1: (94.02%) (37426/39808)\n",
      "Epoch: 121 | Batch_idx: 320 |  Loss_1: (0.1626) | Acc_1: (94.02%) (38629/41088)\n",
      "Epoch: 121 | Batch_idx: 330 |  Loss_1: (0.1620) | Acc_1: (94.04%) (39841/42368)\n",
      "Epoch: 121 | Batch_idx: 340 |  Loss_1: (0.1615) | Acc_1: (94.05%) (41049/43648)\n",
      "Epoch: 121 | Batch_idx: 350 |  Loss_1: (0.1610) | Acc_1: (94.07%) (42263/44928)\n",
      "Epoch: 121 | Batch_idx: 360 |  Loss_1: (0.1610) | Acc_1: (94.07%) (43470/46208)\n",
      "Epoch: 121 | Batch_idx: 370 |  Loss_1: (0.1604) | Acc_1: (94.10%) (44685/47488)\n",
      "Epoch: 121 | Batch_idx: 380 |  Loss_1: (0.1601) | Acc_1: (94.11%) (45896/48768)\n",
      "Epoch: 121 | Batch_idx: 390 |  Loss_1: (0.1594) | Acc_1: (94.13%) (47067/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3085) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 122 | Batch_idx: 0 |  Loss_1: (0.1250) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 122 | Batch_idx: 10 |  Loss_1: (0.1353) | Acc_1: (95.17%) (1340/1408)\n",
      "Epoch: 122 | Batch_idx: 20 |  Loss_1: (0.1367) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 122 | Batch_idx: 30 |  Loss_1: (0.1416) | Acc_1: (94.68%) (3757/3968)\n",
      "Epoch: 122 | Batch_idx: 40 |  Loss_1: (0.1440) | Acc_1: (94.59%) (4964/5248)\n",
      "Epoch: 122 | Batch_idx: 50 |  Loss_1: (0.1448) | Acc_1: (94.56%) (6173/6528)\n",
      "Epoch: 122 | Batch_idx: 60 |  Loss_1: (0.1532) | Acc_1: (94.16%) (7352/7808)\n",
      "Epoch: 122 | Batch_idx: 70 |  Loss_1: (0.1527) | Acc_1: (94.21%) (8562/9088)\n",
      "Epoch: 122 | Batch_idx: 80 |  Loss_1: (0.1530) | Acc_1: (94.25%) (9772/10368)\n",
      "Epoch: 122 | Batch_idx: 90 |  Loss_1: (0.1504) | Acc_1: (94.34%) (10989/11648)\n",
      "Epoch: 122 | Batch_idx: 100 |  Loss_1: (0.1481) | Acc_1: (94.43%) (12208/12928)\n",
      "Epoch: 122 | Batch_idx: 110 |  Loss_1: (0.1494) | Acc_1: (94.43%) (13416/14208)\n",
      "Epoch: 122 | Batch_idx: 120 |  Loss_1: (0.1531) | Acc_1: (94.30%) (14605/15488)\n",
      "Epoch: 122 | Batch_idx: 130 |  Loss_1: (0.1516) | Acc_1: (94.35%) (15820/16768)\n",
      "Epoch: 122 | Batch_idx: 140 |  Loss_1: (0.1506) | Acc_1: (94.40%) (17038/18048)\n",
      "Epoch: 122 | Batch_idx: 150 |  Loss_1: (0.1499) | Acc_1: (94.42%) (18250/19328)\n",
      "Epoch: 122 | Batch_idx: 160 |  Loss_1: (0.1494) | Acc_1: (94.45%) (19465/20608)\n",
      "Epoch: 122 | Batch_idx: 170 |  Loss_1: (0.1519) | Acc_1: (94.39%) (20659/21888)\n",
      "Epoch: 122 | Batch_idx: 180 |  Loss_1: (0.1537) | Acc_1: (94.30%) (21848/23168)\n",
      "Epoch: 122 | Batch_idx: 190 |  Loss_1: (0.1546) | Acc_1: (94.29%) (23053/24448)\n",
      "Epoch: 122 | Batch_idx: 200 |  Loss_1: (0.1544) | Acc_1: (94.33%) (24269/25728)\n",
      "Epoch: 122 | Batch_idx: 210 |  Loss_1: (0.1544) | Acc_1: (94.32%) (25474/27008)\n",
      "Epoch: 122 | Batch_idx: 220 |  Loss_1: (0.1533) | Acc_1: (94.35%) (26690/28288)\n",
      "Epoch: 122 | Batch_idx: 230 |  Loss_1: (0.1534) | Acc_1: (94.36%) (27901/29568)\n",
      "Epoch: 122 | Batch_idx: 240 |  Loss_1: (0.1538) | Acc_1: (94.36%) (29107/30848)\n",
      "Epoch: 122 | Batch_idx: 250 |  Loss_1: (0.1538) | Acc_1: (94.35%) (30314/32128)\n",
      "Epoch: 122 | Batch_idx: 260 |  Loss_1: (0.1542) | Acc_1: (94.34%) (31517/33408)\n",
      "Epoch: 122 | Batch_idx: 270 |  Loss_1: (0.1541) | Acc_1: (94.35%) (32729/34688)\n",
      "Epoch: 122 | Batch_idx: 280 |  Loss_1: (0.1543) | Acc_1: (94.35%) (33936/35968)\n",
      "Epoch: 122 | Batch_idx: 290 |  Loss_1: (0.1549) | Acc_1: (94.34%) (35140/37248)\n",
      "Epoch: 122 | Batch_idx: 300 |  Loss_1: (0.1545) | Acc_1: (94.36%) (36354/38528)\n",
      "Epoch: 122 | Batch_idx: 310 |  Loss_1: (0.1552) | Acc_1: (94.34%) (37553/39808)\n",
      "Epoch: 122 | Batch_idx: 320 |  Loss_1: (0.1555) | Acc_1: (94.33%) (38759/41088)\n",
      "Epoch: 122 | Batch_idx: 330 |  Loss_1: (0.1559) | Acc_1: (94.31%) (39956/42368)\n",
      "Epoch: 122 | Batch_idx: 340 |  Loss_1: (0.1554) | Acc_1: (94.33%) (41174/43648)\n",
      "Epoch: 122 | Batch_idx: 350 |  Loss_1: (0.1557) | Acc_1: (94.33%) (42379/44928)\n",
      "Epoch: 122 | Batch_idx: 360 |  Loss_1: (0.1562) | Acc_1: (94.31%) (43578/46208)\n",
      "Epoch: 122 | Batch_idx: 370 |  Loss_1: (0.1569) | Acc_1: (94.29%) (44777/47488)\n",
      "Epoch: 122 | Batch_idx: 380 |  Loss_1: (0.1568) | Acc_1: (94.29%) (45981/48768)\n",
      "Epoch: 122 | Batch_idx: 390 |  Loss_1: (0.1567) | Acc_1: (94.28%) (47139/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3271) | Acc: (91.74%) (9174/10000)\n",
      "Epoch: 123 | Batch_idx: 0 |  Loss_1: (0.2100) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 123 | Batch_idx: 10 |  Loss_1: (0.1671) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 123 | Batch_idx: 20 |  Loss_1: (0.1565) | Acc_1: (94.27%) (2534/2688)\n",
      "Epoch: 123 | Batch_idx: 30 |  Loss_1: (0.1493) | Acc_1: (94.63%) (3755/3968)\n",
      "Epoch: 123 | Batch_idx: 40 |  Loss_1: (0.1494) | Acc_1: (94.65%) (4967/5248)\n",
      "Epoch: 123 | Batch_idx: 50 |  Loss_1: (0.1552) | Acc_1: (94.44%) (6165/6528)\n",
      "Epoch: 123 | Batch_idx: 60 |  Loss_1: (0.1545) | Acc_1: (94.42%) (7372/7808)\n",
      "Epoch: 123 | Batch_idx: 70 |  Loss_1: (0.1554) | Acc_1: (94.34%) (8574/9088)\n",
      "Epoch: 123 | Batch_idx: 80 |  Loss_1: (0.1539) | Acc_1: (94.29%) (9776/10368)\n",
      "Epoch: 123 | Batch_idx: 90 |  Loss_1: (0.1528) | Acc_1: (94.39%) (10994/11648)\n",
      "Epoch: 123 | Batch_idx: 100 |  Loss_1: (0.1549) | Acc_1: (94.35%) (12197/12928)\n",
      "Epoch: 123 | Batch_idx: 110 |  Loss_1: (0.1530) | Acc_1: (94.41%) (13414/14208)\n",
      "Epoch: 123 | Batch_idx: 120 |  Loss_1: (0.1525) | Acc_1: (94.45%) (14629/15488)\n",
      "Epoch: 123 | Batch_idx: 130 |  Loss_1: (0.1520) | Acc_1: (94.45%) (15838/16768)\n",
      "Epoch: 123 | Batch_idx: 140 |  Loss_1: (0.1500) | Acc_1: (94.50%) (17055/18048)\n",
      "Epoch: 123 | Batch_idx: 150 |  Loss_1: (0.1501) | Acc_1: (94.52%) (18269/19328)\n",
      "Epoch: 123 | Batch_idx: 160 |  Loss_1: (0.1496) | Acc_1: (94.52%) (19479/20608)\n",
      "Epoch: 123 | Batch_idx: 170 |  Loss_1: (0.1496) | Acc_1: (94.54%) (20694/21888)\n",
      "Epoch: 123 | Batch_idx: 180 |  Loss_1: (0.1496) | Acc_1: (94.51%) (21897/23168)\n",
      "Epoch: 123 | Batch_idx: 190 |  Loss_1: (0.1492) | Acc_1: (94.51%) (23106/24448)\n",
      "Epoch: 123 | Batch_idx: 200 |  Loss_1: (0.1486) | Acc_1: (94.54%) (24323/25728)\n",
      "Epoch: 123 | Batch_idx: 210 |  Loss_1: (0.1485) | Acc_1: (94.52%) (25529/27008)\n",
      "Epoch: 123 | Batch_idx: 220 |  Loss_1: (0.1483) | Acc_1: (94.50%) (26733/28288)\n",
      "Epoch: 123 | Batch_idx: 230 |  Loss_1: (0.1486) | Acc_1: (94.49%) (27940/29568)\n",
      "Epoch: 123 | Batch_idx: 240 |  Loss_1: (0.1498) | Acc_1: (94.42%) (29128/30848)\n",
      "Epoch: 123 | Batch_idx: 250 |  Loss_1: (0.1498) | Acc_1: (94.43%) (30340/32128)\n",
      "Epoch: 123 | Batch_idx: 260 |  Loss_1: (0.1504) | Acc_1: (94.43%) (31546/33408)\n",
      "Epoch: 123 | Batch_idx: 270 |  Loss_1: (0.1510) | Acc_1: (94.39%) (32743/34688)\n",
      "Epoch: 123 | Batch_idx: 280 |  Loss_1: (0.1502) | Acc_1: (94.43%) (33963/35968)\n",
      "Epoch: 123 | Batch_idx: 290 |  Loss_1: (0.1505) | Acc_1: (94.43%) (35173/37248)\n",
      "Epoch: 123 | Batch_idx: 300 |  Loss_1: (0.1511) | Acc_1: (94.41%) (36374/38528)\n",
      "Epoch: 123 | Batch_idx: 310 |  Loss_1: (0.1512) | Acc_1: (94.42%) (37587/39808)\n",
      "Epoch: 123 | Batch_idx: 320 |  Loss_1: (0.1520) | Acc_1: (94.39%) (38784/41088)\n",
      "Epoch: 123 | Batch_idx: 330 |  Loss_1: (0.1522) | Acc_1: (94.39%) (39991/42368)\n",
      "Epoch: 123 | Batch_idx: 340 |  Loss_1: (0.1523) | Acc_1: (94.38%) (41194/43648)\n",
      "Epoch: 123 | Batch_idx: 350 |  Loss_1: (0.1520) | Acc_1: (94.40%) (42410/44928)\n",
      "Epoch: 123 | Batch_idx: 360 |  Loss_1: (0.1528) | Acc_1: (94.36%) (43600/46208)\n",
      "Epoch: 123 | Batch_idx: 370 |  Loss_1: (0.1531) | Acc_1: (94.35%) (44807/47488)\n",
      "Epoch: 123 | Batch_idx: 380 |  Loss_1: (0.1532) | Acc_1: (94.34%) (46008/48768)\n",
      "Epoch: 123 | Batch_idx: 390 |  Loss_1: (0.1532) | Acc_1: (94.35%) (47175/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3017) | Acc: (91.89%) (9189/10000)\n",
      "Epoch: 124 | Batch_idx: 0 |  Loss_1: (0.1738) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 124 | Batch_idx: 10 |  Loss_1: (0.1405) | Acc_1: (94.82%) (1335/1408)\n",
      "Epoch: 124 | Batch_idx: 20 |  Loss_1: (0.1450) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 124 | Batch_idx: 30 |  Loss_1: (0.1471) | Acc_1: (94.68%) (3757/3968)\n",
      "Epoch: 124 | Batch_idx: 40 |  Loss_1: (0.1534) | Acc_1: (94.51%) (4960/5248)\n",
      "Epoch: 124 | Batch_idx: 50 |  Loss_1: (0.1551) | Acc_1: (94.41%) (6163/6528)\n",
      "Epoch: 124 | Batch_idx: 60 |  Loss_1: (0.1544) | Acc_1: (94.40%) (7371/7808)\n",
      "Epoch: 124 | Batch_idx: 70 |  Loss_1: (0.1577) | Acc_1: (94.31%) (8571/9088)\n",
      "Epoch: 124 | Batch_idx: 80 |  Loss_1: (0.1547) | Acc_1: (94.43%) (9790/10368)\n",
      "Epoch: 124 | Batch_idx: 90 |  Loss_1: (0.1539) | Acc_1: (94.46%) (11003/11648)\n",
      "Epoch: 124 | Batch_idx: 100 |  Loss_1: (0.1531) | Acc_1: (94.48%) (12214/12928)\n",
      "Epoch: 124 | Batch_idx: 110 |  Loss_1: (0.1530) | Acc_1: (94.49%) (13425/14208)\n",
      "Epoch: 124 | Batch_idx: 120 |  Loss_1: (0.1539) | Acc_1: (94.51%) (14637/15488)\n",
      "Epoch: 124 | Batch_idx: 130 |  Loss_1: (0.1530) | Acc_1: (94.49%) (15844/16768)\n",
      "Epoch: 124 | Batch_idx: 140 |  Loss_1: (0.1528) | Acc_1: (94.53%) (17060/18048)\n",
      "Epoch: 124 | Batch_idx: 150 |  Loss_1: (0.1500) | Acc_1: (94.63%) (18291/19328)\n",
      "Epoch: 124 | Batch_idx: 160 |  Loss_1: (0.1485) | Acc_1: (94.67%) (19509/20608)\n",
      "Epoch: 124 | Batch_idx: 170 |  Loss_1: (0.1485) | Acc_1: (94.68%) (20724/21888)\n",
      "Epoch: 124 | Batch_idx: 180 |  Loss_1: (0.1484) | Acc_1: (94.68%) (21936/23168)\n",
      "Epoch: 124 | Batch_idx: 190 |  Loss_1: (0.1495) | Acc_1: (94.60%) (23128/24448)\n",
      "Epoch: 124 | Batch_idx: 200 |  Loss_1: (0.1499) | Acc_1: (94.57%) (24330/25728)\n",
      "Epoch: 124 | Batch_idx: 210 |  Loss_1: (0.1517) | Acc_1: (94.49%) (25519/27008)\n",
      "Epoch: 124 | Batch_idx: 220 |  Loss_1: (0.1527) | Acc_1: (94.45%) (26719/28288)\n",
      "Epoch: 124 | Batch_idx: 230 |  Loss_1: (0.1517) | Acc_1: (94.47%) (27934/29568)\n",
      "Epoch: 124 | Batch_idx: 240 |  Loss_1: (0.1519) | Acc_1: (94.49%) (29148/30848)\n",
      "Epoch: 124 | Batch_idx: 250 |  Loss_1: (0.1523) | Acc_1: (94.48%) (30356/32128)\n",
      "Epoch: 124 | Batch_idx: 260 |  Loss_1: (0.1527) | Acc_1: (94.46%) (31557/33408)\n",
      "Epoch: 124 | Batch_idx: 270 |  Loss_1: (0.1524) | Acc_1: (94.48%) (32774/34688)\n",
      "Epoch: 124 | Batch_idx: 280 |  Loss_1: (0.1521) | Acc_1: (94.49%) (33987/35968)\n",
      "Epoch: 124 | Batch_idx: 290 |  Loss_1: (0.1522) | Acc_1: (94.47%) (35188/37248)\n",
      "Epoch: 124 | Batch_idx: 300 |  Loss_1: (0.1528) | Acc_1: (94.45%) (36390/38528)\n",
      "Epoch: 124 | Batch_idx: 310 |  Loss_1: (0.1527) | Acc_1: (94.46%) (37602/39808)\n",
      "Epoch: 124 | Batch_idx: 320 |  Loss_1: (0.1518) | Acc_1: (94.49%) (38824/41088)\n",
      "Epoch: 124 | Batch_idx: 330 |  Loss_1: (0.1520) | Acc_1: (94.47%) (40027/42368)\n",
      "Epoch: 124 | Batch_idx: 340 |  Loss_1: (0.1523) | Acc_1: (94.46%) (41231/43648)\n",
      "Epoch: 124 | Batch_idx: 350 |  Loss_1: (0.1527) | Acc_1: (94.43%) (42426/44928)\n",
      "Epoch: 124 | Batch_idx: 360 |  Loss_1: (0.1531) | Acc_1: (94.42%) (43631/46208)\n",
      "Epoch: 124 | Batch_idx: 370 |  Loss_1: (0.1532) | Acc_1: (94.43%) (44842/47488)\n",
      "Epoch: 124 | Batch_idx: 380 |  Loss_1: (0.1534) | Acc_1: (94.42%) (46049/48768)\n",
      "Epoch: 124 | Batch_idx: 390 |  Loss_1: (0.1535) | Acc_1: (94.41%) (47205/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3028) | Acc: (92.02%) (9202/10000)\n",
      "Epoch: 125 | Batch_idx: 0 |  Loss_1: (0.0404) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 125 | Batch_idx: 10 |  Loss_1: (0.1455) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 125 | Batch_idx: 20 |  Loss_1: (0.1442) | Acc_1: (94.31%) (2535/2688)\n",
      "Epoch: 125 | Batch_idx: 30 |  Loss_1: (0.1517) | Acc_1: (94.08%) (3733/3968)\n",
      "Epoch: 125 | Batch_idx: 40 |  Loss_1: (0.1551) | Acc_1: (94.02%) (4934/5248)\n",
      "Epoch: 125 | Batch_idx: 50 |  Loss_1: (0.1572) | Acc_1: (93.96%) (6134/6528)\n",
      "Epoch: 125 | Batch_idx: 60 |  Loss_1: (0.1580) | Acc_1: (93.94%) (7335/7808)\n",
      "Epoch: 125 | Batch_idx: 70 |  Loss_1: (0.1571) | Acc_1: (93.95%) (8538/9088)\n",
      "Epoch: 125 | Batch_idx: 80 |  Loss_1: (0.1549) | Acc_1: (94.09%) (9755/10368)\n",
      "Epoch: 125 | Batch_idx: 90 |  Loss_1: (0.1538) | Acc_1: (94.10%) (10961/11648)\n",
      "Epoch: 125 | Batch_idx: 100 |  Loss_1: (0.1523) | Acc_1: (94.14%) (12170/12928)\n",
      "Epoch: 125 | Batch_idx: 110 |  Loss_1: (0.1497) | Acc_1: (94.24%) (13389/14208)\n",
      "Epoch: 125 | Batch_idx: 120 |  Loss_1: (0.1504) | Acc_1: (94.19%) (14588/15488)\n",
      "Epoch: 125 | Batch_idx: 130 |  Loss_1: (0.1496) | Acc_1: (94.27%) (15807/16768)\n",
      "Epoch: 125 | Batch_idx: 140 |  Loss_1: (0.1510) | Acc_1: (94.25%) (17010/18048)\n",
      "Epoch: 125 | Batch_idx: 150 |  Loss_1: (0.1524) | Acc_1: (94.24%) (18214/19328)\n",
      "Epoch: 125 | Batch_idx: 160 |  Loss_1: (0.1520) | Acc_1: (94.26%) (19426/20608)\n",
      "Epoch: 125 | Batch_idx: 170 |  Loss_1: (0.1534) | Acc_1: (94.25%) (20629/21888)\n",
      "Epoch: 125 | Batch_idx: 180 |  Loss_1: (0.1533) | Acc_1: (94.26%) (21839/23168)\n",
      "Epoch: 125 | Batch_idx: 190 |  Loss_1: (0.1529) | Acc_1: (94.27%) (23048/24448)\n",
      "Epoch: 125 | Batch_idx: 200 |  Loss_1: (0.1529) | Acc_1: (94.26%) (24251/25728)\n",
      "Epoch: 125 | Batch_idx: 210 |  Loss_1: (0.1532) | Acc_1: (94.26%) (25457/27008)\n",
      "Epoch: 125 | Batch_idx: 220 |  Loss_1: (0.1531) | Acc_1: (94.28%) (26669/28288)\n",
      "Epoch: 125 | Batch_idx: 230 |  Loss_1: (0.1528) | Acc_1: (94.31%) (27886/29568)\n",
      "Epoch: 125 | Batch_idx: 240 |  Loss_1: (0.1522) | Acc_1: (94.33%) (29099/30848)\n",
      "Epoch: 125 | Batch_idx: 250 |  Loss_1: (0.1528) | Acc_1: (94.34%) (30310/32128)\n",
      "Epoch: 125 | Batch_idx: 260 |  Loss_1: (0.1522) | Acc_1: (94.36%) (31525/33408)\n",
      "Epoch: 125 | Batch_idx: 270 |  Loss_1: (0.1521) | Acc_1: (94.37%) (32736/34688)\n",
      "Epoch: 125 | Batch_idx: 280 |  Loss_1: (0.1529) | Acc_1: (94.35%) (33936/35968)\n",
      "Epoch: 125 | Batch_idx: 290 |  Loss_1: (0.1522) | Acc_1: (94.39%) (35158/37248)\n",
      "Epoch: 125 | Batch_idx: 300 |  Loss_1: (0.1527) | Acc_1: (94.37%) (36357/38528)\n",
      "Epoch: 125 | Batch_idx: 310 |  Loss_1: (0.1523) | Acc_1: (94.37%) (37566/39808)\n",
      "Epoch: 125 | Batch_idx: 320 |  Loss_1: (0.1522) | Acc_1: (94.36%) (38769/41088)\n",
      "Epoch: 125 | Batch_idx: 330 |  Loss_1: (0.1522) | Acc_1: (94.35%) (39976/42368)\n",
      "Epoch: 125 | Batch_idx: 340 |  Loss_1: (0.1516) | Acc_1: (94.38%) (41194/43648)\n",
      "Epoch: 125 | Batch_idx: 350 |  Loss_1: (0.1525) | Acc_1: (94.34%) (42384/44928)\n",
      "Epoch: 125 | Batch_idx: 360 |  Loss_1: (0.1524) | Acc_1: (94.34%) (43593/46208)\n",
      "Epoch: 125 | Batch_idx: 370 |  Loss_1: (0.1525) | Acc_1: (94.33%) (44797/47488)\n",
      "Epoch: 125 | Batch_idx: 380 |  Loss_1: (0.1525) | Acc_1: (94.34%) (46006/48768)\n",
      "Epoch: 125 | Batch_idx: 390 |  Loss_1: (0.1522) | Acc_1: (94.34%) (47168/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3348) | Acc: (91.83%) (9183/10000)\n",
      "Epoch: 126 | Batch_idx: 0 |  Loss_1: (0.1857) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 126 | Batch_idx: 10 |  Loss_1: (0.1748) | Acc_1: (93.96%) (1323/1408)\n",
      "Epoch: 126 | Batch_idx: 20 |  Loss_1: (0.1699) | Acc_1: (93.90%) (2524/2688)\n",
      "Epoch: 126 | Batch_idx: 30 |  Loss_1: (0.1711) | Acc_1: (93.98%) (3729/3968)\n",
      "Epoch: 126 | Batch_idx: 40 |  Loss_1: (0.1627) | Acc_1: (94.19%) (4943/5248)\n",
      "Epoch: 126 | Batch_idx: 50 |  Loss_1: (0.1668) | Acc_1: (94.01%) (6137/6528)\n",
      "Epoch: 126 | Batch_idx: 60 |  Loss_1: (0.1621) | Acc_1: (94.10%) (7347/7808)\n",
      "Epoch: 126 | Batch_idx: 70 |  Loss_1: (0.1578) | Acc_1: (94.23%) (8564/9088)\n",
      "Epoch: 126 | Batch_idx: 80 |  Loss_1: (0.1593) | Acc_1: (94.20%) (9767/10368)\n",
      "Epoch: 126 | Batch_idx: 90 |  Loss_1: (0.1577) | Acc_1: (94.30%) (10984/11648)\n",
      "Epoch: 126 | Batch_idx: 100 |  Loss_1: (0.1585) | Acc_1: (94.28%) (12188/12928)\n",
      "Epoch: 126 | Batch_idx: 110 |  Loss_1: (0.1601) | Acc_1: (94.25%) (13391/14208)\n",
      "Epoch: 126 | Batch_idx: 120 |  Loss_1: (0.1589) | Acc_1: (94.26%) (14599/15488)\n",
      "Epoch: 126 | Batch_idx: 130 |  Loss_1: (0.1587) | Acc_1: (94.28%) (15809/16768)\n",
      "Epoch: 126 | Batch_idx: 140 |  Loss_1: (0.1579) | Acc_1: (94.28%) (17016/18048)\n",
      "Epoch: 126 | Batch_idx: 150 |  Loss_1: (0.1560) | Acc_1: (94.32%) (18231/19328)\n",
      "Epoch: 126 | Batch_idx: 160 |  Loss_1: (0.1552) | Acc_1: (94.33%) (19440/20608)\n",
      "Epoch: 126 | Batch_idx: 170 |  Loss_1: (0.1573) | Acc_1: (94.27%) (20634/21888)\n",
      "Epoch: 126 | Batch_idx: 180 |  Loss_1: (0.1552) | Acc_1: (94.32%) (21853/23168)\n",
      "Epoch: 126 | Batch_idx: 190 |  Loss_1: (0.1545) | Acc_1: (94.35%) (23066/24448)\n",
      "Epoch: 126 | Batch_idx: 200 |  Loss_1: (0.1533) | Acc_1: (94.38%) (24282/25728)\n",
      "Epoch: 126 | Batch_idx: 210 |  Loss_1: (0.1533) | Acc_1: (94.40%) (25495/27008)\n",
      "Epoch: 126 | Batch_idx: 220 |  Loss_1: (0.1535) | Acc_1: (94.40%) (26705/28288)\n",
      "Epoch: 126 | Batch_idx: 230 |  Loss_1: (0.1539) | Acc_1: (94.39%) (27910/29568)\n",
      "Epoch: 126 | Batch_idx: 240 |  Loss_1: (0.1540) | Acc_1: (94.40%) (29120/30848)\n",
      "Epoch: 126 | Batch_idx: 250 |  Loss_1: (0.1528) | Acc_1: (94.45%) (30345/32128)\n",
      "Epoch: 126 | Batch_idx: 260 |  Loss_1: (0.1523) | Acc_1: (94.47%) (31560/33408)\n",
      "Epoch: 126 | Batch_idx: 270 |  Loss_1: (0.1530) | Acc_1: (94.43%) (32755/34688)\n",
      "Epoch: 126 | Batch_idx: 280 |  Loss_1: (0.1536) | Acc_1: (94.41%) (33957/35968)\n",
      "Epoch: 126 | Batch_idx: 290 |  Loss_1: (0.1547) | Acc_1: (94.36%) (35147/37248)\n",
      "Epoch: 126 | Batch_idx: 300 |  Loss_1: (0.1557) | Acc_1: (94.34%) (36347/38528)\n",
      "Epoch: 126 | Batch_idx: 310 |  Loss_1: (0.1569) | Acc_1: (94.29%) (37534/39808)\n",
      "Epoch: 126 | Batch_idx: 320 |  Loss_1: (0.1565) | Acc_1: (94.29%) (38741/41088)\n",
      "Epoch: 126 | Batch_idx: 330 |  Loss_1: (0.1568) | Acc_1: (94.29%) (39948/42368)\n",
      "Epoch: 126 | Batch_idx: 340 |  Loss_1: (0.1573) | Acc_1: (94.27%) (41147/43648)\n",
      "Epoch: 126 | Batch_idx: 350 |  Loss_1: (0.1571) | Acc_1: (94.28%) (42356/44928)\n",
      "Epoch: 126 | Batch_idx: 360 |  Loss_1: (0.1571) | Acc_1: (94.30%) (43575/46208)\n",
      "Epoch: 126 | Batch_idx: 370 |  Loss_1: (0.1572) | Acc_1: (94.31%) (44786/47488)\n",
      "Epoch: 126 | Batch_idx: 380 |  Loss_1: (0.1575) | Acc_1: (94.29%) (45985/48768)\n",
      "Epoch: 126 | Batch_idx: 390 |  Loss_1: (0.1576) | Acc_1: (94.30%) (47148/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3007) | Acc: (92.08%) (9208/10000)\n",
      "Epoch: 127 | Batch_idx: 0 |  Loss_1: (0.1590) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 127 | Batch_idx: 10 |  Loss_1: (0.1487) | Acc_1: (94.82%) (1335/1408)\n",
      "Epoch: 127 | Batch_idx: 20 |  Loss_1: (0.1582) | Acc_1: (94.38%) (2537/2688)\n",
      "Epoch: 127 | Batch_idx: 30 |  Loss_1: (0.1590) | Acc_1: (94.33%) (3743/3968)\n",
      "Epoch: 127 | Batch_idx: 40 |  Loss_1: (0.1567) | Acc_1: (94.44%) (4956/5248)\n",
      "Epoch: 127 | Batch_idx: 50 |  Loss_1: (0.1610) | Acc_1: (94.27%) (6154/6528)\n",
      "Epoch: 127 | Batch_idx: 60 |  Loss_1: (0.1628) | Acc_1: (94.17%) (7353/7808)\n",
      "Epoch: 127 | Batch_idx: 70 |  Loss_1: (0.1596) | Acc_1: (94.28%) (8568/9088)\n",
      "Epoch: 127 | Batch_idx: 80 |  Loss_1: (0.1561) | Acc_1: (94.35%) (9782/10368)\n",
      "Epoch: 127 | Batch_idx: 90 |  Loss_1: (0.1547) | Acc_1: (94.39%) (10994/11648)\n",
      "Epoch: 127 | Batch_idx: 100 |  Loss_1: (0.1541) | Acc_1: (94.41%) (12205/12928)\n",
      "Epoch: 127 | Batch_idx: 110 |  Loss_1: (0.1544) | Acc_1: (94.45%) (13420/14208)\n",
      "Epoch: 127 | Batch_idx: 120 |  Loss_1: (0.1555) | Acc_1: (94.38%) (14617/15488)\n",
      "Epoch: 127 | Batch_idx: 130 |  Loss_1: (0.1546) | Acc_1: (94.39%) (15828/16768)\n",
      "Epoch: 127 | Batch_idx: 140 |  Loss_1: (0.1535) | Acc_1: (94.42%) (17041/18048)\n",
      "Epoch: 127 | Batch_idx: 150 |  Loss_1: (0.1553) | Acc_1: (94.37%) (18240/19328)\n",
      "Epoch: 127 | Batch_idx: 160 |  Loss_1: (0.1548) | Acc_1: (94.43%) (19461/20608)\n",
      "Epoch: 127 | Batch_idx: 170 |  Loss_1: (0.1546) | Acc_1: (94.46%) (20675/21888)\n",
      "Epoch: 127 | Batch_idx: 180 |  Loss_1: (0.1564) | Acc_1: (94.41%) (21873/23168)\n",
      "Epoch: 127 | Batch_idx: 190 |  Loss_1: (0.1570) | Acc_1: (94.37%) (23071/24448)\n",
      "Epoch: 127 | Batch_idx: 200 |  Loss_1: (0.1564) | Acc_1: (94.41%) (24290/25728)\n",
      "Epoch: 127 | Batch_idx: 210 |  Loss_1: (0.1557) | Acc_1: (94.44%) (25507/27008)\n",
      "Epoch: 127 | Batch_idx: 220 |  Loss_1: (0.1547) | Acc_1: (94.46%) (26722/28288)\n",
      "Epoch: 127 | Batch_idx: 230 |  Loss_1: (0.1549) | Acc_1: (94.46%) (27929/29568)\n",
      "Epoch: 127 | Batch_idx: 240 |  Loss_1: (0.1546) | Acc_1: (94.46%) (29138/30848)\n",
      "Epoch: 127 | Batch_idx: 250 |  Loss_1: (0.1547) | Acc_1: (94.44%) (30343/32128)\n",
      "Epoch: 127 | Batch_idx: 260 |  Loss_1: (0.1554) | Acc_1: (94.40%) (31536/33408)\n",
      "Epoch: 127 | Batch_idx: 270 |  Loss_1: (0.1553) | Acc_1: (94.39%) (32743/34688)\n",
      "Epoch: 127 | Batch_idx: 280 |  Loss_1: (0.1545) | Acc_1: (94.41%) (33959/35968)\n",
      "Epoch: 127 | Batch_idx: 290 |  Loss_1: (0.1546) | Acc_1: (94.40%) (35161/37248)\n",
      "Epoch: 127 | Batch_idx: 300 |  Loss_1: (0.1559) | Acc_1: (94.37%) (36357/38528)\n",
      "Epoch: 127 | Batch_idx: 310 |  Loss_1: (0.1558) | Acc_1: (94.38%) (37569/39808)\n",
      "Epoch: 127 | Batch_idx: 320 |  Loss_1: (0.1567) | Acc_1: (94.33%) (38758/41088)\n",
      "Epoch: 127 | Batch_idx: 330 |  Loss_1: (0.1567) | Acc_1: (94.34%) (39971/42368)\n",
      "Epoch: 127 | Batch_idx: 340 |  Loss_1: (0.1559) | Acc_1: (94.37%) (41191/43648)\n",
      "Epoch: 127 | Batch_idx: 350 |  Loss_1: (0.1555) | Acc_1: (94.38%) (42404/44928)\n",
      "Epoch: 127 | Batch_idx: 360 |  Loss_1: (0.1559) | Acc_1: (94.37%) (43605/46208)\n",
      "Epoch: 127 | Batch_idx: 370 |  Loss_1: (0.1555) | Acc_1: (94.38%) (44817/47488)\n",
      "Epoch: 127 | Batch_idx: 380 |  Loss_1: (0.1554) | Acc_1: (94.36%) (46019/48768)\n",
      "Epoch: 127 | Batch_idx: 390 |  Loss_1: (0.1558) | Acc_1: (94.35%) (47174/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2959) | Acc: (92.48%) (9248/10000)\n",
      "Epoch: 128 | Batch_idx: 0 |  Loss_1: (0.1692) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 128 | Batch_idx: 10 |  Loss_1: (0.1411) | Acc_1: (94.82%) (1335/1408)\n",
      "Epoch: 128 | Batch_idx: 20 |  Loss_1: (0.1547) | Acc_1: (94.27%) (2534/2688)\n",
      "Epoch: 128 | Batch_idx: 30 |  Loss_1: (0.1497) | Acc_1: (94.41%) (3746/3968)\n",
      "Epoch: 128 | Batch_idx: 40 |  Loss_1: (0.1477) | Acc_1: (94.49%) (4959/5248)\n",
      "Epoch: 128 | Batch_idx: 50 |  Loss_1: (0.1476) | Acc_1: (94.49%) (6168/6528)\n",
      "Epoch: 128 | Batch_idx: 60 |  Loss_1: (0.1458) | Acc_1: (94.56%) (7383/7808)\n",
      "Epoch: 128 | Batch_idx: 70 |  Loss_1: (0.1455) | Acc_1: (94.55%) (8593/9088)\n",
      "Epoch: 128 | Batch_idx: 80 |  Loss_1: (0.1478) | Acc_1: (94.51%) (9799/10368)\n",
      "Epoch: 128 | Batch_idx: 90 |  Loss_1: (0.1496) | Acc_1: (94.44%) (11000/11648)\n",
      "Epoch: 128 | Batch_idx: 100 |  Loss_1: (0.1497) | Acc_1: (94.49%) (12216/12928)\n",
      "Epoch: 128 | Batch_idx: 110 |  Loss_1: (0.1501) | Acc_1: (94.46%) (13421/14208)\n",
      "Epoch: 128 | Batch_idx: 120 |  Loss_1: (0.1504) | Acc_1: (94.43%) (14626/15488)\n",
      "Epoch: 128 | Batch_idx: 130 |  Loss_1: (0.1515) | Acc_1: (94.39%) (15827/16768)\n",
      "Epoch: 128 | Batch_idx: 140 |  Loss_1: (0.1505) | Acc_1: (94.45%) (17047/18048)\n",
      "Epoch: 128 | Batch_idx: 150 |  Loss_1: (0.1494) | Acc_1: (94.51%) (18266/19328)\n",
      "Epoch: 128 | Batch_idx: 160 |  Loss_1: (0.1505) | Acc_1: (94.46%) (19466/20608)\n",
      "Epoch: 128 | Batch_idx: 170 |  Loss_1: (0.1513) | Acc_1: (94.41%) (20665/21888)\n",
      "Epoch: 128 | Batch_idx: 180 |  Loss_1: (0.1496) | Acc_1: (94.48%) (21888/23168)\n",
      "Epoch: 128 | Batch_idx: 190 |  Loss_1: (0.1496) | Acc_1: (94.49%) (23100/24448)\n",
      "Epoch: 128 | Batch_idx: 200 |  Loss_1: (0.1492) | Acc_1: (94.49%) (24311/25728)\n",
      "Epoch: 128 | Batch_idx: 210 |  Loss_1: (0.1494) | Acc_1: (94.49%) (25520/27008)\n",
      "Epoch: 128 | Batch_idx: 220 |  Loss_1: (0.1504) | Acc_1: (94.48%) (26726/28288)\n",
      "Epoch: 128 | Batch_idx: 230 |  Loss_1: (0.1492) | Acc_1: (94.54%) (27953/29568)\n",
      "Epoch: 128 | Batch_idx: 240 |  Loss_1: (0.1499) | Acc_1: (94.52%) (29156/30848)\n",
      "Epoch: 128 | Batch_idx: 250 |  Loss_1: (0.1499) | Acc_1: (94.51%) (30363/32128)\n",
      "Epoch: 128 | Batch_idx: 260 |  Loss_1: (0.1495) | Acc_1: (94.50%) (31572/33408)\n",
      "Epoch: 128 | Batch_idx: 270 |  Loss_1: (0.1497) | Acc_1: (94.49%) (32776/34688)\n",
      "Epoch: 128 | Batch_idx: 280 |  Loss_1: (0.1498) | Acc_1: (94.50%) (33991/35968)\n",
      "Epoch: 128 | Batch_idx: 290 |  Loss_1: (0.1507) | Acc_1: (94.47%) (35188/37248)\n",
      "Epoch: 128 | Batch_idx: 300 |  Loss_1: (0.1506) | Acc_1: (94.47%) (36399/38528)\n",
      "Epoch: 128 | Batch_idx: 310 |  Loss_1: (0.1506) | Acc_1: (94.47%) (37606/39808)\n",
      "Epoch: 128 | Batch_idx: 320 |  Loss_1: (0.1501) | Acc_1: (94.49%) (38823/41088)\n",
      "Epoch: 128 | Batch_idx: 330 |  Loss_1: (0.1503) | Acc_1: (94.49%) (40033/42368)\n",
      "Epoch: 128 | Batch_idx: 340 |  Loss_1: (0.1506) | Acc_1: (94.48%) (41238/43648)\n",
      "Epoch: 128 | Batch_idx: 350 |  Loss_1: (0.1506) | Acc_1: (94.48%) (42446/44928)\n",
      "Epoch: 128 | Batch_idx: 360 |  Loss_1: (0.1507) | Acc_1: (94.47%) (43654/46208)\n",
      "Epoch: 128 | Batch_idx: 370 |  Loss_1: (0.1506) | Acc_1: (94.46%) (44856/47488)\n",
      "Epoch: 128 | Batch_idx: 380 |  Loss_1: (0.1505) | Acc_1: (94.47%) (46069/48768)\n",
      "Epoch: 128 | Batch_idx: 390 |  Loss_1: (0.1515) | Acc_1: (94.44%) (47218/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2999) | Acc: (92.18%) (9218/10000)\n",
      "Epoch: 129 | Batch_idx: 0 |  Loss_1: (0.1208) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 129 | Batch_idx: 10 |  Loss_1: (0.1563) | Acc_1: (93.75%) (1320/1408)\n",
      "Epoch: 129 | Batch_idx: 20 |  Loss_1: (0.1540) | Acc_1: (93.97%) (2526/2688)\n",
      "Epoch: 129 | Batch_idx: 30 |  Loss_1: (0.1531) | Acc_1: (94.13%) (3735/3968)\n",
      "Epoch: 129 | Batch_idx: 40 |  Loss_1: (0.1514) | Acc_1: (94.21%) (4944/5248)\n",
      "Epoch: 129 | Batch_idx: 50 |  Loss_1: (0.1508) | Acc_1: (94.18%) (6148/6528)\n",
      "Epoch: 129 | Batch_idx: 60 |  Loss_1: (0.1522) | Acc_1: (94.15%) (7351/7808)\n",
      "Epoch: 129 | Batch_idx: 70 |  Loss_1: (0.1531) | Acc_1: (94.19%) (8560/9088)\n",
      "Epoch: 129 | Batch_idx: 80 |  Loss_1: (0.1542) | Acc_1: (94.15%) (9761/10368)\n",
      "Epoch: 129 | Batch_idx: 90 |  Loss_1: (0.1512) | Acc_1: (94.23%) (10976/11648)\n",
      "Epoch: 129 | Batch_idx: 100 |  Loss_1: (0.1532) | Acc_1: (94.16%) (12173/12928)\n",
      "Epoch: 129 | Batch_idx: 110 |  Loss_1: (0.1529) | Acc_1: (94.17%) (13380/14208)\n",
      "Epoch: 129 | Batch_idx: 120 |  Loss_1: (0.1519) | Acc_1: (94.21%) (14591/15488)\n",
      "Epoch: 129 | Batch_idx: 130 |  Loss_1: (0.1497) | Acc_1: (94.28%) (15809/16768)\n",
      "Epoch: 129 | Batch_idx: 140 |  Loss_1: (0.1496) | Acc_1: (94.30%) (17020/18048)\n",
      "Epoch: 129 | Batch_idx: 150 |  Loss_1: (0.1488) | Acc_1: (94.34%) (18234/19328)\n",
      "Epoch: 129 | Batch_idx: 160 |  Loss_1: (0.1503) | Acc_1: (94.31%) (19435/20608)\n",
      "Epoch: 129 | Batch_idx: 170 |  Loss_1: (0.1495) | Acc_1: (94.33%) (20648/21888)\n",
      "Epoch: 129 | Batch_idx: 180 |  Loss_1: (0.1488) | Acc_1: (94.37%) (21863/23168)\n",
      "Epoch: 129 | Batch_idx: 190 |  Loss_1: (0.1497) | Acc_1: (94.33%) (23061/24448)\n",
      "Epoch: 129 | Batch_idx: 200 |  Loss_1: (0.1505) | Acc_1: (94.29%) (24260/25728)\n",
      "Epoch: 129 | Batch_idx: 210 |  Loss_1: (0.1507) | Acc_1: (94.29%) (25466/27008)\n",
      "Epoch: 129 | Batch_idx: 220 |  Loss_1: (0.1516) | Acc_1: (94.27%) (26667/28288)\n",
      "Epoch: 129 | Batch_idx: 230 |  Loss_1: (0.1519) | Acc_1: (94.26%) (27871/29568)\n",
      "Epoch: 129 | Batch_idx: 240 |  Loss_1: (0.1512) | Acc_1: (94.31%) (29092/30848)\n",
      "Epoch: 129 | Batch_idx: 250 |  Loss_1: (0.1509) | Acc_1: (94.34%) (30310/32128)\n",
      "Epoch: 129 | Batch_idx: 260 |  Loss_1: (0.1520) | Acc_1: (94.32%) (31512/33408)\n",
      "Epoch: 129 | Batch_idx: 270 |  Loss_1: (0.1533) | Acc_1: (94.27%) (32699/34688)\n",
      "Epoch: 129 | Batch_idx: 280 |  Loss_1: (0.1530) | Acc_1: (94.28%) (33911/35968)\n",
      "Epoch: 129 | Batch_idx: 290 |  Loss_1: (0.1533) | Acc_1: (94.26%) (35111/37248)\n",
      "Epoch: 129 | Batch_idx: 300 |  Loss_1: (0.1535) | Acc_1: (94.24%) (36310/38528)\n",
      "Epoch: 129 | Batch_idx: 310 |  Loss_1: (0.1539) | Acc_1: (94.23%) (37512/39808)\n",
      "Epoch: 129 | Batch_idx: 320 |  Loss_1: (0.1542) | Acc_1: (94.23%) (38716/41088)\n",
      "Epoch: 129 | Batch_idx: 330 |  Loss_1: (0.1540) | Acc_1: (94.23%) (39923/42368)\n",
      "Epoch: 129 | Batch_idx: 340 |  Loss_1: (0.1535) | Acc_1: (94.24%) (41134/43648)\n",
      "Epoch: 129 | Batch_idx: 350 |  Loss_1: (0.1541) | Acc_1: (94.22%) (42332/44928)\n",
      "Epoch: 129 | Batch_idx: 360 |  Loss_1: (0.1536) | Acc_1: (94.25%) (43553/46208)\n",
      "Epoch: 129 | Batch_idx: 370 |  Loss_1: (0.1536) | Acc_1: (94.27%) (44765/47488)\n",
      "Epoch: 129 | Batch_idx: 380 |  Loss_1: (0.1536) | Acc_1: (94.28%) (45978/48768)\n",
      "Epoch: 129 | Batch_idx: 390 |  Loss_1: (0.1548) | Acc_1: (94.24%) (47119/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3078) | Acc: (92.21%) (9221/10000)\n",
      "Epoch: 130 | Batch_idx: 0 |  Loss_1: (0.2104) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 130 | Batch_idx: 10 |  Loss_1: (0.1417) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 130 | Batch_idx: 20 |  Loss_1: (0.1383) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 130 | Batch_idx: 30 |  Loss_1: (0.1347) | Acc_1: (95.04%) (3771/3968)\n",
      "Epoch: 130 | Batch_idx: 40 |  Loss_1: (0.1386) | Acc_1: (95.03%) (4987/5248)\n",
      "Epoch: 130 | Batch_idx: 50 |  Loss_1: (0.1394) | Acc_1: (94.93%) (6197/6528)\n",
      "Epoch: 130 | Batch_idx: 60 |  Loss_1: (0.1384) | Acc_1: (94.93%) (7412/7808)\n",
      "Epoch: 130 | Batch_idx: 70 |  Loss_1: (0.1390) | Acc_1: (94.91%) (8625/9088)\n",
      "Epoch: 130 | Batch_idx: 80 |  Loss_1: (0.1437) | Acc_1: (94.77%) (9826/10368)\n",
      "Epoch: 130 | Batch_idx: 90 |  Loss_1: (0.1436) | Acc_1: (94.73%) (11034/11648)\n",
      "Epoch: 130 | Batch_idx: 100 |  Loss_1: (0.1424) | Acc_1: (94.79%) (12255/12928)\n",
      "Epoch: 130 | Batch_idx: 110 |  Loss_1: (0.1417) | Acc_1: (94.80%) (13469/14208)\n",
      "Epoch: 130 | Batch_idx: 120 |  Loss_1: (0.1434) | Acc_1: (94.73%) (14672/15488)\n",
      "Epoch: 130 | Batch_idx: 130 |  Loss_1: (0.1424) | Acc_1: (94.81%) (15897/16768)\n",
      "Epoch: 130 | Batch_idx: 140 |  Loss_1: (0.1426) | Acc_1: (94.79%) (17107/18048)\n",
      "Epoch: 130 | Batch_idx: 150 |  Loss_1: (0.1429) | Acc_1: (94.79%) (18321/19328)\n",
      "Epoch: 130 | Batch_idx: 160 |  Loss_1: (0.1428) | Acc_1: (94.80%) (19536/20608)\n",
      "Epoch: 130 | Batch_idx: 170 |  Loss_1: (0.1442) | Acc_1: (94.73%) (20734/21888)\n",
      "Epoch: 130 | Batch_idx: 180 |  Loss_1: (0.1447) | Acc_1: (94.72%) (21944/23168)\n",
      "Epoch: 130 | Batch_idx: 190 |  Loss_1: (0.1437) | Acc_1: (94.75%) (23165/24448)\n",
      "Epoch: 130 | Batch_idx: 200 |  Loss_1: (0.1440) | Acc_1: (94.75%) (24376/25728)\n",
      "Epoch: 130 | Batch_idx: 210 |  Loss_1: (0.1451) | Acc_1: (94.72%) (25582/27008)\n",
      "Epoch: 130 | Batch_idx: 220 |  Loss_1: (0.1435) | Acc_1: (94.78%) (26812/28288)\n",
      "Epoch: 130 | Batch_idx: 230 |  Loss_1: (0.1445) | Acc_1: (94.75%) (28015/29568)\n",
      "Epoch: 130 | Batch_idx: 240 |  Loss_1: (0.1453) | Acc_1: (94.73%) (29223/30848)\n",
      "Epoch: 130 | Batch_idx: 250 |  Loss_1: (0.1457) | Acc_1: (94.70%) (30426/32128)\n",
      "Epoch: 130 | Batch_idx: 260 |  Loss_1: (0.1461) | Acc_1: (94.68%) (31632/33408)\n",
      "Epoch: 130 | Batch_idx: 270 |  Loss_1: (0.1467) | Acc_1: (94.66%) (32835/34688)\n",
      "Epoch: 130 | Batch_idx: 280 |  Loss_1: (0.1475) | Acc_1: (94.60%) (34027/35968)\n",
      "Epoch: 130 | Batch_idx: 290 |  Loss_1: (0.1479) | Acc_1: (94.60%) (35237/37248)\n",
      "Epoch: 130 | Batch_idx: 300 |  Loss_1: (0.1481) | Acc_1: (94.58%) (36440/38528)\n",
      "Epoch: 130 | Batch_idx: 310 |  Loss_1: (0.1478) | Acc_1: (94.59%) (37656/39808)\n",
      "Epoch: 130 | Batch_idx: 320 |  Loss_1: (0.1485) | Acc_1: (94.58%) (38861/41088)\n",
      "Epoch: 130 | Batch_idx: 330 |  Loss_1: (0.1489) | Acc_1: (94.55%) (40061/42368)\n",
      "Epoch: 130 | Batch_idx: 340 |  Loss_1: (0.1494) | Acc_1: (94.54%) (41263/43648)\n",
      "Epoch: 130 | Batch_idx: 350 |  Loss_1: (0.1505) | Acc_1: (94.50%) (42457/44928)\n",
      "Epoch: 130 | Batch_idx: 360 |  Loss_1: (0.1504) | Acc_1: (94.50%) (43668/46208)\n",
      "Epoch: 130 | Batch_idx: 370 |  Loss_1: (0.1506) | Acc_1: (94.50%) (44874/47488)\n",
      "Epoch: 130 | Batch_idx: 380 |  Loss_1: (0.1510) | Acc_1: (94.48%) (46076/48768)\n",
      "Epoch: 130 | Batch_idx: 390 |  Loss_1: (0.1512) | Acc_1: (94.49%) (47244/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3202) | Acc: (92.15%) (9215/10000)\n",
      "Epoch: 131 | Batch_idx: 0 |  Loss_1: (0.1686) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 131 | Batch_idx: 10 |  Loss_1: (0.1853) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 131 | Batch_idx: 20 |  Loss_1: (0.1729) | Acc_1: (93.56%) (2515/2688)\n",
      "Epoch: 131 | Batch_idx: 30 |  Loss_1: (0.1587) | Acc_1: (94.10%) (3734/3968)\n",
      "Epoch: 131 | Batch_idx: 40 |  Loss_1: (0.1550) | Acc_1: (94.23%) (4945/5248)\n",
      "Epoch: 131 | Batch_idx: 50 |  Loss_1: (0.1521) | Acc_1: (94.27%) (6154/6528)\n",
      "Epoch: 131 | Batch_idx: 60 |  Loss_1: (0.1488) | Acc_1: (94.43%) (7373/7808)\n",
      "Epoch: 131 | Batch_idx: 70 |  Loss_1: (0.1451) | Acc_1: (94.54%) (8592/9088)\n",
      "Epoch: 131 | Batch_idx: 80 |  Loss_1: (0.1434) | Acc_1: (94.62%) (9810/10368)\n",
      "Epoch: 131 | Batch_idx: 90 |  Loss_1: (0.1423) | Acc_1: (94.75%) (11036/11648)\n",
      "Epoch: 131 | Batch_idx: 100 |  Loss_1: (0.1434) | Acc_1: (94.72%) (12246/12928)\n",
      "Epoch: 131 | Batch_idx: 110 |  Loss_1: (0.1439) | Acc_1: (94.66%) (13450/14208)\n",
      "Epoch: 131 | Batch_idx: 120 |  Loss_1: (0.1453) | Acc_1: (94.60%) (14652/15488)\n",
      "Epoch: 131 | Batch_idx: 130 |  Loss_1: (0.1477) | Acc_1: (94.53%) (15851/16768)\n",
      "Epoch: 131 | Batch_idx: 140 |  Loss_1: (0.1490) | Acc_1: (94.45%) (17047/18048)\n",
      "Epoch: 131 | Batch_idx: 150 |  Loss_1: (0.1485) | Acc_1: (94.50%) (18264/19328)\n",
      "Epoch: 131 | Batch_idx: 160 |  Loss_1: (0.1493) | Acc_1: (94.49%) (19473/20608)\n",
      "Epoch: 131 | Batch_idx: 170 |  Loss_1: (0.1494) | Acc_1: (94.54%) (20692/21888)\n",
      "Epoch: 131 | Batch_idx: 180 |  Loss_1: (0.1485) | Acc_1: (94.56%) (21908/23168)\n",
      "Epoch: 131 | Batch_idx: 190 |  Loss_1: (0.1486) | Acc_1: (94.57%) (23120/24448)\n",
      "Epoch: 131 | Batch_idx: 200 |  Loss_1: (0.1475) | Acc_1: (94.62%) (24344/25728)\n",
      "Epoch: 131 | Batch_idx: 210 |  Loss_1: (0.1479) | Acc_1: (94.60%) (25550/27008)\n",
      "Epoch: 131 | Batch_idx: 220 |  Loss_1: (0.1470) | Acc_1: (94.65%) (26775/28288)\n",
      "Epoch: 131 | Batch_idx: 230 |  Loss_1: (0.1471) | Acc_1: (94.65%) (27987/29568)\n",
      "Epoch: 131 | Batch_idx: 240 |  Loss_1: (0.1469) | Acc_1: (94.65%) (29198/30848)\n",
      "Epoch: 131 | Batch_idx: 250 |  Loss_1: (0.1475) | Acc_1: (94.64%) (30406/32128)\n",
      "Epoch: 131 | Batch_idx: 260 |  Loss_1: (0.1476) | Acc_1: (94.63%) (31615/33408)\n",
      "Epoch: 131 | Batch_idx: 270 |  Loss_1: (0.1471) | Acc_1: (94.66%) (32835/34688)\n",
      "Epoch: 131 | Batch_idx: 280 |  Loss_1: (0.1474) | Acc_1: (94.64%) (34041/35968)\n",
      "Epoch: 131 | Batch_idx: 290 |  Loss_1: (0.1478) | Acc_1: (94.61%) (35241/37248)\n",
      "Epoch: 131 | Batch_idx: 300 |  Loss_1: (0.1486) | Acc_1: (94.57%) (36436/38528)\n",
      "Epoch: 131 | Batch_idx: 310 |  Loss_1: (0.1491) | Acc_1: (94.56%) (37643/39808)\n",
      "Epoch: 131 | Batch_idx: 320 |  Loss_1: (0.1490) | Acc_1: (94.58%) (38863/41088)\n",
      "Epoch: 131 | Batch_idx: 330 |  Loss_1: (0.1487) | Acc_1: (94.58%) (40071/42368)\n",
      "Epoch: 131 | Batch_idx: 340 |  Loss_1: (0.1481) | Acc_1: (94.59%) (41287/43648)\n",
      "Epoch: 131 | Batch_idx: 350 |  Loss_1: (0.1479) | Acc_1: (94.58%) (42491/44928)\n",
      "Epoch: 131 | Batch_idx: 360 |  Loss_1: (0.1481) | Acc_1: (94.56%) (43695/46208)\n",
      "Epoch: 131 | Batch_idx: 370 |  Loss_1: (0.1487) | Acc_1: (94.54%) (44895/47488)\n",
      "Epoch: 131 | Batch_idx: 380 |  Loss_1: (0.1483) | Acc_1: (94.55%) (46110/48768)\n",
      "Epoch: 131 | Batch_idx: 390 |  Loss_1: (0.1483) | Acc_1: (94.54%) (47270/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3233) | Acc: (91.90%) (9190/10000)\n",
      "Epoch: 132 | Batch_idx: 0 |  Loss_1: (0.1752) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 132 | Batch_idx: 10 |  Loss_1: (0.1549) | Acc_1: (94.11%) (1325/1408)\n",
      "Epoch: 132 | Batch_idx: 20 |  Loss_1: (0.1393) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 132 | Batch_idx: 30 |  Loss_1: (0.1400) | Acc_1: (94.93%) (3767/3968)\n",
      "Epoch: 132 | Batch_idx: 40 |  Loss_1: (0.1368) | Acc_1: (95.16%) (4994/5248)\n",
      "Epoch: 132 | Batch_idx: 50 |  Loss_1: (0.1363) | Acc_1: (95.11%) (6209/6528)\n",
      "Epoch: 132 | Batch_idx: 60 |  Loss_1: (0.1381) | Acc_1: (95.08%) (7424/7808)\n",
      "Epoch: 132 | Batch_idx: 70 |  Loss_1: (0.1373) | Acc_1: (95.09%) (8642/9088)\n",
      "Epoch: 132 | Batch_idx: 80 |  Loss_1: (0.1383) | Acc_1: (95.01%) (9851/10368)\n",
      "Epoch: 132 | Batch_idx: 90 |  Loss_1: (0.1365) | Acc_1: (95.00%) (11066/11648)\n",
      "Epoch: 132 | Batch_idx: 100 |  Loss_1: (0.1371) | Acc_1: (94.99%) (12280/12928)\n",
      "Epoch: 132 | Batch_idx: 110 |  Loss_1: (0.1407) | Acc_1: (94.85%) (13476/14208)\n",
      "Epoch: 132 | Batch_idx: 120 |  Loss_1: (0.1416) | Acc_1: (94.82%) (14685/15488)\n",
      "Epoch: 132 | Batch_idx: 130 |  Loss_1: (0.1401) | Acc_1: (94.86%) (15906/16768)\n",
      "Epoch: 132 | Batch_idx: 140 |  Loss_1: (0.1401) | Acc_1: (94.88%) (17124/18048)\n",
      "Epoch: 132 | Batch_idx: 150 |  Loss_1: (0.1397) | Acc_1: (94.88%) (18338/19328)\n",
      "Epoch: 132 | Batch_idx: 160 |  Loss_1: (0.1390) | Acc_1: (94.92%) (19561/20608)\n",
      "Epoch: 132 | Batch_idx: 170 |  Loss_1: (0.1391) | Acc_1: (94.89%) (20770/21888)\n",
      "Epoch: 132 | Batch_idx: 180 |  Loss_1: (0.1400) | Acc_1: (94.85%) (21975/23168)\n",
      "Epoch: 132 | Batch_idx: 190 |  Loss_1: (0.1395) | Acc_1: (94.86%) (23192/24448)\n",
      "Epoch: 132 | Batch_idx: 200 |  Loss_1: (0.1396) | Acc_1: (94.85%) (24402/25728)\n",
      "Epoch: 132 | Batch_idx: 210 |  Loss_1: (0.1379) | Acc_1: (94.91%) (25632/27008)\n",
      "Epoch: 132 | Batch_idx: 220 |  Loss_1: (0.1385) | Acc_1: (94.91%) (26847/28288)\n",
      "Epoch: 132 | Batch_idx: 230 |  Loss_1: (0.1391) | Acc_1: (94.85%) (28045/29568)\n",
      "Epoch: 132 | Batch_idx: 240 |  Loss_1: (0.1386) | Acc_1: (94.87%) (29265/30848)\n",
      "Epoch: 132 | Batch_idx: 250 |  Loss_1: (0.1395) | Acc_1: (94.81%) (30462/32128)\n",
      "Epoch: 132 | Batch_idx: 260 |  Loss_1: (0.1397) | Acc_1: (94.81%) (31673/33408)\n",
      "Epoch: 132 | Batch_idx: 270 |  Loss_1: (0.1406) | Acc_1: (94.78%) (32877/34688)\n",
      "Epoch: 132 | Batch_idx: 280 |  Loss_1: (0.1411) | Acc_1: (94.75%) (34079/35968)\n",
      "Epoch: 132 | Batch_idx: 290 |  Loss_1: (0.1419) | Acc_1: (94.72%) (35282/37248)\n",
      "Epoch: 132 | Batch_idx: 300 |  Loss_1: (0.1424) | Acc_1: (94.72%) (36492/38528)\n",
      "Epoch: 132 | Batch_idx: 310 |  Loss_1: (0.1427) | Acc_1: (94.70%) (37699/39808)\n",
      "Epoch: 132 | Batch_idx: 320 |  Loss_1: (0.1437) | Acc_1: (94.66%) (38893/41088)\n",
      "Epoch: 132 | Batch_idx: 330 |  Loss_1: (0.1435) | Acc_1: (94.65%) (40103/42368)\n",
      "Epoch: 132 | Batch_idx: 340 |  Loss_1: (0.1430) | Acc_1: (94.68%) (41325/43648)\n",
      "Epoch: 132 | Batch_idx: 350 |  Loss_1: (0.1443) | Acc_1: (94.64%) (42522/44928)\n",
      "Epoch: 132 | Batch_idx: 360 |  Loss_1: (0.1445) | Acc_1: (94.65%) (43734/46208)\n",
      "Epoch: 132 | Batch_idx: 370 |  Loss_1: (0.1446) | Acc_1: (94.63%) (44938/47488)\n",
      "Epoch: 132 | Batch_idx: 380 |  Loss_1: (0.1444) | Acc_1: (94.64%) (46155/48768)\n",
      "Epoch: 132 | Batch_idx: 390 |  Loss_1: (0.1441) | Acc_1: (94.66%) (47331/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3114) | Acc: (92.33%) (9233/10000)\n",
      "Epoch: 133 | Batch_idx: 0 |  Loss_1: (0.1362) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 133 | Batch_idx: 10 |  Loss_1: (0.1271) | Acc_1: (95.60%) (1346/1408)\n",
      "Epoch: 133 | Batch_idx: 20 |  Loss_1: (0.1404) | Acc_1: (94.94%) (2552/2688)\n",
      "Epoch: 133 | Batch_idx: 30 |  Loss_1: (0.1474) | Acc_1: (94.68%) (3757/3968)\n",
      "Epoch: 133 | Batch_idx: 40 |  Loss_1: (0.1484) | Acc_1: (94.65%) (4967/5248)\n",
      "Epoch: 133 | Batch_idx: 50 |  Loss_1: (0.1461) | Acc_1: (94.72%) (6183/6528)\n",
      "Epoch: 133 | Batch_idx: 60 |  Loss_1: (0.1464) | Acc_1: (94.65%) (7390/7808)\n",
      "Epoch: 133 | Batch_idx: 70 |  Loss_1: (0.1445) | Acc_1: (94.71%) (8607/9088)\n",
      "Epoch: 133 | Batch_idx: 80 |  Loss_1: (0.1449) | Acc_1: (94.70%) (9819/10368)\n",
      "Epoch: 133 | Batch_idx: 90 |  Loss_1: (0.1459) | Acc_1: (94.65%) (11025/11648)\n",
      "Epoch: 133 | Batch_idx: 100 |  Loss_1: (0.1435) | Acc_1: (94.72%) (12246/12928)\n",
      "Epoch: 133 | Batch_idx: 110 |  Loss_1: (0.1430) | Acc_1: (94.74%) (13460/14208)\n",
      "Epoch: 133 | Batch_idx: 120 |  Loss_1: (0.1426) | Acc_1: (94.74%) (14674/15488)\n",
      "Epoch: 133 | Batch_idx: 130 |  Loss_1: (0.1418) | Acc_1: (94.77%) (15891/16768)\n",
      "Epoch: 133 | Batch_idx: 140 |  Loss_1: (0.1414) | Acc_1: (94.80%) (17109/18048)\n",
      "Epoch: 133 | Batch_idx: 150 |  Loss_1: (0.1421) | Acc_1: (94.76%) (18316/19328)\n",
      "Epoch: 133 | Batch_idx: 160 |  Loss_1: (0.1435) | Acc_1: (94.70%) (19515/20608)\n",
      "Epoch: 133 | Batch_idx: 170 |  Loss_1: (0.1433) | Acc_1: (94.68%) (20723/21888)\n",
      "Epoch: 133 | Batch_idx: 180 |  Loss_1: (0.1429) | Acc_1: (94.71%) (21943/23168)\n",
      "Epoch: 133 | Batch_idx: 190 |  Loss_1: (0.1431) | Acc_1: (94.74%) (23162/24448)\n",
      "Epoch: 133 | Batch_idx: 200 |  Loss_1: (0.1431) | Acc_1: (94.75%) (24378/25728)\n",
      "Epoch: 133 | Batch_idx: 210 |  Loss_1: (0.1441) | Acc_1: (94.71%) (25580/27008)\n",
      "Epoch: 133 | Batch_idx: 220 |  Loss_1: (0.1439) | Acc_1: (94.72%) (26795/28288)\n",
      "Epoch: 133 | Batch_idx: 230 |  Loss_1: (0.1436) | Acc_1: (94.72%) (28007/29568)\n",
      "Epoch: 133 | Batch_idx: 240 |  Loss_1: (0.1444) | Acc_1: (94.69%) (29211/30848)\n",
      "Epoch: 133 | Batch_idx: 250 |  Loss_1: (0.1444) | Acc_1: (94.71%) (30427/32128)\n",
      "Epoch: 133 | Batch_idx: 260 |  Loss_1: (0.1446) | Acc_1: (94.68%) (31630/33408)\n",
      "Epoch: 133 | Batch_idx: 270 |  Loss_1: (0.1447) | Acc_1: (94.69%) (32845/34688)\n",
      "Epoch: 133 | Batch_idx: 280 |  Loss_1: (0.1443) | Acc_1: (94.70%) (34061/35968)\n",
      "Epoch: 133 | Batch_idx: 290 |  Loss_1: (0.1446) | Acc_1: (94.70%) (35275/37248)\n",
      "Epoch: 133 | Batch_idx: 300 |  Loss_1: (0.1444) | Acc_1: (94.70%) (36486/38528)\n",
      "Epoch: 133 | Batch_idx: 310 |  Loss_1: (0.1449) | Acc_1: (94.67%) (37685/39808)\n",
      "Epoch: 133 | Batch_idx: 320 |  Loss_1: (0.1450) | Acc_1: (94.67%) (38897/41088)\n",
      "Epoch: 133 | Batch_idx: 330 |  Loss_1: (0.1448) | Acc_1: (94.69%) (40117/42368)\n",
      "Epoch: 133 | Batch_idx: 340 |  Loss_1: (0.1451) | Acc_1: (94.67%) (41322/43648)\n",
      "Epoch: 133 | Batch_idx: 350 |  Loss_1: (0.1451) | Acc_1: (94.65%) (42526/44928)\n",
      "Epoch: 133 | Batch_idx: 360 |  Loss_1: (0.1453) | Acc_1: (94.65%) (43734/46208)\n",
      "Epoch: 133 | Batch_idx: 370 |  Loss_1: (0.1457) | Acc_1: (94.65%) (44948/47488)\n",
      "Epoch: 133 | Batch_idx: 380 |  Loss_1: (0.1461) | Acc_1: (94.64%) (46155/48768)\n",
      "Epoch: 133 | Batch_idx: 390 |  Loss_1: (0.1461) | Acc_1: (94.64%) (47321/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3161) | Acc: (92.39%) (9239/10000)\n",
      "Epoch: 134 | Batch_idx: 0 |  Loss_1: (0.1180) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 134 | Batch_idx: 10 |  Loss_1: (0.1506) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 134 | Batch_idx: 20 |  Loss_1: (0.1404) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 134 | Batch_idx: 30 |  Loss_1: (0.1517) | Acc_1: (94.46%) (3748/3968)\n",
      "Epoch: 134 | Batch_idx: 40 |  Loss_1: (0.1575) | Acc_1: (94.26%) (4947/5248)\n",
      "Epoch: 134 | Batch_idx: 50 |  Loss_1: (0.1531) | Acc_1: (94.33%) (6158/6528)\n",
      "Epoch: 134 | Batch_idx: 60 |  Loss_1: (0.1488) | Acc_1: (94.47%) (7376/7808)\n",
      "Epoch: 134 | Batch_idx: 70 |  Loss_1: (0.1441) | Acc_1: (94.60%) (8597/9088)\n",
      "Epoch: 134 | Batch_idx: 80 |  Loss_1: (0.1443) | Acc_1: (94.64%) (9812/10368)\n",
      "Epoch: 134 | Batch_idx: 90 |  Loss_1: (0.1432) | Acc_1: (94.69%) (11030/11648)\n",
      "Epoch: 134 | Batch_idx: 100 |  Loss_1: (0.1418) | Acc_1: (94.72%) (12245/12928)\n",
      "Epoch: 134 | Batch_idx: 110 |  Loss_1: (0.1423) | Acc_1: (94.71%) (13457/14208)\n",
      "Epoch: 134 | Batch_idx: 120 |  Loss_1: (0.1430) | Acc_1: (94.70%) (14667/15488)\n",
      "Epoch: 134 | Batch_idx: 130 |  Loss_1: (0.1442) | Acc_1: (94.67%) (15874/16768)\n",
      "Epoch: 134 | Batch_idx: 140 |  Loss_1: (0.1443) | Acc_1: (94.62%) (17077/18048)\n",
      "Epoch: 134 | Batch_idx: 150 |  Loss_1: (0.1449) | Acc_1: (94.59%) (18282/19328)\n",
      "Epoch: 134 | Batch_idx: 160 |  Loss_1: (0.1439) | Acc_1: (94.65%) (19505/20608)\n",
      "Epoch: 134 | Batch_idx: 170 |  Loss_1: (0.1439) | Acc_1: (94.65%) (20718/21888)\n",
      "Epoch: 134 | Batch_idx: 180 |  Loss_1: (0.1424) | Acc_1: (94.74%) (21949/23168)\n",
      "Epoch: 134 | Batch_idx: 190 |  Loss_1: (0.1422) | Acc_1: (94.74%) (23162/24448)\n",
      "Epoch: 134 | Batch_idx: 200 |  Loss_1: (0.1433) | Acc_1: (94.73%) (24371/25728)\n",
      "Epoch: 134 | Batch_idx: 210 |  Loss_1: (0.1430) | Acc_1: (94.74%) (25588/27008)\n",
      "Epoch: 134 | Batch_idx: 220 |  Loss_1: (0.1439) | Acc_1: (94.74%) (26801/28288)\n",
      "Epoch: 134 | Batch_idx: 230 |  Loss_1: (0.1440) | Acc_1: (94.74%) (28014/29568)\n",
      "Epoch: 134 | Batch_idx: 240 |  Loss_1: (0.1441) | Acc_1: (94.76%) (29231/30848)\n",
      "Epoch: 134 | Batch_idx: 250 |  Loss_1: (0.1435) | Acc_1: (94.77%) (30449/32128)\n",
      "Epoch: 134 | Batch_idx: 260 |  Loss_1: (0.1443) | Acc_1: (94.72%) (31644/33408)\n",
      "Epoch: 134 | Batch_idx: 270 |  Loss_1: (0.1446) | Acc_1: (94.73%) (32859/34688)\n",
      "Epoch: 134 | Batch_idx: 280 |  Loss_1: (0.1447) | Acc_1: (94.71%) (34067/35968)\n",
      "Epoch: 134 | Batch_idx: 290 |  Loss_1: (0.1453) | Acc_1: (94.70%) (35272/37248)\n",
      "Epoch: 134 | Batch_idx: 300 |  Loss_1: (0.1454) | Acc_1: (94.69%) (36484/38528)\n",
      "Epoch: 134 | Batch_idx: 310 |  Loss_1: (0.1452) | Acc_1: (94.70%) (37700/39808)\n",
      "Epoch: 134 | Batch_idx: 320 |  Loss_1: (0.1449) | Acc_1: (94.71%) (38914/41088)\n",
      "Epoch: 134 | Batch_idx: 330 |  Loss_1: (0.1449) | Acc_1: (94.72%) (40132/42368)\n",
      "Epoch: 134 | Batch_idx: 340 |  Loss_1: (0.1448) | Acc_1: (94.73%) (41348/43648)\n",
      "Epoch: 134 | Batch_idx: 350 |  Loss_1: (0.1449) | Acc_1: (94.72%) (42555/44928)\n",
      "Epoch: 134 | Batch_idx: 360 |  Loss_1: (0.1449) | Acc_1: (94.73%) (43772/46208)\n",
      "Epoch: 134 | Batch_idx: 370 |  Loss_1: (0.1450) | Acc_1: (94.72%) (44979/47488)\n",
      "Epoch: 134 | Batch_idx: 380 |  Loss_1: (0.1445) | Acc_1: (94.73%) (46199/48768)\n",
      "Epoch: 134 | Batch_idx: 390 |  Loss_1: (0.1445) | Acc_1: (94.74%) (47368/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3280) | Acc: (92.01%) (9201/10000)\n",
      "Epoch: 135 | Batch_idx: 0 |  Loss_1: (0.2211) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 135 | Batch_idx: 10 |  Loss_1: (0.1386) | Acc_1: (94.96%) (1337/1408)\n",
      "Epoch: 135 | Batch_idx: 20 |  Loss_1: (0.1416) | Acc_1: (95.05%) (2555/2688)\n",
      "Epoch: 135 | Batch_idx: 30 |  Loss_1: (0.1407) | Acc_1: (95.09%) (3773/3968)\n",
      "Epoch: 135 | Batch_idx: 40 |  Loss_1: (0.1468) | Acc_1: (94.72%) (4971/5248)\n",
      "Epoch: 135 | Batch_idx: 50 |  Loss_1: (0.1482) | Acc_1: (94.64%) (6178/6528)\n",
      "Epoch: 135 | Batch_idx: 60 |  Loss_1: (0.1442) | Acc_1: (94.71%) (7395/7808)\n",
      "Epoch: 135 | Batch_idx: 70 |  Loss_1: (0.1424) | Acc_1: (94.78%) (8614/9088)\n",
      "Epoch: 135 | Batch_idx: 80 |  Loss_1: (0.1453) | Acc_1: (94.68%) (9816/10368)\n",
      "Epoch: 135 | Batch_idx: 90 |  Loss_1: (0.1448) | Acc_1: (94.70%) (11031/11648)\n",
      "Epoch: 135 | Batch_idx: 100 |  Loss_1: (0.1468) | Acc_1: (94.66%) (12238/12928)\n",
      "Epoch: 135 | Batch_idx: 110 |  Loss_1: (0.1478) | Acc_1: (94.59%) (13440/14208)\n",
      "Epoch: 135 | Batch_idx: 120 |  Loss_1: (0.1477) | Acc_1: (94.65%) (14660/15488)\n",
      "Epoch: 135 | Batch_idx: 130 |  Loss_1: (0.1480) | Acc_1: (94.61%) (15864/16768)\n",
      "Epoch: 135 | Batch_idx: 140 |  Loss_1: (0.1484) | Acc_1: (94.59%) (17072/18048)\n",
      "Epoch: 135 | Batch_idx: 150 |  Loss_1: (0.1492) | Acc_1: (94.57%) (18278/19328)\n",
      "Epoch: 135 | Batch_idx: 160 |  Loss_1: (0.1488) | Acc_1: (94.58%) (19492/20608)\n",
      "Epoch: 135 | Batch_idx: 170 |  Loss_1: (0.1484) | Acc_1: (94.60%) (20705/21888)\n",
      "Epoch: 135 | Batch_idx: 180 |  Loss_1: (0.1492) | Acc_1: (94.57%) (21911/23168)\n",
      "Epoch: 135 | Batch_idx: 190 |  Loss_1: (0.1480) | Acc_1: (94.60%) (23129/24448)\n",
      "Epoch: 135 | Batch_idx: 200 |  Loss_1: (0.1477) | Acc_1: (94.62%) (24345/25728)\n",
      "Epoch: 135 | Batch_idx: 210 |  Loss_1: (0.1476) | Acc_1: (94.62%) (25555/27008)\n",
      "Epoch: 135 | Batch_idx: 220 |  Loss_1: (0.1485) | Acc_1: (94.59%) (26757/28288)\n",
      "Epoch: 135 | Batch_idx: 230 |  Loss_1: (0.1487) | Acc_1: (94.60%) (27970/29568)\n",
      "Epoch: 135 | Batch_idx: 240 |  Loss_1: (0.1493) | Acc_1: (94.58%) (29175/30848)\n",
      "Epoch: 135 | Batch_idx: 250 |  Loss_1: (0.1500) | Acc_1: (94.54%) (30375/32128)\n",
      "Epoch: 135 | Batch_idx: 260 |  Loss_1: (0.1507) | Acc_1: (94.50%) (31572/33408)\n",
      "Epoch: 135 | Batch_idx: 270 |  Loss_1: (0.1511) | Acc_1: (94.47%) (32770/34688)\n",
      "Epoch: 135 | Batch_idx: 280 |  Loss_1: (0.1525) | Acc_1: (94.43%) (33963/35968)\n",
      "Epoch: 135 | Batch_idx: 290 |  Loss_1: (0.1532) | Acc_1: (94.39%) (35158/37248)\n",
      "Epoch: 135 | Batch_idx: 300 |  Loss_1: (0.1526) | Acc_1: (94.41%) (36374/38528)\n",
      "Epoch: 135 | Batch_idx: 310 |  Loss_1: (0.1524) | Acc_1: (94.42%) (37585/39808)\n",
      "Epoch: 135 | Batch_idx: 320 |  Loss_1: (0.1525) | Acc_1: (94.41%) (38791/41088)\n",
      "Epoch: 135 | Batch_idx: 330 |  Loss_1: (0.1532) | Acc_1: (94.39%) (39990/42368)\n",
      "Epoch: 135 | Batch_idx: 340 |  Loss_1: (0.1539) | Acc_1: (94.36%) (41188/43648)\n",
      "Epoch: 135 | Batch_idx: 350 |  Loss_1: (0.1538) | Acc_1: (94.37%) (42397/44928)\n",
      "Epoch: 135 | Batch_idx: 360 |  Loss_1: (0.1542) | Acc_1: (94.36%) (43603/46208)\n",
      "Epoch: 135 | Batch_idx: 370 |  Loss_1: (0.1541) | Acc_1: (94.37%) (44815/47488)\n",
      "Epoch: 135 | Batch_idx: 380 |  Loss_1: (0.1535) | Acc_1: (94.40%) (46035/48768)\n",
      "Epoch: 135 | Batch_idx: 390 |  Loss_1: (0.1538) | Acc_1: (94.39%) (47194/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3497) | Acc: (91.71%) (9171/10000)\n",
      "Epoch: 136 | Batch_idx: 0 |  Loss_1: (0.0669) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 136 | Batch_idx: 10 |  Loss_1: (0.1403) | Acc_1: (94.32%) (1328/1408)\n",
      "Epoch: 136 | Batch_idx: 20 |  Loss_1: (0.1373) | Acc_1: (94.53%) (2541/2688)\n",
      "Epoch: 136 | Batch_idx: 30 |  Loss_1: (0.1377) | Acc_1: (94.63%) (3755/3968)\n",
      "Epoch: 136 | Batch_idx: 40 |  Loss_1: (0.1378) | Acc_1: (94.80%) (4975/5248)\n",
      "Epoch: 136 | Batch_idx: 50 |  Loss_1: (0.1448) | Acc_1: (94.49%) (6168/6528)\n",
      "Epoch: 136 | Batch_idx: 60 |  Loss_1: (0.1448) | Acc_1: (94.53%) (7381/7808)\n",
      "Epoch: 136 | Batch_idx: 70 |  Loss_1: (0.1452) | Acc_1: (94.50%) (8588/9088)\n",
      "Epoch: 136 | Batch_idx: 80 |  Loss_1: (0.1473) | Acc_1: (94.44%) (9792/10368)\n",
      "Epoch: 136 | Batch_idx: 90 |  Loss_1: (0.1449) | Acc_1: (94.56%) (11014/11648)\n",
      "Epoch: 136 | Batch_idx: 100 |  Loss_1: (0.1455) | Acc_1: (94.55%) (12224/12928)\n",
      "Epoch: 136 | Batch_idx: 110 |  Loss_1: (0.1456) | Acc_1: (94.57%) (13436/14208)\n",
      "Epoch: 136 | Batch_idx: 120 |  Loss_1: (0.1475) | Acc_1: (94.50%) (14636/15488)\n",
      "Epoch: 136 | Batch_idx: 130 |  Loss_1: (0.1498) | Acc_1: (94.44%) (15836/16768)\n",
      "Epoch: 136 | Batch_idx: 140 |  Loss_1: (0.1486) | Acc_1: (94.52%) (17059/18048)\n",
      "Epoch: 136 | Batch_idx: 150 |  Loss_1: (0.1478) | Acc_1: (94.59%) (18282/19328)\n",
      "Epoch: 136 | Batch_idx: 160 |  Loss_1: (0.1495) | Acc_1: (94.51%) (19477/20608)\n",
      "Epoch: 136 | Batch_idx: 170 |  Loss_1: (0.1492) | Acc_1: (94.51%) (20686/21888)\n",
      "Epoch: 136 | Batch_idx: 180 |  Loss_1: (0.1487) | Acc_1: (94.52%) (21898/23168)\n",
      "Epoch: 136 | Batch_idx: 190 |  Loss_1: (0.1473) | Acc_1: (94.57%) (23120/24448)\n",
      "Epoch: 136 | Batch_idx: 200 |  Loss_1: (0.1469) | Acc_1: (94.59%) (24336/25728)\n",
      "Epoch: 136 | Batch_idx: 210 |  Loss_1: (0.1477) | Acc_1: (94.56%) (25539/27008)\n",
      "Epoch: 136 | Batch_idx: 220 |  Loss_1: (0.1478) | Acc_1: (94.54%) (26744/28288)\n",
      "Epoch: 136 | Batch_idx: 230 |  Loss_1: (0.1473) | Acc_1: (94.56%) (27959/29568)\n",
      "Epoch: 136 | Batch_idx: 240 |  Loss_1: (0.1476) | Acc_1: (94.56%) (29171/30848)\n",
      "Epoch: 136 | Batch_idx: 250 |  Loss_1: (0.1476) | Acc_1: (94.56%) (30379/32128)\n",
      "Epoch: 136 | Batch_idx: 260 |  Loss_1: (0.1481) | Acc_1: (94.54%) (31583/33408)\n",
      "Epoch: 136 | Batch_idx: 270 |  Loss_1: (0.1479) | Acc_1: (94.55%) (32797/34688)\n",
      "Epoch: 136 | Batch_idx: 280 |  Loss_1: (0.1483) | Acc_1: (94.53%) (34000/35968)\n",
      "Epoch: 136 | Batch_idx: 290 |  Loss_1: (0.1487) | Acc_1: (94.51%) (35202/37248)\n",
      "Epoch: 136 | Batch_idx: 300 |  Loss_1: (0.1485) | Acc_1: (94.52%) (36415/38528)\n",
      "Epoch: 136 | Batch_idx: 310 |  Loss_1: (0.1489) | Acc_1: (94.51%) (37624/39808)\n",
      "Epoch: 136 | Batch_idx: 320 |  Loss_1: (0.1492) | Acc_1: (94.50%) (38827/41088)\n",
      "Epoch: 136 | Batch_idx: 330 |  Loss_1: (0.1494) | Acc_1: (94.50%) (40037/42368)\n",
      "Epoch: 136 | Batch_idx: 340 |  Loss_1: (0.1498) | Acc_1: (94.48%) (41237/43648)\n",
      "Epoch: 136 | Batch_idx: 350 |  Loss_1: (0.1496) | Acc_1: (94.48%) (42449/44928)\n",
      "Epoch: 136 | Batch_idx: 360 |  Loss_1: (0.1498) | Acc_1: (94.47%) (43654/46208)\n",
      "Epoch: 136 | Batch_idx: 370 |  Loss_1: (0.1493) | Acc_1: (94.50%) (44874/47488)\n",
      "Epoch: 136 | Batch_idx: 380 |  Loss_1: (0.1497) | Acc_1: (94.49%) (46083/48768)\n",
      "Epoch: 136 | Batch_idx: 390 |  Loss_1: (0.1493) | Acc_1: (94.51%) (47253/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3128) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 137 | Batch_idx: 0 |  Loss_1: (0.1512) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 137 | Batch_idx: 10 |  Loss_1: (0.1454) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 137 | Batch_idx: 20 |  Loss_1: (0.1411) | Acc_1: (94.68%) (2545/2688)\n",
      "Epoch: 137 | Batch_idx: 30 |  Loss_1: (0.1374) | Acc_1: (94.93%) (3767/3968)\n",
      "Epoch: 137 | Batch_idx: 40 |  Loss_1: (0.1390) | Acc_1: (94.95%) (4983/5248)\n",
      "Epoch: 137 | Batch_idx: 50 |  Loss_1: (0.1400) | Acc_1: (94.79%) (6188/6528)\n",
      "Epoch: 137 | Batch_idx: 60 |  Loss_1: (0.1425) | Acc_1: (94.76%) (7399/7808)\n",
      "Epoch: 137 | Batch_idx: 70 |  Loss_1: (0.1407) | Acc_1: (94.82%) (8617/9088)\n",
      "Epoch: 137 | Batch_idx: 80 |  Loss_1: (0.1411) | Acc_1: (94.75%) (9824/10368)\n",
      "Epoch: 137 | Batch_idx: 90 |  Loss_1: (0.1446) | Acc_1: (94.63%) (11022/11648)\n",
      "Epoch: 137 | Batch_idx: 100 |  Loss_1: (0.1425) | Acc_1: (94.72%) (12245/12928)\n",
      "Epoch: 137 | Batch_idx: 110 |  Loss_1: (0.1421) | Acc_1: (94.71%) (13457/14208)\n",
      "Epoch: 137 | Batch_idx: 120 |  Loss_1: (0.1427) | Acc_1: (94.70%) (14667/15488)\n",
      "Epoch: 137 | Batch_idx: 130 |  Loss_1: (0.1419) | Acc_1: (94.75%) (15887/16768)\n",
      "Epoch: 137 | Batch_idx: 140 |  Loss_1: (0.1423) | Acc_1: (94.73%) (17096/18048)\n",
      "Epoch: 137 | Batch_idx: 150 |  Loss_1: (0.1426) | Acc_1: (94.70%) (18303/19328)\n",
      "Epoch: 137 | Batch_idx: 160 |  Loss_1: (0.1433) | Acc_1: (94.67%) (19509/20608)\n",
      "Epoch: 137 | Batch_idx: 170 |  Loss_1: (0.1439) | Acc_1: (94.63%) (20712/21888)\n",
      "Epoch: 137 | Batch_idx: 180 |  Loss_1: (0.1449) | Acc_1: (94.60%) (21918/23168)\n",
      "Epoch: 137 | Batch_idx: 190 |  Loss_1: (0.1459) | Acc_1: (94.56%) (23119/24448)\n",
      "Epoch: 137 | Batch_idx: 200 |  Loss_1: (0.1481) | Acc_1: (94.45%) (24300/25728)\n",
      "Epoch: 137 | Batch_idx: 210 |  Loss_1: (0.1487) | Acc_1: (94.40%) (25495/27008)\n",
      "Epoch: 137 | Batch_idx: 220 |  Loss_1: (0.1482) | Acc_1: (94.43%) (26711/28288)\n",
      "Epoch: 137 | Batch_idx: 230 |  Loss_1: (0.1465) | Acc_1: (94.49%) (27939/29568)\n",
      "Epoch: 137 | Batch_idx: 240 |  Loss_1: (0.1464) | Acc_1: (94.51%) (29153/30848)\n",
      "Epoch: 137 | Batch_idx: 250 |  Loss_1: (0.1468) | Acc_1: (94.51%) (30365/32128)\n",
      "Epoch: 137 | Batch_idx: 260 |  Loss_1: (0.1469) | Acc_1: (94.50%) (31569/33408)\n",
      "Epoch: 137 | Batch_idx: 270 |  Loss_1: (0.1470) | Acc_1: (94.49%) (32775/34688)\n",
      "Epoch: 137 | Batch_idx: 280 |  Loss_1: (0.1471) | Acc_1: (94.49%) (33985/35968)\n",
      "Epoch: 137 | Batch_idx: 290 |  Loss_1: (0.1465) | Acc_1: (94.50%) (35201/37248)\n",
      "Epoch: 137 | Batch_idx: 300 |  Loss_1: (0.1472) | Acc_1: (94.47%) (36398/38528)\n",
      "Epoch: 137 | Batch_idx: 310 |  Loss_1: (0.1465) | Acc_1: (94.50%) (37620/39808)\n",
      "Epoch: 137 | Batch_idx: 320 |  Loss_1: (0.1470) | Acc_1: (94.47%) (38817/41088)\n",
      "Epoch: 137 | Batch_idx: 330 |  Loss_1: (0.1468) | Acc_1: (94.50%) (40037/42368)\n",
      "Epoch: 137 | Batch_idx: 340 |  Loss_1: (0.1476) | Acc_1: (94.48%) (41239/43648)\n",
      "Epoch: 137 | Batch_idx: 350 |  Loss_1: (0.1481) | Acc_1: (94.45%) (42435/44928)\n",
      "Epoch: 137 | Batch_idx: 360 |  Loss_1: (0.1483) | Acc_1: (94.44%) (43641/46208)\n",
      "Epoch: 137 | Batch_idx: 370 |  Loss_1: (0.1480) | Acc_1: (94.46%) (44857/47488)\n",
      "Epoch: 137 | Batch_idx: 380 |  Loss_1: (0.1482) | Acc_1: (94.47%) (46072/48768)\n",
      "Epoch: 137 | Batch_idx: 390 |  Loss_1: (0.1484) | Acc_1: (94.47%) (47235/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3244) | Acc: (91.91%) (9191/10000)\n",
      "Epoch: 138 | Batch_idx: 0 |  Loss_1: (0.1472) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 138 | Batch_idx: 10 |  Loss_1: (0.1467) | Acc_1: (94.25%) (1327/1408)\n",
      "Epoch: 138 | Batch_idx: 20 |  Loss_1: (0.1432) | Acc_1: (94.68%) (2545/2688)\n",
      "Epoch: 138 | Batch_idx: 30 |  Loss_1: (0.1430) | Acc_1: (94.66%) (3756/3968)\n",
      "Epoch: 138 | Batch_idx: 40 |  Loss_1: (0.1394) | Acc_1: (94.93%) (4982/5248)\n",
      "Epoch: 138 | Batch_idx: 50 |  Loss_1: (0.1369) | Acc_1: (95.05%) (6205/6528)\n",
      "Epoch: 138 | Batch_idx: 60 |  Loss_1: (0.1398) | Acc_1: (94.94%) (7413/7808)\n",
      "Epoch: 138 | Batch_idx: 70 |  Loss_1: (0.1380) | Acc_1: (94.99%) (8633/9088)\n",
      "Epoch: 138 | Batch_idx: 80 |  Loss_1: (0.1379) | Acc_1: (95.02%) (9852/10368)\n",
      "Epoch: 138 | Batch_idx: 90 |  Loss_1: (0.1378) | Acc_1: (95.05%) (11071/11648)\n",
      "Epoch: 138 | Batch_idx: 100 |  Loss_1: (0.1398) | Acc_1: (94.96%) (12276/12928)\n",
      "Epoch: 138 | Batch_idx: 110 |  Loss_1: (0.1401) | Acc_1: (94.93%) (13487/14208)\n",
      "Epoch: 138 | Batch_idx: 120 |  Loss_1: (0.1388) | Acc_1: (95.03%) (14718/15488)\n",
      "Epoch: 138 | Batch_idx: 130 |  Loss_1: (0.1397) | Acc_1: (94.97%) (15924/16768)\n",
      "Epoch: 138 | Batch_idx: 140 |  Loss_1: (0.1406) | Acc_1: (94.91%) (17130/18048)\n",
      "Epoch: 138 | Batch_idx: 150 |  Loss_1: (0.1429) | Acc_1: (94.82%) (18326/19328)\n",
      "Epoch: 138 | Batch_idx: 160 |  Loss_1: (0.1455) | Acc_1: (94.70%) (19515/20608)\n",
      "Epoch: 138 | Batch_idx: 170 |  Loss_1: (0.1451) | Acc_1: (94.68%) (20723/21888)\n",
      "Epoch: 138 | Batch_idx: 180 |  Loss_1: (0.1443) | Acc_1: (94.75%) (21951/23168)\n",
      "Epoch: 138 | Batch_idx: 190 |  Loss_1: (0.1449) | Acc_1: (94.72%) (23157/24448)\n",
      "Epoch: 138 | Batch_idx: 200 |  Loss_1: (0.1456) | Acc_1: (94.69%) (24363/25728)\n",
      "Epoch: 138 | Batch_idx: 210 |  Loss_1: (0.1458) | Acc_1: (94.69%) (25575/27008)\n",
      "Epoch: 138 | Batch_idx: 220 |  Loss_1: (0.1466) | Acc_1: (94.69%) (26785/28288)\n",
      "Epoch: 138 | Batch_idx: 230 |  Loss_1: (0.1466) | Acc_1: (94.69%) (27999/29568)\n",
      "Epoch: 138 | Batch_idx: 240 |  Loss_1: (0.1466) | Acc_1: (94.69%) (29210/30848)\n",
      "Epoch: 138 | Batch_idx: 250 |  Loss_1: (0.1469) | Acc_1: (94.70%) (30425/32128)\n",
      "Epoch: 138 | Batch_idx: 260 |  Loss_1: (0.1470) | Acc_1: (94.69%) (31635/33408)\n",
      "Epoch: 138 | Batch_idx: 270 |  Loss_1: (0.1466) | Acc_1: (94.70%) (32849/34688)\n",
      "Epoch: 138 | Batch_idx: 280 |  Loss_1: (0.1467) | Acc_1: (94.69%) (34059/35968)\n",
      "Epoch: 138 | Batch_idx: 290 |  Loss_1: (0.1461) | Acc_1: (94.71%) (35276/37248)\n",
      "Epoch: 138 | Batch_idx: 300 |  Loss_1: (0.1468) | Acc_1: (94.70%) (36485/38528)\n",
      "Epoch: 138 | Batch_idx: 310 |  Loss_1: (0.1468) | Acc_1: (94.71%) (37703/39808)\n",
      "Epoch: 138 | Batch_idx: 320 |  Loss_1: (0.1476) | Acc_1: (94.68%) (38904/41088)\n",
      "Epoch: 138 | Batch_idx: 330 |  Loss_1: (0.1486) | Acc_1: (94.65%) (40100/42368)\n",
      "Epoch: 138 | Batch_idx: 340 |  Loss_1: (0.1495) | Acc_1: (94.60%) (41292/43648)\n",
      "Epoch: 138 | Batch_idx: 350 |  Loss_1: (0.1487) | Acc_1: (94.62%) (42513/44928)\n",
      "Epoch: 138 | Batch_idx: 360 |  Loss_1: (0.1499) | Acc_1: (94.59%) (43706/46208)\n",
      "Epoch: 138 | Batch_idx: 370 |  Loss_1: (0.1495) | Acc_1: (94.60%) (44923/47488)\n",
      "Epoch: 138 | Batch_idx: 380 |  Loss_1: (0.1491) | Acc_1: (94.61%) (46138/48768)\n",
      "Epoch: 138 | Batch_idx: 390 |  Loss_1: (0.1484) | Acc_1: (94.64%) (47318/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3202) | Acc: (91.94%) (9194/10000)\n",
      "Epoch: 139 | Batch_idx: 0 |  Loss_1: (0.1334) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 139 | Batch_idx: 10 |  Loss_1: (0.1804) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 139 | Batch_idx: 20 |  Loss_1: (0.1692) | Acc_1: (93.90%) (2524/2688)\n",
      "Epoch: 139 | Batch_idx: 30 |  Loss_1: (0.1596) | Acc_1: (94.23%) (3739/3968)\n",
      "Epoch: 139 | Batch_idx: 40 |  Loss_1: (0.1615) | Acc_1: (94.11%) (4939/5248)\n",
      "Epoch: 139 | Batch_idx: 50 |  Loss_1: (0.1618) | Acc_1: (94.10%) (6143/6528)\n",
      "Epoch: 139 | Batch_idx: 60 |  Loss_1: (0.1620) | Acc_1: (94.04%) (7343/7808)\n",
      "Epoch: 139 | Batch_idx: 70 |  Loss_1: (0.1601) | Acc_1: (94.10%) (8552/9088)\n",
      "Epoch: 139 | Batch_idx: 80 |  Loss_1: (0.1589) | Acc_1: (94.15%) (9761/10368)\n",
      "Epoch: 139 | Batch_idx: 90 |  Loss_1: (0.1575) | Acc_1: (94.21%) (10973/11648)\n",
      "Epoch: 139 | Batch_idx: 100 |  Loss_1: (0.1568) | Acc_1: (94.25%) (12184/12928)\n",
      "Epoch: 139 | Batch_idx: 110 |  Loss_1: (0.1557) | Acc_1: (94.33%) (13403/14208)\n",
      "Epoch: 139 | Batch_idx: 120 |  Loss_1: (0.1561) | Acc_1: (94.32%) (14609/15488)\n",
      "Epoch: 139 | Batch_idx: 130 |  Loss_1: (0.1568) | Acc_1: (94.32%) (15816/16768)\n",
      "Epoch: 139 | Batch_idx: 140 |  Loss_1: (0.1567) | Acc_1: (94.34%) (17026/18048)\n",
      "Epoch: 139 | Batch_idx: 150 |  Loss_1: (0.1573) | Acc_1: (94.29%) (18225/19328)\n",
      "Epoch: 139 | Batch_idx: 160 |  Loss_1: (0.1566) | Acc_1: (94.31%) (19436/20608)\n",
      "Epoch: 139 | Batch_idx: 170 |  Loss_1: (0.1565) | Acc_1: (94.33%) (20647/21888)\n",
      "Epoch: 139 | Batch_idx: 180 |  Loss_1: (0.1567) | Acc_1: (94.31%) (21850/23168)\n",
      "Epoch: 139 | Batch_idx: 190 |  Loss_1: (0.1567) | Acc_1: (94.32%) (23060/24448)\n",
      "Epoch: 139 | Batch_idx: 200 |  Loss_1: (0.1568) | Acc_1: (94.31%) (24265/25728)\n",
      "Epoch: 139 | Batch_idx: 210 |  Loss_1: (0.1571) | Acc_1: (94.29%) (25465/27008)\n",
      "Epoch: 139 | Batch_idx: 220 |  Loss_1: (0.1567) | Acc_1: (94.31%) (26678/28288)\n",
      "Epoch: 139 | Batch_idx: 230 |  Loss_1: (0.1559) | Acc_1: (94.32%) (27890/29568)\n",
      "Epoch: 139 | Batch_idx: 240 |  Loss_1: (0.1566) | Acc_1: (94.30%) (29091/30848)\n",
      "Epoch: 139 | Batch_idx: 250 |  Loss_1: (0.1560) | Acc_1: (94.33%) (30305/32128)\n",
      "Epoch: 139 | Batch_idx: 260 |  Loss_1: (0.1563) | Acc_1: (94.31%) (31508/33408)\n",
      "Epoch: 139 | Batch_idx: 270 |  Loss_1: (0.1559) | Acc_1: (94.34%) (32725/34688)\n",
      "Epoch: 139 | Batch_idx: 280 |  Loss_1: (0.1560) | Acc_1: (94.34%) (33931/35968)\n",
      "Epoch: 139 | Batch_idx: 290 |  Loss_1: (0.1555) | Acc_1: (94.35%) (35144/37248)\n",
      "Epoch: 139 | Batch_idx: 300 |  Loss_1: (0.1544) | Acc_1: (94.40%) (36370/38528)\n",
      "Epoch: 139 | Batch_idx: 310 |  Loss_1: (0.1545) | Acc_1: (94.39%) (37575/39808)\n",
      "Epoch: 139 | Batch_idx: 320 |  Loss_1: (0.1544) | Acc_1: (94.37%) (38775/41088)\n",
      "Epoch: 139 | Batch_idx: 330 |  Loss_1: (0.1538) | Acc_1: (94.39%) (39990/42368)\n",
      "Epoch: 139 | Batch_idx: 340 |  Loss_1: (0.1539) | Acc_1: (94.38%) (41196/43648)\n",
      "Epoch: 139 | Batch_idx: 350 |  Loss_1: (0.1540) | Acc_1: (94.37%) (42400/44928)\n",
      "Epoch: 139 | Batch_idx: 360 |  Loss_1: (0.1543) | Acc_1: (94.35%) (43598/46208)\n",
      "Epoch: 139 | Batch_idx: 370 |  Loss_1: (0.1538) | Acc_1: (94.37%) (44814/47488)\n",
      "Epoch: 139 | Batch_idx: 380 |  Loss_1: (0.1535) | Acc_1: (94.39%) (46030/48768)\n",
      "Epoch: 139 | Batch_idx: 390 |  Loss_1: (0.1542) | Acc_1: (94.36%) (47181/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3257) | Acc: (92.19%) (9219/10000)\n",
      "Epoch: 140 | Batch_idx: 0 |  Loss_1: (0.1936) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 140 | Batch_idx: 10 |  Loss_1: (0.1435) | Acc_1: (94.25%) (1327/1408)\n",
      "Epoch: 140 | Batch_idx: 20 |  Loss_1: (0.1396) | Acc_1: (94.83%) (2549/2688)\n",
      "Epoch: 140 | Batch_idx: 30 |  Loss_1: (0.1363) | Acc_1: (94.86%) (3764/3968)\n",
      "Epoch: 140 | Batch_idx: 40 |  Loss_1: (0.1384) | Acc_1: (94.82%) (4976/5248)\n",
      "Epoch: 140 | Batch_idx: 50 |  Loss_1: (0.1379) | Acc_1: (94.85%) (6192/6528)\n",
      "Epoch: 140 | Batch_idx: 60 |  Loss_1: (0.1400) | Acc_1: (94.81%) (7403/7808)\n",
      "Epoch: 140 | Batch_idx: 70 |  Loss_1: (0.1397) | Acc_1: (94.89%) (8624/9088)\n",
      "Epoch: 140 | Batch_idx: 80 |  Loss_1: (0.1396) | Acc_1: (94.92%) (9841/10368)\n",
      "Epoch: 140 | Batch_idx: 90 |  Loss_1: (0.1412) | Acc_1: (94.79%) (11041/11648)\n",
      "Epoch: 140 | Batch_idx: 100 |  Loss_1: (0.1406) | Acc_1: (94.78%) (12253/12928)\n",
      "Epoch: 140 | Batch_idx: 110 |  Loss_1: (0.1451) | Acc_1: (94.62%) (13443/14208)\n",
      "Epoch: 140 | Batch_idx: 120 |  Loss_1: (0.1437) | Acc_1: (94.69%) (14665/15488)\n",
      "Epoch: 140 | Batch_idx: 130 |  Loss_1: (0.1436) | Acc_1: (94.69%) (15877/16768)\n",
      "Epoch: 140 | Batch_idx: 140 |  Loss_1: (0.1444) | Acc_1: (94.62%) (17077/18048)\n",
      "Epoch: 140 | Batch_idx: 150 |  Loss_1: (0.1459) | Acc_1: (94.55%) (18275/19328)\n",
      "Epoch: 140 | Batch_idx: 160 |  Loss_1: (0.1464) | Acc_1: (94.54%) (19483/20608)\n",
      "Epoch: 140 | Batch_idx: 170 |  Loss_1: (0.1466) | Acc_1: (94.55%) (20695/21888)\n",
      "Epoch: 140 | Batch_idx: 180 |  Loss_1: (0.1478) | Acc_1: (94.52%) (21898/23168)\n",
      "Epoch: 140 | Batch_idx: 190 |  Loss_1: (0.1490) | Acc_1: (94.47%) (23095/24448)\n",
      "Epoch: 140 | Batch_idx: 200 |  Loss_1: (0.1500) | Acc_1: (94.43%) (24296/25728)\n",
      "Epoch: 140 | Batch_idx: 210 |  Loss_1: (0.1489) | Acc_1: (94.47%) (25514/27008)\n",
      "Epoch: 140 | Batch_idx: 220 |  Loss_1: (0.1495) | Acc_1: (94.44%) (26715/28288)\n",
      "Epoch: 140 | Batch_idx: 230 |  Loss_1: (0.1499) | Acc_1: (94.43%) (27920/29568)\n",
      "Epoch: 140 | Batch_idx: 240 |  Loss_1: (0.1493) | Acc_1: (94.44%) (29132/30848)\n",
      "Epoch: 140 | Batch_idx: 250 |  Loss_1: (0.1495) | Acc_1: (94.44%) (30343/32128)\n",
      "Epoch: 140 | Batch_idx: 260 |  Loss_1: (0.1491) | Acc_1: (94.46%) (31556/33408)\n",
      "Epoch: 140 | Batch_idx: 270 |  Loss_1: (0.1491) | Acc_1: (94.46%) (32767/34688)\n",
      "Epoch: 140 | Batch_idx: 280 |  Loss_1: (0.1489) | Acc_1: (94.47%) (33978/35968)\n",
      "Epoch: 140 | Batch_idx: 290 |  Loss_1: (0.1482) | Acc_1: (94.49%) (35195/37248)\n",
      "Epoch: 140 | Batch_idx: 300 |  Loss_1: (0.1479) | Acc_1: (94.50%) (36409/38528)\n",
      "Epoch: 140 | Batch_idx: 310 |  Loss_1: (0.1475) | Acc_1: (94.51%) (37624/39808)\n",
      "Epoch: 140 | Batch_idx: 320 |  Loss_1: (0.1471) | Acc_1: (94.53%) (38841/41088)\n",
      "Epoch: 140 | Batch_idx: 330 |  Loss_1: (0.1476) | Acc_1: (94.51%) (40042/42368)\n",
      "Epoch: 140 | Batch_idx: 340 |  Loss_1: (0.1481) | Acc_1: (94.48%) (41240/43648)\n",
      "Epoch: 140 | Batch_idx: 350 |  Loss_1: (0.1484) | Acc_1: (94.46%) (42440/44928)\n",
      "Epoch: 140 | Batch_idx: 360 |  Loss_1: (0.1489) | Acc_1: (94.44%) (43638/46208)\n",
      "Epoch: 140 | Batch_idx: 370 |  Loss_1: (0.1491) | Acc_1: (94.43%) (44843/47488)\n",
      "Epoch: 140 | Batch_idx: 380 |  Loss_1: (0.1492) | Acc_1: (94.44%) (46055/48768)\n",
      "Epoch: 140 | Batch_idx: 390 |  Loss_1: (0.1505) | Acc_1: (94.40%) (47202/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3392) | Acc: (91.57%) (9157/10000)\n",
      "Epoch: 141 | Batch_idx: 0 |  Loss_1: (0.1508) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 141 | Batch_idx: 10 |  Loss_1: (0.1679) | Acc_1: (93.47%) (1316/1408)\n",
      "Epoch: 141 | Batch_idx: 20 |  Loss_1: (0.1598) | Acc_1: (93.97%) (2526/2688)\n",
      "Epoch: 141 | Batch_idx: 30 |  Loss_1: (0.1410) | Acc_1: (94.73%) (3759/3968)\n",
      "Epoch: 141 | Batch_idx: 40 |  Loss_1: (0.1397) | Acc_1: (94.66%) (4968/5248)\n",
      "Epoch: 141 | Batch_idx: 50 |  Loss_1: (0.1385) | Acc_1: (94.79%) (6188/6528)\n",
      "Epoch: 141 | Batch_idx: 60 |  Loss_1: (0.1373) | Acc_1: (94.86%) (7407/7808)\n",
      "Epoch: 141 | Batch_idx: 70 |  Loss_1: (0.1373) | Acc_1: (94.86%) (8621/9088)\n",
      "Epoch: 141 | Batch_idx: 80 |  Loss_1: (0.1407) | Acc_1: (94.79%) (9828/10368)\n",
      "Epoch: 141 | Batch_idx: 90 |  Loss_1: (0.1400) | Acc_1: (94.80%) (11042/11648)\n",
      "Epoch: 141 | Batch_idx: 100 |  Loss_1: (0.1428) | Acc_1: (94.75%) (12249/12928)\n",
      "Epoch: 141 | Batch_idx: 110 |  Loss_1: (0.1433) | Acc_1: (94.78%) (13467/14208)\n",
      "Epoch: 141 | Batch_idx: 120 |  Loss_1: (0.1458) | Acc_1: (94.71%) (14669/15488)\n",
      "Epoch: 141 | Batch_idx: 130 |  Loss_1: (0.1457) | Acc_1: (94.70%) (15879/16768)\n",
      "Epoch: 141 | Batch_idx: 140 |  Loss_1: (0.1460) | Acc_1: (94.70%) (17092/18048)\n",
      "Epoch: 141 | Batch_idx: 150 |  Loss_1: (0.1467) | Acc_1: (94.67%) (18298/19328)\n",
      "Epoch: 141 | Batch_idx: 160 |  Loss_1: (0.1471) | Acc_1: (94.64%) (19504/20608)\n",
      "Epoch: 141 | Batch_idx: 170 |  Loss_1: (0.1449) | Acc_1: (94.72%) (20732/21888)\n",
      "Epoch: 141 | Batch_idx: 180 |  Loss_1: (0.1451) | Acc_1: (94.72%) (21945/23168)\n",
      "Epoch: 141 | Batch_idx: 190 |  Loss_1: (0.1444) | Acc_1: (94.75%) (23164/24448)\n",
      "Epoch: 141 | Batch_idx: 200 |  Loss_1: (0.1449) | Acc_1: (94.72%) (24369/25728)\n",
      "Epoch: 141 | Batch_idx: 210 |  Loss_1: (0.1440) | Acc_1: (94.74%) (25588/27008)\n",
      "Epoch: 141 | Batch_idx: 220 |  Loss_1: (0.1437) | Acc_1: (94.74%) (26799/28288)\n",
      "Epoch: 141 | Batch_idx: 230 |  Loss_1: (0.1441) | Acc_1: (94.72%) (28008/29568)\n",
      "Epoch: 141 | Batch_idx: 240 |  Loss_1: (0.1443) | Acc_1: (94.71%) (29217/30848)\n",
      "Epoch: 141 | Batch_idx: 250 |  Loss_1: (0.1458) | Acc_1: (94.64%) (30407/32128)\n",
      "Epoch: 141 | Batch_idx: 260 |  Loss_1: (0.1455) | Acc_1: (94.66%) (31624/33408)\n",
      "Epoch: 141 | Batch_idx: 270 |  Loss_1: (0.1455) | Acc_1: (94.66%) (32836/34688)\n",
      "Epoch: 141 | Batch_idx: 280 |  Loss_1: (0.1450) | Acc_1: (94.66%) (34049/35968)\n",
      "Epoch: 141 | Batch_idx: 290 |  Loss_1: (0.1459) | Acc_1: (94.63%) (35249/37248)\n",
      "Epoch: 141 | Batch_idx: 300 |  Loss_1: (0.1465) | Acc_1: (94.62%) (36454/38528)\n",
      "Epoch: 141 | Batch_idx: 310 |  Loss_1: (0.1468) | Acc_1: (94.60%) (37658/39808)\n",
      "Epoch: 141 | Batch_idx: 320 |  Loss_1: (0.1466) | Acc_1: (94.59%) (38865/41088)\n",
      "Epoch: 141 | Batch_idx: 330 |  Loss_1: (0.1460) | Acc_1: (94.59%) (40076/42368)\n",
      "Epoch: 141 | Batch_idx: 340 |  Loss_1: (0.1468) | Acc_1: (94.57%) (41279/43648)\n",
      "Epoch: 141 | Batch_idx: 350 |  Loss_1: (0.1468) | Acc_1: (94.56%) (42486/44928)\n",
      "Epoch: 141 | Batch_idx: 360 |  Loss_1: (0.1469) | Acc_1: (94.56%) (43695/46208)\n",
      "Epoch: 141 | Batch_idx: 370 |  Loss_1: (0.1474) | Acc_1: (94.54%) (44896/47488)\n",
      "Epoch: 141 | Batch_idx: 380 |  Loss_1: (0.1479) | Acc_1: (94.52%) (46097/48768)\n",
      "Epoch: 141 | Batch_idx: 390 |  Loss_1: (0.1477) | Acc_1: (94.53%) (47264/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3184) | Acc: (92.35%) (9235/10000)\n",
      "Epoch: 142 | Batch_idx: 0 |  Loss_1: (0.1201) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 142 | Batch_idx: 10 |  Loss_1: (0.1362) | Acc_1: (95.17%) (1340/1408)\n",
      "Epoch: 142 | Batch_idx: 20 |  Loss_1: (0.1409) | Acc_1: (94.94%) (2552/2688)\n",
      "Epoch: 142 | Batch_idx: 30 |  Loss_1: (0.1405) | Acc_1: (95.09%) (3773/3968)\n",
      "Epoch: 142 | Batch_idx: 40 |  Loss_1: (0.1354) | Acc_1: (95.26%) (4999/5248)\n",
      "Epoch: 142 | Batch_idx: 50 |  Loss_1: (0.1353) | Acc_1: (95.14%) (6211/6528)\n",
      "Epoch: 142 | Batch_idx: 60 |  Loss_1: (0.1363) | Acc_1: (95.06%) (7422/7808)\n",
      "Epoch: 142 | Batch_idx: 70 |  Loss_1: (0.1385) | Acc_1: (94.97%) (8631/9088)\n",
      "Epoch: 142 | Batch_idx: 80 |  Loss_1: (0.1388) | Acc_1: (94.99%) (9849/10368)\n",
      "Epoch: 142 | Batch_idx: 90 |  Loss_1: (0.1415) | Acc_1: (94.86%) (11049/11648)\n",
      "Epoch: 142 | Batch_idx: 100 |  Loss_1: (0.1393) | Acc_1: (94.95%) (12275/12928)\n",
      "Epoch: 142 | Batch_idx: 110 |  Loss_1: (0.1385) | Acc_1: (94.97%) (13494/14208)\n",
      "Epoch: 142 | Batch_idx: 120 |  Loss_1: (0.1392) | Acc_1: (94.94%) (14704/15488)\n",
      "Epoch: 142 | Batch_idx: 130 |  Loss_1: (0.1394) | Acc_1: (94.90%) (15912/16768)\n",
      "Epoch: 142 | Batch_idx: 140 |  Loss_1: (0.1415) | Acc_1: (94.85%) (17118/18048)\n",
      "Epoch: 142 | Batch_idx: 150 |  Loss_1: (0.1407) | Acc_1: (94.89%) (18340/19328)\n",
      "Epoch: 142 | Batch_idx: 160 |  Loss_1: (0.1393) | Acc_1: (94.92%) (19561/20608)\n",
      "Epoch: 142 | Batch_idx: 170 |  Loss_1: (0.1407) | Acc_1: (94.85%) (20760/21888)\n",
      "Epoch: 142 | Batch_idx: 180 |  Loss_1: (0.1409) | Acc_1: (94.84%) (21972/23168)\n",
      "Epoch: 142 | Batch_idx: 190 |  Loss_1: (0.1413) | Acc_1: (94.84%) (23186/24448)\n",
      "Epoch: 142 | Batch_idx: 200 |  Loss_1: (0.1411) | Acc_1: (94.83%) (24397/25728)\n",
      "Epoch: 142 | Batch_idx: 210 |  Loss_1: (0.1410) | Acc_1: (94.82%) (25608/27008)\n",
      "Epoch: 142 | Batch_idx: 220 |  Loss_1: (0.1407) | Acc_1: (94.83%) (26826/28288)\n",
      "Epoch: 142 | Batch_idx: 230 |  Loss_1: (0.1398) | Acc_1: (94.88%) (28053/29568)\n",
      "Epoch: 142 | Batch_idx: 240 |  Loss_1: (0.1402) | Acc_1: (94.84%) (29257/30848)\n",
      "Epoch: 142 | Batch_idx: 250 |  Loss_1: (0.1410) | Acc_1: (94.82%) (30465/32128)\n",
      "Epoch: 142 | Batch_idx: 260 |  Loss_1: (0.1414) | Acc_1: (94.81%) (31674/33408)\n",
      "Epoch: 142 | Batch_idx: 270 |  Loss_1: (0.1413) | Acc_1: (94.81%) (32886/34688)\n",
      "Epoch: 142 | Batch_idx: 280 |  Loss_1: (0.1426) | Acc_1: (94.75%) (34081/35968)\n",
      "Epoch: 142 | Batch_idx: 290 |  Loss_1: (0.1433) | Acc_1: (94.73%) (35286/37248)\n",
      "Epoch: 142 | Batch_idx: 300 |  Loss_1: (0.1444) | Acc_1: (94.69%) (36484/38528)\n",
      "Epoch: 142 | Batch_idx: 310 |  Loss_1: (0.1446) | Acc_1: (94.68%) (37692/39808)\n",
      "Epoch: 142 | Batch_idx: 320 |  Loss_1: (0.1451) | Acc_1: (94.66%) (38895/41088)\n",
      "Epoch: 142 | Batch_idx: 330 |  Loss_1: (0.1457) | Acc_1: (94.62%) (40088/42368)\n",
      "Epoch: 142 | Batch_idx: 340 |  Loss_1: (0.1463) | Acc_1: (94.59%) (41287/43648)\n",
      "Epoch: 142 | Batch_idx: 350 |  Loss_1: (0.1471) | Acc_1: (94.57%) (42489/44928)\n",
      "Epoch: 142 | Batch_idx: 360 |  Loss_1: (0.1465) | Acc_1: (94.59%) (43706/46208)\n",
      "Epoch: 142 | Batch_idx: 370 |  Loss_1: (0.1465) | Acc_1: (94.59%) (44917/47488)\n",
      "Epoch: 142 | Batch_idx: 380 |  Loss_1: (0.1460) | Acc_1: (94.60%) (46133/48768)\n",
      "Epoch: 142 | Batch_idx: 390 |  Loss_1: (0.1469) | Acc_1: (94.57%) (47283/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3183) | Acc: (92.21%) (9221/10000)\n",
      "Epoch: 143 | Batch_idx: 0 |  Loss_1: (0.1735) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 143 | Batch_idx: 10 |  Loss_1: (0.1352) | Acc_1: (94.89%) (1336/1408)\n",
      "Epoch: 143 | Batch_idx: 20 |  Loss_1: (0.1301) | Acc_1: (95.01%) (2554/2688)\n",
      "Epoch: 143 | Batch_idx: 30 |  Loss_1: (0.1407) | Acc_1: (94.78%) (3761/3968)\n",
      "Epoch: 143 | Batch_idx: 40 |  Loss_1: (0.1412) | Acc_1: (94.82%) (4976/5248)\n",
      "Epoch: 143 | Batch_idx: 50 |  Loss_1: (0.1471) | Acc_1: (94.49%) (6168/6528)\n",
      "Epoch: 143 | Batch_idx: 60 |  Loss_1: (0.1417) | Acc_1: (94.70%) (7394/7808)\n",
      "Epoch: 143 | Batch_idx: 70 |  Loss_1: (0.1409) | Acc_1: (94.77%) (8613/9088)\n",
      "Epoch: 143 | Batch_idx: 80 |  Loss_1: (0.1412) | Acc_1: (94.75%) (9824/10368)\n",
      "Epoch: 143 | Batch_idx: 90 |  Loss_1: (0.1408) | Acc_1: (94.77%) (11039/11648)\n",
      "Epoch: 143 | Batch_idx: 100 |  Loss_1: (0.1402) | Acc_1: (94.76%) (12250/12928)\n",
      "Epoch: 143 | Batch_idx: 110 |  Loss_1: (0.1393) | Acc_1: (94.78%) (13467/14208)\n",
      "Epoch: 143 | Batch_idx: 120 |  Loss_1: (0.1376) | Acc_1: (94.89%) (14697/15488)\n",
      "Epoch: 143 | Batch_idx: 130 |  Loss_1: (0.1401) | Acc_1: (94.84%) (15902/16768)\n",
      "Epoch: 143 | Batch_idx: 140 |  Loss_1: (0.1421) | Acc_1: (94.76%) (17103/18048)\n",
      "Epoch: 143 | Batch_idx: 150 |  Loss_1: (0.1409) | Acc_1: (94.84%) (18330/19328)\n",
      "Epoch: 143 | Batch_idx: 160 |  Loss_1: (0.1409) | Acc_1: (94.82%) (19540/20608)\n",
      "Epoch: 143 | Batch_idx: 170 |  Loss_1: (0.1401) | Acc_1: (94.84%) (20759/21888)\n",
      "Epoch: 143 | Batch_idx: 180 |  Loss_1: (0.1405) | Acc_1: (94.82%) (21968/23168)\n",
      "Epoch: 143 | Batch_idx: 190 |  Loss_1: (0.1413) | Acc_1: (94.78%) (23173/24448)\n",
      "Epoch: 143 | Batch_idx: 200 |  Loss_1: (0.1429) | Acc_1: (94.75%) (24378/25728)\n",
      "Epoch: 143 | Batch_idx: 210 |  Loss_1: (0.1433) | Acc_1: (94.74%) (25587/27008)\n",
      "Epoch: 143 | Batch_idx: 220 |  Loss_1: (0.1428) | Acc_1: (94.76%) (26805/28288)\n",
      "Epoch: 143 | Batch_idx: 230 |  Loss_1: (0.1422) | Acc_1: (94.76%) (28019/29568)\n",
      "Epoch: 143 | Batch_idx: 240 |  Loss_1: (0.1422) | Acc_1: (94.75%) (29227/30848)\n",
      "Epoch: 143 | Batch_idx: 250 |  Loss_1: (0.1416) | Acc_1: (94.76%) (30446/32128)\n",
      "Epoch: 143 | Batch_idx: 260 |  Loss_1: (0.1405) | Acc_1: (94.81%) (31674/33408)\n",
      "Epoch: 143 | Batch_idx: 270 |  Loss_1: (0.1408) | Acc_1: (94.80%) (32885/34688)\n",
      "Epoch: 143 | Batch_idx: 280 |  Loss_1: (0.1408) | Acc_1: (94.80%) (34096/35968)\n",
      "Epoch: 143 | Batch_idx: 290 |  Loss_1: (0.1413) | Acc_1: (94.77%) (35300/37248)\n",
      "Epoch: 143 | Batch_idx: 300 |  Loss_1: (0.1415) | Acc_1: (94.76%) (36508/38528)\n",
      "Epoch: 143 | Batch_idx: 310 |  Loss_1: (0.1420) | Acc_1: (94.75%) (37717/39808)\n",
      "Epoch: 143 | Batch_idx: 320 |  Loss_1: (0.1423) | Acc_1: (94.72%) (38919/41088)\n",
      "Epoch: 143 | Batch_idx: 330 |  Loss_1: (0.1413) | Acc_1: (94.76%) (40148/42368)\n",
      "Epoch: 143 | Batch_idx: 340 |  Loss_1: (0.1418) | Acc_1: (94.74%) (41353/43648)\n",
      "Epoch: 143 | Batch_idx: 350 |  Loss_1: (0.1417) | Acc_1: (94.75%) (42569/44928)\n",
      "Epoch: 143 | Batch_idx: 360 |  Loss_1: (0.1429) | Acc_1: (94.71%) (43763/46208)\n",
      "Epoch: 143 | Batch_idx: 370 |  Loss_1: (0.1434) | Acc_1: (94.68%) (44962/47488)\n",
      "Epoch: 143 | Batch_idx: 380 |  Loss_1: (0.1435) | Acc_1: (94.69%) (46177/48768)\n",
      "Epoch: 143 | Batch_idx: 390 |  Loss_1: (0.1446) | Acc_1: (94.66%) (47331/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3132) | Acc: (92.13%) (9213/10000)\n",
      "Epoch: 144 | Batch_idx: 0 |  Loss_1: (0.3074) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 144 | Batch_idx: 10 |  Loss_1: (0.1568) | Acc_1: (94.18%) (1326/1408)\n",
      "Epoch: 144 | Batch_idx: 20 |  Loss_1: (0.1584) | Acc_1: (94.12%) (2530/2688)\n",
      "Epoch: 144 | Batch_idx: 30 |  Loss_1: (0.1592) | Acc_1: (94.00%) (3730/3968)\n",
      "Epoch: 144 | Batch_idx: 40 |  Loss_1: (0.1550) | Acc_1: (94.15%) (4941/5248)\n",
      "Epoch: 144 | Batch_idx: 50 |  Loss_1: (0.1543) | Acc_1: (94.18%) (6148/6528)\n",
      "Epoch: 144 | Batch_idx: 60 |  Loss_1: (0.1546) | Acc_1: (94.24%) (7358/7808)\n",
      "Epoch: 144 | Batch_idx: 70 |  Loss_1: (0.1509) | Acc_1: (94.37%) (8576/9088)\n",
      "Epoch: 144 | Batch_idx: 80 |  Loss_1: (0.1509) | Acc_1: (94.40%) (9787/10368)\n",
      "Epoch: 144 | Batch_idx: 90 |  Loss_1: (0.1509) | Acc_1: (94.39%) (10994/11648)\n",
      "Epoch: 144 | Batch_idx: 100 |  Loss_1: (0.1488) | Acc_1: (94.47%) (12213/12928)\n",
      "Epoch: 144 | Batch_idx: 110 |  Loss_1: (0.1471) | Acc_1: (94.51%) (13428/14208)\n",
      "Epoch: 144 | Batch_idx: 120 |  Loss_1: (0.1490) | Acc_1: (94.43%) (14626/15488)\n",
      "Epoch: 144 | Batch_idx: 130 |  Loss_1: (0.1482) | Acc_1: (94.47%) (15841/16768)\n",
      "Epoch: 144 | Batch_idx: 140 |  Loss_1: (0.1494) | Acc_1: (94.41%) (17040/18048)\n",
      "Epoch: 144 | Batch_idx: 150 |  Loss_1: (0.1493) | Acc_1: (94.45%) (18255/19328)\n",
      "Epoch: 144 | Batch_idx: 160 |  Loss_1: (0.1489) | Acc_1: (94.46%) (19467/20608)\n",
      "Epoch: 144 | Batch_idx: 170 |  Loss_1: (0.1493) | Acc_1: (94.44%) (20671/21888)\n",
      "Epoch: 144 | Batch_idx: 180 |  Loss_1: (0.1494) | Acc_1: (94.45%) (21882/23168)\n",
      "Epoch: 144 | Batch_idx: 190 |  Loss_1: (0.1479) | Acc_1: (94.53%) (23111/24448)\n",
      "Epoch: 144 | Batch_idx: 200 |  Loss_1: (0.1472) | Acc_1: (94.56%) (24328/25728)\n",
      "Epoch: 144 | Batch_idx: 210 |  Loss_1: (0.1472) | Acc_1: (94.56%) (25539/27008)\n",
      "Epoch: 144 | Batch_idx: 220 |  Loss_1: (0.1480) | Acc_1: (94.55%) (26747/28288)\n",
      "Epoch: 144 | Batch_idx: 230 |  Loss_1: (0.1482) | Acc_1: (94.53%) (27950/29568)\n",
      "Epoch: 144 | Batch_idx: 240 |  Loss_1: (0.1484) | Acc_1: (94.53%) (29162/30848)\n",
      "Epoch: 144 | Batch_idx: 250 |  Loss_1: (0.1482) | Acc_1: (94.55%) (30378/32128)\n",
      "Epoch: 144 | Batch_idx: 260 |  Loss_1: (0.1480) | Acc_1: (94.55%) (31588/33408)\n",
      "Epoch: 144 | Batch_idx: 270 |  Loss_1: (0.1483) | Acc_1: (94.55%) (32799/34688)\n",
      "Epoch: 144 | Batch_idx: 280 |  Loss_1: (0.1496) | Acc_1: (94.51%) (33992/35968)\n",
      "Epoch: 144 | Batch_idx: 290 |  Loss_1: (0.1499) | Acc_1: (94.49%) (35194/37248)\n",
      "Epoch: 144 | Batch_idx: 300 |  Loss_1: (0.1493) | Acc_1: (94.50%) (36410/38528)\n",
      "Epoch: 144 | Batch_idx: 310 |  Loss_1: (0.1496) | Acc_1: (94.51%) (37621/39808)\n",
      "Epoch: 144 | Batch_idx: 320 |  Loss_1: (0.1486) | Acc_1: (94.54%) (38846/41088)\n",
      "Epoch: 144 | Batch_idx: 330 |  Loss_1: (0.1485) | Acc_1: (94.54%) (40055/42368)\n",
      "Epoch: 144 | Batch_idx: 340 |  Loss_1: (0.1486) | Acc_1: (94.55%) (41269/43648)\n",
      "Epoch: 144 | Batch_idx: 350 |  Loss_1: (0.1493) | Acc_1: (94.52%) (42466/44928)\n",
      "Epoch: 144 | Batch_idx: 360 |  Loss_1: (0.1488) | Acc_1: (94.54%) (43684/46208)\n",
      "Epoch: 144 | Batch_idx: 370 |  Loss_1: (0.1481) | Acc_1: (94.56%) (44906/47488)\n",
      "Epoch: 144 | Batch_idx: 380 |  Loss_1: (0.1481) | Acc_1: (94.57%) (46118/48768)\n",
      "Epoch: 144 | Batch_idx: 390 |  Loss_1: (0.1476) | Acc_1: (94.58%) (47291/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3367) | Acc: (91.84%) (9184/10000)\n",
      "Epoch: 145 | Batch_idx: 0 |  Loss_1: (0.1560) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 145 | Batch_idx: 10 |  Loss_1: (0.1907) | Acc_1: (93.11%) (1311/1408)\n",
      "Epoch: 145 | Batch_idx: 20 |  Loss_1: (0.1791) | Acc_1: (93.08%) (2502/2688)\n",
      "Epoch: 145 | Batch_idx: 30 |  Loss_1: (0.1754) | Acc_1: (93.42%) (3707/3968)\n",
      "Epoch: 145 | Batch_idx: 40 |  Loss_1: (0.1731) | Acc_1: (93.45%) (4904/5248)\n",
      "Epoch: 145 | Batch_idx: 50 |  Loss_1: (0.1673) | Acc_1: (93.77%) (6121/6528)\n",
      "Epoch: 145 | Batch_idx: 60 |  Loss_1: (0.1594) | Acc_1: (94.01%) (7340/7808)\n",
      "Epoch: 145 | Batch_idx: 70 |  Loss_1: (0.1599) | Acc_1: (94.01%) (8544/9088)\n",
      "Epoch: 145 | Batch_idx: 80 |  Loss_1: (0.1611) | Acc_1: (93.98%) (9744/10368)\n",
      "Epoch: 145 | Batch_idx: 90 |  Loss_1: (0.1598) | Acc_1: (94.02%) (10951/11648)\n",
      "Epoch: 145 | Batch_idx: 100 |  Loss_1: (0.1595) | Acc_1: (94.03%) (12156/12928)\n",
      "Epoch: 145 | Batch_idx: 110 |  Loss_1: (0.1567) | Acc_1: (94.14%) (13375/14208)\n",
      "Epoch: 145 | Batch_idx: 120 |  Loss_1: (0.1559) | Acc_1: (94.14%) (14581/15488)\n",
      "Epoch: 145 | Batch_idx: 130 |  Loss_1: (0.1542) | Acc_1: (94.22%) (15798/16768)\n",
      "Epoch: 145 | Batch_idx: 140 |  Loss_1: (0.1560) | Acc_1: (94.18%) (16998/18048)\n",
      "Epoch: 145 | Batch_idx: 150 |  Loss_1: (0.1546) | Acc_1: (94.25%) (18217/19328)\n",
      "Epoch: 145 | Batch_idx: 160 |  Loss_1: (0.1542) | Acc_1: (94.26%) (19426/20608)\n",
      "Epoch: 145 | Batch_idx: 170 |  Loss_1: (0.1542) | Acc_1: (94.28%) (20636/21888)\n",
      "Epoch: 145 | Batch_idx: 180 |  Loss_1: (0.1550) | Acc_1: (94.25%) (21836/23168)\n",
      "Epoch: 145 | Batch_idx: 190 |  Loss_1: (0.1549) | Acc_1: (94.25%) (23042/24448)\n",
      "Epoch: 145 | Batch_idx: 200 |  Loss_1: (0.1556) | Acc_1: (94.24%) (24245/25728)\n",
      "Epoch: 145 | Batch_idx: 210 |  Loss_1: (0.1552) | Acc_1: (94.24%) (25452/27008)\n",
      "Epoch: 145 | Batch_idx: 220 |  Loss_1: (0.1565) | Acc_1: (94.22%) (26652/28288)\n",
      "Epoch: 145 | Batch_idx: 230 |  Loss_1: (0.1567) | Acc_1: (94.21%) (27857/29568)\n",
      "Epoch: 145 | Batch_idx: 240 |  Loss_1: (0.1563) | Acc_1: (94.23%) (29067/30848)\n",
      "Epoch: 145 | Batch_idx: 250 |  Loss_1: (0.1559) | Acc_1: (94.25%) (30281/32128)\n",
      "Epoch: 145 | Batch_idx: 260 |  Loss_1: (0.1559) | Acc_1: (94.26%) (31489/33408)\n",
      "Epoch: 145 | Batch_idx: 270 |  Loss_1: (0.1561) | Acc_1: (94.25%) (32695/34688)\n",
      "Epoch: 145 | Batch_idx: 280 |  Loss_1: (0.1565) | Acc_1: (94.23%) (33894/35968)\n",
      "Epoch: 145 | Batch_idx: 290 |  Loss_1: (0.1558) | Acc_1: (94.26%) (35109/37248)\n",
      "Epoch: 145 | Batch_idx: 300 |  Loss_1: (0.1563) | Acc_1: (94.24%) (36310/38528)\n",
      "Epoch: 145 | Batch_idx: 310 |  Loss_1: (0.1560) | Acc_1: (94.24%) (37517/39808)\n",
      "Epoch: 145 | Batch_idx: 320 |  Loss_1: (0.1553) | Acc_1: (94.27%) (38732/41088)\n",
      "Epoch: 145 | Batch_idx: 330 |  Loss_1: (0.1553) | Acc_1: (94.27%) (39940/42368)\n",
      "Epoch: 145 | Batch_idx: 340 |  Loss_1: (0.1553) | Acc_1: (94.27%) (41145/43648)\n",
      "Epoch: 145 | Batch_idx: 350 |  Loss_1: (0.1547) | Acc_1: (94.29%) (42361/44928)\n",
      "Epoch: 145 | Batch_idx: 360 |  Loss_1: (0.1543) | Acc_1: (94.30%) (43572/46208)\n",
      "Epoch: 145 | Batch_idx: 370 |  Loss_1: (0.1541) | Acc_1: (94.31%) (44787/47488)\n",
      "Epoch: 145 | Batch_idx: 380 |  Loss_1: (0.1538) | Acc_1: (94.32%) (45996/48768)\n",
      "Epoch: 145 | Batch_idx: 390 |  Loss_1: (0.1535) | Acc_1: (94.33%) (47163/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3063) | Acc: (92.01%) (9201/10000)\n",
      "Epoch: 146 | Batch_idx: 0 |  Loss_1: (0.1495) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 146 | Batch_idx: 10 |  Loss_1: (0.1474) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 146 | Batch_idx: 20 |  Loss_1: (0.1447) | Acc_1: (94.68%) (2545/2688)\n",
      "Epoch: 146 | Batch_idx: 30 |  Loss_1: (0.1505) | Acc_1: (94.51%) (3750/3968)\n",
      "Epoch: 146 | Batch_idx: 40 |  Loss_1: (0.1503) | Acc_1: (94.38%) (4953/5248)\n",
      "Epoch: 146 | Batch_idx: 50 |  Loss_1: (0.1471) | Acc_1: (94.50%) (6169/6528)\n",
      "Epoch: 146 | Batch_idx: 60 |  Loss_1: (0.1464) | Acc_1: (94.56%) (7383/7808)\n",
      "Epoch: 146 | Batch_idx: 70 |  Loss_1: (0.1503) | Acc_1: (94.36%) (8575/9088)\n",
      "Epoch: 146 | Batch_idx: 80 |  Loss_1: (0.1505) | Acc_1: (94.38%) (9785/10368)\n",
      "Epoch: 146 | Batch_idx: 90 |  Loss_1: (0.1507) | Acc_1: (94.39%) (10994/11648)\n",
      "Epoch: 146 | Batch_idx: 100 |  Loss_1: (0.1498) | Acc_1: (94.45%) (12210/12928)\n",
      "Epoch: 146 | Batch_idx: 110 |  Loss_1: (0.1511) | Acc_1: (94.41%) (13414/14208)\n",
      "Epoch: 146 | Batch_idx: 120 |  Loss_1: (0.1516) | Acc_1: (94.37%) (14616/15488)\n",
      "Epoch: 146 | Batch_idx: 130 |  Loss_1: (0.1505) | Acc_1: (94.41%) (15831/16768)\n",
      "Epoch: 146 | Batch_idx: 140 |  Loss_1: (0.1506) | Acc_1: (94.40%) (17037/18048)\n",
      "Epoch: 146 | Batch_idx: 150 |  Loss_1: (0.1491) | Acc_1: (94.46%) (18258/19328)\n",
      "Epoch: 146 | Batch_idx: 160 |  Loss_1: (0.1495) | Acc_1: (94.43%) (19461/20608)\n",
      "Epoch: 146 | Batch_idx: 170 |  Loss_1: (0.1492) | Acc_1: (94.44%) (20670/21888)\n",
      "Epoch: 146 | Batch_idx: 180 |  Loss_1: (0.1492) | Acc_1: (94.44%) (21879/23168)\n",
      "Epoch: 146 | Batch_idx: 190 |  Loss_1: (0.1481) | Acc_1: (94.47%) (23095/24448)\n",
      "Epoch: 146 | Batch_idx: 200 |  Loss_1: (0.1481) | Acc_1: (94.45%) (24301/25728)\n",
      "Epoch: 146 | Batch_idx: 210 |  Loss_1: (0.1474) | Acc_1: (94.50%) (25522/27008)\n",
      "Epoch: 146 | Batch_idx: 220 |  Loss_1: (0.1463) | Acc_1: (94.54%) (26743/28288)\n",
      "Epoch: 146 | Batch_idx: 230 |  Loss_1: (0.1459) | Acc_1: (94.58%) (27964/29568)\n",
      "Epoch: 146 | Batch_idx: 240 |  Loss_1: (0.1456) | Acc_1: (94.58%) (29176/30848)\n",
      "Epoch: 146 | Batch_idx: 250 |  Loss_1: (0.1465) | Acc_1: (94.57%) (30384/32128)\n",
      "Epoch: 146 | Batch_idx: 260 |  Loss_1: (0.1453) | Acc_1: (94.63%) (31615/33408)\n",
      "Epoch: 146 | Batch_idx: 270 |  Loss_1: (0.1456) | Acc_1: (94.63%) (32824/34688)\n",
      "Epoch: 146 | Batch_idx: 280 |  Loss_1: (0.1459) | Acc_1: (94.60%) (34027/35968)\n",
      "Epoch: 146 | Batch_idx: 290 |  Loss_1: (0.1457) | Acc_1: (94.60%) (35237/37248)\n",
      "Epoch: 146 | Batch_idx: 300 |  Loss_1: (0.1453) | Acc_1: (94.60%) (36448/38528)\n",
      "Epoch: 146 | Batch_idx: 310 |  Loss_1: (0.1450) | Acc_1: (94.60%) (37659/39808)\n",
      "Epoch: 146 | Batch_idx: 320 |  Loss_1: (0.1459) | Acc_1: (94.57%) (38857/41088)\n",
      "Epoch: 146 | Batch_idx: 330 |  Loss_1: (0.1466) | Acc_1: (94.55%) (40057/42368)\n",
      "Epoch: 146 | Batch_idx: 340 |  Loss_1: (0.1468) | Acc_1: (94.55%) (41271/43648)\n",
      "Epoch: 146 | Batch_idx: 350 |  Loss_1: (0.1466) | Acc_1: (94.56%) (42482/44928)\n",
      "Epoch: 146 | Batch_idx: 360 |  Loss_1: (0.1470) | Acc_1: (94.54%) (43685/46208)\n",
      "Epoch: 146 | Batch_idx: 370 |  Loss_1: (0.1465) | Acc_1: (94.54%) (44897/47488)\n",
      "Epoch: 146 | Batch_idx: 380 |  Loss_1: (0.1466) | Acc_1: (94.54%) (46104/48768)\n",
      "Epoch: 146 | Batch_idx: 390 |  Loss_1: (0.1468) | Acc_1: (94.53%) (47264/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3657) | Acc: (91.29%) (9129/10000)\n",
      "Epoch: 147 | Batch_idx: 0 |  Loss_1: (0.1405) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 147 | Batch_idx: 10 |  Loss_1: (0.1285) | Acc_1: (95.24%) (1341/1408)\n",
      "Epoch: 147 | Batch_idx: 20 |  Loss_1: (0.1476) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 147 | Batch_idx: 30 |  Loss_1: (0.1510) | Acc_1: (94.63%) (3755/3968)\n",
      "Epoch: 147 | Batch_idx: 40 |  Loss_1: (0.1519) | Acc_1: (94.55%) (4962/5248)\n",
      "Epoch: 147 | Batch_idx: 50 |  Loss_1: (0.1493) | Acc_1: (94.65%) (6179/6528)\n",
      "Epoch: 147 | Batch_idx: 60 |  Loss_1: (0.1525) | Acc_1: (94.52%) (7380/7808)\n",
      "Epoch: 147 | Batch_idx: 70 |  Loss_1: (0.1523) | Acc_1: (94.48%) (8586/9088)\n",
      "Epoch: 147 | Batch_idx: 80 |  Loss_1: (0.1509) | Acc_1: (94.58%) (9806/10368)\n",
      "Epoch: 147 | Batch_idx: 90 |  Loss_1: (0.1495) | Acc_1: (94.59%) (11018/11648)\n",
      "Epoch: 147 | Batch_idx: 100 |  Loss_1: (0.1498) | Acc_1: (94.58%) (12227/12928)\n",
      "Epoch: 147 | Batch_idx: 110 |  Loss_1: (0.1505) | Acc_1: (94.60%) (13441/14208)\n",
      "Epoch: 147 | Batch_idx: 120 |  Loss_1: (0.1490) | Acc_1: (94.62%) (14654/15488)\n",
      "Epoch: 147 | Batch_idx: 130 |  Loss_1: (0.1497) | Acc_1: (94.60%) (15862/16768)\n",
      "Epoch: 147 | Batch_idx: 140 |  Loss_1: (0.1521) | Acc_1: (94.49%) (17053/18048)\n",
      "Epoch: 147 | Batch_idx: 150 |  Loss_1: (0.1524) | Acc_1: (94.47%) (18259/19328)\n",
      "Epoch: 147 | Batch_idx: 160 |  Loss_1: (0.1523) | Acc_1: (94.47%) (19468/20608)\n",
      "Epoch: 147 | Batch_idx: 170 |  Loss_1: (0.1525) | Acc_1: (94.45%) (20673/21888)\n",
      "Epoch: 147 | Batch_idx: 180 |  Loss_1: (0.1529) | Acc_1: (94.43%) (21877/23168)\n",
      "Epoch: 147 | Batch_idx: 190 |  Loss_1: (0.1526) | Acc_1: (94.42%) (23085/24448)\n",
      "Epoch: 147 | Batch_idx: 200 |  Loss_1: (0.1516) | Acc_1: (94.46%) (24303/25728)\n",
      "Epoch: 147 | Batch_idx: 210 |  Loss_1: (0.1505) | Acc_1: (94.49%) (25521/27008)\n",
      "Epoch: 147 | Batch_idx: 220 |  Loss_1: (0.1488) | Acc_1: (94.57%) (26751/28288)\n",
      "Epoch: 147 | Batch_idx: 230 |  Loss_1: (0.1491) | Acc_1: (94.54%) (27955/29568)\n",
      "Epoch: 147 | Batch_idx: 240 |  Loss_1: (0.1482) | Acc_1: (94.57%) (29172/30848)\n",
      "Epoch: 147 | Batch_idx: 250 |  Loss_1: (0.1480) | Acc_1: (94.58%) (30387/32128)\n",
      "Epoch: 147 | Batch_idx: 260 |  Loss_1: (0.1475) | Acc_1: (94.61%) (31607/33408)\n",
      "Epoch: 147 | Batch_idx: 270 |  Loss_1: (0.1486) | Acc_1: (94.55%) (32798/34688)\n",
      "Epoch: 147 | Batch_idx: 280 |  Loss_1: (0.1486) | Acc_1: (94.54%) (34005/35968)\n",
      "Epoch: 147 | Batch_idx: 290 |  Loss_1: (0.1479) | Acc_1: (94.56%) (35221/37248)\n",
      "Epoch: 147 | Batch_idx: 300 |  Loss_1: (0.1479) | Acc_1: (94.55%) (36429/38528)\n",
      "Epoch: 147 | Batch_idx: 310 |  Loss_1: (0.1476) | Acc_1: (94.56%) (37643/39808)\n",
      "Epoch: 147 | Batch_idx: 320 |  Loss_1: (0.1481) | Acc_1: (94.55%) (38848/41088)\n",
      "Epoch: 147 | Batch_idx: 330 |  Loss_1: (0.1481) | Acc_1: (94.55%) (40057/42368)\n",
      "Epoch: 147 | Batch_idx: 340 |  Loss_1: (0.1478) | Acc_1: (94.55%) (41269/43648)\n",
      "Epoch: 147 | Batch_idx: 350 |  Loss_1: (0.1475) | Acc_1: (94.56%) (42484/44928)\n",
      "Epoch: 147 | Batch_idx: 360 |  Loss_1: (0.1472) | Acc_1: (94.56%) (43694/46208)\n",
      "Epoch: 147 | Batch_idx: 370 |  Loss_1: (0.1465) | Acc_1: (94.60%) (44922/47488)\n",
      "Epoch: 147 | Batch_idx: 380 |  Loss_1: (0.1459) | Acc_1: (94.62%) (46144/48768)\n",
      "Epoch: 147 | Batch_idx: 390 |  Loss_1: (0.1460) | Acc_1: (94.61%) (47307/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3301) | Acc: (91.90%) (9190/10000)\n",
      "Epoch: 148 | Batch_idx: 0 |  Loss_1: (0.0746) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 148 | Batch_idx: 10 |  Loss_1: (0.1317) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 148 | Batch_idx: 20 |  Loss_1: (0.1425) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 148 | Batch_idx: 30 |  Loss_1: (0.1410) | Acc_1: (94.96%) (3768/3968)\n",
      "Epoch: 148 | Batch_idx: 40 |  Loss_1: (0.1437) | Acc_1: (94.89%) (4980/5248)\n",
      "Epoch: 148 | Batch_idx: 50 |  Loss_1: (0.1470) | Acc_1: (94.67%) (6180/6528)\n",
      "Epoch: 148 | Batch_idx: 60 |  Loss_1: (0.1504) | Acc_1: (94.63%) (7389/7808)\n",
      "Epoch: 148 | Batch_idx: 70 |  Loss_1: (0.1506) | Acc_1: (94.61%) (8598/9088)\n",
      "Epoch: 148 | Batch_idx: 80 |  Loss_1: (0.1485) | Acc_1: (94.65%) (9813/10368)\n",
      "Epoch: 148 | Batch_idx: 90 |  Loss_1: (0.1479) | Acc_1: (94.65%) (11025/11648)\n",
      "Epoch: 148 | Batch_idx: 100 |  Loss_1: (0.1450) | Acc_1: (94.73%) (12247/12928)\n",
      "Epoch: 148 | Batch_idx: 110 |  Loss_1: (0.1456) | Acc_1: (94.71%) (13456/14208)\n",
      "Epoch: 148 | Batch_idx: 120 |  Loss_1: (0.1462) | Acc_1: (94.69%) (14665/15488)\n",
      "Epoch: 148 | Batch_idx: 130 |  Loss_1: (0.1470) | Acc_1: (94.61%) (15864/16768)\n",
      "Epoch: 148 | Batch_idx: 140 |  Loss_1: (0.1475) | Acc_1: (94.59%) (17071/18048)\n",
      "Epoch: 148 | Batch_idx: 150 |  Loss_1: (0.1459) | Acc_1: (94.61%) (18287/19328)\n",
      "Epoch: 148 | Batch_idx: 160 |  Loss_1: (0.1445) | Acc_1: (94.66%) (19508/20608)\n",
      "Epoch: 148 | Batch_idx: 170 |  Loss_1: (0.1444) | Acc_1: (94.64%) (20715/21888)\n",
      "Epoch: 148 | Batch_idx: 180 |  Loss_1: (0.1440) | Acc_1: (94.65%) (21928/23168)\n",
      "Epoch: 148 | Batch_idx: 190 |  Loss_1: (0.1449) | Acc_1: (94.63%) (23135/24448)\n",
      "Epoch: 148 | Batch_idx: 200 |  Loss_1: (0.1453) | Acc_1: (94.61%) (24341/25728)\n",
      "Epoch: 148 | Batch_idx: 210 |  Loss_1: (0.1456) | Acc_1: (94.58%) (25545/27008)\n",
      "Epoch: 148 | Batch_idx: 220 |  Loss_1: (0.1462) | Acc_1: (94.61%) (26762/28288)\n",
      "Epoch: 148 | Batch_idx: 230 |  Loss_1: (0.1456) | Acc_1: (94.63%) (27980/29568)\n",
      "Epoch: 148 | Batch_idx: 240 |  Loss_1: (0.1453) | Acc_1: (94.64%) (29194/30848)\n",
      "Epoch: 148 | Batch_idx: 250 |  Loss_1: (0.1452) | Acc_1: (94.63%) (30403/32128)\n",
      "Epoch: 148 | Batch_idx: 260 |  Loss_1: (0.1454) | Acc_1: (94.63%) (31614/33408)\n",
      "Epoch: 148 | Batch_idx: 270 |  Loss_1: (0.1443) | Acc_1: (94.66%) (32837/34688)\n",
      "Epoch: 148 | Batch_idx: 280 |  Loss_1: (0.1442) | Acc_1: (94.67%) (34050/35968)\n",
      "Epoch: 148 | Batch_idx: 290 |  Loss_1: (0.1444) | Acc_1: (94.65%) (35256/37248)\n",
      "Epoch: 148 | Batch_idx: 300 |  Loss_1: (0.1439) | Acc_1: (94.68%) (36477/38528)\n",
      "Epoch: 148 | Batch_idx: 310 |  Loss_1: (0.1440) | Acc_1: (94.68%) (37691/39808)\n",
      "Epoch: 148 | Batch_idx: 320 |  Loss_1: (0.1447) | Acc_1: (94.66%) (38894/41088)\n",
      "Epoch: 148 | Batch_idx: 330 |  Loss_1: (0.1451) | Acc_1: (94.64%) (40097/42368)\n",
      "Epoch: 148 | Batch_idx: 340 |  Loss_1: (0.1449) | Acc_1: (94.65%) (41314/43648)\n",
      "Epoch: 148 | Batch_idx: 350 |  Loss_1: (0.1451) | Acc_1: (94.63%) (42516/44928)\n",
      "Epoch: 148 | Batch_idx: 360 |  Loss_1: (0.1449) | Acc_1: (94.64%) (43729/46208)\n",
      "Epoch: 148 | Batch_idx: 370 |  Loss_1: (0.1445) | Acc_1: (94.65%) (44946/47488)\n",
      "Epoch: 148 | Batch_idx: 380 |  Loss_1: (0.1441) | Acc_1: (94.67%) (46170/48768)\n",
      "Epoch: 148 | Batch_idx: 390 |  Loss_1: (0.1436) | Acc_1: (94.69%) (47344/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3160) | Acc: (92.39%) (9239/10000)\n",
      "Epoch: 149 | Batch_idx: 0 |  Loss_1: (0.1849) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 149 | Batch_idx: 10 |  Loss_1: (0.1349) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 149 | Batch_idx: 20 |  Loss_1: (0.1342) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 149 | Batch_idx: 30 |  Loss_1: (0.1362) | Acc_1: (94.88%) (3765/3968)\n",
      "Epoch: 149 | Batch_idx: 40 |  Loss_1: (0.1357) | Acc_1: (94.91%) (4981/5248)\n",
      "Epoch: 149 | Batch_idx: 50 |  Loss_1: (0.1365) | Acc_1: (94.90%) (6195/6528)\n",
      "Epoch: 149 | Batch_idx: 60 |  Loss_1: (0.1370) | Acc_1: (94.92%) (7411/7808)\n",
      "Epoch: 149 | Batch_idx: 70 |  Loss_1: (0.1384) | Acc_1: (94.83%) (8618/9088)\n",
      "Epoch: 149 | Batch_idx: 80 |  Loss_1: (0.1400) | Acc_1: (94.80%) (9829/10368)\n",
      "Epoch: 149 | Batch_idx: 90 |  Loss_1: (0.1411) | Acc_1: (94.79%) (11041/11648)\n",
      "Epoch: 149 | Batch_idx: 100 |  Loss_1: (0.1426) | Acc_1: (94.76%) (12251/12928)\n",
      "Epoch: 149 | Batch_idx: 110 |  Loss_1: (0.1434) | Acc_1: (94.76%) (13463/14208)\n",
      "Epoch: 149 | Batch_idx: 120 |  Loss_1: (0.1424) | Acc_1: (94.78%) (14679/15488)\n",
      "Epoch: 149 | Batch_idx: 130 |  Loss_1: (0.1415) | Acc_1: (94.79%) (15895/16768)\n",
      "Epoch: 149 | Batch_idx: 140 |  Loss_1: (0.1411) | Acc_1: (94.79%) (17108/18048)\n",
      "Epoch: 149 | Batch_idx: 150 |  Loss_1: (0.1407) | Acc_1: (94.84%) (18330/19328)\n",
      "Epoch: 149 | Batch_idx: 160 |  Loss_1: (0.1410) | Acc_1: (94.82%) (19540/20608)\n",
      "Epoch: 149 | Batch_idx: 170 |  Loss_1: (0.1419) | Acc_1: (94.78%) (20746/21888)\n",
      "Epoch: 149 | Batch_idx: 180 |  Loss_1: (0.1425) | Acc_1: (94.76%) (21955/23168)\n",
      "Epoch: 149 | Batch_idx: 190 |  Loss_1: (0.1434) | Acc_1: (94.74%) (23163/24448)\n",
      "Epoch: 149 | Batch_idx: 200 |  Loss_1: (0.1426) | Acc_1: (94.76%) (24380/25728)\n",
      "Epoch: 149 | Batch_idx: 210 |  Loss_1: (0.1411) | Acc_1: (94.79%) (25602/27008)\n",
      "Epoch: 149 | Batch_idx: 220 |  Loss_1: (0.1418) | Acc_1: (94.76%) (26807/28288)\n",
      "Epoch: 149 | Batch_idx: 230 |  Loss_1: (0.1421) | Acc_1: (94.74%) (28014/29568)\n",
      "Epoch: 149 | Batch_idx: 240 |  Loss_1: (0.1411) | Acc_1: (94.78%) (29238/30848)\n",
      "Epoch: 149 | Batch_idx: 250 |  Loss_1: (0.1413) | Acc_1: (94.77%) (30448/32128)\n",
      "Epoch: 149 | Batch_idx: 260 |  Loss_1: (0.1408) | Acc_1: (94.79%) (31669/33408)\n",
      "Epoch: 149 | Batch_idx: 270 |  Loss_1: (0.1413) | Acc_1: (94.79%) (32880/34688)\n",
      "Epoch: 149 | Batch_idx: 280 |  Loss_1: (0.1419) | Acc_1: (94.76%) (34082/35968)\n",
      "Epoch: 149 | Batch_idx: 290 |  Loss_1: (0.1419) | Acc_1: (94.76%) (35297/37248)\n",
      "Epoch: 149 | Batch_idx: 300 |  Loss_1: (0.1414) | Acc_1: (94.78%) (36515/38528)\n",
      "Epoch: 149 | Batch_idx: 310 |  Loss_1: (0.1416) | Acc_1: (94.77%) (37728/39808)\n",
      "Epoch: 149 | Batch_idx: 320 |  Loss_1: (0.1418) | Acc_1: (94.77%) (38938/41088)\n",
      "Epoch: 149 | Batch_idx: 330 |  Loss_1: (0.1421) | Acc_1: (94.76%) (40147/42368)\n",
      "Epoch: 149 | Batch_idx: 340 |  Loss_1: (0.1422) | Acc_1: (94.75%) (41357/43648)\n",
      "Epoch: 149 | Batch_idx: 350 |  Loss_1: (0.1431) | Acc_1: (94.73%) (42559/44928)\n",
      "Epoch: 149 | Batch_idx: 360 |  Loss_1: (0.1431) | Acc_1: (94.73%) (43772/46208)\n",
      "Epoch: 149 | Batch_idx: 370 |  Loss_1: (0.1431) | Acc_1: (94.73%) (44985/47488)\n",
      "Epoch: 149 | Batch_idx: 380 |  Loss_1: (0.1433) | Acc_1: (94.73%) (46200/48768)\n",
      "Epoch: 149 | Batch_idx: 390 |  Loss_1: (0.1432) | Acc_1: (94.72%) (47362/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3224) | Acc: (92.10%) (9210/10000)\n",
      "Epoch: 150 | Batch_idx: 0 |  Loss_1: (0.1719) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 150 | Batch_idx: 10 |  Loss_1: (0.1466) | Acc_1: (94.53%) (1331/1408)\n",
      "Epoch: 150 | Batch_idx: 20 |  Loss_1: (0.1495) | Acc_1: (94.72%) (2546/2688)\n",
      "Epoch: 150 | Batch_idx: 30 |  Loss_1: (0.1454) | Acc_1: (94.76%) (3760/3968)\n",
      "Epoch: 150 | Batch_idx: 40 |  Loss_1: (0.1457) | Acc_1: (94.82%) (4976/5248)\n",
      "Epoch: 150 | Batch_idx: 50 |  Loss_1: (0.1407) | Acc_1: (94.91%) (6196/6528)\n",
      "Epoch: 150 | Batch_idx: 60 |  Loss_1: (0.1426) | Acc_1: (94.83%) (7404/7808)\n",
      "Epoch: 150 | Batch_idx: 70 |  Loss_1: (0.1406) | Acc_1: (94.84%) (8619/9088)\n",
      "Epoch: 150 | Batch_idx: 80 |  Loss_1: (0.1408) | Acc_1: (94.81%) (9830/10368)\n",
      "Epoch: 150 | Batch_idx: 90 |  Loss_1: (0.1443) | Acc_1: (94.71%) (11032/11648)\n",
      "Epoch: 150 | Batch_idx: 100 |  Loss_1: (0.1436) | Acc_1: (94.72%) (12245/12928)\n",
      "Epoch: 150 | Batch_idx: 110 |  Loss_1: (0.1441) | Acc_1: (94.71%) (13456/14208)\n",
      "Epoch: 150 | Batch_idx: 120 |  Loss_1: (0.1429) | Acc_1: (94.72%) (14671/15488)\n",
      "Epoch: 150 | Batch_idx: 130 |  Loss_1: (0.1446) | Acc_1: (94.64%) (15869/16768)\n",
      "Epoch: 150 | Batch_idx: 140 |  Loss_1: (0.1449) | Acc_1: (94.61%) (17075/18048)\n",
      "Epoch: 150 | Batch_idx: 150 |  Loss_1: (0.1468) | Acc_1: (94.56%) (18276/19328)\n",
      "Epoch: 150 | Batch_idx: 160 |  Loss_1: (0.1464) | Acc_1: (94.62%) (19500/20608)\n",
      "Epoch: 150 | Batch_idx: 170 |  Loss_1: (0.1463) | Acc_1: (94.62%) (20710/21888)\n",
      "Epoch: 150 | Batch_idx: 180 |  Loss_1: (0.1464) | Acc_1: (94.60%) (21916/23168)\n",
      "Epoch: 150 | Batch_idx: 190 |  Loss_1: (0.1472) | Acc_1: (94.56%) (23119/24448)\n",
      "Epoch: 150 | Batch_idx: 200 |  Loss_1: (0.1462) | Acc_1: (94.59%) (24335/25728)\n",
      "Epoch: 150 | Batch_idx: 210 |  Loss_1: (0.1465) | Acc_1: (94.58%) (25544/27008)\n",
      "Epoch: 150 | Batch_idx: 220 |  Loss_1: (0.1459) | Acc_1: (94.59%) (26759/28288)\n",
      "Epoch: 150 | Batch_idx: 230 |  Loss_1: (0.1475) | Acc_1: (94.52%) (27948/29568)\n",
      "Epoch: 150 | Batch_idx: 240 |  Loss_1: (0.1479) | Acc_1: (94.52%) (29158/30848)\n",
      "Epoch: 150 | Batch_idx: 250 |  Loss_1: (0.1472) | Acc_1: (94.55%) (30376/32128)\n",
      "Epoch: 150 | Batch_idx: 260 |  Loss_1: (0.1463) | Acc_1: (94.59%) (31599/33408)\n",
      "Epoch: 150 | Batch_idx: 270 |  Loss_1: (0.1459) | Acc_1: (94.61%) (32819/34688)\n",
      "Epoch: 150 | Batch_idx: 280 |  Loss_1: (0.1448) | Acc_1: (94.64%) (34041/35968)\n",
      "Epoch: 150 | Batch_idx: 290 |  Loss_1: (0.1456) | Acc_1: (94.62%) (35244/37248)\n",
      "Epoch: 150 | Batch_idx: 300 |  Loss_1: (0.1448) | Acc_1: (94.65%) (36466/38528)\n",
      "Epoch: 150 | Batch_idx: 310 |  Loss_1: (0.1462) | Acc_1: (94.61%) (37662/39808)\n",
      "Epoch: 150 | Batch_idx: 320 |  Loss_1: (0.1472) | Acc_1: (94.56%) (38852/41088)\n",
      "Epoch: 150 | Batch_idx: 330 |  Loss_1: (0.1476) | Acc_1: (94.55%) (40057/42368)\n",
      "Epoch: 150 | Batch_idx: 340 |  Loss_1: (0.1474) | Acc_1: (94.56%) (41273/43648)\n",
      "Epoch: 150 | Batch_idx: 350 |  Loss_1: (0.1466) | Acc_1: (94.58%) (42493/44928)\n",
      "Epoch: 150 | Batch_idx: 360 |  Loss_1: (0.1462) | Acc_1: (94.61%) (43717/46208)\n",
      "Epoch: 150 | Batch_idx: 370 |  Loss_1: (0.1459) | Acc_1: (94.62%) (44932/47488)\n",
      "Epoch: 150 | Batch_idx: 380 |  Loss_1: (0.1453) | Acc_1: (94.63%) (46151/48768)\n",
      "Epoch: 150 | Batch_idx: 390 |  Loss_1: (0.1459) | Acc_1: (94.61%) (47306/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3170) | Acc: (92.56%) (9256/10000)\n",
      "Epoch: 151 | Batch_idx: 0 |  Loss_1: (0.1063) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 151 | Batch_idx: 10 |  Loss_1: (0.1301) | Acc_1: (95.24%) (1341/1408)\n",
      "Epoch: 151 | Batch_idx: 20 |  Loss_1: (0.1346) | Acc_1: (95.09%) (2556/2688)\n",
      "Epoch: 151 | Batch_idx: 30 |  Loss_1: (0.1364) | Acc_1: (95.04%) (3771/3968)\n",
      "Epoch: 151 | Batch_idx: 40 |  Loss_1: (0.1344) | Acc_1: (95.16%) (4994/5248)\n",
      "Epoch: 151 | Batch_idx: 50 |  Loss_1: (0.1352) | Acc_1: (95.10%) (6208/6528)\n",
      "Epoch: 151 | Batch_idx: 60 |  Loss_1: (0.1355) | Acc_1: (95.09%) (7425/7808)\n",
      "Epoch: 151 | Batch_idx: 70 |  Loss_1: (0.1385) | Acc_1: (94.93%) (8627/9088)\n",
      "Epoch: 151 | Batch_idx: 80 |  Loss_1: (0.1333) | Acc_1: (95.12%) (9862/10368)\n",
      "Epoch: 151 | Batch_idx: 90 |  Loss_1: (0.1361) | Acc_1: (94.94%) (11059/11648)\n",
      "Epoch: 151 | Batch_idx: 100 |  Loss_1: (0.1391) | Acc_1: (94.81%) (12257/12928)\n",
      "Epoch: 151 | Batch_idx: 110 |  Loss_1: (0.1388) | Acc_1: (94.78%) (13466/14208)\n",
      "Epoch: 151 | Batch_idx: 120 |  Loss_1: (0.1388) | Acc_1: (94.79%) (14681/15488)\n",
      "Epoch: 151 | Batch_idx: 130 |  Loss_1: (0.1391) | Acc_1: (94.76%) (15889/16768)\n",
      "Epoch: 151 | Batch_idx: 140 |  Loss_1: (0.1408) | Acc_1: (94.70%) (17091/18048)\n",
      "Epoch: 151 | Batch_idx: 150 |  Loss_1: (0.1402) | Acc_1: (94.72%) (18307/19328)\n",
      "Epoch: 151 | Batch_idx: 160 |  Loss_1: (0.1394) | Acc_1: (94.77%) (19531/20608)\n",
      "Epoch: 151 | Batch_idx: 170 |  Loss_1: (0.1399) | Acc_1: (94.76%) (20741/21888)\n",
      "Epoch: 151 | Batch_idx: 180 |  Loss_1: (0.1422) | Acc_1: (94.70%) (21939/23168)\n",
      "Epoch: 151 | Batch_idx: 190 |  Loss_1: (0.1428) | Acc_1: (94.68%) (23147/24448)\n",
      "Epoch: 151 | Batch_idx: 200 |  Loss_1: (0.1436) | Acc_1: (94.66%) (24353/25728)\n",
      "Epoch: 151 | Batch_idx: 210 |  Loss_1: (0.1436) | Acc_1: (94.65%) (25562/27008)\n",
      "Epoch: 151 | Batch_idx: 220 |  Loss_1: (0.1444) | Acc_1: (94.63%) (26769/28288)\n",
      "Epoch: 151 | Batch_idx: 230 |  Loss_1: (0.1440) | Acc_1: (94.67%) (27993/29568)\n",
      "Epoch: 151 | Batch_idx: 240 |  Loss_1: (0.1445) | Acc_1: (94.64%) (29194/30848)\n",
      "Epoch: 151 | Batch_idx: 250 |  Loss_1: (0.1453) | Acc_1: (94.63%) (30404/32128)\n",
      "Epoch: 151 | Batch_idx: 260 |  Loss_1: (0.1454) | Acc_1: (94.64%) (31618/33408)\n",
      "Epoch: 151 | Batch_idx: 270 |  Loss_1: (0.1461) | Acc_1: (94.61%) (32819/34688)\n",
      "Epoch: 151 | Batch_idx: 280 |  Loss_1: (0.1464) | Acc_1: (94.60%) (34024/35968)\n",
      "Epoch: 151 | Batch_idx: 290 |  Loss_1: (0.1467) | Acc_1: (94.58%) (35231/37248)\n",
      "Epoch: 151 | Batch_idx: 300 |  Loss_1: (0.1468) | Acc_1: (94.59%) (36442/38528)\n",
      "Epoch: 151 | Batch_idx: 310 |  Loss_1: (0.1469) | Acc_1: (94.57%) (37646/39808)\n",
      "Epoch: 151 | Batch_idx: 320 |  Loss_1: (0.1465) | Acc_1: (94.59%) (38867/41088)\n",
      "Epoch: 151 | Batch_idx: 330 |  Loss_1: (0.1462) | Acc_1: (94.61%) (40085/42368)\n",
      "Epoch: 151 | Batch_idx: 340 |  Loss_1: (0.1462) | Acc_1: (94.61%) (41296/43648)\n",
      "Epoch: 151 | Batch_idx: 350 |  Loss_1: (0.1455) | Acc_1: (94.64%) (42519/44928)\n",
      "Epoch: 151 | Batch_idx: 360 |  Loss_1: (0.1455) | Acc_1: (94.63%) (43726/46208)\n",
      "Epoch: 151 | Batch_idx: 370 |  Loss_1: (0.1460) | Acc_1: (94.61%) (44928/47488)\n",
      "Epoch: 151 | Batch_idx: 380 |  Loss_1: (0.1457) | Acc_1: (94.62%) (46143/48768)\n",
      "Epoch: 151 | Batch_idx: 390 |  Loss_1: (0.1458) | Acc_1: (94.60%) (47301/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3121) | Acc: (92.39%) (9239/10000)\n",
      "Epoch: 152 | Batch_idx: 0 |  Loss_1: (0.1115) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 152 | Batch_idx: 10 |  Loss_1: (0.1312) | Acc_1: (95.17%) (1340/1408)\n",
      "Epoch: 152 | Batch_idx: 20 |  Loss_1: (0.1423) | Acc_1: (94.53%) (2541/2688)\n",
      "Epoch: 152 | Batch_idx: 30 |  Loss_1: (0.1389) | Acc_1: (94.76%) (3760/3968)\n",
      "Epoch: 152 | Batch_idx: 40 |  Loss_1: (0.1384) | Acc_1: (94.72%) (4971/5248)\n",
      "Epoch: 152 | Batch_idx: 50 |  Loss_1: (0.1415) | Acc_1: (94.65%) (6179/6528)\n",
      "Epoch: 152 | Batch_idx: 60 |  Loss_1: (0.1441) | Acc_1: (94.57%) (7384/7808)\n",
      "Epoch: 152 | Batch_idx: 70 |  Loss_1: (0.1451) | Acc_1: (94.45%) (8584/9088)\n",
      "Epoch: 152 | Batch_idx: 80 |  Loss_1: (0.1449) | Acc_1: (94.44%) (9792/10368)\n",
      "Epoch: 152 | Batch_idx: 90 |  Loss_1: (0.1412) | Acc_1: (94.63%) (11022/11648)\n",
      "Epoch: 152 | Batch_idx: 100 |  Loss_1: (0.1395) | Acc_1: (94.69%) (12242/12928)\n",
      "Epoch: 152 | Batch_idx: 110 |  Loss_1: (0.1402) | Acc_1: (94.70%) (13455/14208)\n",
      "Epoch: 152 | Batch_idx: 120 |  Loss_1: (0.1404) | Acc_1: (94.71%) (14669/15488)\n",
      "Epoch: 152 | Batch_idx: 130 |  Loss_1: (0.1411) | Acc_1: (94.73%) (15885/16768)\n",
      "Epoch: 152 | Batch_idx: 140 |  Loss_1: (0.1430) | Acc_1: (94.69%) (17089/18048)\n",
      "Epoch: 152 | Batch_idx: 150 |  Loss_1: (0.1431) | Acc_1: (94.72%) (18307/19328)\n",
      "Epoch: 152 | Batch_idx: 160 |  Loss_1: (0.1430) | Acc_1: (94.70%) (19516/20608)\n",
      "Epoch: 152 | Batch_idx: 170 |  Loss_1: (0.1416) | Acc_1: (94.76%) (20740/21888)\n",
      "Epoch: 152 | Batch_idx: 180 |  Loss_1: (0.1422) | Acc_1: (94.73%) (21946/23168)\n",
      "Epoch: 152 | Batch_idx: 190 |  Loss_1: (0.1426) | Acc_1: (94.72%) (23157/24448)\n",
      "Epoch: 152 | Batch_idx: 200 |  Loss_1: (0.1418) | Acc_1: (94.76%) (24379/25728)\n",
      "Epoch: 152 | Batch_idx: 210 |  Loss_1: (0.1417) | Acc_1: (94.77%) (25595/27008)\n",
      "Epoch: 152 | Batch_idx: 220 |  Loss_1: (0.1422) | Acc_1: (94.75%) (26803/28288)\n",
      "Epoch: 152 | Batch_idx: 230 |  Loss_1: (0.1419) | Acc_1: (94.75%) (28017/29568)\n",
      "Epoch: 152 | Batch_idx: 240 |  Loss_1: (0.1422) | Acc_1: (94.74%) (29226/30848)\n",
      "Epoch: 152 | Batch_idx: 250 |  Loss_1: (0.1423) | Acc_1: (94.74%) (30439/32128)\n",
      "Epoch: 152 | Batch_idx: 260 |  Loss_1: (0.1429) | Acc_1: (94.72%) (31644/33408)\n",
      "Epoch: 152 | Batch_idx: 270 |  Loss_1: (0.1437) | Acc_1: (94.66%) (32836/34688)\n",
      "Epoch: 152 | Batch_idx: 280 |  Loss_1: (0.1442) | Acc_1: (94.65%) (34044/35968)\n",
      "Epoch: 152 | Batch_idx: 290 |  Loss_1: (0.1441) | Acc_1: (94.65%) (35257/37248)\n",
      "Epoch: 152 | Batch_idx: 300 |  Loss_1: (0.1439) | Acc_1: (94.67%) (36473/38528)\n",
      "Epoch: 152 | Batch_idx: 310 |  Loss_1: (0.1446) | Acc_1: (94.63%) (37672/39808)\n",
      "Epoch: 152 | Batch_idx: 320 |  Loss_1: (0.1454) | Acc_1: (94.60%) (38870/41088)\n",
      "Epoch: 152 | Batch_idx: 330 |  Loss_1: (0.1454) | Acc_1: (94.62%) (40087/42368)\n",
      "Epoch: 152 | Batch_idx: 340 |  Loss_1: (0.1449) | Acc_1: (94.63%) (41306/43648)\n",
      "Epoch: 152 | Batch_idx: 350 |  Loss_1: (0.1449) | Acc_1: (94.63%) (42517/44928)\n",
      "Epoch: 152 | Batch_idx: 360 |  Loss_1: (0.1450) | Acc_1: (94.62%) (43722/46208)\n",
      "Epoch: 152 | Batch_idx: 370 |  Loss_1: (0.1454) | Acc_1: (94.60%) (44924/47488)\n",
      "Epoch: 152 | Batch_idx: 380 |  Loss_1: (0.1449) | Acc_1: (94.62%) (46146/48768)\n",
      "Epoch: 152 | Batch_idx: 390 |  Loss_1: (0.1447) | Acc_1: (94.63%) (47313/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3314) | Acc: (92.05%) (9205/10000)\n",
      "Epoch: 153 | Batch_idx: 0 |  Loss_1: (0.0708) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 153 | Batch_idx: 10 |  Loss_1: (0.1087) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 153 | Batch_idx: 20 |  Loss_1: (0.1361) | Acc_1: (95.13%) (2557/2688)\n",
      "Epoch: 153 | Batch_idx: 30 |  Loss_1: (0.1320) | Acc_1: (95.26%) (3780/3968)\n",
      "Epoch: 153 | Batch_idx: 40 |  Loss_1: (0.1388) | Acc_1: (94.97%) (4984/5248)\n",
      "Epoch: 153 | Batch_idx: 50 |  Loss_1: (0.1376) | Acc_1: (94.99%) (6201/6528)\n",
      "Epoch: 153 | Batch_idx: 60 |  Loss_1: (0.1340) | Acc_1: (95.12%) (7427/7808)\n",
      "Epoch: 153 | Batch_idx: 70 |  Loss_1: (0.1382) | Acc_1: (94.92%) (8626/9088)\n",
      "Epoch: 153 | Batch_idx: 80 |  Loss_1: (0.1422) | Acc_1: (94.78%) (9827/10368)\n",
      "Epoch: 153 | Batch_idx: 90 |  Loss_1: (0.1426) | Acc_1: (94.81%) (11044/11648)\n",
      "Epoch: 153 | Batch_idx: 100 |  Loss_1: (0.1418) | Acc_1: (94.83%) (12259/12928)\n",
      "Epoch: 153 | Batch_idx: 110 |  Loss_1: (0.1432) | Acc_1: (94.73%) (13459/14208)\n",
      "Epoch: 153 | Batch_idx: 120 |  Loss_1: (0.1435) | Acc_1: (94.72%) (14670/15488)\n",
      "Epoch: 153 | Batch_idx: 130 |  Loss_1: (0.1438) | Acc_1: (94.72%) (15882/16768)\n",
      "Epoch: 153 | Batch_idx: 140 |  Loss_1: (0.1445) | Acc_1: (94.67%) (17086/18048)\n",
      "Epoch: 153 | Batch_idx: 150 |  Loss_1: (0.1432) | Acc_1: (94.70%) (18304/19328)\n",
      "Epoch: 153 | Batch_idx: 160 |  Loss_1: (0.1428) | Acc_1: (94.71%) (19517/20608)\n",
      "Epoch: 153 | Batch_idx: 170 |  Loss_1: (0.1425) | Acc_1: (94.69%) (20726/21888)\n",
      "Epoch: 153 | Batch_idx: 180 |  Loss_1: (0.1428) | Acc_1: (94.68%) (21935/23168)\n",
      "Epoch: 153 | Batch_idx: 190 |  Loss_1: (0.1436) | Acc_1: (94.63%) (23136/24448)\n",
      "Epoch: 153 | Batch_idx: 200 |  Loss_1: (0.1451) | Acc_1: (94.59%) (24335/25728)\n",
      "Epoch: 153 | Batch_idx: 210 |  Loss_1: (0.1448) | Acc_1: (94.60%) (25549/27008)\n",
      "Epoch: 153 | Batch_idx: 220 |  Loss_1: (0.1440) | Acc_1: (94.63%) (26768/28288)\n",
      "Epoch: 153 | Batch_idx: 230 |  Loss_1: (0.1444) | Acc_1: (94.61%) (27973/29568)\n",
      "Epoch: 153 | Batch_idx: 240 |  Loss_1: (0.1466) | Acc_1: (94.54%) (29164/30848)\n",
      "Epoch: 153 | Batch_idx: 250 |  Loss_1: (0.1466) | Acc_1: (94.54%) (30373/32128)\n",
      "Epoch: 153 | Batch_idx: 260 |  Loss_1: (0.1471) | Acc_1: (94.52%) (31576/33408)\n",
      "Epoch: 153 | Batch_idx: 270 |  Loss_1: (0.1471) | Acc_1: (94.52%) (32787/34688)\n",
      "Epoch: 153 | Batch_idx: 280 |  Loss_1: (0.1458) | Acc_1: (94.57%) (34014/35968)\n",
      "Epoch: 153 | Batch_idx: 290 |  Loss_1: (0.1457) | Acc_1: (94.58%) (35230/37248)\n",
      "Epoch: 153 | Batch_idx: 300 |  Loss_1: (0.1450) | Acc_1: (94.61%) (36453/38528)\n",
      "Epoch: 153 | Batch_idx: 310 |  Loss_1: (0.1442) | Acc_1: (94.63%) (37671/39808)\n",
      "Epoch: 153 | Batch_idx: 320 |  Loss_1: (0.1439) | Acc_1: (94.64%) (38887/41088)\n",
      "Epoch: 153 | Batch_idx: 330 |  Loss_1: (0.1440) | Acc_1: (94.64%) (40097/42368)\n",
      "Epoch: 153 | Batch_idx: 340 |  Loss_1: (0.1448) | Acc_1: (94.61%) (41294/43648)\n",
      "Epoch: 153 | Batch_idx: 350 |  Loss_1: (0.1453) | Acc_1: (94.60%) (42500/44928)\n",
      "Epoch: 153 | Batch_idx: 360 |  Loss_1: (0.1456) | Acc_1: (94.59%) (43709/46208)\n",
      "Epoch: 153 | Batch_idx: 370 |  Loss_1: (0.1466) | Acc_1: (94.54%) (44893/47488)\n",
      "Epoch: 153 | Batch_idx: 380 |  Loss_1: (0.1460) | Acc_1: (94.55%) (46110/48768)\n",
      "Epoch: 153 | Batch_idx: 390 |  Loss_1: (0.1463) | Acc_1: (94.56%) (47280/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4195) | Acc: (90.33%) (9033/10000)\n",
      "Epoch: 154 | Batch_idx: 0 |  Loss_1: (0.2120) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 154 | Batch_idx: 10 |  Loss_1: (0.1540) | Acc_1: (94.32%) (1328/1408)\n",
      "Epoch: 154 | Batch_idx: 20 |  Loss_1: (0.1477) | Acc_1: (94.61%) (2543/2688)\n",
      "Epoch: 154 | Batch_idx: 30 |  Loss_1: (0.1444) | Acc_1: (94.83%) (3763/3968)\n",
      "Epoch: 154 | Batch_idx: 40 |  Loss_1: (0.1428) | Acc_1: (94.86%) (4978/5248)\n",
      "Epoch: 154 | Batch_idx: 50 |  Loss_1: (0.1407) | Acc_1: (94.96%) (6199/6528)\n",
      "Epoch: 154 | Batch_idx: 60 |  Loss_1: (0.1365) | Acc_1: (95.06%) (7422/7808)\n",
      "Epoch: 154 | Batch_idx: 70 |  Loss_1: (0.1359) | Acc_1: (95.04%) (8637/9088)\n",
      "Epoch: 154 | Batch_idx: 80 |  Loss_1: (0.1346) | Acc_1: (95.06%) (9856/10368)\n",
      "Epoch: 154 | Batch_idx: 90 |  Loss_1: (0.1337) | Acc_1: (95.07%) (11074/11648)\n",
      "Epoch: 154 | Batch_idx: 100 |  Loss_1: (0.1340) | Acc_1: (95.04%) (12287/12928)\n",
      "Epoch: 154 | Batch_idx: 110 |  Loss_1: (0.1336) | Acc_1: (95.05%) (13505/14208)\n",
      "Epoch: 154 | Batch_idx: 120 |  Loss_1: (0.1318) | Acc_1: (95.12%) (14732/15488)\n",
      "Epoch: 154 | Batch_idx: 130 |  Loss_1: (0.1329) | Acc_1: (95.07%) (15941/16768)\n",
      "Epoch: 154 | Batch_idx: 140 |  Loss_1: (0.1340) | Acc_1: (95.05%) (17155/18048)\n",
      "Epoch: 154 | Batch_idx: 150 |  Loss_1: (0.1324) | Acc_1: (95.08%) (18377/19328)\n",
      "Epoch: 154 | Batch_idx: 160 |  Loss_1: (0.1327) | Acc_1: (95.11%) (19601/20608)\n",
      "Epoch: 154 | Batch_idx: 170 |  Loss_1: (0.1328) | Acc_1: (95.11%) (20818/21888)\n",
      "Epoch: 154 | Batch_idx: 180 |  Loss_1: (0.1335) | Acc_1: (95.09%) (22031/23168)\n",
      "Epoch: 154 | Batch_idx: 190 |  Loss_1: (0.1333) | Acc_1: (95.10%) (23249/24448)\n",
      "Epoch: 154 | Batch_idx: 200 |  Loss_1: (0.1331) | Acc_1: (95.11%) (24469/25728)\n",
      "Epoch: 154 | Batch_idx: 210 |  Loss_1: (0.1326) | Acc_1: (95.10%) (25685/27008)\n",
      "Epoch: 154 | Batch_idx: 220 |  Loss_1: (0.1313) | Acc_1: (95.15%) (26917/28288)\n",
      "Epoch: 154 | Batch_idx: 230 |  Loss_1: (0.1325) | Acc_1: (95.11%) (28121/29568)\n",
      "Epoch: 154 | Batch_idx: 240 |  Loss_1: (0.1327) | Acc_1: (95.11%) (29339/30848)\n",
      "Epoch: 154 | Batch_idx: 250 |  Loss_1: (0.1330) | Acc_1: (95.10%) (30553/32128)\n",
      "Epoch: 154 | Batch_idx: 260 |  Loss_1: (0.1343) | Acc_1: (95.04%) (31750/33408)\n",
      "Epoch: 154 | Batch_idx: 270 |  Loss_1: (0.1346) | Acc_1: (95.02%) (32962/34688)\n",
      "Epoch: 154 | Batch_idx: 280 |  Loss_1: (0.1356) | Acc_1: (95.00%) (34169/35968)\n",
      "Epoch: 154 | Batch_idx: 290 |  Loss_1: (0.1366) | Acc_1: (94.94%) (35365/37248)\n",
      "Epoch: 154 | Batch_idx: 300 |  Loss_1: (0.1376) | Acc_1: (94.93%) (36576/38528)\n",
      "Epoch: 154 | Batch_idx: 310 |  Loss_1: (0.1378) | Acc_1: (94.94%) (37792/39808)\n",
      "Epoch: 154 | Batch_idx: 320 |  Loss_1: (0.1376) | Acc_1: (94.95%) (39012/41088)\n",
      "Epoch: 154 | Batch_idx: 330 |  Loss_1: (0.1384) | Acc_1: (94.91%) (40213/42368)\n",
      "Epoch: 154 | Batch_idx: 340 |  Loss_1: (0.1389) | Acc_1: (94.88%) (41415/43648)\n",
      "Epoch: 154 | Batch_idx: 350 |  Loss_1: (0.1389) | Acc_1: (94.89%) (42633/44928)\n",
      "Epoch: 154 | Batch_idx: 360 |  Loss_1: (0.1399) | Acc_1: (94.84%) (43823/46208)\n",
      "Epoch: 154 | Batch_idx: 370 |  Loss_1: (0.1411) | Acc_1: (94.80%) (45020/47488)\n",
      "Epoch: 154 | Batch_idx: 380 |  Loss_1: (0.1410) | Acc_1: (94.80%) (46231/48768)\n",
      "Epoch: 154 | Batch_idx: 390 |  Loss_1: (0.1414) | Acc_1: (94.79%) (47394/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3076) | Acc: (92.29%) (9229/10000)\n",
      "Epoch: 155 | Batch_idx: 0 |  Loss_1: (0.0089) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 155 | Batch_idx: 10 |  Loss_1: (0.1096) | Acc_1: (96.38%) (1357/1408)\n",
      "Epoch: 155 | Batch_idx: 20 |  Loss_1: (0.1159) | Acc_1: (95.98%) (2580/2688)\n",
      "Epoch: 155 | Batch_idx: 30 |  Loss_1: (0.1270) | Acc_1: (95.51%) (3790/3968)\n",
      "Epoch: 155 | Batch_idx: 40 |  Loss_1: (0.1316) | Acc_1: (95.22%) (4997/5248)\n",
      "Epoch: 155 | Batch_idx: 50 |  Loss_1: (0.1309) | Acc_1: (95.27%) (6219/6528)\n",
      "Epoch: 155 | Batch_idx: 60 |  Loss_1: (0.1340) | Acc_1: (95.24%) (7436/7808)\n",
      "Epoch: 155 | Batch_idx: 70 |  Loss_1: (0.1317) | Acc_1: (95.28%) (8659/9088)\n",
      "Epoch: 155 | Batch_idx: 80 |  Loss_1: (0.1353) | Acc_1: (95.13%) (9863/10368)\n",
      "Epoch: 155 | Batch_idx: 90 |  Loss_1: (0.1349) | Acc_1: (95.14%) (11082/11648)\n",
      "Epoch: 155 | Batch_idx: 100 |  Loss_1: (0.1361) | Acc_1: (95.06%) (12289/12928)\n",
      "Epoch: 155 | Batch_idx: 110 |  Loss_1: (0.1353) | Acc_1: (95.10%) (13512/14208)\n",
      "Epoch: 155 | Batch_idx: 120 |  Loss_1: (0.1337) | Acc_1: (95.13%) (14734/15488)\n",
      "Epoch: 155 | Batch_idx: 130 |  Loss_1: (0.1347) | Acc_1: (95.09%) (15944/16768)\n",
      "Epoch: 155 | Batch_idx: 140 |  Loss_1: (0.1362) | Acc_1: (95.03%) (17151/18048)\n",
      "Epoch: 155 | Batch_idx: 150 |  Loss_1: (0.1365) | Acc_1: (95.04%) (18370/19328)\n",
      "Epoch: 155 | Batch_idx: 160 |  Loss_1: (0.1354) | Acc_1: (95.07%) (19593/20608)\n",
      "Epoch: 155 | Batch_idx: 170 |  Loss_1: (0.1365) | Acc_1: (95.05%) (20804/21888)\n",
      "Epoch: 155 | Batch_idx: 180 |  Loss_1: (0.1370) | Acc_1: (95.04%) (22019/23168)\n",
      "Epoch: 155 | Batch_idx: 190 |  Loss_1: (0.1384) | Acc_1: (94.99%) (23224/24448)\n",
      "Epoch: 155 | Batch_idx: 200 |  Loss_1: (0.1381) | Acc_1: (95.01%) (24443/25728)\n",
      "Epoch: 155 | Batch_idx: 210 |  Loss_1: (0.1390) | Acc_1: (94.99%) (25656/27008)\n",
      "Epoch: 155 | Batch_idx: 220 |  Loss_1: (0.1388) | Acc_1: (95.00%) (26874/28288)\n",
      "Epoch: 155 | Batch_idx: 230 |  Loss_1: (0.1384) | Acc_1: (95.02%) (28095/29568)\n",
      "Epoch: 155 | Batch_idx: 240 |  Loss_1: (0.1386) | Acc_1: (95.03%) (29315/30848)\n",
      "Epoch: 155 | Batch_idx: 250 |  Loss_1: (0.1390) | Acc_1: (95.02%) (30529/32128)\n",
      "Epoch: 155 | Batch_idx: 260 |  Loss_1: (0.1393) | Acc_1: (95.01%) (31740/33408)\n",
      "Epoch: 155 | Batch_idx: 270 |  Loss_1: (0.1388) | Acc_1: (95.02%) (32961/34688)\n",
      "Epoch: 155 | Batch_idx: 280 |  Loss_1: (0.1398) | Acc_1: (94.98%) (34161/35968)\n",
      "Epoch: 155 | Batch_idx: 290 |  Loss_1: (0.1401) | Acc_1: (94.96%) (35369/37248)\n",
      "Epoch: 155 | Batch_idx: 300 |  Loss_1: (0.1406) | Acc_1: (94.93%) (36573/38528)\n",
      "Epoch: 155 | Batch_idx: 310 |  Loss_1: (0.1412) | Acc_1: (94.88%) (37771/39808)\n",
      "Epoch: 155 | Batch_idx: 320 |  Loss_1: (0.1413) | Acc_1: (94.87%) (38980/41088)\n",
      "Epoch: 155 | Batch_idx: 330 |  Loss_1: (0.1418) | Acc_1: (94.86%) (40191/42368)\n",
      "Epoch: 155 | Batch_idx: 340 |  Loss_1: (0.1412) | Acc_1: (94.88%) (41412/43648)\n",
      "Epoch: 155 | Batch_idx: 350 |  Loss_1: (0.1416) | Acc_1: (94.86%) (42617/44928)\n",
      "Epoch: 155 | Batch_idx: 360 |  Loss_1: (0.1414) | Acc_1: (94.86%) (43831/46208)\n",
      "Epoch: 155 | Batch_idx: 370 |  Loss_1: (0.1420) | Acc_1: (94.82%) (45030/47488)\n",
      "Epoch: 155 | Batch_idx: 380 |  Loss_1: (0.1417) | Acc_1: (94.83%) (46249/48768)\n",
      "Epoch: 155 | Batch_idx: 390 |  Loss_1: (0.1417) | Acc_1: (94.83%) (47416/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3249) | Acc: (92.08%) (9208/10000)\n",
      "Epoch: 156 | Batch_idx: 0 |  Loss_1: (0.1851) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 156 | Batch_idx: 10 |  Loss_1: (0.1579) | Acc_1: (94.03%) (1324/1408)\n",
      "Epoch: 156 | Batch_idx: 20 |  Loss_1: (0.1540) | Acc_1: (94.05%) (2528/2688)\n",
      "Epoch: 156 | Batch_idx: 30 |  Loss_1: (0.1486) | Acc_1: (94.43%) (3747/3968)\n",
      "Epoch: 156 | Batch_idx: 40 |  Loss_1: (0.1469) | Acc_1: (94.47%) (4958/5248)\n",
      "Epoch: 156 | Batch_idx: 50 |  Loss_1: (0.1477) | Acc_1: (94.44%) (6165/6528)\n",
      "Epoch: 156 | Batch_idx: 60 |  Loss_1: (0.1492) | Acc_1: (94.35%) (7367/7808)\n",
      "Epoch: 156 | Batch_idx: 70 |  Loss_1: (0.1477) | Acc_1: (94.37%) (8576/9088)\n",
      "Epoch: 156 | Batch_idx: 80 |  Loss_1: (0.1445) | Acc_1: (94.50%) (9798/10368)\n",
      "Epoch: 156 | Batch_idx: 90 |  Loss_1: (0.1454) | Acc_1: (94.45%) (11001/11648)\n",
      "Epoch: 156 | Batch_idx: 100 |  Loss_1: (0.1447) | Acc_1: (94.45%) (12210/12928)\n",
      "Epoch: 156 | Batch_idx: 110 |  Loss_1: (0.1410) | Acc_1: (94.57%) (13437/14208)\n",
      "Epoch: 156 | Batch_idx: 120 |  Loss_1: (0.1404) | Acc_1: (94.63%) (14656/15488)\n",
      "Epoch: 156 | Batch_idx: 130 |  Loss_1: (0.1429) | Acc_1: (94.54%) (15853/16768)\n",
      "Epoch: 156 | Batch_idx: 140 |  Loss_1: (0.1410) | Acc_1: (94.63%) (17078/18048)\n",
      "Epoch: 156 | Batch_idx: 150 |  Loss_1: (0.1393) | Acc_1: (94.68%) (18300/19328)\n",
      "Epoch: 156 | Batch_idx: 160 |  Loss_1: (0.1376) | Acc_1: (94.74%) (19525/20608)\n",
      "Epoch: 156 | Batch_idx: 170 |  Loss_1: (0.1385) | Acc_1: (94.73%) (20734/21888)\n",
      "Epoch: 156 | Batch_idx: 180 |  Loss_1: (0.1381) | Acc_1: (94.75%) (21952/23168)\n",
      "Epoch: 156 | Batch_idx: 190 |  Loss_1: (0.1386) | Acc_1: (94.74%) (23162/24448)\n",
      "Epoch: 156 | Batch_idx: 200 |  Loss_1: (0.1398) | Acc_1: (94.71%) (24367/25728)\n",
      "Epoch: 156 | Batch_idx: 210 |  Loss_1: (0.1395) | Acc_1: (94.72%) (25582/27008)\n",
      "Epoch: 156 | Batch_idx: 220 |  Loss_1: (0.1396) | Acc_1: (94.73%) (26797/28288)\n",
      "Epoch: 156 | Batch_idx: 230 |  Loss_1: (0.1405) | Acc_1: (94.69%) (27998/29568)\n",
      "Epoch: 156 | Batch_idx: 240 |  Loss_1: (0.1406) | Acc_1: (94.68%) (29207/30848)\n",
      "Epoch: 156 | Batch_idx: 250 |  Loss_1: (0.1399) | Acc_1: (94.73%) (30435/32128)\n",
      "Epoch: 156 | Batch_idx: 260 |  Loss_1: (0.1398) | Acc_1: (94.74%) (31650/33408)\n",
      "Epoch: 156 | Batch_idx: 270 |  Loss_1: (0.1406) | Acc_1: (94.71%) (32852/34688)\n",
      "Epoch: 156 | Batch_idx: 280 |  Loss_1: (0.1412) | Acc_1: (94.68%) (34053/35968)\n",
      "Epoch: 156 | Batch_idx: 290 |  Loss_1: (0.1416) | Acc_1: (94.68%) (35266/37248)\n",
      "Epoch: 156 | Batch_idx: 300 |  Loss_1: (0.1419) | Acc_1: (94.68%) (36478/38528)\n",
      "Epoch: 156 | Batch_idx: 310 |  Loss_1: (0.1416) | Acc_1: (94.68%) (37692/39808)\n",
      "Epoch: 156 | Batch_idx: 320 |  Loss_1: (0.1423) | Acc_1: (94.66%) (38894/41088)\n",
      "Epoch: 156 | Batch_idx: 330 |  Loss_1: (0.1435) | Acc_1: (94.63%) (40092/42368)\n",
      "Epoch: 156 | Batch_idx: 340 |  Loss_1: (0.1443) | Acc_1: (94.63%) (41302/43648)\n",
      "Epoch: 156 | Batch_idx: 350 |  Loss_1: (0.1439) | Acc_1: (94.64%) (42518/44928)\n",
      "Epoch: 156 | Batch_idx: 360 |  Loss_1: (0.1443) | Acc_1: (94.63%) (43725/46208)\n",
      "Epoch: 156 | Batch_idx: 370 |  Loss_1: (0.1436) | Acc_1: (94.66%) (44954/47488)\n",
      "Epoch: 156 | Batch_idx: 380 |  Loss_1: (0.1446) | Acc_1: (94.63%) (46150/48768)\n",
      "Epoch: 156 | Batch_idx: 390 |  Loss_1: (0.1445) | Acc_1: (94.64%) (47319/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3235) | Acc: (92.34%) (9234/10000)\n",
      "Epoch: 157 | Batch_idx: 0 |  Loss_1: (0.1493) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 157 | Batch_idx: 10 |  Loss_1: (0.1434) | Acc_1: (94.89%) (1336/1408)\n",
      "Epoch: 157 | Batch_idx: 20 |  Loss_1: (0.1419) | Acc_1: (94.79%) (2548/2688)\n",
      "Epoch: 157 | Batch_idx: 30 |  Loss_1: (0.1504) | Acc_1: (94.53%) (3751/3968)\n",
      "Epoch: 157 | Batch_idx: 40 |  Loss_1: (0.1445) | Acc_1: (94.78%) (4974/5248)\n",
      "Epoch: 157 | Batch_idx: 50 |  Loss_1: (0.1373) | Acc_1: (95.04%) (6204/6528)\n",
      "Epoch: 157 | Batch_idx: 60 |  Loss_1: (0.1430) | Acc_1: (94.83%) (7404/7808)\n",
      "Epoch: 157 | Batch_idx: 70 |  Loss_1: (0.1433) | Acc_1: (94.83%) (8618/9088)\n",
      "Epoch: 157 | Batch_idx: 80 |  Loss_1: (0.1441) | Acc_1: (94.87%) (9836/10368)\n",
      "Epoch: 157 | Batch_idx: 90 |  Loss_1: (0.1449) | Acc_1: (94.83%) (11046/11648)\n",
      "Epoch: 157 | Batch_idx: 100 |  Loss_1: (0.1462) | Acc_1: (94.79%) (12254/12928)\n",
      "Epoch: 157 | Batch_idx: 110 |  Loss_1: (0.1463) | Acc_1: (94.76%) (13464/14208)\n",
      "Epoch: 157 | Batch_idx: 120 |  Loss_1: (0.1446) | Acc_1: (94.78%) (14679/15488)\n",
      "Epoch: 157 | Batch_idx: 130 |  Loss_1: (0.1433) | Acc_1: (94.82%) (15899/16768)\n",
      "Epoch: 157 | Batch_idx: 140 |  Loss_1: (0.1453) | Acc_1: (94.74%) (17098/18048)\n",
      "Epoch: 157 | Batch_idx: 150 |  Loss_1: (0.1459) | Acc_1: (94.70%) (18303/19328)\n",
      "Epoch: 157 | Batch_idx: 160 |  Loss_1: (0.1439) | Acc_1: (94.79%) (19535/20608)\n",
      "Epoch: 157 | Batch_idx: 170 |  Loss_1: (0.1418) | Acc_1: (94.86%) (20763/21888)\n",
      "Epoch: 157 | Batch_idx: 180 |  Loss_1: (0.1427) | Acc_1: (94.84%) (21972/23168)\n",
      "Epoch: 157 | Batch_idx: 190 |  Loss_1: (0.1446) | Acc_1: (94.73%) (23160/24448)\n",
      "Epoch: 157 | Batch_idx: 200 |  Loss_1: (0.1454) | Acc_1: (94.69%) (24361/25728)\n",
      "Epoch: 157 | Batch_idx: 210 |  Loss_1: (0.1435) | Acc_1: (94.75%) (25591/27008)\n",
      "Epoch: 157 | Batch_idx: 220 |  Loss_1: (0.1432) | Acc_1: (94.75%) (26803/28288)\n",
      "Epoch: 157 | Batch_idx: 230 |  Loss_1: (0.1421) | Acc_1: (94.78%) (28025/29568)\n",
      "Epoch: 157 | Batch_idx: 240 |  Loss_1: (0.1415) | Acc_1: (94.81%) (29246/30848)\n",
      "Epoch: 157 | Batch_idx: 250 |  Loss_1: (0.1415) | Acc_1: (94.81%) (30461/32128)\n",
      "Epoch: 157 | Batch_idx: 260 |  Loss_1: (0.1415) | Acc_1: (94.82%) (31679/33408)\n",
      "Epoch: 157 | Batch_idx: 270 |  Loss_1: (0.1412) | Acc_1: (94.83%) (32895/34688)\n",
      "Epoch: 157 | Batch_idx: 280 |  Loss_1: (0.1412) | Acc_1: (94.83%) (34109/35968)\n",
      "Epoch: 157 | Batch_idx: 290 |  Loss_1: (0.1415) | Acc_1: (94.79%) (35309/37248)\n",
      "Epoch: 157 | Batch_idx: 300 |  Loss_1: (0.1413) | Acc_1: (94.80%) (36524/38528)\n",
      "Epoch: 157 | Batch_idx: 310 |  Loss_1: (0.1417) | Acc_1: (94.81%) (37741/39808)\n",
      "Epoch: 157 | Batch_idx: 320 |  Loss_1: (0.1415) | Acc_1: (94.82%) (38958/41088)\n",
      "Epoch: 157 | Batch_idx: 330 |  Loss_1: (0.1412) | Acc_1: (94.81%) (40168/42368)\n",
      "Epoch: 157 | Batch_idx: 340 |  Loss_1: (0.1416) | Acc_1: (94.80%) (41379/43648)\n",
      "Epoch: 157 | Batch_idx: 350 |  Loss_1: (0.1414) | Acc_1: (94.79%) (42589/44928)\n",
      "Epoch: 157 | Batch_idx: 360 |  Loss_1: (0.1419) | Acc_1: (94.78%) (43796/46208)\n",
      "Epoch: 157 | Batch_idx: 370 |  Loss_1: (0.1417) | Acc_1: (94.78%) (45007/47488)\n",
      "Epoch: 157 | Batch_idx: 380 |  Loss_1: (0.1418) | Acc_1: (94.78%) (46222/48768)\n",
      "Epoch: 157 | Batch_idx: 390 |  Loss_1: (0.1419) | Acc_1: (94.78%) (47391/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3307) | Acc: (92.00%) (9200/10000)\n",
      "Epoch: 158 | Batch_idx: 0 |  Loss_1: (0.1925) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 158 | Batch_idx: 10 |  Loss_1: (0.1329) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 158 | Batch_idx: 20 |  Loss_1: (0.1394) | Acc_1: (94.61%) (2543/2688)\n",
      "Epoch: 158 | Batch_idx: 30 |  Loss_1: (0.1371) | Acc_1: (94.86%) (3764/3968)\n",
      "Epoch: 158 | Batch_idx: 40 |  Loss_1: (0.1349) | Acc_1: (94.93%) (4982/5248)\n",
      "Epoch: 158 | Batch_idx: 50 |  Loss_1: (0.1348) | Acc_1: (94.90%) (6195/6528)\n",
      "Epoch: 158 | Batch_idx: 60 |  Loss_1: (0.1343) | Acc_1: (94.90%) (7410/7808)\n",
      "Epoch: 158 | Batch_idx: 70 |  Loss_1: (0.1335) | Acc_1: (94.95%) (8629/9088)\n",
      "Epoch: 158 | Batch_idx: 80 |  Loss_1: (0.1338) | Acc_1: (94.96%) (9845/10368)\n",
      "Epoch: 158 | Batch_idx: 90 |  Loss_1: (0.1373) | Acc_1: (94.85%) (11048/11648)\n",
      "Epoch: 158 | Batch_idx: 100 |  Loss_1: (0.1383) | Acc_1: (94.80%) (12256/12928)\n",
      "Epoch: 158 | Batch_idx: 110 |  Loss_1: (0.1391) | Acc_1: (94.78%) (13466/14208)\n",
      "Epoch: 158 | Batch_idx: 120 |  Loss_1: (0.1383) | Acc_1: (94.85%) (14690/15488)\n",
      "Epoch: 158 | Batch_idx: 130 |  Loss_1: (0.1378) | Acc_1: (94.89%) (15911/16768)\n",
      "Epoch: 158 | Batch_idx: 140 |  Loss_1: (0.1379) | Acc_1: (94.90%) (17128/18048)\n",
      "Epoch: 158 | Batch_idx: 150 |  Loss_1: (0.1368) | Acc_1: (94.96%) (18353/19328)\n",
      "Epoch: 158 | Batch_idx: 160 |  Loss_1: (0.1376) | Acc_1: (94.89%) (19554/20608)\n",
      "Epoch: 158 | Batch_idx: 170 |  Loss_1: (0.1391) | Acc_1: (94.81%) (20752/21888)\n",
      "Epoch: 158 | Batch_idx: 180 |  Loss_1: (0.1391) | Acc_1: (94.81%) (21966/23168)\n",
      "Epoch: 158 | Batch_idx: 190 |  Loss_1: (0.1388) | Acc_1: (94.82%) (23182/24448)\n",
      "Epoch: 158 | Batch_idx: 200 |  Loss_1: (0.1389) | Acc_1: (94.80%) (24391/25728)\n",
      "Epoch: 158 | Batch_idx: 210 |  Loss_1: (0.1394) | Acc_1: (94.80%) (25603/27008)\n",
      "Epoch: 158 | Batch_idx: 220 |  Loss_1: (0.1403) | Acc_1: (94.76%) (26806/28288)\n",
      "Epoch: 158 | Batch_idx: 230 |  Loss_1: (0.1399) | Acc_1: (94.77%) (28021/29568)\n",
      "Epoch: 158 | Batch_idx: 240 |  Loss_1: (0.1403) | Acc_1: (94.75%) (29229/30848)\n",
      "Epoch: 158 | Batch_idx: 250 |  Loss_1: (0.1396) | Acc_1: (94.75%) (30442/32128)\n",
      "Epoch: 158 | Batch_idx: 260 |  Loss_1: (0.1390) | Acc_1: (94.78%) (31663/33408)\n",
      "Epoch: 158 | Batch_idx: 270 |  Loss_1: (0.1393) | Acc_1: (94.76%) (32869/34688)\n",
      "Epoch: 158 | Batch_idx: 280 |  Loss_1: (0.1395) | Acc_1: (94.75%) (34078/35968)\n",
      "Epoch: 158 | Batch_idx: 290 |  Loss_1: (0.1397) | Acc_1: (94.75%) (35292/37248)\n",
      "Epoch: 158 | Batch_idx: 300 |  Loss_1: (0.1400) | Acc_1: (94.75%) (36504/38528)\n",
      "Epoch: 158 | Batch_idx: 310 |  Loss_1: (0.1397) | Acc_1: (94.76%) (37724/39808)\n",
      "Epoch: 158 | Batch_idx: 320 |  Loss_1: (0.1401) | Acc_1: (94.76%) (38933/41088)\n",
      "Epoch: 158 | Batch_idx: 330 |  Loss_1: (0.1396) | Acc_1: (94.76%) (40150/42368)\n",
      "Epoch: 158 | Batch_idx: 340 |  Loss_1: (0.1388) | Acc_1: (94.79%) (41376/43648)\n",
      "Epoch: 158 | Batch_idx: 350 |  Loss_1: (0.1385) | Acc_1: (94.81%) (42595/44928)\n",
      "Epoch: 158 | Batch_idx: 360 |  Loss_1: (0.1382) | Acc_1: (94.81%) (43812/46208)\n",
      "Epoch: 158 | Batch_idx: 370 |  Loss_1: (0.1377) | Acc_1: (94.83%) (45035/47488)\n",
      "Epoch: 158 | Batch_idx: 380 |  Loss_1: (0.1377) | Acc_1: (94.83%) (46248/48768)\n",
      "Epoch: 158 | Batch_idx: 390 |  Loss_1: (0.1377) | Acc_1: (94.83%) (47413/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3570) | Acc: (91.88%) (9188/10000)\n",
      "Epoch: 159 | Batch_idx: 0 |  Loss_1: (0.1289) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 159 | Batch_idx: 10 |  Loss_1: (0.1337) | Acc_1: (95.45%) (1344/1408)\n",
      "Epoch: 159 | Batch_idx: 20 |  Loss_1: (0.1417) | Acc_1: (94.94%) (2552/2688)\n",
      "Epoch: 159 | Batch_idx: 30 |  Loss_1: (0.1440) | Acc_1: (94.86%) (3764/3968)\n",
      "Epoch: 159 | Batch_idx: 40 |  Loss_1: (0.1479) | Acc_1: (94.61%) (4965/5248)\n",
      "Epoch: 159 | Batch_idx: 50 |  Loss_1: (0.1444) | Acc_1: (94.70%) (6182/6528)\n",
      "Epoch: 159 | Batch_idx: 60 |  Loss_1: (0.1488) | Acc_1: (94.56%) (7383/7808)\n",
      "Epoch: 159 | Batch_idx: 70 |  Loss_1: (0.1497) | Acc_1: (94.51%) (8589/9088)\n",
      "Epoch: 159 | Batch_idx: 80 |  Loss_1: (0.1492) | Acc_1: (94.48%) (9796/10368)\n",
      "Epoch: 159 | Batch_idx: 90 |  Loss_1: (0.1495) | Acc_1: (94.47%) (11004/11648)\n",
      "Epoch: 159 | Batch_idx: 100 |  Loss_1: (0.1486) | Acc_1: (94.53%) (12221/12928)\n",
      "Epoch: 159 | Batch_idx: 110 |  Loss_1: (0.1476) | Acc_1: (94.55%) (13434/14208)\n",
      "Epoch: 159 | Batch_idx: 120 |  Loss_1: (0.1469) | Acc_1: (94.57%) (14647/15488)\n",
      "Epoch: 159 | Batch_idx: 130 |  Loss_1: (0.1480) | Acc_1: (94.55%) (15854/16768)\n",
      "Epoch: 159 | Batch_idx: 140 |  Loss_1: (0.1474) | Acc_1: (94.58%) (17069/18048)\n",
      "Epoch: 159 | Batch_idx: 150 |  Loss_1: (0.1491) | Acc_1: (94.51%) (18267/19328)\n",
      "Epoch: 159 | Batch_idx: 160 |  Loss_1: (0.1489) | Acc_1: (94.51%) (19477/20608)\n",
      "Epoch: 159 | Batch_idx: 170 |  Loss_1: (0.1490) | Acc_1: (94.49%) (20682/21888)\n",
      "Epoch: 159 | Batch_idx: 180 |  Loss_1: (0.1478) | Acc_1: (94.54%) (21902/23168)\n",
      "Epoch: 159 | Batch_idx: 190 |  Loss_1: (0.1475) | Acc_1: (94.54%) (23112/24448)\n",
      "Epoch: 159 | Batch_idx: 200 |  Loss_1: (0.1474) | Acc_1: (94.53%) (24320/25728)\n",
      "Epoch: 159 | Batch_idx: 210 |  Loss_1: (0.1467) | Acc_1: (94.54%) (25533/27008)\n",
      "Epoch: 159 | Batch_idx: 220 |  Loss_1: (0.1460) | Acc_1: (94.56%) (26750/28288)\n",
      "Epoch: 159 | Batch_idx: 230 |  Loss_1: (0.1462) | Acc_1: (94.57%) (27961/29568)\n",
      "Epoch: 159 | Batch_idx: 240 |  Loss_1: (0.1460) | Acc_1: (94.56%) (29171/30848)\n",
      "Epoch: 159 | Batch_idx: 250 |  Loss_1: (0.1454) | Acc_1: (94.59%) (30389/32128)\n",
      "Epoch: 159 | Batch_idx: 260 |  Loss_1: (0.1445) | Acc_1: (94.62%) (31609/33408)\n",
      "Epoch: 159 | Batch_idx: 270 |  Loss_1: (0.1435) | Acc_1: (94.66%) (32834/34688)\n",
      "Epoch: 159 | Batch_idx: 280 |  Loss_1: (0.1430) | Acc_1: (94.69%) (34058/35968)\n",
      "Epoch: 159 | Batch_idx: 290 |  Loss_1: (0.1425) | Acc_1: (94.70%) (35275/37248)\n",
      "Epoch: 159 | Batch_idx: 300 |  Loss_1: (0.1418) | Acc_1: (94.72%) (36495/38528)\n",
      "Epoch: 159 | Batch_idx: 310 |  Loss_1: (0.1416) | Acc_1: (94.75%) (37719/39808)\n",
      "Epoch: 159 | Batch_idx: 320 |  Loss_1: (0.1411) | Acc_1: (94.77%) (38940/41088)\n",
      "Epoch: 159 | Batch_idx: 330 |  Loss_1: (0.1419) | Acc_1: (94.74%) (40139/42368)\n",
      "Epoch: 159 | Batch_idx: 340 |  Loss_1: (0.1416) | Acc_1: (94.74%) (41353/43648)\n",
      "Epoch: 159 | Batch_idx: 350 |  Loss_1: (0.1418) | Acc_1: (94.72%) (42555/44928)\n",
      "Epoch: 159 | Batch_idx: 360 |  Loss_1: (0.1428) | Acc_1: (94.68%) (43752/46208)\n",
      "Epoch: 159 | Batch_idx: 370 |  Loss_1: (0.1431) | Acc_1: (94.68%) (44961/47488)\n",
      "Epoch: 159 | Batch_idx: 380 |  Loss_1: (0.1433) | Acc_1: (94.67%) (46171/48768)\n",
      "Epoch: 159 | Batch_idx: 390 |  Loss_1: (0.1433) | Acc_1: (94.67%) (47335/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3430) | Acc: (91.72%) (9172/10000)\n",
      "Epoch: 160 | Batch_idx: 0 |  Loss_1: (0.1466) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 160 | Batch_idx: 10 |  Loss_1: (0.1453) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 160 | Batch_idx: 20 |  Loss_1: (0.1311) | Acc_1: (95.16%) (2558/2688)\n",
      "Epoch: 160 | Batch_idx: 30 |  Loss_1: (0.1392) | Acc_1: (94.88%) (3765/3968)\n",
      "Epoch: 160 | Batch_idx: 40 |  Loss_1: (0.1390) | Acc_1: (94.84%) (4977/5248)\n",
      "Epoch: 160 | Batch_idx: 50 |  Loss_1: (0.1392) | Acc_1: (94.75%) (6185/6528)\n",
      "Epoch: 160 | Batch_idx: 60 |  Loss_1: (0.1390) | Acc_1: (94.71%) (7395/7808)\n",
      "Epoch: 160 | Batch_idx: 70 |  Loss_1: (0.1361) | Acc_1: (94.84%) (8619/9088)\n",
      "Epoch: 160 | Batch_idx: 80 |  Loss_1: (0.1339) | Acc_1: (94.97%) (9847/10368)\n",
      "Epoch: 160 | Batch_idx: 90 |  Loss_1: (0.1340) | Acc_1: (94.96%) (11061/11648)\n",
      "Epoch: 160 | Batch_idx: 100 |  Loss_1: (0.1323) | Acc_1: (95.03%) (12286/12928)\n",
      "Epoch: 160 | Batch_idx: 110 |  Loss_1: (0.1336) | Acc_1: (94.99%) (13496/14208)\n",
      "Epoch: 160 | Batch_idx: 120 |  Loss_1: (0.1330) | Acc_1: (94.98%) (14710/15488)\n",
      "Epoch: 160 | Batch_idx: 130 |  Loss_1: (0.1330) | Acc_1: (94.98%) (15927/16768)\n",
      "Epoch: 160 | Batch_idx: 140 |  Loss_1: (0.1320) | Acc_1: (95.01%) (17147/18048)\n",
      "Epoch: 160 | Batch_idx: 150 |  Loss_1: (0.1336) | Acc_1: (94.95%) (18352/19328)\n",
      "Epoch: 160 | Batch_idx: 160 |  Loss_1: (0.1354) | Acc_1: (94.87%) (19550/20608)\n",
      "Epoch: 160 | Batch_idx: 170 |  Loss_1: (0.1337) | Acc_1: (94.93%) (20779/21888)\n",
      "Epoch: 160 | Batch_idx: 180 |  Loss_1: (0.1327) | Acc_1: (94.96%) (22001/23168)\n",
      "Epoch: 160 | Batch_idx: 190 |  Loss_1: (0.1333) | Acc_1: (94.95%) (23213/24448)\n",
      "Epoch: 160 | Batch_idx: 200 |  Loss_1: (0.1351) | Acc_1: (94.90%) (24417/25728)\n",
      "Epoch: 160 | Batch_idx: 210 |  Loss_1: (0.1355) | Acc_1: (94.90%) (25630/27008)\n",
      "Epoch: 160 | Batch_idx: 220 |  Loss_1: (0.1357) | Acc_1: (94.91%) (26848/28288)\n",
      "Epoch: 160 | Batch_idx: 230 |  Loss_1: (0.1364) | Acc_1: (94.88%) (28055/29568)\n",
      "Epoch: 160 | Batch_idx: 240 |  Loss_1: (0.1379) | Acc_1: (94.82%) (29251/30848)\n",
      "Epoch: 160 | Batch_idx: 250 |  Loss_1: (0.1379) | Acc_1: (94.82%) (30465/32128)\n",
      "Epoch: 160 | Batch_idx: 260 |  Loss_1: (0.1379) | Acc_1: (94.82%) (31676/33408)\n",
      "Epoch: 160 | Batch_idx: 270 |  Loss_1: (0.1380) | Acc_1: (94.82%) (32890/34688)\n",
      "Epoch: 160 | Batch_idx: 280 |  Loss_1: (0.1377) | Acc_1: (94.83%) (34110/35968)\n",
      "Epoch: 160 | Batch_idx: 290 |  Loss_1: (0.1380) | Acc_1: (94.82%) (35319/37248)\n",
      "Epoch: 160 | Batch_idx: 300 |  Loss_1: (0.1374) | Acc_1: (94.84%) (36539/38528)\n",
      "Epoch: 160 | Batch_idx: 310 |  Loss_1: (0.1372) | Acc_1: (94.85%) (37759/39808)\n",
      "Epoch: 160 | Batch_idx: 320 |  Loss_1: (0.1367) | Acc_1: (94.88%) (38985/41088)\n",
      "Epoch: 160 | Batch_idx: 330 |  Loss_1: (0.1369) | Acc_1: (94.88%) (40199/42368)\n",
      "Epoch: 160 | Batch_idx: 340 |  Loss_1: (0.1374) | Acc_1: (94.86%) (41404/43648)\n",
      "Epoch: 160 | Batch_idx: 350 |  Loss_1: (0.1367) | Acc_1: (94.89%) (42633/44928)\n",
      "Epoch: 160 | Batch_idx: 360 |  Loss_1: (0.1370) | Acc_1: (94.89%) (43849/46208)\n",
      "Epoch: 160 | Batch_idx: 370 |  Loss_1: (0.1375) | Acc_1: (94.87%) (45050/47488)\n",
      "Epoch: 160 | Batch_idx: 380 |  Loss_1: (0.1382) | Acc_1: (94.85%) (46258/48768)\n",
      "Epoch: 160 | Batch_idx: 390 |  Loss_1: (0.1391) | Acc_1: (94.82%) (47412/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3345) | Acc: (92.50%) (9250/10000)\n",
      "Epoch: 161 | Batch_idx: 0 |  Loss_1: (0.0992) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 161 | Batch_idx: 10 |  Loss_1: (0.1308) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 161 | Batch_idx: 20 |  Loss_1: (0.1467) | Acc_1: (94.12%) (2530/2688)\n",
      "Epoch: 161 | Batch_idx: 30 |  Loss_1: (0.1457) | Acc_1: (94.25%) (3740/3968)\n",
      "Epoch: 161 | Batch_idx: 40 |  Loss_1: (0.1449) | Acc_1: (94.38%) (4953/5248)\n",
      "Epoch: 161 | Batch_idx: 50 |  Loss_1: (0.1421) | Acc_1: (94.55%) (6172/6528)\n",
      "Epoch: 161 | Batch_idx: 60 |  Loss_1: (0.1399) | Acc_1: (94.71%) (7395/7808)\n",
      "Epoch: 161 | Batch_idx: 70 |  Loss_1: (0.1410) | Acc_1: (94.65%) (8602/9088)\n",
      "Epoch: 161 | Batch_idx: 80 |  Loss_1: (0.1421) | Acc_1: (94.60%) (9808/10368)\n",
      "Epoch: 161 | Batch_idx: 90 |  Loss_1: (0.1430) | Acc_1: (94.57%) (11015/11648)\n",
      "Epoch: 161 | Batch_idx: 100 |  Loss_1: (0.1432) | Acc_1: (94.55%) (12224/12928)\n",
      "Epoch: 161 | Batch_idx: 110 |  Loss_1: (0.1450) | Acc_1: (94.51%) (13428/14208)\n",
      "Epoch: 161 | Batch_idx: 120 |  Loss_1: (0.1453) | Acc_1: (94.53%) (14641/15488)\n",
      "Epoch: 161 | Batch_idx: 130 |  Loss_1: (0.1439) | Acc_1: (94.58%) (15859/16768)\n",
      "Epoch: 161 | Batch_idx: 140 |  Loss_1: (0.1446) | Acc_1: (94.56%) (17067/18048)\n",
      "Epoch: 161 | Batch_idx: 150 |  Loss_1: (0.1437) | Acc_1: (94.63%) (18290/19328)\n",
      "Epoch: 161 | Batch_idx: 160 |  Loss_1: (0.1444) | Acc_1: (94.59%) (19493/20608)\n",
      "Epoch: 161 | Batch_idx: 170 |  Loss_1: (0.1423) | Acc_1: (94.66%) (20720/21888)\n",
      "Epoch: 161 | Batch_idx: 180 |  Loss_1: (0.1409) | Acc_1: (94.72%) (21944/23168)\n",
      "Epoch: 161 | Batch_idx: 190 |  Loss_1: (0.1415) | Acc_1: (94.69%) (23151/24448)\n",
      "Epoch: 161 | Batch_idx: 200 |  Loss_1: (0.1417) | Acc_1: (94.69%) (24362/25728)\n",
      "Epoch: 161 | Batch_idx: 210 |  Loss_1: (0.1420) | Acc_1: (94.68%) (25570/27008)\n",
      "Epoch: 161 | Batch_idx: 220 |  Loss_1: (0.1417) | Acc_1: (94.70%) (26790/28288)\n",
      "Epoch: 161 | Batch_idx: 230 |  Loss_1: (0.1426) | Acc_1: (94.67%) (27993/29568)\n",
      "Epoch: 161 | Batch_idx: 240 |  Loss_1: (0.1433) | Acc_1: (94.62%) (29189/30848)\n",
      "Epoch: 161 | Batch_idx: 250 |  Loss_1: (0.1427) | Acc_1: (94.63%) (30404/32128)\n",
      "Epoch: 161 | Batch_idx: 260 |  Loss_1: (0.1433) | Acc_1: (94.63%) (31614/33408)\n",
      "Epoch: 161 | Batch_idx: 270 |  Loss_1: (0.1435) | Acc_1: (94.63%) (32824/34688)\n",
      "Epoch: 161 | Batch_idx: 280 |  Loss_1: (0.1428) | Acc_1: (94.65%) (34044/35968)\n",
      "Epoch: 161 | Batch_idx: 290 |  Loss_1: (0.1431) | Acc_1: (94.63%) (35246/37248)\n",
      "Epoch: 161 | Batch_idx: 300 |  Loss_1: (0.1431) | Acc_1: (94.61%) (36453/38528)\n",
      "Epoch: 161 | Batch_idx: 310 |  Loss_1: (0.1432) | Acc_1: (94.61%) (37661/39808)\n",
      "Epoch: 161 | Batch_idx: 320 |  Loss_1: (0.1438) | Acc_1: (94.60%) (38868/41088)\n",
      "Epoch: 161 | Batch_idx: 330 |  Loss_1: (0.1440) | Acc_1: (94.59%) (40076/42368)\n",
      "Epoch: 161 | Batch_idx: 340 |  Loss_1: (0.1439) | Acc_1: (94.60%) (41289/43648)\n",
      "Epoch: 161 | Batch_idx: 350 |  Loss_1: (0.1436) | Acc_1: (94.60%) (42503/44928)\n",
      "Epoch: 161 | Batch_idx: 360 |  Loss_1: (0.1436) | Acc_1: (94.60%) (43715/46208)\n",
      "Epoch: 161 | Batch_idx: 370 |  Loss_1: (0.1435) | Acc_1: (94.61%) (44928/47488)\n",
      "Epoch: 161 | Batch_idx: 380 |  Loss_1: (0.1432) | Acc_1: (94.62%) (46143/48768)\n",
      "Epoch: 161 | Batch_idx: 390 |  Loss_1: (0.1424) | Acc_1: (94.64%) (47320/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3408) | Acc: (92.07%) (9207/10000)\n",
      "Epoch: 162 | Batch_idx: 0 |  Loss_1: (0.1587) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 162 | Batch_idx: 10 |  Loss_1: (0.1294) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 162 | Batch_idx: 20 |  Loss_1: (0.1344) | Acc_1: (95.01%) (2554/2688)\n",
      "Epoch: 162 | Batch_idx: 30 |  Loss_1: (0.1402) | Acc_1: (94.71%) (3758/3968)\n",
      "Epoch: 162 | Batch_idx: 40 |  Loss_1: (0.1410) | Acc_1: (94.74%) (4972/5248)\n",
      "Epoch: 162 | Batch_idx: 50 |  Loss_1: (0.1402) | Acc_1: (94.76%) (6186/6528)\n",
      "Epoch: 162 | Batch_idx: 60 |  Loss_1: (0.1418) | Acc_1: (94.76%) (7399/7808)\n",
      "Epoch: 162 | Batch_idx: 70 |  Loss_1: (0.1434) | Acc_1: (94.67%) (8604/9088)\n",
      "Epoch: 162 | Batch_idx: 80 |  Loss_1: (0.1444) | Acc_1: (94.67%) (9815/10368)\n",
      "Epoch: 162 | Batch_idx: 90 |  Loss_1: (0.1435) | Acc_1: (94.73%) (11034/11648)\n",
      "Epoch: 162 | Batch_idx: 100 |  Loss_1: (0.1421) | Acc_1: (94.76%) (12251/12928)\n",
      "Epoch: 162 | Batch_idx: 110 |  Loss_1: (0.1407) | Acc_1: (94.80%) (13469/14208)\n",
      "Epoch: 162 | Batch_idx: 120 |  Loss_1: (0.1412) | Acc_1: (94.80%) (14683/15488)\n",
      "Epoch: 162 | Batch_idx: 130 |  Loss_1: (0.1408) | Acc_1: (94.79%) (15894/16768)\n",
      "Epoch: 162 | Batch_idx: 140 |  Loss_1: (0.1410) | Acc_1: (94.79%) (17107/18048)\n",
      "Epoch: 162 | Batch_idx: 150 |  Loss_1: (0.1413) | Acc_1: (94.78%) (18319/19328)\n",
      "Epoch: 162 | Batch_idx: 160 |  Loss_1: (0.1413) | Acc_1: (94.78%) (19532/20608)\n",
      "Epoch: 162 | Batch_idx: 170 |  Loss_1: (0.1400) | Acc_1: (94.82%) (20754/21888)\n",
      "Epoch: 162 | Batch_idx: 180 |  Loss_1: (0.1396) | Acc_1: (94.83%) (21971/23168)\n",
      "Epoch: 162 | Batch_idx: 190 |  Loss_1: (0.1408) | Acc_1: (94.77%) (23170/24448)\n",
      "Epoch: 162 | Batch_idx: 200 |  Loss_1: (0.1408) | Acc_1: (94.78%) (24386/25728)\n",
      "Epoch: 162 | Batch_idx: 210 |  Loss_1: (0.1418) | Acc_1: (94.76%) (25594/27008)\n",
      "Epoch: 162 | Batch_idx: 220 |  Loss_1: (0.1415) | Acc_1: (94.76%) (26805/28288)\n",
      "Epoch: 162 | Batch_idx: 230 |  Loss_1: (0.1414) | Acc_1: (94.76%) (28018/29568)\n",
      "Epoch: 162 | Batch_idx: 240 |  Loss_1: (0.1417) | Acc_1: (94.75%) (29229/30848)\n",
      "Epoch: 162 | Batch_idx: 250 |  Loss_1: (0.1422) | Acc_1: (94.73%) (30436/32128)\n",
      "Epoch: 162 | Batch_idx: 260 |  Loss_1: (0.1430) | Acc_1: (94.70%) (31638/33408)\n",
      "Epoch: 162 | Batch_idx: 270 |  Loss_1: (0.1439) | Acc_1: (94.68%) (32841/34688)\n",
      "Epoch: 162 | Batch_idx: 280 |  Loss_1: (0.1435) | Acc_1: (94.68%) (34054/35968)\n",
      "Epoch: 162 | Batch_idx: 290 |  Loss_1: (0.1439) | Acc_1: (94.64%) (35253/37248)\n",
      "Epoch: 162 | Batch_idx: 300 |  Loss_1: (0.1443) | Acc_1: (94.62%) (36456/38528)\n",
      "Epoch: 162 | Batch_idx: 310 |  Loss_1: (0.1443) | Acc_1: (94.62%) (37665/39808)\n",
      "Epoch: 162 | Batch_idx: 320 |  Loss_1: (0.1443) | Acc_1: (94.61%) (38872/41088)\n",
      "Epoch: 162 | Batch_idx: 330 |  Loss_1: (0.1440) | Acc_1: (94.62%) (40090/42368)\n",
      "Epoch: 162 | Batch_idx: 340 |  Loss_1: (0.1438) | Acc_1: (94.63%) (41302/43648)\n",
      "Epoch: 162 | Batch_idx: 350 |  Loss_1: (0.1433) | Acc_1: (94.64%) (42522/44928)\n",
      "Epoch: 162 | Batch_idx: 360 |  Loss_1: (0.1429) | Acc_1: (94.67%) (43743/46208)\n",
      "Epoch: 162 | Batch_idx: 370 |  Loss_1: (0.1431) | Acc_1: (94.66%) (44950/47488)\n",
      "Epoch: 162 | Batch_idx: 380 |  Loss_1: (0.1431) | Acc_1: (94.66%) (46165/48768)\n",
      "Epoch: 162 | Batch_idx: 390 |  Loss_1: (0.1432) | Acc_1: (94.66%) (47329/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3506) | Acc: (91.65%) (9165/10000)\n",
      "Epoch: 163 | Batch_idx: 0 |  Loss_1: (0.2111) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 163 | Batch_idx: 10 |  Loss_1: (0.1571) | Acc_1: (94.18%) (1326/1408)\n",
      "Epoch: 163 | Batch_idx: 20 |  Loss_1: (0.1497) | Acc_1: (94.61%) (2543/2688)\n",
      "Epoch: 163 | Batch_idx: 30 |  Loss_1: (0.1418) | Acc_1: (94.88%) (3765/3968)\n",
      "Epoch: 163 | Batch_idx: 40 |  Loss_1: (0.1361) | Acc_1: (95.06%) (4989/5248)\n",
      "Epoch: 163 | Batch_idx: 50 |  Loss_1: (0.1360) | Acc_1: (95.02%) (6203/6528)\n",
      "Epoch: 163 | Batch_idx: 60 |  Loss_1: (0.1342) | Acc_1: (95.06%) (7422/7808)\n",
      "Epoch: 163 | Batch_idx: 70 |  Loss_1: (0.1363) | Acc_1: (94.95%) (8629/9088)\n",
      "Epoch: 163 | Batch_idx: 80 |  Loss_1: (0.1369) | Acc_1: (94.95%) (9844/10368)\n",
      "Epoch: 163 | Batch_idx: 90 |  Loss_1: (0.1401) | Acc_1: (94.79%) (11041/11648)\n",
      "Epoch: 163 | Batch_idx: 100 |  Loss_1: (0.1411) | Acc_1: (94.73%) (12247/12928)\n",
      "Epoch: 163 | Batch_idx: 110 |  Loss_1: (0.1408) | Acc_1: (94.75%) (13462/14208)\n",
      "Epoch: 163 | Batch_idx: 120 |  Loss_1: (0.1391) | Acc_1: (94.80%) (14683/15488)\n",
      "Epoch: 163 | Batch_idx: 130 |  Loss_1: (0.1398) | Acc_1: (94.80%) (15896/16768)\n",
      "Epoch: 163 | Batch_idx: 140 |  Loss_1: (0.1400) | Acc_1: (94.81%) (17111/18048)\n",
      "Epoch: 163 | Batch_idx: 150 |  Loss_1: (0.1405) | Acc_1: (94.78%) (18319/19328)\n",
      "Epoch: 163 | Batch_idx: 160 |  Loss_1: (0.1401) | Acc_1: (94.81%) (19539/20608)\n",
      "Epoch: 163 | Batch_idx: 170 |  Loss_1: (0.1399) | Acc_1: (94.86%) (20762/21888)\n",
      "Epoch: 163 | Batch_idx: 180 |  Loss_1: (0.1390) | Acc_1: (94.86%) (21977/23168)\n",
      "Epoch: 163 | Batch_idx: 190 |  Loss_1: (0.1390) | Acc_1: (94.83%) (23183/24448)\n",
      "Epoch: 163 | Batch_idx: 200 |  Loss_1: (0.1409) | Acc_1: (94.76%) (24381/25728)\n",
      "Epoch: 163 | Batch_idx: 210 |  Loss_1: (0.1396) | Acc_1: (94.81%) (25606/27008)\n",
      "Epoch: 163 | Batch_idx: 220 |  Loss_1: (0.1400) | Acc_1: (94.81%) (26819/28288)\n",
      "Epoch: 163 | Batch_idx: 230 |  Loss_1: (0.1399) | Acc_1: (94.82%) (28036/29568)\n",
      "Epoch: 163 | Batch_idx: 240 |  Loss_1: (0.1395) | Acc_1: (94.85%) (29258/30848)\n",
      "Epoch: 163 | Batch_idx: 250 |  Loss_1: (0.1401) | Acc_1: (94.83%) (30468/32128)\n",
      "Epoch: 163 | Batch_idx: 260 |  Loss_1: (0.1398) | Acc_1: (94.85%) (31686/33408)\n",
      "Epoch: 163 | Batch_idx: 270 |  Loss_1: (0.1396) | Acc_1: (94.85%) (32901/34688)\n",
      "Epoch: 163 | Batch_idx: 280 |  Loss_1: (0.1403) | Acc_1: (94.82%) (34104/35968)\n",
      "Epoch: 163 | Batch_idx: 290 |  Loss_1: (0.1414) | Acc_1: (94.79%) (35308/37248)\n",
      "Epoch: 163 | Batch_idx: 300 |  Loss_1: (0.1414) | Acc_1: (94.80%) (36523/38528)\n",
      "Epoch: 163 | Batch_idx: 310 |  Loss_1: (0.1413) | Acc_1: (94.79%) (37734/39808)\n",
      "Epoch: 163 | Batch_idx: 320 |  Loss_1: (0.1416) | Acc_1: (94.80%) (38950/41088)\n",
      "Epoch: 163 | Batch_idx: 330 |  Loss_1: (0.1423) | Acc_1: (94.77%) (40153/42368)\n",
      "Epoch: 163 | Batch_idx: 340 |  Loss_1: (0.1423) | Acc_1: (94.78%) (41368/43648)\n",
      "Epoch: 163 | Batch_idx: 350 |  Loss_1: (0.1422) | Acc_1: (94.77%) (42579/44928)\n",
      "Epoch: 163 | Batch_idx: 360 |  Loss_1: (0.1421) | Acc_1: (94.77%) (43792/46208)\n",
      "Epoch: 163 | Batch_idx: 370 |  Loss_1: (0.1429) | Acc_1: (94.76%) (45000/47488)\n",
      "Epoch: 163 | Batch_idx: 380 |  Loss_1: (0.1429) | Acc_1: (94.76%) (46211/48768)\n",
      "Epoch: 163 | Batch_idx: 390 |  Loss_1: (0.1423) | Acc_1: (94.77%) (47387/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3087) | Acc: (92.53%) (9253/10000)\n",
      "Epoch: 164 | Batch_idx: 0 |  Loss_1: (0.0963) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 164 | Batch_idx: 10 |  Loss_1: (0.1207) | Acc_1: (95.45%) (1344/1408)\n",
      "Epoch: 164 | Batch_idx: 20 |  Loss_1: (0.1410) | Acc_1: (94.57%) (2542/2688)\n",
      "Epoch: 164 | Batch_idx: 30 |  Loss_1: (0.1316) | Acc_1: (95.01%) (3770/3968)\n",
      "Epoch: 164 | Batch_idx: 40 |  Loss_1: (0.1317) | Acc_1: (95.06%) (4989/5248)\n",
      "Epoch: 164 | Batch_idx: 50 |  Loss_1: (0.1276) | Acc_1: (95.28%) (6220/6528)\n",
      "Epoch: 164 | Batch_idx: 60 |  Loss_1: (0.1308) | Acc_1: (95.13%) (7428/7808)\n",
      "Epoch: 164 | Batch_idx: 70 |  Loss_1: (0.1304) | Acc_1: (95.10%) (8643/9088)\n",
      "Epoch: 164 | Batch_idx: 80 |  Loss_1: (0.1320) | Acc_1: (95.08%) (9858/10368)\n",
      "Epoch: 164 | Batch_idx: 90 |  Loss_1: (0.1305) | Acc_1: (95.16%) (11084/11648)\n",
      "Epoch: 164 | Batch_idx: 100 |  Loss_1: (0.1322) | Acc_1: (95.04%) (12287/12928)\n",
      "Epoch: 164 | Batch_idx: 110 |  Loss_1: (0.1324) | Acc_1: (95.08%) (13509/14208)\n",
      "Epoch: 164 | Batch_idx: 120 |  Loss_1: (0.1352) | Acc_1: (94.97%) (14709/15488)\n",
      "Epoch: 164 | Batch_idx: 130 |  Loss_1: (0.1351) | Acc_1: (94.95%) (15921/16768)\n",
      "Epoch: 164 | Batch_idx: 140 |  Loss_1: (0.1359) | Acc_1: (94.91%) (17129/18048)\n",
      "Epoch: 164 | Batch_idx: 150 |  Loss_1: (0.1383) | Acc_1: (94.84%) (18331/19328)\n",
      "Epoch: 164 | Batch_idx: 160 |  Loss_1: (0.1386) | Acc_1: (94.81%) (19539/20608)\n",
      "Epoch: 164 | Batch_idx: 170 |  Loss_1: (0.1395) | Acc_1: (94.79%) (20748/21888)\n",
      "Epoch: 164 | Batch_idx: 180 |  Loss_1: (0.1398) | Acc_1: (94.78%) (21958/23168)\n",
      "Epoch: 164 | Batch_idx: 190 |  Loss_1: (0.1385) | Acc_1: (94.83%) (23184/24448)\n",
      "Epoch: 164 | Batch_idx: 200 |  Loss_1: (0.1381) | Acc_1: (94.85%) (24402/25728)\n",
      "Epoch: 164 | Batch_idx: 210 |  Loss_1: (0.1380) | Acc_1: (94.84%) (25614/27008)\n",
      "Epoch: 164 | Batch_idx: 220 |  Loss_1: (0.1378) | Acc_1: (94.84%) (26829/28288)\n",
      "Epoch: 164 | Batch_idx: 230 |  Loss_1: (0.1377) | Acc_1: (94.85%) (28044/29568)\n",
      "Epoch: 164 | Batch_idx: 240 |  Loss_1: (0.1371) | Acc_1: (94.87%) (29267/30848)\n",
      "Epoch: 164 | Batch_idx: 250 |  Loss_1: (0.1374) | Acc_1: (94.86%) (30478/32128)\n",
      "Epoch: 164 | Batch_idx: 260 |  Loss_1: (0.1372) | Acc_1: (94.87%) (31694/33408)\n",
      "Epoch: 164 | Batch_idx: 270 |  Loss_1: (0.1372) | Acc_1: (94.87%) (32907/34688)\n",
      "Epoch: 164 | Batch_idx: 280 |  Loss_1: (0.1376) | Acc_1: (94.84%) (34113/35968)\n",
      "Epoch: 164 | Batch_idx: 290 |  Loss_1: (0.1379) | Acc_1: (94.83%) (35322/37248)\n",
      "Epoch: 164 | Batch_idx: 300 |  Loss_1: (0.1384) | Acc_1: (94.82%) (36532/38528)\n",
      "Epoch: 164 | Batch_idx: 310 |  Loss_1: (0.1394) | Acc_1: (94.79%) (37734/39808)\n",
      "Epoch: 164 | Batch_idx: 320 |  Loss_1: (0.1386) | Acc_1: (94.82%) (38959/41088)\n",
      "Epoch: 164 | Batch_idx: 330 |  Loss_1: (0.1382) | Acc_1: (94.84%) (40181/42368)\n",
      "Epoch: 164 | Batch_idx: 340 |  Loss_1: (0.1385) | Acc_1: (94.82%) (41387/43648)\n",
      "Epoch: 164 | Batch_idx: 350 |  Loss_1: (0.1384) | Acc_1: (94.84%) (42611/44928)\n",
      "Epoch: 164 | Batch_idx: 360 |  Loss_1: (0.1380) | Acc_1: (94.87%) (43837/46208)\n",
      "Epoch: 164 | Batch_idx: 370 |  Loss_1: (0.1377) | Acc_1: (94.88%) (45056/47488)\n",
      "Epoch: 164 | Batch_idx: 380 |  Loss_1: (0.1373) | Acc_1: (94.88%) (46273/48768)\n",
      "Epoch: 164 | Batch_idx: 390 |  Loss_1: (0.1376) | Acc_1: (94.88%) (47438/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2910) | Acc: (92.99%) (9299/10000)\n",
      "Epoch: 165 | Batch_idx: 0 |  Loss_1: (0.1339) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 165 | Batch_idx: 10 |  Loss_1: (0.1169) | Acc_1: (95.45%) (1344/1408)\n",
      "Epoch: 165 | Batch_idx: 20 |  Loss_1: (0.1275) | Acc_1: (95.16%) (2558/2688)\n",
      "Epoch: 165 | Batch_idx: 30 |  Loss_1: (0.1312) | Acc_1: (95.19%) (3777/3968)\n",
      "Epoch: 165 | Batch_idx: 40 |  Loss_1: (0.1354) | Acc_1: (94.95%) (4983/5248)\n",
      "Epoch: 165 | Batch_idx: 50 |  Loss_1: (0.1287) | Acc_1: (95.22%) (6216/6528)\n",
      "Epoch: 165 | Batch_idx: 60 |  Loss_1: (0.1295) | Acc_1: (95.17%) (7431/7808)\n",
      "Epoch: 165 | Batch_idx: 70 |  Loss_1: (0.1311) | Acc_1: (95.16%) (8648/9088)\n",
      "Epoch: 165 | Batch_idx: 80 |  Loss_1: (0.1299) | Acc_1: (95.18%) (9868/10368)\n",
      "Epoch: 165 | Batch_idx: 90 |  Loss_1: (0.1289) | Acc_1: (95.21%) (11090/11648)\n",
      "Epoch: 165 | Batch_idx: 100 |  Loss_1: (0.1287) | Acc_1: (95.20%) (12307/12928)\n",
      "Epoch: 165 | Batch_idx: 110 |  Loss_1: (0.1290) | Acc_1: (95.19%) (13524/14208)\n",
      "Epoch: 165 | Batch_idx: 120 |  Loss_1: (0.1316) | Acc_1: (95.06%) (14723/15488)\n",
      "Epoch: 165 | Batch_idx: 130 |  Loss_1: (0.1313) | Acc_1: (95.08%) (15943/16768)\n",
      "Epoch: 165 | Batch_idx: 140 |  Loss_1: (0.1317) | Acc_1: (95.06%) (17156/18048)\n",
      "Epoch: 165 | Batch_idx: 150 |  Loss_1: (0.1307) | Acc_1: (95.10%) (18381/19328)\n",
      "Epoch: 165 | Batch_idx: 160 |  Loss_1: (0.1316) | Acc_1: (95.08%) (19595/20608)\n",
      "Epoch: 165 | Batch_idx: 170 |  Loss_1: (0.1318) | Acc_1: (95.07%) (20809/21888)\n",
      "Epoch: 165 | Batch_idx: 180 |  Loss_1: (0.1315) | Acc_1: (95.09%) (22030/23168)\n",
      "Epoch: 165 | Batch_idx: 190 |  Loss_1: (0.1319) | Acc_1: (95.08%) (23245/24448)\n",
      "Epoch: 165 | Batch_idx: 200 |  Loss_1: (0.1326) | Acc_1: (95.04%) (24452/25728)\n",
      "Epoch: 165 | Batch_idx: 210 |  Loss_1: (0.1329) | Acc_1: (95.03%) (25665/27008)\n",
      "Epoch: 165 | Batch_idx: 220 |  Loss_1: (0.1336) | Acc_1: (94.99%) (26871/28288)\n",
      "Epoch: 165 | Batch_idx: 230 |  Loss_1: (0.1324) | Acc_1: (95.02%) (28096/29568)\n",
      "Epoch: 165 | Batch_idx: 240 |  Loss_1: (0.1329) | Acc_1: (95.02%) (29312/30848)\n",
      "Epoch: 165 | Batch_idx: 250 |  Loss_1: (0.1326) | Acc_1: (95.04%) (30535/32128)\n",
      "Epoch: 165 | Batch_idx: 260 |  Loss_1: (0.1328) | Acc_1: (95.05%) (31754/33408)\n",
      "Epoch: 165 | Batch_idx: 270 |  Loss_1: (0.1332) | Acc_1: (95.04%) (32966/34688)\n",
      "Epoch: 165 | Batch_idx: 280 |  Loss_1: (0.1336) | Acc_1: (95.02%) (34178/35968)\n",
      "Epoch: 165 | Batch_idx: 290 |  Loss_1: (0.1341) | Acc_1: (95.01%) (35389/37248)\n",
      "Epoch: 165 | Batch_idx: 300 |  Loss_1: (0.1339) | Acc_1: (95.02%) (36611/38528)\n",
      "Epoch: 165 | Batch_idx: 310 |  Loss_1: (0.1340) | Acc_1: (95.01%) (37823/39808)\n",
      "Epoch: 165 | Batch_idx: 320 |  Loss_1: (0.1347) | Acc_1: (94.99%) (39028/41088)\n",
      "Epoch: 165 | Batch_idx: 330 |  Loss_1: (0.1354) | Acc_1: (94.96%) (40233/42368)\n",
      "Epoch: 165 | Batch_idx: 340 |  Loss_1: (0.1349) | Acc_1: (94.99%) (41461/43648)\n",
      "Epoch: 165 | Batch_idx: 350 |  Loss_1: (0.1350) | Acc_1: (94.99%) (42675/44928)\n",
      "Epoch: 165 | Batch_idx: 360 |  Loss_1: (0.1360) | Acc_1: (94.95%) (43874/46208)\n",
      "Epoch: 165 | Batch_idx: 370 |  Loss_1: (0.1368) | Acc_1: (94.92%) (45075/47488)\n",
      "Epoch: 165 | Batch_idx: 380 |  Loss_1: (0.1370) | Acc_1: (94.90%) (46282/48768)\n",
      "Epoch: 165 | Batch_idx: 390 |  Loss_1: (0.1372) | Acc_1: (94.88%) (47442/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3436) | Acc: (91.78%) (9178/10000)\n",
      "Epoch: 166 | Batch_idx: 0 |  Loss_1: (0.0972) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 166 | Batch_idx: 10 |  Loss_1: (0.1180) | Acc_1: (95.31%) (1342/1408)\n",
      "Epoch: 166 | Batch_idx: 20 |  Loss_1: (0.1384) | Acc_1: (94.83%) (2549/2688)\n",
      "Epoch: 166 | Batch_idx: 30 |  Loss_1: (0.1404) | Acc_1: (94.73%) (3759/3968)\n",
      "Epoch: 166 | Batch_idx: 40 |  Loss_1: (0.1417) | Acc_1: (94.72%) (4971/5248)\n",
      "Epoch: 166 | Batch_idx: 50 |  Loss_1: (0.1445) | Acc_1: (94.64%) (6178/6528)\n",
      "Epoch: 166 | Batch_idx: 60 |  Loss_1: (0.1417) | Acc_1: (94.77%) (7400/7808)\n",
      "Epoch: 166 | Batch_idx: 70 |  Loss_1: (0.1455) | Acc_1: (94.71%) (8607/9088)\n",
      "Epoch: 166 | Batch_idx: 80 |  Loss_1: (0.1460) | Acc_1: (94.69%) (9817/10368)\n",
      "Epoch: 166 | Batch_idx: 90 |  Loss_1: (0.1439) | Acc_1: (94.73%) (11034/11648)\n",
      "Epoch: 166 | Batch_idx: 100 |  Loss_1: (0.1429) | Acc_1: (94.73%) (12247/12928)\n",
      "Epoch: 166 | Batch_idx: 110 |  Loss_1: (0.1412) | Acc_1: (94.81%) (13470/14208)\n",
      "Epoch: 166 | Batch_idx: 120 |  Loss_1: (0.1418) | Acc_1: (94.74%) (14674/15488)\n",
      "Epoch: 166 | Batch_idx: 130 |  Loss_1: (0.1410) | Acc_1: (94.76%) (15890/16768)\n",
      "Epoch: 166 | Batch_idx: 140 |  Loss_1: (0.1427) | Acc_1: (94.66%) (17085/18048)\n",
      "Epoch: 166 | Batch_idx: 150 |  Loss_1: (0.1414) | Acc_1: (94.71%) (18305/19328)\n",
      "Epoch: 166 | Batch_idx: 160 |  Loss_1: (0.1423) | Acc_1: (94.69%) (19513/20608)\n",
      "Epoch: 166 | Batch_idx: 170 |  Loss_1: (0.1421) | Acc_1: (94.69%) (20725/21888)\n",
      "Epoch: 166 | Batch_idx: 180 |  Loss_1: (0.1425) | Acc_1: (94.69%) (21937/23168)\n",
      "Epoch: 166 | Batch_idx: 190 |  Loss_1: (0.1411) | Acc_1: (94.73%) (23160/24448)\n",
      "Epoch: 166 | Batch_idx: 200 |  Loss_1: (0.1404) | Acc_1: (94.75%) (24376/25728)\n",
      "Epoch: 166 | Batch_idx: 210 |  Loss_1: (0.1411) | Acc_1: (94.72%) (25581/27008)\n",
      "Epoch: 166 | Batch_idx: 220 |  Loss_1: (0.1400) | Acc_1: (94.75%) (26804/28288)\n",
      "Epoch: 166 | Batch_idx: 230 |  Loss_1: (0.1398) | Acc_1: (94.76%) (28018/29568)\n",
      "Epoch: 166 | Batch_idx: 240 |  Loss_1: (0.1400) | Acc_1: (94.75%) (29230/30848)\n",
      "Epoch: 166 | Batch_idx: 250 |  Loss_1: (0.1408) | Acc_1: (94.73%) (30435/32128)\n",
      "Epoch: 166 | Batch_idx: 260 |  Loss_1: (0.1399) | Acc_1: (94.77%) (31661/33408)\n",
      "Epoch: 166 | Batch_idx: 270 |  Loss_1: (0.1400) | Acc_1: (94.77%) (32875/34688)\n",
      "Epoch: 166 | Batch_idx: 280 |  Loss_1: (0.1403) | Acc_1: (94.75%) (34081/35968)\n",
      "Epoch: 166 | Batch_idx: 290 |  Loss_1: (0.1406) | Acc_1: (94.75%) (35292/37248)\n",
      "Epoch: 166 | Batch_idx: 300 |  Loss_1: (0.1404) | Acc_1: (94.75%) (36506/38528)\n",
      "Epoch: 166 | Batch_idx: 310 |  Loss_1: (0.1407) | Acc_1: (94.76%) (37723/39808)\n",
      "Epoch: 166 | Batch_idx: 320 |  Loss_1: (0.1411) | Acc_1: (94.76%) (38935/41088)\n",
      "Epoch: 166 | Batch_idx: 330 |  Loss_1: (0.1407) | Acc_1: (94.76%) (40149/42368)\n",
      "Epoch: 166 | Batch_idx: 340 |  Loss_1: (0.1406) | Acc_1: (94.76%) (41363/43648)\n",
      "Epoch: 166 | Batch_idx: 350 |  Loss_1: (0.1405) | Acc_1: (94.78%) (42581/44928)\n",
      "Epoch: 166 | Batch_idx: 360 |  Loss_1: (0.1399) | Acc_1: (94.81%) (43809/46208)\n",
      "Epoch: 166 | Batch_idx: 370 |  Loss_1: (0.1394) | Acc_1: (94.83%) (45031/47488)\n",
      "Epoch: 166 | Batch_idx: 380 |  Loss_1: (0.1395) | Acc_1: (94.82%) (46241/48768)\n",
      "Epoch: 166 | Batch_idx: 390 |  Loss_1: (0.1394) | Acc_1: (94.82%) (47409/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3099) | Acc: (92.43%) (9243/10000)\n",
      "Epoch: 167 | Batch_idx: 0 |  Loss_1: (0.2288) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 167 | Batch_idx: 10 |  Loss_1: (0.1504) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 167 | Batch_idx: 20 |  Loss_1: (0.1425) | Acc_1: (94.75%) (2547/2688)\n",
      "Epoch: 167 | Batch_idx: 30 |  Loss_1: (0.1494) | Acc_1: (94.48%) (3749/3968)\n",
      "Epoch: 167 | Batch_idx: 40 |  Loss_1: (0.1465) | Acc_1: (94.57%) (4963/5248)\n",
      "Epoch: 167 | Batch_idx: 50 |  Loss_1: (0.1435) | Acc_1: (94.73%) (6184/6528)\n",
      "Epoch: 167 | Batch_idx: 60 |  Loss_1: (0.1433) | Acc_1: (94.70%) (7394/7808)\n",
      "Epoch: 167 | Batch_idx: 70 |  Loss_1: (0.1403) | Acc_1: (94.77%) (8613/9088)\n",
      "Epoch: 167 | Batch_idx: 80 |  Loss_1: (0.1397) | Acc_1: (94.85%) (9834/10368)\n",
      "Epoch: 167 | Batch_idx: 90 |  Loss_1: (0.1406) | Acc_1: (94.85%) (11048/11648)\n",
      "Epoch: 167 | Batch_idx: 100 |  Loss_1: (0.1374) | Acc_1: (94.93%) (12273/12928)\n",
      "Epoch: 167 | Batch_idx: 110 |  Loss_1: (0.1401) | Acc_1: (94.88%) (13480/14208)\n",
      "Epoch: 167 | Batch_idx: 120 |  Loss_1: (0.1400) | Acc_1: (94.83%) (14687/15488)\n",
      "Epoch: 167 | Batch_idx: 130 |  Loss_1: (0.1394) | Acc_1: (94.88%) (15909/16768)\n",
      "Epoch: 167 | Batch_idx: 140 |  Loss_1: (0.1392) | Acc_1: (94.86%) (17120/18048)\n",
      "Epoch: 167 | Batch_idx: 150 |  Loss_1: (0.1398) | Acc_1: (94.82%) (18327/19328)\n",
      "Epoch: 167 | Batch_idx: 160 |  Loss_1: (0.1399) | Acc_1: (94.87%) (19550/20608)\n",
      "Epoch: 167 | Batch_idx: 170 |  Loss_1: (0.1408) | Acc_1: (94.81%) (20753/21888)\n",
      "Epoch: 167 | Batch_idx: 180 |  Loss_1: (0.1413) | Acc_1: (94.79%) (21962/23168)\n",
      "Epoch: 167 | Batch_idx: 190 |  Loss_1: (0.1423) | Acc_1: (94.75%) (23164/24448)\n",
      "Epoch: 167 | Batch_idx: 200 |  Loss_1: (0.1421) | Acc_1: (94.78%) (24385/25728)\n",
      "Epoch: 167 | Batch_idx: 210 |  Loss_1: (0.1417) | Acc_1: (94.78%) (25599/27008)\n",
      "Epoch: 167 | Batch_idx: 220 |  Loss_1: (0.1412) | Acc_1: (94.80%) (26817/28288)\n",
      "Epoch: 167 | Batch_idx: 230 |  Loss_1: (0.1419) | Acc_1: (94.75%) (28017/29568)\n",
      "Epoch: 167 | Batch_idx: 240 |  Loss_1: (0.1407) | Acc_1: (94.79%) (29242/30848)\n",
      "Epoch: 167 | Batch_idx: 250 |  Loss_1: (0.1406) | Acc_1: (94.79%) (30453/32128)\n",
      "Epoch: 167 | Batch_idx: 260 |  Loss_1: (0.1398) | Acc_1: (94.83%) (31682/33408)\n",
      "Epoch: 167 | Batch_idx: 270 |  Loss_1: (0.1399) | Acc_1: (94.84%) (32897/34688)\n",
      "Epoch: 167 | Batch_idx: 280 |  Loss_1: (0.1399) | Acc_1: (94.83%) (34110/35968)\n",
      "Epoch: 167 | Batch_idx: 290 |  Loss_1: (0.1401) | Acc_1: (94.83%) (35321/37248)\n",
      "Epoch: 167 | Batch_idx: 300 |  Loss_1: (0.1401) | Acc_1: (94.83%) (36537/38528)\n",
      "Epoch: 167 | Batch_idx: 310 |  Loss_1: (0.1409) | Acc_1: (94.81%) (37740/39808)\n",
      "Epoch: 167 | Batch_idx: 320 |  Loss_1: (0.1405) | Acc_1: (94.84%) (38966/41088)\n",
      "Epoch: 167 | Batch_idx: 330 |  Loss_1: (0.1406) | Acc_1: (94.84%) (40180/42368)\n",
      "Epoch: 167 | Batch_idx: 340 |  Loss_1: (0.1406) | Acc_1: (94.82%) (41389/43648)\n",
      "Epoch: 167 | Batch_idx: 350 |  Loss_1: (0.1412) | Acc_1: (94.81%) (42596/44928)\n",
      "Epoch: 167 | Batch_idx: 360 |  Loss_1: (0.1418) | Acc_1: (94.79%) (43801/46208)\n",
      "Epoch: 167 | Batch_idx: 370 |  Loss_1: (0.1414) | Acc_1: (94.80%) (45017/47488)\n",
      "Epoch: 167 | Batch_idx: 380 |  Loss_1: (0.1409) | Acc_1: (94.82%) (46240/48768)\n",
      "Epoch: 167 | Batch_idx: 390 |  Loss_1: (0.1406) | Acc_1: (94.82%) (47412/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2986) | Acc: (92.61%) (9261/10000)\n",
      "Epoch: 168 | Batch_idx: 0 |  Loss_1: (0.1704) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 168 | Batch_idx: 10 |  Loss_1: (0.1387) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 168 | Batch_idx: 20 |  Loss_1: (0.1446) | Acc_1: (94.46%) (2539/2688)\n",
      "Epoch: 168 | Batch_idx: 30 |  Loss_1: (0.1373) | Acc_1: (94.78%) (3761/3968)\n",
      "Epoch: 168 | Batch_idx: 40 |  Loss_1: (0.1362) | Acc_1: (94.82%) (4976/5248)\n",
      "Epoch: 168 | Batch_idx: 50 |  Loss_1: (0.1334) | Acc_1: (94.96%) (6199/6528)\n",
      "Epoch: 168 | Batch_idx: 60 |  Loss_1: (0.1365) | Acc_1: (94.83%) (7404/7808)\n",
      "Epoch: 168 | Batch_idx: 70 |  Loss_1: (0.1354) | Acc_1: (94.91%) (8625/9088)\n",
      "Epoch: 168 | Batch_idx: 80 |  Loss_1: (0.1394) | Acc_1: (94.74%) (9823/10368)\n",
      "Epoch: 168 | Batch_idx: 90 |  Loss_1: (0.1384) | Acc_1: (94.79%) (11041/11648)\n",
      "Epoch: 168 | Batch_idx: 100 |  Loss_1: (0.1380) | Acc_1: (94.83%) (12259/12928)\n",
      "Epoch: 168 | Batch_idx: 110 |  Loss_1: (0.1364) | Acc_1: (94.89%) (13482/14208)\n",
      "Epoch: 168 | Batch_idx: 120 |  Loss_1: (0.1365) | Acc_1: (94.90%) (14698/15488)\n",
      "Epoch: 168 | Batch_idx: 130 |  Loss_1: (0.1354) | Acc_1: (94.95%) (15922/16768)\n",
      "Epoch: 168 | Batch_idx: 140 |  Loss_1: (0.1348) | Acc_1: (94.99%) (17143/18048)\n",
      "Epoch: 168 | Batch_idx: 150 |  Loss_1: (0.1340) | Acc_1: (95.03%) (18367/19328)\n",
      "Epoch: 168 | Batch_idx: 160 |  Loss_1: (0.1343) | Acc_1: (95.03%) (19583/20608)\n",
      "Epoch: 168 | Batch_idx: 170 |  Loss_1: (0.1350) | Acc_1: (94.96%) (20785/21888)\n",
      "Epoch: 168 | Batch_idx: 180 |  Loss_1: (0.1356) | Acc_1: (94.94%) (21995/23168)\n",
      "Epoch: 168 | Batch_idx: 190 |  Loss_1: (0.1360) | Acc_1: (94.93%) (23209/24448)\n",
      "Epoch: 168 | Batch_idx: 200 |  Loss_1: (0.1359) | Acc_1: (94.94%) (24427/25728)\n",
      "Epoch: 168 | Batch_idx: 210 |  Loss_1: (0.1352) | Acc_1: (94.97%) (25649/27008)\n",
      "Epoch: 168 | Batch_idx: 220 |  Loss_1: (0.1349) | Acc_1: (94.98%) (26869/28288)\n",
      "Epoch: 168 | Batch_idx: 230 |  Loss_1: (0.1355) | Acc_1: (94.97%) (28080/29568)\n",
      "Epoch: 168 | Batch_idx: 240 |  Loss_1: (0.1368) | Acc_1: (94.93%) (29284/30848)\n",
      "Epoch: 168 | Batch_idx: 250 |  Loss_1: (0.1374) | Acc_1: (94.92%) (30497/32128)\n",
      "Epoch: 168 | Batch_idx: 260 |  Loss_1: (0.1377) | Acc_1: (94.91%) (31709/33408)\n",
      "Epoch: 168 | Batch_idx: 270 |  Loss_1: (0.1378) | Acc_1: (94.91%) (32921/34688)\n",
      "Epoch: 168 | Batch_idx: 280 |  Loss_1: (0.1378) | Acc_1: (94.92%) (34140/35968)\n",
      "Epoch: 168 | Batch_idx: 290 |  Loss_1: (0.1376) | Acc_1: (94.92%) (35356/37248)\n",
      "Epoch: 168 | Batch_idx: 300 |  Loss_1: (0.1369) | Acc_1: (94.94%) (36580/38528)\n",
      "Epoch: 168 | Batch_idx: 310 |  Loss_1: (0.1366) | Acc_1: (94.94%) (37795/39808)\n",
      "Epoch: 168 | Batch_idx: 320 |  Loss_1: (0.1364) | Acc_1: (94.94%) (39011/41088)\n",
      "Epoch: 168 | Batch_idx: 330 |  Loss_1: (0.1361) | Acc_1: (94.95%) (40227/42368)\n",
      "Epoch: 168 | Batch_idx: 340 |  Loss_1: (0.1359) | Acc_1: (94.96%) (41448/43648)\n",
      "Epoch: 168 | Batch_idx: 350 |  Loss_1: (0.1363) | Acc_1: (94.95%) (42661/44928)\n",
      "Epoch: 168 | Batch_idx: 360 |  Loss_1: (0.1368) | Acc_1: (94.96%) (43877/46208)\n",
      "Epoch: 168 | Batch_idx: 370 |  Loss_1: (0.1367) | Acc_1: (94.95%) (45091/47488)\n",
      "Epoch: 168 | Batch_idx: 380 |  Loss_1: (0.1370) | Acc_1: (94.94%) (46302/48768)\n",
      "Epoch: 168 | Batch_idx: 390 |  Loss_1: (0.1369) | Acc_1: (94.94%) (47471/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3186) | Acc: (92.37%) (9237/10000)\n",
      "Epoch: 169 | Batch_idx: 0 |  Loss_1: (0.1643) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 169 | Batch_idx: 10 |  Loss_1: (0.1222) | Acc_1: (95.38%) (1343/1408)\n",
      "Epoch: 169 | Batch_idx: 20 |  Loss_1: (0.1320) | Acc_1: (95.13%) (2557/2688)\n",
      "Epoch: 169 | Batch_idx: 30 |  Loss_1: (0.1426) | Acc_1: (94.71%) (3758/3968)\n",
      "Epoch: 169 | Batch_idx: 40 |  Loss_1: (0.1454) | Acc_1: (94.59%) (4964/5248)\n",
      "Epoch: 169 | Batch_idx: 50 |  Loss_1: (0.1486) | Acc_1: (94.49%) (6168/6528)\n",
      "Epoch: 169 | Batch_idx: 60 |  Loss_1: (0.1483) | Acc_1: (94.47%) (7376/7808)\n",
      "Epoch: 169 | Batch_idx: 70 |  Loss_1: (0.1481) | Acc_1: (94.49%) (8587/9088)\n",
      "Epoch: 169 | Batch_idx: 80 |  Loss_1: (0.1483) | Acc_1: (94.49%) (9797/10368)\n",
      "Epoch: 169 | Batch_idx: 90 |  Loss_1: (0.1459) | Acc_1: (94.62%) (11021/11648)\n",
      "Epoch: 169 | Batch_idx: 100 |  Loss_1: (0.1459) | Acc_1: (94.65%) (12236/12928)\n",
      "Epoch: 169 | Batch_idx: 110 |  Loss_1: (0.1457) | Acc_1: (94.66%) (13449/14208)\n",
      "Epoch: 169 | Batch_idx: 120 |  Loss_1: (0.1465) | Acc_1: (94.60%) (14652/15488)\n",
      "Epoch: 169 | Batch_idx: 130 |  Loss_1: (0.1473) | Acc_1: (94.60%) (15862/16768)\n",
      "Epoch: 169 | Batch_idx: 140 |  Loss_1: (0.1472) | Acc_1: (94.60%) (17074/18048)\n",
      "Epoch: 169 | Batch_idx: 150 |  Loss_1: (0.1472) | Acc_1: (94.61%) (18286/19328)\n",
      "Epoch: 169 | Batch_idx: 160 |  Loss_1: (0.1471) | Acc_1: (94.61%) (19498/20608)\n",
      "Epoch: 169 | Batch_idx: 170 |  Loss_1: (0.1458) | Acc_1: (94.67%) (20721/21888)\n",
      "Epoch: 169 | Batch_idx: 180 |  Loss_1: (0.1448) | Acc_1: (94.72%) (21944/23168)\n",
      "Epoch: 169 | Batch_idx: 190 |  Loss_1: (0.1443) | Acc_1: (94.73%) (23160/24448)\n",
      "Epoch: 169 | Batch_idx: 200 |  Loss_1: (0.1435) | Acc_1: (94.74%) (24374/25728)\n",
      "Epoch: 169 | Batch_idx: 210 |  Loss_1: (0.1434) | Acc_1: (94.75%) (25589/27008)\n",
      "Epoch: 169 | Batch_idx: 220 |  Loss_1: (0.1431) | Acc_1: (94.77%) (26808/28288)\n",
      "Epoch: 169 | Batch_idx: 230 |  Loss_1: (0.1425) | Acc_1: (94.79%) (28027/29568)\n",
      "Epoch: 169 | Batch_idx: 240 |  Loss_1: (0.1415) | Acc_1: (94.82%) (29250/30848)\n",
      "Epoch: 169 | Batch_idx: 250 |  Loss_1: (0.1416) | Acc_1: (94.83%) (30468/32128)\n",
      "Epoch: 169 | Batch_idx: 260 |  Loss_1: (0.1417) | Acc_1: (94.82%) (31678/33408)\n",
      "Epoch: 169 | Batch_idx: 270 |  Loss_1: (0.1417) | Acc_1: (94.81%) (32888/34688)\n",
      "Epoch: 169 | Batch_idx: 280 |  Loss_1: (0.1419) | Acc_1: (94.79%) (34094/35968)\n",
      "Epoch: 169 | Batch_idx: 290 |  Loss_1: (0.1429) | Acc_1: (94.73%) (35286/37248)\n",
      "Epoch: 169 | Batch_idx: 300 |  Loss_1: (0.1431) | Acc_1: (94.73%) (36499/38528)\n",
      "Epoch: 169 | Batch_idx: 310 |  Loss_1: (0.1419) | Acc_1: (94.77%) (37728/39808)\n",
      "Epoch: 169 | Batch_idx: 320 |  Loss_1: (0.1413) | Acc_1: (94.79%) (38947/41088)\n",
      "Epoch: 169 | Batch_idx: 330 |  Loss_1: (0.1409) | Acc_1: (94.81%) (40168/42368)\n",
      "Epoch: 169 | Batch_idx: 340 |  Loss_1: (0.1405) | Acc_1: (94.82%) (41389/43648)\n",
      "Epoch: 169 | Batch_idx: 350 |  Loss_1: (0.1403) | Acc_1: (94.83%) (42606/44928)\n",
      "Epoch: 169 | Batch_idx: 360 |  Loss_1: (0.1403) | Acc_1: (94.83%) (43819/46208)\n",
      "Epoch: 169 | Batch_idx: 370 |  Loss_1: (0.1395) | Acc_1: (94.86%) (45045/47488)\n",
      "Epoch: 169 | Batch_idx: 380 |  Loss_1: (0.1395) | Acc_1: (94.84%) (46253/48768)\n",
      "Epoch: 169 | Batch_idx: 390 |  Loss_1: (0.1392) | Acc_1: (94.85%) (47427/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3023) | Acc: (92.85%) (9285/10000)\n",
      "Epoch: 170 | Batch_idx: 0 |  Loss_1: (0.1308) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 170 | Batch_idx: 10 |  Loss_1: (0.1482) | Acc_1: (94.74%) (1334/1408)\n",
      "Epoch: 170 | Batch_idx: 20 |  Loss_1: (0.1585) | Acc_1: (94.38%) (2537/2688)\n",
      "Epoch: 170 | Batch_idx: 30 |  Loss_1: (0.1497) | Acc_1: (94.43%) (3747/3968)\n",
      "Epoch: 170 | Batch_idx: 40 |  Loss_1: (0.1542) | Acc_1: (94.34%) (4951/5248)\n",
      "Epoch: 170 | Batch_idx: 50 |  Loss_1: (0.1487) | Acc_1: (94.55%) (6172/6528)\n",
      "Epoch: 170 | Batch_idx: 60 |  Loss_1: (0.1470) | Acc_1: (94.54%) (7382/7808)\n",
      "Epoch: 170 | Batch_idx: 70 |  Loss_1: (0.1451) | Acc_1: (94.61%) (8598/9088)\n",
      "Epoch: 170 | Batch_idx: 80 |  Loss_1: (0.1424) | Acc_1: (94.74%) (9823/10368)\n",
      "Epoch: 170 | Batch_idx: 90 |  Loss_1: (0.1428) | Acc_1: (94.73%) (11034/11648)\n",
      "Epoch: 170 | Batch_idx: 100 |  Loss_1: (0.1395) | Acc_1: (94.86%) (12264/12928)\n",
      "Epoch: 170 | Batch_idx: 110 |  Loss_1: (0.1397) | Acc_1: (94.88%) (13480/14208)\n",
      "Epoch: 170 | Batch_idx: 120 |  Loss_1: (0.1416) | Acc_1: (94.82%) (14686/15488)\n",
      "Epoch: 170 | Batch_idx: 130 |  Loss_1: (0.1401) | Acc_1: (94.84%) (15903/16768)\n",
      "Epoch: 170 | Batch_idx: 140 |  Loss_1: (0.1389) | Acc_1: (94.89%) (17125/18048)\n",
      "Epoch: 170 | Batch_idx: 150 |  Loss_1: (0.1387) | Acc_1: (94.88%) (18338/19328)\n",
      "Epoch: 170 | Batch_idx: 160 |  Loss_1: (0.1395) | Acc_1: (94.83%) (19543/20608)\n",
      "Epoch: 170 | Batch_idx: 170 |  Loss_1: (0.1394) | Acc_1: (94.84%) (20758/21888)\n",
      "Epoch: 170 | Batch_idx: 180 |  Loss_1: (0.1396) | Acc_1: (94.84%) (21973/23168)\n",
      "Epoch: 170 | Batch_idx: 190 |  Loss_1: (0.1392) | Acc_1: (94.86%) (23191/24448)\n",
      "Epoch: 170 | Batch_idx: 200 |  Loss_1: (0.1379) | Acc_1: (94.91%) (24418/25728)\n",
      "Epoch: 170 | Batch_idx: 210 |  Loss_1: (0.1392) | Acc_1: (94.87%) (25623/27008)\n",
      "Epoch: 170 | Batch_idx: 220 |  Loss_1: (0.1388) | Acc_1: (94.88%) (26841/28288)\n",
      "Epoch: 170 | Batch_idx: 230 |  Loss_1: (0.1382) | Acc_1: (94.89%) (28058/29568)\n",
      "Epoch: 170 | Batch_idx: 240 |  Loss_1: (0.1380) | Acc_1: (94.91%) (29277/30848)\n",
      "Epoch: 170 | Batch_idx: 250 |  Loss_1: (0.1375) | Acc_1: (94.93%) (30499/32128)\n",
      "Epoch: 170 | Batch_idx: 260 |  Loss_1: (0.1370) | Acc_1: (94.95%) (31720/33408)\n",
      "Epoch: 170 | Batch_idx: 270 |  Loss_1: (0.1369) | Acc_1: (94.94%) (32934/34688)\n",
      "Epoch: 170 | Batch_idx: 280 |  Loss_1: (0.1373) | Acc_1: (94.92%) (34140/35968)\n",
      "Epoch: 170 | Batch_idx: 290 |  Loss_1: (0.1375) | Acc_1: (94.91%) (35352/37248)\n",
      "Epoch: 170 | Batch_idx: 300 |  Loss_1: (0.1391) | Acc_1: (94.84%) (36541/38528)\n",
      "Epoch: 170 | Batch_idx: 310 |  Loss_1: (0.1390) | Acc_1: (94.86%) (37761/39808)\n",
      "Epoch: 170 | Batch_idx: 320 |  Loss_1: (0.1383) | Acc_1: (94.88%) (38983/41088)\n",
      "Epoch: 170 | Batch_idx: 330 |  Loss_1: (0.1384) | Acc_1: (94.87%) (40196/42368)\n",
      "Epoch: 170 | Batch_idx: 340 |  Loss_1: (0.1390) | Acc_1: (94.84%) (41394/43648)\n",
      "Epoch: 170 | Batch_idx: 350 |  Loss_1: (0.1395) | Acc_1: (94.81%) (42597/44928)\n",
      "Epoch: 170 | Batch_idx: 360 |  Loss_1: (0.1394) | Acc_1: (94.81%) (43809/46208)\n",
      "Epoch: 170 | Batch_idx: 370 |  Loss_1: (0.1392) | Acc_1: (94.82%) (45029/47488)\n",
      "Epoch: 170 | Batch_idx: 380 |  Loss_1: (0.1393) | Acc_1: (94.81%) (46235/48768)\n",
      "Epoch: 170 | Batch_idx: 390 |  Loss_1: (0.1394) | Acc_1: (94.80%) (47398/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3270) | Acc: (92.15%) (9215/10000)\n",
      "Epoch: 171 | Batch_idx: 0 |  Loss_1: (0.1293) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 171 | Batch_idx: 10 |  Loss_1: (0.1192) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 171 | Batch_idx: 20 |  Loss_1: (0.1254) | Acc_1: (95.35%) (2563/2688)\n",
      "Epoch: 171 | Batch_idx: 30 |  Loss_1: (0.1263) | Acc_1: (95.39%) (3785/3968)\n",
      "Epoch: 171 | Batch_idx: 40 |  Loss_1: (0.1220) | Acc_1: (95.52%) (5013/5248)\n",
      "Epoch: 171 | Batch_idx: 50 |  Loss_1: (0.1192) | Acc_1: (95.62%) (6242/6528)\n",
      "Epoch: 171 | Batch_idx: 60 |  Loss_1: (0.1195) | Acc_1: (95.63%) (7467/7808)\n",
      "Epoch: 171 | Batch_idx: 70 |  Loss_1: (0.1184) | Acc_1: (95.66%) (8694/9088)\n",
      "Epoch: 171 | Batch_idx: 80 |  Loss_1: (0.1199) | Acc_1: (95.60%) (9912/10368)\n",
      "Epoch: 171 | Batch_idx: 90 |  Loss_1: (0.1235) | Acc_1: (95.45%) (11118/11648)\n",
      "Epoch: 171 | Batch_idx: 100 |  Loss_1: (0.1261) | Acc_1: (95.37%) (12330/12928)\n",
      "Epoch: 171 | Batch_idx: 110 |  Loss_1: (0.1256) | Acc_1: (95.44%) (13560/14208)\n",
      "Epoch: 171 | Batch_idx: 120 |  Loss_1: (0.1267) | Acc_1: (95.41%) (14777/15488)\n",
      "Epoch: 171 | Batch_idx: 130 |  Loss_1: (0.1287) | Acc_1: (95.29%) (15978/16768)\n",
      "Epoch: 171 | Batch_idx: 140 |  Loss_1: (0.1296) | Acc_1: (95.23%) (17188/18048)\n",
      "Epoch: 171 | Batch_idx: 150 |  Loss_1: (0.1303) | Acc_1: (95.20%) (18400/19328)\n",
      "Epoch: 171 | Batch_idx: 160 |  Loss_1: (0.1307) | Acc_1: (95.16%) (19611/20608)\n",
      "Epoch: 171 | Batch_idx: 170 |  Loss_1: (0.1307) | Acc_1: (95.15%) (20826/21888)\n",
      "Epoch: 171 | Batch_idx: 180 |  Loss_1: (0.1300) | Acc_1: (95.15%) (22044/23168)\n",
      "Epoch: 171 | Batch_idx: 190 |  Loss_1: (0.1301) | Acc_1: (95.14%) (23261/24448)\n",
      "Epoch: 171 | Batch_idx: 200 |  Loss_1: (0.1299) | Acc_1: (95.16%) (24484/25728)\n",
      "Epoch: 171 | Batch_idx: 210 |  Loss_1: (0.1319) | Acc_1: (95.11%) (25686/27008)\n",
      "Epoch: 171 | Batch_idx: 220 |  Loss_1: (0.1330) | Acc_1: (95.07%) (26894/28288)\n",
      "Epoch: 171 | Batch_idx: 230 |  Loss_1: (0.1331) | Acc_1: (95.07%) (28111/29568)\n",
      "Epoch: 171 | Batch_idx: 240 |  Loss_1: (0.1331) | Acc_1: (95.08%) (29329/30848)\n",
      "Epoch: 171 | Batch_idx: 250 |  Loss_1: (0.1334) | Acc_1: (95.06%) (30542/32128)\n",
      "Epoch: 171 | Batch_idx: 260 |  Loss_1: (0.1329) | Acc_1: (95.08%) (31763/33408)\n",
      "Epoch: 171 | Batch_idx: 270 |  Loss_1: (0.1333) | Acc_1: (95.06%) (32974/34688)\n",
      "Epoch: 171 | Batch_idx: 280 |  Loss_1: (0.1334) | Acc_1: (95.05%) (34188/35968)\n",
      "Epoch: 171 | Batch_idx: 290 |  Loss_1: (0.1330) | Acc_1: (95.07%) (35413/37248)\n",
      "Epoch: 171 | Batch_idx: 300 |  Loss_1: (0.1323) | Acc_1: (95.09%) (36638/38528)\n",
      "Epoch: 171 | Batch_idx: 310 |  Loss_1: (0.1320) | Acc_1: (95.11%) (37861/39808)\n",
      "Epoch: 171 | Batch_idx: 320 |  Loss_1: (0.1316) | Acc_1: (95.12%) (39083/41088)\n",
      "Epoch: 171 | Batch_idx: 330 |  Loss_1: (0.1308) | Acc_1: (95.15%) (40315/42368)\n",
      "Epoch: 171 | Batch_idx: 340 |  Loss_1: (0.1309) | Acc_1: (95.15%) (41531/43648)\n",
      "Epoch: 171 | Batch_idx: 350 |  Loss_1: (0.1317) | Acc_1: (95.12%) (42737/44928)\n",
      "Epoch: 171 | Batch_idx: 360 |  Loss_1: (0.1326) | Acc_1: (95.08%) (43935/46208)\n",
      "Epoch: 171 | Batch_idx: 370 |  Loss_1: (0.1329) | Acc_1: (95.06%) (45144/47488)\n",
      "Epoch: 171 | Batch_idx: 380 |  Loss_1: (0.1325) | Acc_1: (95.08%) (46370/48768)\n",
      "Epoch: 171 | Batch_idx: 390 |  Loss_1: (0.1325) | Acc_1: (95.09%) (47546/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3494) | Acc: (92.25%) (9225/10000)\n",
      "Epoch: 172 | Batch_idx: 0 |  Loss_1: (0.1541) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 172 | Batch_idx: 10 |  Loss_1: (0.1555) | Acc_1: (93.96%) (1323/1408)\n",
      "Epoch: 172 | Batch_idx: 20 |  Loss_1: (0.1519) | Acc_1: (94.46%) (2539/2688)\n",
      "Epoch: 172 | Batch_idx: 30 |  Loss_1: (0.1448) | Acc_1: (94.76%) (3760/3968)\n",
      "Epoch: 172 | Batch_idx: 40 |  Loss_1: (0.1389) | Acc_1: (94.89%) (4980/5248)\n",
      "Epoch: 172 | Batch_idx: 50 |  Loss_1: (0.1374) | Acc_1: (94.93%) (6197/6528)\n",
      "Epoch: 172 | Batch_idx: 60 |  Loss_1: (0.1406) | Acc_1: (94.74%) (7397/7808)\n",
      "Epoch: 172 | Batch_idx: 70 |  Loss_1: (0.1441) | Acc_1: (94.62%) (8599/9088)\n",
      "Epoch: 172 | Batch_idx: 80 |  Loss_1: (0.1462) | Acc_1: (94.54%) (9802/10368)\n",
      "Epoch: 172 | Batch_idx: 90 |  Loss_1: (0.1447) | Acc_1: (94.58%) (11017/11648)\n",
      "Epoch: 172 | Batch_idx: 100 |  Loss_1: (0.1415) | Acc_1: (94.74%) (12248/12928)\n",
      "Epoch: 172 | Batch_idx: 110 |  Loss_1: (0.1417) | Acc_1: (94.76%) (13464/14208)\n",
      "Epoch: 172 | Batch_idx: 120 |  Loss_1: (0.1411) | Acc_1: (94.77%) (14678/15488)\n",
      "Epoch: 172 | Batch_idx: 130 |  Loss_1: (0.1431) | Acc_1: (94.68%) (15876/16768)\n",
      "Epoch: 172 | Batch_idx: 140 |  Loss_1: (0.1434) | Acc_1: (94.65%) (17083/18048)\n",
      "Epoch: 172 | Batch_idx: 150 |  Loss_1: (0.1438) | Acc_1: (94.65%) (18294/19328)\n",
      "Epoch: 172 | Batch_idx: 160 |  Loss_1: (0.1433) | Acc_1: (94.68%) (19512/20608)\n",
      "Epoch: 172 | Batch_idx: 170 |  Loss_1: (0.1434) | Acc_1: (94.68%) (20723/21888)\n",
      "Epoch: 172 | Batch_idx: 180 |  Loss_1: (0.1430) | Acc_1: (94.66%) (21931/23168)\n",
      "Epoch: 172 | Batch_idx: 190 |  Loss_1: (0.1425) | Acc_1: (94.68%) (23147/24448)\n",
      "Epoch: 172 | Batch_idx: 200 |  Loss_1: (0.1409) | Acc_1: (94.74%) (24374/25728)\n",
      "Epoch: 172 | Batch_idx: 210 |  Loss_1: (0.1409) | Acc_1: (94.73%) (25585/27008)\n",
      "Epoch: 172 | Batch_idx: 220 |  Loss_1: (0.1411) | Acc_1: (94.74%) (26799/28288)\n",
      "Epoch: 172 | Batch_idx: 230 |  Loss_1: (0.1400) | Acc_1: (94.80%) (28029/29568)\n",
      "Epoch: 172 | Batch_idx: 240 |  Loss_1: (0.1394) | Acc_1: (94.82%) (29249/30848)\n",
      "Epoch: 172 | Batch_idx: 250 |  Loss_1: (0.1394) | Acc_1: (94.81%) (30460/32128)\n",
      "Epoch: 172 | Batch_idx: 260 |  Loss_1: (0.1387) | Acc_1: (94.84%) (31683/33408)\n",
      "Epoch: 172 | Batch_idx: 270 |  Loss_1: (0.1377) | Acc_1: (94.87%) (32907/34688)\n",
      "Epoch: 172 | Batch_idx: 280 |  Loss_1: (0.1377) | Acc_1: (94.87%) (34122/35968)\n",
      "Epoch: 172 | Batch_idx: 290 |  Loss_1: (0.1373) | Acc_1: (94.87%) (35339/37248)\n",
      "Epoch: 172 | Batch_idx: 300 |  Loss_1: (0.1378) | Acc_1: (94.85%) (36544/38528)\n",
      "Epoch: 172 | Batch_idx: 310 |  Loss_1: (0.1377) | Acc_1: (94.85%) (37756/39808)\n",
      "Epoch: 172 | Batch_idx: 320 |  Loss_1: (0.1374) | Acc_1: (94.87%) (38979/41088)\n",
      "Epoch: 172 | Batch_idx: 330 |  Loss_1: (0.1374) | Acc_1: (94.88%) (40197/42368)\n",
      "Epoch: 172 | Batch_idx: 340 |  Loss_1: (0.1370) | Acc_1: (94.90%) (41421/43648)\n",
      "Epoch: 172 | Batch_idx: 350 |  Loss_1: (0.1372) | Acc_1: (94.90%) (42635/44928)\n",
      "Epoch: 172 | Batch_idx: 360 |  Loss_1: (0.1369) | Acc_1: (94.91%) (43856/46208)\n",
      "Epoch: 172 | Batch_idx: 370 |  Loss_1: (0.1372) | Acc_1: (94.90%) (45064/47488)\n",
      "Epoch: 172 | Batch_idx: 380 |  Loss_1: (0.1375) | Acc_1: (94.88%) (46270/48768)\n",
      "Epoch: 172 | Batch_idx: 390 |  Loss_1: (0.1373) | Acc_1: (94.89%) (47445/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2987) | Acc: (92.72%) (9272/10000)\n",
      "Epoch: 173 | Batch_idx: 0 |  Loss_1: (0.1669) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 173 | Batch_idx: 10 |  Loss_1: (0.1351) | Acc_1: (95.17%) (1340/1408)\n",
      "Epoch: 173 | Batch_idx: 20 |  Loss_1: (0.1369) | Acc_1: (95.01%) (2554/2688)\n",
      "Epoch: 173 | Batch_idx: 30 |  Loss_1: (0.1376) | Acc_1: (95.04%) (3771/3968)\n",
      "Epoch: 173 | Batch_idx: 40 |  Loss_1: (0.1370) | Acc_1: (95.06%) (4989/5248)\n",
      "Epoch: 173 | Batch_idx: 50 |  Loss_1: (0.1350) | Acc_1: (95.07%) (6206/6528)\n",
      "Epoch: 173 | Batch_idx: 60 |  Loss_1: (0.1336) | Acc_1: (95.11%) (7426/7808)\n",
      "Epoch: 173 | Batch_idx: 70 |  Loss_1: (0.1340) | Acc_1: (95.08%) (8641/9088)\n",
      "Epoch: 173 | Batch_idx: 80 |  Loss_1: (0.1325) | Acc_1: (95.14%) (9864/10368)\n",
      "Epoch: 173 | Batch_idx: 90 |  Loss_1: (0.1360) | Acc_1: (94.97%) (11062/11648)\n",
      "Epoch: 173 | Batch_idx: 100 |  Loss_1: (0.1372) | Acc_1: (94.90%) (12269/12928)\n",
      "Epoch: 173 | Batch_idx: 110 |  Loss_1: (0.1385) | Acc_1: (94.81%) (13470/14208)\n",
      "Epoch: 173 | Batch_idx: 120 |  Loss_1: (0.1381) | Acc_1: (94.80%) (14682/15488)\n",
      "Epoch: 173 | Batch_idx: 130 |  Loss_1: (0.1393) | Acc_1: (94.73%) (15884/16768)\n",
      "Epoch: 173 | Batch_idx: 140 |  Loss_1: (0.1388) | Acc_1: (94.76%) (17103/18048)\n",
      "Epoch: 173 | Batch_idx: 150 |  Loss_1: (0.1389) | Acc_1: (94.75%) (18314/19328)\n",
      "Epoch: 173 | Batch_idx: 160 |  Loss_1: (0.1398) | Acc_1: (94.73%) (19521/20608)\n",
      "Epoch: 173 | Batch_idx: 170 |  Loss_1: (0.1393) | Acc_1: (94.76%) (20740/21888)\n",
      "Epoch: 173 | Batch_idx: 180 |  Loss_1: (0.1387) | Acc_1: (94.79%) (21960/23168)\n",
      "Epoch: 173 | Batch_idx: 190 |  Loss_1: (0.1392) | Acc_1: (94.77%) (23169/24448)\n",
      "Epoch: 173 | Batch_idx: 200 |  Loss_1: (0.1394) | Acc_1: (94.75%) (24378/25728)\n",
      "Epoch: 173 | Batch_idx: 210 |  Loss_1: (0.1393) | Acc_1: (94.74%) (25587/27008)\n",
      "Epoch: 173 | Batch_idx: 220 |  Loss_1: (0.1395) | Acc_1: (94.75%) (26803/28288)\n",
      "Epoch: 173 | Batch_idx: 230 |  Loss_1: (0.1395) | Acc_1: (94.76%) (28019/29568)\n",
      "Epoch: 173 | Batch_idx: 240 |  Loss_1: (0.1380) | Acc_1: (94.82%) (29249/30848)\n",
      "Epoch: 173 | Batch_idx: 250 |  Loss_1: (0.1382) | Acc_1: (94.79%) (30455/32128)\n",
      "Epoch: 173 | Batch_idx: 260 |  Loss_1: (0.1376) | Acc_1: (94.81%) (31674/33408)\n",
      "Epoch: 173 | Batch_idx: 270 |  Loss_1: (0.1374) | Acc_1: (94.82%) (32892/34688)\n",
      "Epoch: 173 | Batch_idx: 280 |  Loss_1: (0.1380) | Acc_1: (94.80%) (34097/35968)\n",
      "Epoch: 173 | Batch_idx: 290 |  Loss_1: (0.1377) | Acc_1: (94.79%) (35309/37248)\n",
      "Epoch: 173 | Batch_idx: 300 |  Loss_1: (0.1371) | Acc_1: (94.83%) (36536/38528)\n",
      "Epoch: 173 | Batch_idx: 310 |  Loss_1: (0.1372) | Acc_1: (94.83%) (37749/39808)\n",
      "Epoch: 173 | Batch_idx: 320 |  Loss_1: (0.1364) | Acc_1: (94.87%) (38979/41088)\n",
      "Epoch: 173 | Batch_idx: 330 |  Loss_1: (0.1368) | Acc_1: (94.85%) (40184/42368)\n",
      "Epoch: 173 | Batch_idx: 340 |  Loss_1: (0.1364) | Acc_1: (94.85%) (41400/43648)\n",
      "Epoch: 173 | Batch_idx: 350 |  Loss_1: (0.1364) | Acc_1: (94.85%) (42614/44928)\n",
      "Epoch: 173 | Batch_idx: 360 |  Loss_1: (0.1363) | Acc_1: (94.85%) (43830/46208)\n",
      "Epoch: 173 | Batch_idx: 370 |  Loss_1: (0.1357) | Acc_1: (94.88%) (45055/47488)\n",
      "Epoch: 173 | Batch_idx: 380 |  Loss_1: (0.1352) | Acc_1: (94.91%) (46285/48768)\n",
      "Epoch: 173 | Batch_idx: 390 |  Loss_1: (0.1349) | Acc_1: (94.91%) (47457/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3158) | Acc: (92.47%) (9247/10000)\n",
      "Epoch: 174 | Batch_idx: 0 |  Loss_1: (0.0821) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 174 | Batch_idx: 10 |  Loss_1: (0.1236) | Acc_1: (95.45%) (1344/1408)\n",
      "Epoch: 174 | Batch_idx: 20 |  Loss_1: (0.1151) | Acc_1: (95.94%) (2579/2688)\n",
      "Epoch: 174 | Batch_idx: 30 |  Loss_1: (0.1182) | Acc_1: (95.87%) (3804/3968)\n",
      "Epoch: 174 | Batch_idx: 40 |  Loss_1: (0.1226) | Acc_1: (95.56%) (5015/5248)\n",
      "Epoch: 174 | Batch_idx: 50 |  Loss_1: (0.1206) | Acc_1: (95.60%) (6241/6528)\n",
      "Epoch: 174 | Batch_idx: 60 |  Loss_1: (0.1213) | Acc_1: (95.59%) (7464/7808)\n",
      "Epoch: 174 | Batch_idx: 70 |  Loss_1: (0.1274) | Acc_1: (95.32%) (8663/9088)\n",
      "Epoch: 174 | Batch_idx: 80 |  Loss_1: (0.1268) | Acc_1: (95.32%) (9883/10368)\n",
      "Epoch: 174 | Batch_idx: 90 |  Loss_1: (0.1292) | Acc_1: (95.23%) (11092/11648)\n",
      "Epoch: 174 | Batch_idx: 100 |  Loss_1: (0.1304) | Acc_1: (95.13%) (12298/12928)\n",
      "Epoch: 174 | Batch_idx: 110 |  Loss_1: (0.1310) | Acc_1: (95.09%) (13510/14208)\n",
      "Epoch: 174 | Batch_idx: 120 |  Loss_1: (0.1304) | Acc_1: (95.11%) (14730/15488)\n",
      "Epoch: 174 | Batch_idx: 130 |  Loss_1: (0.1288) | Acc_1: (95.17%) (15958/16768)\n",
      "Epoch: 174 | Batch_idx: 140 |  Loss_1: (0.1296) | Acc_1: (95.14%) (17171/18048)\n",
      "Epoch: 174 | Batch_idx: 150 |  Loss_1: (0.1283) | Acc_1: (95.18%) (18397/19328)\n",
      "Epoch: 174 | Batch_idx: 160 |  Loss_1: (0.1290) | Acc_1: (95.16%) (19611/20608)\n",
      "Epoch: 174 | Batch_idx: 170 |  Loss_1: (0.1293) | Acc_1: (95.16%) (20828/21888)\n",
      "Epoch: 174 | Batch_idx: 180 |  Loss_1: (0.1319) | Acc_1: (95.08%) (22029/23168)\n",
      "Epoch: 174 | Batch_idx: 190 |  Loss_1: (0.1306) | Acc_1: (95.15%) (23263/24448)\n",
      "Epoch: 174 | Batch_idx: 200 |  Loss_1: (0.1309) | Acc_1: (95.16%) (24483/25728)\n",
      "Epoch: 174 | Batch_idx: 210 |  Loss_1: (0.1307) | Acc_1: (95.16%) (25701/27008)\n",
      "Epoch: 174 | Batch_idx: 220 |  Loss_1: (0.1304) | Acc_1: (95.16%) (26918/28288)\n",
      "Epoch: 174 | Batch_idx: 230 |  Loss_1: (0.1301) | Acc_1: (95.15%) (28135/29568)\n",
      "Epoch: 174 | Batch_idx: 240 |  Loss_1: (0.1294) | Acc_1: (95.16%) (29355/30848)\n",
      "Epoch: 174 | Batch_idx: 250 |  Loss_1: (0.1288) | Acc_1: (95.19%) (30583/32128)\n",
      "Epoch: 174 | Batch_idx: 260 |  Loss_1: (0.1290) | Acc_1: (95.19%) (31800/33408)\n",
      "Epoch: 174 | Batch_idx: 270 |  Loss_1: (0.1299) | Acc_1: (95.16%) (33010/34688)\n",
      "Epoch: 174 | Batch_idx: 280 |  Loss_1: (0.1307) | Acc_1: (95.14%) (34219/35968)\n",
      "Epoch: 174 | Batch_idx: 290 |  Loss_1: (0.1306) | Acc_1: (95.15%) (35440/37248)\n",
      "Epoch: 174 | Batch_idx: 300 |  Loss_1: (0.1310) | Acc_1: (95.14%) (36656/38528)\n",
      "Epoch: 174 | Batch_idx: 310 |  Loss_1: (0.1320) | Acc_1: (95.11%) (37861/39808)\n",
      "Epoch: 174 | Batch_idx: 320 |  Loss_1: (0.1325) | Acc_1: (95.09%) (39071/41088)\n",
      "Epoch: 174 | Batch_idx: 330 |  Loss_1: (0.1335) | Acc_1: (95.06%) (40276/42368)\n",
      "Epoch: 174 | Batch_idx: 340 |  Loss_1: (0.1336) | Acc_1: (95.06%) (41493/43648)\n",
      "Epoch: 174 | Batch_idx: 350 |  Loss_1: (0.1338) | Acc_1: (95.04%) (42699/44928)\n",
      "Epoch: 174 | Batch_idx: 360 |  Loss_1: (0.1338) | Acc_1: (95.04%) (43917/46208)\n",
      "Epoch: 174 | Batch_idx: 370 |  Loss_1: (0.1337) | Acc_1: (95.04%) (45133/47488)\n",
      "Epoch: 174 | Batch_idx: 380 |  Loss_1: (0.1337) | Acc_1: (95.03%) (46346/48768)\n",
      "Epoch: 174 | Batch_idx: 390 |  Loss_1: (0.1340) | Acc_1: (95.03%) (47516/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3141) | Acc: (92.65%) (9265/10000)\n",
      "Epoch: 175 | Batch_idx: 0 |  Loss_1: (0.1161) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 175 | Batch_idx: 10 |  Loss_1: (0.1155) | Acc_1: (96.09%) (1353/1408)\n",
      "Epoch: 175 | Batch_idx: 20 |  Loss_1: (0.1225) | Acc_1: (95.42%) (2565/2688)\n",
      "Epoch: 175 | Batch_idx: 30 |  Loss_1: (0.1283) | Acc_1: (95.14%) (3775/3968)\n",
      "Epoch: 175 | Batch_idx: 40 |  Loss_1: (0.1266) | Acc_1: (95.18%) (4995/5248)\n",
      "Epoch: 175 | Batch_idx: 50 |  Loss_1: (0.1265) | Acc_1: (95.19%) (6214/6528)\n",
      "Epoch: 175 | Batch_idx: 60 |  Loss_1: (0.1256) | Acc_1: (95.22%) (7435/7808)\n",
      "Epoch: 175 | Batch_idx: 70 |  Loss_1: (0.1275) | Acc_1: (95.26%) (8657/9088)\n",
      "Epoch: 175 | Batch_idx: 80 |  Loss_1: (0.1308) | Acc_1: (95.13%) (9863/10368)\n",
      "Epoch: 175 | Batch_idx: 90 |  Loss_1: (0.1336) | Acc_1: (95.04%) (11070/11648)\n",
      "Epoch: 175 | Batch_idx: 100 |  Loss_1: (0.1305) | Acc_1: (95.16%) (12302/12928)\n",
      "Epoch: 175 | Batch_idx: 110 |  Loss_1: (0.1320) | Acc_1: (95.07%) (13508/14208)\n",
      "Epoch: 175 | Batch_idx: 120 |  Loss_1: (0.1336) | Acc_1: (95.04%) (14720/15488)\n",
      "Epoch: 175 | Batch_idx: 130 |  Loss_1: (0.1342) | Acc_1: (94.99%) (15928/16768)\n",
      "Epoch: 175 | Batch_idx: 140 |  Loss_1: (0.1339) | Acc_1: (95.00%) (17146/18048)\n",
      "Epoch: 175 | Batch_idx: 150 |  Loss_1: (0.1348) | Acc_1: (94.96%) (18353/19328)\n",
      "Epoch: 175 | Batch_idx: 160 |  Loss_1: (0.1352) | Acc_1: (94.94%) (19565/20608)\n",
      "Epoch: 175 | Batch_idx: 170 |  Loss_1: (0.1346) | Acc_1: (94.97%) (20786/21888)\n",
      "Epoch: 175 | Batch_idx: 180 |  Loss_1: (0.1336) | Acc_1: (95.02%) (22015/23168)\n",
      "Epoch: 175 | Batch_idx: 190 |  Loss_1: (0.1337) | Acc_1: (95.03%) (23232/24448)\n",
      "Epoch: 175 | Batch_idx: 200 |  Loss_1: (0.1348) | Acc_1: (94.99%) (24438/25728)\n",
      "Epoch: 175 | Batch_idx: 210 |  Loss_1: (0.1341) | Acc_1: (95.00%) (25658/27008)\n",
      "Epoch: 175 | Batch_idx: 220 |  Loss_1: (0.1342) | Acc_1: (95.00%) (26874/28288)\n",
      "Epoch: 175 | Batch_idx: 230 |  Loss_1: (0.1346) | Acc_1: (95.00%) (28089/29568)\n",
      "Epoch: 175 | Batch_idx: 240 |  Loss_1: (0.1355) | Acc_1: (94.99%) (29303/30848)\n",
      "Epoch: 175 | Batch_idx: 250 |  Loss_1: (0.1358) | Acc_1: (94.98%) (30516/32128)\n",
      "Epoch: 175 | Batch_idx: 260 |  Loss_1: (0.1365) | Acc_1: (94.95%) (31720/33408)\n",
      "Epoch: 175 | Batch_idx: 270 |  Loss_1: (0.1369) | Acc_1: (94.92%) (32926/34688)\n",
      "Epoch: 175 | Batch_idx: 280 |  Loss_1: (0.1369) | Acc_1: (94.92%) (34142/35968)\n",
      "Epoch: 175 | Batch_idx: 290 |  Loss_1: (0.1372) | Acc_1: (94.91%) (35353/37248)\n",
      "Epoch: 175 | Batch_idx: 300 |  Loss_1: (0.1383) | Acc_1: (94.86%) (36549/38528)\n",
      "Epoch: 175 | Batch_idx: 310 |  Loss_1: (0.1395) | Acc_1: (94.83%) (37750/39808)\n",
      "Epoch: 175 | Batch_idx: 320 |  Loss_1: (0.1396) | Acc_1: (94.82%) (38961/41088)\n",
      "Epoch: 175 | Batch_idx: 330 |  Loss_1: (0.1403) | Acc_1: (94.79%) (40162/42368)\n",
      "Epoch: 175 | Batch_idx: 340 |  Loss_1: (0.1412) | Acc_1: (94.77%) (41367/43648)\n",
      "Epoch: 175 | Batch_idx: 350 |  Loss_1: (0.1411) | Acc_1: (94.78%) (42582/44928)\n",
      "Epoch: 175 | Batch_idx: 360 |  Loss_1: (0.1411) | Acc_1: (94.76%) (43789/46208)\n",
      "Epoch: 175 | Batch_idx: 370 |  Loss_1: (0.1423) | Acc_1: (94.73%) (44985/47488)\n",
      "Epoch: 175 | Batch_idx: 380 |  Loss_1: (0.1423) | Acc_1: (94.74%) (46203/48768)\n",
      "Epoch: 175 | Batch_idx: 390 |  Loss_1: (0.1427) | Acc_1: (94.72%) (47362/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3584) | Acc: (92.23%) (9223/10000)\n",
      "Epoch: 176 | Batch_idx: 0 |  Loss_1: (0.1536) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 176 | Batch_idx: 10 |  Loss_1: (0.1348) | Acc_1: (94.82%) (1335/1408)\n",
      "Epoch: 176 | Batch_idx: 20 |  Loss_1: (0.1408) | Acc_1: (94.83%) (2549/2688)\n",
      "Epoch: 176 | Batch_idx: 30 |  Loss_1: (0.1416) | Acc_1: (94.83%) (3763/3968)\n",
      "Epoch: 176 | Batch_idx: 40 |  Loss_1: (0.1368) | Acc_1: (94.97%) (4984/5248)\n",
      "Epoch: 176 | Batch_idx: 50 |  Loss_1: (0.1430) | Acc_1: (94.75%) (6185/6528)\n",
      "Epoch: 176 | Batch_idx: 60 |  Loss_1: (0.1421) | Acc_1: (94.76%) (7399/7808)\n",
      "Epoch: 176 | Batch_idx: 70 |  Loss_1: (0.1426) | Acc_1: (94.73%) (8609/9088)\n",
      "Epoch: 176 | Batch_idx: 80 |  Loss_1: (0.1434) | Acc_1: (94.69%) (9817/10368)\n",
      "Epoch: 176 | Batch_idx: 90 |  Loss_1: (0.1441) | Acc_1: (94.68%) (11028/11648)\n",
      "Epoch: 176 | Batch_idx: 100 |  Loss_1: (0.1426) | Acc_1: (94.72%) (12246/12928)\n",
      "Epoch: 176 | Batch_idx: 110 |  Loss_1: (0.1407) | Acc_1: (94.80%) (13469/14208)\n",
      "Epoch: 176 | Batch_idx: 120 |  Loss_1: (0.1422) | Acc_1: (94.76%) (14677/15488)\n",
      "Epoch: 176 | Batch_idx: 130 |  Loss_1: (0.1419) | Acc_1: (94.75%) (15888/16768)\n",
      "Epoch: 176 | Batch_idx: 140 |  Loss_1: (0.1421) | Acc_1: (94.71%) (17094/18048)\n",
      "Epoch: 176 | Batch_idx: 150 |  Loss_1: (0.1408) | Acc_1: (94.76%) (18315/19328)\n",
      "Epoch: 176 | Batch_idx: 160 |  Loss_1: (0.1400) | Acc_1: (94.78%) (19533/20608)\n",
      "Epoch: 176 | Batch_idx: 170 |  Loss_1: (0.1393) | Acc_1: (94.80%) (20749/21888)\n",
      "Epoch: 176 | Batch_idx: 180 |  Loss_1: (0.1404) | Acc_1: (94.77%) (21956/23168)\n",
      "Epoch: 176 | Batch_idx: 190 |  Loss_1: (0.1407) | Acc_1: (94.77%) (23169/24448)\n",
      "Epoch: 176 | Batch_idx: 200 |  Loss_1: (0.1402) | Acc_1: (94.76%) (24381/25728)\n",
      "Epoch: 176 | Batch_idx: 210 |  Loss_1: (0.1402) | Acc_1: (94.78%) (25597/27008)\n",
      "Epoch: 176 | Batch_idx: 220 |  Loss_1: (0.1396) | Acc_1: (94.80%) (26816/28288)\n",
      "Epoch: 176 | Batch_idx: 230 |  Loss_1: (0.1394) | Acc_1: (94.80%) (28030/29568)\n",
      "Epoch: 176 | Batch_idx: 240 |  Loss_1: (0.1394) | Acc_1: (94.81%) (29246/30848)\n",
      "Epoch: 176 | Batch_idx: 250 |  Loss_1: (0.1390) | Acc_1: (94.82%) (30464/32128)\n",
      "Epoch: 176 | Batch_idx: 260 |  Loss_1: (0.1390) | Acc_1: (94.83%) (31681/33408)\n",
      "Epoch: 176 | Batch_idx: 270 |  Loss_1: (0.1389) | Acc_1: (94.83%) (32896/34688)\n",
      "Epoch: 176 | Batch_idx: 280 |  Loss_1: (0.1392) | Acc_1: (94.83%) (34109/35968)\n",
      "Epoch: 176 | Batch_idx: 290 |  Loss_1: (0.1393) | Acc_1: (94.83%) (35322/37248)\n",
      "Epoch: 176 | Batch_idx: 300 |  Loss_1: (0.1385) | Acc_1: (94.86%) (36549/38528)\n",
      "Epoch: 176 | Batch_idx: 310 |  Loss_1: (0.1383) | Acc_1: (94.87%) (37766/39808)\n",
      "Epoch: 176 | Batch_idx: 320 |  Loss_1: (0.1376) | Acc_1: (94.90%) (38991/41088)\n",
      "Epoch: 176 | Batch_idx: 330 |  Loss_1: (0.1380) | Acc_1: (94.87%) (40196/42368)\n",
      "Epoch: 176 | Batch_idx: 340 |  Loss_1: (0.1381) | Acc_1: (94.87%) (41410/43648)\n",
      "Epoch: 176 | Batch_idx: 350 |  Loss_1: (0.1382) | Acc_1: (94.86%) (42620/44928)\n",
      "Epoch: 176 | Batch_idx: 360 |  Loss_1: (0.1385) | Acc_1: (94.85%) (43830/46208)\n",
      "Epoch: 176 | Batch_idx: 370 |  Loss_1: (0.1387) | Acc_1: (94.85%) (45040/47488)\n",
      "Epoch: 176 | Batch_idx: 380 |  Loss_1: (0.1391) | Acc_1: (94.82%) (46243/48768)\n",
      "Epoch: 176 | Batch_idx: 390 |  Loss_1: (0.1394) | Acc_1: (94.80%) (47401/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3264) | Acc: (92.49%) (9249/10000)\n",
      "Epoch: 177 | Batch_idx: 0 |  Loss_1: (0.1325) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 177 | Batch_idx: 10 |  Loss_1: (0.1420) | Acc_1: (94.60%) (1332/1408)\n",
      "Epoch: 177 | Batch_idx: 20 |  Loss_1: (0.1481) | Acc_1: (94.16%) (2531/2688)\n",
      "Epoch: 177 | Batch_idx: 30 |  Loss_1: (0.1508) | Acc_1: (94.20%) (3738/3968)\n",
      "Epoch: 177 | Batch_idx: 40 |  Loss_1: (0.1451) | Acc_1: (94.42%) (4955/5248)\n",
      "Epoch: 177 | Batch_idx: 50 |  Loss_1: (0.1484) | Acc_1: (94.39%) (6162/6528)\n",
      "Epoch: 177 | Batch_idx: 60 |  Loss_1: (0.1497) | Acc_1: (94.35%) (7367/7808)\n",
      "Epoch: 177 | Batch_idx: 70 |  Loss_1: (0.1485) | Acc_1: (94.45%) (8584/9088)\n",
      "Epoch: 177 | Batch_idx: 80 |  Loss_1: (0.1489) | Acc_1: (94.43%) (9791/10368)\n",
      "Epoch: 177 | Batch_idx: 90 |  Loss_1: (0.1488) | Acc_1: (94.42%) (10998/11648)\n",
      "Epoch: 177 | Batch_idx: 100 |  Loss_1: (0.1477) | Acc_1: (94.42%) (12207/12928)\n",
      "Epoch: 177 | Batch_idx: 110 |  Loss_1: (0.1457) | Acc_1: (94.50%) (13426/14208)\n",
      "Epoch: 177 | Batch_idx: 120 |  Loss_1: (0.1438) | Acc_1: (94.57%) (14647/15488)\n",
      "Epoch: 177 | Batch_idx: 130 |  Loss_1: (0.1408) | Acc_1: (94.68%) (15876/16768)\n",
      "Epoch: 177 | Batch_idx: 140 |  Loss_1: (0.1418) | Acc_1: (94.66%) (17085/18048)\n",
      "Epoch: 177 | Batch_idx: 150 |  Loss_1: (0.1412) | Acc_1: (94.73%) (18309/19328)\n",
      "Epoch: 177 | Batch_idx: 160 |  Loss_1: (0.1418) | Acc_1: (94.71%) (19517/20608)\n",
      "Epoch: 177 | Batch_idx: 170 |  Loss_1: (0.1415) | Acc_1: (94.70%) (20728/21888)\n",
      "Epoch: 177 | Batch_idx: 180 |  Loss_1: (0.1398) | Acc_1: (94.78%) (21959/23168)\n",
      "Epoch: 177 | Batch_idx: 190 |  Loss_1: (0.1404) | Acc_1: (94.74%) (23163/24448)\n",
      "Epoch: 177 | Batch_idx: 200 |  Loss_1: (0.1400) | Acc_1: (94.78%) (24386/25728)\n",
      "Epoch: 177 | Batch_idx: 210 |  Loss_1: (0.1398) | Acc_1: (94.79%) (25602/27008)\n",
      "Epoch: 177 | Batch_idx: 220 |  Loss_1: (0.1399) | Acc_1: (94.79%) (26813/28288)\n",
      "Epoch: 177 | Batch_idx: 230 |  Loss_1: (0.1413) | Acc_1: (94.74%) (28012/29568)\n",
      "Epoch: 177 | Batch_idx: 240 |  Loss_1: (0.1411) | Acc_1: (94.77%) (29236/30848)\n",
      "Epoch: 177 | Batch_idx: 250 |  Loss_1: (0.1417) | Acc_1: (94.75%) (30441/32128)\n",
      "Epoch: 177 | Batch_idx: 260 |  Loss_1: (0.1427) | Acc_1: (94.73%) (31647/33408)\n",
      "Epoch: 177 | Batch_idx: 270 |  Loss_1: (0.1436) | Acc_1: (94.71%) (32852/34688)\n",
      "Epoch: 177 | Batch_idx: 280 |  Loss_1: (0.1436) | Acc_1: (94.70%) (34061/35968)\n",
      "Epoch: 177 | Batch_idx: 290 |  Loss_1: (0.1430) | Acc_1: (94.72%) (35281/37248)\n",
      "Epoch: 177 | Batch_idx: 300 |  Loss_1: (0.1431) | Acc_1: (94.72%) (36493/38528)\n",
      "Epoch: 177 | Batch_idx: 310 |  Loss_1: (0.1428) | Acc_1: (94.72%) (37706/39808)\n",
      "Epoch: 177 | Batch_idx: 320 |  Loss_1: (0.1424) | Acc_1: (94.75%) (38930/41088)\n",
      "Epoch: 177 | Batch_idx: 330 |  Loss_1: (0.1424) | Acc_1: (94.76%) (40146/42368)\n",
      "Epoch: 177 | Batch_idx: 340 |  Loss_1: (0.1427) | Acc_1: (94.75%) (41357/43648)\n",
      "Epoch: 177 | Batch_idx: 350 |  Loss_1: (0.1426) | Acc_1: (94.76%) (42573/44928)\n",
      "Epoch: 177 | Batch_idx: 360 |  Loss_1: (0.1429) | Acc_1: (94.74%) (43779/46208)\n",
      "Epoch: 177 | Batch_idx: 370 |  Loss_1: (0.1429) | Acc_1: (94.73%) (44987/47488)\n",
      "Epoch: 177 | Batch_idx: 380 |  Loss_1: (0.1431) | Acc_1: (94.73%) (46196/48768)\n",
      "Epoch: 177 | Batch_idx: 390 |  Loss_1: (0.1434) | Acc_1: (94.71%) (47357/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3480) | Acc: (92.18%) (9218/10000)\n",
      "Epoch: 178 | Batch_idx: 0 |  Loss_1: (0.1770) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 178 | Batch_idx: 10 |  Loss_1: (0.1518) | Acc_1: (94.18%) (1326/1408)\n",
      "Epoch: 178 | Batch_idx: 20 |  Loss_1: (0.1348) | Acc_1: (94.94%) (2552/2688)\n",
      "Epoch: 178 | Batch_idx: 30 |  Loss_1: (0.1405) | Acc_1: (94.63%) (3755/3968)\n",
      "Epoch: 178 | Batch_idx: 40 |  Loss_1: (0.1428) | Acc_1: (94.53%) (4961/5248)\n",
      "Epoch: 178 | Batch_idx: 50 |  Loss_1: (0.1488) | Acc_1: (94.44%) (6165/6528)\n",
      "Epoch: 178 | Batch_idx: 60 |  Loss_1: (0.1476) | Acc_1: (94.52%) (7380/7808)\n",
      "Epoch: 178 | Batch_idx: 70 |  Loss_1: (0.1479) | Acc_1: (94.49%) (8587/9088)\n",
      "Epoch: 178 | Batch_idx: 80 |  Loss_1: (0.1481) | Acc_1: (94.50%) (9798/10368)\n",
      "Epoch: 178 | Batch_idx: 90 |  Loss_1: (0.1466) | Acc_1: (94.58%) (11017/11648)\n",
      "Epoch: 178 | Batch_idx: 100 |  Loss_1: (0.1447) | Acc_1: (94.66%) (12238/12928)\n",
      "Epoch: 178 | Batch_idx: 110 |  Loss_1: (0.1431) | Acc_1: (94.71%) (13457/14208)\n",
      "Epoch: 178 | Batch_idx: 120 |  Loss_1: (0.1423) | Acc_1: (94.74%) (14674/15488)\n",
      "Epoch: 178 | Batch_idx: 130 |  Loss_1: (0.1425) | Acc_1: (94.75%) (15887/16768)\n",
      "Epoch: 178 | Batch_idx: 140 |  Loss_1: (0.1430) | Acc_1: (94.71%) (17093/18048)\n",
      "Epoch: 178 | Batch_idx: 150 |  Loss_1: (0.1432) | Acc_1: (94.70%) (18304/19328)\n",
      "Epoch: 178 | Batch_idx: 160 |  Loss_1: (0.1418) | Acc_1: (94.76%) (19529/20608)\n",
      "Epoch: 178 | Batch_idx: 170 |  Loss_1: (0.1413) | Acc_1: (94.75%) (20738/21888)\n",
      "Epoch: 178 | Batch_idx: 180 |  Loss_1: (0.1421) | Acc_1: (94.72%) (21945/23168)\n",
      "Epoch: 178 | Batch_idx: 190 |  Loss_1: (0.1419) | Acc_1: (94.74%) (23163/24448)\n",
      "Epoch: 178 | Batch_idx: 200 |  Loss_1: (0.1426) | Acc_1: (94.73%) (24372/25728)\n",
      "Epoch: 178 | Batch_idx: 210 |  Loss_1: (0.1415) | Acc_1: (94.77%) (25596/27008)\n",
      "Epoch: 178 | Batch_idx: 220 |  Loss_1: (0.1415) | Acc_1: (94.77%) (26808/28288)\n",
      "Epoch: 178 | Batch_idx: 230 |  Loss_1: (0.1403) | Acc_1: (94.82%) (28037/29568)\n",
      "Epoch: 178 | Batch_idx: 240 |  Loss_1: (0.1397) | Acc_1: (94.84%) (29257/30848)\n",
      "Epoch: 178 | Batch_idx: 250 |  Loss_1: (0.1392) | Acc_1: (94.85%) (30475/32128)\n",
      "Epoch: 178 | Batch_idx: 260 |  Loss_1: (0.1393) | Acc_1: (94.85%) (31686/33408)\n",
      "Epoch: 178 | Batch_idx: 270 |  Loss_1: (0.1384) | Acc_1: (94.87%) (32910/34688)\n",
      "Epoch: 178 | Batch_idx: 280 |  Loss_1: (0.1390) | Acc_1: (94.84%) (34112/35968)\n",
      "Epoch: 178 | Batch_idx: 290 |  Loss_1: (0.1398) | Acc_1: (94.81%) (35315/37248)\n",
      "Epoch: 178 | Batch_idx: 300 |  Loss_1: (0.1394) | Acc_1: (94.82%) (36531/38528)\n",
      "Epoch: 178 | Batch_idx: 310 |  Loss_1: (0.1390) | Acc_1: (94.83%) (37749/39808)\n",
      "Epoch: 178 | Batch_idx: 320 |  Loss_1: (0.1384) | Acc_1: (94.85%) (38973/41088)\n",
      "Epoch: 178 | Batch_idx: 330 |  Loss_1: (0.1381) | Acc_1: (94.88%) (40197/42368)\n",
      "Epoch: 178 | Batch_idx: 340 |  Loss_1: (0.1372) | Acc_1: (94.91%) (41426/43648)\n",
      "Epoch: 178 | Batch_idx: 350 |  Loss_1: (0.1376) | Acc_1: (94.91%) (42639/44928)\n",
      "Epoch: 178 | Batch_idx: 360 |  Loss_1: (0.1374) | Acc_1: (94.90%) (43852/46208)\n",
      "Epoch: 178 | Batch_idx: 370 |  Loss_1: (0.1374) | Acc_1: (94.91%) (45069/47488)\n",
      "Epoch: 178 | Batch_idx: 380 |  Loss_1: (0.1373) | Acc_1: (94.90%) (46282/48768)\n",
      "Epoch: 178 | Batch_idx: 390 |  Loss_1: (0.1373) | Acc_1: (94.90%) (47449/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3194) | Acc: (92.55%) (9255/10000)\n",
      "Epoch: 179 | Batch_idx: 0 |  Loss_1: (0.1900) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 179 | Batch_idx: 10 |  Loss_1: (0.1401) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 179 | Batch_idx: 20 |  Loss_1: (0.1476) | Acc_1: (94.49%) (2540/2688)\n",
      "Epoch: 179 | Batch_idx: 30 |  Loss_1: (0.1449) | Acc_1: (94.48%) (3749/3968)\n",
      "Epoch: 179 | Batch_idx: 40 |  Loss_1: (0.1374) | Acc_1: (94.84%) (4977/5248)\n",
      "Epoch: 179 | Batch_idx: 50 |  Loss_1: (0.1409) | Acc_1: (94.75%) (6185/6528)\n",
      "Epoch: 179 | Batch_idx: 60 |  Loss_1: (0.1362) | Acc_1: (94.86%) (7407/7808)\n",
      "Epoch: 179 | Batch_idx: 70 |  Loss_1: (0.1376) | Acc_1: (94.78%) (8614/9088)\n",
      "Epoch: 179 | Batch_idx: 80 |  Loss_1: (0.1385) | Acc_1: (94.82%) (9831/10368)\n",
      "Epoch: 179 | Batch_idx: 90 |  Loss_1: (0.1388) | Acc_1: (94.81%) (11044/11648)\n",
      "Epoch: 179 | Batch_idx: 100 |  Loss_1: (0.1383) | Acc_1: (94.79%) (12254/12928)\n",
      "Epoch: 179 | Batch_idx: 110 |  Loss_1: (0.1404) | Acc_1: (94.72%) (13458/14208)\n",
      "Epoch: 179 | Batch_idx: 120 |  Loss_1: (0.1412) | Acc_1: (94.71%) (14668/15488)\n",
      "Epoch: 179 | Batch_idx: 130 |  Loss_1: (0.1415) | Acc_1: (94.68%) (15876/16768)\n",
      "Epoch: 179 | Batch_idx: 140 |  Loss_1: (0.1402) | Acc_1: (94.71%) (17094/18048)\n",
      "Epoch: 179 | Batch_idx: 150 |  Loss_1: (0.1405) | Acc_1: (94.71%) (18305/19328)\n",
      "Epoch: 179 | Batch_idx: 160 |  Loss_1: (0.1420) | Acc_1: (94.64%) (19503/20608)\n",
      "Epoch: 179 | Batch_idx: 170 |  Loss_1: (0.1432) | Acc_1: (94.60%) (20705/21888)\n",
      "Epoch: 179 | Batch_idx: 180 |  Loss_1: (0.1426) | Acc_1: (94.62%) (21921/23168)\n",
      "Epoch: 179 | Batch_idx: 190 |  Loss_1: (0.1422) | Acc_1: (94.63%) (23136/24448)\n",
      "Epoch: 179 | Batch_idx: 200 |  Loss_1: (0.1415) | Acc_1: (94.66%) (24353/25728)\n",
      "Epoch: 179 | Batch_idx: 210 |  Loss_1: (0.1414) | Acc_1: (94.66%) (25567/27008)\n",
      "Epoch: 179 | Batch_idx: 220 |  Loss_1: (0.1401) | Acc_1: (94.72%) (26794/28288)\n",
      "Epoch: 179 | Batch_idx: 230 |  Loss_1: (0.1393) | Acc_1: (94.75%) (28015/29568)\n",
      "Epoch: 179 | Batch_idx: 240 |  Loss_1: (0.1403) | Acc_1: (94.73%) (29222/30848)\n",
      "Epoch: 179 | Batch_idx: 250 |  Loss_1: (0.1405) | Acc_1: (94.73%) (30434/32128)\n",
      "Epoch: 179 | Batch_idx: 260 |  Loss_1: (0.1409) | Acc_1: (94.73%) (31647/33408)\n",
      "Epoch: 179 | Batch_idx: 270 |  Loss_1: (0.1412) | Acc_1: (94.74%) (32863/34688)\n",
      "Epoch: 179 | Batch_idx: 280 |  Loss_1: (0.1411) | Acc_1: (94.73%) (34074/35968)\n",
      "Epoch: 179 | Batch_idx: 290 |  Loss_1: (0.1408) | Acc_1: (94.75%) (35292/37248)\n",
      "Epoch: 179 | Batch_idx: 300 |  Loss_1: (0.1413) | Acc_1: (94.72%) (36493/38528)\n",
      "Epoch: 179 | Batch_idx: 310 |  Loss_1: (0.1417) | Acc_1: (94.70%) (37698/39808)\n",
      "Epoch: 179 | Batch_idx: 320 |  Loss_1: (0.1413) | Acc_1: (94.71%) (38916/41088)\n",
      "Epoch: 179 | Batch_idx: 330 |  Loss_1: (0.1412) | Acc_1: (94.72%) (40133/42368)\n",
      "Epoch: 179 | Batch_idx: 340 |  Loss_1: (0.1402) | Acc_1: (94.76%) (41361/43648)\n",
      "Epoch: 179 | Batch_idx: 350 |  Loss_1: (0.1397) | Acc_1: (94.77%) (42580/44928)\n",
      "Epoch: 179 | Batch_idx: 360 |  Loss_1: (0.1395) | Acc_1: (94.78%) (43798/46208)\n",
      "Epoch: 179 | Batch_idx: 370 |  Loss_1: (0.1386) | Acc_1: (94.82%) (45029/47488)\n",
      "Epoch: 179 | Batch_idx: 380 |  Loss_1: (0.1393) | Acc_1: (94.79%) (46229/48768)\n",
      "Epoch: 179 | Batch_idx: 390 |  Loss_1: (0.1397) | Acc_1: (94.78%) (47388/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3529) | Acc: (92.41%) (9241/10000)\n",
      "Epoch: 180 | Batch_idx: 0 |  Loss_1: (0.0533) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 180 | Batch_idx: 10 |  Loss_1: (0.1220) | Acc_1: (95.53%) (1345/1408)\n",
      "Epoch: 180 | Batch_idx: 20 |  Loss_1: (0.1382) | Acc_1: (94.94%) (2552/2688)\n",
      "Epoch: 180 | Batch_idx: 30 |  Loss_1: (0.1261) | Acc_1: (95.41%) (3786/3968)\n",
      "Epoch: 180 | Batch_idx: 40 |  Loss_1: (0.1325) | Acc_1: (95.16%) (4994/5248)\n",
      "Epoch: 180 | Batch_idx: 50 |  Loss_1: (0.1376) | Acc_1: (95.05%) (6205/6528)\n",
      "Epoch: 180 | Batch_idx: 60 |  Loss_1: (0.1358) | Acc_1: (95.12%) (7427/7808)\n",
      "Epoch: 180 | Batch_idx: 70 |  Loss_1: (0.1388) | Acc_1: (95.00%) (8634/9088)\n",
      "Epoch: 180 | Batch_idx: 80 |  Loss_1: (0.1377) | Acc_1: (95.07%) (9857/10368)\n",
      "Epoch: 180 | Batch_idx: 90 |  Loss_1: (0.1334) | Acc_1: (95.20%) (11089/11648)\n",
      "Epoch: 180 | Batch_idx: 100 |  Loss_1: (0.1310) | Acc_1: (95.26%) (12315/12928)\n",
      "Epoch: 180 | Batch_idx: 110 |  Loss_1: (0.1301) | Acc_1: (95.31%) (13542/14208)\n",
      "Epoch: 180 | Batch_idx: 120 |  Loss_1: (0.1288) | Acc_1: (95.36%) (14770/15488)\n",
      "Epoch: 180 | Batch_idx: 130 |  Loss_1: (0.1290) | Acc_1: (95.38%) (15994/16768)\n",
      "Epoch: 180 | Batch_idx: 140 |  Loss_1: (0.1273) | Acc_1: (95.44%) (17225/18048)\n",
      "Epoch: 180 | Batch_idx: 150 |  Loss_1: (0.1274) | Acc_1: (95.43%) (18444/19328)\n",
      "Epoch: 180 | Batch_idx: 160 |  Loss_1: (0.1280) | Acc_1: (95.39%) (19659/20608)\n",
      "Epoch: 180 | Batch_idx: 170 |  Loss_1: (0.1271) | Acc_1: (95.43%) (20887/21888)\n",
      "Epoch: 180 | Batch_idx: 180 |  Loss_1: (0.1274) | Acc_1: (95.41%) (22104/23168)\n",
      "Epoch: 180 | Batch_idx: 190 |  Loss_1: (0.1273) | Acc_1: (95.40%) (23323/24448)\n",
      "Epoch: 180 | Batch_idx: 200 |  Loss_1: (0.1277) | Acc_1: (95.39%) (24541/25728)\n",
      "Epoch: 180 | Batch_idx: 210 |  Loss_1: (0.1281) | Acc_1: (95.38%) (25761/27008)\n",
      "Epoch: 180 | Batch_idx: 220 |  Loss_1: (0.1284) | Acc_1: (95.35%) (26972/28288)\n",
      "Epoch: 180 | Batch_idx: 230 |  Loss_1: (0.1282) | Acc_1: (95.34%) (28191/29568)\n",
      "Epoch: 180 | Batch_idx: 240 |  Loss_1: (0.1287) | Acc_1: (95.33%) (29408/30848)\n",
      "Epoch: 180 | Batch_idx: 250 |  Loss_1: (0.1291) | Acc_1: (95.32%) (30623/32128)\n",
      "Epoch: 180 | Batch_idx: 260 |  Loss_1: (0.1289) | Acc_1: (95.32%) (31846/33408)\n",
      "Epoch: 180 | Batch_idx: 270 |  Loss_1: (0.1283) | Acc_1: (95.33%) (33068/34688)\n",
      "Epoch: 180 | Batch_idx: 280 |  Loss_1: (0.1292) | Acc_1: (95.28%) (34270/35968)\n",
      "Epoch: 180 | Batch_idx: 290 |  Loss_1: (0.1301) | Acc_1: (95.25%) (35477/37248)\n",
      "Epoch: 180 | Batch_idx: 300 |  Loss_1: (0.1296) | Acc_1: (95.27%) (36707/38528)\n",
      "Epoch: 180 | Batch_idx: 310 |  Loss_1: (0.1289) | Acc_1: (95.29%) (37934/39808)\n",
      "Epoch: 180 | Batch_idx: 320 |  Loss_1: (0.1297) | Acc_1: (95.25%) (39138/41088)\n",
      "Epoch: 180 | Batch_idx: 330 |  Loss_1: (0.1298) | Acc_1: (95.24%) (40352/42368)\n",
      "Epoch: 180 | Batch_idx: 340 |  Loss_1: (0.1291) | Acc_1: (95.27%) (41585/43648)\n",
      "Epoch: 180 | Batch_idx: 350 |  Loss_1: (0.1289) | Acc_1: (95.27%) (42804/44928)\n",
      "Epoch: 180 | Batch_idx: 360 |  Loss_1: (0.1295) | Acc_1: (95.26%) (44017/46208)\n",
      "Epoch: 180 | Batch_idx: 370 |  Loss_1: (0.1295) | Acc_1: (95.26%) (45235/47488)\n",
      "Epoch: 180 | Batch_idx: 380 |  Loss_1: (0.1303) | Acc_1: (95.24%) (46446/48768)\n",
      "Epoch: 180 | Batch_idx: 390 |  Loss_1: (0.1305) | Acc_1: (95.22%) (47612/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3414) | Acc: (92.31%) (9231/10000)\n",
      "Epoch: 181 | Batch_idx: 0 |  Loss_1: (0.1055) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 181 | Batch_idx: 10 |  Loss_1: (0.1434) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 181 | Batch_idx: 20 |  Loss_1: (0.1315) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 181 | Batch_idx: 30 |  Loss_1: (0.1326) | Acc_1: (94.83%) (3763/3968)\n",
      "Epoch: 181 | Batch_idx: 40 |  Loss_1: (0.1352) | Acc_1: (94.76%) (4973/5248)\n",
      "Epoch: 181 | Batch_idx: 50 |  Loss_1: (0.1334) | Acc_1: (94.87%) (6193/6528)\n",
      "Epoch: 181 | Batch_idx: 60 |  Loss_1: (0.1308) | Acc_1: (94.94%) (7413/7808)\n",
      "Epoch: 181 | Batch_idx: 70 |  Loss_1: (0.1299) | Acc_1: (95.00%) (8634/9088)\n",
      "Epoch: 181 | Batch_idx: 80 |  Loss_1: (0.1321) | Acc_1: (94.98%) (9848/10368)\n",
      "Epoch: 181 | Batch_idx: 90 |  Loss_1: (0.1371) | Acc_1: (94.81%) (11043/11648)\n",
      "Epoch: 181 | Batch_idx: 100 |  Loss_1: (0.1351) | Acc_1: (94.90%) (12269/12928)\n",
      "Epoch: 181 | Batch_idx: 110 |  Loss_1: (0.1330) | Acc_1: (95.01%) (13499/14208)\n",
      "Epoch: 181 | Batch_idx: 120 |  Loss_1: (0.1355) | Acc_1: (94.95%) (14706/15488)\n",
      "Epoch: 181 | Batch_idx: 130 |  Loss_1: (0.1347) | Acc_1: (95.00%) (15929/16768)\n",
      "Epoch: 181 | Batch_idx: 140 |  Loss_1: (0.1356) | Acc_1: (94.93%) (17133/18048)\n",
      "Epoch: 181 | Batch_idx: 150 |  Loss_1: (0.1356) | Acc_1: (94.95%) (18352/19328)\n",
      "Epoch: 181 | Batch_idx: 160 |  Loss_1: (0.1356) | Acc_1: (94.96%) (19570/20608)\n",
      "Epoch: 181 | Batch_idx: 170 |  Loss_1: (0.1363) | Acc_1: (94.96%) (20784/21888)\n",
      "Epoch: 181 | Batch_idx: 180 |  Loss_1: (0.1364) | Acc_1: (94.95%) (21997/23168)\n",
      "Epoch: 181 | Batch_idx: 190 |  Loss_1: (0.1374) | Acc_1: (94.90%) (23201/24448)\n",
      "Epoch: 181 | Batch_idx: 200 |  Loss_1: (0.1372) | Acc_1: (94.91%) (24418/25728)\n",
      "Epoch: 181 | Batch_idx: 210 |  Loss_1: (0.1386) | Acc_1: (94.85%) (25617/27008)\n",
      "Epoch: 181 | Batch_idx: 220 |  Loss_1: (0.1383) | Acc_1: (94.87%) (26836/28288)\n",
      "Epoch: 181 | Batch_idx: 230 |  Loss_1: (0.1387) | Acc_1: (94.83%) (28039/29568)\n",
      "Epoch: 181 | Batch_idx: 240 |  Loss_1: (0.1381) | Acc_1: (94.86%) (29262/30848)\n",
      "Epoch: 181 | Batch_idx: 250 |  Loss_1: (0.1386) | Acc_1: (94.87%) (30481/32128)\n",
      "Epoch: 181 | Batch_idx: 260 |  Loss_1: (0.1394) | Acc_1: (94.84%) (31683/33408)\n",
      "Epoch: 181 | Batch_idx: 270 |  Loss_1: (0.1382) | Acc_1: (94.89%) (32914/34688)\n",
      "Epoch: 181 | Batch_idx: 280 |  Loss_1: (0.1378) | Acc_1: (94.89%) (34129/35968)\n",
      "Epoch: 181 | Batch_idx: 290 |  Loss_1: (0.1377) | Acc_1: (94.89%) (35344/37248)\n",
      "Epoch: 181 | Batch_idx: 300 |  Loss_1: (0.1375) | Acc_1: (94.90%) (36562/38528)\n",
      "Epoch: 181 | Batch_idx: 310 |  Loss_1: (0.1373) | Acc_1: (94.89%) (37775/39808)\n",
      "Epoch: 181 | Batch_idx: 320 |  Loss_1: (0.1368) | Acc_1: (94.89%) (38990/41088)\n",
      "Epoch: 181 | Batch_idx: 330 |  Loss_1: (0.1372) | Acc_1: (94.89%) (40201/42368)\n",
      "Epoch: 181 | Batch_idx: 340 |  Loss_1: (0.1371) | Acc_1: (94.89%) (41416/43648)\n",
      "Epoch: 181 | Batch_idx: 350 |  Loss_1: (0.1366) | Acc_1: (94.89%) (42631/44928)\n",
      "Epoch: 181 | Batch_idx: 360 |  Loss_1: (0.1359) | Acc_1: (94.91%) (43856/46208)\n",
      "Epoch: 181 | Batch_idx: 370 |  Loss_1: (0.1361) | Acc_1: (94.93%) (45078/47488)\n",
      "Epoch: 181 | Batch_idx: 380 |  Loss_1: (0.1367) | Acc_1: (94.90%) (46280/48768)\n",
      "Epoch: 181 | Batch_idx: 390 |  Loss_1: (0.1360) | Acc_1: (94.93%) (47463/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2984) | Acc: (93.13%) (9313/10000)\n",
      "Epoch: 182 | Batch_idx: 0 |  Loss_1: (0.0929) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 182 | Batch_idx: 10 |  Loss_1: (0.0898) | Acc_1: (96.52%) (1359/1408)\n",
      "Epoch: 182 | Batch_idx: 20 |  Loss_1: (0.1047) | Acc_1: (96.02%) (2581/2688)\n",
      "Epoch: 182 | Batch_idx: 30 |  Loss_1: (0.1144) | Acc_1: (95.72%) (3798/3968)\n",
      "Epoch: 182 | Batch_idx: 40 |  Loss_1: (0.1242) | Acc_1: (95.27%) (5000/5248)\n",
      "Epoch: 182 | Batch_idx: 50 |  Loss_1: (0.1215) | Acc_1: (95.42%) (6229/6528)\n",
      "Epoch: 182 | Batch_idx: 60 |  Loss_1: (0.1253) | Acc_1: (95.38%) (7447/7808)\n",
      "Epoch: 182 | Batch_idx: 70 |  Loss_1: (0.1265) | Acc_1: (95.30%) (8661/9088)\n",
      "Epoch: 182 | Batch_idx: 80 |  Loss_1: (0.1286) | Acc_1: (95.21%) (9871/10368)\n",
      "Epoch: 182 | Batch_idx: 90 |  Loss_1: (0.1272) | Acc_1: (95.26%) (11096/11648)\n",
      "Epoch: 182 | Batch_idx: 100 |  Loss_1: (0.1259) | Acc_1: (95.34%) (12326/12928)\n",
      "Epoch: 182 | Batch_idx: 110 |  Loss_1: (0.1277) | Acc_1: (95.26%) (13535/14208)\n",
      "Epoch: 182 | Batch_idx: 120 |  Loss_1: (0.1274) | Acc_1: (95.29%) (14759/15488)\n",
      "Epoch: 182 | Batch_idx: 130 |  Loss_1: (0.1286) | Acc_1: (95.25%) (15971/16768)\n",
      "Epoch: 182 | Batch_idx: 140 |  Loss_1: (0.1282) | Acc_1: (95.22%) (17186/18048)\n",
      "Epoch: 182 | Batch_idx: 150 |  Loss_1: (0.1277) | Acc_1: (95.22%) (18404/19328)\n",
      "Epoch: 182 | Batch_idx: 160 |  Loss_1: (0.1281) | Acc_1: (95.19%) (19616/20608)\n",
      "Epoch: 182 | Batch_idx: 170 |  Loss_1: (0.1278) | Acc_1: (95.21%) (20839/21888)\n",
      "Epoch: 182 | Batch_idx: 180 |  Loss_1: (0.1261) | Acc_1: (95.27%) (22072/23168)\n",
      "Epoch: 182 | Batch_idx: 190 |  Loss_1: (0.1269) | Acc_1: (95.24%) (23285/24448)\n",
      "Epoch: 182 | Batch_idx: 200 |  Loss_1: (0.1268) | Acc_1: (95.25%) (24507/25728)\n",
      "Epoch: 182 | Batch_idx: 210 |  Loss_1: (0.1266) | Acc_1: (95.29%) (25736/27008)\n",
      "Epoch: 182 | Batch_idx: 220 |  Loss_1: (0.1274) | Acc_1: (95.27%) (26950/28288)\n",
      "Epoch: 182 | Batch_idx: 230 |  Loss_1: (0.1282) | Acc_1: (95.23%) (28157/29568)\n",
      "Epoch: 182 | Batch_idx: 240 |  Loss_1: (0.1291) | Acc_1: (95.20%) (29368/30848)\n",
      "Epoch: 182 | Batch_idx: 250 |  Loss_1: (0.1297) | Acc_1: (95.17%) (30577/32128)\n",
      "Epoch: 182 | Batch_idx: 260 |  Loss_1: (0.1307) | Acc_1: (95.15%) (31787/33408)\n",
      "Epoch: 182 | Batch_idx: 270 |  Loss_1: (0.1317) | Acc_1: (95.09%) (32986/34688)\n",
      "Epoch: 182 | Batch_idx: 280 |  Loss_1: (0.1329) | Acc_1: (95.05%) (34186/35968)\n",
      "Epoch: 182 | Batch_idx: 290 |  Loss_1: (0.1333) | Acc_1: (95.03%) (35398/37248)\n",
      "Epoch: 182 | Batch_idx: 300 |  Loss_1: (0.1341) | Acc_1: (94.99%) (36599/38528)\n",
      "Epoch: 182 | Batch_idx: 310 |  Loss_1: (0.1346) | Acc_1: (94.97%) (37805/39808)\n",
      "Epoch: 182 | Batch_idx: 320 |  Loss_1: (0.1349) | Acc_1: (94.96%) (39017/41088)\n",
      "Epoch: 182 | Batch_idx: 330 |  Loss_1: (0.1356) | Acc_1: (94.93%) (40218/42368)\n",
      "Epoch: 182 | Batch_idx: 340 |  Loss_1: (0.1352) | Acc_1: (94.95%) (41443/43648)\n",
      "Epoch: 182 | Batch_idx: 350 |  Loss_1: (0.1353) | Acc_1: (94.94%) (42655/44928)\n",
      "Epoch: 182 | Batch_idx: 360 |  Loss_1: (0.1358) | Acc_1: (94.93%) (43864/46208)\n",
      "Epoch: 182 | Batch_idx: 370 |  Loss_1: (0.1357) | Acc_1: (94.94%) (45086/47488)\n",
      "Epoch: 182 | Batch_idx: 380 |  Loss_1: (0.1356) | Acc_1: (94.95%) (46305/48768)\n",
      "Epoch: 182 | Batch_idx: 390 |  Loss_1: (0.1361) | Acc_1: (94.92%) (47461/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3331) | Acc: (92.12%) (9212/10000)\n",
      "Epoch: 183 | Batch_idx: 0 |  Loss_1: (0.1578) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 183 | Batch_idx: 10 |  Loss_1: (0.1168) | Acc_1: (95.60%) (1346/1408)\n",
      "Epoch: 183 | Batch_idx: 20 |  Loss_1: (0.1248) | Acc_1: (95.35%) (2563/2688)\n",
      "Epoch: 183 | Batch_idx: 30 |  Loss_1: (0.1306) | Acc_1: (95.01%) (3770/3968)\n",
      "Epoch: 183 | Batch_idx: 40 |  Loss_1: (0.1373) | Acc_1: (94.70%) (4970/5248)\n",
      "Epoch: 183 | Batch_idx: 50 |  Loss_1: (0.1377) | Acc_1: (94.81%) (6189/6528)\n",
      "Epoch: 183 | Batch_idx: 60 |  Loss_1: (0.1359) | Acc_1: (94.84%) (7405/7808)\n",
      "Epoch: 183 | Batch_idx: 70 |  Loss_1: (0.1330) | Acc_1: (94.94%) (8628/9088)\n",
      "Epoch: 183 | Batch_idx: 80 |  Loss_1: (0.1331) | Acc_1: (94.96%) (9845/10368)\n",
      "Epoch: 183 | Batch_idx: 90 |  Loss_1: (0.1364) | Acc_1: (94.87%) (11050/11648)\n",
      "Epoch: 183 | Batch_idx: 100 |  Loss_1: (0.1357) | Acc_1: (94.89%) (12267/12928)\n",
      "Epoch: 183 | Batch_idx: 110 |  Loss_1: (0.1364) | Acc_1: (94.86%) (13478/14208)\n",
      "Epoch: 183 | Batch_idx: 120 |  Loss_1: (0.1354) | Acc_1: (94.91%) (14700/15488)\n",
      "Epoch: 183 | Batch_idx: 130 |  Loss_1: (0.1354) | Acc_1: (94.92%) (15916/16768)\n",
      "Epoch: 183 | Batch_idx: 140 |  Loss_1: (0.1365) | Acc_1: (94.89%) (17126/18048)\n",
      "Epoch: 183 | Batch_idx: 150 |  Loss_1: (0.1362) | Acc_1: (94.89%) (18341/19328)\n",
      "Epoch: 183 | Batch_idx: 160 |  Loss_1: (0.1368) | Acc_1: (94.89%) (19554/20608)\n",
      "Epoch: 183 | Batch_idx: 170 |  Loss_1: (0.1369) | Acc_1: (94.89%) (20770/21888)\n",
      "Epoch: 183 | Batch_idx: 180 |  Loss_1: (0.1369) | Acc_1: (94.88%) (21982/23168)\n",
      "Epoch: 183 | Batch_idx: 190 |  Loss_1: (0.1367) | Acc_1: (94.89%) (23199/24448)\n",
      "Epoch: 183 | Batch_idx: 200 |  Loss_1: (0.1373) | Acc_1: (94.85%) (24402/25728)\n",
      "Epoch: 183 | Batch_idx: 210 |  Loss_1: (0.1362) | Acc_1: (94.90%) (25630/27008)\n",
      "Epoch: 183 | Batch_idx: 220 |  Loss_1: (0.1353) | Acc_1: (94.93%) (26854/28288)\n",
      "Epoch: 183 | Batch_idx: 230 |  Loss_1: (0.1339) | Acc_1: (94.98%) (28083/29568)\n",
      "Epoch: 183 | Batch_idx: 240 |  Loss_1: (0.1338) | Acc_1: (94.98%) (29300/30848)\n",
      "Epoch: 183 | Batch_idx: 250 |  Loss_1: (0.1336) | Acc_1: (95.00%) (30521/32128)\n",
      "Epoch: 183 | Batch_idx: 260 |  Loss_1: (0.1337) | Acc_1: (94.99%) (31734/33408)\n",
      "Epoch: 183 | Batch_idx: 270 |  Loss_1: (0.1336) | Acc_1: (94.99%) (32949/34688)\n",
      "Epoch: 183 | Batch_idx: 280 |  Loss_1: (0.1333) | Acc_1: (95.02%) (34175/35968)\n",
      "Epoch: 183 | Batch_idx: 290 |  Loss_1: (0.1338) | Acc_1: (94.99%) (35383/37248)\n",
      "Epoch: 183 | Batch_idx: 300 |  Loss_1: (0.1336) | Acc_1: (94.99%) (36596/38528)\n",
      "Epoch: 183 | Batch_idx: 310 |  Loss_1: (0.1334) | Acc_1: (95.00%) (37818/39808)\n",
      "Epoch: 183 | Batch_idx: 320 |  Loss_1: (0.1335) | Acc_1: (94.98%) (39027/41088)\n",
      "Epoch: 183 | Batch_idx: 330 |  Loss_1: (0.1338) | Acc_1: (94.97%) (40236/42368)\n",
      "Epoch: 183 | Batch_idx: 340 |  Loss_1: (0.1340) | Acc_1: (94.98%) (41455/43648)\n",
      "Epoch: 183 | Batch_idx: 350 |  Loss_1: (0.1340) | Acc_1: (94.98%) (42672/44928)\n",
      "Epoch: 183 | Batch_idx: 360 |  Loss_1: (0.1340) | Acc_1: (94.99%) (43895/46208)\n",
      "Epoch: 183 | Batch_idx: 370 |  Loss_1: (0.1351) | Acc_1: (94.95%) (45088/47488)\n",
      "Epoch: 183 | Batch_idx: 380 |  Loss_1: (0.1352) | Acc_1: (94.94%) (46298/48768)\n",
      "Epoch: 183 | Batch_idx: 390 |  Loss_1: (0.1352) | Acc_1: (94.94%) (47471/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3365) | Acc: (92.24%) (9224/10000)\n",
      "Epoch: 184 | Batch_idx: 0 |  Loss_1: (0.2065) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 184 | Batch_idx: 10 |  Loss_1: (0.1393) | Acc_1: (95.24%) (1341/1408)\n",
      "Epoch: 184 | Batch_idx: 20 |  Loss_1: (0.1422) | Acc_1: (94.90%) (2551/2688)\n",
      "Epoch: 184 | Batch_idx: 30 |  Loss_1: (0.1436) | Acc_1: (94.88%) (3765/3968)\n",
      "Epoch: 184 | Batch_idx: 40 |  Loss_1: (0.1477) | Acc_1: (94.63%) (4966/5248)\n",
      "Epoch: 184 | Batch_idx: 50 |  Loss_1: (0.1505) | Acc_1: (94.49%) (6168/6528)\n",
      "Epoch: 184 | Batch_idx: 60 |  Loss_1: (0.1482) | Acc_1: (94.51%) (7379/7808)\n",
      "Epoch: 184 | Batch_idx: 70 |  Loss_1: (0.1471) | Acc_1: (94.52%) (8590/9088)\n",
      "Epoch: 184 | Batch_idx: 80 |  Loss_1: (0.1456) | Acc_1: (94.57%) (9805/10368)\n",
      "Epoch: 184 | Batch_idx: 90 |  Loss_1: (0.1443) | Acc_1: (94.63%) (11022/11648)\n",
      "Epoch: 184 | Batch_idx: 100 |  Loss_1: (0.1478) | Acc_1: (94.52%) (12219/12928)\n",
      "Epoch: 184 | Batch_idx: 110 |  Loss_1: (0.1475) | Acc_1: (94.51%) (13428/14208)\n",
      "Epoch: 184 | Batch_idx: 120 |  Loss_1: (0.1467) | Acc_1: (94.54%) (14643/15488)\n",
      "Epoch: 184 | Batch_idx: 130 |  Loss_1: (0.1470) | Acc_1: (94.52%) (15849/16768)\n",
      "Epoch: 184 | Batch_idx: 140 |  Loss_1: (0.1463) | Acc_1: (94.54%) (17062/18048)\n",
      "Epoch: 184 | Batch_idx: 150 |  Loss_1: (0.1476) | Acc_1: (94.49%) (18263/19328)\n",
      "Epoch: 184 | Batch_idx: 160 |  Loss_1: (0.1469) | Acc_1: (94.54%) (19482/20608)\n",
      "Epoch: 184 | Batch_idx: 170 |  Loss_1: (0.1463) | Acc_1: (94.57%) (20699/21888)\n",
      "Epoch: 184 | Batch_idx: 180 |  Loss_1: (0.1475) | Acc_1: (94.53%) (21900/23168)\n",
      "Epoch: 184 | Batch_idx: 190 |  Loss_1: (0.1468) | Acc_1: (94.55%) (23115/24448)\n",
      "Epoch: 184 | Batch_idx: 200 |  Loss_1: (0.1460) | Acc_1: (94.59%) (24336/25728)\n",
      "Epoch: 184 | Batch_idx: 210 |  Loss_1: (0.1453) | Acc_1: (94.61%) (25551/27008)\n",
      "Epoch: 184 | Batch_idx: 220 |  Loss_1: (0.1446) | Acc_1: (94.62%) (26767/28288)\n",
      "Epoch: 184 | Batch_idx: 230 |  Loss_1: (0.1443) | Acc_1: (94.64%) (27984/29568)\n",
      "Epoch: 184 | Batch_idx: 240 |  Loss_1: (0.1437) | Acc_1: (94.68%) (29208/30848)\n",
      "Epoch: 184 | Batch_idx: 250 |  Loss_1: (0.1446) | Acc_1: (94.68%) (30419/32128)\n",
      "Epoch: 184 | Batch_idx: 260 |  Loss_1: (0.1444) | Acc_1: (94.68%) (31632/33408)\n",
      "Epoch: 184 | Batch_idx: 270 |  Loss_1: (0.1442) | Acc_1: (94.68%) (32842/34688)\n",
      "Epoch: 184 | Batch_idx: 280 |  Loss_1: (0.1432) | Acc_1: (94.70%) (34063/35968)\n",
      "Epoch: 184 | Batch_idx: 290 |  Loss_1: (0.1433) | Acc_1: (94.71%) (35279/37248)\n",
      "Epoch: 184 | Batch_idx: 300 |  Loss_1: (0.1420) | Acc_1: (94.76%) (36508/38528)\n",
      "Epoch: 184 | Batch_idx: 310 |  Loss_1: (0.1410) | Acc_1: (94.80%) (37737/39808)\n",
      "Epoch: 184 | Batch_idx: 320 |  Loss_1: (0.1413) | Acc_1: (94.80%) (38950/41088)\n",
      "Epoch: 184 | Batch_idx: 330 |  Loss_1: (0.1414) | Acc_1: (94.80%) (40165/42368)\n",
      "Epoch: 184 | Batch_idx: 340 |  Loss_1: (0.1412) | Acc_1: (94.80%) (41378/43648)\n",
      "Epoch: 184 | Batch_idx: 350 |  Loss_1: (0.1422) | Acc_1: (94.75%) (42571/44928)\n",
      "Epoch: 184 | Batch_idx: 360 |  Loss_1: (0.1423) | Acc_1: (94.75%) (43784/46208)\n",
      "Epoch: 184 | Batch_idx: 370 |  Loss_1: (0.1425) | Acc_1: (94.74%) (44990/47488)\n",
      "Epoch: 184 | Batch_idx: 380 |  Loss_1: (0.1420) | Acc_1: (94.76%) (46214/48768)\n",
      "Epoch: 184 | Batch_idx: 390 |  Loss_1: (0.1414) | Acc_1: (94.78%) (47390/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3176) | Acc: (92.71%) (9271/10000)\n",
      "Epoch: 185 | Batch_idx: 0 |  Loss_1: (0.0818) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 185 | Batch_idx: 10 |  Loss_1: (0.1286) | Acc_1: (95.53%) (1345/1408)\n",
      "Epoch: 185 | Batch_idx: 20 |  Loss_1: (0.1204) | Acc_1: (95.80%) (2575/2688)\n",
      "Epoch: 185 | Batch_idx: 30 |  Loss_1: (0.1232) | Acc_1: (95.56%) (3792/3968)\n",
      "Epoch: 185 | Batch_idx: 40 |  Loss_1: (0.1269) | Acc_1: (95.41%) (5007/5248)\n",
      "Epoch: 185 | Batch_idx: 50 |  Loss_1: (0.1248) | Acc_1: (95.42%) (6229/6528)\n",
      "Epoch: 185 | Batch_idx: 60 |  Loss_1: (0.1240) | Acc_1: (95.47%) (7454/7808)\n",
      "Epoch: 185 | Batch_idx: 70 |  Loss_1: (0.1268) | Acc_1: (95.37%) (8667/9088)\n",
      "Epoch: 185 | Batch_idx: 80 |  Loss_1: (0.1263) | Acc_1: (95.44%) (9895/10368)\n",
      "Epoch: 185 | Batch_idx: 90 |  Loss_1: (0.1237) | Acc_1: (95.54%) (11128/11648)\n",
      "Epoch: 185 | Batch_idx: 100 |  Loss_1: (0.1216) | Acc_1: (95.61%) (12361/12928)\n",
      "Epoch: 185 | Batch_idx: 110 |  Loss_1: (0.1238) | Acc_1: (95.54%) (13574/14208)\n",
      "Epoch: 185 | Batch_idx: 120 |  Loss_1: (0.1250) | Acc_1: (95.49%) (14790/15488)\n",
      "Epoch: 185 | Batch_idx: 130 |  Loss_1: (0.1270) | Acc_1: (95.41%) (15999/16768)\n",
      "Epoch: 185 | Batch_idx: 140 |  Loss_1: (0.1287) | Acc_1: (95.36%) (17211/18048)\n",
      "Epoch: 185 | Batch_idx: 150 |  Loss_1: (0.1294) | Acc_1: (95.35%) (18430/19328)\n",
      "Epoch: 185 | Batch_idx: 160 |  Loss_1: (0.1279) | Acc_1: (95.40%) (19661/20608)\n",
      "Epoch: 185 | Batch_idx: 170 |  Loss_1: (0.1290) | Acc_1: (95.35%) (20871/21888)\n",
      "Epoch: 185 | Batch_idx: 180 |  Loss_1: (0.1284) | Acc_1: (95.35%) (22090/23168)\n",
      "Epoch: 185 | Batch_idx: 190 |  Loss_1: (0.1281) | Acc_1: (95.36%) (23313/24448)\n",
      "Epoch: 185 | Batch_idx: 200 |  Loss_1: (0.1276) | Acc_1: (95.37%) (24536/25728)\n",
      "Epoch: 185 | Batch_idx: 210 |  Loss_1: (0.1268) | Acc_1: (95.39%) (25763/27008)\n",
      "Epoch: 185 | Batch_idx: 220 |  Loss_1: (0.1273) | Acc_1: (95.37%) (26979/28288)\n",
      "Epoch: 185 | Batch_idx: 230 |  Loss_1: (0.1279) | Acc_1: (95.34%) (28190/29568)\n",
      "Epoch: 185 | Batch_idx: 240 |  Loss_1: (0.1295) | Acc_1: (95.26%) (29387/30848)\n",
      "Epoch: 185 | Batch_idx: 250 |  Loss_1: (0.1296) | Acc_1: (95.26%) (30605/32128)\n",
      "Epoch: 185 | Batch_idx: 260 |  Loss_1: (0.1292) | Acc_1: (95.28%) (31830/33408)\n",
      "Epoch: 185 | Batch_idx: 270 |  Loss_1: (0.1280) | Acc_1: (95.30%) (33059/34688)\n",
      "Epoch: 185 | Batch_idx: 280 |  Loss_1: (0.1283) | Acc_1: (95.30%) (34276/35968)\n",
      "Epoch: 185 | Batch_idx: 290 |  Loss_1: (0.1294) | Acc_1: (95.26%) (35481/37248)\n",
      "Epoch: 185 | Batch_idx: 300 |  Loss_1: (0.1298) | Acc_1: (95.24%) (36695/38528)\n",
      "Epoch: 185 | Batch_idx: 310 |  Loss_1: (0.1307) | Acc_1: (95.21%) (37900/39808)\n",
      "Epoch: 185 | Batch_idx: 320 |  Loss_1: (0.1302) | Acc_1: (95.22%) (39126/41088)\n",
      "Epoch: 185 | Batch_idx: 330 |  Loss_1: (0.1307) | Acc_1: (95.20%) (40336/42368)\n",
      "Epoch: 185 | Batch_idx: 340 |  Loss_1: (0.1311) | Acc_1: (95.19%) (41548/43648)\n",
      "Epoch: 185 | Batch_idx: 350 |  Loss_1: (0.1308) | Acc_1: (95.21%) (42775/44928)\n",
      "Epoch: 185 | Batch_idx: 360 |  Loss_1: (0.1308) | Acc_1: (95.20%) (43990/46208)\n",
      "Epoch: 185 | Batch_idx: 370 |  Loss_1: (0.1310) | Acc_1: (95.18%) (45200/47488)\n",
      "Epoch: 185 | Batch_idx: 380 |  Loss_1: (0.1316) | Acc_1: (95.17%) (46411/48768)\n",
      "Epoch: 185 | Batch_idx: 390 |  Loss_1: (0.1315) | Acc_1: (95.17%) (47583/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.2934) | Acc: (93.09%) (9309/10000)\n",
      "Epoch: 186 | Batch_idx: 0 |  Loss_1: (0.2132) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 186 | Batch_idx: 10 |  Loss_1: (0.1163) | Acc_1: (95.60%) (1346/1408)\n",
      "Epoch: 186 | Batch_idx: 20 |  Loss_1: (0.1214) | Acc_1: (95.50%) (2567/2688)\n",
      "Epoch: 186 | Batch_idx: 30 |  Loss_1: (0.1238) | Acc_1: (95.29%) (3781/3968)\n",
      "Epoch: 186 | Batch_idx: 40 |  Loss_1: (0.1232) | Acc_1: (95.29%) (5001/5248)\n",
      "Epoch: 186 | Batch_idx: 50 |  Loss_1: (0.1253) | Acc_1: (95.21%) (6215/6528)\n",
      "Epoch: 186 | Batch_idx: 60 |  Loss_1: (0.1252) | Acc_1: (95.33%) (7443/7808)\n",
      "Epoch: 186 | Batch_idx: 70 |  Loss_1: (0.1266) | Acc_1: (95.32%) (8663/9088)\n",
      "Epoch: 186 | Batch_idx: 80 |  Loss_1: (0.1266) | Acc_1: (95.28%) (9879/10368)\n",
      "Epoch: 186 | Batch_idx: 90 |  Loss_1: (0.1243) | Acc_1: (95.34%) (11105/11648)\n",
      "Epoch: 186 | Batch_idx: 100 |  Loss_1: (0.1231) | Acc_1: (95.37%) (12329/12928)\n",
      "Epoch: 186 | Batch_idx: 110 |  Loss_1: (0.1239) | Acc_1: (95.35%) (13547/14208)\n",
      "Epoch: 186 | Batch_idx: 120 |  Loss_1: (0.1234) | Acc_1: (95.34%) (14766/15488)\n",
      "Epoch: 186 | Batch_idx: 130 |  Loss_1: (0.1226) | Acc_1: (95.37%) (15991/16768)\n",
      "Epoch: 186 | Batch_idx: 140 |  Loss_1: (0.1239) | Acc_1: (95.32%) (17204/18048)\n",
      "Epoch: 186 | Batch_idx: 150 |  Loss_1: (0.1231) | Acc_1: (95.37%) (18433/19328)\n",
      "Epoch: 186 | Batch_idx: 160 |  Loss_1: (0.1238) | Acc_1: (95.38%) (19656/20608)\n",
      "Epoch: 186 | Batch_idx: 170 |  Loss_1: (0.1250) | Acc_1: (95.34%) (20868/21888)\n",
      "Epoch: 186 | Batch_idx: 180 |  Loss_1: (0.1241) | Acc_1: (95.39%) (22100/23168)\n",
      "Epoch: 186 | Batch_idx: 190 |  Loss_1: (0.1240) | Acc_1: (95.39%) (23321/24448)\n",
      "Epoch: 186 | Batch_idx: 200 |  Loss_1: (0.1229) | Acc_1: (95.43%) (24553/25728)\n",
      "Epoch: 186 | Batch_idx: 210 |  Loss_1: (0.1232) | Acc_1: (95.42%) (25770/27008)\n",
      "Epoch: 186 | Batch_idx: 220 |  Loss_1: (0.1258) | Acc_1: (95.31%) (26962/28288)\n",
      "Epoch: 186 | Batch_idx: 230 |  Loss_1: (0.1258) | Acc_1: (95.31%) (28180/29568)\n",
      "Epoch: 186 | Batch_idx: 240 |  Loss_1: (0.1258) | Acc_1: (95.30%) (29398/30848)\n",
      "Epoch: 186 | Batch_idx: 250 |  Loss_1: (0.1262) | Acc_1: (95.30%) (30617/32128)\n",
      "Epoch: 186 | Batch_idx: 260 |  Loss_1: (0.1268) | Acc_1: (95.29%) (31836/33408)\n",
      "Epoch: 186 | Batch_idx: 270 |  Loss_1: (0.1277) | Acc_1: (95.26%) (33044/34688)\n",
      "Epoch: 186 | Batch_idx: 280 |  Loss_1: (0.1272) | Acc_1: (95.28%) (34269/35968)\n",
      "Epoch: 186 | Batch_idx: 290 |  Loss_1: (0.1272) | Acc_1: (95.27%) (35488/37248)\n",
      "Epoch: 186 | Batch_idx: 300 |  Loss_1: (0.1275) | Acc_1: (95.28%) (36708/38528)\n",
      "Epoch: 186 | Batch_idx: 310 |  Loss_1: (0.1285) | Acc_1: (95.25%) (37916/39808)\n",
      "Epoch: 186 | Batch_idx: 320 |  Loss_1: (0.1282) | Acc_1: (95.24%) (39134/41088)\n",
      "Epoch: 186 | Batch_idx: 330 |  Loss_1: (0.1278) | Acc_1: (95.25%) (40356/42368)\n",
      "Epoch: 186 | Batch_idx: 340 |  Loss_1: (0.1274) | Acc_1: (95.26%) (41580/43648)\n",
      "Epoch: 186 | Batch_idx: 350 |  Loss_1: (0.1267) | Acc_1: (95.29%) (42811/44928)\n",
      "Epoch: 186 | Batch_idx: 360 |  Loss_1: (0.1266) | Acc_1: (95.29%) (44030/46208)\n",
      "Epoch: 186 | Batch_idx: 370 |  Loss_1: (0.1270) | Acc_1: (95.29%) (45249/47488)\n",
      "Epoch: 186 | Batch_idx: 380 |  Loss_1: (0.1270) | Acc_1: (95.28%) (46467/48768)\n",
      "Epoch: 186 | Batch_idx: 390 |  Loss_1: (0.1277) | Acc_1: (95.24%) (47622/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3249) | Acc: (92.35%) (9235/10000)\n",
      "Epoch: 187 | Batch_idx: 0 |  Loss_1: (0.1244) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 187 | Batch_idx: 10 |  Loss_1: (0.1093) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 187 | Batch_idx: 20 |  Loss_1: (0.1238) | Acc_1: (95.31%) (2562/2688)\n",
      "Epoch: 187 | Batch_idx: 30 |  Loss_1: (0.1256) | Acc_1: (95.24%) (3779/3968)\n",
      "Epoch: 187 | Batch_idx: 40 |  Loss_1: (0.1301) | Acc_1: (95.03%) (4987/5248)\n",
      "Epoch: 187 | Batch_idx: 50 |  Loss_1: (0.1284) | Acc_1: (95.24%) (6217/6528)\n",
      "Epoch: 187 | Batch_idx: 60 |  Loss_1: (0.1281) | Acc_1: (95.22%) (7435/7808)\n",
      "Epoch: 187 | Batch_idx: 70 |  Loss_1: (0.1292) | Acc_1: (95.24%) (8655/9088)\n",
      "Epoch: 187 | Batch_idx: 80 |  Loss_1: (0.1286) | Acc_1: (95.20%) (9870/10368)\n",
      "Epoch: 187 | Batch_idx: 90 |  Loss_1: (0.1263) | Acc_1: (95.28%) (11098/11648)\n",
      "Epoch: 187 | Batch_idx: 100 |  Loss_1: (0.1291) | Acc_1: (95.24%) (12312/12928)\n",
      "Epoch: 187 | Batch_idx: 110 |  Loss_1: (0.1285) | Acc_1: (95.27%) (13536/14208)\n",
      "Epoch: 187 | Batch_idx: 120 |  Loss_1: (0.1280) | Acc_1: (95.25%) (14752/15488)\n",
      "Epoch: 187 | Batch_idx: 130 |  Loss_1: (0.1276) | Acc_1: (95.26%) (15973/16768)\n",
      "Epoch: 187 | Batch_idx: 140 |  Loss_1: (0.1282) | Acc_1: (95.21%) (17184/18048)\n",
      "Epoch: 187 | Batch_idx: 150 |  Loss_1: (0.1278) | Acc_1: (95.24%) (18408/19328)\n",
      "Epoch: 187 | Batch_idx: 160 |  Loss_1: (0.1289) | Acc_1: (95.22%) (19622/20608)\n",
      "Epoch: 187 | Batch_idx: 170 |  Loss_1: (0.1294) | Acc_1: (95.18%) (20833/21888)\n",
      "Epoch: 187 | Batch_idx: 180 |  Loss_1: (0.1288) | Acc_1: (95.19%) (22053/23168)\n",
      "Epoch: 187 | Batch_idx: 190 |  Loss_1: (0.1287) | Acc_1: (95.19%) (23271/24448)\n",
      "Epoch: 187 | Batch_idx: 200 |  Loss_1: (0.1304) | Acc_1: (95.13%) (24474/25728)\n",
      "Epoch: 187 | Batch_idx: 210 |  Loss_1: (0.1300) | Acc_1: (95.14%) (25695/27008)\n",
      "Epoch: 187 | Batch_idx: 220 |  Loss_1: (0.1306) | Acc_1: (95.11%) (26904/28288)\n",
      "Epoch: 187 | Batch_idx: 230 |  Loss_1: (0.1308) | Acc_1: (95.09%) (28115/29568)\n",
      "Epoch: 187 | Batch_idx: 240 |  Loss_1: (0.1300) | Acc_1: (95.11%) (29340/30848)\n",
      "Epoch: 187 | Batch_idx: 250 |  Loss_1: (0.1308) | Acc_1: (95.11%) (30556/32128)\n",
      "Epoch: 187 | Batch_idx: 260 |  Loss_1: (0.1309) | Acc_1: (95.11%) (31775/33408)\n",
      "Epoch: 187 | Batch_idx: 270 |  Loss_1: (0.1307) | Acc_1: (95.14%) (33001/34688)\n",
      "Epoch: 187 | Batch_idx: 280 |  Loss_1: (0.1311) | Acc_1: (95.14%) (34219/35968)\n",
      "Epoch: 187 | Batch_idx: 290 |  Loss_1: (0.1301) | Acc_1: (95.18%) (35451/37248)\n",
      "Epoch: 187 | Batch_idx: 300 |  Loss_1: (0.1308) | Acc_1: (95.14%) (36657/38528)\n",
      "Epoch: 187 | Batch_idx: 310 |  Loss_1: (0.1305) | Acc_1: (95.15%) (37879/39808)\n",
      "Epoch: 187 | Batch_idx: 320 |  Loss_1: (0.1303) | Acc_1: (95.17%) (39102/41088)\n",
      "Epoch: 187 | Batch_idx: 330 |  Loss_1: (0.1305) | Acc_1: (95.15%) (40313/42368)\n",
      "Epoch: 187 | Batch_idx: 340 |  Loss_1: (0.1313) | Acc_1: (95.11%) (41512/43648)\n",
      "Epoch: 187 | Batch_idx: 350 |  Loss_1: (0.1317) | Acc_1: (95.10%) (42726/44928)\n",
      "Epoch: 187 | Batch_idx: 360 |  Loss_1: (0.1322) | Acc_1: (95.07%) (43932/46208)\n",
      "Epoch: 187 | Batch_idx: 370 |  Loss_1: (0.1331) | Acc_1: (95.04%) (45133/47488)\n",
      "Epoch: 187 | Batch_idx: 380 |  Loss_1: (0.1332) | Acc_1: (95.03%) (46343/48768)\n",
      "Epoch: 187 | Batch_idx: 390 |  Loss_1: (0.1335) | Acc_1: (95.02%) (47509/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3245) | Acc: (92.27%) (9227/10000)\n",
      "Epoch: 188 | Batch_idx: 0 |  Loss_1: (0.0881) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 188 | Batch_idx: 10 |  Loss_1: (0.1145) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 188 | Batch_idx: 20 |  Loss_1: (0.1320) | Acc_1: (95.16%) (2558/2688)\n",
      "Epoch: 188 | Batch_idx: 30 |  Loss_1: (0.1295) | Acc_1: (95.19%) (3777/3968)\n",
      "Epoch: 188 | Batch_idx: 40 |  Loss_1: (0.1304) | Acc_1: (95.08%) (4990/5248)\n",
      "Epoch: 188 | Batch_idx: 50 |  Loss_1: (0.1280) | Acc_1: (95.13%) (6210/6528)\n",
      "Epoch: 188 | Batch_idx: 60 |  Loss_1: (0.1288) | Acc_1: (95.15%) (7429/7808)\n",
      "Epoch: 188 | Batch_idx: 70 |  Loss_1: (0.1331) | Acc_1: (94.99%) (8633/9088)\n",
      "Epoch: 188 | Batch_idx: 80 |  Loss_1: (0.1319) | Acc_1: (95.01%) (9851/10368)\n",
      "Epoch: 188 | Batch_idx: 90 |  Loss_1: (0.1334) | Acc_1: (94.99%) (11065/11648)\n",
      "Epoch: 188 | Batch_idx: 100 |  Loss_1: (0.1335) | Acc_1: (95.00%) (12282/12928)\n",
      "Epoch: 188 | Batch_idx: 110 |  Loss_1: (0.1335) | Acc_1: (95.03%) (13502/14208)\n",
      "Epoch: 188 | Batch_idx: 120 |  Loss_1: (0.1336) | Acc_1: (95.03%) (14718/15488)\n",
      "Epoch: 188 | Batch_idx: 130 |  Loss_1: (0.1333) | Acc_1: (95.03%) (15935/16768)\n",
      "Epoch: 188 | Batch_idx: 140 |  Loss_1: (0.1341) | Acc_1: (95.01%) (17148/18048)\n",
      "Epoch: 188 | Batch_idx: 150 |  Loss_1: (0.1331) | Acc_1: (95.06%) (18373/19328)\n",
      "Epoch: 188 | Batch_idx: 160 |  Loss_1: (0.1332) | Acc_1: (95.04%) (19585/20608)\n",
      "Epoch: 188 | Batch_idx: 170 |  Loss_1: (0.1324) | Acc_1: (95.08%) (20811/21888)\n",
      "Epoch: 188 | Batch_idx: 180 |  Loss_1: (0.1330) | Acc_1: (95.05%) (22021/23168)\n",
      "Epoch: 188 | Batch_idx: 190 |  Loss_1: (0.1323) | Acc_1: (95.08%) (23245/24448)\n",
      "Epoch: 188 | Batch_idx: 200 |  Loss_1: (0.1332) | Acc_1: (95.06%) (24456/25728)\n",
      "Epoch: 188 | Batch_idx: 210 |  Loss_1: (0.1328) | Acc_1: (95.08%) (25679/27008)\n",
      "Epoch: 188 | Batch_idx: 220 |  Loss_1: (0.1326) | Acc_1: (95.09%) (26899/28288)\n",
      "Epoch: 188 | Batch_idx: 230 |  Loss_1: (0.1344) | Acc_1: (95.02%) (28095/29568)\n",
      "Epoch: 188 | Batch_idx: 240 |  Loss_1: (0.1339) | Acc_1: (95.05%) (29322/30848)\n",
      "Epoch: 188 | Batch_idx: 250 |  Loss_1: (0.1333) | Acc_1: (95.06%) (30542/32128)\n",
      "Epoch: 188 | Batch_idx: 260 |  Loss_1: (0.1336) | Acc_1: (95.06%) (31758/33408)\n",
      "Epoch: 188 | Batch_idx: 270 |  Loss_1: (0.1344) | Acc_1: (95.02%) (32962/34688)\n",
      "Epoch: 188 | Batch_idx: 280 |  Loss_1: (0.1344) | Acc_1: (95.03%) (34181/35968)\n",
      "Epoch: 188 | Batch_idx: 290 |  Loss_1: (0.1350) | Acc_1: (95.02%) (35392/37248)\n",
      "Epoch: 188 | Batch_idx: 300 |  Loss_1: (0.1358) | Acc_1: (94.98%) (36595/38528)\n",
      "Epoch: 188 | Batch_idx: 310 |  Loss_1: (0.1368) | Acc_1: (94.95%) (37799/39808)\n",
      "Epoch: 188 | Batch_idx: 320 |  Loss_1: (0.1379) | Acc_1: (94.91%) (38997/41088)\n",
      "Epoch: 188 | Batch_idx: 330 |  Loss_1: (0.1380) | Acc_1: (94.90%) (40208/42368)\n",
      "Epoch: 188 | Batch_idx: 340 |  Loss_1: (0.1378) | Acc_1: (94.90%) (41423/43648)\n",
      "Epoch: 188 | Batch_idx: 350 |  Loss_1: (0.1381) | Acc_1: (94.90%) (42636/44928)\n",
      "Epoch: 188 | Batch_idx: 360 |  Loss_1: (0.1377) | Acc_1: (94.91%) (43857/46208)\n",
      "Epoch: 188 | Batch_idx: 370 |  Loss_1: (0.1376) | Acc_1: (94.92%) (45076/47488)\n",
      "Epoch: 188 | Batch_idx: 380 |  Loss_1: (0.1377) | Acc_1: (94.91%) (46288/48768)\n",
      "Epoch: 188 | Batch_idx: 390 |  Loss_1: (0.1377) | Acc_1: (94.92%) (47458/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3107) | Acc: (92.30%) (9230/10000)\n",
      "Epoch: 189 | Batch_idx: 0 |  Loss_1: (0.1428) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 189 | Batch_idx: 10 |  Loss_1: (0.1339) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 189 | Batch_idx: 20 |  Loss_1: (0.1257) | Acc_1: (95.13%) (2557/2688)\n",
      "Epoch: 189 | Batch_idx: 30 |  Loss_1: (0.1239) | Acc_1: (95.19%) (3777/3968)\n",
      "Epoch: 189 | Batch_idx: 40 |  Loss_1: (0.1246) | Acc_1: (95.20%) (4996/5248)\n",
      "Epoch: 189 | Batch_idx: 50 |  Loss_1: (0.1264) | Acc_1: (95.19%) (6214/6528)\n",
      "Epoch: 189 | Batch_idx: 60 |  Loss_1: (0.1274) | Acc_1: (95.20%) (7433/7808)\n",
      "Epoch: 189 | Batch_idx: 70 |  Loss_1: (0.1281) | Acc_1: (95.15%) (8647/9088)\n",
      "Epoch: 189 | Batch_idx: 80 |  Loss_1: (0.1283) | Acc_1: (95.19%) (9869/10368)\n",
      "Epoch: 189 | Batch_idx: 90 |  Loss_1: (0.1275) | Acc_1: (95.24%) (11093/11648)\n",
      "Epoch: 189 | Batch_idx: 100 |  Loss_1: (0.1267) | Acc_1: (95.27%) (12316/12928)\n",
      "Epoch: 189 | Batch_idx: 110 |  Loss_1: (0.1283) | Acc_1: (95.19%) (13525/14208)\n",
      "Epoch: 189 | Batch_idx: 120 |  Loss_1: (0.1279) | Acc_1: (95.22%) (14747/15488)\n",
      "Epoch: 189 | Batch_idx: 130 |  Loss_1: (0.1298) | Acc_1: (95.14%) (15953/16768)\n",
      "Epoch: 189 | Batch_idx: 140 |  Loss_1: (0.1298) | Acc_1: (95.14%) (17170/18048)\n",
      "Epoch: 189 | Batch_idx: 150 |  Loss_1: (0.1312) | Acc_1: (95.08%) (18377/19328)\n",
      "Epoch: 189 | Batch_idx: 160 |  Loss_1: (0.1326) | Acc_1: (95.03%) (19584/20608)\n",
      "Epoch: 189 | Batch_idx: 170 |  Loss_1: (0.1332) | Acc_1: (94.98%) (20790/21888)\n",
      "Epoch: 189 | Batch_idx: 180 |  Loss_1: (0.1323) | Acc_1: (95.03%) (22017/23168)\n",
      "Epoch: 189 | Batch_idx: 190 |  Loss_1: (0.1335) | Acc_1: (94.99%) (23222/24448)\n",
      "Epoch: 189 | Batch_idx: 200 |  Loss_1: (0.1327) | Acc_1: (95.01%) (24444/25728)\n",
      "Epoch: 189 | Batch_idx: 210 |  Loss_1: (0.1334) | Acc_1: (94.98%) (25651/27008)\n",
      "Epoch: 189 | Batch_idx: 220 |  Loss_1: (0.1344) | Acc_1: (94.94%) (26857/28288)\n",
      "Epoch: 189 | Batch_idx: 230 |  Loss_1: (0.1348) | Acc_1: (94.92%) (28067/29568)\n",
      "Epoch: 189 | Batch_idx: 240 |  Loss_1: (0.1367) | Acc_1: (94.86%) (29262/30848)\n",
      "Epoch: 189 | Batch_idx: 250 |  Loss_1: (0.1372) | Acc_1: (94.87%) (30480/32128)\n",
      "Epoch: 189 | Batch_idx: 260 |  Loss_1: (0.1373) | Acc_1: (94.88%) (31697/33408)\n",
      "Epoch: 189 | Batch_idx: 270 |  Loss_1: (0.1380) | Acc_1: (94.84%) (32898/34688)\n",
      "Epoch: 189 | Batch_idx: 280 |  Loss_1: (0.1373) | Acc_1: (94.86%) (34119/35968)\n",
      "Epoch: 189 | Batch_idx: 290 |  Loss_1: (0.1377) | Acc_1: (94.87%) (35337/37248)\n",
      "Epoch: 189 | Batch_idx: 300 |  Loss_1: (0.1374) | Acc_1: (94.87%) (36553/38528)\n",
      "Epoch: 189 | Batch_idx: 310 |  Loss_1: (0.1375) | Acc_1: (94.88%) (37769/39808)\n",
      "Epoch: 189 | Batch_idx: 320 |  Loss_1: (0.1375) | Acc_1: (94.87%) (38981/41088)\n",
      "Epoch: 189 | Batch_idx: 330 |  Loss_1: (0.1371) | Acc_1: (94.88%) (40200/42368)\n",
      "Epoch: 189 | Batch_idx: 340 |  Loss_1: (0.1369) | Acc_1: (94.89%) (41418/43648)\n",
      "Epoch: 189 | Batch_idx: 350 |  Loss_1: (0.1366) | Acc_1: (94.90%) (42637/44928)\n",
      "Epoch: 189 | Batch_idx: 360 |  Loss_1: (0.1369) | Acc_1: (94.88%) (43844/46208)\n",
      "Epoch: 189 | Batch_idx: 370 |  Loss_1: (0.1366) | Acc_1: (94.90%) (45067/47488)\n",
      "Epoch: 189 | Batch_idx: 380 |  Loss_1: (0.1363) | Acc_1: (94.91%) (46285/48768)\n",
      "Epoch: 189 | Batch_idx: 390 |  Loss_1: (0.1364) | Acc_1: (94.90%) (47451/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3285) | Acc: (92.44%) (9244/10000)\n",
      "Epoch: 190 | Batch_idx: 0 |  Loss_1: (0.1230) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 190 | Batch_idx: 10 |  Loss_1: (0.1343) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 190 | Batch_idx: 20 |  Loss_1: (0.1413) | Acc_1: (94.94%) (2552/2688)\n",
      "Epoch: 190 | Batch_idx: 30 |  Loss_1: (0.1472) | Acc_1: (94.83%) (3763/3968)\n",
      "Epoch: 190 | Batch_idx: 40 |  Loss_1: (0.1429) | Acc_1: (94.97%) (4984/5248)\n",
      "Epoch: 190 | Batch_idx: 50 |  Loss_1: (0.1397) | Acc_1: (95.08%) (6207/6528)\n",
      "Epoch: 190 | Batch_idx: 60 |  Loss_1: (0.1365) | Acc_1: (95.09%) (7425/7808)\n",
      "Epoch: 190 | Batch_idx: 70 |  Loss_1: (0.1383) | Acc_1: (95.00%) (8634/9088)\n",
      "Epoch: 190 | Batch_idx: 80 |  Loss_1: (0.1363) | Acc_1: (95.04%) (9854/10368)\n",
      "Epoch: 190 | Batch_idx: 90 |  Loss_1: (0.1342) | Acc_1: (95.12%) (11080/11648)\n",
      "Epoch: 190 | Batch_idx: 100 |  Loss_1: (0.1338) | Acc_1: (95.10%) (12295/12928)\n",
      "Epoch: 190 | Batch_idx: 110 |  Loss_1: (0.1341) | Acc_1: (95.06%) (13506/14208)\n",
      "Epoch: 190 | Batch_idx: 120 |  Loss_1: (0.1366) | Acc_1: (94.96%) (14708/15488)\n",
      "Epoch: 190 | Batch_idx: 130 |  Loss_1: (0.1384) | Acc_1: (94.88%) (15909/16768)\n",
      "Epoch: 190 | Batch_idx: 140 |  Loss_1: (0.1374) | Acc_1: (94.93%) (17133/18048)\n",
      "Epoch: 190 | Batch_idx: 150 |  Loss_1: (0.1357) | Acc_1: (94.98%) (18358/19328)\n",
      "Epoch: 190 | Batch_idx: 160 |  Loss_1: (0.1358) | Acc_1: (94.98%) (19574/20608)\n",
      "Epoch: 190 | Batch_idx: 170 |  Loss_1: (0.1378) | Acc_1: (94.90%) (20772/21888)\n",
      "Epoch: 190 | Batch_idx: 180 |  Loss_1: (0.1379) | Acc_1: (94.88%) (21982/23168)\n",
      "Epoch: 190 | Batch_idx: 190 |  Loss_1: (0.1372) | Acc_1: (94.92%) (23205/24448)\n",
      "Epoch: 190 | Batch_idx: 200 |  Loss_1: (0.1385) | Acc_1: (94.88%) (24411/25728)\n",
      "Epoch: 190 | Batch_idx: 210 |  Loss_1: (0.1377) | Acc_1: (94.91%) (25634/27008)\n",
      "Epoch: 190 | Batch_idx: 220 |  Loss_1: (0.1380) | Acc_1: (94.88%) (26841/28288)\n",
      "Epoch: 190 | Batch_idx: 230 |  Loss_1: (0.1377) | Acc_1: (94.89%) (28058/29568)\n",
      "Epoch: 190 | Batch_idx: 240 |  Loss_1: (0.1369) | Acc_1: (94.94%) (29286/30848)\n",
      "Epoch: 190 | Batch_idx: 250 |  Loss_1: (0.1364) | Acc_1: (94.95%) (30507/32128)\n",
      "Epoch: 190 | Batch_idx: 260 |  Loss_1: (0.1366) | Acc_1: (94.94%) (31719/33408)\n",
      "Epoch: 190 | Batch_idx: 270 |  Loss_1: (0.1364) | Acc_1: (94.94%) (32932/34688)\n",
      "Epoch: 190 | Batch_idx: 280 |  Loss_1: (0.1369) | Acc_1: (94.93%) (34143/35968)\n",
      "Epoch: 190 | Batch_idx: 290 |  Loss_1: (0.1377) | Acc_1: (94.90%) (35350/37248)\n",
      "Epoch: 190 | Batch_idx: 300 |  Loss_1: (0.1372) | Acc_1: (94.93%) (36576/38528)\n",
      "Epoch: 190 | Batch_idx: 310 |  Loss_1: (0.1358) | Acc_1: (94.99%) (37812/39808)\n",
      "Epoch: 190 | Batch_idx: 320 |  Loss_1: (0.1359) | Acc_1: (94.99%) (39028/41088)\n",
      "Epoch: 190 | Batch_idx: 330 |  Loss_1: (0.1357) | Acc_1: (95.00%) (40249/42368)\n",
      "Epoch: 190 | Batch_idx: 340 |  Loss_1: (0.1360) | Acc_1: (94.99%) (41461/43648)\n",
      "Epoch: 190 | Batch_idx: 350 |  Loss_1: (0.1363) | Acc_1: (94.97%) (42670/44928)\n",
      "Epoch: 190 | Batch_idx: 360 |  Loss_1: (0.1366) | Acc_1: (94.95%) (43875/46208)\n",
      "Epoch: 190 | Batch_idx: 370 |  Loss_1: (0.1376) | Acc_1: (94.91%) (45072/47488)\n",
      "Epoch: 190 | Batch_idx: 380 |  Loss_1: (0.1373) | Acc_1: (94.92%) (46293/48768)\n",
      "Epoch: 190 | Batch_idx: 390 |  Loss_1: (0.1367) | Acc_1: (94.95%) (47477/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3385) | Acc: (92.45%) (9245/10000)\n",
      "Epoch: 191 | Batch_idx: 0 |  Loss_1: (0.1125) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 191 | Batch_idx: 10 |  Loss_1: (0.1353) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 191 | Batch_idx: 20 |  Loss_1: (0.1380) | Acc_1: (94.38%) (2537/2688)\n",
      "Epoch: 191 | Batch_idx: 30 |  Loss_1: (0.1442) | Acc_1: (94.18%) (3737/3968)\n",
      "Epoch: 191 | Batch_idx: 40 |  Loss_1: (0.1465) | Acc_1: (94.23%) (4945/5248)\n",
      "Epoch: 191 | Batch_idx: 50 |  Loss_1: (0.1499) | Acc_1: (94.18%) (6148/6528)\n",
      "Epoch: 191 | Batch_idx: 60 |  Loss_1: (0.1439) | Acc_1: (94.52%) (7380/7808)\n",
      "Epoch: 191 | Batch_idx: 70 |  Loss_1: (0.1411) | Acc_1: (94.62%) (8599/9088)\n",
      "Epoch: 191 | Batch_idx: 80 |  Loss_1: (0.1424) | Acc_1: (94.57%) (9805/10368)\n",
      "Epoch: 191 | Batch_idx: 90 |  Loss_1: (0.1412) | Acc_1: (94.60%) (11019/11648)\n",
      "Epoch: 191 | Batch_idx: 100 |  Loss_1: (0.1433) | Acc_1: (94.56%) (12225/12928)\n",
      "Epoch: 191 | Batch_idx: 110 |  Loss_1: (0.1449) | Acc_1: (94.53%) (13431/14208)\n",
      "Epoch: 191 | Batch_idx: 120 |  Loss_1: (0.1430) | Acc_1: (94.58%) (14649/15488)\n",
      "Epoch: 191 | Batch_idx: 130 |  Loss_1: (0.1433) | Acc_1: (94.58%) (15860/16768)\n",
      "Epoch: 191 | Batch_idx: 140 |  Loss_1: (0.1430) | Acc_1: (94.59%) (17072/18048)\n",
      "Epoch: 191 | Batch_idx: 150 |  Loss_1: (0.1444) | Acc_1: (94.55%) (18275/19328)\n",
      "Epoch: 191 | Batch_idx: 160 |  Loss_1: (0.1449) | Acc_1: (94.58%) (19492/20608)\n",
      "Epoch: 191 | Batch_idx: 170 |  Loss_1: (0.1443) | Acc_1: (94.64%) (20714/21888)\n",
      "Epoch: 191 | Batch_idx: 180 |  Loss_1: (0.1441) | Acc_1: (94.63%) (21924/23168)\n",
      "Epoch: 191 | Batch_idx: 190 |  Loss_1: (0.1424) | Acc_1: (94.69%) (23151/24448)\n",
      "Epoch: 191 | Batch_idx: 200 |  Loss_1: (0.1422) | Acc_1: (94.72%) (24370/25728)\n",
      "Epoch: 191 | Batch_idx: 210 |  Loss_1: (0.1426) | Acc_1: (94.70%) (25577/27008)\n",
      "Epoch: 191 | Batch_idx: 220 |  Loss_1: (0.1420) | Acc_1: (94.73%) (26796/28288)\n",
      "Epoch: 191 | Batch_idx: 230 |  Loss_1: (0.1422) | Acc_1: (94.70%) (28001/29568)\n",
      "Epoch: 191 | Batch_idx: 240 |  Loss_1: (0.1429) | Acc_1: (94.68%) (29208/30848)\n",
      "Epoch: 191 | Batch_idx: 250 |  Loss_1: (0.1428) | Acc_1: (94.69%) (30423/32128)\n",
      "Epoch: 191 | Batch_idx: 260 |  Loss_1: (0.1431) | Acc_1: (94.67%) (31626/33408)\n",
      "Epoch: 191 | Batch_idx: 270 |  Loss_1: (0.1432) | Acc_1: (94.65%) (32832/34688)\n",
      "Epoch: 191 | Batch_idx: 280 |  Loss_1: (0.1433) | Acc_1: (94.64%) (34039/35968)\n",
      "Epoch: 191 | Batch_idx: 290 |  Loss_1: (0.1428) | Acc_1: (94.65%) (35254/37248)\n",
      "Epoch: 191 | Batch_idx: 300 |  Loss_1: (0.1424) | Acc_1: (94.66%) (36469/38528)\n",
      "Epoch: 191 | Batch_idx: 310 |  Loss_1: (0.1425) | Acc_1: (94.65%) (37678/39808)\n",
      "Epoch: 191 | Batch_idx: 320 |  Loss_1: (0.1417) | Acc_1: (94.68%) (38904/41088)\n",
      "Epoch: 191 | Batch_idx: 330 |  Loss_1: (0.1422) | Acc_1: (94.67%) (40109/42368)\n",
      "Epoch: 191 | Batch_idx: 340 |  Loss_1: (0.1423) | Acc_1: (94.66%) (41317/43648)\n",
      "Epoch: 191 | Batch_idx: 350 |  Loss_1: (0.1417) | Acc_1: (94.68%) (42540/44928)\n",
      "Epoch: 191 | Batch_idx: 360 |  Loss_1: (0.1419) | Acc_1: (94.68%) (43751/46208)\n",
      "Epoch: 191 | Batch_idx: 370 |  Loss_1: (0.1418) | Acc_1: (94.71%) (44974/47488)\n",
      "Epoch: 191 | Batch_idx: 380 |  Loss_1: (0.1421) | Acc_1: (94.70%) (46185/48768)\n",
      "Epoch: 191 | Batch_idx: 390 |  Loss_1: (0.1418) | Acc_1: (94.71%) (47357/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3236) | Acc: (92.32%) (9232/10000)\n",
      "Epoch: 192 | Batch_idx: 0 |  Loss_1: (0.1674) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 192 | Batch_idx: 10 |  Loss_1: (0.1337) | Acc_1: (95.03%) (1338/1408)\n",
      "Epoch: 192 | Batch_idx: 20 |  Loss_1: (0.1281) | Acc_1: (95.46%) (2566/2688)\n",
      "Epoch: 192 | Batch_idx: 30 |  Loss_1: (0.1310) | Acc_1: (95.19%) (3777/3968)\n",
      "Epoch: 192 | Batch_idx: 40 |  Loss_1: (0.1409) | Acc_1: (94.87%) (4979/5248)\n",
      "Epoch: 192 | Batch_idx: 50 |  Loss_1: (0.1374) | Acc_1: (95.01%) (6202/6528)\n",
      "Epoch: 192 | Batch_idx: 60 |  Loss_1: (0.1352) | Acc_1: (95.04%) (7421/7808)\n",
      "Epoch: 192 | Batch_idx: 70 |  Loss_1: (0.1329) | Acc_1: (95.14%) (8646/9088)\n",
      "Epoch: 192 | Batch_idx: 80 |  Loss_1: (0.1347) | Acc_1: (95.01%) (9851/10368)\n",
      "Epoch: 192 | Batch_idx: 90 |  Loss_1: (0.1324) | Acc_1: (95.09%) (11076/11648)\n",
      "Epoch: 192 | Batch_idx: 100 |  Loss_1: (0.1342) | Acc_1: (94.99%) (12280/12928)\n",
      "Epoch: 192 | Batch_idx: 110 |  Loss_1: (0.1345) | Acc_1: (94.98%) (13495/14208)\n",
      "Epoch: 192 | Batch_idx: 120 |  Loss_1: (0.1368) | Acc_1: (94.85%) (14691/15488)\n",
      "Epoch: 192 | Batch_idx: 130 |  Loss_1: (0.1380) | Acc_1: (94.81%) (15897/16768)\n",
      "Epoch: 192 | Batch_idx: 140 |  Loss_1: (0.1376) | Acc_1: (94.82%) (17114/18048)\n",
      "Epoch: 192 | Batch_idx: 150 |  Loss_1: (0.1388) | Acc_1: (94.78%) (18319/19328)\n",
      "Epoch: 192 | Batch_idx: 160 |  Loss_1: (0.1379) | Acc_1: (94.80%) (19537/20608)\n",
      "Epoch: 192 | Batch_idx: 170 |  Loss_1: (0.1375) | Acc_1: (94.79%) (20748/21888)\n",
      "Epoch: 192 | Batch_idx: 180 |  Loss_1: (0.1367) | Acc_1: (94.85%) (21975/23168)\n",
      "Epoch: 192 | Batch_idx: 190 |  Loss_1: (0.1375) | Acc_1: (94.83%) (23183/24448)\n",
      "Epoch: 192 | Batch_idx: 200 |  Loss_1: (0.1375) | Acc_1: (94.87%) (24407/25728)\n",
      "Epoch: 192 | Batch_idx: 210 |  Loss_1: (0.1379) | Acc_1: (94.83%) (25612/27008)\n",
      "Epoch: 192 | Batch_idx: 220 |  Loss_1: (0.1364) | Acc_1: (94.88%) (26839/28288)\n",
      "Epoch: 192 | Batch_idx: 230 |  Loss_1: (0.1357) | Acc_1: (94.90%) (28060/29568)\n",
      "Epoch: 192 | Batch_idx: 240 |  Loss_1: (0.1356) | Acc_1: (94.91%) (29277/30848)\n",
      "Epoch: 192 | Batch_idx: 250 |  Loss_1: (0.1363) | Acc_1: (94.89%) (30486/32128)\n",
      "Epoch: 192 | Batch_idx: 260 |  Loss_1: (0.1361) | Acc_1: (94.91%) (31706/33408)\n",
      "Epoch: 192 | Batch_idx: 270 |  Loss_1: (0.1365) | Acc_1: (94.89%) (32915/34688)\n",
      "Epoch: 192 | Batch_idx: 280 |  Loss_1: (0.1378) | Acc_1: (94.85%) (34117/35968)\n",
      "Epoch: 192 | Batch_idx: 290 |  Loss_1: (0.1374) | Acc_1: (94.86%) (35335/37248)\n",
      "Epoch: 192 | Batch_idx: 300 |  Loss_1: (0.1362) | Acc_1: (94.90%) (36562/38528)\n",
      "Epoch: 192 | Batch_idx: 310 |  Loss_1: (0.1367) | Acc_1: (94.88%) (37771/39808)\n",
      "Epoch: 192 | Batch_idx: 320 |  Loss_1: (0.1378) | Acc_1: (94.84%) (38968/41088)\n",
      "Epoch: 192 | Batch_idx: 330 |  Loss_1: (0.1386) | Acc_1: (94.81%) (40170/42368)\n",
      "Epoch: 192 | Batch_idx: 340 |  Loss_1: (0.1386) | Acc_1: (94.81%) (41381/43648)\n",
      "Epoch: 192 | Batch_idx: 350 |  Loss_1: (0.1386) | Acc_1: (94.81%) (42596/44928)\n",
      "Epoch: 192 | Batch_idx: 360 |  Loss_1: (0.1388) | Acc_1: (94.79%) (43802/46208)\n",
      "Epoch: 192 | Batch_idx: 370 |  Loss_1: (0.1388) | Acc_1: (94.79%) (45015/47488)\n",
      "Epoch: 192 | Batch_idx: 380 |  Loss_1: (0.1388) | Acc_1: (94.79%) (46228/48768)\n",
      "Epoch: 192 | Batch_idx: 390 |  Loss_1: (0.1391) | Acc_1: (94.78%) (47392/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3498) | Acc: (92.12%) (9212/10000)\n",
      "Epoch: 193 | Batch_idx: 0 |  Loss_1: (0.2614) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 193 | Batch_idx: 10 |  Loss_1: (0.1749) | Acc_1: (93.47%) (1316/1408)\n",
      "Epoch: 193 | Batch_idx: 20 |  Loss_1: (0.1546) | Acc_1: (94.27%) (2534/2688)\n",
      "Epoch: 193 | Batch_idx: 30 |  Loss_1: (0.1447) | Acc_1: (94.66%) (3756/3968)\n",
      "Epoch: 193 | Batch_idx: 40 |  Loss_1: (0.1416) | Acc_1: (94.70%) (4970/5248)\n",
      "Epoch: 193 | Batch_idx: 50 |  Loss_1: (0.1370) | Acc_1: (94.93%) (6197/6528)\n",
      "Epoch: 193 | Batch_idx: 60 |  Loss_1: (0.1367) | Acc_1: (94.86%) (7407/7808)\n",
      "Epoch: 193 | Batch_idx: 70 |  Loss_1: (0.1364) | Acc_1: (94.88%) (8623/9088)\n",
      "Epoch: 193 | Batch_idx: 80 |  Loss_1: (0.1384) | Acc_1: (94.80%) (9829/10368)\n",
      "Epoch: 193 | Batch_idx: 90 |  Loss_1: (0.1371) | Acc_1: (94.81%) (11044/11648)\n",
      "Epoch: 193 | Batch_idx: 100 |  Loss_1: (0.1367) | Acc_1: (94.83%) (12259/12928)\n",
      "Epoch: 193 | Batch_idx: 110 |  Loss_1: (0.1352) | Acc_1: (94.87%) (13479/14208)\n",
      "Epoch: 193 | Batch_idx: 120 |  Loss_1: (0.1343) | Acc_1: (94.92%) (14701/15488)\n",
      "Epoch: 193 | Batch_idx: 130 |  Loss_1: (0.1328) | Acc_1: (94.98%) (15926/16768)\n",
      "Epoch: 193 | Batch_idx: 140 |  Loss_1: (0.1322) | Acc_1: (94.99%) (17143/18048)\n",
      "Epoch: 193 | Batch_idx: 150 |  Loss_1: (0.1318) | Acc_1: (95.01%) (18364/19328)\n",
      "Epoch: 193 | Batch_idx: 160 |  Loss_1: (0.1309) | Acc_1: (95.04%) (19585/20608)\n",
      "Epoch: 193 | Batch_idx: 170 |  Loss_1: (0.1305) | Acc_1: (95.06%) (20806/21888)\n",
      "Epoch: 193 | Batch_idx: 180 |  Loss_1: (0.1301) | Acc_1: (95.08%) (22028/23168)\n",
      "Epoch: 193 | Batch_idx: 190 |  Loss_1: (0.1300) | Acc_1: (95.08%) (23244/24448)\n",
      "Epoch: 193 | Batch_idx: 200 |  Loss_1: (0.1290) | Acc_1: (95.14%) (24477/25728)\n",
      "Epoch: 193 | Batch_idx: 210 |  Loss_1: (0.1296) | Acc_1: (95.12%) (25691/27008)\n",
      "Epoch: 193 | Batch_idx: 220 |  Loss_1: (0.1301) | Acc_1: (95.10%) (26903/28288)\n",
      "Epoch: 193 | Batch_idx: 230 |  Loss_1: (0.1314) | Acc_1: (95.07%) (28109/29568)\n",
      "Epoch: 193 | Batch_idx: 240 |  Loss_1: (0.1329) | Acc_1: (95.01%) (29309/30848)\n",
      "Epoch: 193 | Batch_idx: 250 |  Loss_1: (0.1328) | Acc_1: (95.02%) (30528/32128)\n",
      "Epoch: 193 | Batch_idx: 260 |  Loss_1: (0.1335) | Acc_1: (95.00%) (31737/33408)\n",
      "Epoch: 193 | Batch_idx: 270 |  Loss_1: (0.1330) | Acc_1: (95.01%) (32958/34688)\n",
      "Epoch: 193 | Batch_idx: 280 |  Loss_1: (0.1330) | Acc_1: (95.01%) (34174/35968)\n",
      "Epoch: 193 | Batch_idx: 290 |  Loss_1: (0.1330) | Acc_1: (95.01%) (35389/37248)\n",
      "Epoch: 193 | Batch_idx: 300 |  Loss_1: (0.1333) | Acc_1: (95.00%) (36603/38528)\n",
      "Epoch: 193 | Batch_idx: 310 |  Loss_1: (0.1330) | Acc_1: (95.02%) (37824/39808)\n",
      "Epoch: 193 | Batch_idx: 320 |  Loss_1: (0.1336) | Acc_1: (95.00%) (39032/41088)\n",
      "Epoch: 193 | Batch_idx: 330 |  Loss_1: (0.1329) | Acc_1: (95.02%) (40258/42368)\n",
      "Epoch: 193 | Batch_idx: 340 |  Loss_1: (0.1325) | Acc_1: (95.03%) (41479/43648)\n",
      "Epoch: 193 | Batch_idx: 350 |  Loss_1: (0.1325) | Acc_1: (95.02%) (42691/44928)\n",
      "Epoch: 193 | Batch_idx: 360 |  Loss_1: (0.1329) | Acc_1: (95.01%) (43901/46208)\n",
      "Epoch: 193 | Batch_idx: 370 |  Loss_1: (0.1330) | Acc_1: (95.00%) (45115/47488)\n",
      "Epoch: 193 | Batch_idx: 380 |  Loss_1: (0.1326) | Acc_1: (95.02%) (46338/48768)\n",
      "Epoch: 193 | Batch_idx: 390 |  Loss_1: (0.1326) | Acc_1: (95.01%) (47504/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3227) | Acc: (92.56%) (9256/10000)\n",
      "Epoch: 194 | Batch_idx: 0 |  Loss_1: (0.0592) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 194 | Batch_idx: 10 |  Loss_1: (0.1243) | Acc_1: (95.38%) (1343/1408)\n",
      "Epoch: 194 | Batch_idx: 20 |  Loss_1: (0.1237) | Acc_1: (95.24%) (2560/2688)\n",
      "Epoch: 194 | Batch_idx: 30 |  Loss_1: (0.1193) | Acc_1: (95.49%) (3789/3968)\n",
      "Epoch: 194 | Batch_idx: 40 |  Loss_1: (0.1209) | Acc_1: (95.41%) (5007/5248)\n",
      "Epoch: 194 | Batch_idx: 50 |  Loss_1: (0.1228) | Acc_1: (95.30%) (6221/6528)\n",
      "Epoch: 194 | Batch_idx: 60 |  Loss_1: (0.1253) | Acc_1: (95.22%) (7435/7808)\n",
      "Epoch: 194 | Batch_idx: 70 |  Loss_1: (0.1259) | Acc_1: (95.13%) (8645/9088)\n",
      "Epoch: 194 | Batch_idx: 80 |  Loss_1: (0.1286) | Acc_1: (95.03%) (9853/10368)\n",
      "Epoch: 194 | Batch_idx: 90 |  Loss_1: (0.1258) | Acc_1: (95.12%) (11080/11648)\n",
      "Epoch: 194 | Batch_idx: 100 |  Loss_1: (0.1277) | Acc_1: (95.07%) (12291/12928)\n",
      "Epoch: 194 | Batch_idx: 110 |  Loss_1: (0.1305) | Acc_1: (94.98%) (13495/14208)\n",
      "Epoch: 194 | Batch_idx: 120 |  Loss_1: (0.1313) | Acc_1: (94.98%) (14711/15488)\n",
      "Epoch: 194 | Batch_idx: 130 |  Loss_1: (0.1320) | Acc_1: (94.99%) (15928/16768)\n",
      "Epoch: 194 | Batch_idx: 140 |  Loss_1: (0.1309) | Acc_1: (95.03%) (17151/18048)\n",
      "Epoch: 194 | Batch_idx: 150 |  Loss_1: (0.1312) | Acc_1: (95.01%) (18363/19328)\n",
      "Epoch: 194 | Batch_idx: 160 |  Loss_1: (0.1303) | Acc_1: (95.04%) (19586/20608)\n",
      "Epoch: 194 | Batch_idx: 170 |  Loss_1: (0.1298) | Acc_1: (95.07%) (20808/21888)\n",
      "Epoch: 194 | Batch_idx: 180 |  Loss_1: (0.1301) | Acc_1: (95.03%) (22017/23168)\n",
      "Epoch: 194 | Batch_idx: 190 |  Loss_1: (0.1308) | Acc_1: (95.01%) (23228/24448)\n",
      "Epoch: 194 | Batch_idx: 200 |  Loss_1: (0.1310) | Acc_1: (95.01%) (24443/25728)\n",
      "Epoch: 194 | Batch_idx: 210 |  Loss_1: (0.1309) | Acc_1: (95.02%) (25662/27008)\n",
      "Epoch: 194 | Batch_idx: 220 |  Loss_1: (0.1315) | Acc_1: (95.02%) (26880/28288)\n",
      "Epoch: 194 | Batch_idx: 230 |  Loss_1: (0.1311) | Acc_1: (95.05%) (28103/29568)\n",
      "Epoch: 194 | Batch_idx: 240 |  Loss_1: (0.1309) | Acc_1: (95.05%) (29320/30848)\n",
      "Epoch: 194 | Batch_idx: 250 |  Loss_1: (0.1298) | Acc_1: (95.08%) (30548/32128)\n",
      "Epoch: 194 | Batch_idx: 260 |  Loss_1: (0.1294) | Acc_1: (95.09%) (31768/33408)\n",
      "Epoch: 194 | Batch_idx: 270 |  Loss_1: (0.1293) | Acc_1: (95.09%) (32984/34688)\n",
      "Epoch: 194 | Batch_idx: 280 |  Loss_1: (0.1291) | Acc_1: (95.09%) (34201/35968)\n",
      "Epoch: 194 | Batch_idx: 290 |  Loss_1: (0.1288) | Acc_1: (95.10%) (35423/37248)\n",
      "Epoch: 194 | Batch_idx: 300 |  Loss_1: (0.1290) | Acc_1: (95.10%) (36641/38528)\n",
      "Epoch: 194 | Batch_idx: 310 |  Loss_1: (0.1292) | Acc_1: (95.10%) (37856/39808)\n",
      "Epoch: 194 | Batch_idx: 320 |  Loss_1: (0.1292) | Acc_1: (95.11%) (39077/41088)\n",
      "Epoch: 194 | Batch_idx: 330 |  Loss_1: (0.1294) | Acc_1: (95.10%) (40293/42368)\n",
      "Epoch: 194 | Batch_idx: 340 |  Loss_1: (0.1291) | Acc_1: (95.12%) (41520/43648)\n",
      "Epoch: 194 | Batch_idx: 350 |  Loss_1: (0.1295) | Acc_1: (95.11%) (42732/44928)\n",
      "Epoch: 194 | Batch_idx: 360 |  Loss_1: (0.1296) | Acc_1: (95.11%) (43950/46208)\n",
      "Epoch: 194 | Batch_idx: 370 |  Loss_1: (0.1301) | Acc_1: (95.09%) (45158/47488)\n",
      "Epoch: 194 | Batch_idx: 380 |  Loss_1: (0.1296) | Acc_1: (95.12%) (46390/48768)\n",
      "Epoch: 194 | Batch_idx: 390 |  Loss_1: (0.1297) | Acc_1: (95.13%) (47567/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3187) | Acc: (92.83%) (9283/10000)\n",
      "Epoch: 195 | Batch_idx: 0 |  Loss_1: (0.1337) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 195 | Batch_idx: 10 |  Loss_1: (0.1305) | Acc_1: (95.17%) (1340/1408)\n",
      "Epoch: 195 | Batch_idx: 20 |  Loss_1: (0.1326) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 195 | Batch_idx: 30 |  Loss_1: (0.1343) | Acc_1: (94.81%) (3762/3968)\n",
      "Epoch: 195 | Batch_idx: 40 |  Loss_1: (0.1374) | Acc_1: (94.72%) (4971/5248)\n",
      "Epoch: 195 | Batch_idx: 50 |  Loss_1: (0.1406) | Acc_1: (94.58%) (6174/6528)\n",
      "Epoch: 195 | Batch_idx: 60 |  Loss_1: (0.1415) | Acc_1: (94.53%) (7381/7808)\n",
      "Epoch: 195 | Batch_idx: 70 |  Loss_1: (0.1406) | Acc_1: (94.60%) (8597/9088)\n",
      "Epoch: 195 | Batch_idx: 80 |  Loss_1: (0.1416) | Acc_1: (94.67%) (9815/10368)\n",
      "Epoch: 195 | Batch_idx: 90 |  Loss_1: (0.1385) | Acc_1: (94.77%) (11039/11648)\n",
      "Epoch: 195 | Batch_idx: 100 |  Loss_1: (0.1395) | Acc_1: (94.71%) (12244/12928)\n",
      "Epoch: 195 | Batch_idx: 110 |  Loss_1: (0.1380) | Acc_1: (94.76%) (13464/14208)\n",
      "Epoch: 195 | Batch_idx: 120 |  Loss_1: (0.1396) | Acc_1: (94.67%) (14663/15488)\n",
      "Epoch: 195 | Batch_idx: 130 |  Loss_1: (0.1381) | Acc_1: (94.71%) (15881/16768)\n",
      "Epoch: 195 | Batch_idx: 140 |  Loss_1: (0.1354) | Acc_1: (94.81%) (17112/18048)\n",
      "Epoch: 195 | Batch_idx: 150 |  Loss_1: (0.1369) | Acc_1: (94.81%) (18325/19328)\n",
      "Epoch: 195 | Batch_idx: 160 |  Loss_1: (0.1363) | Acc_1: (94.83%) (19542/20608)\n",
      "Epoch: 195 | Batch_idx: 170 |  Loss_1: (0.1361) | Acc_1: (94.81%) (20751/21888)\n",
      "Epoch: 195 | Batch_idx: 180 |  Loss_1: (0.1366) | Acc_1: (94.82%) (21967/23168)\n",
      "Epoch: 195 | Batch_idx: 190 |  Loss_1: (0.1352) | Acc_1: (94.85%) (23189/24448)\n",
      "Epoch: 195 | Batch_idx: 200 |  Loss_1: (0.1352) | Acc_1: (94.88%) (24410/25728)\n",
      "Epoch: 195 | Batch_idx: 210 |  Loss_1: (0.1340) | Acc_1: (94.93%) (25638/27008)\n",
      "Epoch: 195 | Batch_idx: 220 |  Loss_1: (0.1336) | Acc_1: (94.95%) (26860/28288)\n",
      "Epoch: 195 | Batch_idx: 230 |  Loss_1: (0.1337) | Acc_1: (94.96%) (28078/29568)\n",
      "Epoch: 195 | Batch_idx: 240 |  Loss_1: (0.1340) | Acc_1: (94.94%) (29288/30848)\n",
      "Epoch: 195 | Batch_idx: 250 |  Loss_1: (0.1335) | Acc_1: (94.96%) (30510/32128)\n",
      "Epoch: 195 | Batch_idx: 260 |  Loss_1: (0.1330) | Acc_1: (94.97%) (31728/33408)\n",
      "Epoch: 195 | Batch_idx: 270 |  Loss_1: (0.1332) | Acc_1: (94.98%) (32945/34688)\n",
      "Epoch: 195 | Batch_idx: 280 |  Loss_1: (0.1332) | Acc_1: (94.97%) (34158/35968)\n",
      "Epoch: 195 | Batch_idx: 290 |  Loss_1: (0.1343) | Acc_1: (94.94%) (35365/37248)\n",
      "Epoch: 195 | Batch_idx: 300 |  Loss_1: (0.1341) | Acc_1: (94.96%) (36586/38528)\n",
      "Epoch: 195 | Batch_idx: 310 |  Loss_1: (0.1345) | Acc_1: (94.94%) (37794/39808)\n",
      "Epoch: 195 | Batch_idx: 320 |  Loss_1: (0.1346) | Acc_1: (94.93%) (39005/41088)\n",
      "Epoch: 195 | Batch_idx: 330 |  Loss_1: (0.1344) | Acc_1: (94.93%) (40222/42368)\n",
      "Epoch: 195 | Batch_idx: 340 |  Loss_1: (0.1341) | Acc_1: (94.95%) (41443/43648)\n",
      "Epoch: 195 | Batch_idx: 350 |  Loss_1: (0.1340) | Acc_1: (94.95%) (42657/44928)\n",
      "Epoch: 195 | Batch_idx: 360 |  Loss_1: (0.1337) | Acc_1: (94.96%) (43878/46208)\n",
      "Epoch: 195 | Batch_idx: 370 |  Loss_1: (0.1344) | Acc_1: (94.93%) (45081/47488)\n",
      "Epoch: 195 | Batch_idx: 380 |  Loss_1: (0.1343) | Acc_1: (94.93%) (46296/48768)\n",
      "Epoch: 195 | Batch_idx: 390 |  Loss_1: (0.1344) | Acc_1: (94.93%) (47466/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3054) | Acc: (92.70%) (9270/10000)\n",
      "Epoch: 196 | Batch_idx: 0 |  Loss_1: (0.1589) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 196 | Batch_idx: 10 |  Loss_1: (0.1232) | Acc_1: (95.67%) (1347/1408)\n",
      "Epoch: 196 | Batch_idx: 20 |  Loss_1: (0.1377) | Acc_1: (95.35%) (2563/2688)\n",
      "Epoch: 196 | Batch_idx: 30 |  Loss_1: (0.1353) | Acc_1: (95.24%) (3779/3968)\n",
      "Epoch: 196 | Batch_idx: 40 |  Loss_1: (0.1355) | Acc_1: (95.22%) (4997/5248)\n",
      "Epoch: 196 | Batch_idx: 50 |  Loss_1: (0.1319) | Acc_1: (95.28%) (6220/6528)\n",
      "Epoch: 196 | Batch_idx: 60 |  Loss_1: (0.1320) | Acc_1: (95.20%) (7433/7808)\n",
      "Epoch: 196 | Batch_idx: 70 |  Loss_1: (0.1290) | Acc_1: (95.29%) (8660/9088)\n",
      "Epoch: 196 | Batch_idx: 80 |  Loss_1: (0.1287) | Acc_1: (95.30%) (9881/10368)\n",
      "Epoch: 196 | Batch_idx: 90 |  Loss_1: (0.1299) | Acc_1: (95.23%) (11092/11648)\n",
      "Epoch: 196 | Batch_idx: 100 |  Loss_1: (0.1311) | Acc_1: (95.13%) (12298/12928)\n",
      "Epoch: 196 | Batch_idx: 110 |  Loss_1: (0.1304) | Acc_1: (95.12%) (13515/14208)\n",
      "Epoch: 196 | Batch_idx: 120 |  Loss_1: (0.1337) | Acc_1: (95.03%) (14719/15488)\n",
      "Epoch: 196 | Batch_idx: 130 |  Loss_1: (0.1338) | Acc_1: (95.04%) (15937/16768)\n",
      "Epoch: 196 | Batch_idx: 140 |  Loss_1: (0.1329) | Acc_1: (95.07%) (17159/18048)\n",
      "Epoch: 196 | Batch_idx: 150 |  Loss_1: (0.1317) | Acc_1: (95.13%) (18387/19328)\n",
      "Epoch: 196 | Batch_idx: 160 |  Loss_1: (0.1307) | Acc_1: (95.17%) (19613/20608)\n",
      "Epoch: 196 | Batch_idx: 170 |  Loss_1: (0.1323) | Acc_1: (95.09%) (20814/21888)\n",
      "Epoch: 196 | Batch_idx: 180 |  Loss_1: (0.1324) | Acc_1: (95.08%) (22028/23168)\n",
      "Epoch: 196 | Batch_idx: 190 |  Loss_1: (0.1326) | Acc_1: (95.07%) (23242/24448)\n",
      "Epoch: 196 | Batch_idx: 200 |  Loss_1: (0.1310) | Acc_1: (95.13%) (24475/25728)\n",
      "Epoch: 196 | Batch_idx: 210 |  Loss_1: (0.1311) | Acc_1: (95.12%) (25690/27008)\n",
      "Epoch: 196 | Batch_idx: 220 |  Loss_1: (0.1315) | Acc_1: (95.12%) (26907/28288)\n",
      "Epoch: 196 | Batch_idx: 230 |  Loss_1: (0.1319) | Acc_1: (95.11%) (28123/29568)\n",
      "Epoch: 196 | Batch_idx: 240 |  Loss_1: (0.1321) | Acc_1: (95.09%) (29333/30848)\n",
      "Epoch: 196 | Batch_idx: 250 |  Loss_1: (0.1324) | Acc_1: (95.07%) (30545/32128)\n",
      "Epoch: 196 | Batch_idx: 260 |  Loss_1: (0.1336) | Acc_1: (95.03%) (31749/33408)\n",
      "Epoch: 196 | Batch_idx: 270 |  Loss_1: (0.1338) | Acc_1: (95.04%) (32967/34688)\n",
      "Epoch: 196 | Batch_idx: 280 |  Loss_1: (0.1334) | Acc_1: (95.04%) (34185/35968)\n",
      "Epoch: 196 | Batch_idx: 290 |  Loss_1: (0.1345) | Acc_1: (95.01%) (35388/37248)\n",
      "Epoch: 196 | Batch_idx: 300 |  Loss_1: (0.1349) | Acc_1: (94.98%) (36592/38528)\n",
      "Epoch: 196 | Batch_idx: 310 |  Loss_1: (0.1348) | Acc_1: (94.98%) (37808/39808)\n",
      "Epoch: 196 | Batch_idx: 320 |  Loss_1: (0.1338) | Acc_1: (95.00%) (39035/41088)\n",
      "Epoch: 196 | Batch_idx: 330 |  Loss_1: (0.1336) | Acc_1: (95.02%) (40259/42368)\n",
      "Epoch: 196 | Batch_idx: 340 |  Loss_1: (0.1336) | Acc_1: (95.03%) (41479/43648)\n",
      "Epoch: 196 | Batch_idx: 350 |  Loss_1: (0.1340) | Acc_1: (95.01%) (42686/44928)\n",
      "Epoch: 196 | Batch_idx: 360 |  Loss_1: (0.1338) | Acc_1: (95.02%) (43907/46208)\n",
      "Epoch: 196 | Batch_idx: 370 |  Loss_1: (0.1336) | Acc_1: (95.02%) (45123/47488)\n",
      "Epoch: 196 | Batch_idx: 380 |  Loss_1: (0.1330) | Acc_1: (95.04%) (46349/48768)\n",
      "Epoch: 196 | Batch_idx: 390 |  Loss_1: (0.1335) | Acc_1: (95.01%) (47506/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3159) | Acc: (92.89%) (9289/10000)\n",
      "Epoch: 197 | Batch_idx: 0 |  Loss_1: (0.1662) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 197 | Batch_idx: 10 |  Loss_1: (0.1497) | Acc_1: (94.46%) (1330/1408)\n",
      "Epoch: 197 | Batch_idx: 20 |  Loss_1: (0.1488) | Acc_1: (94.46%) (2539/2688)\n",
      "Epoch: 197 | Batch_idx: 30 |  Loss_1: (0.1446) | Acc_1: (94.61%) (3754/3968)\n",
      "Epoch: 197 | Batch_idx: 40 |  Loss_1: (0.1449) | Acc_1: (94.59%) (4964/5248)\n",
      "Epoch: 197 | Batch_idx: 50 |  Loss_1: (0.1490) | Acc_1: (94.58%) (6174/6528)\n",
      "Epoch: 197 | Batch_idx: 60 |  Loss_1: (0.1489) | Acc_1: (94.58%) (7385/7808)\n",
      "Epoch: 197 | Batch_idx: 70 |  Loss_1: (0.1467) | Acc_1: (94.59%) (8596/9088)\n",
      "Epoch: 197 | Batch_idx: 80 |  Loss_1: (0.1480) | Acc_1: (94.55%) (9803/10368)\n",
      "Epoch: 197 | Batch_idx: 90 |  Loss_1: (0.1474) | Acc_1: (94.57%) (11015/11648)\n",
      "Epoch: 197 | Batch_idx: 100 |  Loss_1: (0.1442) | Acc_1: (94.66%) (12238/12928)\n",
      "Epoch: 197 | Batch_idx: 110 |  Loss_1: (0.1452) | Acc_1: (94.62%) (13443/14208)\n",
      "Epoch: 197 | Batch_idx: 120 |  Loss_1: (0.1424) | Acc_1: (94.71%) (14669/15488)\n",
      "Epoch: 197 | Batch_idx: 130 |  Loss_1: (0.1424) | Acc_1: (94.67%) (15875/16768)\n",
      "Epoch: 197 | Batch_idx: 140 |  Loss_1: (0.1420) | Acc_1: (94.69%) (17089/18048)\n",
      "Epoch: 197 | Batch_idx: 150 |  Loss_1: (0.1401) | Acc_1: (94.76%) (18315/19328)\n",
      "Epoch: 197 | Batch_idx: 160 |  Loss_1: (0.1393) | Acc_1: (94.80%) (19536/20608)\n",
      "Epoch: 197 | Batch_idx: 170 |  Loss_1: (0.1389) | Acc_1: (94.80%) (20750/21888)\n",
      "Epoch: 197 | Batch_idx: 180 |  Loss_1: (0.1376) | Acc_1: (94.84%) (21972/23168)\n",
      "Epoch: 197 | Batch_idx: 190 |  Loss_1: (0.1377) | Acc_1: (94.83%) (23184/24448)\n",
      "Epoch: 197 | Batch_idx: 200 |  Loss_1: (0.1380) | Acc_1: (94.83%) (24399/25728)\n",
      "Epoch: 197 | Batch_idx: 210 |  Loss_1: (0.1371) | Acc_1: (94.86%) (25620/27008)\n",
      "Epoch: 197 | Batch_idx: 220 |  Loss_1: (0.1371) | Acc_1: (94.85%) (26832/28288)\n",
      "Epoch: 197 | Batch_idx: 230 |  Loss_1: (0.1356) | Acc_1: (94.91%) (28063/29568)\n",
      "Epoch: 197 | Batch_idx: 240 |  Loss_1: (0.1354) | Acc_1: (94.92%) (29281/30848)\n",
      "Epoch: 197 | Batch_idx: 250 |  Loss_1: (0.1344) | Acc_1: (94.95%) (30507/32128)\n",
      "Epoch: 197 | Batch_idx: 260 |  Loss_1: (0.1353) | Acc_1: (94.94%) (31716/33408)\n",
      "Epoch: 197 | Batch_idx: 270 |  Loss_1: (0.1347) | Acc_1: (94.95%) (32937/34688)\n",
      "Epoch: 197 | Batch_idx: 280 |  Loss_1: (0.1347) | Acc_1: (94.95%) (34153/35968)\n",
      "Epoch: 197 | Batch_idx: 290 |  Loss_1: (0.1346) | Acc_1: (94.97%) (35373/37248)\n",
      "Epoch: 197 | Batch_idx: 300 |  Loss_1: (0.1348) | Acc_1: (94.95%) (36584/38528)\n",
      "Epoch: 197 | Batch_idx: 310 |  Loss_1: (0.1345) | Acc_1: (94.97%) (37804/39808)\n",
      "Epoch: 197 | Batch_idx: 320 |  Loss_1: (0.1350) | Acc_1: (94.94%) (39009/41088)\n",
      "Epoch: 197 | Batch_idx: 330 |  Loss_1: (0.1345) | Acc_1: (94.96%) (40233/42368)\n",
      "Epoch: 197 | Batch_idx: 340 |  Loss_1: (0.1352) | Acc_1: (94.94%) (41440/43648)\n",
      "Epoch: 197 | Batch_idx: 350 |  Loss_1: (0.1355) | Acc_1: (94.94%) (42655/44928)\n",
      "Epoch: 197 | Batch_idx: 360 |  Loss_1: (0.1357) | Acc_1: (94.94%) (43871/46208)\n",
      "Epoch: 197 | Batch_idx: 370 |  Loss_1: (0.1353) | Acc_1: (94.95%) (45088/47488)\n",
      "Epoch: 197 | Batch_idx: 380 |  Loss_1: (0.1358) | Acc_1: (94.93%) (46296/48768)\n",
      "Epoch: 197 | Batch_idx: 390 |  Loss_1: (0.1353) | Acc_1: (94.94%) (47469/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3201) | Acc: (92.43%) (9243/10000)\n",
      "Epoch: 198 | Batch_idx: 0 |  Loss_1: (0.1859) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 198 | Batch_idx: 10 |  Loss_1: (0.1220) | Acc_1: (95.24%) (1341/1408)\n",
      "Epoch: 198 | Batch_idx: 20 |  Loss_1: (0.1302) | Acc_1: (94.87%) (2550/2688)\n",
      "Epoch: 198 | Batch_idx: 30 |  Loss_1: (0.1307) | Acc_1: (94.93%) (3767/3968)\n",
      "Epoch: 198 | Batch_idx: 40 |  Loss_1: (0.1296) | Acc_1: (94.89%) (4980/5248)\n",
      "Epoch: 198 | Batch_idx: 50 |  Loss_1: (0.1337) | Acc_1: (94.82%) (6190/6528)\n",
      "Epoch: 198 | Batch_idx: 60 |  Loss_1: (0.1330) | Acc_1: (94.90%) (7410/7808)\n",
      "Epoch: 198 | Batch_idx: 70 |  Loss_1: (0.1351) | Acc_1: (94.82%) (8617/9088)\n",
      "Epoch: 198 | Batch_idx: 80 |  Loss_1: (0.1353) | Acc_1: (94.84%) (9833/10368)\n",
      "Epoch: 198 | Batch_idx: 90 |  Loss_1: (0.1325) | Acc_1: (94.96%) (11061/11648)\n",
      "Epoch: 198 | Batch_idx: 100 |  Loss_1: (0.1305) | Acc_1: (95.04%) (12287/12928)\n",
      "Epoch: 198 | Batch_idx: 110 |  Loss_1: (0.1304) | Acc_1: (95.07%) (13507/14208)\n",
      "Epoch: 198 | Batch_idx: 120 |  Loss_1: (0.1299) | Acc_1: (95.10%) (14729/15488)\n",
      "Epoch: 198 | Batch_idx: 130 |  Loss_1: (0.1318) | Acc_1: (95.03%) (15934/16768)\n",
      "Epoch: 198 | Batch_idx: 140 |  Loss_1: (0.1319) | Acc_1: (95.02%) (17149/18048)\n",
      "Epoch: 198 | Batch_idx: 150 |  Loss_1: (0.1313) | Acc_1: (95.03%) (18368/19328)\n",
      "Epoch: 198 | Batch_idx: 160 |  Loss_1: (0.1312) | Acc_1: (95.06%) (19590/20608)\n",
      "Epoch: 198 | Batch_idx: 170 |  Loss_1: (0.1305) | Acc_1: (95.07%) (20810/21888)\n",
      "Epoch: 198 | Batch_idx: 180 |  Loss_1: (0.1300) | Acc_1: (95.10%) (22033/23168)\n",
      "Epoch: 198 | Batch_idx: 190 |  Loss_1: (0.1302) | Acc_1: (95.10%) (23249/24448)\n",
      "Epoch: 198 | Batch_idx: 200 |  Loss_1: (0.1309) | Acc_1: (95.06%) (24457/25728)\n",
      "Epoch: 198 | Batch_idx: 210 |  Loss_1: (0.1312) | Acc_1: (95.06%) (25674/27008)\n",
      "Epoch: 198 | Batch_idx: 220 |  Loss_1: (0.1307) | Acc_1: (95.06%) (26891/28288)\n",
      "Epoch: 198 | Batch_idx: 230 |  Loss_1: (0.1308) | Acc_1: (95.07%) (28111/29568)\n",
      "Epoch: 198 | Batch_idx: 240 |  Loss_1: (0.1312) | Acc_1: (95.05%) (29320/30848)\n",
      "Epoch: 198 | Batch_idx: 250 |  Loss_1: (0.1300) | Acc_1: (95.09%) (30552/32128)\n",
      "Epoch: 198 | Batch_idx: 260 |  Loss_1: (0.1318) | Acc_1: (95.05%) (31755/33408)\n",
      "Epoch: 198 | Batch_idx: 270 |  Loss_1: (0.1320) | Acc_1: (95.04%) (32969/34688)\n",
      "Epoch: 198 | Batch_idx: 280 |  Loss_1: (0.1322) | Acc_1: (95.03%) (34182/35968)\n",
      "Epoch: 198 | Batch_idx: 290 |  Loss_1: (0.1327) | Acc_1: (95.03%) (35395/37248)\n",
      "Epoch: 198 | Batch_idx: 300 |  Loss_1: (0.1334) | Acc_1: (94.99%) (36599/38528)\n",
      "Epoch: 198 | Batch_idx: 310 |  Loss_1: (0.1335) | Acc_1: (94.99%) (37814/39808)\n",
      "Epoch: 198 | Batch_idx: 320 |  Loss_1: (0.1330) | Acc_1: (95.00%) (39034/41088)\n",
      "Epoch: 198 | Batch_idx: 330 |  Loss_1: (0.1335) | Acc_1: (94.97%) (40239/42368)\n",
      "Epoch: 198 | Batch_idx: 340 |  Loss_1: (0.1344) | Acc_1: (94.94%) (41439/43648)\n",
      "Epoch: 198 | Batch_idx: 350 |  Loss_1: (0.1333) | Acc_1: (94.97%) (42670/44928)\n",
      "Epoch: 198 | Batch_idx: 360 |  Loss_1: (0.1342) | Acc_1: (94.94%) (43869/46208)\n",
      "Epoch: 198 | Batch_idx: 370 |  Loss_1: (0.1339) | Acc_1: (94.94%) (45085/47488)\n",
      "Epoch: 198 | Batch_idx: 380 |  Loss_1: (0.1336) | Acc_1: (94.95%) (46305/48768)\n",
      "Epoch: 198 | Batch_idx: 390 |  Loss_1: (0.1338) | Acc_1: (94.94%) (47468/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3175) | Acc: (92.59%) (9259/10000)\n",
      "Epoch: 199 | Batch_idx: 0 |  Loss_1: (0.2005) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 199 | Batch_idx: 10 |  Loss_1: (0.1416) | Acc_1: (94.32%) (1328/1408)\n",
      "Epoch: 199 | Batch_idx: 20 |  Loss_1: (0.1362) | Acc_1: (94.64%) (2544/2688)\n",
      "Epoch: 199 | Batch_idx: 30 |  Loss_1: (0.1323) | Acc_1: (94.93%) (3767/3968)\n",
      "Epoch: 199 | Batch_idx: 40 |  Loss_1: (0.1275) | Acc_1: (95.20%) (4996/5248)\n",
      "Epoch: 199 | Batch_idx: 50 |  Loss_1: (0.1281) | Acc_1: (95.24%) (6217/6528)\n",
      "Epoch: 199 | Batch_idx: 60 |  Loss_1: (0.1267) | Acc_1: (95.34%) (7444/7808)\n",
      "Epoch: 199 | Batch_idx: 70 |  Loss_1: (0.1260) | Acc_1: (95.37%) (8667/9088)\n",
      "Epoch: 199 | Batch_idx: 80 |  Loss_1: (0.1281) | Acc_1: (95.26%) (9877/10368)\n",
      "Epoch: 199 | Batch_idx: 90 |  Loss_1: (0.1327) | Acc_1: (95.11%) (11078/11648)\n",
      "Epoch: 199 | Batch_idx: 100 |  Loss_1: (0.1331) | Acc_1: (95.07%) (12291/12928)\n",
      "Epoch: 199 | Batch_idx: 110 |  Loss_1: (0.1322) | Acc_1: (95.06%) (13506/14208)\n",
      "Epoch: 199 | Batch_idx: 120 |  Loss_1: (0.1329) | Acc_1: (95.06%) (14723/15488)\n",
      "Epoch: 199 | Batch_idx: 130 |  Loss_1: (0.1325) | Acc_1: (95.09%) (15945/16768)\n",
      "Epoch: 199 | Batch_idx: 140 |  Loss_1: (0.1324) | Acc_1: (95.09%) (17161/18048)\n",
      "Epoch: 199 | Batch_idx: 150 |  Loss_1: (0.1308) | Acc_1: (95.15%) (18390/19328)\n",
      "Epoch: 199 | Batch_idx: 160 |  Loss_1: (0.1313) | Acc_1: (95.13%) (19604/20608)\n",
      "Epoch: 199 | Batch_idx: 170 |  Loss_1: (0.1311) | Acc_1: (95.13%) (20823/21888)\n",
      "Epoch: 199 | Batch_idx: 180 |  Loss_1: (0.1329) | Acc_1: (95.08%) (22027/23168)\n",
      "Epoch: 199 | Batch_idx: 190 |  Loss_1: (0.1335) | Acc_1: (95.06%) (23241/24448)\n",
      "Epoch: 199 | Batch_idx: 200 |  Loss_1: (0.1343) | Acc_1: (95.05%) (24455/25728)\n",
      "Epoch: 199 | Batch_idx: 210 |  Loss_1: (0.1344) | Acc_1: (95.04%) (25669/27008)\n",
      "Epoch: 199 | Batch_idx: 220 |  Loss_1: (0.1349) | Acc_1: (95.02%) (26880/28288)\n",
      "Epoch: 199 | Batch_idx: 230 |  Loss_1: (0.1337) | Acc_1: (95.04%) (28102/29568)\n",
      "Epoch: 199 | Batch_idx: 240 |  Loss_1: (0.1338) | Acc_1: (95.05%) (29322/30848)\n",
      "Epoch: 199 | Batch_idx: 250 |  Loss_1: (0.1327) | Acc_1: (95.07%) (30544/32128)\n",
      "Epoch: 199 | Batch_idx: 260 |  Loss_1: (0.1327) | Acc_1: (95.08%) (31763/33408)\n",
      "Epoch: 199 | Batch_idx: 270 |  Loss_1: (0.1337) | Acc_1: (95.02%) (32962/34688)\n",
      "Epoch: 199 | Batch_idx: 280 |  Loss_1: (0.1341) | Acc_1: (95.02%) (34176/35968)\n",
      "Epoch: 199 | Batch_idx: 290 |  Loss_1: (0.1343) | Acc_1: (95.01%) (35388/37248)\n",
      "Epoch: 199 | Batch_idx: 300 |  Loss_1: (0.1339) | Acc_1: (95.00%) (36601/38528)\n",
      "Epoch: 199 | Batch_idx: 310 |  Loss_1: (0.1342) | Acc_1: (94.99%) (37812/39808)\n",
      "Epoch: 199 | Batch_idx: 320 |  Loss_1: (0.1342) | Acc_1: (94.99%) (39029/41088)\n",
      "Epoch: 199 | Batch_idx: 330 |  Loss_1: (0.1348) | Acc_1: (94.96%) (40234/42368)\n",
      "Epoch: 199 | Batch_idx: 340 |  Loss_1: (0.1347) | Acc_1: (94.96%) (41449/43648)\n",
      "Epoch: 199 | Batch_idx: 350 |  Loss_1: (0.1350) | Acc_1: (94.95%) (42660/44928)\n",
      "Epoch: 199 | Batch_idx: 360 |  Loss_1: (0.1348) | Acc_1: (94.96%) (43877/46208)\n",
      "Epoch: 199 | Batch_idx: 370 |  Loss_1: (0.1351) | Acc_1: (94.96%) (45095/47488)\n",
      "Epoch: 199 | Batch_idx: 380 |  Loss_1: (0.1351) | Acc_1: (94.96%) (46310/48768)\n",
      "Epoch: 199 | Batch_idx: 390 |  Loss_1: (0.1347) | Acc_1: (94.97%) (47486/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3283) | Acc: (92.44%) (9244/10000)\n",
      "3 hours 40 mins 39 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet_50_dropblock.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 200):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet_50_dropblock.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8766a17f2584f17ae9875767170f3464b2a051bfe2b6423fb227ac503acbc200"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hw2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
