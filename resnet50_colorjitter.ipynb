{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.003\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/resnet_50_colojitter') #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformer_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ColorJitter(brightness=(0.2, 2), contrast=(0.3, 2), saturation=(0.2, 2), hue=(-0.3, 0.3)),\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_2 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_3 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "validation_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_2 = torch.utils.data.DataLoader(dataset=training_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_3 = torch.utils.data.DataLoader(dataset=training_dataset_3, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "       \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #self.dropblock.step()\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        #out = self.dropblock(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        #out = self.dropblock(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BottleNeck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 1 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
      "             ReLU-13          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-14          [-1, 256, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "           Conv2d-18           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
      "             ReLU-20           [-1, 64, 32, 32]               0\n",
      "           Conv2d-21          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-22          [-1, 256, 32, 32]             512\n",
      "             ReLU-23          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
      "             ReLU-27           [-1, 64, 32, 32]               0\n",
      "           Conv2d-28           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 32, 32]             128\n",
      "             ReLU-30           [-1, 64, 32, 32]               0\n",
      "           Conv2d-31          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
      "             ReLU-33          [-1, 256, 32, 32]               0\n",
      "       BottleNeck-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "             ReLU-40          [-1, 128, 16, 16]               0\n",
      "           Conv2d-41          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-42          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-43          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-45          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-46          [-1, 512, 16, 16]               0\n",
      "           Conv2d-47          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-48          [-1, 128, 16, 16]             256\n",
      "             ReLU-49          [-1, 128, 16, 16]               0\n",
      "           Conv2d-50          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-51          [-1, 128, 16, 16]             256\n",
      "             ReLU-52          [-1, 128, 16, 16]               0\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-55          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-56          [-1, 512, 16, 16]               0\n",
      "           Conv2d-57          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-58          [-1, 128, 16, 16]             256\n",
      "             ReLU-59          [-1, 128, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-66          [-1, 512, 16, 16]               0\n",
      "           Conv2d-67          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-68          [-1, 128, 16, 16]             256\n",
      "             ReLU-69          [-1, 128, 16, 16]               0\n",
      "           Conv2d-70          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-71          [-1, 128, 16, 16]             256\n",
      "             ReLU-72          [-1, 128, 16, 16]               0\n",
      "           Conv2d-73          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-74          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-75          [-1, 512, 16, 16]               0\n",
      "       BottleNeck-76          [-1, 512, 16, 16]               0\n",
      "           Conv2d-77          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-78          [-1, 256, 16, 16]             512\n",
      "             ReLU-79          [-1, 256, 16, 16]               0\n",
      "           Conv2d-80            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-81            [-1, 256, 8, 8]             512\n",
      "             ReLU-82            [-1, 256, 8, 8]               0\n",
      "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-85           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-87           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-88           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-89            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-90            [-1, 256, 8, 8]             512\n",
      "             ReLU-91            [-1, 256, 8, 8]               0\n",
      "           Conv2d-92            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-93            [-1, 256, 8, 8]             512\n",
      "             ReLU-94            [-1, 256, 8, 8]               0\n",
      "           Conv2d-95           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-96           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-97           [-1, 1024, 8, 8]               0\n",
      "       BottleNeck-98           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-99            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-100            [-1, 256, 8, 8]             512\n",
      "            ReLU-101            [-1, 256, 8, 8]               0\n",
      "          Conv2d-102            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-103            [-1, 256, 8, 8]             512\n",
      "            ReLU-104            [-1, 256, 8, 8]               0\n",
      "          Conv2d-105           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-106           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-107           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-108           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-109            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-110            [-1, 256, 8, 8]             512\n",
      "            ReLU-111            [-1, 256, 8, 8]               0\n",
      "          Conv2d-112            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-113            [-1, 256, 8, 8]             512\n",
      "            ReLU-114            [-1, 256, 8, 8]               0\n",
      "          Conv2d-115           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-116           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-117           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-118           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-119            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-120            [-1, 256, 8, 8]             512\n",
      "            ReLU-121            [-1, 256, 8, 8]               0\n",
      "          Conv2d-122            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "            ReLU-124            [-1, 256, 8, 8]               0\n",
      "          Conv2d-125           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-126           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-127           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-128           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-129            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-130            [-1, 256, 8, 8]             512\n",
      "            ReLU-131            [-1, 256, 8, 8]               0\n",
      "          Conv2d-132            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-133            [-1, 256, 8, 8]             512\n",
      "            ReLU-134            [-1, 256, 8, 8]               0\n",
      "          Conv2d-135           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-136           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-137           [-1, 1024, 8, 8]               0\n",
      "      BottleNeck-138           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-139            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-140            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-141            [-1, 512, 8, 8]               0\n",
      "          Conv2d-142            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-143            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-144            [-1, 512, 4, 4]               0\n",
      "          Conv2d-145           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-146           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-147           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-148           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-149           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-150           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-151            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-152            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-153            [-1, 512, 4, 4]               0\n",
      "          Conv2d-154            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-155            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-156            [-1, 512, 4, 4]               0\n",
      "          Conv2d-157           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-158           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-159           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-160           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-161            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-162            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-163            [-1, 512, 4, 4]               0\n",
      "          Conv2d-164            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-165            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-166            [-1, 512, 4, 4]               0\n",
      "          Conv2d-167           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-168           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-169           [-1, 2048, 4, 4]               0\n",
      "      BottleNeck-170           [-1, 2048, 4, 4]               0\n",
      "          Linear-171                   [-1, 10]          20,490\n",
      "          ResNet-172                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 23,520,842\n",
      "Trainable params: 23,520,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 88.06\n",
      "Params size (MB): 89.72\n",
      "Estimated Total Size (MB): 177.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(training_loader)\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(training_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(validation_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(validation_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (2.5134) | Acc_1: (6.25%) (8/128)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (2.4497) | Acc_1: (8.31%) (117/1408)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (2.4560) | Acc_1: (8.82%) (237/2688)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (2.4355) | Acc_1: (9.53%) (378/3968)\n",
      "Epoch: 0 | Batch_idx: 40 |  Loss_1: (2.4271) | Acc_1: (10.23%) (537/5248)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss_1: (2.4120) | Acc_1: (10.69%) (698/6528)\n",
      "Epoch: 0 | Batch_idx: 60 |  Loss_1: (2.4042) | Acc_1: (11.09%) (866/7808)\n",
      "Epoch: 0 | Batch_idx: 70 |  Loss_1: (2.3911) | Acc_1: (11.51%) (1046/9088)\n",
      "Epoch: 0 | Batch_idx: 80 |  Loss_1: (2.3756) | Acc_1: (12.16%) (1261/10368)\n",
      "Epoch: 0 | Batch_idx: 90 |  Loss_1: (2.3615) | Acc_1: (12.82%) (1493/11648)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss_1: (2.3458) | Acc_1: (13.52%) (1748/12928)\n",
      "Epoch: 0 | Batch_idx: 110 |  Loss_1: (2.3360) | Acc_1: (13.75%) (1953/14208)\n",
      "Epoch: 0 | Batch_idx: 120 |  Loss_1: (2.3218) | Acc_1: (14.14%) (2190/15488)\n",
      "Epoch: 0 | Batch_idx: 130 |  Loss_1: (2.3099) | Acc_1: (14.37%) (2410/16768)\n",
      "Epoch: 0 | Batch_idx: 140 |  Loss_1: (2.2989) | Acc_1: (14.89%) (2688/18048)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss_1: (2.2864) | Acc_1: (15.40%) (2976/19328)\n",
      "Epoch: 0 | Batch_idx: 160 |  Loss_1: (2.2760) | Acc_1: (15.68%) (3231/20608)\n",
      "Epoch: 0 | Batch_idx: 170 |  Loss_1: (2.2674) | Acc_1: (15.97%) (3495/21888)\n",
      "Epoch: 0 | Batch_idx: 180 |  Loss_1: (2.2559) | Acc_1: (16.50%) (3822/23168)\n",
      "Epoch: 0 | Batch_idx: 190 |  Loss_1: (2.2470) | Acc_1: (16.87%) (4124/24448)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss_1: (2.2381) | Acc_1: (17.11%) (4402/25728)\n",
      "Epoch: 0 | Batch_idx: 210 |  Loss_1: (2.2302) | Acc_1: (17.41%) (4703/27008)\n",
      "Epoch: 0 | Batch_idx: 220 |  Loss_1: (2.2251) | Acc_1: (17.63%) (4988/28288)\n",
      "Epoch: 0 | Batch_idx: 230 |  Loss_1: (2.2164) | Acc_1: (17.99%) (5320/29568)\n",
      "Epoch: 0 | Batch_idx: 240 |  Loss_1: (2.2087) | Acc_1: (18.19%) (5612/30848)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss_1: (2.1988) | Acc_1: (18.57%) (5967/32128)\n",
      "Epoch: 0 | Batch_idx: 260 |  Loss_1: (2.1907) | Acc_1: (18.85%) (6298/33408)\n",
      "Epoch: 0 | Batch_idx: 270 |  Loss_1: (2.1831) | Acc_1: (19.18%) (6652/34688)\n",
      "Epoch: 0 | Batch_idx: 280 |  Loss_1: (2.1759) | Acc_1: (19.40%) (6979/35968)\n",
      "Epoch: 0 | Batch_idx: 290 |  Loss_1: (2.1685) | Acc_1: (19.64%) (7317/37248)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss_1: (2.1622) | Acc_1: (19.91%) (7671/38528)\n",
      "Epoch: 0 | Batch_idx: 310 |  Loss_1: (2.1557) | Acc_1: (20.11%) (8004/39808)\n",
      "Epoch: 0 | Batch_idx: 320 |  Loss_1: (2.1493) | Acc_1: (20.34%) (8357/41088)\n",
      "Epoch: 0 | Batch_idx: 330 |  Loss_1: (2.1422) | Acc_1: (20.56%) (8710/42368)\n",
      "Epoch: 0 | Batch_idx: 340 |  Loss_1: (2.1367) | Acc_1: (20.82%) (9086/43648)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss_1: (2.1301) | Acc_1: (21.09%) (9476/44928)\n",
      "Epoch: 0 | Batch_idx: 360 |  Loss_1: (2.1236) | Acc_1: (21.34%) (9863/46208)\n",
      "Epoch: 0 | Batch_idx: 370 |  Loss_1: (2.1177) | Acc_1: (21.63%) (10270/47488)\n",
      "Epoch: 0 | Batch_idx: 380 |  Loss_1: (2.1123) | Acc_1: (21.88%) (10669/48768)\n",
      "Epoch: 0 | Batch_idx: 390 |  Loss_1: (2.1068) | Acc_1: (22.12%) (11058/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.7629) | Acc: (35.97%) (3597/10000)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.9845) | Acc_1: (32.81%) (42/128)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.8696) | Acc_1: (31.82%) (448/1408)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.8653) | Acc_1: (31.88%) (857/2688)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.8503) | Acc_1: (32.59%) (1293/3968)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss_1: (1.8477) | Acc_1: (32.41%) (1701/5248)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss_1: (1.8520) | Acc_1: (32.02%) (2090/6528)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss_1: (1.8504) | Acc_1: (32.04%) (2502/7808)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss_1: (1.8477) | Acc_1: (32.28%) (2934/9088)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss_1: (1.8465) | Acc_1: (32.22%) (3341/10368)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss_1: (1.8449) | Acc_1: (32.21%) (3752/11648)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss_1: (1.8358) | Acc_1: (32.58%) (4212/12928)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss_1: (1.8358) | Acc_1: (32.62%) (4634/14208)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss_1: (1.8275) | Acc_1: (32.83%) (5084/15488)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss_1: (1.8286) | Acc_1: (32.82%) (5504/16768)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss_1: (1.8264) | Acc_1: (33.10%) (5973/18048)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss_1: (1.8216) | Acc_1: (33.39%) (6453/19328)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss_1: (1.8206) | Acc_1: (33.40%) (6883/20608)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss_1: (1.8202) | Acc_1: (33.45%) (7321/21888)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss_1: (1.8171) | Acc_1: (33.62%) (7789/23168)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss_1: (1.8155) | Acc_1: (33.59%) (8213/24448)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss_1: (1.8105) | Acc_1: (33.73%) (8677/25728)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss_1: (1.8073) | Acc_1: (33.77%) (9120/27008)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss_1: (1.8038) | Acc_1: (33.97%) (9610/28288)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss_1: (1.8004) | Acc_1: (34.01%) (10056/29568)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss_1: (1.7983) | Acc_1: (34.06%) (10507/30848)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss_1: (1.7967) | Acc_1: (34.10%) (10956/32128)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss_1: (1.7940) | Acc_1: (34.24%) (11438/33408)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss_1: (1.7937) | Acc_1: (34.26%) (11885/34688)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss_1: (1.7905) | Acc_1: (34.41%) (12375/35968)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss_1: (1.7867) | Acc_1: (34.55%) (12870/37248)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss_1: (1.7830) | Acc_1: (34.66%) (13352/38528)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss_1: (1.7798) | Acc_1: (34.79%) (13848/39808)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss_1: (1.7762) | Acc_1: (34.91%) (14345/41088)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss_1: (1.7727) | Acc_1: (35.07%) (14858/42368)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss_1: (1.7683) | Acc_1: (35.27%) (15395/43648)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss_1: (1.7649) | Acc_1: (35.38%) (15894/44928)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss_1: (1.7617) | Acc_1: (35.52%) (16414/46208)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss_1: (1.7592) | Acc_1: (35.68%) (16943/47488)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss_1: (1.7555) | Acc_1: (35.82%) (17470/48768)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss_1: (1.7511) | Acc_1: (36.00%) (17998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4043) | Acc: (47.71%) (4771/10000)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (1.6794) | Acc_1: (39.06%) (50/128)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.5833) | Acc_1: (43.89%) (618/1408)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.5734) | Acc_1: (43.79%) (1177/2688)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.5729) | Acc_1: (44.23%) (1755/3968)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss_1: (1.5627) | Acc_1: (44.04%) (2311/5248)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss_1: (1.5529) | Acc_1: (44.38%) (2897/6528)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss_1: (1.5577) | Acc_1: (44.20%) (3451/7808)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss_1: (1.5510) | Acc_1: (44.56%) (4050/9088)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss_1: (1.5466) | Acc_1: (44.74%) (4639/10368)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss_1: (1.5472) | Acc_1: (44.73%) (5210/11648)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss_1: (1.5424) | Acc_1: (45.02%) (5820/12928)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss_1: (1.5442) | Acc_1: (44.83%) (6369/14208)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss_1: (1.5401) | Acc_1: (44.82%) (6942/15488)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss_1: (1.5384) | Acc_1: (44.91%) (7530/16768)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss_1: (1.5335) | Acc_1: (45.00%) (8122/18048)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss_1: (1.5298) | Acc_1: (45.17%) (8731/19328)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss_1: (1.5280) | Acc_1: (45.31%) (9338/20608)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss_1: (1.5252) | Acc_1: (45.40%) (9937/21888)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss_1: (1.5223) | Acc_1: (45.56%) (10555/23168)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss_1: (1.5190) | Acc_1: (45.60%) (11149/24448)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss_1: (1.5174) | Acc_1: (45.70%) (11757/25728)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss_1: (1.5123) | Acc_1: (45.99%) (12422/27008)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss_1: (1.5107) | Acc_1: (46.12%) (13047/28288)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss_1: (1.5071) | Acc_1: (46.22%) (13665/29568)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss_1: (1.5051) | Acc_1: (46.26%) (14271/30848)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss_1: (1.5004) | Acc_1: (46.47%) (14930/32128)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss_1: (1.4958) | Acc_1: (46.66%) (15587/33408)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss_1: (1.4930) | Acc_1: (46.71%) (16204/34688)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss_1: (1.4900) | Acc_1: (46.82%) (16842/35968)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss_1: (1.4875) | Acc_1: (46.93%) (17479/37248)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss_1: (1.4833) | Acc_1: (47.05%) (18129/38528)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss_1: (1.4792) | Acc_1: (47.21%) (18792/39808)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss_1: (1.4754) | Acc_1: (47.35%) (19457/41088)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss_1: (1.4703) | Acc_1: (47.55%) (20148/42368)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss_1: (1.4673) | Acc_1: (47.64%) (20794/43648)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss_1: (1.4620) | Acc_1: (47.79%) (21473/44928)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss_1: (1.4589) | Acc_1: (47.91%) (22138/46208)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss_1: (1.4541) | Acc_1: (48.12%) (22852/47488)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss_1: (1.4517) | Acc_1: (48.22%) (23516/48768)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss_1: (1.4486) | Acc_1: (48.34%) (24170/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1898) | Acc: (58.05%) (5805/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.2580) | Acc_1: (53.91%) (69/128)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (1.2581) | Acc_1: (55.11%) (776/1408)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (1.2901) | Acc_1: (53.83%) (1447/2688)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (1.3022) | Acc_1: (53.60%) (2127/3968)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss_1: (1.3119) | Acc_1: (53.53%) (2809/5248)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss_1: (1.3017) | Acc_1: (54.01%) (3526/6528)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss_1: (1.2930) | Acc_1: (54.44%) (4251/7808)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss_1: (1.2928) | Acc_1: (54.17%) (4923/9088)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss_1: (1.2845) | Acc_1: (54.48%) (5649/10368)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss_1: (1.2842) | Acc_1: (54.70%) (6372/11648)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss_1: (1.2895) | Acc_1: (54.41%) (7034/12928)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss_1: (1.2907) | Acc_1: (54.43%) (7734/14208)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss_1: (1.2882) | Acc_1: (54.50%) (8441/15488)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss_1: (1.2856) | Acc_1: (54.57%) (9151/16768)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss_1: (1.2825) | Acc_1: (54.65%) (9863/18048)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss_1: (1.2767) | Acc_1: (54.85%) (10601/19328)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss_1: (1.2724) | Acc_1: (55.08%) (11350/20608)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss_1: (1.2675) | Acc_1: (55.33%) (12111/21888)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss_1: (1.2640) | Acc_1: (55.45%) (12846/23168)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss_1: (1.2589) | Acc_1: (55.59%) (13590/24448)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss_1: (1.2563) | Acc_1: (55.64%) (14314/25728)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss_1: (1.2524) | Acc_1: (55.77%) (15062/27008)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss_1: (1.2479) | Acc_1: (55.94%) (15823/28288)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss_1: (1.2459) | Acc_1: (56.02%) (16563/29568)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss_1: (1.2429) | Acc_1: (56.08%) (17301/30848)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss_1: (1.2405) | Acc_1: (56.09%) (18022/32128)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss_1: (1.2380) | Acc_1: (56.18%) (18770/33408)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss_1: (1.2333) | Acc_1: (56.36%) (19551/34688)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss_1: (1.2310) | Acc_1: (56.46%) (20308/35968)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss_1: (1.2266) | Acc_1: (56.56%) (21069/37248)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss_1: (1.2250) | Acc_1: (56.57%) (21795/38528)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss_1: (1.2232) | Acc_1: (56.70%) (22572/39808)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss_1: (1.2206) | Acc_1: (56.81%) (23342/41088)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss_1: (1.2187) | Acc_1: (56.85%) (24088/42368)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss_1: (1.2164) | Acc_1: (56.93%) (24850/43648)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss_1: (1.2124) | Acc_1: (57.07%) (25640/44928)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss_1: (1.2100) | Acc_1: (57.14%) (26401/46208)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss_1: (1.2079) | Acc_1: (57.23%) (27178/47488)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss_1: (1.2051) | Acc_1: (57.36%) (27971/48768)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss_1: (1.2032) | Acc_1: (57.43%) (28713/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0081) | Acc: (64.83%) (6483/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (1.1010) | Acc_1: (58.59%) (75/128)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (1.0723) | Acc_1: (62.00%) (873/1408)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (1.0884) | Acc_1: (61.53%) (1654/2688)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (1.1017) | Acc_1: (61.21%) (2429/3968)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss_1: (1.1033) | Acc_1: (61.55%) (3230/5248)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss_1: (1.1048) | Acc_1: (61.69%) (4027/6528)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss_1: (1.1031) | Acc_1: (61.67%) (4815/7808)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss_1: (1.0945) | Acc_1: (61.76%) (5613/9088)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss_1: (1.0897) | Acc_1: (61.96%) (6424/10368)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss_1: (1.0887) | Acc_1: (61.95%) (7216/11648)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss_1: (1.0860) | Acc_1: (62.00%) (8016/12928)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss_1: (1.0846) | Acc_1: (61.86%) (8789/14208)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss_1: (1.0800) | Acc_1: (61.91%) (9588/15488)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss_1: (1.0746) | Acc_1: (62.07%) (10408/16768)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss_1: (1.0690) | Acc_1: (62.36%) (11254/18048)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss_1: (1.0657) | Acc_1: (62.53%) (12085/19328)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss_1: (1.0677) | Acc_1: (62.45%) (12869/20608)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss_1: (1.0671) | Acc_1: (62.46%) (13672/21888)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss_1: (1.0642) | Acc_1: (62.52%) (14485/23168)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss_1: (1.0652) | Acc_1: (62.48%) (15276/24448)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss_1: (1.0621) | Acc_1: (62.58%) (16100/25728)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss_1: (1.0594) | Acc_1: (62.61%) (16910/27008)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss_1: (1.0557) | Acc_1: (62.68%) (17732/28288)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss_1: (1.0542) | Acc_1: (62.73%) (18549/29568)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss_1: (1.0508) | Acc_1: (62.81%) (19377/30848)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss_1: (1.0494) | Acc_1: (62.88%) (20203/32128)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss_1: (1.0492) | Acc_1: (62.92%) (21020/33408)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss_1: (1.0467) | Acc_1: (63.08%) (21880/34688)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss_1: (1.0433) | Acc_1: (63.20%) (22730/35968)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss_1: (1.0403) | Acc_1: (63.29%) (23573/37248)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss_1: (1.0363) | Acc_1: (63.49%) (24461/38528)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss_1: (1.0353) | Acc_1: (63.53%) (25289/39808)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss_1: (1.0344) | Acc_1: (63.58%) (26123/41088)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss_1: (1.0316) | Acc_1: (63.69%) (26985/42368)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss_1: (1.0324) | Acc_1: (63.68%) (27797/43648)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss_1: (1.0308) | Acc_1: (63.72%) (28627/44928)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss_1: (1.0278) | Acc_1: (63.83%) (29495/46208)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss_1: (1.0260) | Acc_1: (63.86%) (30326/47488)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss_1: (1.0235) | Acc_1: (63.93%) (31179/48768)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss_1: (1.0210) | Acc_1: (64.02%) (32010/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9212) | Acc: (68.59%) (6859/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (0.9962) | Acc_1: (61.72%) (79/128)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (0.9230) | Acc_1: (67.05%) (944/1408)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (0.9443) | Acc_1: (66.41%) (1785/2688)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (0.9393) | Acc_1: (66.61%) (2643/3968)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss_1: (0.9425) | Acc_1: (66.62%) (3496/5248)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss_1: (0.9317) | Acc_1: (66.91%) (4368/6528)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss_1: (0.9268) | Acc_1: (67.00%) (5231/7808)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss_1: (0.9221) | Acc_1: (67.21%) (6108/9088)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss_1: (0.9275) | Acc_1: (67.16%) (6963/10368)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss_1: (0.9243) | Acc_1: (67.23%) (7831/11648)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss_1: (0.9190) | Acc_1: (67.50%) (8726/12928)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss_1: (0.9173) | Acc_1: (67.47%) (9586/14208)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss_1: (0.9129) | Acc_1: (67.63%) (10474/15488)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss_1: (0.9116) | Acc_1: (67.78%) (11366/16768)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss_1: (0.9093) | Acc_1: (68.00%) (12273/18048)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss_1: (0.9112) | Acc_1: (67.95%) (13133/19328)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss_1: (0.9119) | Acc_1: (67.98%) (14009/20608)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss_1: (0.9135) | Acc_1: (67.94%) (14870/21888)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss_1: (0.9125) | Acc_1: (67.97%) (15747/23168)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss_1: (0.9080) | Acc_1: (68.10%) (16650/24448)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss_1: (0.9067) | Acc_1: (68.09%) (17519/25728)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss_1: (0.9058) | Acc_1: (68.14%) (18403/27008)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss_1: (0.9084) | Acc_1: (68.11%) (19266/28288)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss_1: (0.9058) | Acc_1: (68.18%) (20158/29568)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss_1: (0.9038) | Acc_1: (68.22%) (21044/30848)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss_1: (0.9022) | Acc_1: (68.23%) (21922/32128)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss_1: (0.9015) | Acc_1: (68.30%) (22817/33408)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss_1: (0.8996) | Acc_1: (68.38%) (23718/34688)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss_1: (0.8994) | Acc_1: (68.41%) (24605/35968)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss_1: (0.8970) | Acc_1: (68.49%) (25511/37248)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss_1: (0.8960) | Acc_1: (68.47%) (26379/38528)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss_1: (0.8953) | Acc_1: (68.50%) (27267/39808)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss_1: (0.8956) | Acc_1: (68.50%) (28146/41088)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss_1: (0.8956) | Acc_1: (68.50%) (29022/42368)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss_1: (0.8939) | Acc_1: (68.57%) (29928/43648)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss_1: (0.8932) | Acc_1: (68.63%) (30832/44928)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss_1: (0.8932) | Acc_1: (68.65%) (31723/46208)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss_1: (0.8908) | Acc_1: (68.78%) (32663/47488)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss_1: (0.8908) | Acc_1: (68.79%) (33548/48768)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss_1: (0.8892) | Acc_1: (68.86%) (34428/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8052) | Acc: (71.87%) (7187/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (0.7692) | Acc_1: (71.09%) (91/128)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (0.8052) | Acc_1: (70.88%) (998/1408)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (0.8219) | Acc_1: (71.09%) (1911/2688)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (0.8042) | Acc_1: (71.93%) (2854/3968)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss_1: (0.8141) | Acc_1: (71.51%) (3753/5248)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss_1: (0.8112) | Acc_1: (71.52%) (4669/6528)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss_1: (0.8068) | Acc_1: (72.00%) (5622/7808)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss_1: (0.8056) | Acc_1: (71.97%) (6541/9088)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss_1: (0.8016) | Acc_1: (71.86%) (7450/10368)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss_1: (0.8051) | Acc_1: (71.75%) (8358/11648)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss_1: (0.8117) | Acc_1: (71.46%) (9238/12928)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss_1: (0.8111) | Acc_1: (71.54%) (10164/14208)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss_1: (0.8087) | Acc_1: (71.63%) (11094/15488)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss_1: (0.8098) | Acc_1: (71.67%) (12017/16768)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss_1: (0.8110) | Acc_1: (71.68%) (12937/18048)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss_1: (0.8081) | Acc_1: (71.76%) (13869/19328)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss_1: (0.8105) | Acc_1: (71.71%) (14779/20608)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss_1: (0.8121) | Acc_1: (71.71%) (15695/21888)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss_1: (0.8144) | Acc_1: (71.65%) (16599/23168)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss_1: (0.8162) | Acc_1: (71.51%) (17483/24448)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss_1: (0.8160) | Acc_1: (71.48%) (18390/25728)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss_1: (0.8147) | Acc_1: (71.54%) (19322/27008)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss_1: (0.8141) | Acc_1: (71.57%) (20246/28288)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss_1: (0.8143) | Acc_1: (71.55%) (21157/29568)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss_1: (0.8129) | Acc_1: (71.61%) (22091/30848)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss_1: (0.8121) | Acc_1: (71.66%) (23022/32128)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss_1: (0.8111) | Acc_1: (71.67%) (23944/33408)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss_1: (0.8108) | Acc_1: (71.61%) (24839/34688)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss_1: (0.8103) | Acc_1: (71.59%) (25750/35968)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss_1: (0.8092) | Acc_1: (71.64%) (26685/37248)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss_1: (0.8074) | Acc_1: (71.68%) (27617/38528)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss_1: (0.8054) | Acc_1: (71.75%) (28564/39808)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss_1: (0.8024) | Acc_1: (71.85%) (29520/41088)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss_1: (0.8011) | Acc_1: (71.88%) (30453/42368)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss_1: (0.8002) | Acc_1: (71.88%) (31372/43648)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss_1: (0.7979) | Acc_1: (71.95%) (32327/44928)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss_1: (0.7965) | Acc_1: (72.02%) (33281/46208)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss_1: (0.7958) | Acc_1: (72.04%) (34209/47488)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss_1: (0.7946) | Acc_1: (72.11%) (35166/48768)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss_1: (0.7947) | Acc_1: (72.10%) (36050/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7279) | Acc: (75.61%) (7561/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (0.6992) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (0.7514) | Acc_1: (74.22%) (1045/1408)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (0.7423) | Acc_1: (74.70%) (2008/2688)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (0.7504) | Acc_1: (74.17%) (2943/3968)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss_1: (0.7455) | Acc_1: (74.09%) (3888/5248)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss_1: (0.7416) | Acc_1: (74.20%) (4844/6528)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss_1: (0.7401) | Acc_1: (74.36%) (5806/7808)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss_1: (0.7303) | Acc_1: (74.69%) (6788/9088)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss_1: (0.7287) | Acc_1: (74.74%) (7749/10368)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss_1: (0.7357) | Acc_1: (74.52%) (8680/11648)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss_1: (0.7322) | Acc_1: (74.47%) (9627/12928)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss_1: (0.7320) | Acc_1: (74.52%) (10588/14208)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss_1: (0.7303) | Acc_1: (74.56%) (11548/15488)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss_1: (0.7272) | Acc_1: (74.64%) (12515/16768)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss_1: (0.7289) | Acc_1: (74.59%) (13462/18048)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss_1: (0.7300) | Acc_1: (74.49%) (14397/19328)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss_1: (0.7293) | Acc_1: (74.56%) (15365/20608)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss_1: (0.7290) | Acc_1: (74.57%) (16322/21888)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss_1: (0.7327) | Acc_1: (74.43%) (17245/23168)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss_1: (0.7344) | Acc_1: (74.39%) (18188/24448)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss_1: (0.7299) | Acc_1: (74.53%) (19174/25728)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss_1: (0.7290) | Acc_1: (74.64%) (20158/27008)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss_1: (0.7279) | Acc_1: (74.64%) (21113/28288)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss_1: (0.7269) | Acc_1: (74.64%) (22069/29568)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss_1: (0.7270) | Acc_1: (74.68%) (23038/30848)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss_1: (0.7273) | Acc_1: (74.62%) (23975/32128)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss_1: (0.7271) | Acc_1: (74.63%) (24932/33408)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss_1: (0.7288) | Acc_1: (74.57%) (25866/34688)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss_1: (0.7269) | Acc_1: (74.65%) (26850/35968)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss_1: (0.7277) | Acc_1: (74.60%) (27788/37248)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss_1: (0.7267) | Acc_1: (74.68%) (28773/38528)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss_1: (0.7270) | Acc_1: (74.73%) (29749/39808)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss_1: (0.7265) | Acc_1: (74.76%) (30718/41088)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss_1: (0.7244) | Acc_1: (74.83%) (31702/42368)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss_1: (0.7242) | Acc_1: (74.84%) (32665/43648)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss_1: (0.7234) | Acc_1: (74.88%) (33641/44928)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss_1: (0.7240) | Acc_1: (74.88%) (34599/46208)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss_1: (0.7236) | Acc_1: (74.92%) (35577/47488)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss_1: (0.7225) | Acc_1: (74.98%) (36567/48768)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss_1: (0.7219) | Acc_1: (74.99%) (37497/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6667) | Acc: (77.38%) (7738/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (0.4836) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (0.6499) | Acc_1: (76.85%) (1082/1408)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (0.6510) | Acc_1: (77.34%) (2079/2688)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.6608) | Acc_1: (76.76%) (3046/3968)\n",
      "Epoch: 8 | Batch_idx: 40 |  Loss_1: (0.6705) | Acc_1: (76.51%) (4015/5248)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss_1: (0.6712) | Acc_1: (76.49%) (4993/6528)\n",
      "Epoch: 8 | Batch_idx: 60 |  Loss_1: (0.6732) | Acc_1: (76.55%) (5977/7808)\n",
      "Epoch: 8 | Batch_idx: 70 |  Loss_1: (0.6700) | Acc_1: (76.65%) (6966/9088)\n",
      "Epoch: 8 | Batch_idx: 80 |  Loss_1: (0.6694) | Acc_1: (76.61%) (7943/10368)\n",
      "Epoch: 8 | Batch_idx: 90 |  Loss_1: (0.6672) | Acc_1: (76.67%) (8930/11648)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss_1: (0.6741) | Acc_1: (76.27%) (9860/12928)\n",
      "Epoch: 8 | Batch_idx: 110 |  Loss_1: (0.6742) | Acc_1: (76.41%) (10857/14208)\n",
      "Epoch: 8 | Batch_idx: 120 |  Loss_1: (0.6741) | Acc_1: (76.38%) (11829/15488)\n",
      "Epoch: 8 | Batch_idx: 130 |  Loss_1: (0.6781) | Acc_1: (76.35%) (12802/16768)\n",
      "Epoch: 8 | Batch_idx: 140 |  Loss_1: (0.6755) | Acc_1: (76.51%) (13808/18048)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss_1: (0.6761) | Acc_1: (76.52%) (14790/19328)\n",
      "Epoch: 8 | Batch_idx: 160 |  Loss_1: (0.6776) | Acc_1: (76.39%) (15743/20608)\n",
      "Epoch: 8 | Batch_idx: 170 |  Loss_1: (0.6776) | Acc_1: (76.45%) (16734/21888)\n",
      "Epoch: 8 | Batch_idx: 180 |  Loss_1: (0.6753) | Acc_1: (76.47%) (17716/23168)\n",
      "Epoch: 8 | Batch_idx: 190 |  Loss_1: (0.6728) | Acc_1: (76.65%) (18740/24448)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss_1: (0.6737) | Acc_1: (76.64%) (19717/25728)\n",
      "Epoch: 8 | Batch_idx: 210 |  Loss_1: (0.6715) | Acc_1: (76.69%) (20712/27008)\n",
      "Epoch: 8 | Batch_idx: 220 |  Loss_1: (0.6729) | Acc_1: (76.65%) (21682/28288)\n",
      "Epoch: 8 | Batch_idx: 230 |  Loss_1: (0.6734) | Acc_1: (76.68%) (22673/29568)\n",
      "Epoch: 8 | Batch_idx: 240 |  Loss_1: (0.6750) | Acc_1: (76.68%) (23654/30848)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss_1: (0.6740) | Acc_1: (76.72%) (24647/32128)\n",
      "Epoch: 8 | Batch_idx: 260 |  Loss_1: (0.6736) | Acc_1: (76.72%) (25629/33408)\n",
      "Epoch: 8 | Batch_idx: 270 |  Loss_1: (0.6739) | Acc_1: (76.71%) (26608/34688)\n",
      "Epoch: 8 | Batch_idx: 280 |  Loss_1: (0.6733) | Acc_1: (76.71%) (27590/35968)\n",
      "Epoch: 8 | Batch_idx: 290 |  Loss_1: (0.6706) | Acc_1: (76.80%) (28605/37248)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss_1: (0.6698) | Acc_1: (76.81%) (29595/38528)\n",
      "Epoch: 8 | Batch_idx: 310 |  Loss_1: (0.6696) | Acc_1: (76.80%) (30572/39808)\n",
      "Epoch: 8 | Batch_idx: 320 |  Loss_1: (0.6693) | Acc_1: (76.82%) (31562/41088)\n",
      "Epoch: 8 | Batch_idx: 330 |  Loss_1: (0.6690) | Acc_1: (76.80%) (32540/42368)\n",
      "Epoch: 8 | Batch_idx: 340 |  Loss_1: (0.6692) | Acc_1: (76.82%) (33532/43648)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss_1: (0.6699) | Acc_1: (76.82%) (34513/44928)\n",
      "Epoch: 8 | Batch_idx: 360 |  Loss_1: (0.6692) | Acc_1: (76.84%) (35505/46208)\n",
      "Epoch: 8 | Batch_idx: 370 |  Loss_1: (0.6687) | Acc_1: (76.88%) (36507/47488)\n",
      "Epoch: 8 | Batch_idx: 380 |  Loss_1: (0.6696) | Acc_1: (76.85%) (37479/48768)\n",
      "Epoch: 8 | Batch_idx: 390 |  Loss_1: (0.6691) | Acc_1: (76.87%) (38435/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6394) | Acc: (78.16%) (7816/10000)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (0.5749) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (0.5856) | Acc_1: (79.97%) (1126/1408)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (0.6329) | Acc_1: (78.27%) (2104/2688)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (0.6321) | Acc_1: (77.80%) (3087/3968)\n",
      "Epoch: 9 | Batch_idx: 40 |  Loss_1: (0.6218) | Acc_1: (78.37%) (4113/5248)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss_1: (0.6238) | Acc_1: (78.43%) (5120/6528)\n",
      "Epoch: 9 | Batch_idx: 60 |  Loss_1: (0.6196) | Acc_1: (78.50%) (6129/7808)\n",
      "Epoch: 9 | Batch_idx: 70 |  Loss_1: (0.6197) | Acc_1: (78.50%) (7134/9088)\n",
      "Epoch: 9 | Batch_idx: 80 |  Loss_1: (0.6211) | Acc_1: (78.39%) (8127/10368)\n",
      "Epoch: 9 | Batch_idx: 90 |  Loss_1: (0.6234) | Acc_1: (78.31%) (9121/11648)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss_1: (0.6275) | Acc_1: (78.13%) (10101/12928)\n",
      "Epoch: 9 | Batch_idx: 110 |  Loss_1: (0.6329) | Acc_1: (77.98%) (11079/14208)\n",
      "Epoch: 9 | Batch_idx: 120 |  Loss_1: (0.6311) | Acc_1: (78.09%) (12094/15488)\n",
      "Epoch: 9 | Batch_idx: 130 |  Loss_1: (0.6300) | Acc_1: (78.09%) (13094/16768)\n",
      "Epoch: 9 | Batch_idx: 140 |  Loss_1: (0.6325) | Acc_1: (77.99%) (14076/18048)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss_1: (0.6330) | Acc_1: (77.93%) (15062/19328)\n",
      "Epoch: 9 | Batch_idx: 160 |  Loss_1: (0.6320) | Acc_1: (78.00%) (16074/20608)\n",
      "Epoch: 9 | Batch_idx: 170 |  Loss_1: (0.6294) | Acc_1: (78.12%) (17098/21888)\n",
      "Epoch: 9 | Batch_idx: 180 |  Loss_1: (0.6283) | Acc_1: (78.18%) (18112/23168)\n",
      "Epoch: 9 | Batch_idx: 190 |  Loss_1: (0.6282) | Acc_1: (78.13%) (19102/24448)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss_1: (0.6269) | Acc_1: (78.21%) (20121/25728)\n",
      "Epoch: 9 | Batch_idx: 210 |  Loss_1: (0.6231) | Acc_1: (78.39%) (21172/27008)\n",
      "Epoch: 9 | Batch_idx: 220 |  Loss_1: (0.6231) | Acc_1: (78.35%) (22163/28288)\n",
      "Epoch: 9 | Batch_idx: 230 |  Loss_1: (0.6220) | Acc_1: (78.39%) (23179/29568)\n",
      "Epoch: 9 | Batch_idx: 240 |  Loss_1: (0.6206) | Acc_1: (78.49%) (24214/30848)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss_1: (0.6226) | Acc_1: (78.45%) (25206/32128)\n",
      "Epoch: 9 | Batch_idx: 260 |  Loss_1: (0.6205) | Acc_1: (78.48%) (26217/33408)\n",
      "Epoch: 9 | Batch_idx: 270 |  Loss_1: (0.6192) | Acc_1: (78.55%) (27247/34688)\n",
      "Epoch: 9 | Batch_idx: 280 |  Loss_1: (0.6209) | Acc_1: (78.47%) (28223/35968)\n",
      "Epoch: 9 | Batch_idx: 290 |  Loss_1: (0.6185) | Acc_1: (78.53%) (29250/37248)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss_1: (0.6172) | Acc_1: (78.55%) (30265/38528)\n",
      "Epoch: 9 | Batch_idx: 310 |  Loss_1: (0.6162) | Acc_1: (78.55%) (31269/39808)\n",
      "Epoch: 9 | Batch_idx: 320 |  Loss_1: (0.6158) | Acc_1: (78.58%) (32289/41088)\n",
      "Epoch: 9 | Batch_idx: 330 |  Loss_1: (0.6163) | Acc_1: (78.57%) (33287/42368)\n",
      "Epoch: 9 | Batch_idx: 340 |  Loss_1: (0.6153) | Acc_1: (78.62%) (34318/43648)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss_1: (0.6132) | Acc_1: (78.71%) (35365/44928)\n",
      "Epoch: 9 | Batch_idx: 360 |  Loss_1: (0.6131) | Acc_1: (78.76%) (36393/46208)\n",
      "Epoch: 9 | Batch_idx: 370 |  Loss_1: (0.6124) | Acc_1: (78.77%) (37405/47488)\n",
      "Epoch: 9 | Batch_idx: 380 |  Loss_1: (0.6127) | Acc_1: (78.74%) (38401/48768)\n",
      "Epoch: 9 | Batch_idx: 390 |  Loss_1: (0.6136) | Acc_1: (78.73%) (39363/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5600) | Acc: (81.09%) (8109/10000)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (0.6700) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.5337) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.5513) | Acc_1: (81.44%) (2189/2688)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.5507) | Acc_1: (81.00%) (3214/3968)\n",
      "Epoch: 10 | Batch_idx: 40 |  Loss_1: (0.5628) | Acc_1: (80.70%) (4235/5248)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss_1: (0.5671) | Acc_1: (80.68%) (5267/6528)\n",
      "Epoch: 10 | Batch_idx: 60 |  Loss_1: (0.5701) | Acc_1: (80.40%) (6278/7808)\n",
      "Epoch: 10 | Batch_idx: 70 |  Loss_1: (0.5751) | Acc_1: (80.17%) (7286/9088)\n",
      "Epoch: 10 | Batch_idx: 80 |  Loss_1: (0.5744) | Acc_1: (80.21%) (8316/10368)\n",
      "Epoch: 10 | Batch_idx: 90 |  Loss_1: (0.5745) | Acc_1: (80.36%) (9360/11648)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss_1: (0.5703) | Acc_1: (80.52%) (10409/12928)\n",
      "Epoch: 10 | Batch_idx: 110 |  Loss_1: (0.5705) | Acc_1: (80.45%) (11430/14208)\n",
      "Epoch: 10 | Batch_idx: 120 |  Loss_1: (0.5714) | Acc_1: (80.32%) (12440/15488)\n",
      "Epoch: 10 | Batch_idx: 130 |  Loss_1: (0.5733) | Acc_1: (80.23%) (13453/16768)\n",
      "Epoch: 10 | Batch_idx: 140 |  Loss_1: (0.5774) | Acc_1: (80.13%) (14462/18048)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss_1: (0.5758) | Acc_1: (80.15%) (15492/19328)\n",
      "Epoch: 10 | Batch_idx: 160 |  Loss_1: (0.5754) | Acc_1: (80.20%) (16528/20608)\n",
      "Epoch: 10 | Batch_idx: 170 |  Loss_1: (0.5763) | Acc_1: (80.20%) (17555/21888)\n",
      "Epoch: 10 | Batch_idx: 180 |  Loss_1: (0.5759) | Acc_1: (80.24%) (18590/23168)\n",
      "Epoch: 10 | Batch_idx: 190 |  Loss_1: (0.5772) | Acc_1: (80.15%) (19595/24448)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss_1: (0.5786) | Acc_1: (80.12%) (20614/25728)\n",
      "Epoch: 10 | Batch_idx: 210 |  Loss_1: (0.5785) | Acc_1: (80.11%) (21636/27008)\n",
      "Epoch: 10 | Batch_idx: 220 |  Loss_1: (0.5770) | Acc_1: (80.16%) (22675/28288)\n",
      "Epoch: 10 | Batch_idx: 230 |  Loss_1: (0.5780) | Acc_1: (80.12%) (23689/29568)\n",
      "Epoch: 10 | Batch_idx: 240 |  Loss_1: (0.5799) | Acc_1: (80.07%) (24701/30848)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss_1: (0.5800) | Acc_1: (80.04%) (25716/32128)\n",
      "Epoch: 10 | Batch_idx: 260 |  Loss_1: (0.5813) | Acc_1: (79.96%) (26713/33408)\n",
      "Epoch: 10 | Batch_idx: 270 |  Loss_1: (0.5808) | Acc_1: (80.00%) (27749/34688)\n",
      "Epoch: 10 | Batch_idx: 280 |  Loss_1: (0.5799) | Acc_1: (80.05%) (28791/35968)\n",
      "Epoch: 10 | Batch_idx: 290 |  Loss_1: (0.5795) | Acc_1: (80.04%) (29812/37248)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss_1: (0.5784) | Acc_1: (80.11%) (30863/38528)\n",
      "Epoch: 10 | Batch_idx: 310 |  Loss_1: (0.5779) | Acc_1: (80.10%) (31886/39808)\n",
      "Epoch: 10 | Batch_idx: 320 |  Loss_1: (0.5777) | Acc_1: (80.09%) (32907/41088)\n",
      "Epoch: 10 | Batch_idx: 330 |  Loss_1: (0.5789) | Acc_1: (80.05%) (33917/42368)\n",
      "Epoch: 10 | Batch_idx: 340 |  Loss_1: (0.5793) | Acc_1: (80.03%) (34933/43648)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss_1: (0.5793) | Acc_1: (80.05%) (35963/44928)\n",
      "Epoch: 10 | Batch_idx: 360 |  Loss_1: (0.5799) | Acc_1: (80.01%) (36973/46208)\n",
      "Epoch: 10 | Batch_idx: 370 |  Loss_1: (0.5793) | Acc_1: (80.05%) (38015/47488)\n",
      "Epoch: 10 | Batch_idx: 380 |  Loss_1: (0.5816) | Acc_1: (79.97%) (38998/48768)\n",
      "Epoch: 10 | Batch_idx: 390 |  Loss_1: (0.5815) | Acc_1: (79.98%) (39989/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6352) | Acc: (79.11%) (7911/10000)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.5331) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.4976) | Acc_1: (81.96%) (1154/1408)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.5048) | Acc_1: (81.96%) (2203/2688)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.5221) | Acc_1: (81.50%) (3234/3968)\n",
      "Epoch: 11 | Batch_idx: 40 |  Loss_1: (0.5236) | Acc_1: (81.46%) (4275/5248)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss_1: (0.5272) | Acc_1: (81.19%) (5300/6528)\n",
      "Epoch: 11 | Batch_idx: 60 |  Loss_1: (0.5299) | Acc_1: (81.19%) (6339/7808)\n",
      "Epoch: 11 | Batch_idx: 70 |  Loss_1: (0.5286) | Acc_1: (81.31%) (7389/9088)\n",
      "Epoch: 11 | Batch_idx: 80 |  Loss_1: (0.5286) | Acc_1: (81.43%) (8443/10368)\n",
      "Epoch: 11 | Batch_idx: 90 |  Loss_1: (0.5314) | Acc_1: (81.47%) (9490/11648)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss_1: (0.5291) | Acc_1: (81.55%) (10543/12928)\n",
      "Epoch: 11 | Batch_idx: 110 |  Loss_1: (0.5316) | Acc_1: (81.41%) (11567/14208)\n",
      "Epoch: 11 | Batch_idx: 120 |  Loss_1: (0.5340) | Acc_1: (81.37%) (12603/15488)\n",
      "Epoch: 11 | Batch_idx: 130 |  Loss_1: (0.5343) | Acc_1: (81.37%) (13644/16768)\n",
      "Epoch: 11 | Batch_idx: 140 |  Loss_1: (0.5379) | Acc_1: (81.24%) (14663/18048)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss_1: (0.5392) | Acc_1: (81.24%) (15703/19328)\n",
      "Epoch: 11 | Batch_idx: 160 |  Loss_1: (0.5391) | Acc_1: (81.30%) (16754/20608)\n",
      "Epoch: 11 | Batch_idx: 170 |  Loss_1: (0.5392) | Acc_1: (81.29%) (17793/21888)\n",
      "Epoch: 11 | Batch_idx: 180 |  Loss_1: (0.5422) | Acc_1: (81.20%) (18812/23168)\n",
      "Epoch: 11 | Batch_idx: 190 |  Loss_1: (0.5407) | Acc_1: (81.24%) (19861/24448)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss_1: (0.5409) | Acc_1: (81.25%) (20904/25728)\n",
      "Epoch: 11 | Batch_idx: 210 |  Loss_1: (0.5413) | Acc_1: (81.25%) (21945/27008)\n",
      "Epoch: 11 | Batch_idx: 220 |  Loss_1: (0.5418) | Acc_1: (81.22%) (22975/28288)\n",
      "Epoch: 11 | Batch_idx: 230 |  Loss_1: (0.5415) | Acc_1: (81.23%) (24017/29568)\n",
      "Epoch: 11 | Batch_idx: 240 |  Loss_1: (0.5424) | Acc_1: (81.17%) (25038/30848)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss_1: (0.5429) | Acc_1: (81.14%) (26070/32128)\n",
      "Epoch: 11 | Batch_idx: 260 |  Loss_1: (0.5431) | Acc_1: (81.13%) (27105/33408)\n",
      "Epoch: 11 | Batch_idx: 270 |  Loss_1: (0.5430) | Acc_1: (81.14%) (28146/34688)\n",
      "Epoch: 11 | Batch_idx: 280 |  Loss_1: (0.5419) | Acc_1: (81.19%) (29201/35968)\n",
      "Epoch: 11 | Batch_idx: 290 |  Loss_1: (0.5404) | Acc_1: (81.22%) (30254/37248)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss_1: (0.5406) | Acc_1: (81.22%) (31292/38528)\n",
      "Epoch: 11 | Batch_idx: 310 |  Loss_1: (0.5404) | Acc_1: (81.19%) (32322/39808)\n",
      "Epoch: 11 | Batch_idx: 320 |  Loss_1: (0.5403) | Acc_1: (81.18%) (33354/41088)\n",
      "Epoch: 11 | Batch_idx: 330 |  Loss_1: (0.5402) | Acc_1: (81.19%) (34397/42368)\n",
      "Epoch: 11 | Batch_idx: 340 |  Loss_1: (0.5391) | Acc_1: (81.22%) (35450/43648)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss_1: (0.5382) | Acc_1: (81.25%) (36502/44928)\n",
      "Epoch: 11 | Batch_idx: 360 |  Loss_1: (0.5383) | Acc_1: (81.26%) (37549/46208)\n",
      "Epoch: 11 | Batch_idx: 370 |  Loss_1: (0.5385) | Acc_1: (81.25%) (38586/47488)\n",
      "Epoch: 11 | Batch_idx: 380 |  Loss_1: (0.5380) | Acc_1: (81.27%) (39634/48768)\n",
      "Epoch: 11 | Batch_idx: 390 |  Loss_1: (0.5378) | Acc_1: (81.28%) (40642/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5523) | Acc: (81.93%) (8193/10000)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.4464) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.4850) | Acc_1: (83.66%) (1178/1408)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.5032) | Acc_1: (82.33%) (2213/2688)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.5003) | Acc_1: (82.66%) (3280/3968)\n",
      "Epoch: 12 | Batch_idx: 40 |  Loss_1: (0.5075) | Acc_1: (82.55%) (4332/5248)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss_1: (0.5066) | Acc_1: (82.72%) (5400/6528)\n",
      "Epoch: 12 | Batch_idx: 60 |  Loss_1: (0.5044) | Acc_1: (82.95%) (6477/7808)\n",
      "Epoch: 12 | Batch_idx: 70 |  Loss_1: (0.5094) | Acc_1: (82.78%) (7523/9088)\n",
      "Epoch: 12 | Batch_idx: 80 |  Loss_1: (0.5052) | Acc_1: (82.85%) (8590/10368)\n",
      "Epoch: 12 | Batch_idx: 90 |  Loss_1: (0.5041) | Acc_1: (82.92%) (9658/11648)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss_1: (0.5051) | Acc_1: (82.84%) (10710/12928)\n",
      "Epoch: 12 | Batch_idx: 110 |  Loss_1: (0.5091) | Acc_1: (82.85%) (11771/14208)\n",
      "Epoch: 12 | Batch_idx: 120 |  Loss_1: (0.5109) | Acc_1: (82.69%) (12807/15488)\n",
      "Epoch: 12 | Batch_idx: 130 |  Loss_1: (0.5126) | Acc_1: (82.54%) (13840/16768)\n",
      "Epoch: 12 | Batch_idx: 140 |  Loss_1: (0.5145) | Acc_1: (82.46%) (14882/18048)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss_1: (0.5150) | Acc_1: (82.39%) (15924/19328)\n",
      "Epoch: 12 | Batch_idx: 160 |  Loss_1: (0.5162) | Acc_1: (82.39%) (16978/20608)\n",
      "Epoch: 12 | Batch_idx: 170 |  Loss_1: (0.5173) | Acc_1: (82.28%) (18009/21888)\n",
      "Epoch: 12 | Batch_idx: 180 |  Loss_1: (0.5186) | Acc_1: (82.21%) (19046/23168)\n",
      "Epoch: 12 | Batch_idx: 190 |  Loss_1: (0.5184) | Acc_1: (82.21%) (20099/24448)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss_1: (0.5163) | Acc_1: (82.30%) (21173/25728)\n",
      "Epoch: 12 | Batch_idx: 210 |  Loss_1: (0.5157) | Acc_1: (82.31%) (22230/27008)\n",
      "Epoch: 12 | Batch_idx: 220 |  Loss_1: (0.5164) | Acc_1: (82.26%) (23271/28288)\n",
      "Epoch: 12 | Batch_idx: 230 |  Loss_1: (0.5149) | Acc_1: (82.30%) (24334/29568)\n",
      "Epoch: 12 | Batch_idx: 240 |  Loss_1: (0.5157) | Acc_1: (82.23%) (25367/30848)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss_1: (0.5152) | Acc_1: (82.30%) (26441/32128)\n",
      "Epoch: 12 | Batch_idx: 260 |  Loss_1: (0.5156) | Acc_1: (82.29%) (27490/33408)\n",
      "Epoch: 12 | Batch_idx: 270 |  Loss_1: (0.5148) | Acc_1: (82.34%) (28563/34688)\n",
      "Epoch: 12 | Batch_idx: 280 |  Loss_1: (0.5161) | Acc_1: (82.28%) (29593/35968)\n",
      "Epoch: 12 | Batch_idx: 290 |  Loss_1: (0.5158) | Acc_1: (82.27%) (30645/37248)\n",
      "Epoch: 12 | Batch_idx: 300 |  Loss_1: (0.5156) | Acc_1: (82.28%) (31699/38528)\n",
      "Epoch: 12 | Batch_idx: 310 |  Loss_1: (0.5155) | Acc_1: (82.26%) (32748/39808)\n",
      "Epoch: 12 | Batch_idx: 320 |  Loss_1: (0.5164) | Acc_1: (82.24%) (33792/41088)\n",
      "Epoch: 12 | Batch_idx: 330 |  Loss_1: (0.5160) | Acc_1: (82.26%) (34851/42368)\n",
      "Epoch: 12 | Batch_idx: 340 |  Loss_1: (0.5170) | Acc_1: (82.21%) (35883/43648)\n",
      "Epoch: 12 | Batch_idx: 350 |  Loss_1: (0.5157) | Acc_1: (82.25%) (36954/44928)\n",
      "Epoch: 12 | Batch_idx: 360 |  Loss_1: (0.5153) | Acc_1: (82.25%) (38005/46208)\n",
      "Epoch: 12 | Batch_idx: 370 |  Loss_1: (0.5148) | Acc_1: (82.28%) (39072/47488)\n",
      "Epoch: 12 | Batch_idx: 380 |  Loss_1: (0.5144) | Acc_1: (82.28%) (40128/48768)\n",
      "Epoch: 12 | Batch_idx: 390 |  Loss_1: (0.5142) | Acc_1: (82.30%) (41148/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5083) | Acc: (83.25%) (8325/10000)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.4766) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.4981) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.4769) | Acc_1: (83.41%) (2242/2688)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.4789) | Acc_1: (83.11%) (3298/3968)\n",
      "Epoch: 13 | Batch_idx: 40 |  Loss_1: (0.4740) | Acc_1: (83.21%) (4367/5248)\n",
      "Epoch: 13 | Batch_idx: 50 |  Loss_1: (0.4774) | Acc_1: (83.03%) (5420/6528)\n",
      "Epoch: 13 | Batch_idx: 60 |  Loss_1: (0.4813) | Acc_1: (82.97%) (6478/7808)\n",
      "Epoch: 13 | Batch_idx: 70 |  Loss_1: (0.4843) | Acc_1: (83.00%) (7543/9088)\n",
      "Epoch: 13 | Batch_idx: 80 |  Loss_1: (0.4889) | Acc_1: (82.92%) (8597/10368)\n",
      "Epoch: 13 | Batch_idx: 90 |  Loss_1: (0.4860) | Acc_1: (83.04%) (9672/11648)\n",
      "Epoch: 13 | Batch_idx: 100 |  Loss_1: (0.4851) | Acc_1: (83.08%) (10740/12928)\n",
      "Epoch: 13 | Batch_idx: 110 |  Loss_1: (0.4873) | Acc_1: (83.05%) (11800/14208)\n",
      "Epoch: 13 | Batch_idx: 120 |  Loss_1: (0.4837) | Acc_1: (83.17%) (12881/15488)\n",
      "Epoch: 13 | Batch_idx: 130 |  Loss_1: (0.4863) | Acc_1: (83.02%) (13920/16768)\n",
      "Epoch: 13 | Batch_idx: 140 |  Loss_1: (0.4883) | Acc_1: (83.02%) (14984/18048)\n",
      "Epoch: 13 | Batch_idx: 150 |  Loss_1: (0.4883) | Acc_1: (83.03%) (16048/19328)\n",
      "Epoch: 13 | Batch_idx: 160 |  Loss_1: (0.4876) | Acc_1: (83.08%) (17121/20608)\n",
      "Epoch: 13 | Batch_idx: 170 |  Loss_1: (0.4887) | Acc_1: (83.05%) (18177/21888)\n",
      "Epoch: 13 | Batch_idx: 180 |  Loss_1: (0.4885) | Acc_1: (83.09%) (19251/23168)\n",
      "Epoch: 13 | Batch_idx: 190 |  Loss_1: (0.4881) | Acc_1: (83.06%) (20306/24448)\n",
      "Epoch: 13 | Batch_idx: 200 |  Loss_1: (0.4863) | Acc_1: (83.10%) (21380/25728)\n",
      "Epoch: 13 | Batch_idx: 210 |  Loss_1: (0.4857) | Acc_1: (83.12%) (22448/27008)\n",
      "Epoch: 13 | Batch_idx: 220 |  Loss_1: (0.4857) | Acc_1: (83.11%) (23509/28288)\n",
      "Epoch: 13 | Batch_idx: 230 |  Loss_1: (0.4866) | Acc_1: (83.04%) (24552/29568)\n",
      "Epoch: 13 | Batch_idx: 240 |  Loss_1: (0.4886) | Acc_1: (82.96%) (25590/30848)\n",
      "Epoch: 13 | Batch_idx: 250 |  Loss_1: (0.4880) | Acc_1: (82.98%) (26661/32128)\n",
      "Epoch: 13 | Batch_idx: 260 |  Loss_1: (0.4899) | Acc_1: (82.95%) (27713/33408)\n",
      "Epoch: 13 | Batch_idx: 270 |  Loss_1: (0.4903) | Acc_1: (82.88%) (28749/34688)\n",
      "Epoch: 13 | Batch_idx: 280 |  Loss_1: (0.4899) | Acc_1: (82.89%) (29815/35968)\n",
      "Epoch: 13 | Batch_idx: 290 |  Loss_1: (0.4884) | Acc_1: (82.93%) (30891/37248)\n",
      "Epoch: 13 | Batch_idx: 300 |  Loss_1: (0.4866) | Acc_1: (83.02%) (31985/38528)\n",
      "Epoch: 13 | Batch_idx: 310 |  Loss_1: (0.4867) | Acc_1: (83.02%) (33050/39808)\n",
      "Epoch: 13 | Batch_idx: 320 |  Loss_1: (0.4877) | Acc_1: (83.03%) (34116/41088)\n",
      "Epoch: 13 | Batch_idx: 330 |  Loss_1: (0.4879) | Acc_1: (83.04%) (35182/42368)\n",
      "Epoch: 13 | Batch_idx: 340 |  Loss_1: (0.4882) | Acc_1: (83.01%) (36232/43648)\n",
      "Epoch: 13 | Batch_idx: 350 |  Loss_1: (0.4877) | Acc_1: (83.05%) (37312/44928)\n",
      "Epoch: 13 | Batch_idx: 360 |  Loss_1: (0.4886) | Acc_1: (83.05%) (38377/46208)\n",
      "Epoch: 13 | Batch_idx: 370 |  Loss_1: (0.4871) | Acc_1: (83.09%) (39459/47488)\n",
      "Epoch: 13 | Batch_idx: 380 |  Loss_1: (0.4872) | Acc_1: (83.10%) (40524/48768)\n",
      "Epoch: 13 | Batch_idx: 390 |  Loss_1: (0.4871) | Acc_1: (83.11%) (41556/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6804) | Acc: (78.50%) (7850/10000)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.5642) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.4647) | Acc_1: (83.66%) (1178/1408)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.4612) | Acc_1: (83.85%) (2254/2688)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.4521) | Acc_1: (83.77%) (3324/3968)\n",
      "Epoch: 14 | Batch_idx: 40 |  Loss_1: (0.4495) | Acc_1: (84.03%) (4410/5248)\n",
      "Epoch: 14 | Batch_idx: 50 |  Loss_1: (0.4464) | Acc_1: (84.27%) (5501/6528)\n",
      "Epoch: 14 | Batch_idx: 60 |  Loss_1: (0.4551) | Acc_1: (84.02%) (6560/7808)\n",
      "Epoch: 14 | Batch_idx: 70 |  Loss_1: (0.4590) | Acc_1: (83.82%) (7618/9088)\n",
      "Epoch: 14 | Batch_idx: 80 |  Loss_1: (0.4578) | Acc_1: (83.83%) (8691/10368)\n",
      "Epoch: 14 | Batch_idx: 90 |  Loss_1: (0.4636) | Acc_1: (83.75%) (9755/11648)\n",
      "Epoch: 14 | Batch_idx: 100 |  Loss_1: (0.4650) | Acc_1: (83.71%) (10822/12928)\n",
      "Epoch: 14 | Batch_idx: 110 |  Loss_1: (0.4651) | Acc_1: (83.76%) (11900/14208)\n",
      "Epoch: 14 | Batch_idx: 120 |  Loss_1: (0.4622) | Acc_1: (83.79%) (12977/15488)\n",
      "Epoch: 14 | Batch_idx: 130 |  Loss_1: (0.4635) | Acc_1: (83.80%) (14051/16768)\n",
      "Epoch: 14 | Batch_idx: 140 |  Loss_1: (0.4656) | Acc_1: (83.75%) (15115/18048)\n",
      "Epoch: 14 | Batch_idx: 150 |  Loss_1: (0.4664) | Acc_1: (83.80%) (16196/19328)\n",
      "Epoch: 14 | Batch_idx: 160 |  Loss_1: (0.4665) | Acc_1: (83.72%) (17252/20608)\n",
      "Epoch: 14 | Batch_idx: 170 |  Loss_1: (0.4655) | Acc_1: (83.77%) (18335/21888)\n",
      "Epoch: 14 | Batch_idx: 180 |  Loss_1: (0.4666) | Acc_1: (83.70%) (19392/23168)\n",
      "Epoch: 14 | Batch_idx: 190 |  Loss_1: (0.4701) | Acc_1: (83.58%) (20434/24448)\n",
      "Epoch: 14 | Batch_idx: 200 |  Loss_1: (0.4703) | Acc_1: (83.57%) (21500/25728)\n",
      "Epoch: 14 | Batch_idx: 210 |  Loss_1: (0.4706) | Acc_1: (83.58%) (22572/27008)\n",
      "Epoch: 14 | Batch_idx: 220 |  Loss_1: (0.4711) | Acc_1: (83.54%) (23632/28288)\n",
      "Epoch: 14 | Batch_idx: 230 |  Loss_1: (0.4721) | Acc_1: (83.55%) (24703/29568)\n",
      "Epoch: 14 | Batch_idx: 240 |  Loss_1: (0.4714) | Acc_1: (83.55%) (25773/30848)\n",
      "Epoch: 14 | Batch_idx: 250 |  Loss_1: (0.4725) | Acc_1: (83.54%) (26840/32128)\n",
      "Epoch: 14 | Batch_idx: 260 |  Loss_1: (0.4710) | Acc_1: (83.62%) (27936/33408)\n",
      "Epoch: 14 | Batch_idx: 270 |  Loss_1: (0.4706) | Acc_1: (83.68%) (29027/34688)\n",
      "Epoch: 14 | Batch_idx: 280 |  Loss_1: (0.4692) | Acc_1: (83.75%) (30122/35968)\n",
      "Epoch: 14 | Batch_idx: 290 |  Loss_1: (0.4691) | Acc_1: (83.79%) (31209/37248)\n",
      "Epoch: 14 | Batch_idx: 300 |  Loss_1: (0.4691) | Acc_1: (83.80%) (32288/38528)\n",
      "Epoch: 14 | Batch_idx: 310 |  Loss_1: (0.4695) | Acc_1: (83.76%) (33344/39808)\n",
      "Epoch: 14 | Batch_idx: 320 |  Loss_1: (0.4705) | Acc_1: (83.74%) (34409/41088)\n",
      "Epoch: 14 | Batch_idx: 330 |  Loss_1: (0.4716) | Acc_1: (83.71%) (35465/42368)\n",
      "Epoch: 14 | Batch_idx: 340 |  Loss_1: (0.4705) | Acc_1: (83.75%) (36554/43648)\n",
      "Epoch: 14 | Batch_idx: 350 |  Loss_1: (0.4706) | Acc_1: (83.77%) (37636/44928)\n",
      "Epoch: 14 | Batch_idx: 360 |  Loss_1: (0.4703) | Acc_1: (83.75%) (38701/46208)\n",
      "Epoch: 14 | Batch_idx: 370 |  Loss_1: (0.4701) | Acc_1: (83.76%) (39778/47488)\n",
      "Epoch: 14 | Batch_idx: 380 |  Loss_1: (0.4698) | Acc_1: (83.78%) (40856/48768)\n",
      "Epoch: 14 | Batch_idx: 390 |  Loss_1: (0.4687) | Acc_1: (83.81%) (41903/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5076) | Acc: (83.30%) (8330/10000)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.5195) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.4195) | Acc_1: (85.23%) (1200/1408)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.4205) | Acc_1: (85.16%) (2289/2688)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.4129) | Acc_1: (85.43%) (3390/3968)\n",
      "Epoch: 15 | Batch_idx: 40 |  Loss_1: (0.4257) | Acc_1: (85.25%) (4474/5248)\n",
      "Epoch: 15 | Batch_idx: 50 |  Loss_1: (0.4216) | Acc_1: (85.51%) (5582/6528)\n",
      "Epoch: 15 | Batch_idx: 60 |  Loss_1: (0.4286) | Acc_1: (85.09%) (6644/7808)\n",
      "Epoch: 15 | Batch_idx: 70 |  Loss_1: (0.4316) | Acc_1: (84.89%) (7715/9088)\n",
      "Epoch: 15 | Batch_idx: 80 |  Loss_1: (0.4343) | Acc_1: (84.82%) (8794/10368)\n",
      "Epoch: 15 | Batch_idx: 90 |  Loss_1: (0.4304) | Acc_1: (84.96%) (9896/11648)\n",
      "Epoch: 15 | Batch_idx: 100 |  Loss_1: (0.4276) | Acc_1: (84.94%) (10981/12928)\n",
      "Epoch: 15 | Batch_idx: 110 |  Loss_1: (0.4274) | Acc_1: (84.95%) (12070/14208)\n",
      "Epoch: 15 | Batch_idx: 120 |  Loss_1: (0.4300) | Acc_1: (84.95%) (13157/15488)\n",
      "Epoch: 15 | Batch_idx: 130 |  Loss_1: (0.4314) | Acc_1: (84.92%) (14240/16768)\n",
      "Epoch: 15 | Batch_idx: 140 |  Loss_1: (0.4331) | Acc_1: (84.85%) (15313/18048)\n",
      "Epoch: 15 | Batch_idx: 150 |  Loss_1: (0.4334) | Acc_1: (84.87%) (16404/19328)\n",
      "Epoch: 15 | Batch_idx: 160 |  Loss_1: (0.4353) | Acc_1: (84.78%) (17472/20608)\n",
      "Epoch: 15 | Batch_idx: 170 |  Loss_1: (0.4316) | Acc_1: (84.89%) (18580/21888)\n",
      "Epoch: 15 | Batch_idx: 180 |  Loss_1: (0.4347) | Acc_1: (84.77%) (19640/23168)\n",
      "Epoch: 15 | Batch_idx: 190 |  Loss_1: (0.4341) | Acc_1: (84.83%) (20739/24448)\n",
      "Epoch: 15 | Batch_idx: 200 |  Loss_1: (0.4351) | Acc_1: (84.81%) (21820/25728)\n",
      "Epoch: 15 | Batch_idx: 210 |  Loss_1: (0.4371) | Acc_1: (84.73%) (22883/27008)\n",
      "Epoch: 15 | Batch_idx: 220 |  Loss_1: (0.4383) | Acc_1: (84.71%) (23963/28288)\n",
      "Epoch: 15 | Batch_idx: 230 |  Loss_1: (0.4380) | Acc_1: (84.73%) (25054/29568)\n",
      "Epoch: 15 | Batch_idx: 240 |  Loss_1: (0.4390) | Acc_1: (84.67%) (26118/30848)\n",
      "Epoch: 15 | Batch_idx: 250 |  Loss_1: (0.4397) | Acc_1: (84.60%) (27181/32128)\n",
      "Epoch: 15 | Batch_idx: 260 |  Loss_1: (0.4400) | Acc_1: (84.61%) (28267/33408)\n",
      "Epoch: 15 | Batch_idx: 270 |  Loss_1: (0.4388) | Acc_1: (84.67%) (29369/34688)\n",
      "Epoch: 15 | Batch_idx: 280 |  Loss_1: (0.4382) | Acc_1: (84.68%) (30458/35968)\n",
      "Epoch: 15 | Batch_idx: 290 |  Loss_1: (0.4386) | Acc_1: (84.69%) (31544/37248)\n",
      "Epoch: 15 | Batch_idx: 300 |  Loss_1: (0.4381) | Acc_1: (84.70%) (32633/38528)\n",
      "Epoch: 15 | Batch_idx: 310 |  Loss_1: (0.4403) | Acc_1: (84.63%) (33689/39808)\n",
      "Epoch: 15 | Batch_idx: 320 |  Loss_1: (0.4407) | Acc_1: (84.65%) (34781/41088)\n",
      "Epoch: 15 | Batch_idx: 330 |  Loss_1: (0.4410) | Acc_1: (84.64%) (35862/42368)\n",
      "Epoch: 15 | Batch_idx: 340 |  Loss_1: (0.4419) | Acc_1: (84.59%) (36922/43648)\n",
      "Epoch: 15 | Batch_idx: 350 |  Loss_1: (0.4426) | Acc_1: (84.58%) (38001/44928)\n",
      "Epoch: 15 | Batch_idx: 360 |  Loss_1: (0.4422) | Acc_1: (84.60%) (39091/46208)\n",
      "Epoch: 15 | Batch_idx: 370 |  Loss_1: (0.4418) | Acc_1: (84.61%) (40181/47488)\n",
      "Epoch: 15 | Batch_idx: 380 |  Loss_1: (0.4424) | Acc_1: (84.59%) (41255/48768)\n",
      "Epoch: 15 | Batch_idx: 390 |  Loss_1: (0.4418) | Acc_1: (84.61%) (42307/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5412) | Acc: (82.76%) (8276/10000)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.4073) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.3652) | Acc_1: (87.57%) (1233/1408)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.3880) | Acc_1: (86.27%) (2319/2688)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.4070) | Acc_1: (85.79%) (3404/3968)\n",
      "Epoch: 16 | Batch_idx: 40 |  Loss_1: (0.4112) | Acc_1: (85.50%) (4487/5248)\n",
      "Epoch: 16 | Batch_idx: 50 |  Loss_1: (0.4102) | Acc_1: (85.51%) (5582/6528)\n",
      "Epoch: 16 | Batch_idx: 60 |  Loss_1: (0.4155) | Acc_1: (85.35%) (6664/7808)\n",
      "Epoch: 16 | Batch_idx: 70 |  Loss_1: (0.4211) | Acc_1: (85.33%) (7755/9088)\n",
      "Epoch: 16 | Batch_idx: 80 |  Loss_1: (0.4237) | Acc_1: (85.28%) (8842/10368)\n",
      "Epoch: 16 | Batch_idx: 90 |  Loss_1: (0.4216) | Acc_1: (85.40%) (9947/11648)\n",
      "Epoch: 16 | Batch_idx: 100 |  Loss_1: (0.4182) | Acc_1: (85.49%) (11052/12928)\n",
      "Epoch: 16 | Batch_idx: 110 |  Loss_1: (0.4191) | Acc_1: (85.42%) (12137/14208)\n",
      "Epoch: 16 | Batch_idx: 120 |  Loss_1: (0.4185) | Acc_1: (85.54%) (13249/15488)\n",
      "Epoch: 16 | Batch_idx: 130 |  Loss_1: (0.4158) | Acc_1: (85.63%) (14358/16768)\n",
      "Epoch: 16 | Batch_idx: 140 |  Loss_1: (0.4142) | Acc_1: (85.67%) (15461/18048)\n",
      "Epoch: 16 | Batch_idx: 150 |  Loss_1: (0.4167) | Acc_1: (85.56%) (16538/19328)\n",
      "Epoch: 16 | Batch_idx: 160 |  Loss_1: (0.4162) | Acc_1: (85.62%) (17644/20608)\n",
      "Epoch: 16 | Batch_idx: 170 |  Loss_1: (0.4158) | Acc_1: (85.63%) (18743/21888)\n",
      "Epoch: 16 | Batch_idx: 180 |  Loss_1: (0.4175) | Acc_1: (85.54%) (19818/23168)\n",
      "Epoch: 16 | Batch_idx: 190 |  Loss_1: (0.4158) | Acc_1: (85.59%) (20926/24448)\n",
      "Epoch: 16 | Batch_idx: 200 |  Loss_1: (0.4157) | Acc_1: (85.65%) (22037/25728)\n",
      "Epoch: 16 | Batch_idx: 210 |  Loss_1: (0.4158) | Acc_1: (85.64%) (23129/27008)\n",
      "Epoch: 16 | Batch_idx: 220 |  Loss_1: (0.4146) | Acc_1: (85.63%) (24222/28288)\n",
      "Epoch: 16 | Batch_idx: 230 |  Loss_1: (0.4147) | Acc_1: (85.62%) (25316/29568)\n",
      "Epoch: 16 | Batch_idx: 240 |  Loss_1: (0.4163) | Acc_1: (85.58%) (26400/30848)\n",
      "Epoch: 16 | Batch_idx: 250 |  Loss_1: (0.4165) | Acc_1: (85.59%) (27497/32128)\n",
      "Epoch: 16 | Batch_idx: 260 |  Loss_1: (0.4169) | Acc_1: (85.56%) (28585/33408)\n",
      "Epoch: 16 | Batch_idx: 270 |  Loss_1: (0.4181) | Acc_1: (85.53%) (29669/34688)\n",
      "Epoch: 16 | Batch_idx: 280 |  Loss_1: (0.4195) | Acc_1: (85.50%) (30752/35968)\n",
      "Epoch: 16 | Batch_idx: 290 |  Loss_1: (0.4197) | Acc_1: (85.48%) (31841/37248)\n",
      "Epoch: 16 | Batch_idx: 300 |  Loss_1: (0.4185) | Acc_1: (85.50%) (32943/38528)\n",
      "Epoch: 16 | Batch_idx: 310 |  Loss_1: (0.4190) | Acc_1: (85.48%) (34026/39808)\n",
      "Epoch: 16 | Batch_idx: 320 |  Loss_1: (0.4203) | Acc_1: (85.40%) (35088/41088)\n",
      "Epoch: 16 | Batch_idx: 330 |  Loss_1: (0.4209) | Acc_1: (85.40%) (36181/42368)\n",
      "Epoch: 16 | Batch_idx: 340 |  Loss_1: (0.4215) | Acc_1: (85.37%) (37264/43648)\n",
      "Epoch: 16 | Batch_idx: 350 |  Loss_1: (0.4217) | Acc_1: (85.33%) (38335/44928)\n",
      "Epoch: 16 | Batch_idx: 360 |  Loss_1: (0.4229) | Acc_1: (85.31%) (39420/46208)\n",
      "Epoch: 16 | Batch_idx: 370 |  Loss_1: (0.4234) | Acc_1: (85.31%) (40512/47488)\n",
      "Epoch: 16 | Batch_idx: 380 |  Loss_1: (0.4240) | Acc_1: (85.31%) (41606/48768)\n",
      "Epoch: 16 | Batch_idx: 390 |  Loss_1: (0.4246) | Acc_1: (85.30%) (42648/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4816) | Acc: (84.03%) (8403/10000)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.3695) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.3926) | Acc_1: (86.43%) (1217/1408)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.3913) | Acc_1: (86.35%) (2321/2688)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.4094) | Acc_1: (85.94%) (3410/3968)\n",
      "Epoch: 17 | Batch_idx: 40 |  Loss_1: (0.4064) | Acc_1: (86.09%) (4518/5248)\n",
      "Epoch: 17 | Batch_idx: 50 |  Loss_1: (0.4111) | Acc_1: (86.00%) (5614/6528)\n",
      "Epoch: 17 | Batch_idx: 60 |  Loss_1: (0.4072) | Acc_1: (85.98%) (6713/7808)\n",
      "Epoch: 17 | Batch_idx: 70 |  Loss_1: (0.4054) | Acc_1: (85.85%) (7802/9088)\n",
      "Epoch: 17 | Batch_idx: 80 |  Loss_1: (0.4078) | Acc_1: (85.88%) (8904/10368)\n",
      "Epoch: 17 | Batch_idx: 90 |  Loss_1: (0.4006) | Acc_1: (86.13%) (10032/11648)\n",
      "Epoch: 17 | Batch_idx: 100 |  Loss_1: (0.3985) | Acc_1: (86.19%) (11142/12928)\n",
      "Epoch: 17 | Batch_idx: 110 |  Loss_1: (0.3983) | Acc_1: (86.25%) (12254/14208)\n",
      "Epoch: 17 | Batch_idx: 120 |  Loss_1: (0.3972) | Acc_1: (86.22%) (13353/15488)\n",
      "Epoch: 17 | Batch_idx: 130 |  Loss_1: (0.3981) | Acc_1: (86.21%) (14455/16768)\n",
      "Epoch: 17 | Batch_idx: 140 |  Loss_1: (0.3996) | Acc_1: (86.11%) (15542/18048)\n",
      "Epoch: 17 | Batch_idx: 150 |  Loss_1: (0.4002) | Acc_1: (86.11%) (16643/19328)\n",
      "Epoch: 17 | Batch_idx: 160 |  Loss_1: (0.4000) | Acc_1: (86.16%) (17755/20608)\n",
      "Epoch: 17 | Batch_idx: 170 |  Loss_1: (0.4003) | Acc_1: (86.17%) (18861/21888)\n",
      "Epoch: 17 | Batch_idx: 180 |  Loss_1: (0.3990) | Acc_1: (86.19%) (19969/23168)\n",
      "Epoch: 17 | Batch_idx: 190 |  Loss_1: (0.3998) | Acc_1: (86.22%) (21080/24448)\n",
      "Epoch: 17 | Batch_idx: 200 |  Loss_1: (0.4002) | Acc_1: (86.21%) (22181/25728)\n",
      "Epoch: 17 | Batch_idx: 210 |  Loss_1: (0.4006) | Acc_1: (86.18%) (23275/27008)\n",
      "Epoch: 17 | Batch_idx: 220 |  Loss_1: (0.3997) | Acc_1: (86.23%) (24392/28288)\n",
      "Epoch: 17 | Batch_idx: 230 |  Loss_1: (0.4008) | Acc_1: (86.18%) (25483/29568)\n",
      "Epoch: 17 | Batch_idx: 240 |  Loss_1: (0.4004) | Acc_1: (86.22%) (26596/30848)\n",
      "Epoch: 17 | Batch_idx: 250 |  Loss_1: (0.4005) | Acc_1: (86.20%) (27693/32128)\n",
      "Epoch: 17 | Batch_idx: 260 |  Loss_1: (0.3995) | Acc_1: (86.21%) (28800/33408)\n",
      "Epoch: 17 | Batch_idx: 270 |  Loss_1: (0.3983) | Acc_1: (86.24%) (29914/34688)\n",
      "Epoch: 17 | Batch_idx: 280 |  Loss_1: (0.3991) | Acc_1: (86.20%) (31005/35968)\n",
      "Epoch: 17 | Batch_idx: 290 |  Loss_1: (0.4007) | Acc_1: (86.13%) (32082/37248)\n",
      "Epoch: 17 | Batch_idx: 300 |  Loss_1: (0.3999) | Acc_1: (86.17%) (33199/38528)\n",
      "Epoch: 17 | Batch_idx: 310 |  Loss_1: (0.4001) | Acc_1: (86.15%) (34294/39808)\n",
      "Epoch: 17 | Batch_idx: 320 |  Loss_1: (0.4007) | Acc_1: (86.12%) (35387/41088)\n",
      "Epoch: 17 | Batch_idx: 330 |  Loss_1: (0.4011) | Acc_1: (86.11%) (36484/42368)\n",
      "Epoch: 17 | Batch_idx: 340 |  Loss_1: (0.4017) | Acc_1: (86.10%) (37582/43648)\n",
      "Epoch: 17 | Batch_idx: 350 |  Loss_1: (0.4021) | Acc_1: (86.08%) (38676/44928)\n",
      "Epoch: 17 | Batch_idx: 360 |  Loss_1: (0.4017) | Acc_1: (86.09%) (39781/46208)\n",
      "Epoch: 17 | Batch_idx: 370 |  Loss_1: (0.4023) | Acc_1: (86.05%) (40862/47488)\n",
      "Epoch: 17 | Batch_idx: 380 |  Loss_1: (0.4033) | Acc_1: (86.02%) (41952/48768)\n",
      "Epoch: 17 | Batch_idx: 390 |  Loss_1: (0.4035) | Acc_1: (86.01%) (43004/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4244) | Acc: (86.02%) (8602/10000)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.3881) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.3438) | Acc_1: (88.42%) (1245/1408)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.3713) | Acc_1: (87.39%) (2349/2688)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.3654) | Acc_1: (87.37%) (3467/3968)\n",
      "Epoch: 18 | Batch_idx: 40 |  Loss_1: (0.3643) | Acc_1: (87.58%) (4596/5248)\n",
      "Epoch: 18 | Batch_idx: 50 |  Loss_1: (0.3627) | Acc_1: (87.58%) (5717/6528)\n",
      "Epoch: 18 | Batch_idx: 60 |  Loss_1: (0.3635) | Acc_1: (87.45%) (6828/7808)\n",
      "Epoch: 18 | Batch_idx: 70 |  Loss_1: (0.3654) | Acc_1: (87.30%) (7934/9088)\n",
      "Epoch: 18 | Batch_idx: 80 |  Loss_1: (0.3690) | Acc_1: (87.22%) (9043/10368)\n",
      "Epoch: 18 | Batch_idx: 90 |  Loss_1: (0.3733) | Acc_1: (86.90%) (10122/11648)\n",
      "Epoch: 18 | Batch_idx: 100 |  Loss_1: (0.3732) | Acc_1: (87.08%) (11258/12928)\n",
      "Epoch: 18 | Batch_idx: 110 |  Loss_1: (0.3745) | Acc_1: (87.10%) (12375/14208)\n",
      "Epoch: 18 | Batch_idx: 120 |  Loss_1: (0.3729) | Acc_1: (87.07%) (13486/15488)\n",
      "Epoch: 18 | Batch_idx: 130 |  Loss_1: (0.3714) | Acc_1: (87.13%) (14610/16768)\n",
      "Epoch: 18 | Batch_idx: 140 |  Loss_1: (0.3734) | Acc_1: (87.07%) (15715/18048)\n",
      "Epoch: 18 | Batch_idx: 150 |  Loss_1: (0.3751) | Acc_1: (86.99%) (16814/19328)\n",
      "Epoch: 18 | Batch_idx: 160 |  Loss_1: (0.3768) | Acc_1: (86.93%) (17915/20608)\n",
      "Epoch: 18 | Batch_idx: 170 |  Loss_1: (0.3782) | Acc_1: (86.89%) (19018/21888)\n",
      "Epoch: 18 | Batch_idx: 180 |  Loss_1: (0.3788) | Acc_1: (86.87%) (20127/23168)\n",
      "Epoch: 18 | Batch_idx: 190 |  Loss_1: (0.3787) | Acc_1: (86.89%) (21242/24448)\n",
      "Epoch: 18 | Batch_idx: 200 |  Loss_1: (0.3771) | Acc_1: (86.95%) (22370/25728)\n",
      "Epoch: 18 | Batch_idx: 210 |  Loss_1: (0.3775) | Acc_1: (86.92%) (23475/27008)\n",
      "Epoch: 18 | Batch_idx: 220 |  Loss_1: (0.3795) | Acc_1: (86.84%) (24566/28288)\n",
      "Epoch: 18 | Batch_idx: 230 |  Loss_1: (0.3804) | Acc_1: (86.80%) (25666/29568)\n",
      "Epoch: 18 | Batch_idx: 240 |  Loss_1: (0.3810) | Acc_1: (86.79%) (26774/30848)\n",
      "Epoch: 18 | Batch_idx: 250 |  Loss_1: (0.3794) | Acc_1: (86.87%) (27908/32128)\n",
      "Epoch: 18 | Batch_idx: 260 |  Loss_1: (0.3797) | Acc_1: (86.89%) (29028/33408)\n",
      "Epoch: 18 | Batch_idx: 270 |  Loss_1: (0.3810) | Acc_1: (86.83%) (30118/34688)\n",
      "Epoch: 18 | Batch_idx: 280 |  Loss_1: (0.3829) | Acc_1: (86.75%) (31204/35968)\n",
      "Epoch: 18 | Batch_idx: 290 |  Loss_1: (0.3828) | Acc_1: (86.78%) (32323/37248)\n",
      "Epoch: 18 | Batch_idx: 300 |  Loss_1: (0.3824) | Acc_1: (86.77%) (33431/38528)\n",
      "Epoch: 18 | Batch_idx: 310 |  Loss_1: (0.3834) | Acc_1: (86.72%) (34521/39808)\n",
      "Epoch: 18 | Batch_idx: 320 |  Loss_1: (0.3848) | Acc_1: (86.67%) (35610/41088)\n",
      "Epoch: 18 | Batch_idx: 330 |  Loss_1: (0.3849) | Acc_1: (86.65%) (36711/42368)\n",
      "Epoch: 18 | Batch_idx: 340 |  Loss_1: (0.3850) | Acc_1: (86.63%) (37814/43648)\n",
      "Epoch: 18 | Batch_idx: 350 |  Loss_1: (0.3859) | Acc_1: (86.63%) (38921/44928)\n",
      "Epoch: 18 | Batch_idx: 360 |  Loss_1: (0.3848) | Acc_1: (86.65%) (40039/46208)\n",
      "Epoch: 18 | Batch_idx: 370 |  Loss_1: (0.3848) | Acc_1: (86.65%) (41149/47488)\n",
      "Epoch: 18 | Batch_idx: 380 |  Loss_1: (0.3849) | Acc_1: (86.65%) (42259/48768)\n",
      "Epoch: 18 | Batch_idx: 390 |  Loss_1: (0.3845) | Acc_1: (86.65%) (43326/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4604) | Acc: (84.75%) (8475/10000)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.2857) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.3473) | Acc_1: (87.43%) (1231/1408)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.3470) | Acc_1: (87.46%) (2351/2688)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.3446) | Acc_1: (87.98%) (3491/3968)\n",
      "Epoch: 19 | Batch_idx: 40 |  Loss_1: (0.3518) | Acc_1: (87.56%) (4595/5248)\n",
      "Epoch: 19 | Batch_idx: 50 |  Loss_1: (0.3565) | Acc_1: (87.52%) (5713/6528)\n",
      "Epoch: 19 | Batch_idx: 60 |  Loss_1: (0.3575) | Acc_1: (87.47%) (6830/7808)\n",
      "Epoch: 19 | Batch_idx: 70 |  Loss_1: (0.3588) | Acc_1: (87.37%) (7940/9088)\n",
      "Epoch: 19 | Batch_idx: 80 |  Loss_1: (0.3615) | Acc_1: (87.23%) (9044/10368)\n",
      "Epoch: 19 | Batch_idx: 90 |  Loss_1: (0.3647) | Acc_1: (87.04%) (10139/11648)\n",
      "Epoch: 19 | Batch_idx: 100 |  Loss_1: (0.3682) | Acc_1: (86.95%) (11241/12928)\n",
      "Epoch: 19 | Batch_idx: 110 |  Loss_1: (0.3693) | Acc_1: (86.94%) (12353/14208)\n",
      "Epoch: 19 | Batch_idx: 120 |  Loss_1: (0.3724) | Acc_1: (86.73%) (13432/15488)\n",
      "Epoch: 19 | Batch_idx: 130 |  Loss_1: (0.3723) | Acc_1: (86.76%) (14548/16768)\n",
      "Epoch: 19 | Batch_idx: 140 |  Loss_1: (0.3688) | Acc_1: (86.94%) (15691/18048)\n",
      "Epoch: 19 | Batch_idx: 150 |  Loss_1: (0.3705) | Acc_1: (86.91%) (16797/19328)\n",
      "Epoch: 19 | Batch_idx: 160 |  Loss_1: (0.3728) | Acc_1: (86.82%) (17891/20608)\n",
      "Epoch: 19 | Batch_idx: 170 |  Loss_1: (0.3738) | Acc_1: (86.86%) (19013/21888)\n",
      "Epoch: 19 | Batch_idx: 180 |  Loss_1: (0.3731) | Acc_1: (86.87%) (20127/23168)\n",
      "Epoch: 19 | Batch_idx: 190 |  Loss_1: (0.3767) | Acc_1: (86.86%) (21236/24448)\n",
      "Epoch: 19 | Batch_idx: 200 |  Loss_1: (0.3754) | Acc_1: (86.86%) (22348/25728)\n",
      "Epoch: 19 | Batch_idx: 210 |  Loss_1: (0.3757) | Acc_1: (86.84%) (23453/27008)\n",
      "Epoch: 19 | Batch_idx: 220 |  Loss_1: (0.3753) | Acc_1: (86.81%) (24556/28288)\n",
      "Epoch: 19 | Batch_idx: 230 |  Loss_1: (0.3739) | Acc_1: (86.82%) (25671/29568)\n",
      "Epoch: 19 | Batch_idx: 240 |  Loss_1: (0.3743) | Acc_1: (86.79%) (26774/30848)\n",
      "Epoch: 19 | Batch_idx: 250 |  Loss_1: (0.3753) | Acc_1: (86.75%) (27871/32128)\n",
      "Epoch: 19 | Batch_idx: 260 |  Loss_1: (0.3762) | Acc_1: (86.75%) (28981/33408)\n",
      "Epoch: 19 | Batch_idx: 270 |  Loss_1: (0.3765) | Acc_1: (86.73%) (30086/34688)\n",
      "Epoch: 19 | Batch_idx: 280 |  Loss_1: (0.3769) | Acc_1: (86.75%) (31203/35968)\n",
      "Epoch: 19 | Batch_idx: 290 |  Loss_1: (0.3768) | Acc_1: (86.73%) (32306/37248)\n",
      "Epoch: 19 | Batch_idx: 300 |  Loss_1: (0.3782) | Acc_1: (86.68%) (33395/38528)\n",
      "Epoch: 19 | Batch_idx: 310 |  Loss_1: (0.3772) | Acc_1: (86.70%) (34513/39808)\n",
      "Epoch: 19 | Batch_idx: 320 |  Loss_1: (0.3772) | Acc_1: (86.68%) (35615/41088)\n",
      "Epoch: 19 | Batch_idx: 330 |  Loss_1: (0.3755) | Acc_1: (86.74%) (36751/42368)\n",
      "Epoch: 19 | Batch_idx: 340 |  Loss_1: (0.3760) | Acc_1: (86.75%) (37863/43648)\n",
      "Epoch: 19 | Batch_idx: 350 |  Loss_1: (0.3754) | Acc_1: (86.78%) (38987/44928)\n",
      "Epoch: 19 | Batch_idx: 360 |  Loss_1: (0.3752) | Acc_1: (86.79%) (40105/46208)\n",
      "Epoch: 19 | Batch_idx: 370 |  Loss_1: (0.3743) | Acc_1: (86.83%) (41234/47488)\n",
      "Epoch: 19 | Batch_idx: 380 |  Loss_1: (0.3749) | Acc_1: (86.83%) (42343/48768)\n",
      "Epoch: 19 | Batch_idx: 390 |  Loss_1: (0.3745) | Acc_1: (86.85%) (43427/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4702) | Acc: (85.04%) (8504/10000)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.4727) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.3760) | Acc_1: (86.01%) (1211/1408)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.3589) | Acc_1: (87.17%) (2343/2688)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.3669) | Acc_1: (87.05%) (3454/3968)\n",
      "Epoch: 20 | Batch_idx: 40 |  Loss_1: (0.3566) | Acc_1: (87.23%) (4578/5248)\n",
      "Epoch: 20 | Batch_idx: 50 |  Loss_1: (0.3515) | Acc_1: (87.42%) (5707/6528)\n",
      "Epoch: 20 | Batch_idx: 60 |  Loss_1: (0.3492) | Acc_1: (87.56%) (6837/7808)\n",
      "Epoch: 20 | Batch_idx: 70 |  Loss_1: (0.3496) | Acc_1: (87.58%) (7959/9088)\n",
      "Epoch: 20 | Batch_idx: 80 |  Loss_1: (0.3552) | Acc_1: (87.37%) (9059/10368)\n",
      "Epoch: 20 | Batch_idx: 90 |  Loss_1: (0.3545) | Acc_1: (87.38%) (10178/11648)\n",
      "Epoch: 20 | Batch_idx: 100 |  Loss_1: (0.3510) | Acc_1: (87.58%) (11322/12928)\n",
      "Epoch: 20 | Batch_idx: 110 |  Loss_1: (0.3539) | Acc_1: (87.49%) (12430/14208)\n",
      "Epoch: 20 | Batch_idx: 120 |  Loss_1: (0.3557) | Acc_1: (87.44%) (13542/15488)\n",
      "Epoch: 20 | Batch_idx: 130 |  Loss_1: (0.3585) | Acc_1: (87.36%) (14649/16768)\n",
      "Epoch: 20 | Batch_idx: 140 |  Loss_1: (0.3577) | Acc_1: (87.40%) (15774/18048)\n",
      "Epoch: 20 | Batch_idx: 150 |  Loss_1: (0.3574) | Acc_1: (87.39%) (16891/19328)\n",
      "Epoch: 20 | Batch_idx: 160 |  Loss_1: (0.3567) | Acc_1: (87.41%) (18014/20608)\n",
      "Epoch: 20 | Batch_idx: 170 |  Loss_1: (0.3566) | Acc_1: (87.41%) (19132/21888)\n",
      "Epoch: 20 | Batch_idx: 180 |  Loss_1: (0.3566) | Acc_1: (87.44%) (20258/23168)\n",
      "Epoch: 20 | Batch_idx: 190 |  Loss_1: (0.3564) | Acc_1: (87.42%) (21373/24448)\n",
      "Epoch: 20 | Batch_idx: 200 |  Loss_1: (0.3567) | Acc_1: (87.45%) (22498/25728)\n",
      "Epoch: 20 | Batch_idx: 210 |  Loss_1: (0.3564) | Acc_1: (87.46%) (23622/27008)\n",
      "Epoch: 20 | Batch_idx: 220 |  Loss_1: (0.3562) | Acc_1: (87.47%) (24744/28288)\n",
      "Epoch: 20 | Batch_idx: 230 |  Loss_1: (0.3569) | Acc_1: (87.46%) (25859/29568)\n",
      "Epoch: 20 | Batch_idx: 240 |  Loss_1: (0.3561) | Acc_1: (87.48%) (26985/30848)\n",
      "Epoch: 20 | Batch_idx: 250 |  Loss_1: (0.3561) | Acc_1: (87.50%) (28111/32128)\n",
      "Epoch: 20 | Batch_idx: 260 |  Loss_1: (0.3575) | Acc_1: (87.44%) (29213/33408)\n",
      "Epoch: 20 | Batch_idx: 270 |  Loss_1: (0.3572) | Acc_1: (87.43%) (30329/34688)\n",
      "Epoch: 20 | Batch_idx: 280 |  Loss_1: (0.3580) | Acc_1: (87.40%) (31437/35968)\n",
      "Epoch: 20 | Batch_idx: 290 |  Loss_1: (0.3587) | Acc_1: (87.40%) (32553/37248)\n",
      "Epoch: 20 | Batch_idx: 300 |  Loss_1: (0.3577) | Acc_1: (87.47%) (33699/38528)\n",
      "Epoch: 20 | Batch_idx: 310 |  Loss_1: (0.3578) | Acc_1: (87.47%) (34820/39808)\n",
      "Epoch: 20 | Batch_idx: 320 |  Loss_1: (0.3582) | Acc_1: (87.47%) (35941/41088)\n",
      "Epoch: 20 | Batch_idx: 330 |  Loss_1: (0.3581) | Acc_1: (87.50%) (37071/42368)\n",
      "Epoch: 20 | Batch_idx: 340 |  Loss_1: (0.3577) | Acc_1: (87.52%) (38199/43648)\n",
      "Epoch: 20 | Batch_idx: 350 |  Loss_1: (0.3573) | Acc_1: (87.52%) (39320/44928)\n",
      "Epoch: 20 | Batch_idx: 360 |  Loss_1: (0.3580) | Acc_1: (87.48%) (40423/46208)\n",
      "Epoch: 20 | Batch_idx: 370 |  Loss_1: (0.3572) | Acc_1: (87.53%) (41564/47488)\n",
      "Epoch: 20 | Batch_idx: 380 |  Loss_1: (0.3564) | Acc_1: (87.53%) (42688/48768)\n",
      "Epoch: 20 | Batch_idx: 390 |  Loss_1: (0.3556) | Acc_1: (87.57%) (43783/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4380) | Acc: (85.70%) (8570/10000)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.3664) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.3548) | Acc_1: (88.14%) (1241/1408)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.3358) | Acc_1: (88.06%) (2367/2688)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.3578) | Acc_1: (87.27%) (3463/3968)\n",
      "Epoch: 21 | Batch_idx: 40 |  Loss_1: (0.3505) | Acc_1: (87.60%) (4597/5248)\n",
      "Epoch: 21 | Batch_idx: 50 |  Loss_1: (0.3425) | Acc_1: (87.91%) (5739/6528)\n",
      "Epoch: 21 | Batch_idx: 60 |  Loss_1: (0.3440) | Acc_1: (88.04%) (6874/7808)\n",
      "Epoch: 21 | Batch_idx: 70 |  Loss_1: (0.3462) | Acc_1: (87.84%) (7983/9088)\n",
      "Epoch: 21 | Batch_idx: 80 |  Loss_1: (0.3427) | Acc_1: (87.92%) (9116/10368)\n",
      "Epoch: 21 | Batch_idx: 90 |  Loss_1: (0.3434) | Acc_1: (87.86%) (10234/11648)\n",
      "Epoch: 21 | Batch_idx: 100 |  Loss_1: (0.3407) | Acc_1: (87.93%) (11367/12928)\n",
      "Epoch: 21 | Batch_idx: 110 |  Loss_1: (0.3391) | Acc_1: (88.00%) (12503/14208)\n",
      "Epoch: 21 | Batch_idx: 120 |  Loss_1: (0.3405) | Acc_1: (87.98%) (13627/15488)\n",
      "Epoch: 21 | Batch_idx: 130 |  Loss_1: (0.3377) | Acc_1: (88.04%) (14763/16768)\n",
      "Epoch: 21 | Batch_idx: 140 |  Loss_1: (0.3386) | Acc_1: (88.02%) (15885/18048)\n",
      "Epoch: 21 | Batch_idx: 150 |  Loss_1: (0.3384) | Acc_1: (88.07%) (17022/19328)\n",
      "Epoch: 21 | Batch_idx: 160 |  Loss_1: (0.3380) | Acc_1: (88.09%) (18154/20608)\n",
      "Epoch: 21 | Batch_idx: 170 |  Loss_1: (0.3392) | Acc_1: (88.00%) (19262/21888)\n",
      "Epoch: 21 | Batch_idx: 180 |  Loss_1: (0.3380) | Acc_1: (87.99%) (20385/23168)\n",
      "Epoch: 21 | Batch_idx: 190 |  Loss_1: (0.3397) | Acc_1: (87.92%) (21494/24448)\n",
      "Epoch: 21 | Batch_idx: 200 |  Loss_1: (0.3380) | Acc_1: (88.01%) (22644/25728)\n",
      "Epoch: 21 | Batch_idx: 210 |  Loss_1: (0.3377) | Acc_1: (88.06%) (23784/27008)\n",
      "Epoch: 21 | Batch_idx: 220 |  Loss_1: (0.3393) | Acc_1: (88.04%) (24904/28288)\n",
      "Epoch: 21 | Batch_idx: 230 |  Loss_1: (0.3419) | Acc_1: (87.94%) (26001/29568)\n",
      "Epoch: 21 | Batch_idx: 240 |  Loss_1: (0.3430) | Acc_1: (87.89%) (27112/30848)\n",
      "Epoch: 21 | Batch_idx: 250 |  Loss_1: (0.3442) | Acc_1: (87.86%) (28228/32128)\n",
      "Epoch: 21 | Batch_idx: 260 |  Loss_1: (0.3454) | Acc_1: (87.84%) (29347/33408)\n",
      "Epoch: 21 | Batch_idx: 270 |  Loss_1: (0.3452) | Acc_1: (87.86%) (30478/34688)\n",
      "Epoch: 21 | Batch_idx: 280 |  Loss_1: (0.3450) | Acc_1: (87.87%) (31606/35968)\n",
      "Epoch: 21 | Batch_idx: 290 |  Loss_1: (0.3457) | Acc_1: (87.85%) (32721/37248)\n",
      "Epoch: 21 | Batch_idx: 300 |  Loss_1: (0.3449) | Acc_1: (87.88%) (33859/38528)\n",
      "Epoch: 21 | Batch_idx: 310 |  Loss_1: (0.3442) | Acc_1: (87.91%) (34995/39808)\n",
      "Epoch: 21 | Batch_idx: 320 |  Loss_1: (0.3436) | Acc_1: (87.93%) (36130/41088)\n",
      "Epoch: 21 | Batch_idx: 330 |  Loss_1: (0.3446) | Acc_1: (87.94%) (37257/42368)\n",
      "Epoch: 21 | Batch_idx: 340 |  Loss_1: (0.3444) | Acc_1: (87.93%) (38380/43648)\n",
      "Epoch: 21 | Batch_idx: 350 |  Loss_1: (0.3456) | Acc_1: (87.86%) (39472/44928)\n",
      "Epoch: 21 | Batch_idx: 360 |  Loss_1: (0.3451) | Acc_1: (87.87%) (40604/46208)\n",
      "Epoch: 21 | Batch_idx: 370 |  Loss_1: (0.3447) | Acc_1: (87.90%) (41743/47488)\n",
      "Epoch: 21 | Batch_idx: 380 |  Loss_1: (0.3463) | Acc_1: (87.86%) (42848/48768)\n",
      "Epoch: 21 | Batch_idx: 390 |  Loss_1: (0.3461) | Acc_1: (87.85%) (43926/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4096) | Acc: (86.71%) (8671/10000)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.3861) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.3089) | Acc_1: (89.42%) (1259/1408)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.3244) | Acc_1: (88.43%) (2377/2688)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.3269) | Acc_1: (88.18%) (3499/3968)\n",
      "Epoch: 22 | Batch_idx: 40 |  Loss_1: (0.3189) | Acc_1: (88.45%) (4642/5248)\n",
      "Epoch: 22 | Batch_idx: 50 |  Loss_1: (0.3150) | Acc_1: (88.62%) (5785/6528)\n",
      "Epoch: 22 | Batch_idx: 60 |  Loss_1: (0.3148) | Acc_1: (88.63%) (6920/7808)\n",
      "Epoch: 22 | Batch_idx: 70 |  Loss_1: (0.3159) | Acc_1: (88.71%) (8062/9088)\n",
      "Epoch: 22 | Batch_idx: 80 |  Loss_1: (0.3165) | Acc_1: (88.84%) (9211/10368)\n",
      "Epoch: 22 | Batch_idx: 90 |  Loss_1: (0.3216) | Acc_1: (88.73%) (10335/11648)\n",
      "Epoch: 22 | Batch_idx: 100 |  Loss_1: (0.3185) | Acc_1: (88.85%) (11487/12928)\n",
      "Epoch: 22 | Batch_idx: 110 |  Loss_1: (0.3186) | Acc_1: (88.88%) (12628/14208)\n",
      "Epoch: 22 | Batch_idx: 120 |  Loss_1: (0.3189) | Acc_1: (88.84%) (13760/15488)\n",
      "Epoch: 22 | Batch_idx: 130 |  Loss_1: (0.3213) | Acc_1: (88.72%) (14877/16768)\n",
      "Epoch: 22 | Batch_idx: 140 |  Loss_1: (0.3218) | Acc_1: (88.74%) (16016/18048)\n",
      "Epoch: 22 | Batch_idx: 150 |  Loss_1: (0.3223) | Acc_1: (88.67%) (17139/19328)\n",
      "Epoch: 22 | Batch_idx: 160 |  Loss_1: (0.3228) | Acc_1: (88.65%) (18270/20608)\n",
      "Epoch: 22 | Batch_idx: 170 |  Loss_1: (0.3228) | Acc_1: (88.65%) (19403/21888)\n",
      "Epoch: 22 | Batch_idx: 180 |  Loss_1: (0.3225) | Acc_1: (88.62%) (20531/23168)\n",
      "Epoch: 22 | Batch_idx: 190 |  Loss_1: (0.3213) | Acc_1: (88.67%) (21677/24448)\n",
      "Epoch: 22 | Batch_idx: 200 |  Loss_1: (0.3225) | Acc_1: (88.62%) (22801/25728)\n",
      "Epoch: 22 | Batch_idx: 210 |  Loss_1: (0.3253) | Acc_1: (88.56%) (23917/27008)\n",
      "Epoch: 22 | Batch_idx: 220 |  Loss_1: (0.3284) | Acc_1: (88.48%) (25030/28288)\n",
      "Epoch: 22 | Batch_idx: 230 |  Loss_1: (0.3287) | Acc_1: (88.49%) (26164/29568)\n",
      "Epoch: 22 | Batch_idx: 240 |  Loss_1: (0.3302) | Acc_1: (88.42%) (27275/30848)\n",
      "Epoch: 22 | Batch_idx: 250 |  Loss_1: (0.3295) | Acc_1: (88.42%) (28409/32128)\n",
      "Epoch: 22 | Batch_idx: 260 |  Loss_1: (0.3293) | Acc_1: (88.42%) (29540/33408)\n",
      "Epoch: 22 | Batch_idx: 270 |  Loss_1: (0.3295) | Acc_1: (88.43%) (30676/34688)\n",
      "Epoch: 22 | Batch_idx: 280 |  Loss_1: (0.3287) | Acc_1: (88.45%) (31815/35968)\n",
      "Epoch: 22 | Batch_idx: 290 |  Loss_1: (0.3307) | Acc_1: (88.39%) (32924/37248)\n",
      "Epoch: 22 | Batch_idx: 300 |  Loss_1: (0.3309) | Acc_1: (88.41%) (34064/38528)\n",
      "Epoch: 22 | Batch_idx: 310 |  Loss_1: (0.3308) | Acc_1: (88.41%) (35195/39808)\n",
      "Epoch: 22 | Batch_idx: 320 |  Loss_1: (0.3313) | Acc_1: (88.40%) (36320/41088)\n",
      "Epoch: 22 | Batch_idx: 330 |  Loss_1: (0.3319) | Acc_1: (88.39%) (37449/42368)\n",
      "Epoch: 22 | Batch_idx: 340 |  Loss_1: (0.3316) | Acc_1: (88.43%) (38599/43648)\n",
      "Epoch: 22 | Batch_idx: 350 |  Loss_1: (0.3316) | Acc_1: (88.44%) (39734/44928)\n",
      "Epoch: 22 | Batch_idx: 360 |  Loss_1: (0.3317) | Acc_1: (88.44%) (40865/46208)\n",
      "Epoch: 22 | Batch_idx: 370 |  Loss_1: (0.3327) | Acc_1: (88.42%) (41988/47488)\n",
      "Epoch: 22 | Batch_idx: 380 |  Loss_1: (0.3320) | Acc_1: (88.45%) (43137/48768)\n",
      "Epoch: 22 | Batch_idx: 390 |  Loss_1: (0.3328) | Acc_1: (88.42%) (44211/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4158) | Acc: (86.59%) (8659/10000)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.2197) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.3295) | Acc_1: (88.28%) (1243/1408)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.3259) | Acc_1: (88.17%) (2370/2688)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.3181) | Acc_1: (88.48%) (3511/3968)\n",
      "Epoch: 23 | Batch_idx: 40 |  Loss_1: (0.3159) | Acc_1: (88.68%) (4654/5248)\n",
      "Epoch: 23 | Batch_idx: 50 |  Loss_1: (0.3227) | Acc_1: (88.62%) (5785/6528)\n",
      "Epoch: 23 | Batch_idx: 60 |  Loss_1: (0.3263) | Acc_1: (88.50%) (6910/7808)\n",
      "Epoch: 23 | Batch_idx: 70 |  Loss_1: (0.3214) | Acc_1: (88.67%) (8058/9088)\n",
      "Epoch: 23 | Batch_idx: 80 |  Loss_1: (0.3180) | Acc_1: (88.71%) (9197/10368)\n",
      "Epoch: 23 | Batch_idx: 90 |  Loss_1: (0.3163) | Acc_1: (88.89%) (10354/11648)\n",
      "Epoch: 23 | Batch_idx: 100 |  Loss_1: (0.3125) | Acc_1: (89.01%) (11507/12928)\n",
      "Epoch: 23 | Batch_idx: 110 |  Loss_1: (0.3128) | Acc_1: (88.99%) (12644/14208)\n",
      "Epoch: 23 | Batch_idx: 120 |  Loss_1: (0.3135) | Acc_1: (88.97%) (13780/15488)\n",
      "Epoch: 23 | Batch_idx: 130 |  Loss_1: (0.3159) | Acc_1: (88.88%) (14904/16768)\n",
      "Epoch: 23 | Batch_idx: 140 |  Loss_1: (0.3153) | Acc_1: (88.94%) (16052/18048)\n",
      "Epoch: 23 | Batch_idx: 150 |  Loss_1: (0.3169) | Acc_1: (88.90%) (17182/19328)\n",
      "Epoch: 23 | Batch_idx: 160 |  Loss_1: (0.3218) | Acc_1: (88.73%) (18285/20608)\n",
      "Epoch: 23 | Batch_idx: 170 |  Loss_1: (0.3211) | Acc_1: (88.79%) (19435/21888)\n",
      "Epoch: 23 | Batch_idx: 180 |  Loss_1: (0.3203) | Acc_1: (88.84%) (20583/23168)\n",
      "Epoch: 23 | Batch_idx: 190 |  Loss_1: (0.3202) | Acc_1: (88.84%) (21720/24448)\n",
      "Epoch: 23 | Batch_idx: 200 |  Loss_1: (0.3195) | Acc_1: (88.83%) (22853/25728)\n",
      "Epoch: 23 | Batch_idx: 210 |  Loss_1: (0.3194) | Acc_1: (88.84%) (23994/27008)\n",
      "Epoch: 23 | Batch_idx: 220 |  Loss_1: (0.3187) | Acc_1: (88.86%) (25138/28288)\n",
      "Epoch: 23 | Batch_idx: 230 |  Loss_1: (0.3183) | Acc_1: (88.86%) (26273/29568)\n",
      "Epoch: 23 | Batch_idx: 240 |  Loss_1: (0.3169) | Acc_1: (88.93%) (27434/30848)\n",
      "Epoch: 23 | Batch_idx: 250 |  Loss_1: (0.3167) | Acc_1: (88.92%) (28567/32128)\n",
      "Epoch: 23 | Batch_idx: 260 |  Loss_1: (0.3187) | Acc_1: (88.82%) (29673/33408)\n",
      "Epoch: 23 | Batch_idx: 270 |  Loss_1: (0.3191) | Acc_1: (88.78%) (30796/34688)\n",
      "Epoch: 23 | Batch_idx: 280 |  Loss_1: (0.3192) | Acc_1: (88.78%) (31933/35968)\n",
      "Epoch: 23 | Batch_idx: 290 |  Loss_1: (0.3187) | Acc_1: (88.79%) (33071/37248)\n",
      "Epoch: 23 | Batch_idx: 300 |  Loss_1: (0.3194) | Acc_1: (88.81%) (34216/38528)\n",
      "Epoch: 23 | Batch_idx: 310 |  Loss_1: (0.3185) | Acc_1: (88.85%) (35368/39808)\n",
      "Epoch: 23 | Batch_idx: 320 |  Loss_1: (0.3178) | Acc_1: (88.88%) (36517/41088)\n",
      "Epoch: 23 | Batch_idx: 330 |  Loss_1: (0.3179) | Acc_1: (88.88%) (37657/42368)\n",
      "Epoch: 23 | Batch_idx: 340 |  Loss_1: (0.3182) | Acc_1: (88.87%) (38790/43648)\n",
      "Epoch: 23 | Batch_idx: 350 |  Loss_1: (0.3174) | Acc_1: (88.91%) (39945/44928)\n",
      "Epoch: 23 | Batch_idx: 360 |  Loss_1: (0.3183) | Acc_1: (88.88%) (41070/46208)\n",
      "Epoch: 23 | Batch_idx: 370 |  Loss_1: (0.3183) | Acc_1: (88.87%) (42204/47488)\n",
      "Epoch: 23 | Batch_idx: 380 |  Loss_1: (0.3198) | Acc_1: (88.83%) (43319/48768)\n",
      "Epoch: 23 | Batch_idx: 390 |  Loss_1: (0.3200) | Acc_1: (88.81%) (44405/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3980) | Acc: (87.33%) (8733/10000)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.2041) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.2797) | Acc_1: (89.42%) (1259/1408)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.2989) | Acc_1: (88.99%) (2392/2688)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.2951) | Acc_1: (89.04%) (3533/3968)\n",
      "Epoch: 24 | Batch_idx: 40 |  Loss_1: (0.2924) | Acc_1: (89.44%) (4694/5248)\n",
      "Epoch: 24 | Batch_idx: 50 |  Loss_1: (0.2946) | Acc_1: (89.43%) (5838/6528)\n",
      "Epoch: 24 | Batch_idx: 60 |  Loss_1: (0.2936) | Acc_1: (89.38%) (6979/7808)\n",
      "Epoch: 24 | Batch_idx: 70 |  Loss_1: (0.2940) | Acc_1: (89.47%) (8131/9088)\n",
      "Epoch: 24 | Batch_idx: 80 |  Loss_1: (0.2900) | Acc_1: (89.63%) (9293/10368)\n",
      "Epoch: 24 | Batch_idx: 90 |  Loss_1: (0.2928) | Acc_1: (89.49%) (10424/11648)\n",
      "Epoch: 24 | Batch_idx: 100 |  Loss_1: (0.2934) | Acc_1: (89.46%) (11565/12928)\n",
      "Epoch: 24 | Batch_idx: 110 |  Loss_1: (0.2933) | Acc_1: (89.49%) (12715/14208)\n",
      "Epoch: 24 | Batch_idx: 120 |  Loss_1: (0.2933) | Acc_1: (89.51%) (13863/15488)\n",
      "Epoch: 24 | Batch_idx: 130 |  Loss_1: (0.2930) | Acc_1: (89.48%) (15004/16768)\n",
      "Epoch: 24 | Batch_idx: 140 |  Loss_1: (0.2948) | Acc_1: (89.43%) (16141/18048)\n",
      "Epoch: 24 | Batch_idx: 150 |  Loss_1: (0.2938) | Acc_1: (89.54%) (17307/19328)\n",
      "Epoch: 24 | Batch_idx: 160 |  Loss_1: (0.2963) | Acc_1: (89.52%) (18449/20608)\n",
      "Epoch: 24 | Batch_idx: 170 |  Loss_1: (0.2962) | Acc_1: (89.55%) (19601/21888)\n",
      "Epoch: 24 | Batch_idx: 180 |  Loss_1: (0.2972) | Acc_1: (89.53%) (20743/23168)\n",
      "Epoch: 24 | Batch_idx: 190 |  Loss_1: (0.2969) | Acc_1: (89.52%) (21885/24448)\n",
      "Epoch: 24 | Batch_idx: 200 |  Loss_1: (0.2982) | Acc_1: (89.46%) (23015/25728)\n",
      "Epoch: 24 | Batch_idx: 210 |  Loss_1: (0.2991) | Acc_1: (89.44%) (24157/27008)\n",
      "Epoch: 24 | Batch_idx: 220 |  Loss_1: (0.3017) | Acc_1: (89.33%) (25270/28288)\n",
      "Epoch: 24 | Batch_idx: 230 |  Loss_1: (0.3016) | Acc_1: (89.30%) (26403/29568)\n",
      "Epoch: 24 | Batch_idx: 240 |  Loss_1: (0.3016) | Acc_1: (89.30%) (27548/30848)\n",
      "Epoch: 24 | Batch_idx: 250 |  Loss_1: (0.3025) | Acc_1: (89.26%) (28676/32128)\n",
      "Epoch: 24 | Batch_idx: 260 |  Loss_1: (0.3041) | Acc_1: (89.22%) (29806/33408)\n",
      "Epoch: 24 | Batch_idx: 270 |  Loss_1: (0.3048) | Acc_1: (89.19%) (30937/34688)\n",
      "Epoch: 24 | Batch_idx: 280 |  Loss_1: (0.3056) | Acc_1: (89.17%) (32072/35968)\n",
      "Epoch: 24 | Batch_idx: 290 |  Loss_1: (0.3063) | Acc_1: (89.17%) (33214/37248)\n",
      "Epoch: 24 | Batch_idx: 300 |  Loss_1: (0.3073) | Acc_1: (89.15%) (34346/38528)\n",
      "Epoch: 24 | Batch_idx: 310 |  Loss_1: (0.3071) | Acc_1: (89.16%) (35492/39808)\n",
      "Epoch: 24 | Batch_idx: 320 |  Loss_1: (0.3071) | Acc_1: (89.17%) (36639/41088)\n",
      "Epoch: 24 | Batch_idx: 330 |  Loss_1: (0.3071) | Acc_1: (89.18%) (37782/42368)\n",
      "Epoch: 24 | Batch_idx: 340 |  Loss_1: (0.3065) | Acc_1: (89.20%) (38934/43648)\n",
      "Epoch: 24 | Batch_idx: 350 |  Loss_1: (0.3062) | Acc_1: (89.20%) (40074/44928)\n",
      "Epoch: 24 | Batch_idx: 360 |  Loss_1: (0.3067) | Acc_1: (89.17%) (41203/46208)\n",
      "Epoch: 24 | Batch_idx: 370 |  Loss_1: (0.3070) | Acc_1: (89.19%) (42355/47488)\n",
      "Epoch: 24 | Batch_idx: 380 |  Loss_1: (0.3075) | Acc_1: (89.19%) (43495/48768)\n",
      "Epoch: 24 | Batch_idx: 390 |  Loss_1: (0.3085) | Acc_1: (89.15%) (44575/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3959) | Acc: (87.21%) (8721/10000)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.4655) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.2930) | Acc_1: (89.42%) (1259/1408)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.2959) | Acc_1: (89.25%) (2399/2688)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.3062) | Acc_1: (89.11%) (3536/3968)\n",
      "Epoch: 25 | Batch_idx: 40 |  Loss_1: (0.2963) | Acc_1: (89.46%) (4695/5248)\n",
      "Epoch: 25 | Batch_idx: 50 |  Loss_1: (0.2926) | Acc_1: (89.46%) (5840/6528)\n",
      "Epoch: 25 | Batch_idx: 60 |  Loss_1: (0.2913) | Acc_1: (89.65%) (7000/7808)\n",
      "Epoch: 25 | Batch_idx: 70 |  Loss_1: (0.2930) | Acc_1: (89.70%) (8152/9088)\n",
      "Epoch: 25 | Batch_idx: 80 |  Loss_1: (0.2922) | Acc_1: (89.76%) (9306/10368)\n",
      "Epoch: 25 | Batch_idx: 90 |  Loss_1: (0.2899) | Acc_1: (89.81%) (10461/11648)\n",
      "Epoch: 25 | Batch_idx: 100 |  Loss_1: (0.2904) | Acc_1: (89.78%) (11607/12928)\n",
      "Epoch: 25 | Batch_idx: 110 |  Loss_1: (0.2915) | Acc_1: (89.71%) (12746/14208)\n",
      "Epoch: 25 | Batch_idx: 120 |  Loss_1: (0.2955) | Acc_1: (89.60%) (13878/15488)\n",
      "Epoch: 25 | Batch_idx: 130 |  Loss_1: (0.2978) | Acc_1: (89.52%) (15010/16768)\n",
      "Epoch: 25 | Batch_idx: 140 |  Loss_1: (0.3000) | Acc_1: (89.44%) (16142/18048)\n",
      "Epoch: 25 | Batch_idx: 150 |  Loss_1: (0.2998) | Acc_1: (89.49%) (17296/19328)\n",
      "Epoch: 25 | Batch_idx: 160 |  Loss_1: (0.2993) | Acc_1: (89.44%) (18432/20608)\n",
      "Epoch: 25 | Batch_idx: 170 |  Loss_1: (0.2977) | Acc_1: (89.60%) (19612/21888)\n",
      "Epoch: 25 | Batch_idx: 180 |  Loss_1: (0.2991) | Acc_1: (89.55%) (20747/23168)\n",
      "Epoch: 25 | Batch_idx: 190 |  Loss_1: (0.2993) | Acc_1: (89.51%) (21883/24448)\n",
      "Epoch: 25 | Batch_idx: 200 |  Loss_1: (0.2978) | Acc_1: (89.55%) (23040/25728)\n",
      "Epoch: 25 | Batch_idx: 210 |  Loss_1: (0.2979) | Acc_1: (89.55%) (24187/27008)\n",
      "Epoch: 25 | Batch_idx: 220 |  Loss_1: (0.2997) | Acc_1: (89.50%) (25317/28288)\n",
      "Epoch: 25 | Batch_idx: 230 |  Loss_1: (0.2999) | Acc_1: (89.50%) (26463/29568)\n",
      "Epoch: 25 | Batch_idx: 240 |  Loss_1: (0.2995) | Acc_1: (89.49%) (27606/30848)\n",
      "Epoch: 25 | Batch_idx: 250 |  Loss_1: (0.3006) | Acc_1: (89.41%) (28726/32128)\n",
      "Epoch: 25 | Batch_idx: 260 |  Loss_1: (0.3004) | Acc_1: (89.40%) (29868/33408)\n",
      "Epoch: 25 | Batch_idx: 270 |  Loss_1: (0.3012) | Acc_1: (89.39%) (31007/34688)\n",
      "Epoch: 25 | Batch_idx: 280 |  Loss_1: (0.3005) | Acc_1: (89.40%) (32154/35968)\n",
      "Epoch: 25 | Batch_idx: 290 |  Loss_1: (0.3006) | Acc_1: (89.37%) (33288/37248)\n",
      "Epoch: 25 | Batch_idx: 300 |  Loss_1: (0.3015) | Acc_1: (89.32%) (34415/38528)\n",
      "Epoch: 25 | Batch_idx: 310 |  Loss_1: (0.3002) | Acc_1: (89.40%) (35588/39808)\n",
      "Epoch: 25 | Batch_idx: 320 |  Loss_1: (0.2997) | Acc_1: (89.43%) (36743/41088)\n",
      "Epoch: 25 | Batch_idx: 330 |  Loss_1: (0.2997) | Acc_1: (89.44%) (37893/42368)\n",
      "Epoch: 25 | Batch_idx: 340 |  Loss_1: (0.3001) | Acc_1: (89.45%) (39044/43648)\n",
      "Epoch: 25 | Batch_idx: 350 |  Loss_1: (0.3009) | Acc_1: (89.45%) (40190/44928)\n",
      "Epoch: 25 | Batch_idx: 360 |  Loss_1: (0.3001) | Acc_1: (89.48%) (41348/46208)\n",
      "Epoch: 25 | Batch_idx: 370 |  Loss_1: (0.2992) | Acc_1: (89.52%) (42509/47488)\n",
      "Epoch: 25 | Batch_idx: 380 |  Loss_1: (0.2986) | Acc_1: (89.52%) (43659/48768)\n",
      "Epoch: 25 | Batch_idx: 390 |  Loss_1: (0.2993) | Acc_1: (89.53%) (44764/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3964) | Acc: (87.69%) (8769/10000)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.2375) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.2926) | Acc_1: (90.20%) (1270/1408)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.2813) | Acc_1: (90.25%) (2426/2688)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.2737) | Acc_1: (90.30%) (3583/3968)\n",
      "Epoch: 26 | Batch_idx: 40 |  Loss_1: (0.2734) | Acc_1: (90.40%) (4744/5248)\n",
      "Epoch: 26 | Batch_idx: 50 |  Loss_1: (0.2790) | Acc_1: (90.20%) (5888/6528)\n",
      "Epoch: 26 | Batch_idx: 60 |  Loss_1: (0.2802) | Acc_1: (90.11%) (7036/7808)\n",
      "Epoch: 26 | Batch_idx: 70 |  Loss_1: (0.2742) | Acc_1: (90.46%) (8221/9088)\n",
      "Epoch: 26 | Batch_idx: 80 |  Loss_1: (0.2788) | Acc_1: (90.31%) (9363/10368)\n",
      "Epoch: 26 | Batch_idx: 90 |  Loss_1: (0.2795) | Acc_1: (90.32%) (10520/11648)\n",
      "Epoch: 26 | Batch_idx: 100 |  Loss_1: (0.2807) | Acc_1: (90.28%) (11672/12928)\n",
      "Epoch: 26 | Batch_idx: 110 |  Loss_1: (0.2823) | Acc_1: (90.29%) (12829/14208)\n",
      "Epoch: 26 | Batch_idx: 120 |  Loss_1: (0.2820) | Acc_1: (90.35%) (13993/15488)\n",
      "Epoch: 26 | Batch_idx: 130 |  Loss_1: (0.2813) | Acc_1: (90.38%) (15155/16768)\n",
      "Epoch: 26 | Batch_idx: 140 |  Loss_1: (0.2814) | Acc_1: (90.38%) (16311/18048)\n",
      "Epoch: 26 | Batch_idx: 150 |  Loss_1: (0.2829) | Acc_1: (90.32%) (17457/19328)\n",
      "Epoch: 26 | Batch_idx: 160 |  Loss_1: (0.2839) | Acc_1: (90.27%) (18602/20608)\n",
      "Epoch: 26 | Batch_idx: 170 |  Loss_1: (0.2827) | Acc_1: (90.28%) (19761/21888)\n",
      "Epoch: 26 | Batch_idx: 180 |  Loss_1: (0.2825) | Acc_1: (90.29%) (20918/23168)\n",
      "Epoch: 26 | Batch_idx: 190 |  Loss_1: (0.2816) | Acc_1: (90.33%) (22085/24448)\n",
      "Epoch: 26 | Batch_idx: 200 |  Loss_1: (0.2831) | Acc_1: (90.27%) (23224/25728)\n",
      "Epoch: 26 | Batch_idx: 210 |  Loss_1: (0.2838) | Acc_1: (90.26%) (24377/27008)\n",
      "Epoch: 26 | Batch_idx: 220 |  Loss_1: (0.2844) | Acc_1: (90.23%) (25523/28288)\n",
      "Epoch: 26 | Batch_idx: 230 |  Loss_1: (0.2848) | Acc_1: (90.22%) (26676/29568)\n",
      "Epoch: 26 | Batch_idx: 240 |  Loss_1: (0.2849) | Acc_1: (90.17%) (27817/30848)\n",
      "Epoch: 26 | Batch_idx: 250 |  Loss_1: (0.2857) | Acc_1: (90.11%) (28950/32128)\n",
      "Epoch: 26 | Batch_idx: 260 |  Loss_1: (0.2863) | Acc_1: (90.05%) (30083/33408)\n",
      "Epoch: 26 | Batch_idx: 270 |  Loss_1: (0.2871) | Acc_1: (89.99%) (31216/34688)\n",
      "Epoch: 26 | Batch_idx: 280 |  Loss_1: (0.2866) | Acc_1: (89.98%) (32364/35968)\n",
      "Epoch: 26 | Batch_idx: 290 |  Loss_1: (0.2858) | Acc_1: (90.00%) (33525/37248)\n",
      "Epoch: 26 | Batch_idx: 300 |  Loss_1: (0.2854) | Acc_1: (90.01%) (34678/38528)\n",
      "Epoch: 26 | Batch_idx: 310 |  Loss_1: (0.2847) | Acc_1: (90.03%) (35841/39808)\n",
      "Epoch: 26 | Batch_idx: 320 |  Loss_1: (0.2848) | Acc_1: (89.99%) (36975/41088)\n",
      "Epoch: 26 | Batch_idx: 330 |  Loss_1: (0.2859) | Acc_1: (89.98%) (38121/42368)\n",
      "Epoch: 26 | Batch_idx: 340 |  Loss_1: (0.2860) | Acc_1: (89.99%) (39278/43648)\n",
      "Epoch: 26 | Batch_idx: 350 |  Loss_1: (0.2863) | Acc_1: (90.00%) (40434/44928)\n",
      "Epoch: 26 | Batch_idx: 360 |  Loss_1: (0.2868) | Acc_1: (89.98%) (41579/46208)\n",
      "Epoch: 26 | Batch_idx: 370 |  Loss_1: (0.2873) | Acc_1: (89.96%) (42722/47488)\n",
      "Epoch: 26 | Batch_idx: 380 |  Loss_1: (0.2871) | Acc_1: (89.95%) (43869/48768)\n",
      "Epoch: 26 | Batch_idx: 390 |  Loss_1: (0.2874) | Acc_1: (89.93%) (44966/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3792) | Acc: (88.40%) (8840/10000)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.2379) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.2563) | Acc_1: (90.84%) (1279/1408)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.2651) | Acc_1: (90.48%) (2432/2688)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.2567) | Acc_1: (91.13%) (3616/3968)\n",
      "Epoch: 27 | Batch_idx: 40 |  Loss_1: (0.2563) | Acc_1: (91.06%) (4779/5248)\n",
      "Epoch: 27 | Batch_idx: 50 |  Loss_1: (0.2639) | Acc_1: (90.76%) (5925/6528)\n",
      "Epoch: 27 | Batch_idx: 60 |  Loss_1: (0.2610) | Acc_1: (90.82%) (7091/7808)\n",
      "Epoch: 27 | Batch_idx: 70 |  Loss_1: (0.2643) | Acc_1: (90.72%) (8245/9088)\n",
      "Epoch: 27 | Batch_idx: 80 |  Loss_1: (0.2623) | Acc_1: (90.80%) (9414/10368)\n",
      "Epoch: 27 | Batch_idx: 90 |  Loss_1: (0.2643) | Acc_1: (90.68%) (10562/11648)\n",
      "Epoch: 27 | Batch_idx: 100 |  Loss_1: (0.2616) | Acc_1: (90.78%) (11736/12928)\n",
      "Epoch: 27 | Batch_idx: 110 |  Loss_1: (0.2617) | Acc_1: (90.75%) (12894/14208)\n",
      "Epoch: 27 | Batch_idx: 120 |  Loss_1: (0.2595) | Acc_1: (90.87%) (14074/15488)\n",
      "Epoch: 27 | Batch_idx: 130 |  Loss_1: (0.2598) | Acc_1: (90.86%) (15235/16768)\n",
      "Epoch: 27 | Batch_idx: 140 |  Loss_1: (0.2607) | Acc_1: (90.83%) (16393/18048)\n",
      "Epoch: 27 | Batch_idx: 150 |  Loss_1: (0.2625) | Acc_1: (90.76%) (17542/19328)\n",
      "Epoch: 27 | Batch_idx: 160 |  Loss_1: (0.2631) | Acc_1: (90.72%) (18696/20608)\n",
      "Epoch: 27 | Batch_idx: 170 |  Loss_1: (0.2655) | Acc_1: (90.61%) (19832/21888)\n",
      "Epoch: 27 | Batch_idx: 180 |  Loss_1: (0.2647) | Acc_1: (90.61%) (20992/23168)\n",
      "Epoch: 27 | Batch_idx: 190 |  Loss_1: (0.2646) | Acc_1: (90.65%) (22162/24448)\n",
      "Epoch: 27 | Batch_idx: 200 |  Loss_1: (0.2644) | Acc_1: (90.66%) (23325/25728)\n",
      "Epoch: 27 | Batch_idx: 210 |  Loss_1: (0.2645) | Acc_1: (90.65%) (24483/27008)\n",
      "Epoch: 27 | Batch_idx: 220 |  Loss_1: (0.2655) | Acc_1: (90.62%) (25635/28288)\n",
      "Epoch: 27 | Batch_idx: 230 |  Loss_1: (0.2657) | Acc_1: (90.61%) (26793/29568)\n",
      "Epoch: 27 | Batch_idx: 240 |  Loss_1: (0.2662) | Acc_1: (90.60%) (27948/30848)\n",
      "Epoch: 27 | Batch_idx: 250 |  Loss_1: (0.2656) | Acc_1: (90.62%) (29114/32128)\n",
      "Epoch: 27 | Batch_idx: 260 |  Loss_1: (0.2652) | Acc_1: (90.65%) (30285/33408)\n",
      "Epoch: 27 | Batch_idx: 270 |  Loss_1: (0.2662) | Acc_1: (90.61%) (31432/34688)\n",
      "Epoch: 27 | Batch_idx: 280 |  Loss_1: (0.2671) | Acc_1: (90.61%) (32591/35968)\n",
      "Epoch: 27 | Batch_idx: 290 |  Loss_1: (0.2687) | Acc_1: (90.54%) (33725/37248)\n",
      "Epoch: 27 | Batch_idx: 300 |  Loss_1: (0.2689) | Acc_1: (90.54%) (34884/38528)\n",
      "Epoch: 27 | Batch_idx: 310 |  Loss_1: (0.2694) | Acc_1: (90.54%) (36042/39808)\n",
      "Epoch: 27 | Batch_idx: 320 |  Loss_1: (0.2702) | Acc_1: (90.52%) (37194/41088)\n",
      "Epoch: 27 | Batch_idx: 330 |  Loss_1: (0.2703) | Acc_1: (90.52%) (38352/42368)\n",
      "Epoch: 27 | Batch_idx: 340 |  Loss_1: (0.2715) | Acc_1: (90.46%) (39484/43648)\n",
      "Epoch: 27 | Batch_idx: 350 |  Loss_1: (0.2714) | Acc_1: (90.48%) (40650/44928)\n",
      "Epoch: 27 | Batch_idx: 360 |  Loss_1: (0.2719) | Acc_1: (90.47%) (41804/46208)\n",
      "Epoch: 27 | Batch_idx: 370 |  Loss_1: (0.2725) | Acc_1: (90.46%) (42957/47488)\n",
      "Epoch: 27 | Batch_idx: 380 |  Loss_1: (0.2720) | Acc_1: (90.50%) (44133/48768)\n",
      "Epoch: 27 | Batch_idx: 390 |  Loss_1: (0.2717) | Acc_1: (90.53%) (45263/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4860) | Acc: (85.62%) (8562/10000)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.2187) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.2459) | Acc_1: (91.12%) (1283/1408)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.2515) | Acc_1: (91.07%) (2448/2688)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.2527) | Acc_1: (91.15%) (3617/3968)\n",
      "Epoch: 28 | Batch_idx: 40 |  Loss_1: (0.2593) | Acc_1: (90.62%) (4756/5248)\n",
      "Epoch: 28 | Batch_idx: 50 |  Loss_1: (0.2570) | Acc_1: (90.90%) (5934/6528)\n",
      "Epoch: 28 | Batch_idx: 60 |  Loss_1: (0.2556) | Acc_1: (90.93%) (7100/7808)\n",
      "Epoch: 28 | Batch_idx: 70 |  Loss_1: (0.2546) | Acc_1: (90.97%) (8267/9088)\n",
      "Epoch: 28 | Batch_idx: 80 |  Loss_1: (0.2555) | Acc_1: (90.90%) (9425/10368)\n",
      "Epoch: 28 | Batch_idx: 90 |  Loss_1: (0.2522) | Acc_1: (91.12%) (10614/11648)\n",
      "Epoch: 28 | Batch_idx: 100 |  Loss_1: (0.2510) | Acc_1: (91.21%) (11791/12928)\n",
      "Epoch: 28 | Batch_idx: 110 |  Loss_1: (0.2516) | Acc_1: (91.19%) (12956/14208)\n",
      "Epoch: 28 | Batch_idx: 120 |  Loss_1: (0.2556) | Acc_1: (91.04%) (14100/15488)\n",
      "Epoch: 28 | Batch_idx: 130 |  Loss_1: (0.2576) | Acc_1: (90.95%) (15251/16768)\n",
      "Epoch: 28 | Batch_idx: 140 |  Loss_1: (0.2585) | Acc_1: (90.91%) (16407/18048)\n",
      "Epoch: 28 | Batch_idx: 150 |  Loss_1: (0.2601) | Acc_1: (90.84%) (17558/19328)\n",
      "Epoch: 28 | Batch_idx: 160 |  Loss_1: (0.2631) | Acc_1: (90.73%) (18697/20608)\n",
      "Epoch: 28 | Batch_idx: 170 |  Loss_1: (0.2643) | Acc_1: (90.66%) (19844/21888)\n",
      "Epoch: 28 | Batch_idx: 180 |  Loss_1: (0.2654) | Acc_1: (90.61%) (20992/23168)\n",
      "Epoch: 28 | Batch_idx: 190 |  Loss_1: (0.2654) | Acc_1: (90.61%) (22152/24448)\n",
      "Epoch: 28 | Batch_idx: 200 |  Loss_1: (0.2639) | Acc_1: (90.70%) (23336/25728)\n",
      "Epoch: 28 | Batch_idx: 210 |  Loss_1: (0.2636) | Acc_1: (90.72%) (24502/27008)\n",
      "Epoch: 28 | Batch_idx: 220 |  Loss_1: (0.2644) | Acc_1: (90.72%) (25663/28288)\n",
      "Epoch: 28 | Batch_idx: 230 |  Loss_1: (0.2653) | Acc_1: (90.66%) (26805/29568)\n",
      "Epoch: 28 | Batch_idx: 240 |  Loss_1: (0.2653) | Acc_1: (90.64%) (27961/30848)\n",
      "Epoch: 28 | Batch_idx: 250 |  Loss_1: (0.2652) | Acc_1: (90.63%) (29119/32128)\n",
      "Epoch: 28 | Batch_idx: 260 |  Loss_1: (0.2645) | Acc_1: (90.68%) (30294/33408)\n",
      "Epoch: 28 | Batch_idx: 270 |  Loss_1: (0.2651) | Acc_1: (90.68%) (31454/34688)\n",
      "Epoch: 28 | Batch_idx: 280 |  Loss_1: (0.2655) | Acc_1: (90.68%) (32615/35968)\n",
      "Epoch: 28 | Batch_idx: 290 |  Loss_1: (0.2676) | Acc_1: (90.60%) (33747/37248)\n",
      "Epoch: 28 | Batch_idx: 300 |  Loss_1: (0.2682) | Acc_1: (90.62%) (34913/38528)\n",
      "Epoch: 28 | Batch_idx: 310 |  Loss_1: (0.2691) | Acc_1: (90.57%) (36054/39808)\n",
      "Epoch: 28 | Batch_idx: 320 |  Loss_1: (0.2698) | Acc_1: (90.54%) (37203/41088)\n",
      "Epoch: 28 | Batch_idx: 330 |  Loss_1: (0.2696) | Acc_1: (90.54%) (38360/42368)\n",
      "Epoch: 28 | Batch_idx: 340 |  Loss_1: (0.2693) | Acc_1: (90.57%) (39532/43648)\n",
      "Epoch: 28 | Batch_idx: 350 |  Loss_1: (0.2706) | Acc_1: (90.55%) (40684/44928)\n",
      "Epoch: 28 | Batch_idx: 360 |  Loss_1: (0.2707) | Acc_1: (90.54%) (41838/46208)\n",
      "Epoch: 28 | Batch_idx: 370 |  Loss_1: (0.2711) | Acc_1: (90.52%) (42986/47488)\n",
      "Epoch: 28 | Batch_idx: 380 |  Loss_1: (0.2708) | Acc_1: (90.55%) (44158/48768)\n",
      "Epoch: 28 | Batch_idx: 390 |  Loss_1: (0.2709) | Acc_1: (90.54%) (45268/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4660) | Acc: (86.37%) (8637/10000)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.2315) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.2582) | Acc_1: (90.55%) (1275/1408)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.2470) | Acc_1: (91.29%) (2454/2688)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.2519) | Acc_1: (90.95%) (3609/3968)\n",
      "Epoch: 29 | Batch_idx: 40 |  Loss_1: (0.2454) | Acc_1: (91.12%) (4782/5248)\n",
      "Epoch: 29 | Batch_idx: 50 |  Loss_1: (0.2455) | Acc_1: (91.24%) (5956/6528)\n",
      "Epoch: 29 | Batch_idx: 60 |  Loss_1: (0.2454) | Acc_1: (91.29%) (7128/7808)\n",
      "Epoch: 29 | Batch_idx: 70 |  Loss_1: (0.2437) | Acc_1: (91.42%) (8308/9088)\n",
      "Epoch: 29 | Batch_idx: 80 |  Loss_1: (0.2411) | Acc_1: (91.50%) (9487/10368)\n",
      "Epoch: 29 | Batch_idx: 90 |  Loss_1: (0.2408) | Acc_1: (91.47%) (10655/11648)\n",
      "Epoch: 29 | Batch_idx: 100 |  Loss_1: (0.2408) | Acc_1: (91.46%) (11824/12928)\n",
      "Epoch: 29 | Batch_idx: 110 |  Loss_1: (0.2442) | Acc_1: (91.29%) (12970/14208)\n",
      "Epoch: 29 | Batch_idx: 120 |  Loss_1: (0.2447) | Acc_1: (91.28%) (14137/15488)\n",
      "Epoch: 29 | Batch_idx: 130 |  Loss_1: (0.2461) | Acc_1: (91.28%) (15306/16768)\n",
      "Epoch: 29 | Batch_idx: 140 |  Loss_1: (0.2472) | Acc_1: (91.28%) (16474/18048)\n",
      "Epoch: 29 | Batch_idx: 150 |  Loss_1: (0.2468) | Acc_1: (91.29%) (17645/19328)\n",
      "Epoch: 29 | Batch_idx: 160 |  Loss_1: (0.2475) | Acc_1: (91.26%) (18807/20608)\n",
      "Epoch: 29 | Batch_idx: 170 |  Loss_1: (0.2484) | Acc_1: (91.27%) (19978/21888)\n",
      "Epoch: 29 | Batch_idx: 180 |  Loss_1: (0.2479) | Acc_1: (91.29%) (21151/23168)\n",
      "Epoch: 29 | Batch_idx: 190 |  Loss_1: (0.2495) | Acc_1: (91.25%) (22308/24448)\n",
      "Epoch: 29 | Batch_idx: 200 |  Loss_1: (0.2489) | Acc_1: (91.27%) (23482/25728)\n",
      "Epoch: 29 | Batch_idx: 210 |  Loss_1: (0.2492) | Acc_1: (91.26%) (24648/27008)\n",
      "Epoch: 29 | Batch_idx: 220 |  Loss_1: (0.2488) | Acc_1: (91.25%) (25812/28288)\n",
      "Epoch: 29 | Batch_idx: 230 |  Loss_1: (0.2493) | Acc_1: (91.23%) (26974/29568)\n",
      "Epoch: 29 | Batch_idx: 240 |  Loss_1: (0.2499) | Acc_1: (91.24%) (28147/30848)\n",
      "Epoch: 29 | Batch_idx: 250 |  Loss_1: (0.2503) | Acc_1: (91.22%) (29307/32128)\n",
      "Epoch: 29 | Batch_idx: 260 |  Loss_1: (0.2503) | Acc_1: (91.21%) (30473/33408)\n",
      "Epoch: 29 | Batch_idx: 270 |  Loss_1: (0.2505) | Acc_1: (91.22%) (31642/34688)\n",
      "Epoch: 29 | Batch_idx: 280 |  Loss_1: (0.2501) | Acc_1: (91.25%) (32822/35968)\n",
      "Epoch: 29 | Batch_idx: 290 |  Loss_1: (0.2505) | Acc_1: (91.22%) (33979/37248)\n",
      "Epoch: 29 | Batch_idx: 300 |  Loss_1: (0.2507) | Acc_1: (91.21%) (35142/38528)\n",
      "Epoch: 29 | Batch_idx: 310 |  Loss_1: (0.2498) | Acc_1: (91.27%) (36334/39808)\n",
      "Epoch: 29 | Batch_idx: 320 |  Loss_1: (0.2509) | Acc_1: (91.24%) (37490/41088)\n",
      "Epoch: 29 | Batch_idx: 330 |  Loss_1: (0.2520) | Acc_1: (91.20%) (38638/42368)\n",
      "Epoch: 29 | Batch_idx: 340 |  Loss_1: (0.2530) | Acc_1: (91.17%) (39794/43648)\n",
      "Epoch: 29 | Batch_idx: 350 |  Loss_1: (0.2532) | Acc_1: (91.15%) (40951/44928)\n",
      "Epoch: 29 | Batch_idx: 360 |  Loss_1: (0.2549) | Acc_1: (91.08%) (42084/46208)\n",
      "Epoch: 29 | Batch_idx: 370 |  Loss_1: (0.2554) | Acc_1: (91.08%) (43254/47488)\n",
      "Epoch: 29 | Batch_idx: 380 |  Loss_1: (0.2560) | Acc_1: (91.06%) (44408/48768)\n",
      "Epoch: 29 | Batch_idx: 390 |  Loss_1: (0.2556) | Acc_1: (91.09%) (45543/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4052) | Acc: (87.46%) (8746/10000)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.2800) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.2151) | Acc_1: (92.19%) (1298/1408)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.2342) | Acc_1: (91.52%) (2460/2688)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.2380) | Acc_1: (91.41%) (3627/3968)\n",
      "Epoch: 30 | Batch_idx: 40 |  Loss_1: (0.2409) | Acc_1: (91.29%) (4791/5248)\n",
      "Epoch: 30 | Batch_idx: 50 |  Loss_1: (0.2396) | Acc_1: (91.38%) (5965/6528)\n",
      "Epoch: 30 | Batch_idx: 60 |  Loss_1: (0.2366) | Acc_1: (91.66%) (7157/7808)\n",
      "Epoch: 30 | Batch_idx: 70 |  Loss_1: (0.2375) | Acc_1: (91.62%) (8326/9088)\n",
      "Epoch: 30 | Batch_idx: 80 |  Loss_1: (0.2378) | Acc_1: (91.65%) (9502/10368)\n",
      "Epoch: 30 | Batch_idx: 90 |  Loss_1: (0.2386) | Acc_1: (91.64%) (10674/11648)\n",
      "Epoch: 30 | Batch_idx: 100 |  Loss_1: (0.2397) | Acc_1: (91.71%) (11856/12928)\n",
      "Epoch: 30 | Batch_idx: 110 |  Loss_1: (0.2402) | Acc_1: (91.65%) (13021/14208)\n",
      "Epoch: 30 | Batch_idx: 120 |  Loss_1: (0.2430) | Acc_1: (91.53%) (14176/15488)\n",
      "Epoch: 30 | Batch_idx: 130 |  Loss_1: (0.2439) | Acc_1: (91.48%) (15339/16768)\n",
      "Epoch: 30 | Batch_idx: 140 |  Loss_1: (0.2442) | Acc_1: (91.48%) (16511/18048)\n",
      "Epoch: 30 | Batch_idx: 150 |  Loss_1: (0.2465) | Acc_1: (91.45%) (17676/19328)\n",
      "Epoch: 30 | Batch_idx: 160 |  Loss_1: (0.2499) | Acc_1: (91.36%) (18828/20608)\n",
      "Epoch: 30 | Batch_idx: 170 |  Loss_1: (0.2496) | Acc_1: (91.31%) (19986/21888)\n",
      "Epoch: 30 | Batch_idx: 180 |  Loss_1: (0.2490) | Acc_1: (91.30%) (21152/23168)\n",
      "Epoch: 30 | Batch_idx: 190 |  Loss_1: (0.2500) | Acc_1: (91.27%) (22314/24448)\n",
      "Epoch: 30 | Batch_idx: 200 |  Loss_1: (0.2497) | Acc_1: (91.28%) (23485/25728)\n",
      "Epoch: 30 | Batch_idx: 210 |  Loss_1: (0.2509) | Acc_1: (91.25%) (24644/27008)\n",
      "Epoch: 30 | Batch_idx: 220 |  Loss_1: (0.2509) | Acc_1: (91.23%) (25808/28288)\n",
      "Epoch: 30 | Batch_idx: 230 |  Loss_1: (0.2505) | Acc_1: (91.26%) (26984/29568)\n",
      "Epoch: 30 | Batch_idx: 240 |  Loss_1: (0.2506) | Acc_1: (91.24%) (28147/30848)\n",
      "Epoch: 30 | Batch_idx: 250 |  Loss_1: (0.2511) | Acc_1: (91.24%) (29313/32128)\n",
      "Epoch: 30 | Batch_idx: 260 |  Loss_1: (0.2506) | Acc_1: (91.27%) (30492/33408)\n",
      "Epoch: 30 | Batch_idx: 270 |  Loss_1: (0.2523) | Acc_1: (91.20%) (31636/34688)\n",
      "Epoch: 30 | Batch_idx: 280 |  Loss_1: (0.2517) | Acc_1: (91.22%) (32811/35968)\n",
      "Epoch: 30 | Batch_idx: 290 |  Loss_1: (0.2523) | Acc_1: (91.19%) (33968/37248)\n",
      "Epoch: 30 | Batch_idx: 300 |  Loss_1: (0.2524) | Acc_1: (91.17%) (35127/38528)\n",
      "Epoch: 30 | Batch_idx: 310 |  Loss_1: (0.2522) | Acc_1: (91.18%) (36297/39808)\n",
      "Epoch: 30 | Batch_idx: 320 |  Loss_1: (0.2516) | Acc_1: (91.21%) (37475/41088)\n",
      "Epoch: 30 | Batch_idx: 330 |  Loss_1: (0.2502) | Acc_1: (91.28%) (38675/42368)\n",
      "Epoch: 30 | Batch_idx: 340 |  Loss_1: (0.2501) | Acc_1: (91.31%) (39857/43648)\n",
      "Epoch: 30 | Batch_idx: 350 |  Loss_1: (0.2510) | Acc_1: (91.27%) (41007/44928)\n",
      "Epoch: 30 | Batch_idx: 360 |  Loss_1: (0.2511) | Acc_1: (91.24%) (42160/46208)\n",
      "Epoch: 30 | Batch_idx: 370 |  Loss_1: (0.2520) | Acc_1: (91.21%) (43314/47488)\n",
      "Epoch: 30 | Batch_idx: 380 |  Loss_1: (0.2517) | Acc_1: (91.24%) (44494/48768)\n",
      "Epoch: 30 | Batch_idx: 390 |  Loss_1: (0.2528) | Acc_1: (91.20%) (45600/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3861) | Acc: (88.16%) (8816/10000)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.2038) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.2236) | Acc_1: (92.97%) (1309/1408)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.2290) | Acc_1: (92.37%) (2483/2688)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.2288) | Acc_1: (92.09%) (3654/3968)\n",
      "Epoch: 31 | Batch_idx: 40 |  Loss_1: (0.2298) | Acc_1: (92.13%) (4835/5248)\n",
      "Epoch: 31 | Batch_idx: 50 |  Loss_1: (0.2295) | Acc_1: (92.22%) (6020/6528)\n",
      "Epoch: 31 | Batch_idx: 60 |  Loss_1: (0.2282) | Acc_1: (92.30%) (7207/7808)\n",
      "Epoch: 31 | Batch_idx: 70 |  Loss_1: (0.2317) | Acc_1: (92.20%) (8379/9088)\n",
      "Epoch: 31 | Batch_idx: 80 |  Loss_1: (0.2301) | Acc_1: (92.15%) (9554/10368)\n",
      "Epoch: 31 | Batch_idx: 90 |  Loss_1: (0.2332) | Acc_1: (92.17%) (10736/11648)\n",
      "Epoch: 31 | Batch_idx: 100 |  Loss_1: (0.2351) | Acc_1: (91.94%) (11886/12928)\n",
      "Epoch: 31 | Batch_idx: 110 |  Loss_1: (0.2370) | Acc_1: (91.88%) (13054/14208)\n",
      "Epoch: 31 | Batch_idx: 120 |  Loss_1: (0.2401) | Acc_1: (91.72%) (14206/15488)\n",
      "Epoch: 31 | Batch_idx: 130 |  Loss_1: (0.2401) | Acc_1: (91.69%) (15375/16768)\n",
      "Epoch: 31 | Batch_idx: 140 |  Loss_1: (0.2406) | Acc_1: (91.65%) (16541/18048)\n",
      "Epoch: 31 | Batch_idx: 150 |  Loss_1: (0.2402) | Acc_1: (91.65%) (17715/19328)\n",
      "Epoch: 31 | Batch_idx: 160 |  Loss_1: (0.2405) | Acc_1: (91.65%) (18887/20608)\n",
      "Epoch: 31 | Batch_idx: 170 |  Loss_1: (0.2407) | Acc_1: (91.61%) (20051/21888)\n",
      "Epoch: 31 | Batch_idx: 180 |  Loss_1: (0.2406) | Acc_1: (91.64%) (21230/23168)\n",
      "Epoch: 31 | Batch_idx: 190 |  Loss_1: (0.2398) | Acc_1: (91.64%) (22403/24448)\n",
      "Epoch: 31 | Batch_idx: 200 |  Loss_1: (0.2398) | Acc_1: (91.62%) (23573/25728)\n",
      "Epoch: 31 | Batch_idx: 210 |  Loss_1: (0.2395) | Acc_1: (91.66%) (24755/27008)\n",
      "Epoch: 31 | Batch_idx: 220 |  Loss_1: (0.2399) | Acc_1: (91.63%) (25920/28288)\n",
      "Epoch: 31 | Batch_idx: 230 |  Loss_1: (0.2410) | Acc_1: (91.59%) (27081/29568)\n",
      "Epoch: 31 | Batch_idx: 240 |  Loss_1: (0.2403) | Acc_1: (91.60%) (28256/30848)\n",
      "Epoch: 31 | Batch_idx: 250 |  Loss_1: (0.2398) | Acc_1: (91.60%) (29429/32128)\n",
      "Epoch: 31 | Batch_idx: 260 |  Loss_1: (0.2385) | Acc_1: (91.62%) (30610/33408)\n",
      "Epoch: 31 | Batch_idx: 270 |  Loss_1: (0.2386) | Acc_1: (91.61%) (31777/34688)\n",
      "Epoch: 31 | Batch_idx: 280 |  Loss_1: (0.2392) | Acc_1: (91.57%) (32935/35968)\n",
      "Epoch: 31 | Batch_idx: 290 |  Loss_1: (0.2388) | Acc_1: (91.59%) (34117/37248)\n",
      "Epoch: 31 | Batch_idx: 300 |  Loss_1: (0.2381) | Acc_1: (91.61%) (35294/38528)\n",
      "Epoch: 31 | Batch_idx: 310 |  Loss_1: (0.2389) | Acc_1: (91.57%) (36451/39808)\n",
      "Epoch: 31 | Batch_idx: 320 |  Loss_1: (0.2385) | Acc_1: (91.58%) (37628/41088)\n",
      "Epoch: 31 | Batch_idx: 330 |  Loss_1: (0.2390) | Acc_1: (91.58%) (38800/42368)\n",
      "Epoch: 31 | Batch_idx: 340 |  Loss_1: (0.2404) | Acc_1: (91.55%) (39960/43648)\n",
      "Epoch: 31 | Batch_idx: 350 |  Loss_1: (0.2405) | Acc_1: (91.54%) (41127/44928)\n",
      "Epoch: 31 | Batch_idx: 360 |  Loss_1: (0.2399) | Acc_1: (91.56%) (42306/46208)\n",
      "Epoch: 31 | Batch_idx: 370 |  Loss_1: (0.2411) | Acc_1: (91.52%) (43461/47488)\n",
      "Epoch: 31 | Batch_idx: 380 |  Loss_1: (0.2421) | Acc_1: (91.52%) (44631/48768)\n",
      "Epoch: 31 | Batch_idx: 390 |  Loss_1: (0.2419) | Acc_1: (91.50%) (45751/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4636) | Acc: (85.72%) (8572/10000)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.1747) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.2289) | Acc_1: (92.12%) (1297/1408)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.2324) | Acc_1: (92.34%) (2482/2688)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.2220) | Acc_1: (92.69%) (3678/3968)\n",
      "Epoch: 32 | Batch_idx: 40 |  Loss_1: (0.2272) | Acc_1: (92.30%) (4844/5248)\n",
      "Epoch: 32 | Batch_idx: 50 |  Loss_1: (0.2235) | Acc_1: (92.29%) (6025/6528)\n",
      "Epoch: 32 | Batch_idx: 60 |  Loss_1: (0.2269) | Acc_1: (92.16%) (7196/7808)\n",
      "Epoch: 32 | Batch_idx: 70 |  Loss_1: (0.2244) | Acc_1: (92.21%) (8380/9088)\n",
      "Epoch: 32 | Batch_idx: 80 |  Loss_1: (0.2220) | Acc_1: (92.34%) (9574/10368)\n",
      "Epoch: 32 | Batch_idx: 90 |  Loss_1: (0.2233) | Acc_1: (92.25%) (10745/11648)\n",
      "Epoch: 32 | Batch_idx: 100 |  Loss_1: (0.2248) | Acc_1: (92.14%) (11912/12928)\n",
      "Epoch: 32 | Batch_idx: 110 |  Loss_1: (0.2240) | Acc_1: (92.12%) (13089/14208)\n",
      "Epoch: 32 | Batch_idx: 120 |  Loss_1: (0.2249) | Acc_1: (92.13%) (14269/15488)\n",
      "Epoch: 32 | Batch_idx: 130 |  Loss_1: (0.2279) | Acc_1: (92.04%) (15434/16768)\n",
      "Epoch: 32 | Batch_idx: 140 |  Loss_1: (0.2291) | Acc_1: (92.03%) (16609/18048)\n",
      "Epoch: 32 | Batch_idx: 150 |  Loss_1: (0.2280) | Acc_1: (92.06%) (17794/19328)\n",
      "Epoch: 32 | Batch_idx: 160 |  Loss_1: (0.2273) | Acc_1: (92.09%) (18978/20608)\n",
      "Epoch: 32 | Batch_idx: 170 |  Loss_1: (0.2294) | Acc_1: (92.01%) (20140/21888)\n",
      "Epoch: 32 | Batch_idx: 180 |  Loss_1: (0.2283) | Acc_1: (92.08%) (21332/23168)\n",
      "Epoch: 32 | Batch_idx: 190 |  Loss_1: (0.2278) | Acc_1: (92.09%) (22513/24448)\n",
      "Epoch: 32 | Batch_idx: 200 |  Loss_1: (0.2283) | Acc_1: (92.06%) (23684/25728)\n",
      "Epoch: 32 | Batch_idx: 210 |  Loss_1: (0.2280) | Acc_1: (92.07%) (24866/27008)\n",
      "Epoch: 32 | Batch_idx: 220 |  Loss_1: (0.2298) | Acc_1: (92.04%) (26035/28288)\n",
      "Epoch: 32 | Batch_idx: 230 |  Loss_1: (0.2302) | Acc_1: (92.06%) (27219/29568)\n",
      "Epoch: 32 | Batch_idx: 240 |  Loss_1: (0.2313) | Acc_1: (92.05%) (28397/30848)\n",
      "Epoch: 32 | Batch_idx: 250 |  Loss_1: (0.2332) | Acc_1: (92.01%) (29560/32128)\n",
      "Epoch: 32 | Batch_idx: 260 |  Loss_1: (0.2330) | Acc_1: (91.99%) (30731/33408)\n",
      "Epoch: 32 | Batch_idx: 270 |  Loss_1: (0.2336) | Acc_1: (91.95%) (31894/34688)\n",
      "Epoch: 32 | Batch_idx: 280 |  Loss_1: (0.2334) | Acc_1: (91.94%) (33070/35968)\n",
      "Epoch: 32 | Batch_idx: 290 |  Loss_1: (0.2344) | Acc_1: (91.92%) (34238/37248)\n",
      "Epoch: 32 | Batch_idx: 300 |  Loss_1: (0.2349) | Acc_1: (91.90%) (35409/38528)\n",
      "Epoch: 32 | Batch_idx: 310 |  Loss_1: (0.2358) | Acc_1: (91.90%) (36582/39808)\n",
      "Epoch: 32 | Batch_idx: 320 |  Loss_1: (0.2361) | Acc_1: (91.87%) (37747/41088)\n",
      "Epoch: 32 | Batch_idx: 330 |  Loss_1: (0.2346) | Acc_1: (91.91%) (38939/42368)\n",
      "Epoch: 32 | Batch_idx: 340 |  Loss_1: (0.2350) | Acc_1: (91.88%) (40104/43648)\n",
      "Epoch: 32 | Batch_idx: 350 |  Loss_1: (0.2359) | Acc_1: (91.86%) (41272/44928)\n",
      "Epoch: 32 | Batch_idx: 360 |  Loss_1: (0.2360) | Acc_1: (91.85%) (42443/46208)\n",
      "Epoch: 32 | Batch_idx: 370 |  Loss_1: (0.2360) | Acc_1: (91.88%) (43631/47488)\n",
      "Epoch: 32 | Batch_idx: 380 |  Loss_1: (0.2362) | Acc_1: (91.86%) (44796/48768)\n",
      "Epoch: 32 | Batch_idx: 390 |  Loss_1: (0.2363) | Acc_1: (91.85%) (45924/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4119) | Acc: (87.72%) (8772/10000)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.2962) | Acc_1: (91.41%) (117/128)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.2069) | Acc_1: (92.47%) (1302/1408)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.2053) | Acc_1: (92.63%) (2490/2688)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.2065) | Acc_1: (92.49%) (3670/3968)\n",
      "Epoch: 33 | Batch_idx: 40 |  Loss_1: (0.2094) | Acc_1: (92.36%) (4847/5248)\n",
      "Epoch: 33 | Batch_idx: 50 |  Loss_1: (0.2099) | Acc_1: (92.42%) (6033/6528)\n",
      "Epoch: 33 | Batch_idx: 60 |  Loss_1: (0.2120) | Acc_1: (92.49%) (7222/7808)\n",
      "Epoch: 33 | Batch_idx: 70 |  Loss_1: (0.2144) | Acc_1: (92.32%) (8390/9088)\n",
      "Epoch: 33 | Batch_idx: 80 |  Loss_1: (0.2169) | Acc_1: (92.36%) (9576/10368)\n",
      "Epoch: 33 | Batch_idx: 90 |  Loss_1: (0.2153) | Acc_1: (92.38%) (10761/11648)\n",
      "Epoch: 33 | Batch_idx: 100 |  Loss_1: (0.2158) | Acc_1: (92.38%) (11943/12928)\n",
      "Epoch: 33 | Batch_idx: 110 |  Loss_1: (0.2150) | Acc_1: (92.50%) (13142/14208)\n",
      "Epoch: 33 | Batch_idx: 120 |  Loss_1: (0.2147) | Acc_1: (92.55%) (14334/15488)\n",
      "Epoch: 33 | Batch_idx: 130 |  Loss_1: (0.2151) | Acc_1: (92.51%) (15512/16768)\n",
      "Epoch: 33 | Batch_idx: 140 |  Loss_1: (0.2169) | Acc_1: (92.49%) (16692/18048)\n",
      "Epoch: 33 | Batch_idx: 150 |  Loss_1: (0.2168) | Acc_1: (92.50%) (17879/19328)\n",
      "Epoch: 33 | Batch_idx: 160 |  Loss_1: (0.2176) | Acc_1: (92.50%) (19062/20608)\n",
      "Epoch: 33 | Batch_idx: 170 |  Loss_1: (0.2199) | Acc_1: (92.41%) (20226/21888)\n",
      "Epoch: 33 | Batch_idx: 180 |  Loss_1: (0.2207) | Acc_1: (92.35%) (21395/23168)\n",
      "Epoch: 33 | Batch_idx: 190 |  Loss_1: (0.2211) | Acc_1: (92.32%) (22570/24448)\n",
      "Epoch: 33 | Batch_idx: 200 |  Loss_1: (0.2214) | Acc_1: (92.30%) (23748/25728)\n",
      "Epoch: 33 | Batch_idx: 210 |  Loss_1: (0.2200) | Acc_1: (92.35%) (24942/27008)\n",
      "Epoch: 33 | Batch_idx: 220 |  Loss_1: (0.2194) | Acc_1: (92.35%) (26124/28288)\n",
      "Epoch: 33 | Batch_idx: 230 |  Loss_1: (0.2197) | Acc_1: (92.34%) (27303/29568)\n",
      "Epoch: 33 | Batch_idx: 240 |  Loss_1: (0.2202) | Acc_1: (92.29%) (28471/30848)\n",
      "Epoch: 33 | Batch_idx: 250 |  Loss_1: (0.2202) | Acc_1: (92.30%) (29655/32128)\n",
      "Epoch: 33 | Batch_idx: 260 |  Loss_1: (0.2204) | Acc_1: (92.30%) (30835/33408)\n",
      "Epoch: 33 | Batch_idx: 270 |  Loss_1: (0.2204) | Acc_1: (92.28%) (32011/34688)\n",
      "Epoch: 33 | Batch_idx: 280 |  Loss_1: (0.2207) | Acc_1: (92.25%) (33179/35968)\n",
      "Epoch: 33 | Batch_idx: 290 |  Loss_1: (0.2206) | Acc_1: (92.24%) (34359/37248)\n",
      "Epoch: 33 | Batch_idx: 300 |  Loss_1: (0.2205) | Acc_1: (92.27%) (35548/38528)\n",
      "Epoch: 33 | Batch_idx: 310 |  Loss_1: (0.2209) | Acc_1: (92.25%) (36721/39808)\n",
      "Epoch: 33 | Batch_idx: 320 |  Loss_1: (0.2213) | Acc_1: (92.25%) (37904/41088)\n",
      "Epoch: 33 | Batch_idx: 330 |  Loss_1: (0.2214) | Acc_1: (92.24%) (39079/42368)\n",
      "Epoch: 33 | Batch_idx: 340 |  Loss_1: (0.2219) | Acc_1: (92.22%) (40253/43648)\n",
      "Epoch: 33 | Batch_idx: 350 |  Loss_1: (0.2221) | Acc_1: (92.21%) (41428/44928)\n",
      "Epoch: 33 | Batch_idx: 360 |  Loss_1: (0.2225) | Acc_1: (92.20%) (42602/46208)\n",
      "Epoch: 33 | Batch_idx: 370 |  Loss_1: (0.2231) | Acc_1: (92.18%) (43776/47488)\n",
      "Epoch: 33 | Batch_idx: 380 |  Loss_1: (0.2228) | Acc_1: (92.18%) (44956/48768)\n",
      "Epoch: 33 | Batch_idx: 390 |  Loss_1: (0.2237) | Acc_1: (92.15%) (46074/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3770) | Acc: (88.78%) (8878/10000)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.2758) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.2325) | Acc_1: (90.98%) (1281/1408)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.2140) | Acc_1: (92.34%) (2482/2688)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.2186) | Acc_1: (92.52%) (3671/3968)\n",
      "Epoch: 34 | Batch_idx: 40 |  Loss_1: (0.2150) | Acc_1: (92.59%) (4859/5248)\n",
      "Epoch: 34 | Batch_idx: 50 |  Loss_1: (0.2158) | Acc_1: (92.60%) (6045/6528)\n",
      "Epoch: 34 | Batch_idx: 60 |  Loss_1: (0.2139) | Acc_1: (92.67%) (7236/7808)\n",
      "Epoch: 34 | Batch_idx: 70 |  Loss_1: (0.2139) | Acc_1: (92.56%) (8412/9088)\n",
      "Epoch: 34 | Batch_idx: 80 |  Loss_1: (0.2134) | Acc_1: (92.50%) (9590/10368)\n",
      "Epoch: 34 | Batch_idx: 90 |  Loss_1: (0.2122) | Acc_1: (92.51%) (10775/11648)\n",
      "Epoch: 34 | Batch_idx: 100 |  Loss_1: (0.2086) | Acc_1: (92.61%) (11973/12928)\n",
      "Epoch: 34 | Batch_idx: 110 |  Loss_1: (0.2095) | Acc_1: (92.55%) (13149/14208)\n",
      "Epoch: 34 | Batch_idx: 120 |  Loss_1: (0.2091) | Acc_1: (92.60%) (14342/15488)\n",
      "Epoch: 34 | Batch_idx: 130 |  Loss_1: (0.2117) | Acc_1: (92.55%) (15519/16768)\n",
      "Epoch: 34 | Batch_idx: 140 |  Loss_1: (0.2124) | Acc_1: (92.55%) (16704/18048)\n",
      "Epoch: 34 | Batch_idx: 150 |  Loss_1: (0.2125) | Acc_1: (92.53%) (17884/19328)\n",
      "Epoch: 34 | Batch_idx: 160 |  Loss_1: (0.2152) | Acc_1: (92.50%) (19062/20608)\n",
      "Epoch: 34 | Batch_idx: 170 |  Loss_1: (0.2151) | Acc_1: (92.54%) (20255/21888)\n",
      "Epoch: 34 | Batch_idx: 180 |  Loss_1: (0.2170) | Acc_1: (92.44%) (21417/23168)\n",
      "Epoch: 34 | Batch_idx: 190 |  Loss_1: (0.2177) | Acc_1: (92.46%) (22604/24448)\n",
      "Epoch: 34 | Batch_idx: 200 |  Loss_1: (0.2189) | Acc_1: (92.45%) (23785/25728)\n",
      "Epoch: 34 | Batch_idx: 210 |  Loss_1: (0.2192) | Acc_1: (92.47%) (24973/27008)\n",
      "Epoch: 34 | Batch_idx: 220 |  Loss_1: (0.2193) | Acc_1: (92.47%) (26158/28288)\n",
      "Epoch: 34 | Batch_idx: 230 |  Loss_1: (0.2193) | Acc_1: (92.44%) (27334/29568)\n",
      "Epoch: 34 | Batch_idx: 240 |  Loss_1: (0.2200) | Acc_1: (92.40%) (28505/30848)\n",
      "Epoch: 34 | Batch_idx: 250 |  Loss_1: (0.2208) | Acc_1: (92.38%) (29679/32128)\n",
      "Epoch: 34 | Batch_idx: 260 |  Loss_1: (0.2226) | Acc_1: (92.31%) (30839/33408)\n",
      "Epoch: 34 | Batch_idx: 270 |  Loss_1: (0.2242) | Acc_1: (92.25%) (31998/34688)\n",
      "Epoch: 34 | Batch_idx: 280 |  Loss_1: (0.2241) | Acc_1: (92.24%) (33176/35968)\n",
      "Epoch: 34 | Batch_idx: 290 |  Loss_1: (0.2245) | Acc_1: (92.22%) (34351/37248)\n",
      "Epoch: 34 | Batch_idx: 300 |  Loss_1: (0.2242) | Acc_1: (92.24%) (35538/38528)\n",
      "Epoch: 34 | Batch_idx: 310 |  Loss_1: (0.2242) | Acc_1: (92.24%) (36719/39808)\n",
      "Epoch: 34 | Batch_idx: 320 |  Loss_1: (0.2243) | Acc_1: (92.24%) (37901/41088)\n",
      "Epoch: 34 | Batch_idx: 330 |  Loss_1: (0.2235) | Acc_1: (92.27%) (39091/42368)\n",
      "Epoch: 34 | Batch_idx: 340 |  Loss_1: (0.2226) | Acc_1: (92.30%) (40286/43648)\n",
      "Epoch: 34 | Batch_idx: 350 |  Loss_1: (0.2227) | Acc_1: (92.29%) (41463/44928)\n",
      "Epoch: 34 | Batch_idx: 360 |  Loss_1: (0.2227) | Acc_1: (92.28%) (42641/46208)\n",
      "Epoch: 34 | Batch_idx: 370 |  Loss_1: (0.2235) | Acc_1: (92.25%) (43808/47488)\n",
      "Epoch: 34 | Batch_idx: 380 |  Loss_1: (0.2232) | Acc_1: (92.25%) (44988/48768)\n",
      "Epoch: 34 | Batch_idx: 390 |  Loss_1: (0.2247) | Acc_1: (92.20%) (46100/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3773) | Acc: (88.68%) (8868/10000)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.2295) | Acc_1: (92.19%) (118/128)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.2089) | Acc_1: (92.05%) (1296/1408)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.2202) | Acc_1: (91.85%) (2469/2688)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.2207) | Acc_1: (91.94%) (3648/3968)\n",
      "Epoch: 35 | Batch_idx: 40 |  Loss_1: (0.2053) | Acc_1: (92.59%) (4859/5248)\n",
      "Epoch: 35 | Batch_idx: 50 |  Loss_1: (0.1999) | Acc_1: (92.88%) (6063/6528)\n",
      "Epoch: 35 | Batch_idx: 60 |  Loss_1: (0.2021) | Acc_1: (92.76%) (7243/7808)\n",
      "Epoch: 35 | Batch_idx: 70 |  Loss_1: (0.2020) | Acc_1: (92.75%) (8429/9088)\n",
      "Epoch: 35 | Batch_idx: 80 |  Loss_1: (0.1983) | Acc_1: (92.84%) (9626/10368)\n",
      "Epoch: 35 | Batch_idx: 90 |  Loss_1: (0.1967) | Acc_1: (92.92%) (10823/11648)\n",
      "Epoch: 35 | Batch_idx: 100 |  Loss_1: (0.1977) | Acc_1: (92.88%) (12007/12928)\n",
      "Epoch: 35 | Batch_idx: 110 |  Loss_1: (0.1995) | Acc_1: (92.81%) (13186/14208)\n",
      "Epoch: 35 | Batch_idx: 120 |  Loss_1: (0.1992) | Acc_1: (92.85%) (14380/15488)\n",
      "Epoch: 35 | Batch_idx: 130 |  Loss_1: (0.2012) | Acc_1: (92.81%) (15562/16768)\n",
      "Epoch: 35 | Batch_idx: 140 |  Loss_1: (0.2018) | Acc_1: (92.82%) (16753/18048)\n",
      "Epoch: 35 | Batch_idx: 150 |  Loss_1: (0.2022) | Acc_1: (92.77%) (17931/19328)\n",
      "Epoch: 35 | Batch_idx: 160 |  Loss_1: (0.2025) | Acc_1: (92.81%) (19126/20608)\n",
      "Epoch: 35 | Batch_idx: 170 |  Loss_1: (0.2021) | Acc_1: (92.80%) (20311/21888)\n",
      "Epoch: 35 | Batch_idx: 180 |  Loss_1: (0.2026) | Acc_1: (92.78%) (21496/23168)\n",
      "Epoch: 35 | Batch_idx: 190 |  Loss_1: (0.2043) | Acc_1: (92.71%) (22665/24448)\n",
      "Epoch: 35 | Batch_idx: 200 |  Loss_1: (0.2051) | Acc_1: (92.69%) (23847/25728)\n",
      "Epoch: 35 | Batch_idx: 210 |  Loss_1: (0.2064) | Acc_1: (92.58%) (25005/27008)\n",
      "Epoch: 35 | Batch_idx: 220 |  Loss_1: (0.2073) | Acc_1: (92.54%) (26178/28288)\n",
      "Epoch: 35 | Batch_idx: 230 |  Loss_1: (0.2075) | Acc_1: (92.54%) (27362/29568)\n",
      "Epoch: 35 | Batch_idx: 240 |  Loss_1: (0.2083) | Acc_1: (92.50%) (28534/30848)\n",
      "Epoch: 35 | Batch_idx: 250 |  Loss_1: (0.2085) | Acc_1: (92.50%) (29720/32128)\n",
      "Epoch: 35 | Batch_idx: 260 |  Loss_1: (0.2080) | Acc_1: (92.53%) (30911/33408)\n",
      "Epoch: 35 | Batch_idx: 270 |  Loss_1: (0.2075) | Acc_1: (92.54%) (32102/34688)\n",
      "Epoch: 35 | Batch_idx: 280 |  Loss_1: (0.2083) | Acc_1: (92.53%) (33281/35968)\n",
      "Epoch: 35 | Batch_idx: 290 |  Loss_1: (0.2079) | Acc_1: (92.56%) (34475/37248)\n",
      "Epoch: 35 | Batch_idx: 300 |  Loss_1: (0.2090) | Acc_1: (92.55%) (35656/38528)\n",
      "Epoch: 35 | Batch_idx: 310 |  Loss_1: (0.2098) | Acc_1: (92.52%) (36831/39808)\n",
      "Epoch: 35 | Batch_idx: 320 |  Loss_1: (0.2097) | Acc_1: (92.52%) (38014/41088)\n",
      "Epoch: 35 | Batch_idx: 330 |  Loss_1: (0.2102) | Acc_1: (92.50%) (39192/42368)\n",
      "Epoch: 35 | Batch_idx: 340 |  Loss_1: (0.2100) | Acc_1: (92.52%) (40385/43648)\n",
      "Epoch: 35 | Batch_idx: 350 |  Loss_1: (0.2103) | Acc_1: (92.50%) (41560/44928)\n",
      "Epoch: 35 | Batch_idx: 360 |  Loss_1: (0.2105) | Acc_1: (92.49%) (42739/46208)\n",
      "Epoch: 35 | Batch_idx: 370 |  Loss_1: (0.2099) | Acc_1: (92.51%) (43932/47488)\n",
      "Epoch: 35 | Batch_idx: 380 |  Loss_1: (0.2103) | Acc_1: (92.51%) (45117/48768)\n",
      "Epoch: 35 | Batch_idx: 390 |  Loss_1: (0.2105) | Acc_1: (92.51%) (46256/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3857) | Acc: (88.78%) (8878/10000)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.2234) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.1883) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.1975) | Acc_1: (92.75%) (2493/2688)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.1950) | Acc_1: (93.07%) (3693/3968)\n",
      "Epoch: 36 | Batch_idx: 40 |  Loss_1: (0.1879) | Acc_1: (93.37%) (4900/5248)\n",
      "Epoch: 36 | Batch_idx: 50 |  Loss_1: (0.1912) | Acc_1: (93.28%) (6089/6528)\n",
      "Epoch: 36 | Batch_idx: 60 |  Loss_1: (0.1941) | Acc_1: (93.19%) (7276/7808)\n",
      "Epoch: 36 | Batch_idx: 70 |  Loss_1: (0.1940) | Acc_1: (93.20%) (8470/9088)\n",
      "Epoch: 36 | Batch_idx: 80 |  Loss_1: (0.1956) | Acc_1: (93.16%) (9659/10368)\n",
      "Epoch: 36 | Batch_idx: 90 |  Loss_1: (0.1958) | Acc_1: (93.07%) (10841/11648)\n",
      "Epoch: 36 | Batch_idx: 100 |  Loss_1: (0.1980) | Acc_1: (92.99%) (12022/12928)\n",
      "Epoch: 36 | Batch_idx: 110 |  Loss_1: (0.1954) | Acc_1: (93.14%) (13233/14208)\n",
      "Epoch: 36 | Batch_idx: 120 |  Loss_1: (0.1939) | Acc_1: (93.15%) (14427/15488)\n",
      "Epoch: 36 | Batch_idx: 130 |  Loss_1: (0.1940) | Acc_1: (93.13%) (15616/16768)\n",
      "Epoch: 36 | Batch_idx: 140 |  Loss_1: (0.1955) | Acc_1: (93.09%) (16800/18048)\n",
      "Epoch: 36 | Batch_idx: 150 |  Loss_1: (0.1968) | Acc_1: (93.05%) (17984/19328)\n",
      "Epoch: 36 | Batch_idx: 160 |  Loss_1: (0.1956) | Acc_1: (93.05%) (19176/20608)\n",
      "Epoch: 36 | Batch_idx: 170 |  Loss_1: (0.1981) | Acc_1: (93.00%) (20356/21888)\n",
      "Epoch: 36 | Batch_idx: 180 |  Loss_1: (0.1976) | Acc_1: (93.01%) (21549/23168)\n",
      "Epoch: 36 | Batch_idx: 190 |  Loss_1: (0.1992) | Acc_1: (92.96%) (22726/24448)\n",
      "Epoch: 36 | Batch_idx: 200 |  Loss_1: (0.1980) | Acc_1: (92.95%) (23915/25728)\n",
      "Epoch: 36 | Batch_idx: 210 |  Loss_1: (0.1985) | Acc_1: (92.96%) (25107/27008)\n",
      "Epoch: 36 | Batch_idx: 220 |  Loss_1: (0.1986) | Acc_1: (92.97%) (26300/28288)\n",
      "Epoch: 36 | Batch_idx: 230 |  Loss_1: (0.1982) | Acc_1: (93.00%) (27498/29568)\n",
      "Epoch: 36 | Batch_idx: 240 |  Loss_1: (0.1978) | Acc_1: (93.00%) (28689/30848)\n",
      "Epoch: 36 | Batch_idx: 250 |  Loss_1: (0.1975) | Acc_1: (92.99%) (29876/32128)\n",
      "Epoch: 36 | Batch_idx: 260 |  Loss_1: (0.1977) | Acc_1: (92.99%) (31066/33408)\n",
      "Epoch: 36 | Batch_idx: 270 |  Loss_1: (0.1980) | Acc_1: (93.00%) (32259/34688)\n",
      "Epoch: 36 | Batch_idx: 280 |  Loss_1: (0.1996) | Acc_1: (92.95%) (33434/35968)\n",
      "Epoch: 36 | Batch_idx: 290 |  Loss_1: (0.1994) | Acc_1: (92.96%) (34627/37248)\n",
      "Epoch: 36 | Batch_idx: 300 |  Loss_1: (0.2001) | Acc_1: (92.96%) (35814/38528)\n",
      "Epoch: 36 | Batch_idx: 310 |  Loss_1: (0.2002) | Acc_1: (92.95%) (37002/39808)\n",
      "Epoch: 36 | Batch_idx: 320 |  Loss_1: (0.2007) | Acc_1: (92.93%) (38182/41088)\n",
      "Epoch: 36 | Batch_idx: 330 |  Loss_1: (0.2017) | Acc_1: (92.90%) (39359/42368)\n",
      "Epoch: 36 | Batch_idx: 340 |  Loss_1: (0.2011) | Acc_1: (92.93%) (40561/43648)\n",
      "Epoch: 36 | Batch_idx: 350 |  Loss_1: (0.2022) | Acc_1: (92.89%) (41733/44928)\n",
      "Epoch: 36 | Batch_idx: 360 |  Loss_1: (0.2018) | Acc_1: (92.89%) (42922/46208)\n",
      "Epoch: 36 | Batch_idx: 370 |  Loss_1: (0.2025) | Acc_1: (92.85%) (44093/47488)\n",
      "Epoch: 36 | Batch_idx: 380 |  Loss_1: (0.2033) | Acc_1: (92.84%) (45277/48768)\n",
      "Epoch: 36 | Batch_idx: 390 |  Loss_1: (0.2033) | Acc_1: (92.84%) (46422/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4254) | Acc: (88.12%) (8812/10000)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.2628) | Acc_1: (90.62%) (116/128)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.2070) | Acc_1: (92.68%) (1305/1408)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.1956) | Acc_1: (93.04%) (2501/2688)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.1906) | Acc_1: (93.30%) (3702/3968)\n",
      "Epoch: 37 | Batch_idx: 40 |  Loss_1: (0.1946) | Acc_1: (93.16%) (4889/5248)\n",
      "Epoch: 37 | Batch_idx: 50 |  Loss_1: (0.1961) | Acc_1: (93.09%) (6077/6528)\n",
      "Epoch: 37 | Batch_idx: 60 |  Loss_1: (0.1958) | Acc_1: (92.93%) (7256/7808)\n",
      "Epoch: 37 | Batch_idx: 70 |  Loss_1: (0.1953) | Acc_1: (93.00%) (8452/9088)\n",
      "Epoch: 37 | Batch_idx: 80 |  Loss_1: (0.1944) | Acc_1: (92.99%) (9641/10368)\n",
      "Epoch: 37 | Batch_idx: 90 |  Loss_1: (0.1940) | Acc_1: (93.02%) (10835/11648)\n",
      "Epoch: 37 | Batch_idx: 100 |  Loss_1: (0.1933) | Acc_1: (93.01%) (12024/12928)\n",
      "Epoch: 37 | Batch_idx: 110 |  Loss_1: (0.1919) | Acc_1: (93.09%) (13226/14208)\n",
      "Epoch: 37 | Batch_idx: 120 |  Loss_1: (0.1937) | Acc_1: (93.07%) (14414/15488)\n",
      "Epoch: 37 | Batch_idx: 130 |  Loss_1: (0.1920) | Acc_1: (93.17%) (15623/16768)\n",
      "Epoch: 37 | Batch_idx: 140 |  Loss_1: (0.1924) | Acc_1: (93.18%) (16818/18048)\n",
      "Epoch: 37 | Batch_idx: 150 |  Loss_1: (0.1901) | Acc_1: (93.27%) (18027/19328)\n",
      "Epoch: 37 | Batch_idx: 160 |  Loss_1: (0.1889) | Acc_1: (93.31%) (19229/20608)\n",
      "Epoch: 37 | Batch_idx: 170 |  Loss_1: (0.1872) | Acc_1: (93.43%) (20450/21888)\n",
      "Epoch: 37 | Batch_idx: 180 |  Loss_1: (0.1887) | Acc_1: (93.40%) (21640/23168)\n",
      "Epoch: 37 | Batch_idx: 190 |  Loss_1: (0.1883) | Acc_1: (93.37%) (22828/24448)\n",
      "Epoch: 37 | Batch_idx: 200 |  Loss_1: (0.1900) | Acc_1: (93.31%) (24008/25728)\n",
      "Epoch: 37 | Batch_idx: 210 |  Loss_1: (0.1900) | Acc_1: (93.30%) (25198/27008)\n",
      "Epoch: 37 | Batch_idx: 220 |  Loss_1: (0.1893) | Acc_1: (93.32%) (26397/28288)\n",
      "Epoch: 37 | Batch_idx: 230 |  Loss_1: (0.1891) | Acc_1: (93.31%) (27589/29568)\n",
      "Epoch: 37 | Batch_idx: 240 |  Loss_1: (0.1894) | Acc_1: (93.31%) (28783/30848)\n",
      "Epoch: 37 | Batch_idx: 250 |  Loss_1: (0.1920) | Acc_1: (93.22%) (29950/32128)\n",
      "Epoch: 37 | Batch_idx: 260 |  Loss_1: (0.1922) | Acc_1: (93.21%) (31140/33408)\n",
      "Epoch: 37 | Batch_idx: 270 |  Loss_1: (0.1928) | Acc_1: (93.20%) (32328/34688)\n",
      "Epoch: 37 | Batch_idx: 280 |  Loss_1: (0.1916) | Acc_1: (93.26%) (33542/35968)\n",
      "Epoch: 37 | Batch_idx: 290 |  Loss_1: (0.1916) | Acc_1: (93.26%) (34736/37248)\n",
      "Epoch: 37 | Batch_idx: 300 |  Loss_1: (0.1926) | Acc_1: (93.22%) (35914/38528)\n",
      "Epoch: 37 | Batch_idx: 310 |  Loss_1: (0.1925) | Acc_1: (93.22%) (37111/39808)\n",
      "Epoch: 37 | Batch_idx: 320 |  Loss_1: (0.1928) | Acc_1: (93.24%) (38310/41088)\n",
      "Epoch: 37 | Batch_idx: 330 |  Loss_1: (0.1942) | Acc_1: (93.20%) (39485/42368)\n",
      "Epoch: 37 | Batch_idx: 340 |  Loss_1: (0.1950) | Acc_1: (93.15%) (40659/43648)\n",
      "Epoch: 37 | Batch_idx: 350 |  Loss_1: (0.1946) | Acc_1: (93.17%) (41859/44928)\n",
      "Epoch: 37 | Batch_idx: 360 |  Loss_1: (0.1950) | Acc_1: (93.15%) (43042/46208)\n",
      "Epoch: 37 | Batch_idx: 370 |  Loss_1: (0.1954) | Acc_1: (93.11%) (44217/47488)\n",
      "Epoch: 37 | Batch_idx: 380 |  Loss_1: (0.1953) | Acc_1: (93.12%) (45411/48768)\n",
      "Epoch: 37 | Batch_idx: 390 |  Loss_1: (0.1959) | Acc_1: (93.11%) (46553/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4106) | Acc: (88.47%) (8847/10000)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.1855) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.1817) | Acc_1: (93.18%) (1312/1408)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.1766) | Acc_1: (93.94%) (2525/2688)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.1701) | Acc_1: (94.20%) (3738/3968)\n",
      "Epoch: 38 | Batch_idx: 40 |  Loss_1: (0.1719) | Acc_1: (94.00%) (4933/5248)\n",
      "Epoch: 38 | Batch_idx: 50 |  Loss_1: (0.1768) | Acc_1: (93.83%) (6125/6528)\n",
      "Epoch: 38 | Batch_idx: 60 |  Loss_1: (0.1818) | Acc_1: (93.56%) (7305/7808)\n",
      "Epoch: 38 | Batch_idx: 70 |  Loss_1: (0.1848) | Acc_1: (93.42%) (8490/9088)\n",
      "Epoch: 38 | Batch_idx: 80 |  Loss_1: (0.1872) | Acc_1: (93.34%) (9677/10368)\n",
      "Epoch: 38 | Batch_idx: 90 |  Loss_1: (0.1895) | Acc_1: (93.34%) (10872/11648)\n",
      "Epoch: 38 | Batch_idx: 100 |  Loss_1: (0.1911) | Acc_1: (93.34%) (12067/12928)\n",
      "Epoch: 38 | Batch_idx: 110 |  Loss_1: (0.1930) | Acc_1: (93.33%) (13260/14208)\n",
      "Epoch: 38 | Batch_idx: 120 |  Loss_1: (0.1924) | Acc_1: (93.29%) (14449/15488)\n",
      "Epoch: 38 | Batch_idx: 130 |  Loss_1: (0.1934) | Acc_1: (93.28%) (15641/16768)\n",
      "Epoch: 38 | Batch_idx: 140 |  Loss_1: (0.1932) | Acc_1: (93.28%) (16835/18048)\n",
      "Epoch: 38 | Batch_idx: 150 |  Loss_1: (0.1942) | Acc_1: (93.22%) (18018/19328)\n",
      "Epoch: 38 | Batch_idx: 160 |  Loss_1: (0.1930) | Acc_1: (93.26%) (19220/20608)\n",
      "Epoch: 38 | Batch_idx: 170 |  Loss_1: (0.1926) | Acc_1: (93.24%) (20409/21888)\n",
      "Epoch: 38 | Batch_idx: 180 |  Loss_1: (0.1944) | Acc_1: (93.21%) (21595/23168)\n",
      "Epoch: 38 | Batch_idx: 190 |  Loss_1: (0.1936) | Acc_1: (93.26%) (22800/24448)\n",
      "Epoch: 38 | Batch_idx: 200 |  Loss_1: (0.1941) | Acc_1: (93.24%) (23990/25728)\n",
      "Epoch: 38 | Batch_idx: 210 |  Loss_1: (0.1948) | Acc_1: (93.25%) (25184/27008)\n",
      "Epoch: 38 | Batch_idx: 220 |  Loss_1: (0.1950) | Acc_1: (93.22%) (26371/28288)\n",
      "Epoch: 38 | Batch_idx: 230 |  Loss_1: (0.1951) | Acc_1: (93.23%) (27565/29568)\n",
      "Epoch: 38 | Batch_idx: 240 |  Loss_1: (0.1955) | Acc_1: (93.21%) (28754/30848)\n",
      "Epoch: 38 | Batch_idx: 250 |  Loss_1: (0.1951) | Acc_1: (93.25%) (29959/32128)\n",
      "Epoch: 38 | Batch_idx: 260 |  Loss_1: (0.1957) | Acc_1: (93.24%) (31150/33408)\n",
      "Epoch: 38 | Batch_idx: 270 |  Loss_1: (0.1948) | Acc_1: (93.29%) (32360/34688)\n",
      "Epoch: 38 | Batch_idx: 280 |  Loss_1: (0.1950) | Acc_1: (93.27%) (33547/35968)\n",
      "Epoch: 38 | Batch_idx: 290 |  Loss_1: (0.1942) | Acc_1: (93.29%) (34748/37248)\n",
      "Epoch: 38 | Batch_idx: 300 |  Loss_1: (0.1946) | Acc_1: (93.25%) (35929/38528)\n",
      "Epoch: 38 | Batch_idx: 310 |  Loss_1: (0.1946) | Acc_1: (93.27%) (37128/39808)\n",
      "Epoch: 38 | Batch_idx: 320 |  Loss_1: (0.1955) | Acc_1: (93.22%) (38304/41088)\n",
      "Epoch: 38 | Batch_idx: 330 |  Loss_1: (0.1961) | Acc_1: (93.20%) (39487/42368)\n",
      "Epoch: 38 | Batch_idx: 340 |  Loss_1: (0.1961) | Acc_1: (93.18%) (40672/43648)\n",
      "Epoch: 38 | Batch_idx: 350 |  Loss_1: (0.1969) | Acc_1: (93.14%) (41846/44928)\n",
      "Epoch: 38 | Batch_idx: 360 |  Loss_1: (0.1967) | Acc_1: (93.14%) (43037/46208)\n",
      "Epoch: 38 | Batch_idx: 370 |  Loss_1: (0.1960) | Acc_1: (93.17%) (44246/47488)\n",
      "Epoch: 38 | Batch_idx: 380 |  Loss_1: (0.1958) | Acc_1: (93.17%) (45437/48768)\n",
      "Epoch: 38 | Batch_idx: 390 |  Loss_1: (0.1955) | Acc_1: (93.17%) (46584/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4070) | Acc: (88.49%) (8849/10000)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.2037) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.1726) | Acc_1: (94.46%) (1330/1408)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.1545) | Acc_1: (94.83%) (2549/2688)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.1551) | Acc_1: (94.51%) (3750/3968)\n",
      "Epoch: 39 | Batch_idx: 40 |  Loss_1: (0.1576) | Acc_1: (94.49%) (4959/5248)\n",
      "Epoch: 39 | Batch_idx: 50 |  Loss_1: (0.1614) | Acc_1: (94.35%) (6159/6528)\n",
      "Epoch: 39 | Batch_idx: 60 |  Loss_1: (0.1647) | Acc_1: (94.13%) (7350/7808)\n",
      "Epoch: 39 | Batch_idx: 70 |  Loss_1: (0.1647) | Acc_1: (94.17%) (8558/9088)\n",
      "Epoch: 39 | Batch_idx: 80 |  Loss_1: (0.1684) | Acc_1: (93.98%) (9744/10368)\n",
      "Epoch: 39 | Batch_idx: 90 |  Loss_1: (0.1711) | Acc_1: (93.90%) (10938/11648)\n",
      "Epoch: 39 | Batch_idx: 100 |  Loss_1: (0.1719) | Acc_1: (93.89%) (12138/12928)\n",
      "Epoch: 39 | Batch_idx: 110 |  Loss_1: (0.1747) | Acc_1: (93.81%) (13329/14208)\n",
      "Epoch: 39 | Batch_idx: 120 |  Loss_1: (0.1731) | Acc_1: (93.86%) (14537/15488)\n",
      "Epoch: 39 | Batch_idx: 130 |  Loss_1: (0.1756) | Acc_1: (93.76%) (15722/16768)\n",
      "Epoch: 39 | Batch_idx: 140 |  Loss_1: (0.1748) | Acc_1: (93.77%) (16923/18048)\n",
      "Epoch: 39 | Batch_idx: 150 |  Loss_1: (0.1747) | Acc_1: (93.69%) (18109/19328)\n",
      "Epoch: 39 | Batch_idx: 160 |  Loss_1: (0.1759) | Acc_1: (93.66%) (19301/20608)\n",
      "Epoch: 39 | Batch_idx: 170 |  Loss_1: (0.1779) | Acc_1: (93.58%) (20483/21888)\n",
      "Epoch: 39 | Batch_idx: 180 |  Loss_1: (0.1776) | Acc_1: (93.59%) (21683/23168)\n",
      "Epoch: 39 | Batch_idx: 190 |  Loss_1: (0.1789) | Acc_1: (93.55%) (22870/24448)\n",
      "Epoch: 39 | Batch_idx: 200 |  Loss_1: (0.1802) | Acc_1: (93.54%) (24066/25728)\n",
      "Epoch: 39 | Batch_idx: 210 |  Loss_1: (0.1807) | Acc_1: (93.52%) (25259/27008)\n",
      "Epoch: 39 | Batch_idx: 220 |  Loss_1: (0.1805) | Acc_1: (93.52%) (26456/28288)\n",
      "Epoch: 39 | Batch_idx: 230 |  Loss_1: (0.1817) | Acc_1: (93.50%) (27646/29568)\n",
      "Epoch: 39 | Batch_idx: 240 |  Loss_1: (0.1826) | Acc_1: (93.50%) (28843/30848)\n",
      "Epoch: 39 | Batch_idx: 250 |  Loss_1: (0.1827) | Acc_1: (93.50%) (30039/32128)\n",
      "Epoch: 39 | Batch_idx: 260 |  Loss_1: (0.1818) | Acc_1: (93.53%) (31245/33408)\n",
      "Epoch: 39 | Batch_idx: 270 |  Loss_1: (0.1822) | Acc_1: (93.55%) (32451/34688)\n",
      "Epoch: 39 | Batch_idx: 280 |  Loss_1: (0.1822) | Acc_1: (93.56%) (33651/35968)\n",
      "Epoch: 39 | Batch_idx: 290 |  Loss_1: (0.1826) | Acc_1: (93.52%) (34836/37248)\n",
      "Epoch: 39 | Batch_idx: 300 |  Loss_1: (0.1830) | Acc_1: (93.51%) (36028/38528)\n",
      "Epoch: 39 | Batch_idx: 310 |  Loss_1: (0.1837) | Acc_1: (93.48%) (37214/39808)\n",
      "Epoch: 39 | Batch_idx: 320 |  Loss_1: (0.1834) | Acc_1: (93.47%) (38404/41088)\n",
      "Epoch: 39 | Batch_idx: 330 |  Loss_1: (0.1835) | Acc_1: (93.46%) (39598/42368)\n",
      "Epoch: 39 | Batch_idx: 340 |  Loss_1: (0.1837) | Acc_1: (93.46%) (40794/43648)\n",
      "Epoch: 39 | Batch_idx: 350 |  Loss_1: (0.1839) | Acc_1: (93.46%) (41988/44928)\n",
      "Epoch: 39 | Batch_idx: 360 |  Loss_1: (0.1844) | Acc_1: (93.45%) (43183/46208)\n",
      "Epoch: 39 | Batch_idx: 370 |  Loss_1: (0.1841) | Acc_1: (93.47%) (44387/47488)\n",
      "Epoch: 39 | Batch_idx: 380 |  Loss_1: (0.1854) | Acc_1: (93.43%) (45564/48768)\n",
      "Epoch: 39 | Batch_idx: 390 |  Loss_1: (0.1859) | Acc_1: (93.41%) (46707/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4210) | Acc: (88.46%) (8846/10000)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.0999) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.1657) | Acc_1: (93.39%) (1315/1408)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.1646) | Acc_1: (93.75%) (2520/2688)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.1635) | Acc_1: (93.95%) (3728/3968)\n",
      "Epoch: 40 | Batch_idx: 40 |  Loss_1: (0.1675) | Acc_1: (93.94%) (4930/5248)\n",
      "Epoch: 40 | Batch_idx: 50 |  Loss_1: (0.1636) | Acc_1: (94.15%) (6146/6528)\n",
      "Epoch: 40 | Batch_idx: 60 |  Loss_1: (0.1633) | Acc_1: (94.26%) (7360/7808)\n",
      "Epoch: 40 | Batch_idx: 70 |  Loss_1: (0.1634) | Acc_1: (94.34%) (8574/9088)\n",
      "Epoch: 40 | Batch_idx: 80 |  Loss_1: (0.1633) | Acc_1: (94.27%) (9774/10368)\n",
      "Epoch: 40 | Batch_idx: 90 |  Loss_1: (0.1688) | Acc_1: (94.05%) (10955/11648)\n",
      "Epoch: 40 | Batch_idx: 100 |  Loss_1: (0.1699) | Acc_1: (93.98%) (12150/12928)\n",
      "Epoch: 40 | Batch_idx: 110 |  Loss_1: (0.1693) | Acc_1: (94.00%) (13355/14208)\n",
      "Epoch: 40 | Batch_idx: 120 |  Loss_1: (0.1695) | Acc_1: (93.97%) (14554/15488)\n",
      "Epoch: 40 | Batch_idx: 130 |  Loss_1: (0.1708) | Acc_1: (93.91%) (15747/16768)\n",
      "Epoch: 40 | Batch_idx: 140 |  Loss_1: (0.1713) | Acc_1: (93.91%) (16948/18048)\n",
      "Epoch: 40 | Batch_idx: 150 |  Loss_1: (0.1727) | Acc_1: (93.84%) (18138/19328)\n",
      "Epoch: 40 | Batch_idx: 160 |  Loss_1: (0.1747) | Acc_1: (93.75%) (19321/20608)\n",
      "Epoch: 40 | Batch_idx: 170 |  Loss_1: (0.1779) | Acc_1: (93.65%) (20498/21888)\n",
      "Epoch: 40 | Batch_idx: 180 |  Loss_1: (0.1759) | Acc_1: (93.73%) (21715/23168)\n",
      "Epoch: 40 | Batch_idx: 190 |  Loss_1: (0.1762) | Acc_1: (93.73%) (22914/24448)\n",
      "Epoch: 40 | Batch_idx: 200 |  Loss_1: (0.1766) | Acc_1: (93.70%) (24107/25728)\n",
      "Epoch: 40 | Batch_idx: 210 |  Loss_1: (0.1766) | Acc_1: (93.67%) (25298/27008)\n",
      "Epoch: 40 | Batch_idx: 220 |  Loss_1: (0.1769) | Acc_1: (93.64%) (26490/28288)\n",
      "Epoch: 40 | Batch_idx: 230 |  Loss_1: (0.1784) | Acc_1: (93.60%) (27675/29568)\n",
      "Epoch: 40 | Batch_idx: 240 |  Loss_1: (0.1799) | Acc_1: (93.58%) (28869/30848)\n",
      "Epoch: 40 | Batch_idx: 250 |  Loss_1: (0.1807) | Acc_1: (93.57%) (30061/32128)\n",
      "Epoch: 40 | Batch_idx: 260 |  Loss_1: (0.1801) | Acc_1: (93.59%) (31267/33408)\n",
      "Epoch: 40 | Batch_idx: 270 |  Loss_1: (0.1798) | Acc_1: (93.61%) (32472/34688)\n",
      "Epoch: 40 | Batch_idx: 280 |  Loss_1: (0.1800) | Acc_1: (93.60%) (33667/35968)\n",
      "Epoch: 40 | Batch_idx: 290 |  Loss_1: (0.1802) | Acc_1: (93.59%) (34861/37248)\n",
      "Epoch: 40 | Batch_idx: 300 |  Loss_1: (0.1801) | Acc_1: (93.61%) (36065/38528)\n",
      "Epoch: 40 | Batch_idx: 310 |  Loss_1: (0.1808) | Acc_1: (93.60%) (37261/39808)\n",
      "Epoch: 40 | Batch_idx: 320 |  Loss_1: (0.1809) | Acc_1: (93.61%) (38464/41088)\n",
      "Epoch: 40 | Batch_idx: 330 |  Loss_1: (0.1814) | Acc_1: (93.59%) (39651/42368)\n",
      "Epoch: 40 | Batch_idx: 340 |  Loss_1: (0.1813) | Acc_1: (93.60%) (40856/43648)\n",
      "Epoch: 40 | Batch_idx: 350 |  Loss_1: (0.1817) | Acc_1: (93.58%) (42043/44928)\n",
      "Epoch: 40 | Batch_idx: 360 |  Loss_1: (0.1819) | Acc_1: (93.55%) (43229/46208)\n",
      "Epoch: 40 | Batch_idx: 370 |  Loss_1: (0.1816) | Acc_1: (93.55%) (44423/47488)\n",
      "Epoch: 40 | Batch_idx: 380 |  Loss_1: (0.1819) | Acc_1: (93.53%) (45614/48768)\n",
      "Epoch: 40 | Batch_idx: 390 |  Loss_1: (0.1822) | Acc_1: (93.54%) (46768/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3869) | Acc: (88.91%) (8891/10000)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.1741) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.1759) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.1785) | Acc_1: (94.53%) (2541/2688)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.1818) | Acc_1: (94.15%) (3736/3968)\n",
      "Epoch: 41 | Batch_idx: 40 |  Loss_1: (0.1749) | Acc_1: (94.21%) (4944/5248)\n",
      "Epoch: 41 | Batch_idx: 50 |  Loss_1: (0.1723) | Acc_1: (94.26%) (6153/6528)\n",
      "Epoch: 41 | Batch_idx: 60 |  Loss_1: (0.1741) | Acc_1: (94.08%) (7346/7808)\n",
      "Epoch: 41 | Batch_idx: 70 |  Loss_1: (0.1735) | Acc_1: (93.98%) (8541/9088)\n",
      "Epoch: 41 | Batch_idx: 80 |  Loss_1: (0.1749) | Acc_1: (93.98%) (9744/10368)\n",
      "Epoch: 41 | Batch_idx: 90 |  Loss_1: (0.1723) | Acc_1: (94.05%) (10955/11648)\n",
      "Epoch: 41 | Batch_idx: 100 |  Loss_1: (0.1693) | Acc_1: (94.14%) (12171/12928)\n",
      "Epoch: 41 | Batch_idx: 110 |  Loss_1: (0.1705) | Acc_1: (94.11%) (13371/14208)\n",
      "Epoch: 41 | Batch_idx: 120 |  Loss_1: (0.1689) | Acc_1: (94.14%) (14581/15488)\n",
      "Epoch: 41 | Batch_idx: 130 |  Loss_1: (0.1688) | Acc_1: (94.17%) (15790/16768)\n",
      "Epoch: 41 | Batch_idx: 140 |  Loss_1: (0.1672) | Acc_1: (94.21%) (17003/18048)\n",
      "Epoch: 41 | Batch_idx: 150 |  Loss_1: (0.1679) | Acc_1: (94.13%) (18193/19328)\n",
      "Epoch: 41 | Batch_idx: 160 |  Loss_1: (0.1673) | Acc_1: (94.15%) (19403/20608)\n",
      "Epoch: 41 | Batch_idx: 170 |  Loss_1: (0.1689) | Acc_1: (94.10%) (20596/21888)\n",
      "Epoch: 41 | Batch_idx: 180 |  Loss_1: (0.1696) | Acc_1: (94.05%) (21789/23168)\n",
      "Epoch: 41 | Batch_idx: 190 |  Loss_1: (0.1710) | Acc_1: (93.99%) (22979/24448)\n",
      "Epoch: 41 | Batch_idx: 200 |  Loss_1: (0.1716) | Acc_1: (93.99%) (24182/25728)\n",
      "Epoch: 41 | Batch_idx: 210 |  Loss_1: (0.1721) | Acc_1: (93.96%) (25377/27008)\n",
      "Epoch: 41 | Batch_idx: 220 |  Loss_1: (0.1711) | Acc_1: (93.97%) (26583/28288)\n",
      "Epoch: 41 | Batch_idx: 230 |  Loss_1: (0.1702) | Acc_1: (93.99%) (27791/29568)\n",
      "Epoch: 41 | Batch_idx: 240 |  Loss_1: (0.1710) | Acc_1: (93.97%) (28989/30848)\n",
      "Epoch: 41 | Batch_idx: 250 |  Loss_1: (0.1711) | Acc_1: (93.95%) (30185/32128)\n",
      "Epoch: 41 | Batch_idx: 260 |  Loss_1: (0.1704) | Acc_1: (93.99%) (31401/33408)\n",
      "Epoch: 41 | Batch_idx: 270 |  Loss_1: (0.1698) | Acc_1: (94.02%) (32612/34688)\n",
      "Epoch: 41 | Batch_idx: 280 |  Loss_1: (0.1703) | Acc_1: (93.99%) (33808/35968)\n",
      "Epoch: 41 | Batch_idx: 290 |  Loss_1: (0.1709) | Acc_1: (94.00%) (35012/37248)\n",
      "Epoch: 41 | Batch_idx: 300 |  Loss_1: (0.1714) | Acc_1: (93.99%) (36212/38528)\n",
      "Epoch: 41 | Batch_idx: 310 |  Loss_1: (0.1712) | Acc_1: (93.98%) (37413/39808)\n",
      "Epoch: 41 | Batch_idx: 320 |  Loss_1: (0.1707) | Acc_1: (93.98%) (38615/41088)\n",
      "Epoch: 41 | Batch_idx: 330 |  Loss_1: (0.1710) | Acc_1: (93.97%) (39814/42368)\n",
      "Epoch: 41 | Batch_idx: 340 |  Loss_1: (0.1712) | Acc_1: (93.96%) (41012/43648)\n",
      "Epoch: 41 | Batch_idx: 350 |  Loss_1: (0.1713) | Acc_1: (93.94%) (42207/44928)\n",
      "Epoch: 41 | Batch_idx: 360 |  Loss_1: (0.1707) | Acc_1: (93.96%) (43418/46208)\n",
      "Epoch: 41 | Batch_idx: 370 |  Loss_1: (0.1705) | Acc_1: (93.99%) (44636/47488)\n",
      "Epoch: 41 | Batch_idx: 380 |  Loss_1: (0.1709) | Acc_1: (93.99%) (45838/48768)\n",
      "Epoch: 41 | Batch_idx: 390 |  Loss_1: (0.1714) | Acc_1: (93.98%) (46992/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3975) | Acc: (89.07%) (8907/10000)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.1480) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.1604) | Acc_1: (93.47%) (1316/1408)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.1633) | Acc_1: (93.82%) (2522/2688)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.1650) | Acc_1: (94.00%) (3730/3968)\n",
      "Epoch: 42 | Batch_idx: 40 |  Loss_1: (0.1652) | Acc_1: (94.11%) (4939/5248)\n",
      "Epoch: 42 | Batch_idx: 50 |  Loss_1: (0.1629) | Acc_1: (94.10%) (6143/6528)\n",
      "Epoch: 42 | Batch_idx: 60 |  Loss_1: (0.1594) | Acc_1: (94.19%) (7354/7808)\n",
      "Epoch: 42 | Batch_idx: 70 |  Loss_1: (0.1616) | Acc_1: (94.17%) (8558/9088)\n",
      "Epoch: 42 | Batch_idx: 80 |  Loss_1: (0.1625) | Acc_1: (94.16%) (9762/10368)\n",
      "Epoch: 42 | Batch_idx: 90 |  Loss_1: (0.1606) | Acc_1: (94.21%) (10974/11648)\n",
      "Epoch: 42 | Batch_idx: 100 |  Loss_1: (0.1627) | Acc_1: (94.14%) (12171/12928)\n",
      "Epoch: 42 | Batch_idx: 110 |  Loss_1: (0.1660) | Acc_1: (94.03%) (13360/14208)\n",
      "Epoch: 42 | Batch_idx: 120 |  Loss_1: (0.1702) | Acc_1: (93.94%) (14549/15488)\n",
      "Epoch: 42 | Batch_idx: 130 |  Loss_1: (0.1703) | Acc_1: (93.97%) (15757/16768)\n",
      "Epoch: 42 | Batch_idx: 140 |  Loss_1: (0.1683) | Acc_1: (94.01%) (16967/18048)\n",
      "Epoch: 42 | Batch_idx: 150 |  Loss_1: (0.1691) | Acc_1: (93.95%) (18158/19328)\n",
      "Epoch: 42 | Batch_idx: 160 |  Loss_1: (0.1696) | Acc_1: (93.92%) (19355/20608)\n",
      "Epoch: 42 | Batch_idx: 170 |  Loss_1: (0.1687) | Acc_1: (93.96%) (20566/21888)\n",
      "Epoch: 42 | Batch_idx: 180 |  Loss_1: (0.1675) | Acc_1: (94.01%) (21780/23168)\n",
      "Epoch: 42 | Batch_idx: 190 |  Loss_1: (0.1683) | Acc_1: (93.99%) (22979/24448)\n",
      "Epoch: 42 | Batch_idx: 200 |  Loss_1: (0.1677) | Acc_1: (94.00%) (24184/25728)\n",
      "Epoch: 42 | Batch_idx: 210 |  Loss_1: (0.1676) | Acc_1: (94.02%) (25392/27008)\n",
      "Epoch: 42 | Batch_idx: 220 |  Loss_1: (0.1685) | Acc_1: (94.02%) (26597/28288)\n",
      "Epoch: 42 | Batch_idx: 230 |  Loss_1: (0.1679) | Acc_1: (94.07%) (27814/29568)\n",
      "Epoch: 42 | Batch_idx: 240 |  Loss_1: (0.1684) | Acc_1: (94.04%) (29010/30848)\n",
      "Epoch: 42 | Batch_idx: 250 |  Loss_1: (0.1691) | Acc_1: (94.04%) (30212/32128)\n",
      "Epoch: 42 | Batch_idx: 260 |  Loss_1: (0.1687) | Acc_1: (94.04%) (31418/33408)\n",
      "Epoch: 42 | Batch_idx: 270 |  Loss_1: (0.1683) | Acc_1: (94.07%) (32632/34688)\n",
      "Epoch: 42 | Batch_idx: 280 |  Loss_1: (0.1674) | Acc_1: (94.09%) (33844/35968)\n",
      "Epoch: 42 | Batch_idx: 290 |  Loss_1: (0.1681) | Acc_1: (94.08%) (35043/37248)\n",
      "Epoch: 42 | Batch_idx: 300 |  Loss_1: (0.1675) | Acc_1: (94.12%) (36262/38528)\n",
      "Epoch: 42 | Batch_idx: 310 |  Loss_1: (0.1675) | Acc_1: (94.12%) (37466/39808)\n",
      "Epoch: 42 | Batch_idx: 320 |  Loss_1: (0.1681) | Acc_1: (94.10%) (38662/41088)\n",
      "Epoch: 42 | Batch_idx: 330 |  Loss_1: (0.1688) | Acc_1: (94.06%) (39853/42368)\n",
      "Epoch: 42 | Batch_idx: 340 |  Loss_1: (0.1701) | Acc_1: (94.04%) (41046/43648)\n",
      "Epoch: 42 | Batch_idx: 350 |  Loss_1: (0.1706) | Acc_1: (94.01%) (42236/44928)\n",
      "Epoch: 42 | Batch_idx: 360 |  Loss_1: (0.1717) | Acc_1: (93.97%) (43423/46208)\n",
      "Epoch: 42 | Batch_idx: 370 |  Loss_1: (0.1734) | Acc_1: (93.91%) (44598/47488)\n",
      "Epoch: 42 | Batch_idx: 380 |  Loss_1: (0.1733) | Acc_1: (93.91%) (45797/48768)\n",
      "Epoch: 42 | Batch_idx: 390 |  Loss_1: (0.1731) | Acc_1: (93.91%) (46953/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3890) | Acc: (89.32%) (8932/10000)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.0806) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.1630) | Acc_1: (93.89%) (1322/1408)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.1549) | Acc_1: (94.31%) (2535/2688)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.1605) | Acc_1: (94.35%) (3744/3968)\n",
      "Epoch: 43 | Batch_idx: 40 |  Loss_1: (0.1577) | Acc_1: (94.40%) (4954/5248)\n",
      "Epoch: 43 | Batch_idx: 50 |  Loss_1: (0.1626) | Acc_1: (94.24%) (6152/6528)\n",
      "Epoch: 43 | Batch_idx: 60 |  Loss_1: (0.1632) | Acc_1: (94.20%) (7355/7808)\n",
      "Epoch: 43 | Batch_idx: 70 |  Loss_1: (0.1671) | Acc_1: (94.03%) (8545/9088)\n",
      "Epoch: 43 | Batch_idx: 80 |  Loss_1: (0.1653) | Acc_1: (94.09%) (9755/10368)\n",
      "Epoch: 43 | Batch_idx: 90 |  Loss_1: (0.1654) | Acc_1: (94.02%) (10951/11648)\n",
      "Epoch: 43 | Batch_idx: 100 |  Loss_1: (0.1610) | Acc_1: (94.16%) (12173/12928)\n",
      "Epoch: 43 | Batch_idx: 110 |  Loss_1: (0.1608) | Acc_1: (94.17%) (13380/14208)\n",
      "Epoch: 43 | Batch_idx: 120 |  Loss_1: (0.1598) | Acc_1: (94.18%) (14586/15488)\n",
      "Epoch: 43 | Batch_idx: 130 |  Loss_1: (0.1578) | Acc_1: (94.27%) (15808/16768)\n",
      "Epoch: 43 | Batch_idx: 140 |  Loss_1: (0.1579) | Acc_1: (94.29%) (17018/18048)\n",
      "Epoch: 43 | Batch_idx: 150 |  Loss_1: (0.1576) | Acc_1: (94.32%) (18230/19328)\n",
      "Epoch: 43 | Batch_idx: 160 |  Loss_1: (0.1593) | Acc_1: (94.32%) (19437/20608)\n",
      "Epoch: 43 | Batch_idx: 170 |  Loss_1: (0.1616) | Acc_1: (94.23%) (20624/21888)\n",
      "Epoch: 43 | Batch_idx: 180 |  Loss_1: (0.1607) | Acc_1: (94.25%) (21836/23168)\n",
      "Epoch: 43 | Batch_idx: 190 |  Loss_1: (0.1605) | Acc_1: (94.24%) (23040/24448)\n",
      "Epoch: 43 | Batch_idx: 200 |  Loss_1: (0.1606) | Acc_1: (94.23%) (24244/25728)\n",
      "Epoch: 43 | Batch_idx: 210 |  Loss_1: (0.1607) | Acc_1: (94.19%) (25439/27008)\n",
      "Epoch: 43 | Batch_idx: 220 |  Loss_1: (0.1607) | Acc_1: (94.16%) (26637/28288)\n",
      "Epoch: 43 | Batch_idx: 230 |  Loss_1: (0.1623) | Acc_1: (94.11%) (27825/29568)\n",
      "Epoch: 43 | Batch_idx: 240 |  Loss_1: (0.1635) | Acc_1: (94.09%) (29025/30848)\n",
      "Epoch: 43 | Batch_idx: 250 |  Loss_1: (0.1650) | Acc_1: (94.05%) (30217/32128)\n",
      "Epoch: 43 | Batch_idx: 260 |  Loss_1: (0.1652) | Acc_1: (94.05%) (31421/33408)\n",
      "Epoch: 43 | Batch_idx: 270 |  Loss_1: (0.1657) | Acc_1: (94.04%) (32622/34688)\n",
      "Epoch: 43 | Batch_idx: 280 |  Loss_1: (0.1672) | Acc_1: (94.00%) (33811/35968)\n",
      "Epoch: 43 | Batch_idx: 290 |  Loss_1: (0.1671) | Acc_1: (94.01%) (35017/37248)\n",
      "Epoch: 43 | Batch_idx: 300 |  Loss_1: (0.1669) | Acc_1: (94.02%) (36223/38528)\n",
      "Epoch: 43 | Batch_idx: 310 |  Loss_1: (0.1669) | Acc_1: (94.02%) (37427/39808)\n",
      "Epoch: 43 | Batch_idx: 320 |  Loss_1: (0.1669) | Acc_1: (94.03%) (38634/41088)\n",
      "Epoch: 43 | Batch_idx: 330 |  Loss_1: (0.1668) | Acc_1: (94.02%) (39836/42368)\n",
      "Epoch: 43 | Batch_idx: 340 |  Loss_1: (0.1673) | Acc_1: (94.01%) (41032/43648)\n",
      "Epoch: 43 | Batch_idx: 350 |  Loss_1: (0.1680) | Acc_1: (93.98%) (42223/44928)\n",
      "Epoch: 43 | Batch_idx: 360 |  Loss_1: (0.1677) | Acc_1: (93.98%) (43428/46208)\n",
      "Epoch: 43 | Batch_idx: 370 |  Loss_1: (0.1683) | Acc_1: (93.95%) (44616/47488)\n",
      "Epoch: 43 | Batch_idx: 380 |  Loss_1: (0.1690) | Acc_1: (93.92%) (45805/48768)\n",
      "Epoch: 43 | Batch_idx: 390 |  Loss_1: (0.1694) | Acc_1: (93.91%) (46953/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3871) | Acc: (89.19%) (8919/10000)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.1574) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.1612) | Acc_1: (94.03%) (1324/1408)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.1463) | Acc_1: (94.46%) (2539/2688)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.1522) | Acc_1: (94.10%) (3734/3968)\n",
      "Epoch: 44 | Batch_idx: 40 |  Loss_1: (0.1577) | Acc_1: (94.07%) (4937/5248)\n",
      "Epoch: 44 | Batch_idx: 50 |  Loss_1: (0.1578) | Acc_1: (94.10%) (6143/6528)\n",
      "Epoch: 44 | Batch_idx: 60 |  Loss_1: (0.1596) | Acc_1: (94.10%) (7347/7808)\n",
      "Epoch: 44 | Batch_idx: 70 |  Loss_1: (0.1604) | Acc_1: (94.10%) (8552/9088)\n",
      "Epoch: 44 | Batch_idx: 80 |  Loss_1: (0.1586) | Acc_1: (94.20%) (9767/10368)\n",
      "Epoch: 44 | Batch_idx: 90 |  Loss_1: (0.1579) | Acc_1: (94.27%) (10980/11648)\n",
      "Epoch: 44 | Batch_idx: 100 |  Loss_1: (0.1581) | Acc_1: (94.30%) (12191/12928)\n",
      "Epoch: 44 | Batch_idx: 110 |  Loss_1: (0.1549) | Acc_1: (94.39%) (13411/14208)\n",
      "Epoch: 44 | Batch_idx: 120 |  Loss_1: (0.1538) | Acc_1: (94.40%) (14621/15488)\n",
      "Epoch: 44 | Batch_idx: 130 |  Loss_1: (0.1567) | Acc_1: (94.30%) (15813/16768)\n",
      "Epoch: 44 | Batch_idx: 140 |  Loss_1: (0.1562) | Acc_1: (94.38%) (17033/18048)\n",
      "Epoch: 44 | Batch_idx: 150 |  Loss_1: (0.1557) | Acc_1: (94.33%) (18233/19328)\n",
      "Epoch: 44 | Batch_idx: 160 |  Loss_1: (0.1568) | Acc_1: (94.29%) (19431/20608)\n",
      "Epoch: 44 | Batch_idx: 170 |  Loss_1: (0.1576) | Acc_1: (94.28%) (20635/21888)\n",
      "Epoch: 44 | Batch_idx: 180 |  Loss_1: (0.1572) | Acc_1: (94.32%) (21851/23168)\n",
      "Epoch: 44 | Batch_idx: 190 |  Loss_1: (0.1587) | Acc_1: (94.26%) (23045/24448)\n",
      "Epoch: 44 | Batch_idx: 200 |  Loss_1: (0.1582) | Acc_1: (94.27%) (24254/25728)\n",
      "Epoch: 44 | Batch_idx: 210 |  Loss_1: (0.1588) | Acc_1: (94.26%) (25457/27008)\n",
      "Epoch: 44 | Batch_idx: 220 |  Loss_1: (0.1602) | Acc_1: (94.20%) (26648/28288)\n",
      "Epoch: 44 | Batch_idx: 230 |  Loss_1: (0.1592) | Acc_1: (94.27%) (27874/29568)\n",
      "Epoch: 44 | Batch_idx: 240 |  Loss_1: (0.1589) | Acc_1: (94.28%) (29085/30848)\n",
      "Epoch: 44 | Batch_idx: 250 |  Loss_1: (0.1581) | Acc_1: (94.31%) (30301/32128)\n",
      "Epoch: 44 | Batch_idx: 260 |  Loss_1: (0.1584) | Acc_1: (94.30%) (31504/33408)\n",
      "Epoch: 44 | Batch_idx: 270 |  Loss_1: (0.1587) | Acc_1: (94.31%) (32715/34688)\n",
      "Epoch: 44 | Batch_idx: 280 |  Loss_1: (0.1581) | Acc_1: (94.35%) (33936/35968)\n",
      "Epoch: 44 | Batch_idx: 290 |  Loss_1: (0.1574) | Acc_1: (94.36%) (35149/37248)\n",
      "Epoch: 44 | Batch_idx: 300 |  Loss_1: (0.1571) | Acc_1: (94.38%) (36363/38528)\n",
      "Epoch: 44 | Batch_idx: 310 |  Loss_1: (0.1572) | Acc_1: (94.37%) (37567/39808)\n",
      "Epoch: 44 | Batch_idx: 320 |  Loss_1: (0.1563) | Acc_1: (94.41%) (38790/41088)\n",
      "Epoch: 44 | Batch_idx: 330 |  Loss_1: (0.1572) | Acc_1: (94.39%) (39992/42368)\n",
      "Epoch: 44 | Batch_idx: 340 |  Loss_1: (0.1575) | Acc_1: (94.38%) (41195/43648)\n",
      "Epoch: 44 | Batch_idx: 350 |  Loss_1: (0.1576) | Acc_1: (94.38%) (42402/44928)\n",
      "Epoch: 44 | Batch_idx: 360 |  Loss_1: (0.1585) | Acc_1: (94.35%) (43599/46208)\n",
      "Epoch: 44 | Batch_idx: 370 |  Loss_1: (0.1586) | Acc_1: (94.37%) (44816/47488)\n",
      "Epoch: 44 | Batch_idx: 380 |  Loss_1: (0.1594) | Acc_1: (94.36%) (46016/48768)\n",
      "Epoch: 44 | Batch_idx: 390 |  Loss_1: (0.1591) | Acc_1: (94.37%) (47187/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3692) | Acc: (89.72%) (8972/10000)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.2073) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.1538) | Acc_1: (94.67%) (1333/1408)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.1659) | Acc_1: (94.35%) (2536/2688)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.1611) | Acc_1: (94.46%) (3748/3968)\n",
      "Epoch: 45 | Batch_idx: 40 |  Loss_1: (0.1601) | Acc_1: (94.34%) (4951/5248)\n",
      "Epoch: 45 | Batch_idx: 50 |  Loss_1: (0.1619) | Acc_1: (94.33%) (6158/6528)\n",
      "Epoch: 45 | Batch_idx: 60 |  Loss_1: (0.1632) | Acc_1: (94.31%) (7364/7808)\n",
      "Epoch: 45 | Batch_idx: 70 |  Loss_1: (0.1629) | Acc_1: (94.25%) (8565/9088)\n",
      "Epoch: 45 | Batch_idx: 80 |  Loss_1: (0.1610) | Acc_1: (94.33%) (9780/10368)\n",
      "Epoch: 45 | Batch_idx: 90 |  Loss_1: (0.1570) | Acc_1: (94.49%) (11006/11648)\n",
      "Epoch: 45 | Batch_idx: 100 |  Loss_1: (0.1558) | Acc_1: (94.57%) (12226/12928)\n",
      "Epoch: 45 | Batch_idx: 110 |  Loss_1: (0.1551) | Acc_1: (94.53%) (13431/14208)\n",
      "Epoch: 45 | Batch_idx: 120 |  Loss_1: (0.1545) | Acc_1: (94.53%) (14641/15488)\n",
      "Epoch: 45 | Batch_idx: 130 |  Loss_1: (0.1553) | Acc_1: (94.53%) (15851/16768)\n",
      "Epoch: 45 | Batch_idx: 140 |  Loss_1: (0.1562) | Acc_1: (94.46%) (17048/18048)\n",
      "Epoch: 45 | Batch_idx: 150 |  Loss_1: (0.1553) | Acc_1: (94.48%) (18262/19328)\n",
      "Epoch: 45 | Batch_idx: 160 |  Loss_1: (0.1552) | Acc_1: (94.49%) (19472/20608)\n",
      "Epoch: 45 | Batch_idx: 170 |  Loss_1: (0.1546) | Acc_1: (94.53%) (20691/21888)\n",
      "Epoch: 45 | Batch_idx: 180 |  Loss_1: (0.1542) | Acc_1: (94.53%) (21900/23168)\n",
      "Epoch: 45 | Batch_idx: 190 |  Loss_1: (0.1551) | Acc_1: (94.53%) (23110/24448)\n",
      "Epoch: 45 | Batch_idx: 200 |  Loss_1: (0.1561) | Acc_1: (94.49%) (24310/25728)\n",
      "Epoch: 45 | Batch_idx: 210 |  Loss_1: (0.1561) | Acc_1: (94.48%) (25517/27008)\n",
      "Epoch: 45 | Batch_idx: 220 |  Loss_1: (0.1568) | Acc_1: (94.46%) (26721/28288)\n",
      "Epoch: 45 | Batch_idx: 230 |  Loss_1: (0.1582) | Acc_1: (94.43%) (27921/29568)\n",
      "Epoch: 45 | Batch_idx: 240 |  Loss_1: (0.1579) | Acc_1: (94.44%) (29132/30848)\n",
      "Epoch: 45 | Batch_idx: 250 |  Loss_1: (0.1578) | Acc_1: (94.43%) (30339/32128)\n",
      "Epoch: 45 | Batch_idx: 260 |  Loss_1: (0.1569) | Acc_1: (94.47%) (31559/33408)\n",
      "Epoch: 45 | Batch_idx: 270 |  Loss_1: (0.1570) | Acc_1: (94.46%) (32767/34688)\n",
      "Epoch: 45 | Batch_idx: 280 |  Loss_1: (0.1577) | Acc_1: (94.43%) (33966/35968)\n",
      "Epoch: 45 | Batch_idx: 290 |  Loss_1: (0.1583) | Acc_1: (94.43%) (35173/37248)\n",
      "Epoch: 45 | Batch_idx: 300 |  Loss_1: (0.1584) | Acc_1: (94.41%) (36373/38528)\n",
      "Epoch: 45 | Batch_idx: 310 |  Loss_1: (0.1590) | Acc_1: (94.39%) (37575/39808)\n",
      "Epoch: 45 | Batch_idx: 320 |  Loss_1: (0.1586) | Acc_1: (94.41%) (38791/41088)\n",
      "Epoch: 45 | Batch_idx: 330 |  Loss_1: (0.1582) | Acc_1: (94.44%) (40011/42368)\n",
      "Epoch: 45 | Batch_idx: 340 |  Loss_1: (0.1584) | Acc_1: (94.43%) (41215/43648)\n",
      "Epoch: 45 | Batch_idx: 350 |  Loss_1: (0.1589) | Acc_1: (94.41%) (42418/44928)\n",
      "Epoch: 45 | Batch_idx: 360 |  Loss_1: (0.1598) | Acc_1: (94.39%) (43617/46208)\n",
      "Epoch: 45 | Batch_idx: 370 |  Loss_1: (0.1596) | Acc_1: (94.41%) (44832/47488)\n",
      "Epoch: 45 | Batch_idx: 380 |  Loss_1: (0.1595) | Acc_1: (94.39%) (46033/48768)\n",
      "Epoch: 45 | Batch_idx: 390 |  Loss_1: (0.1599) | Acc_1: (94.38%) (47192/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4178) | Acc: (88.46%) (8846/10000)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.0844) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.1397) | Acc_1: (94.39%) (1329/1408)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.1539) | Acc_1: (94.20%) (2532/2688)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.1485) | Acc_1: (94.35%) (3744/3968)\n",
      "Epoch: 46 | Batch_idx: 40 |  Loss_1: (0.1454) | Acc_1: (94.51%) (4960/5248)\n",
      "Epoch: 46 | Batch_idx: 50 |  Loss_1: (0.1482) | Acc_1: (94.39%) (6162/6528)\n",
      "Epoch: 46 | Batch_idx: 60 |  Loss_1: (0.1537) | Acc_1: (94.39%) (7370/7808)\n",
      "Epoch: 46 | Batch_idx: 70 |  Loss_1: (0.1533) | Acc_1: (94.44%) (8583/9088)\n",
      "Epoch: 46 | Batch_idx: 80 |  Loss_1: (0.1493) | Acc_1: (94.57%) (9805/10368)\n",
      "Epoch: 46 | Batch_idx: 90 |  Loss_1: (0.1503) | Acc_1: (94.51%) (11008/11648)\n",
      "Epoch: 46 | Batch_idx: 100 |  Loss_1: (0.1482) | Acc_1: (94.62%) (12232/12928)\n",
      "Epoch: 46 | Batch_idx: 110 |  Loss_1: (0.1472) | Acc_1: (94.63%) (13445/14208)\n",
      "Epoch: 46 | Batch_idx: 120 |  Loss_1: (0.1490) | Acc_1: (94.56%) (14645/15488)\n",
      "Epoch: 46 | Batch_idx: 130 |  Loss_1: (0.1482) | Acc_1: (94.60%) (15863/16768)\n",
      "Epoch: 46 | Batch_idx: 140 |  Loss_1: (0.1483) | Acc_1: (94.61%) (17076/18048)\n",
      "Epoch: 46 | Batch_idx: 150 |  Loss_1: (0.1489) | Acc_1: (94.62%) (18289/19328)\n",
      "Epoch: 46 | Batch_idx: 160 |  Loss_1: (0.1488) | Acc_1: (94.67%) (19509/20608)\n",
      "Epoch: 46 | Batch_idx: 170 |  Loss_1: (0.1499) | Acc_1: (94.62%) (20710/21888)\n",
      "Epoch: 46 | Batch_idx: 180 |  Loss_1: (0.1506) | Acc_1: (94.57%) (21911/23168)\n",
      "Epoch: 46 | Batch_idx: 190 |  Loss_1: (0.1517) | Acc_1: (94.53%) (23110/24448)\n",
      "Epoch: 46 | Batch_idx: 200 |  Loss_1: (0.1521) | Acc_1: (94.55%) (24326/25728)\n",
      "Epoch: 46 | Batch_idx: 210 |  Loss_1: (0.1514) | Acc_1: (94.58%) (25545/27008)\n",
      "Epoch: 46 | Batch_idx: 220 |  Loss_1: (0.1526) | Acc_1: (94.51%) (26735/28288)\n",
      "Epoch: 46 | Batch_idx: 230 |  Loss_1: (0.1527) | Acc_1: (94.50%) (27941/29568)\n",
      "Epoch: 46 | Batch_idx: 240 |  Loss_1: (0.1529) | Acc_1: (94.47%) (29143/30848)\n",
      "Epoch: 46 | Batch_idx: 250 |  Loss_1: (0.1528) | Acc_1: (94.50%) (30360/32128)\n",
      "Epoch: 46 | Batch_idx: 260 |  Loss_1: (0.1525) | Acc_1: (94.50%) (31571/33408)\n",
      "Epoch: 46 | Batch_idx: 270 |  Loss_1: (0.1533) | Acc_1: (94.48%) (32772/34688)\n",
      "Epoch: 46 | Batch_idx: 280 |  Loss_1: (0.1543) | Acc_1: (94.43%) (33966/35968)\n",
      "Epoch: 46 | Batch_idx: 290 |  Loss_1: (0.1539) | Acc_1: (94.46%) (35185/37248)\n",
      "Epoch: 46 | Batch_idx: 300 |  Loss_1: (0.1548) | Acc_1: (94.45%) (36388/38528)\n",
      "Epoch: 46 | Batch_idx: 310 |  Loss_1: (0.1544) | Acc_1: (94.48%) (37609/39808)\n",
      "Epoch: 46 | Batch_idx: 320 |  Loss_1: (0.1548) | Acc_1: (94.46%) (38811/41088)\n",
      "Epoch: 46 | Batch_idx: 330 |  Loss_1: (0.1548) | Acc_1: (94.45%) (40017/42368)\n",
      "Epoch: 46 | Batch_idx: 340 |  Loss_1: (0.1548) | Acc_1: (94.46%) (41230/43648)\n",
      "Epoch: 46 | Batch_idx: 350 |  Loss_1: (0.1552) | Acc_1: (94.45%) (42435/44928)\n",
      "Epoch: 46 | Batch_idx: 360 |  Loss_1: (0.1561) | Acc_1: (94.42%) (43629/46208)\n",
      "Epoch: 46 | Batch_idx: 370 |  Loss_1: (0.1559) | Acc_1: (94.43%) (44843/47488)\n",
      "Epoch: 46 | Batch_idx: 380 |  Loss_1: (0.1557) | Acc_1: (94.43%) (46054/48768)\n",
      "Epoch: 46 | Batch_idx: 390 |  Loss_1: (0.1570) | Acc_1: (94.41%) (47207/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4171) | Acc: (89.06%) (8906/10000)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.2075) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.1370) | Acc_1: (95.31%) (1342/1408)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.1461) | Acc_1: (94.98%) (2553/2688)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.1489) | Acc_1: (94.96%) (3768/3968)\n",
      "Epoch: 47 | Batch_idx: 40 |  Loss_1: (0.1595) | Acc_1: (94.59%) (4964/5248)\n",
      "Epoch: 47 | Batch_idx: 50 |  Loss_1: (0.1617) | Acc_1: (94.53%) (6171/6528)\n",
      "Epoch: 47 | Batch_idx: 60 |  Loss_1: (0.1587) | Acc_1: (94.61%) (7387/7808)\n",
      "Epoch: 47 | Batch_idx: 70 |  Loss_1: (0.1580) | Acc_1: (94.55%) (8593/9088)\n",
      "Epoch: 47 | Batch_idx: 80 |  Loss_1: (0.1563) | Acc_1: (94.70%) (9818/10368)\n",
      "Epoch: 47 | Batch_idx: 90 |  Loss_1: (0.1562) | Acc_1: (94.65%) (11025/11648)\n",
      "Epoch: 47 | Batch_idx: 100 |  Loss_1: (0.1549) | Acc_1: (94.68%) (12240/12928)\n",
      "Epoch: 47 | Batch_idx: 110 |  Loss_1: (0.1547) | Acc_1: (94.66%) (13450/14208)\n",
      "Epoch: 47 | Batch_idx: 120 |  Loss_1: (0.1526) | Acc_1: (94.71%) (14669/15488)\n",
      "Epoch: 47 | Batch_idx: 130 |  Loss_1: (0.1529) | Acc_1: (94.72%) (15882/16768)\n",
      "Epoch: 47 | Batch_idx: 140 |  Loss_1: (0.1516) | Acc_1: (94.76%) (17102/18048)\n",
      "Epoch: 47 | Batch_idx: 150 |  Loss_1: (0.1494) | Acc_1: (94.84%) (18330/19328)\n",
      "Epoch: 47 | Batch_idx: 160 |  Loss_1: (0.1489) | Acc_1: (94.84%) (19545/20608)\n",
      "Epoch: 47 | Batch_idx: 170 |  Loss_1: (0.1509) | Acc_1: (94.77%) (20744/21888)\n",
      "Epoch: 47 | Batch_idx: 180 |  Loss_1: (0.1518) | Acc_1: (94.73%) (21947/23168)\n",
      "Epoch: 47 | Batch_idx: 190 |  Loss_1: (0.1522) | Acc_1: (94.68%) (23147/24448)\n",
      "Epoch: 47 | Batch_idx: 200 |  Loss_1: (0.1530) | Acc_1: (94.66%) (24353/25728)\n",
      "Epoch: 47 | Batch_idx: 210 |  Loss_1: (0.1533) | Acc_1: (94.61%) (25553/27008)\n",
      "Epoch: 47 | Batch_idx: 220 |  Loss_1: (0.1525) | Acc_1: (94.62%) (26765/28288)\n",
      "Epoch: 47 | Batch_idx: 230 |  Loss_1: (0.1516) | Acc_1: (94.64%) (27982/29568)\n",
      "Epoch: 47 | Batch_idx: 240 |  Loss_1: (0.1507) | Acc_1: (94.66%) (29200/30848)\n",
      "Epoch: 47 | Batch_idx: 250 |  Loss_1: (0.1505) | Acc_1: (94.68%) (30418/32128)\n",
      "Epoch: 47 | Batch_idx: 260 |  Loss_1: (0.1495) | Acc_1: (94.74%) (31652/33408)\n",
      "Epoch: 47 | Batch_idx: 270 |  Loss_1: (0.1499) | Acc_1: (94.74%) (32862/34688)\n",
      "Epoch: 47 | Batch_idx: 280 |  Loss_1: (0.1498) | Acc_1: (94.75%) (34078/35968)\n",
      "Epoch: 47 | Batch_idx: 290 |  Loss_1: (0.1501) | Acc_1: (94.75%) (35293/37248)\n",
      "Epoch: 47 | Batch_idx: 300 |  Loss_1: (0.1498) | Acc_1: (94.75%) (36504/38528)\n",
      "Epoch: 47 | Batch_idx: 310 |  Loss_1: (0.1494) | Acc_1: (94.76%) (37723/39808)\n",
      "Epoch: 47 | Batch_idx: 320 |  Loss_1: (0.1498) | Acc_1: (94.74%) (38926/41088)\n",
      "Epoch: 47 | Batch_idx: 330 |  Loss_1: (0.1495) | Acc_1: (94.76%) (40146/42368)\n",
      "Epoch: 47 | Batch_idx: 340 |  Loss_1: (0.1501) | Acc_1: (94.73%) (41346/43648)\n",
      "Epoch: 47 | Batch_idx: 350 |  Loss_1: (0.1502) | Acc_1: (94.72%) (42556/44928)\n",
      "Epoch: 47 | Batch_idx: 360 |  Loss_1: (0.1500) | Acc_1: (94.71%) (43763/46208)\n",
      "Epoch: 47 | Batch_idx: 370 |  Loss_1: (0.1506) | Acc_1: (94.70%) (44969/47488)\n",
      "Epoch: 47 | Batch_idx: 380 |  Loss_1: (0.1514) | Acc_1: (94.68%) (46172/48768)\n",
      "Epoch: 47 | Batch_idx: 390 |  Loss_1: (0.1516) | Acc_1: (94.67%) (47336/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4154) | Acc: (89.16%) (8916/10000)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.0951) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.1272) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.1244) | Acc_1: (95.42%) (2565/2688)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.1321) | Acc_1: (95.34%) (3783/3968)\n",
      "Epoch: 48 | Batch_idx: 40 |  Loss_1: (0.1306) | Acc_1: (95.35%) (5004/5248)\n",
      "Epoch: 48 | Batch_idx: 50 |  Loss_1: (0.1284) | Acc_1: (95.51%) (6235/6528)\n",
      "Epoch: 48 | Batch_idx: 60 |  Loss_1: (0.1301) | Acc_1: (95.50%) (7457/7808)\n",
      "Epoch: 48 | Batch_idx: 70 |  Loss_1: (0.1308) | Acc_1: (95.47%) (8676/9088)\n",
      "Epoch: 48 | Batch_idx: 80 |  Loss_1: (0.1342) | Acc_1: (95.35%) (9886/10368)\n",
      "Epoch: 48 | Batch_idx: 90 |  Loss_1: (0.1347) | Acc_1: (95.30%) (11101/11648)\n",
      "Epoch: 48 | Batch_idx: 100 |  Loss_1: (0.1357) | Acc_1: (95.27%) (12317/12928)\n",
      "Epoch: 48 | Batch_idx: 110 |  Loss_1: (0.1359) | Acc_1: (95.26%) (13535/14208)\n",
      "Epoch: 48 | Batch_idx: 120 |  Loss_1: (0.1388) | Acc_1: (95.16%) (14739/15488)\n",
      "Epoch: 48 | Batch_idx: 130 |  Loss_1: (0.1379) | Acc_1: (95.21%) (15965/16768)\n",
      "Epoch: 48 | Batch_idx: 140 |  Loss_1: (0.1384) | Acc_1: (95.14%) (17170/18048)\n",
      "Epoch: 48 | Batch_idx: 150 |  Loss_1: (0.1391) | Acc_1: (95.14%) (18388/19328)\n",
      "Epoch: 48 | Batch_idx: 160 |  Loss_1: (0.1396) | Acc_1: (95.13%) (19605/20608)\n",
      "Epoch: 48 | Batch_idx: 170 |  Loss_1: (0.1389) | Acc_1: (95.15%) (20827/21888)\n",
      "Epoch: 48 | Batch_idx: 180 |  Loss_1: (0.1393) | Acc_1: (95.13%) (22040/23168)\n",
      "Epoch: 48 | Batch_idx: 190 |  Loss_1: (0.1393) | Acc_1: (95.13%) (23258/24448)\n",
      "Epoch: 48 | Batch_idx: 200 |  Loss_1: (0.1383) | Acc_1: (95.20%) (24494/25728)\n",
      "Epoch: 48 | Batch_idx: 210 |  Loss_1: (0.1383) | Acc_1: (95.22%) (25716/27008)\n",
      "Epoch: 48 | Batch_idx: 220 |  Loss_1: (0.1387) | Acc_1: (95.17%) (26922/28288)\n",
      "Epoch: 48 | Batch_idx: 230 |  Loss_1: (0.1387) | Acc_1: (95.15%) (28135/29568)\n",
      "Epoch: 48 | Batch_idx: 240 |  Loss_1: (0.1394) | Acc_1: (95.12%) (29342/30848)\n",
      "Epoch: 48 | Batch_idx: 250 |  Loss_1: (0.1399) | Acc_1: (95.09%) (30549/32128)\n",
      "Epoch: 48 | Batch_idx: 260 |  Loss_1: (0.1403) | Acc_1: (95.07%) (31762/33408)\n",
      "Epoch: 48 | Batch_idx: 270 |  Loss_1: (0.1412) | Acc_1: (95.03%) (32965/34688)\n",
      "Epoch: 48 | Batch_idx: 280 |  Loss_1: (0.1412) | Acc_1: (95.04%) (34183/35968)\n",
      "Epoch: 48 | Batch_idx: 290 |  Loss_1: (0.1411) | Acc_1: (95.04%) (35399/37248)\n",
      "Epoch: 48 | Batch_idx: 300 |  Loss_1: (0.1415) | Acc_1: (95.03%) (36614/38528)\n",
      "Epoch: 48 | Batch_idx: 310 |  Loss_1: (0.1410) | Acc_1: (95.06%) (37840/39808)\n",
      "Epoch: 48 | Batch_idx: 320 |  Loss_1: (0.1408) | Acc_1: (95.06%) (39057/41088)\n",
      "Epoch: 48 | Batch_idx: 330 |  Loss_1: (0.1407) | Acc_1: (95.07%) (40279/42368)\n",
      "Epoch: 48 | Batch_idx: 340 |  Loss_1: (0.1409) | Acc_1: (95.06%) (41492/43648)\n",
      "Epoch: 48 | Batch_idx: 350 |  Loss_1: (0.1419) | Acc_1: (95.04%) (42700/44928)\n",
      "Epoch: 48 | Batch_idx: 360 |  Loss_1: (0.1418) | Acc_1: (95.04%) (43918/46208)\n",
      "Epoch: 48 | Batch_idx: 370 |  Loss_1: (0.1410) | Acc_1: (95.07%) (45149/47488)\n",
      "Epoch: 48 | Batch_idx: 380 |  Loss_1: (0.1411) | Acc_1: (95.08%) (46369/48768)\n",
      "Epoch: 48 | Batch_idx: 390 |  Loss_1: (0.1409) | Acc_1: (95.08%) (47539/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4149) | Acc: (89.15%) (8915/10000)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.1570) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.1384) | Acc_1: (95.31%) (1342/1408)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.1199) | Acc_1: (95.98%) (2580/2688)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.1204) | Acc_1: (95.92%) (3806/3968)\n",
      "Epoch: 49 | Batch_idx: 40 |  Loss_1: (0.1308) | Acc_1: (95.60%) (5017/5248)\n",
      "Epoch: 49 | Batch_idx: 50 |  Loss_1: (0.1274) | Acc_1: (95.70%) (6247/6528)\n",
      "Epoch: 49 | Batch_idx: 60 |  Loss_1: (0.1254) | Acc_1: (95.76%) (7477/7808)\n",
      "Epoch: 49 | Batch_idx: 70 |  Loss_1: (0.1265) | Acc_1: (95.77%) (8704/9088)\n",
      "Epoch: 49 | Batch_idx: 80 |  Loss_1: (0.1292) | Acc_1: (95.64%) (9916/10368)\n",
      "Epoch: 49 | Batch_idx: 90 |  Loss_1: (0.1305) | Acc_1: (95.55%) (11130/11648)\n",
      "Epoch: 49 | Batch_idx: 100 |  Loss_1: (0.1293) | Acc_1: (95.57%) (12355/12928)\n",
      "Epoch: 49 | Batch_idx: 110 |  Loss_1: (0.1304) | Acc_1: (95.53%) (13573/14208)\n",
      "Epoch: 49 | Batch_idx: 120 |  Loss_1: (0.1294) | Acc_1: (95.58%) (14803/15488)\n",
      "Epoch: 49 | Batch_idx: 130 |  Loss_1: (0.1323) | Acc_1: (95.47%) (16008/16768)\n",
      "Epoch: 49 | Batch_idx: 140 |  Loss_1: (0.1330) | Acc_1: (95.42%) (17221/18048)\n",
      "Epoch: 49 | Batch_idx: 150 |  Loss_1: (0.1360) | Acc_1: (95.32%) (18423/19328)\n",
      "Epoch: 49 | Batch_idx: 160 |  Loss_1: (0.1380) | Acc_1: (95.23%) (19625/20608)\n",
      "Epoch: 49 | Batch_idx: 170 |  Loss_1: (0.1387) | Acc_1: (95.23%) (20845/21888)\n",
      "Epoch: 49 | Batch_idx: 180 |  Loss_1: (0.1392) | Acc_1: (95.18%) (22051/23168)\n",
      "Epoch: 49 | Batch_idx: 190 |  Loss_1: (0.1388) | Acc_1: (95.20%) (23275/24448)\n",
      "Epoch: 49 | Batch_idx: 200 |  Loss_1: (0.1380) | Acc_1: (95.22%) (24499/25728)\n",
      "Epoch: 49 | Batch_idx: 210 |  Loss_1: (0.1387) | Acc_1: (95.19%) (25709/27008)\n",
      "Epoch: 49 | Batch_idx: 220 |  Loss_1: (0.1400) | Acc_1: (95.16%) (26918/28288)\n",
      "Epoch: 49 | Batch_idx: 230 |  Loss_1: (0.1405) | Acc_1: (95.12%) (28126/29568)\n",
      "Epoch: 49 | Batch_idx: 240 |  Loss_1: (0.1411) | Acc_1: (95.09%) (29333/30848)\n",
      "Epoch: 49 | Batch_idx: 250 |  Loss_1: (0.1411) | Acc_1: (95.10%) (30555/32128)\n",
      "Epoch: 49 | Batch_idx: 260 |  Loss_1: (0.1423) | Acc_1: (95.08%) (31763/33408)\n",
      "Epoch: 49 | Batch_idx: 270 |  Loss_1: (0.1416) | Acc_1: (95.10%) (32989/34688)\n",
      "Epoch: 49 | Batch_idx: 280 |  Loss_1: (0.1418) | Acc_1: (95.09%) (34201/35968)\n",
      "Epoch: 49 | Batch_idx: 290 |  Loss_1: (0.1416) | Acc_1: (95.10%) (35422/37248)\n",
      "Epoch: 49 | Batch_idx: 300 |  Loss_1: (0.1419) | Acc_1: (95.09%) (36638/38528)\n",
      "Epoch: 49 | Batch_idx: 310 |  Loss_1: (0.1420) | Acc_1: (95.09%) (37853/39808)\n",
      "Epoch: 49 | Batch_idx: 320 |  Loss_1: (0.1424) | Acc_1: (95.08%) (39066/41088)\n",
      "Epoch: 49 | Batch_idx: 330 |  Loss_1: (0.1429) | Acc_1: (95.06%) (40274/42368)\n",
      "Epoch: 49 | Batch_idx: 340 |  Loss_1: (0.1435) | Acc_1: (95.04%) (41483/43648)\n",
      "Epoch: 49 | Batch_idx: 350 |  Loss_1: (0.1436) | Acc_1: (95.03%) (42695/44928)\n",
      "Epoch: 49 | Batch_idx: 360 |  Loss_1: (0.1441) | Acc_1: (95.04%) (43914/46208)\n",
      "Epoch: 49 | Batch_idx: 370 |  Loss_1: (0.1438) | Acc_1: (95.03%) (45130/47488)\n",
      "Epoch: 49 | Batch_idx: 380 |  Loss_1: (0.1440) | Acc_1: (95.04%) (46350/48768)\n",
      "Epoch: 49 | Batch_idx: 390 |  Loss_1: (0.1442) | Acc_1: (95.05%) (47527/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3724) | Acc: (89.99%) (8999/10000)\n",
      "Epoch: 50 | Batch_idx: 0 |  Loss_1: (0.1101) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 50 | Batch_idx: 10 |  Loss_1: (0.1163) | Acc_1: (95.53%) (1345/1408)\n",
      "Epoch: 50 | Batch_idx: 20 |  Loss_1: (0.1144) | Acc_1: (95.83%) (2576/2688)\n",
      "Epoch: 50 | Batch_idx: 30 |  Loss_1: (0.1194) | Acc_1: (95.77%) (3800/3968)\n",
      "Epoch: 50 | Batch_idx: 40 |  Loss_1: (0.1126) | Acc_1: (95.94%) (5035/5248)\n",
      "Epoch: 50 | Batch_idx: 50 |  Loss_1: (0.1156) | Acc_1: (95.85%) (6257/6528)\n",
      "Epoch: 50 | Batch_idx: 60 |  Loss_1: (0.1191) | Acc_1: (95.77%) (7478/7808)\n",
      "Epoch: 50 | Batch_idx: 70 |  Loss_1: (0.1199) | Acc_1: (95.74%) (8701/9088)\n",
      "Epoch: 50 | Batch_idx: 80 |  Loss_1: (0.1216) | Acc_1: (95.70%) (9922/10368)\n",
      "Epoch: 50 | Batch_idx: 90 |  Loss_1: (0.1232) | Acc_1: (95.65%) (11141/11648)\n",
      "Epoch: 50 | Batch_idx: 100 |  Loss_1: (0.1269) | Acc_1: (95.52%) (12349/12928)\n",
      "Epoch: 50 | Batch_idx: 110 |  Loss_1: (0.1249) | Acc_1: (95.60%) (13583/14208)\n",
      "Epoch: 50 | Batch_idx: 120 |  Loss_1: (0.1254) | Acc_1: (95.60%) (14806/15488)\n",
      "Epoch: 50 | Batch_idx: 130 |  Loss_1: (0.1248) | Acc_1: (95.60%) (16031/16768)\n",
      "Epoch: 50 | Batch_idx: 140 |  Loss_1: (0.1243) | Acc_1: (95.61%) (17255/18048)\n",
      "Epoch: 50 | Batch_idx: 150 |  Loss_1: (0.1242) | Acc_1: (95.67%) (18491/19328)\n",
      "Epoch: 50 | Batch_idx: 160 |  Loss_1: (0.1242) | Acc_1: (95.64%) (19710/20608)\n",
      "Epoch: 50 | Batch_idx: 170 |  Loss_1: (0.1259) | Acc_1: (95.60%) (20926/21888)\n",
      "Epoch: 50 | Batch_idx: 180 |  Loss_1: (0.1255) | Acc_1: (95.62%) (22153/23168)\n",
      "Epoch: 50 | Batch_idx: 190 |  Loss_1: (0.1261) | Acc_1: (95.60%) (23373/24448)\n",
      "Epoch: 50 | Batch_idx: 200 |  Loss_1: (0.1267) | Acc_1: (95.59%) (24593/25728)\n",
      "Epoch: 50 | Batch_idx: 210 |  Loss_1: (0.1273) | Acc_1: (95.56%) (25808/27008)\n",
      "Epoch: 50 | Batch_idx: 220 |  Loss_1: (0.1286) | Acc_1: (95.50%) (27015/28288)\n",
      "Epoch: 50 | Batch_idx: 230 |  Loss_1: (0.1287) | Acc_1: (95.48%) (28233/29568)\n",
      "Epoch: 50 | Batch_idx: 240 |  Loss_1: (0.1280) | Acc_1: (95.52%) (29466/30848)\n",
      "Epoch: 50 | Batch_idx: 250 |  Loss_1: (0.1282) | Acc_1: (95.53%) (30691/32128)\n",
      "Epoch: 50 | Batch_idx: 260 |  Loss_1: (0.1283) | Acc_1: (95.53%) (31915/33408)\n",
      "Epoch: 50 | Batch_idx: 270 |  Loss_1: (0.1297) | Acc_1: (95.50%) (33126/34688)\n",
      "Epoch: 50 | Batch_idx: 280 |  Loss_1: (0.1306) | Acc_1: (95.48%) (34344/35968)\n",
      "Epoch: 50 | Batch_idx: 290 |  Loss_1: (0.1315) | Acc_1: (95.46%) (35557/37248)\n",
      "Epoch: 50 | Batch_idx: 300 |  Loss_1: (0.1313) | Acc_1: (95.47%) (36784/38528)\n",
      "Epoch: 50 | Batch_idx: 310 |  Loss_1: (0.1320) | Acc_1: (95.46%) (37999/39808)\n",
      "Epoch: 50 | Batch_idx: 320 |  Loss_1: (0.1322) | Acc_1: (95.45%) (39218/41088)\n",
      "Epoch: 50 | Batch_idx: 330 |  Loss_1: (0.1321) | Acc_1: (95.46%) (40443/42368)\n",
      "Epoch: 50 | Batch_idx: 340 |  Loss_1: (0.1317) | Acc_1: (95.48%) (41676/43648)\n",
      "Epoch: 50 | Batch_idx: 350 |  Loss_1: (0.1320) | Acc_1: (95.47%) (42894/44928)\n",
      "Epoch: 50 | Batch_idx: 360 |  Loss_1: (0.1316) | Acc_1: (95.49%) (44123/46208)\n",
      "Epoch: 50 | Batch_idx: 370 |  Loss_1: (0.1316) | Acc_1: (95.48%) (45343/47488)\n",
      "Epoch: 50 | Batch_idx: 380 |  Loss_1: (0.1312) | Acc_1: (95.50%) (46574/48768)\n",
      "Epoch: 50 | Batch_idx: 390 |  Loss_1: (0.1312) | Acc_1: (95.51%) (47754/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3981) | Acc: (89.63%) (8963/10000)\n",
      "Epoch: 51 | Batch_idx: 0 |  Loss_1: (0.0950) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 51 | Batch_idx: 10 |  Loss_1: (0.1309) | Acc_1: (95.38%) (1343/1408)\n",
      "Epoch: 51 | Batch_idx: 20 |  Loss_1: (0.1302) | Acc_1: (95.57%) (2569/2688)\n",
      "Epoch: 51 | Batch_idx: 30 |  Loss_1: (0.1222) | Acc_1: (95.84%) (3803/3968)\n",
      "Epoch: 51 | Batch_idx: 40 |  Loss_1: (0.1215) | Acc_1: (95.85%) (5030/5248)\n",
      "Epoch: 51 | Batch_idx: 50 |  Loss_1: (0.1244) | Acc_1: (95.68%) (6246/6528)\n",
      "Epoch: 51 | Batch_idx: 60 |  Loss_1: (0.1250) | Acc_1: (95.76%) (7477/7808)\n",
      "Epoch: 51 | Batch_idx: 70 |  Loss_1: (0.1247) | Acc_1: (95.58%) (8686/9088)\n",
      "Epoch: 51 | Batch_idx: 80 |  Loss_1: (0.1261) | Acc_1: (95.62%) (9914/10368)\n",
      "Epoch: 51 | Batch_idx: 90 |  Loss_1: (0.1253) | Acc_1: (95.63%) (11139/11648)\n",
      "Epoch: 51 | Batch_idx: 100 |  Loss_1: (0.1267) | Acc_1: (95.60%) (12359/12928)\n",
      "Epoch: 51 | Batch_idx: 110 |  Loss_1: (0.1266) | Acc_1: (95.65%) (13590/14208)\n",
      "Epoch: 51 | Batch_idx: 120 |  Loss_1: (0.1251) | Acc_1: (95.68%) (14819/15488)\n",
      "Epoch: 51 | Batch_idx: 130 |  Loss_1: (0.1242) | Acc_1: (95.68%) (16043/16768)\n",
      "Epoch: 51 | Batch_idx: 140 |  Loss_1: (0.1239) | Acc_1: (95.74%) (17280/18048)\n",
      "Epoch: 51 | Batch_idx: 150 |  Loss_1: (0.1242) | Acc_1: (95.75%) (18506/19328)\n",
      "Epoch: 51 | Batch_idx: 160 |  Loss_1: (0.1240) | Acc_1: (95.72%) (19726/20608)\n",
      "Epoch: 51 | Batch_idx: 170 |  Loss_1: (0.1230) | Acc_1: (95.74%) (20956/21888)\n",
      "Epoch: 51 | Batch_idx: 180 |  Loss_1: (0.1238) | Acc_1: (95.72%) (22176/23168)\n",
      "Epoch: 51 | Batch_idx: 190 |  Loss_1: (0.1253) | Acc_1: (95.66%) (23386/24448)\n",
      "Epoch: 51 | Batch_idx: 200 |  Loss_1: (0.1253) | Acc_1: (95.64%) (24607/25728)\n",
      "Epoch: 51 | Batch_idx: 210 |  Loss_1: (0.1256) | Acc_1: (95.63%) (25827/27008)\n",
      "Epoch: 51 | Batch_idx: 220 |  Loss_1: (0.1272) | Acc_1: (95.58%) (27037/28288)\n",
      "Epoch: 51 | Batch_idx: 230 |  Loss_1: (0.1275) | Acc_1: (95.56%) (28256/29568)\n",
      "Epoch: 51 | Batch_idx: 240 |  Loss_1: (0.1272) | Acc_1: (95.55%) (29476/30848)\n",
      "Epoch: 51 | Batch_idx: 250 |  Loss_1: (0.1266) | Acc_1: (95.57%) (30704/32128)\n",
      "Epoch: 51 | Batch_idx: 260 |  Loss_1: (0.1265) | Acc_1: (95.57%) (31928/33408)\n",
      "Epoch: 51 | Batch_idx: 270 |  Loss_1: (0.1266) | Acc_1: (95.57%) (33152/34688)\n",
      "Epoch: 51 | Batch_idx: 280 |  Loss_1: (0.1283) | Acc_1: (95.50%) (34349/35968)\n",
      "Epoch: 51 | Batch_idx: 290 |  Loss_1: (0.1295) | Acc_1: (95.45%) (35555/37248)\n",
      "Epoch: 51 | Batch_idx: 300 |  Loss_1: (0.1287) | Acc_1: (95.48%) (36787/38528)\n",
      "Epoch: 51 | Batch_idx: 310 |  Loss_1: (0.1293) | Acc_1: (95.45%) (37997/39808)\n",
      "Epoch: 51 | Batch_idx: 320 |  Loss_1: (0.1300) | Acc_1: (95.44%) (39214/41088)\n",
      "Epoch: 51 | Batch_idx: 330 |  Loss_1: (0.1313) | Acc_1: (95.40%) (40419/42368)\n",
      "Epoch: 51 | Batch_idx: 340 |  Loss_1: (0.1320) | Acc_1: (95.38%) (41630/43648)\n",
      "Epoch: 51 | Batch_idx: 350 |  Loss_1: (0.1318) | Acc_1: (95.37%) (42850/44928)\n",
      "Epoch: 51 | Batch_idx: 360 |  Loss_1: (0.1313) | Acc_1: (95.38%) (44075/46208)\n",
      "Epoch: 51 | Batch_idx: 370 |  Loss_1: (0.1318) | Acc_1: (95.37%) (45289/47488)\n",
      "Epoch: 51 | Batch_idx: 380 |  Loss_1: (0.1317) | Acc_1: (95.37%) (46511/48768)\n",
      "Epoch: 51 | Batch_idx: 390 |  Loss_1: (0.1319) | Acc_1: (95.37%) (47685/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4445) | Acc: (88.67%) (8867/10000)\n",
      "Epoch: 52 | Batch_idx: 0 |  Loss_1: (0.2713) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 52 | Batch_idx: 10 |  Loss_1: (0.1316) | Acc_1: (95.74%) (1348/1408)\n",
      "Epoch: 52 | Batch_idx: 20 |  Loss_1: (0.1398) | Acc_1: (95.24%) (2560/2688)\n",
      "Epoch: 52 | Batch_idx: 30 |  Loss_1: (0.1326) | Acc_1: (95.41%) (3786/3968)\n",
      "Epoch: 52 | Batch_idx: 40 |  Loss_1: (0.1288) | Acc_1: (95.52%) (5013/5248)\n",
      "Epoch: 52 | Batch_idx: 50 |  Loss_1: (0.1285) | Acc_1: (95.44%) (6230/6528)\n",
      "Epoch: 52 | Batch_idx: 60 |  Loss_1: (0.1312) | Acc_1: (95.35%) (7445/7808)\n",
      "Epoch: 52 | Batch_idx: 70 |  Loss_1: (0.1314) | Acc_1: (95.36%) (8666/9088)\n",
      "Epoch: 52 | Batch_idx: 80 |  Loss_1: (0.1307) | Acc_1: (95.38%) (9889/10368)\n",
      "Epoch: 52 | Batch_idx: 90 |  Loss_1: (0.1303) | Acc_1: (95.40%) (11112/11648)\n",
      "Epoch: 52 | Batch_idx: 100 |  Loss_1: (0.1279) | Acc_1: (95.47%) (12343/12928)\n",
      "Epoch: 52 | Batch_idx: 110 |  Loss_1: (0.1273) | Acc_1: (95.50%) (13569/14208)\n",
      "Epoch: 52 | Batch_idx: 120 |  Loss_1: (0.1259) | Acc_1: (95.58%) (14804/15488)\n",
      "Epoch: 52 | Batch_idx: 130 |  Loss_1: (0.1266) | Acc_1: (95.55%) (16022/16768)\n",
      "Epoch: 52 | Batch_idx: 140 |  Loss_1: (0.1265) | Acc_1: (95.53%) (17242/18048)\n",
      "Epoch: 52 | Batch_idx: 150 |  Loss_1: (0.1259) | Acc_1: (95.55%) (18468/19328)\n",
      "Epoch: 52 | Batch_idx: 160 |  Loss_1: (0.1258) | Acc_1: (95.54%) (19688/20608)\n",
      "Epoch: 52 | Batch_idx: 170 |  Loss_1: (0.1247) | Acc_1: (95.59%) (20923/21888)\n",
      "Epoch: 52 | Batch_idx: 180 |  Loss_1: (0.1238) | Acc_1: (95.63%) (22155/23168)\n",
      "Epoch: 52 | Batch_idx: 190 |  Loss_1: (0.1247) | Acc_1: (95.59%) (23370/24448)\n",
      "Epoch: 52 | Batch_idx: 200 |  Loss_1: (0.1248) | Acc_1: (95.60%) (24595/25728)\n",
      "Epoch: 52 | Batch_idx: 210 |  Loss_1: (0.1260) | Acc_1: (95.57%) (25811/27008)\n",
      "Epoch: 52 | Batch_idx: 220 |  Loss_1: (0.1267) | Acc_1: (95.54%) (27026/28288)\n",
      "Epoch: 52 | Batch_idx: 230 |  Loss_1: (0.1269) | Acc_1: (95.54%) (28250/29568)\n",
      "Epoch: 52 | Batch_idx: 240 |  Loss_1: (0.1277) | Acc_1: (95.53%) (29470/30848)\n",
      "Epoch: 52 | Batch_idx: 250 |  Loss_1: (0.1278) | Acc_1: (95.53%) (30693/32128)\n",
      "Epoch: 52 | Batch_idx: 260 |  Loss_1: (0.1274) | Acc_1: (95.55%) (31923/33408)\n",
      "Epoch: 52 | Batch_idx: 270 |  Loss_1: (0.1270) | Acc_1: (95.57%) (33153/34688)\n",
      "Epoch: 52 | Batch_idx: 280 |  Loss_1: (0.1271) | Acc_1: (95.59%) (34382/35968)\n",
      "Epoch: 52 | Batch_idx: 290 |  Loss_1: (0.1271) | Acc_1: (95.59%) (35607/37248)\n",
      "Epoch: 52 | Batch_idx: 300 |  Loss_1: (0.1280) | Acc_1: (95.58%) (36826/38528)\n",
      "Epoch: 52 | Batch_idx: 310 |  Loss_1: (0.1282) | Acc_1: (95.56%) (38041/39808)\n",
      "Epoch: 52 | Batch_idx: 320 |  Loss_1: (0.1283) | Acc_1: (95.55%) (39261/41088)\n",
      "Epoch: 52 | Batch_idx: 330 |  Loss_1: (0.1291) | Acc_1: (95.53%) (40475/42368)\n",
      "Epoch: 52 | Batch_idx: 340 |  Loss_1: (0.1290) | Acc_1: (95.54%) (41701/43648)\n",
      "Epoch: 52 | Batch_idx: 350 |  Loss_1: (0.1285) | Acc_1: (95.56%) (42933/44928)\n",
      "Epoch: 52 | Batch_idx: 360 |  Loss_1: (0.1288) | Acc_1: (95.55%) (44154/46208)\n",
      "Epoch: 52 | Batch_idx: 370 |  Loss_1: (0.1291) | Acc_1: (95.56%) (45378/47488)\n",
      "Epoch: 52 | Batch_idx: 380 |  Loss_1: (0.1293) | Acc_1: (95.53%) (46586/48768)\n",
      "Epoch: 52 | Batch_idx: 390 |  Loss_1: (0.1291) | Acc_1: (95.53%) (47765/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3599) | Acc: (90.28%) (9028/10000)\n",
      "Epoch: 53 | Batch_idx: 0 |  Loss_1: (0.1765) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 53 | Batch_idx: 10 |  Loss_1: (0.1293) | Acc_1: (95.10%) (1339/1408)\n",
      "Epoch: 53 | Batch_idx: 20 |  Loss_1: (0.1244) | Acc_1: (95.57%) (2569/2688)\n",
      "Epoch: 53 | Batch_idx: 30 |  Loss_1: (0.1233) | Acc_1: (95.72%) (3798/3968)\n",
      "Epoch: 53 | Batch_idx: 40 |  Loss_1: (0.1240) | Acc_1: (95.67%) (5021/5248)\n",
      "Epoch: 53 | Batch_idx: 50 |  Loss_1: (0.1250) | Acc_1: (95.60%) (6241/6528)\n",
      "Epoch: 53 | Batch_idx: 60 |  Loss_1: (0.1215) | Acc_1: (95.79%) (7479/7808)\n",
      "Epoch: 53 | Batch_idx: 70 |  Loss_1: (0.1220) | Acc_1: (95.74%) (8701/9088)\n",
      "Epoch: 53 | Batch_idx: 80 |  Loss_1: (0.1220) | Acc_1: (95.77%) (9929/10368)\n",
      "Epoch: 53 | Batch_idx: 90 |  Loss_1: (0.1244) | Acc_1: (95.72%) (11150/11648)\n",
      "Epoch: 53 | Batch_idx: 100 |  Loss_1: (0.1232) | Acc_1: (95.72%) (12375/12928)\n",
      "Epoch: 53 | Batch_idx: 110 |  Loss_1: (0.1213) | Acc_1: (95.76%) (13605/14208)\n",
      "Epoch: 53 | Batch_idx: 120 |  Loss_1: (0.1221) | Acc_1: (95.74%) (14828/15488)\n",
      "Epoch: 53 | Batch_idx: 130 |  Loss_1: (0.1223) | Acc_1: (95.72%) (16050/16768)\n",
      "Epoch: 53 | Batch_idx: 140 |  Loss_1: (0.1213) | Acc_1: (95.74%) (17280/18048)\n",
      "Epoch: 53 | Batch_idx: 150 |  Loss_1: (0.1215) | Acc_1: (95.76%) (18508/19328)\n",
      "Epoch: 53 | Batch_idx: 160 |  Loss_1: (0.1204) | Acc_1: (95.79%) (19741/20608)\n",
      "Epoch: 53 | Batch_idx: 170 |  Loss_1: (0.1213) | Acc_1: (95.77%) (20962/21888)\n",
      "Epoch: 53 | Batch_idx: 180 |  Loss_1: (0.1206) | Acc_1: (95.81%) (22198/23168)\n",
      "Epoch: 53 | Batch_idx: 190 |  Loss_1: (0.1211) | Acc_1: (95.77%) (23415/24448)\n",
      "Epoch: 53 | Batch_idx: 200 |  Loss_1: (0.1204) | Acc_1: (95.81%) (24651/25728)\n",
      "Epoch: 53 | Batch_idx: 210 |  Loss_1: (0.1202) | Acc_1: (95.81%) (25877/27008)\n",
      "Epoch: 53 | Batch_idx: 220 |  Loss_1: (0.1200) | Acc_1: (95.80%) (27101/28288)\n",
      "Epoch: 53 | Batch_idx: 230 |  Loss_1: (0.1199) | Acc_1: (95.80%) (28326/29568)\n",
      "Epoch: 53 | Batch_idx: 240 |  Loss_1: (0.1197) | Acc_1: (95.80%) (29553/30848)\n",
      "Epoch: 53 | Batch_idx: 250 |  Loss_1: (0.1195) | Acc_1: (95.81%) (30781/32128)\n",
      "Epoch: 53 | Batch_idx: 260 |  Loss_1: (0.1198) | Acc_1: (95.81%) (32009/33408)\n",
      "Epoch: 53 | Batch_idx: 270 |  Loss_1: (0.1216) | Acc_1: (95.77%) (33221/34688)\n",
      "Epoch: 53 | Batch_idx: 280 |  Loss_1: (0.1222) | Acc_1: (95.75%) (34438/35968)\n",
      "Epoch: 53 | Batch_idx: 290 |  Loss_1: (0.1227) | Acc_1: (95.73%) (35657/37248)\n",
      "Epoch: 53 | Batch_idx: 300 |  Loss_1: (0.1225) | Acc_1: (95.74%) (36887/38528)\n",
      "Epoch: 53 | Batch_idx: 310 |  Loss_1: (0.1224) | Acc_1: (95.74%) (38111/39808)\n",
      "Epoch: 53 | Batch_idx: 320 |  Loss_1: (0.1236) | Acc_1: (95.70%) (39323/41088)\n",
      "Epoch: 53 | Batch_idx: 330 |  Loss_1: (0.1236) | Acc_1: (95.70%) (40545/42368)\n",
      "Epoch: 53 | Batch_idx: 340 |  Loss_1: (0.1241) | Acc_1: (95.67%) (41760/43648)\n",
      "Epoch: 53 | Batch_idx: 350 |  Loss_1: (0.1241) | Acc_1: (95.66%) (42979/44928)\n",
      "Epoch: 53 | Batch_idx: 360 |  Loss_1: (0.1237) | Acc_1: (95.67%) (44206/46208)\n",
      "Epoch: 53 | Batch_idx: 370 |  Loss_1: (0.1244) | Acc_1: (95.65%) (45420/47488)\n",
      "Epoch: 53 | Batch_idx: 380 |  Loss_1: (0.1247) | Acc_1: (95.64%) (46640/48768)\n",
      "Epoch: 53 | Batch_idx: 390 |  Loss_1: (0.1248) | Acc_1: (95.64%) (47819/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3843) | Acc: (90.18%) (9018/10000)\n",
      "Epoch: 54 | Batch_idx: 0 |  Loss_1: (0.0696) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 54 | Batch_idx: 10 |  Loss_1: (0.1122) | Acc_1: (96.09%) (1353/1408)\n",
      "Epoch: 54 | Batch_idx: 20 |  Loss_1: (0.1357) | Acc_1: (95.09%) (2556/2688)\n",
      "Epoch: 54 | Batch_idx: 30 |  Loss_1: (0.1305) | Acc_1: (95.19%) (3777/3968)\n",
      "Epoch: 54 | Batch_idx: 40 |  Loss_1: (0.1302) | Acc_1: (95.35%) (5004/5248)\n",
      "Epoch: 54 | Batch_idx: 50 |  Loss_1: (0.1316) | Acc_1: (95.33%) (6223/6528)\n",
      "Epoch: 54 | Batch_idx: 60 |  Loss_1: (0.1288) | Acc_1: (95.36%) (7446/7808)\n",
      "Epoch: 54 | Batch_idx: 70 |  Loss_1: (0.1261) | Acc_1: (95.53%) (8682/9088)\n",
      "Epoch: 54 | Batch_idx: 80 |  Loss_1: (0.1270) | Acc_1: (95.48%) (9899/10368)\n",
      "Epoch: 54 | Batch_idx: 90 |  Loss_1: (0.1278) | Acc_1: (95.45%) (11118/11648)\n",
      "Epoch: 54 | Batch_idx: 100 |  Loss_1: (0.1254) | Acc_1: (95.55%) (12353/12928)\n",
      "Epoch: 54 | Batch_idx: 110 |  Loss_1: (0.1243) | Acc_1: (95.57%) (13578/14208)\n",
      "Epoch: 54 | Batch_idx: 120 |  Loss_1: (0.1253) | Acc_1: (95.56%) (14801/15488)\n",
      "Epoch: 54 | Batch_idx: 130 |  Loss_1: (0.1265) | Acc_1: (95.52%) (16017/16768)\n",
      "Epoch: 54 | Batch_idx: 140 |  Loss_1: (0.1265) | Acc_1: (95.53%) (17241/18048)\n",
      "Epoch: 54 | Batch_idx: 150 |  Loss_1: (0.1267) | Acc_1: (95.50%) (18459/19328)\n",
      "Epoch: 54 | Batch_idx: 160 |  Loss_1: (0.1258) | Acc_1: (95.55%) (19690/20608)\n",
      "Epoch: 54 | Batch_idx: 170 |  Loss_1: (0.1250) | Acc_1: (95.55%) (20915/21888)\n",
      "Epoch: 54 | Batch_idx: 180 |  Loss_1: (0.1250) | Acc_1: (95.60%) (22148/23168)\n",
      "Epoch: 54 | Batch_idx: 190 |  Loss_1: (0.1251) | Acc_1: (95.61%) (23374/24448)\n",
      "Epoch: 54 | Batch_idx: 200 |  Loss_1: (0.1259) | Acc_1: (95.55%) (24582/25728)\n",
      "Epoch: 54 | Batch_idx: 210 |  Loss_1: (0.1248) | Acc_1: (95.58%) (25813/27008)\n",
      "Epoch: 54 | Batch_idx: 220 |  Loss_1: (0.1247) | Acc_1: (95.56%) (27031/28288)\n",
      "Epoch: 54 | Batch_idx: 230 |  Loss_1: (0.1264) | Acc_1: (95.49%) (28234/29568)\n",
      "Epoch: 54 | Batch_idx: 240 |  Loss_1: (0.1266) | Acc_1: (95.48%) (29454/30848)\n",
      "Epoch: 54 | Batch_idx: 250 |  Loss_1: (0.1279) | Acc_1: (95.44%) (30664/32128)\n",
      "Epoch: 54 | Batch_idx: 260 |  Loss_1: (0.1279) | Acc_1: (95.44%) (31883/33408)\n",
      "Epoch: 54 | Batch_idx: 270 |  Loss_1: (0.1281) | Acc_1: (95.44%) (33105/34688)\n",
      "Epoch: 54 | Batch_idx: 280 |  Loss_1: (0.1289) | Acc_1: (95.39%) (34310/35968)\n",
      "Epoch: 54 | Batch_idx: 290 |  Loss_1: (0.1282) | Acc_1: (95.43%) (35545/37248)\n",
      "Epoch: 54 | Batch_idx: 300 |  Loss_1: (0.1289) | Acc_1: (95.40%) (36756/38528)\n",
      "Epoch: 54 | Batch_idx: 310 |  Loss_1: (0.1287) | Acc_1: (95.41%) (37981/39808)\n",
      "Epoch: 54 | Batch_idx: 320 |  Loss_1: (0.1285) | Acc_1: (95.42%) (39206/41088)\n",
      "Epoch: 54 | Batch_idx: 330 |  Loss_1: (0.1283) | Acc_1: (95.42%) (40427/42368)\n",
      "Epoch: 54 | Batch_idx: 340 |  Loss_1: (0.1281) | Acc_1: (95.43%) (41653/43648)\n",
      "Epoch: 54 | Batch_idx: 350 |  Loss_1: (0.1276) | Acc_1: (95.45%) (42883/44928)\n",
      "Epoch: 54 | Batch_idx: 360 |  Loss_1: (0.1278) | Acc_1: (95.44%) (44099/46208)\n",
      "Epoch: 54 | Batch_idx: 370 |  Loss_1: (0.1290) | Acc_1: (95.39%) (45301/47488)\n",
      "Epoch: 54 | Batch_idx: 380 |  Loss_1: (0.1284) | Acc_1: (95.41%) (46530/48768)\n",
      "Epoch: 54 | Batch_idx: 390 |  Loss_1: (0.1284) | Acc_1: (95.43%) (47714/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3858) | Acc: (89.95%) (8995/10000)\n",
      "Epoch: 55 | Batch_idx: 0 |  Loss_1: (0.1773) | Acc_1: (92.97%) (119/128)\n",
      "Epoch: 55 | Batch_idx: 10 |  Loss_1: (0.1185) | Acc_1: (95.53%) (1345/1408)\n",
      "Epoch: 55 | Batch_idx: 20 |  Loss_1: (0.1106) | Acc_1: (95.98%) (2580/2688)\n",
      "Epoch: 55 | Batch_idx: 30 |  Loss_1: (0.1133) | Acc_1: (95.99%) (3809/3968)\n",
      "Epoch: 55 | Batch_idx: 40 |  Loss_1: (0.1189) | Acc_1: (95.79%) (5027/5248)\n",
      "Epoch: 55 | Batch_idx: 50 |  Loss_1: (0.1206) | Acc_1: (95.85%) (6257/6528)\n",
      "Epoch: 55 | Batch_idx: 60 |  Loss_1: (0.1227) | Acc_1: (95.74%) (7475/7808)\n",
      "Epoch: 55 | Batch_idx: 70 |  Loss_1: (0.1205) | Acc_1: (95.85%) (8711/9088)\n",
      "Epoch: 55 | Batch_idx: 80 |  Loss_1: (0.1204) | Acc_1: (95.92%) (9945/10368)\n",
      "Epoch: 55 | Batch_idx: 90 |  Loss_1: (0.1178) | Acc_1: (96.01%) (11183/11648)\n",
      "Epoch: 55 | Batch_idx: 100 |  Loss_1: (0.1181) | Acc_1: (95.95%) (12404/12928)\n",
      "Epoch: 55 | Batch_idx: 110 |  Loss_1: (0.1165) | Acc_1: (96.00%) (13640/14208)\n",
      "Epoch: 55 | Batch_idx: 120 |  Loss_1: (0.1161) | Acc_1: (96.01%) (14870/15488)\n",
      "Epoch: 55 | Batch_idx: 130 |  Loss_1: (0.1162) | Acc_1: (95.94%) (16088/16768)\n",
      "Epoch: 55 | Batch_idx: 140 |  Loss_1: (0.1160) | Acc_1: (95.96%) (17318/18048)\n",
      "Epoch: 55 | Batch_idx: 150 |  Loss_1: (0.1158) | Acc_1: (95.92%) (18539/19328)\n",
      "Epoch: 55 | Batch_idx: 160 |  Loss_1: (0.1158) | Acc_1: (95.95%) (19773/20608)\n",
      "Epoch: 55 | Batch_idx: 170 |  Loss_1: (0.1151) | Acc_1: (95.97%) (21007/21888)\n",
      "Epoch: 55 | Batch_idx: 180 |  Loss_1: (0.1156) | Acc_1: (95.99%) (22240/23168)\n",
      "Epoch: 55 | Batch_idx: 190 |  Loss_1: (0.1163) | Acc_1: (95.96%) (23461/24448)\n",
      "Epoch: 55 | Batch_idx: 200 |  Loss_1: (0.1177) | Acc_1: (95.91%) (24675/25728)\n",
      "Epoch: 55 | Batch_idx: 210 |  Loss_1: (0.1168) | Acc_1: (95.93%) (25908/27008)\n",
      "Epoch: 55 | Batch_idx: 220 |  Loss_1: (0.1176) | Acc_1: (95.89%) (27126/28288)\n",
      "Epoch: 55 | Batch_idx: 230 |  Loss_1: (0.1180) | Acc_1: (95.89%) (28353/29568)\n",
      "Epoch: 55 | Batch_idx: 240 |  Loss_1: (0.1180) | Acc_1: (95.90%) (29582/30848)\n",
      "Epoch: 55 | Batch_idx: 250 |  Loss_1: (0.1184) | Acc_1: (95.90%) (30810/32128)\n",
      "Epoch: 55 | Batch_idx: 260 |  Loss_1: (0.1179) | Acc_1: (95.91%) (32042/33408)\n",
      "Epoch: 55 | Batch_idx: 270 |  Loss_1: (0.1177) | Acc_1: (95.92%) (33271/34688)\n",
      "Epoch: 55 | Batch_idx: 280 |  Loss_1: (0.1180) | Acc_1: (95.90%) (34494/35968)\n",
      "Epoch: 55 | Batch_idx: 290 |  Loss_1: (0.1181) | Acc_1: (95.88%) (35714/37248)\n",
      "Epoch: 55 | Batch_idx: 300 |  Loss_1: (0.1180) | Acc_1: (95.89%) (36946/38528)\n",
      "Epoch: 55 | Batch_idx: 310 |  Loss_1: (0.1186) | Acc_1: (95.88%) (38167/39808)\n",
      "Epoch: 55 | Batch_idx: 320 |  Loss_1: (0.1189) | Acc_1: (95.86%) (39386/41088)\n",
      "Epoch: 55 | Batch_idx: 330 |  Loss_1: (0.1188) | Acc_1: (95.86%) (40616/42368)\n",
      "Epoch: 55 | Batch_idx: 340 |  Loss_1: (0.1193) | Acc_1: (95.85%) (41838/43648)\n",
      "Epoch: 55 | Batch_idx: 350 |  Loss_1: (0.1200) | Acc_1: (95.82%) (43048/44928)\n",
      "Epoch: 55 | Batch_idx: 360 |  Loss_1: (0.1209) | Acc_1: (95.79%) (44262/46208)\n",
      "Epoch: 55 | Batch_idx: 370 |  Loss_1: (0.1216) | Acc_1: (95.77%) (45479/47488)\n",
      "Epoch: 55 | Batch_idx: 380 |  Loss_1: (0.1209) | Acc_1: (95.79%) (46717/48768)\n",
      "Epoch: 55 | Batch_idx: 390 |  Loss_1: (0.1212) | Acc_1: (95.78%) (47891/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4004) | Acc: (89.60%) (8960/10000)\n",
      "Epoch: 56 | Batch_idx: 0 |  Loss_1: (0.1284) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 56 | Batch_idx: 10 |  Loss_1: (0.1107) | Acc_1: (96.45%) (1358/1408)\n",
      "Epoch: 56 | Batch_idx: 20 |  Loss_1: (0.1192) | Acc_1: (96.06%) (2582/2688)\n",
      "Epoch: 56 | Batch_idx: 30 |  Loss_1: (0.1166) | Acc_1: (96.17%) (3816/3968)\n",
      "Epoch: 56 | Batch_idx: 40 |  Loss_1: (0.1150) | Acc_1: (96.17%) (5047/5248)\n",
      "Epoch: 56 | Batch_idx: 50 |  Loss_1: (0.1108) | Acc_1: (96.35%) (6290/6528)\n",
      "Epoch: 56 | Batch_idx: 60 |  Loss_1: (0.1097) | Acc_1: (96.40%) (7527/7808)\n",
      "Epoch: 56 | Batch_idx: 70 |  Loss_1: (0.1080) | Acc_1: (96.39%) (8760/9088)\n",
      "Epoch: 56 | Batch_idx: 80 |  Loss_1: (0.1060) | Acc_1: (96.50%) (10005/10368)\n",
      "Epoch: 56 | Batch_idx: 90 |  Loss_1: (0.1040) | Acc_1: (96.53%) (11244/11648)\n",
      "Epoch: 56 | Batch_idx: 100 |  Loss_1: (0.1018) | Acc_1: (96.60%) (12488/12928)\n",
      "Epoch: 56 | Batch_idx: 110 |  Loss_1: (0.1026) | Acc_1: (96.57%) (13720/14208)\n",
      "Epoch: 56 | Batch_idx: 120 |  Loss_1: (0.1035) | Acc_1: (96.55%) (14953/15488)\n",
      "Epoch: 56 | Batch_idx: 130 |  Loss_1: (0.1043) | Acc_1: (96.52%) (16184/16768)\n",
      "Epoch: 56 | Batch_idx: 140 |  Loss_1: (0.1046) | Acc_1: (96.49%) (17414/18048)\n",
      "Epoch: 56 | Batch_idx: 150 |  Loss_1: (0.1067) | Acc_1: (96.40%) (18632/19328)\n",
      "Epoch: 56 | Batch_idx: 160 |  Loss_1: (0.1089) | Acc_1: (96.27%) (19840/20608)\n",
      "Epoch: 56 | Batch_idx: 170 |  Loss_1: (0.1093) | Acc_1: (96.26%) (21069/21888)\n",
      "Epoch: 56 | Batch_idx: 180 |  Loss_1: (0.1092) | Acc_1: (96.26%) (22301/23168)\n",
      "Epoch: 56 | Batch_idx: 190 |  Loss_1: (0.1089) | Acc_1: (96.26%) (23533/24448)\n",
      "Epoch: 56 | Batch_idx: 200 |  Loss_1: (0.1106) | Acc_1: (96.19%) (24748/25728)\n",
      "Epoch: 56 | Batch_idx: 210 |  Loss_1: (0.1105) | Acc_1: (96.18%) (25976/27008)\n",
      "Epoch: 56 | Batch_idx: 220 |  Loss_1: (0.1116) | Acc_1: (96.15%) (27200/28288)\n",
      "Epoch: 56 | Batch_idx: 230 |  Loss_1: (0.1116) | Acc_1: (96.15%) (28430/29568)\n",
      "Epoch: 56 | Batch_idx: 240 |  Loss_1: (0.1134) | Acc_1: (96.08%) (29640/30848)\n",
      "Epoch: 56 | Batch_idx: 250 |  Loss_1: (0.1127) | Acc_1: (96.09%) (30873/32128)\n",
      "Epoch: 56 | Batch_idx: 260 |  Loss_1: (0.1126) | Acc_1: (96.07%) (32094/33408)\n",
      "Epoch: 56 | Batch_idx: 270 |  Loss_1: (0.1129) | Acc_1: (96.07%) (33324/34688)\n",
      "Epoch: 56 | Batch_idx: 280 |  Loss_1: (0.1144) | Acc_1: (96.02%) (34536/35968)\n",
      "Epoch: 56 | Batch_idx: 290 |  Loss_1: (0.1146) | Acc_1: (96.01%) (35760/37248)\n",
      "Epoch: 56 | Batch_idx: 300 |  Loss_1: (0.1152) | Acc_1: (95.99%) (36983/38528)\n",
      "Epoch: 56 | Batch_idx: 310 |  Loss_1: (0.1152) | Acc_1: (96.00%) (38215/39808)\n",
      "Epoch: 56 | Batch_idx: 320 |  Loss_1: (0.1153) | Acc_1: (95.99%) (39441/41088)\n",
      "Epoch: 56 | Batch_idx: 330 |  Loss_1: (0.1153) | Acc_1: (95.98%) (40664/42368)\n",
      "Epoch: 56 | Batch_idx: 340 |  Loss_1: (0.1146) | Acc_1: (96.00%) (41903/43648)\n",
      "Epoch: 56 | Batch_idx: 350 |  Loss_1: (0.1154) | Acc_1: (95.99%) (43128/44928)\n",
      "Epoch: 56 | Batch_idx: 360 |  Loss_1: (0.1152) | Acc_1: (95.99%) (44355/46208)\n",
      "Epoch: 56 | Batch_idx: 370 |  Loss_1: (0.1151) | Acc_1: (95.99%) (45586/47488)\n",
      "Epoch: 56 | Batch_idx: 380 |  Loss_1: (0.1161) | Acc_1: (95.95%) (46795/48768)\n",
      "Epoch: 56 | Batch_idx: 390 |  Loss_1: (0.1167) | Acc_1: (95.93%) (47967/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4347) | Acc: (88.71%) (8871/10000)\n",
      "Epoch: 57 | Batch_idx: 0 |  Loss_1: (0.0930) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 57 | Batch_idx: 10 |  Loss_1: (0.1160) | Acc_1: (95.53%) (1345/1408)\n",
      "Epoch: 57 | Batch_idx: 20 |  Loss_1: (0.1049) | Acc_1: (95.94%) (2579/2688)\n",
      "Epoch: 57 | Batch_idx: 30 |  Loss_1: (0.1057) | Acc_1: (96.22%) (3818/3968)\n",
      "Epoch: 57 | Batch_idx: 40 |  Loss_1: (0.1069) | Acc_1: (96.25%) (5051/5248)\n",
      "Epoch: 57 | Batch_idx: 50 |  Loss_1: (0.1052) | Acc_1: (96.40%) (6293/6528)\n",
      "Epoch: 57 | Batch_idx: 60 |  Loss_1: (0.1063) | Acc_1: (96.35%) (7523/7808)\n",
      "Epoch: 57 | Batch_idx: 70 |  Loss_1: (0.1058) | Acc_1: (96.32%) (8754/9088)\n",
      "Epoch: 57 | Batch_idx: 80 |  Loss_1: (0.1042) | Acc_1: (96.39%) (9994/10368)\n",
      "Epoch: 57 | Batch_idx: 90 |  Loss_1: (0.1060) | Acc_1: (96.32%) (11219/11648)\n",
      "Epoch: 57 | Batch_idx: 100 |  Loss_1: (0.1066) | Acc_1: (96.32%) (12452/12928)\n",
      "Epoch: 57 | Batch_idx: 110 |  Loss_1: (0.1097) | Acc_1: (96.22%) (13671/14208)\n",
      "Epoch: 57 | Batch_idx: 120 |  Loss_1: (0.1086) | Acc_1: (96.27%) (14910/15488)\n",
      "Epoch: 57 | Batch_idx: 130 |  Loss_1: (0.1101) | Acc_1: (96.22%) (16135/16768)\n",
      "Epoch: 57 | Batch_idx: 140 |  Loss_1: (0.1102) | Acc_1: (96.20%) (17363/18048)\n",
      "Epoch: 57 | Batch_idx: 150 |  Loss_1: (0.1108) | Acc_1: (96.20%) (18593/19328)\n",
      "Epoch: 57 | Batch_idx: 160 |  Loss_1: (0.1098) | Acc_1: (96.23%) (19832/20608)\n",
      "Epoch: 57 | Batch_idx: 170 |  Loss_1: (0.1108) | Acc_1: (96.22%) (21060/21888)\n",
      "Epoch: 57 | Batch_idx: 180 |  Loss_1: (0.1126) | Acc_1: (96.15%) (22276/23168)\n",
      "Epoch: 57 | Batch_idx: 190 |  Loss_1: (0.1147) | Acc_1: (96.10%) (23494/24448)\n",
      "Epoch: 57 | Batch_idx: 200 |  Loss_1: (0.1155) | Acc_1: (96.07%) (24716/25728)\n",
      "Epoch: 57 | Batch_idx: 210 |  Loss_1: (0.1156) | Acc_1: (96.05%) (25942/27008)\n",
      "Epoch: 57 | Batch_idx: 220 |  Loss_1: (0.1152) | Acc_1: (96.07%) (27177/28288)\n",
      "Epoch: 57 | Batch_idx: 230 |  Loss_1: (0.1154) | Acc_1: (96.06%) (28404/29568)\n",
      "Epoch: 57 | Batch_idx: 240 |  Loss_1: (0.1152) | Acc_1: (96.08%) (29640/30848)\n",
      "Epoch: 57 | Batch_idx: 250 |  Loss_1: (0.1159) | Acc_1: (96.04%) (30857/32128)\n",
      "Epoch: 57 | Batch_idx: 260 |  Loss_1: (0.1161) | Acc_1: (96.03%) (32083/33408)\n",
      "Epoch: 57 | Batch_idx: 270 |  Loss_1: (0.1153) | Acc_1: (96.06%) (33323/34688)\n",
      "Epoch: 57 | Batch_idx: 280 |  Loss_1: (0.1158) | Acc_1: (96.03%) (34541/35968)\n",
      "Epoch: 57 | Batch_idx: 290 |  Loss_1: (0.1161) | Acc_1: (96.01%) (35760/37248)\n",
      "Epoch: 57 | Batch_idx: 300 |  Loss_1: (0.1157) | Acc_1: (96.01%) (36991/38528)\n",
      "Epoch: 57 | Batch_idx: 310 |  Loss_1: (0.1161) | Acc_1: (95.99%) (38212/39808)\n",
      "Epoch: 57 | Batch_idx: 320 |  Loss_1: (0.1167) | Acc_1: (95.95%) (39422/41088)\n",
      "Epoch: 57 | Batch_idx: 330 |  Loss_1: (0.1160) | Acc_1: (95.95%) (40650/42368)\n",
      "Epoch: 57 | Batch_idx: 340 |  Loss_1: (0.1154) | Acc_1: (95.97%) (41891/43648)\n",
      "Epoch: 57 | Batch_idx: 350 |  Loss_1: (0.1159) | Acc_1: (95.96%) (43111/44928)\n",
      "Epoch: 57 | Batch_idx: 360 |  Loss_1: (0.1163) | Acc_1: (95.94%) (44330/46208)\n",
      "Epoch: 57 | Batch_idx: 370 |  Loss_1: (0.1166) | Acc_1: (95.93%) (45554/47488)\n",
      "Epoch: 57 | Batch_idx: 380 |  Loss_1: (0.1169) | Acc_1: (95.92%) (46778/48768)\n",
      "Epoch: 57 | Batch_idx: 390 |  Loss_1: (0.1171) | Acc_1: (95.92%) (47961/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4340) | Acc: (89.18%) (8918/10000)\n",
      "Epoch: 58 | Batch_idx: 0 |  Loss_1: (0.0882) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 58 | Batch_idx: 10 |  Loss_1: (0.1322) | Acc_1: (95.24%) (1341/1408)\n",
      "Epoch: 58 | Batch_idx: 20 |  Loss_1: (0.1186) | Acc_1: (95.68%) (2572/2688)\n",
      "Epoch: 58 | Batch_idx: 30 |  Loss_1: (0.1149) | Acc_1: (96.02%) (3810/3968)\n",
      "Epoch: 58 | Batch_idx: 40 |  Loss_1: (0.1063) | Acc_1: (96.32%) (5055/5248)\n",
      "Epoch: 58 | Batch_idx: 50 |  Loss_1: (0.1040) | Acc_1: (96.38%) (6292/6528)\n",
      "Epoch: 58 | Batch_idx: 60 |  Loss_1: (0.1016) | Acc_1: (96.44%) (7530/7808)\n",
      "Epoch: 58 | Batch_idx: 70 |  Loss_1: (0.1022) | Acc_1: (96.39%) (8760/9088)\n",
      "Epoch: 58 | Batch_idx: 80 |  Loss_1: (0.1003) | Acc_1: (96.48%) (10003/10368)\n",
      "Epoch: 58 | Batch_idx: 90 |  Loss_1: (0.1001) | Acc_1: (96.45%) (11234/11648)\n",
      "Epoch: 58 | Batch_idx: 100 |  Loss_1: (0.1004) | Acc_1: (96.51%) (12477/12928)\n",
      "Epoch: 58 | Batch_idx: 110 |  Loss_1: (0.1003) | Acc_1: (96.52%) (13714/14208)\n",
      "Epoch: 58 | Batch_idx: 120 |  Loss_1: (0.0998) | Acc_1: (96.53%) (14950/15488)\n",
      "Epoch: 58 | Batch_idx: 130 |  Loss_1: (0.1008) | Acc_1: (96.46%) (16175/16768)\n",
      "Epoch: 58 | Batch_idx: 140 |  Loss_1: (0.1015) | Acc_1: (96.46%) (17409/18048)\n",
      "Epoch: 58 | Batch_idx: 150 |  Loss_1: (0.1019) | Acc_1: (96.45%) (18642/19328)\n",
      "Epoch: 58 | Batch_idx: 160 |  Loss_1: (0.1031) | Acc_1: (96.41%) (19868/20608)\n",
      "Epoch: 58 | Batch_idx: 170 |  Loss_1: (0.1041) | Acc_1: (96.38%) (21095/21888)\n",
      "Epoch: 58 | Batch_idx: 180 |  Loss_1: (0.1029) | Acc_1: (96.41%) (22337/23168)\n",
      "Epoch: 58 | Batch_idx: 190 |  Loss_1: (0.1026) | Acc_1: (96.41%) (23570/24448)\n",
      "Epoch: 58 | Batch_idx: 200 |  Loss_1: (0.1026) | Acc_1: (96.41%) (24804/25728)\n",
      "Epoch: 58 | Batch_idx: 210 |  Loss_1: (0.1033) | Acc_1: (96.38%) (26029/27008)\n",
      "Epoch: 58 | Batch_idx: 220 |  Loss_1: (0.1030) | Acc_1: (96.40%) (27271/28288)\n",
      "Epoch: 58 | Batch_idx: 230 |  Loss_1: (0.1030) | Acc_1: (96.41%) (28506/29568)\n",
      "Epoch: 58 | Batch_idx: 240 |  Loss_1: (0.1042) | Acc_1: (96.36%) (29725/30848)\n",
      "Epoch: 58 | Batch_idx: 250 |  Loss_1: (0.1047) | Acc_1: (96.35%) (30954/32128)\n",
      "Epoch: 58 | Batch_idx: 260 |  Loss_1: (0.1053) | Acc_1: (96.31%) (32175/33408)\n",
      "Epoch: 58 | Batch_idx: 270 |  Loss_1: (0.1055) | Acc_1: (96.30%) (33406/34688)\n",
      "Epoch: 58 | Batch_idx: 280 |  Loss_1: (0.1054) | Acc_1: (96.31%) (34639/35968)\n",
      "Epoch: 58 | Batch_idx: 290 |  Loss_1: (0.1053) | Acc_1: (96.31%) (35872/37248)\n",
      "Epoch: 58 | Batch_idx: 300 |  Loss_1: (0.1052) | Acc_1: (96.30%) (37102/38528)\n",
      "Epoch: 58 | Batch_idx: 310 |  Loss_1: (0.1048) | Acc_1: (96.30%) (38337/39808)\n",
      "Epoch: 58 | Batch_idx: 320 |  Loss_1: (0.1050) | Acc_1: (96.31%) (39572/41088)\n",
      "Epoch: 58 | Batch_idx: 330 |  Loss_1: (0.1049) | Acc_1: (96.31%) (40803/42368)\n",
      "Epoch: 58 | Batch_idx: 340 |  Loss_1: (0.1048) | Acc_1: (96.30%) (42034/43648)\n",
      "Epoch: 58 | Batch_idx: 350 |  Loss_1: (0.1054) | Acc_1: (96.28%) (43257/44928)\n",
      "Epoch: 58 | Batch_idx: 360 |  Loss_1: (0.1059) | Acc_1: (96.25%) (44475/46208)\n",
      "Epoch: 58 | Batch_idx: 370 |  Loss_1: (0.1058) | Acc_1: (96.24%) (45703/47488)\n",
      "Epoch: 58 | Batch_idx: 380 |  Loss_1: (0.1062) | Acc_1: (96.23%) (46930/48768)\n",
      "Epoch: 58 | Batch_idx: 390 |  Loss_1: (0.1062) | Acc_1: (96.25%) (48123/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4573) | Acc: (89.30%) (8930/10000)\n",
      "Epoch: 59 | Batch_idx: 0 |  Loss_1: (0.1506) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 59 | Batch_idx: 10 |  Loss_1: (0.1033) | Acc_1: (96.31%) (1356/1408)\n",
      "Epoch: 59 | Batch_idx: 20 |  Loss_1: (0.1055) | Acc_1: (96.06%) (2582/2688)\n",
      "Epoch: 59 | Batch_idx: 30 |  Loss_1: (0.1140) | Acc_1: (95.97%) (3808/3968)\n",
      "Epoch: 59 | Batch_idx: 40 |  Loss_1: (0.1137) | Acc_1: (95.96%) (5036/5248)\n",
      "Epoch: 59 | Batch_idx: 50 |  Loss_1: (0.1107) | Acc_1: (96.19%) (6279/6528)\n",
      "Epoch: 59 | Batch_idx: 60 |  Loss_1: (0.1125) | Acc_1: (96.16%) (7508/7808)\n",
      "Epoch: 59 | Batch_idx: 70 |  Loss_1: (0.1125) | Acc_1: (96.10%) (8734/9088)\n",
      "Epoch: 59 | Batch_idx: 80 |  Loss_1: (0.1108) | Acc_1: (96.13%) (9967/10368)\n",
      "Epoch: 59 | Batch_idx: 90 |  Loss_1: (0.1103) | Acc_1: (96.15%) (11200/11648)\n",
      "Epoch: 59 | Batch_idx: 100 |  Loss_1: (0.1113) | Acc_1: (96.16%) (12431/12928)\n",
      "Epoch: 59 | Batch_idx: 110 |  Loss_1: (0.1117) | Acc_1: (96.14%) (13660/14208)\n",
      "Epoch: 59 | Batch_idx: 120 |  Loss_1: (0.1128) | Acc_1: (96.12%) (14887/15488)\n",
      "Epoch: 59 | Batch_idx: 130 |  Loss_1: (0.1128) | Acc_1: (96.12%) (16118/16768)\n",
      "Epoch: 59 | Batch_idx: 140 |  Loss_1: (0.1134) | Acc_1: (96.10%) (17345/18048)\n",
      "Epoch: 59 | Batch_idx: 150 |  Loss_1: (0.1121) | Acc_1: (96.11%) (18577/19328)\n",
      "Epoch: 59 | Batch_idx: 160 |  Loss_1: (0.1119) | Acc_1: (96.11%) (19807/20608)\n",
      "Epoch: 59 | Batch_idx: 170 |  Loss_1: (0.1117) | Acc_1: (96.13%) (21042/21888)\n",
      "Epoch: 59 | Batch_idx: 180 |  Loss_1: (0.1131) | Acc_1: (96.08%) (22259/23168)\n",
      "Epoch: 59 | Batch_idx: 190 |  Loss_1: (0.1141) | Acc_1: (96.02%) (23476/24448)\n",
      "Epoch: 59 | Batch_idx: 200 |  Loss_1: (0.1146) | Acc_1: (96.01%) (24702/25728)\n",
      "Epoch: 59 | Batch_idx: 210 |  Loss_1: (0.1147) | Acc_1: (96.01%) (25930/27008)\n",
      "Epoch: 59 | Batch_idx: 220 |  Loss_1: (0.1150) | Acc_1: (95.99%) (27154/28288)\n",
      "Epoch: 59 | Batch_idx: 230 |  Loss_1: (0.1145) | Acc_1: (96.02%) (28391/29568)\n",
      "Epoch: 59 | Batch_idx: 240 |  Loss_1: (0.1152) | Acc_1: (96.00%) (29613/30848)\n",
      "Epoch: 59 | Batch_idx: 250 |  Loss_1: (0.1161) | Acc_1: (95.96%) (30831/32128)\n",
      "Epoch: 59 | Batch_idx: 260 |  Loss_1: (0.1164) | Acc_1: (95.97%) (32063/33408)\n",
      "Epoch: 59 | Batch_idx: 270 |  Loss_1: (0.1164) | Acc_1: (95.96%) (33286/34688)\n",
      "Epoch: 59 | Batch_idx: 280 |  Loss_1: (0.1163) | Acc_1: (95.94%) (34509/35968)\n",
      "Epoch: 59 | Batch_idx: 290 |  Loss_1: (0.1164) | Acc_1: (95.95%) (35739/37248)\n",
      "Epoch: 59 | Batch_idx: 300 |  Loss_1: (0.1159) | Acc_1: (95.96%) (36972/38528)\n",
      "Epoch: 59 | Batch_idx: 310 |  Loss_1: (0.1156) | Acc_1: (95.96%) (38199/39808)\n",
      "Epoch: 59 | Batch_idx: 320 |  Loss_1: (0.1154) | Acc_1: (95.98%) (39435/41088)\n",
      "Epoch: 59 | Batch_idx: 330 |  Loss_1: (0.1156) | Acc_1: (95.98%) (40663/42368)\n",
      "Epoch: 59 | Batch_idx: 340 |  Loss_1: (0.1158) | Acc_1: (95.97%) (41888/43648)\n",
      "Epoch: 59 | Batch_idx: 350 |  Loss_1: (0.1160) | Acc_1: (95.98%) (43121/44928)\n",
      "Epoch: 59 | Batch_idx: 360 |  Loss_1: (0.1157) | Acc_1: (95.98%) (44352/46208)\n",
      "Epoch: 59 | Batch_idx: 370 |  Loss_1: (0.1149) | Acc_1: (96.00%) (45587/47488)\n",
      "Epoch: 59 | Batch_idx: 380 |  Loss_1: (0.1149) | Acc_1: (96.00%) (46817/48768)\n",
      "Epoch: 59 | Batch_idx: 390 |  Loss_1: (0.1148) | Acc_1: (95.99%) (47996/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4186) | Acc: (89.40%) (8940/10000)\n",
      "Epoch: 60 | Batch_idx: 0 |  Loss_1: (0.1514) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 60 | Batch_idx: 10 |  Loss_1: (0.1113) | Acc_1: (96.52%) (1359/1408)\n",
      "Epoch: 60 | Batch_idx: 20 |  Loss_1: (0.1031) | Acc_1: (96.84%) (2603/2688)\n",
      "Epoch: 60 | Batch_idx: 30 |  Loss_1: (0.1047) | Acc_1: (96.55%) (3831/3968)\n",
      "Epoch: 60 | Batch_idx: 40 |  Loss_1: (0.1064) | Acc_1: (96.34%) (5056/5248)\n",
      "Epoch: 60 | Batch_idx: 50 |  Loss_1: (0.1061) | Acc_1: (96.35%) (6290/6528)\n",
      "Epoch: 60 | Batch_idx: 60 |  Loss_1: (0.1031) | Acc_1: (96.48%) (7533/7808)\n",
      "Epoch: 60 | Batch_idx: 70 |  Loss_1: (0.1024) | Acc_1: (96.46%) (8766/9088)\n",
      "Epoch: 60 | Batch_idx: 80 |  Loss_1: (0.1014) | Acc_1: (96.48%) (10003/10368)\n",
      "Epoch: 60 | Batch_idx: 90 |  Loss_1: (0.1025) | Acc_1: (96.50%) (11240/11648)\n",
      "Epoch: 60 | Batch_idx: 100 |  Loss_1: (0.1007) | Acc_1: (96.59%) (12487/12928)\n",
      "Epoch: 60 | Batch_idx: 110 |  Loss_1: (0.1018) | Acc_1: (96.54%) (13716/14208)\n",
      "Epoch: 60 | Batch_idx: 120 |  Loss_1: (0.1018) | Acc_1: (96.55%) (14954/15488)\n",
      "Epoch: 60 | Batch_idx: 130 |  Loss_1: (0.1015) | Acc_1: (96.56%) (16191/16768)\n",
      "Epoch: 60 | Batch_idx: 140 |  Loss_1: (0.1014) | Acc_1: (96.56%) (17427/18048)\n",
      "Epoch: 60 | Batch_idx: 150 |  Loss_1: (0.1030) | Acc_1: (96.51%) (18653/19328)\n",
      "Epoch: 60 | Batch_idx: 160 |  Loss_1: (0.1012) | Acc_1: (96.56%) (19899/20608)\n",
      "Epoch: 60 | Batch_idx: 170 |  Loss_1: (0.1009) | Acc_1: (96.54%) (21131/21888)\n",
      "Epoch: 60 | Batch_idx: 180 |  Loss_1: (0.1008) | Acc_1: (96.53%) (22363/23168)\n",
      "Epoch: 60 | Batch_idx: 190 |  Loss_1: (0.1015) | Acc_1: (96.52%) (23598/24448)\n",
      "Epoch: 60 | Batch_idx: 200 |  Loss_1: (0.1026) | Acc_1: (96.48%) (24823/25728)\n",
      "Epoch: 60 | Batch_idx: 210 |  Loss_1: (0.1024) | Acc_1: (96.49%) (26060/27008)\n",
      "Epoch: 60 | Batch_idx: 220 |  Loss_1: (0.1021) | Acc_1: (96.48%) (27292/28288)\n",
      "Epoch: 60 | Batch_idx: 230 |  Loss_1: (0.1024) | Acc_1: (96.44%) (28516/29568)\n",
      "Epoch: 60 | Batch_idx: 240 |  Loss_1: (0.1034) | Acc_1: (96.41%) (29742/30848)\n",
      "Epoch: 60 | Batch_idx: 250 |  Loss_1: (0.1037) | Acc_1: (96.40%) (30971/32128)\n",
      "Epoch: 60 | Batch_idx: 260 |  Loss_1: (0.1039) | Acc_1: (96.41%) (32207/33408)\n",
      "Epoch: 60 | Batch_idx: 270 |  Loss_1: (0.1044) | Acc_1: (96.38%) (33433/34688)\n",
      "Epoch: 60 | Batch_idx: 280 |  Loss_1: (0.1044) | Acc_1: (96.38%) (34667/35968)\n",
      "Epoch: 60 | Batch_idx: 290 |  Loss_1: (0.1045) | Acc_1: (96.38%) (35899/37248)\n",
      "Epoch: 60 | Batch_idx: 300 |  Loss_1: (0.1045) | Acc_1: (96.38%) (37134/38528)\n",
      "Epoch: 60 | Batch_idx: 310 |  Loss_1: (0.1047) | Acc_1: (96.37%) (38362/39808)\n",
      "Epoch: 60 | Batch_idx: 320 |  Loss_1: (0.1052) | Acc_1: (96.35%) (39587/41088)\n",
      "Epoch: 60 | Batch_idx: 330 |  Loss_1: (0.1053) | Acc_1: (96.34%) (40819/42368)\n",
      "Epoch: 60 | Batch_idx: 340 |  Loss_1: (0.1047) | Acc_1: (96.36%) (42058/43648)\n",
      "Epoch: 60 | Batch_idx: 350 |  Loss_1: (0.1051) | Acc_1: (96.35%) (43286/44928)\n",
      "Epoch: 60 | Batch_idx: 360 |  Loss_1: (0.1050) | Acc_1: (96.34%) (44517/46208)\n",
      "Epoch: 60 | Batch_idx: 370 |  Loss_1: (0.1052) | Acc_1: (96.34%) (45751/47488)\n",
      "Epoch: 60 | Batch_idx: 380 |  Loss_1: (0.1053) | Acc_1: (96.35%) (46987/48768)\n",
      "Epoch: 60 | Batch_idx: 390 |  Loss_1: (0.1060) | Acc_1: (96.31%) (48157/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4304) | Acc: (89.69%) (8969/10000)\n",
      "Epoch: 61 | Batch_idx: 0 |  Loss_1: (0.1392) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 61 | Batch_idx: 10 |  Loss_1: (0.0972) | Acc_1: (96.80%) (1363/1408)\n",
      "Epoch: 61 | Batch_idx: 20 |  Loss_1: (0.1028) | Acc_1: (96.50%) (2594/2688)\n",
      "Epoch: 61 | Batch_idx: 30 |  Loss_1: (0.0946) | Acc_1: (96.85%) (3843/3968)\n",
      "Epoch: 61 | Batch_idx: 40 |  Loss_1: (0.0954) | Acc_1: (96.74%) (5077/5248)\n",
      "Epoch: 61 | Batch_idx: 50 |  Loss_1: (0.0944) | Acc_1: (96.83%) (6321/6528)\n",
      "Epoch: 61 | Batch_idx: 60 |  Loss_1: (0.0942) | Acc_1: (96.82%) (7560/7808)\n",
      "Epoch: 61 | Batch_idx: 70 |  Loss_1: (0.0962) | Acc_1: (96.70%) (8788/9088)\n",
      "Epoch: 61 | Batch_idx: 80 |  Loss_1: (0.0953) | Acc_1: (96.74%) (10030/10368)\n",
      "Epoch: 61 | Batch_idx: 90 |  Loss_1: (0.0961) | Acc_1: (96.75%) (11270/11648)\n",
      "Epoch: 61 | Batch_idx: 100 |  Loss_1: (0.0956) | Acc_1: (96.75%) (12508/12928)\n",
      "Epoch: 61 | Batch_idx: 110 |  Loss_1: (0.0962) | Acc_1: (96.73%) (13744/14208)\n",
      "Epoch: 61 | Batch_idx: 120 |  Loss_1: (0.0950) | Acc_1: (96.76%) (14986/15488)\n",
      "Epoch: 61 | Batch_idx: 130 |  Loss_1: (0.0952) | Acc_1: (96.72%) (16218/16768)\n",
      "Epoch: 61 | Batch_idx: 140 |  Loss_1: (0.0945) | Acc_1: (96.74%) (17459/18048)\n",
      "Epoch: 61 | Batch_idx: 150 |  Loss_1: (0.0936) | Acc_1: (96.79%) (18708/19328)\n",
      "Epoch: 61 | Batch_idx: 160 |  Loss_1: (0.0955) | Acc_1: (96.74%) (19937/20608)\n",
      "Epoch: 61 | Batch_idx: 170 |  Loss_1: (0.0961) | Acc_1: (96.72%) (21170/21888)\n",
      "Epoch: 61 | Batch_idx: 180 |  Loss_1: (0.0956) | Acc_1: (96.74%) (22412/23168)\n",
      "Epoch: 61 | Batch_idx: 190 |  Loss_1: (0.0963) | Acc_1: (96.72%) (23645/24448)\n",
      "Epoch: 61 | Batch_idx: 200 |  Loss_1: (0.0964) | Acc_1: (96.68%) (24875/25728)\n",
      "Epoch: 61 | Batch_idx: 210 |  Loss_1: (0.0966) | Acc_1: (96.67%) (26108/27008)\n",
      "Epoch: 61 | Batch_idx: 220 |  Loss_1: (0.0969) | Acc_1: (96.65%) (27339/28288)\n",
      "Epoch: 61 | Batch_idx: 230 |  Loss_1: (0.0972) | Acc_1: (96.63%) (28573/29568)\n",
      "Epoch: 61 | Batch_idx: 240 |  Loss_1: (0.0977) | Acc_1: (96.62%) (29804/30848)\n",
      "Epoch: 61 | Batch_idx: 250 |  Loss_1: (0.0974) | Acc_1: (96.63%) (31046/32128)\n",
      "Epoch: 61 | Batch_idx: 260 |  Loss_1: (0.0987) | Acc_1: (96.60%) (32271/33408)\n",
      "Epoch: 61 | Batch_idx: 270 |  Loss_1: (0.0996) | Acc_1: (96.55%) (33491/34688)\n",
      "Epoch: 61 | Batch_idx: 280 |  Loss_1: (0.0997) | Acc_1: (96.55%) (34728/35968)\n",
      "Epoch: 61 | Batch_idx: 290 |  Loss_1: (0.1000) | Acc_1: (96.53%) (35955/37248)\n",
      "Epoch: 61 | Batch_idx: 300 |  Loss_1: (0.0997) | Acc_1: (96.53%) (37191/38528)\n",
      "Epoch: 61 | Batch_idx: 310 |  Loss_1: (0.0999) | Acc_1: (96.52%) (38424/39808)\n",
      "Epoch: 61 | Batch_idx: 320 |  Loss_1: (0.1005) | Acc_1: (96.52%) (39660/41088)\n",
      "Epoch: 61 | Batch_idx: 330 |  Loss_1: (0.1006) | Acc_1: (96.53%) (40896/42368)\n",
      "Epoch: 61 | Batch_idx: 340 |  Loss_1: (0.1009) | Acc_1: (96.50%) (42122/43648)\n",
      "Epoch: 61 | Batch_idx: 350 |  Loss_1: (0.1012) | Acc_1: (96.50%) (43356/44928)\n",
      "Epoch: 61 | Batch_idx: 360 |  Loss_1: (0.1013) | Acc_1: (96.49%) (44588/46208)\n",
      "Epoch: 61 | Batch_idx: 370 |  Loss_1: (0.1016) | Acc_1: (96.48%) (45816/47488)\n",
      "Epoch: 61 | Batch_idx: 380 |  Loss_1: (0.1019) | Acc_1: (96.47%) (47048/48768)\n",
      "Epoch: 61 | Batch_idx: 390 |  Loss_1: (0.1017) | Acc_1: (96.48%) (48241/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3929) | Acc: (90.34%) (9034/10000)\n",
      "Epoch: 62 | Batch_idx: 0 |  Loss_1: (0.0491) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 62 | Batch_idx: 10 |  Loss_1: (0.1079) | Acc_1: (96.31%) (1356/1408)\n",
      "Epoch: 62 | Batch_idx: 20 |  Loss_1: (0.1061) | Acc_1: (96.39%) (2591/2688)\n",
      "Epoch: 62 | Batch_idx: 30 |  Loss_1: (0.1021) | Acc_1: (96.50%) (3829/3968)\n",
      "Epoch: 62 | Batch_idx: 40 |  Loss_1: (0.1005) | Acc_1: (96.57%) (5068/5248)\n",
      "Epoch: 62 | Batch_idx: 50 |  Loss_1: (0.1030) | Acc_1: (96.48%) (6298/6528)\n",
      "Epoch: 62 | Batch_idx: 60 |  Loss_1: (0.0992) | Acc_1: (96.58%) (7541/7808)\n",
      "Epoch: 62 | Batch_idx: 70 |  Loss_1: (0.0999) | Acc_1: (96.57%) (8776/9088)\n",
      "Epoch: 62 | Batch_idx: 80 |  Loss_1: (0.1014) | Acc_1: (96.58%) (10013/10368)\n",
      "Epoch: 62 | Batch_idx: 90 |  Loss_1: (0.1003) | Acc_1: (96.63%) (11256/11648)\n",
      "Epoch: 62 | Batch_idx: 100 |  Loss_1: (0.1004) | Acc_1: (96.60%) (12488/12928)\n",
      "Epoch: 62 | Batch_idx: 110 |  Loss_1: (0.0989) | Acc_1: (96.62%) (13728/14208)\n",
      "Epoch: 62 | Batch_idx: 120 |  Loss_1: (0.0983) | Acc_1: (96.59%) (14960/15488)\n",
      "Epoch: 62 | Batch_idx: 130 |  Loss_1: (0.0967) | Acc_1: (96.62%) (16201/16768)\n",
      "Epoch: 62 | Batch_idx: 140 |  Loss_1: (0.0972) | Acc_1: (96.59%) (17432/18048)\n",
      "Epoch: 62 | Batch_idx: 150 |  Loss_1: (0.0978) | Acc_1: (96.56%) (18664/19328)\n",
      "Epoch: 62 | Batch_idx: 160 |  Loss_1: (0.0977) | Acc_1: (96.57%) (19901/20608)\n",
      "Epoch: 62 | Batch_idx: 170 |  Loss_1: (0.0985) | Acc_1: (96.51%) (21124/21888)\n",
      "Epoch: 62 | Batch_idx: 180 |  Loss_1: (0.0987) | Acc_1: (96.50%) (22358/23168)\n",
      "Epoch: 62 | Batch_idx: 190 |  Loss_1: (0.0986) | Acc_1: (96.52%) (23597/24448)\n",
      "Epoch: 62 | Batch_idx: 200 |  Loss_1: (0.0982) | Acc_1: (96.53%) (24836/25728)\n",
      "Epoch: 62 | Batch_idx: 210 |  Loss_1: (0.0980) | Acc_1: (96.53%) (26072/27008)\n",
      "Epoch: 62 | Batch_idx: 220 |  Loss_1: (0.0987) | Acc_1: (96.48%) (27293/28288)\n",
      "Epoch: 62 | Batch_idx: 230 |  Loss_1: (0.0992) | Acc_1: (96.46%) (28522/29568)\n",
      "Epoch: 62 | Batch_idx: 240 |  Loss_1: (0.1001) | Acc_1: (96.43%) (29747/30848)\n",
      "Epoch: 62 | Batch_idx: 250 |  Loss_1: (0.1001) | Acc_1: (96.45%) (30988/32128)\n",
      "Epoch: 62 | Batch_idx: 260 |  Loss_1: (0.1001) | Acc_1: (96.46%) (32224/33408)\n",
      "Epoch: 62 | Batch_idx: 270 |  Loss_1: (0.1005) | Acc_1: (96.45%) (33455/34688)\n",
      "Epoch: 62 | Batch_idx: 280 |  Loss_1: (0.1006) | Acc_1: (96.45%) (34691/35968)\n",
      "Epoch: 62 | Batch_idx: 290 |  Loss_1: (0.1010) | Acc_1: (96.43%) (35919/37248)\n",
      "Epoch: 62 | Batch_idx: 300 |  Loss_1: (0.1004) | Acc_1: (96.46%) (37163/38528)\n",
      "Epoch: 62 | Batch_idx: 310 |  Loss_1: (0.1006) | Acc_1: (96.45%) (38394/39808)\n",
      "Epoch: 62 | Batch_idx: 320 |  Loss_1: (0.1001) | Acc_1: (96.47%) (39636/41088)\n",
      "Epoch: 62 | Batch_idx: 330 |  Loss_1: (0.1002) | Acc_1: (96.46%) (40869/42368)\n",
      "Epoch: 62 | Batch_idx: 340 |  Loss_1: (0.0998) | Acc_1: (96.46%) (42103/43648)\n",
      "Epoch: 62 | Batch_idx: 350 |  Loss_1: (0.0998) | Acc_1: (96.47%) (43344/44928)\n",
      "Epoch: 62 | Batch_idx: 360 |  Loss_1: (0.1004) | Acc_1: (96.45%) (44568/46208)\n",
      "Epoch: 62 | Batch_idx: 370 |  Loss_1: (0.1010) | Acc_1: (96.43%) (45793/47488)\n",
      "Epoch: 62 | Batch_idx: 380 |  Loss_1: (0.1008) | Acc_1: (96.44%) (47032/48768)\n",
      "Epoch: 62 | Batch_idx: 390 |  Loss_1: (0.1015) | Acc_1: (96.40%) (48201/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4023) | Acc: (89.93%) (8993/10000)\n",
      "Epoch: 63 | Batch_idx: 0 |  Loss_1: (0.0519) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 63 | Batch_idx: 10 |  Loss_1: (0.0919) | Acc_1: (96.59%) (1360/1408)\n",
      "Epoch: 63 | Batch_idx: 20 |  Loss_1: (0.0971) | Acc_1: (96.32%) (2589/2688)\n",
      "Epoch: 63 | Batch_idx: 30 |  Loss_1: (0.1035) | Acc_1: (96.19%) (3817/3968)\n",
      "Epoch: 63 | Batch_idx: 40 |  Loss_1: (0.0996) | Acc_1: (96.38%) (5058/5248)\n",
      "Epoch: 63 | Batch_idx: 50 |  Loss_1: (0.1016) | Acc_1: (96.28%) (6285/6528)\n",
      "Epoch: 63 | Batch_idx: 60 |  Loss_1: (0.1023) | Acc_1: (96.35%) (7523/7808)\n",
      "Epoch: 63 | Batch_idx: 70 |  Loss_1: (0.1003) | Acc_1: (96.45%) (8765/9088)\n",
      "Epoch: 63 | Batch_idx: 80 |  Loss_1: (0.0987) | Acc_1: (96.47%) (10002/10368)\n",
      "Epoch: 63 | Batch_idx: 90 |  Loss_1: (0.0999) | Acc_1: (96.40%) (11229/11648)\n",
      "Epoch: 63 | Batch_idx: 100 |  Loss_1: (0.0991) | Acc_1: (96.40%) (12462/12928)\n",
      "Epoch: 63 | Batch_idx: 110 |  Loss_1: (0.0974) | Acc_1: (96.49%) (13710/14208)\n",
      "Epoch: 63 | Batch_idx: 120 |  Loss_1: (0.0982) | Acc_1: (96.48%) (14943/15488)\n",
      "Epoch: 63 | Batch_idx: 130 |  Loss_1: (0.0968) | Acc_1: (96.50%) (16181/16768)\n",
      "Epoch: 63 | Batch_idx: 140 |  Loss_1: (0.0978) | Acc_1: (96.45%) (17408/18048)\n",
      "Epoch: 63 | Batch_idx: 150 |  Loss_1: (0.0976) | Acc_1: (96.48%) (18647/19328)\n",
      "Epoch: 63 | Batch_idx: 160 |  Loss_1: (0.0977) | Acc_1: (96.51%) (19888/20608)\n",
      "Epoch: 63 | Batch_idx: 170 |  Loss_1: (0.0975) | Acc_1: (96.50%) (21122/21888)\n",
      "Epoch: 63 | Batch_idx: 180 |  Loss_1: (0.0972) | Acc_1: (96.52%) (22361/23168)\n",
      "Epoch: 63 | Batch_idx: 190 |  Loss_1: (0.0968) | Acc_1: (96.54%) (23601/24448)\n",
      "Epoch: 63 | Batch_idx: 200 |  Loss_1: (0.0972) | Acc_1: (96.52%) (24832/25728)\n",
      "Epoch: 63 | Batch_idx: 210 |  Loss_1: (0.0959) | Acc_1: (96.56%) (26080/27008)\n",
      "Epoch: 63 | Batch_idx: 220 |  Loss_1: (0.0966) | Acc_1: (96.54%) (27308/28288)\n",
      "Epoch: 63 | Batch_idx: 230 |  Loss_1: (0.0973) | Acc_1: (96.51%) (28535/29568)\n",
      "Epoch: 63 | Batch_idx: 240 |  Loss_1: (0.0977) | Acc_1: (96.50%) (29768/30848)\n",
      "Epoch: 63 | Batch_idx: 250 |  Loss_1: (0.0989) | Acc_1: (96.47%) (30995/32128)\n",
      "Epoch: 63 | Batch_idx: 260 |  Loss_1: (0.0986) | Acc_1: (96.47%) (32228/33408)\n",
      "Epoch: 63 | Batch_idx: 270 |  Loss_1: (0.0982) | Acc_1: (96.49%) (33469/34688)\n",
      "Epoch: 63 | Batch_idx: 280 |  Loss_1: (0.0981) | Acc_1: (96.49%) (34707/35968)\n",
      "Epoch: 63 | Batch_idx: 290 |  Loss_1: (0.0975) | Acc_1: (96.51%) (35948/37248)\n",
      "Epoch: 63 | Batch_idx: 300 |  Loss_1: (0.0979) | Acc_1: (96.50%) (37178/38528)\n",
      "Epoch: 63 | Batch_idx: 310 |  Loss_1: (0.0983) | Acc_1: (96.51%) (38419/39808)\n",
      "Epoch: 63 | Batch_idx: 320 |  Loss_1: (0.0986) | Acc_1: (96.51%) (39653/41088)\n",
      "Epoch: 63 | Batch_idx: 330 |  Loss_1: (0.0982) | Acc_1: (96.53%) (40897/42368)\n",
      "Epoch: 63 | Batch_idx: 340 |  Loss_1: (0.0985) | Acc_1: (96.52%) (42130/43648)\n",
      "Epoch: 63 | Batch_idx: 350 |  Loss_1: (0.0987) | Acc_1: (96.52%) (43365/44928)\n",
      "Epoch: 63 | Batch_idx: 360 |  Loss_1: (0.0993) | Acc_1: (96.49%) (44585/46208)\n",
      "Epoch: 63 | Batch_idx: 370 |  Loss_1: (0.1000) | Acc_1: (96.45%) (45804/47488)\n",
      "Epoch: 63 | Batch_idx: 380 |  Loss_1: (0.1002) | Acc_1: (96.46%) (47040/48768)\n",
      "Epoch: 63 | Batch_idx: 390 |  Loss_1: (0.1006) | Acc_1: (96.46%) (48228/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4424) | Acc: (89.60%) (8960/10000)\n",
      "Epoch: 64 | Batch_idx: 0 |  Loss_1: (0.0462) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 64 | Batch_idx: 10 |  Loss_1: (0.0764) | Acc_1: (97.30%) (1370/1408)\n",
      "Epoch: 64 | Batch_idx: 20 |  Loss_1: (0.0889) | Acc_1: (96.58%) (2596/2688)\n",
      "Epoch: 64 | Batch_idx: 30 |  Loss_1: (0.0921) | Acc_1: (96.62%) (3834/3968)\n",
      "Epoch: 64 | Batch_idx: 40 |  Loss_1: (0.0918) | Acc_1: (96.67%) (5073/5248)\n",
      "Epoch: 64 | Batch_idx: 50 |  Loss_1: (0.0876) | Acc_1: (96.83%) (6321/6528)\n",
      "Epoch: 64 | Batch_idx: 60 |  Loss_1: (0.0873) | Acc_1: (96.80%) (7558/7808)\n",
      "Epoch: 64 | Batch_idx: 70 |  Loss_1: (0.0878) | Acc_1: (96.85%) (8802/9088)\n",
      "Epoch: 64 | Batch_idx: 80 |  Loss_1: (0.0896) | Acc_1: (96.90%) (10047/10368)\n",
      "Epoch: 64 | Batch_idx: 90 |  Loss_1: (0.0916) | Acc_1: (96.87%) (11283/11648)\n",
      "Epoch: 64 | Batch_idx: 100 |  Loss_1: (0.0925) | Acc_1: (96.78%) (12512/12928)\n",
      "Epoch: 64 | Batch_idx: 110 |  Loss_1: (0.0944) | Acc_1: (96.64%) (13731/14208)\n",
      "Epoch: 64 | Batch_idx: 120 |  Loss_1: (0.0940) | Acc_1: (96.65%) (14969/15488)\n",
      "Epoch: 64 | Batch_idx: 130 |  Loss_1: (0.0974) | Acc_1: (96.55%) (16189/16768)\n",
      "Epoch: 64 | Batch_idx: 140 |  Loss_1: (0.0955) | Acc_1: (96.60%) (17435/18048)\n",
      "Epoch: 64 | Batch_idx: 150 |  Loss_1: (0.0962) | Acc_1: (96.60%) (18670/19328)\n",
      "Epoch: 64 | Batch_idx: 160 |  Loss_1: (0.0967) | Acc_1: (96.56%) (19899/20608)\n",
      "Epoch: 64 | Batch_idx: 170 |  Loss_1: (0.0965) | Acc_1: (96.56%) (21136/21888)\n",
      "Epoch: 64 | Batch_idx: 180 |  Loss_1: (0.0967) | Acc_1: (96.55%) (22369/23168)\n",
      "Epoch: 64 | Batch_idx: 190 |  Loss_1: (0.0980) | Acc_1: (96.51%) (23594/24448)\n",
      "Epoch: 64 | Batch_idx: 200 |  Loss_1: (0.0981) | Acc_1: (96.52%) (24833/25728)\n",
      "Epoch: 64 | Batch_idx: 210 |  Loss_1: (0.0973) | Acc_1: (96.55%) (26077/27008)\n",
      "Epoch: 64 | Batch_idx: 220 |  Loss_1: (0.0968) | Acc_1: (96.57%) (27317/28288)\n",
      "Epoch: 64 | Batch_idx: 230 |  Loss_1: (0.0972) | Acc_1: (96.54%) (28546/29568)\n",
      "Epoch: 64 | Batch_idx: 240 |  Loss_1: (0.0978) | Acc_1: (96.52%) (29773/30848)\n",
      "Epoch: 64 | Batch_idx: 250 |  Loss_1: (0.0979) | Acc_1: (96.53%) (31012/32128)\n",
      "Epoch: 64 | Batch_idx: 260 |  Loss_1: (0.0989) | Acc_1: (96.51%) (32243/33408)\n",
      "Epoch: 64 | Batch_idx: 270 |  Loss_1: (0.0979) | Acc_1: (96.56%) (33495/34688)\n",
      "Epoch: 64 | Batch_idx: 280 |  Loss_1: (0.0977) | Acc_1: (96.57%) (34736/35968)\n",
      "Epoch: 64 | Batch_idx: 290 |  Loss_1: (0.0984) | Acc_1: (96.55%) (35963/37248)\n",
      "Epoch: 64 | Batch_idx: 300 |  Loss_1: (0.0988) | Acc_1: (96.55%) (37197/38528)\n",
      "Epoch: 64 | Batch_idx: 310 |  Loss_1: (0.0987) | Acc_1: (96.56%) (38439/39808)\n",
      "Epoch: 64 | Batch_idx: 320 |  Loss_1: (0.0991) | Acc_1: (96.54%) (39667/41088)\n",
      "Epoch: 64 | Batch_idx: 330 |  Loss_1: (0.0991) | Acc_1: (96.53%) (40899/42368)\n",
      "Epoch: 64 | Batch_idx: 340 |  Loss_1: (0.0990) | Acc_1: (96.54%) (42138/43648)\n",
      "Epoch: 64 | Batch_idx: 350 |  Loss_1: (0.0997) | Acc_1: (96.52%) (43363/44928)\n",
      "Epoch: 64 | Batch_idx: 360 |  Loss_1: (0.1000) | Acc_1: (96.50%) (44592/46208)\n",
      "Epoch: 64 | Batch_idx: 370 |  Loss_1: (0.1008) | Acc_1: (96.49%) (45822/47488)\n",
      "Epoch: 64 | Batch_idx: 380 |  Loss_1: (0.1011) | Acc_1: (96.47%) (47046/48768)\n",
      "Epoch: 64 | Batch_idx: 390 |  Loss_1: (0.1009) | Acc_1: (96.48%) (48240/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4169) | Acc: (89.68%) (8968/10000)\n",
      "Epoch: 65 | Batch_idx: 0 |  Loss_1: (0.0753) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 65 | Batch_idx: 10 |  Loss_1: (0.0848) | Acc_1: (97.02%) (1366/1408)\n",
      "Epoch: 65 | Batch_idx: 20 |  Loss_1: (0.0993) | Acc_1: (96.73%) (2600/2688)\n",
      "Epoch: 65 | Batch_idx: 30 |  Loss_1: (0.0954) | Acc_1: (96.75%) (3839/3968)\n",
      "Epoch: 65 | Batch_idx: 40 |  Loss_1: (0.0905) | Acc_1: (97.03%) (5092/5248)\n",
      "Epoch: 65 | Batch_idx: 50 |  Loss_1: (0.0874) | Acc_1: (97.10%) (6339/6528)\n",
      "Epoch: 65 | Batch_idx: 60 |  Loss_1: (0.0866) | Acc_1: (96.99%) (7573/7808)\n",
      "Epoch: 65 | Batch_idx: 70 |  Loss_1: (0.0869) | Acc_1: (96.99%) (8814/9088)\n",
      "Epoch: 65 | Batch_idx: 80 |  Loss_1: (0.0887) | Acc_1: (96.94%) (10051/10368)\n",
      "Epoch: 65 | Batch_idx: 90 |  Loss_1: (0.0891) | Acc_1: (96.94%) (11291/11648)\n",
      "Epoch: 65 | Batch_idx: 100 |  Loss_1: (0.0895) | Acc_1: (96.86%) (12522/12928)\n",
      "Epoch: 65 | Batch_idx: 110 |  Loss_1: (0.0891) | Acc_1: (96.85%) (13761/14208)\n",
      "Epoch: 65 | Batch_idx: 120 |  Loss_1: (0.0893) | Acc_1: (96.84%) (14998/15488)\n",
      "Epoch: 65 | Batch_idx: 130 |  Loss_1: (0.0878) | Acc_1: (96.92%) (16251/16768)\n",
      "Epoch: 65 | Batch_idx: 140 |  Loss_1: (0.0891) | Acc_1: (96.89%) (17487/18048)\n",
      "Epoch: 65 | Batch_idx: 150 |  Loss_1: (0.0878) | Acc_1: (96.94%) (18737/19328)\n",
      "Epoch: 65 | Batch_idx: 160 |  Loss_1: (0.0883) | Acc_1: (96.96%) (19982/20608)\n",
      "Epoch: 65 | Batch_idx: 170 |  Loss_1: (0.0877) | Acc_1: (96.96%) (21222/21888)\n",
      "Epoch: 65 | Batch_idx: 180 |  Loss_1: (0.0871) | Acc_1: (96.99%) (22470/23168)\n",
      "Epoch: 65 | Batch_idx: 190 |  Loss_1: (0.0867) | Acc_1: (96.99%) (23713/24448)\n",
      "Epoch: 65 | Batch_idx: 200 |  Loss_1: (0.0872) | Acc_1: (96.96%) (24945/25728)\n",
      "Epoch: 65 | Batch_idx: 210 |  Loss_1: (0.0891) | Acc_1: (96.91%) (26174/27008)\n",
      "Epoch: 65 | Batch_idx: 220 |  Loss_1: (0.0886) | Acc_1: (96.93%) (27419/28288)\n",
      "Epoch: 65 | Batch_idx: 230 |  Loss_1: (0.0883) | Acc_1: (96.94%) (28664/29568)\n",
      "Epoch: 65 | Batch_idx: 240 |  Loss_1: (0.0899) | Acc_1: (96.90%) (29892/30848)\n",
      "Epoch: 65 | Batch_idx: 250 |  Loss_1: (0.0906) | Acc_1: (96.90%) (31131/32128)\n",
      "Epoch: 65 | Batch_idx: 260 |  Loss_1: (0.0914) | Acc_1: (96.86%) (32359/33408)\n",
      "Epoch: 65 | Batch_idx: 270 |  Loss_1: (0.0910) | Acc_1: (96.86%) (33600/34688)\n",
      "Epoch: 65 | Batch_idx: 280 |  Loss_1: (0.0906) | Acc_1: (96.88%) (34844/35968)\n",
      "Epoch: 65 | Batch_idx: 290 |  Loss_1: (0.0907) | Acc_1: (96.88%) (36084/37248)\n",
      "Epoch: 65 | Batch_idx: 300 |  Loss_1: (0.0905) | Acc_1: (96.87%) (37323/38528)\n",
      "Epoch: 65 | Batch_idx: 310 |  Loss_1: (0.0903) | Acc_1: (96.87%) (38563/39808)\n",
      "Epoch: 65 | Batch_idx: 320 |  Loss_1: (0.0902) | Acc_1: (96.87%) (39800/41088)\n",
      "Epoch: 65 | Batch_idx: 330 |  Loss_1: (0.0907) | Acc_1: (96.84%) (41031/42368)\n",
      "Epoch: 65 | Batch_idx: 340 |  Loss_1: (0.0913) | Acc_1: (96.83%) (42264/43648)\n",
      "Epoch: 65 | Batch_idx: 350 |  Loss_1: (0.0920) | Acc_1: (96.81%) (43496/44928)\n",
      "Epoch: 65 | Batch_idx: 360 |  Loss_1: (0.0922) | Acc_1: (96.81%) (44732/46208)\n",
      "Epoch: 65 | Batch_idx: 370 |  Loss_1: (0.0920) | Acc_1: (96.81%) (45975/47488)\n",
      "Epoch: 65 | Batch_idx: 380 |  Loss_1: (0.0928) | Acc_1: (96.78%) (47196/48768)\n",
      "Epoch: 65 | Batch_idx: 390 |  Loss_1: (0.0938) | Acc_1: (96.73%) (48367/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4262) | Acc: (89.39%) (8939/10000)\n",
      "Epoch: 66 | Batch_idx: 0 |  Loss_1: (0.1068) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 66 | Batch_idx: 10 |  Loss_1: (0.1106) | Acc_1: (96.59%) (1360/1408)\n",
      "Epoch: 66 | Batch_idx: 20 |  Loss_1: (0.0922) | Acc_1: (96.84%) (2603/2688)\n",
      "Epoch: 66 | Batch_idx: 30 |  Loss_1: (0.0957) | Acc_1: (96.67%) (3836/3968)\n",
      "Epoch: 66 | Batch_idx: 40 |  Loss_1: (0.0902) | Acc_1: (96.91%) (5086/5248)\n",
      "Epoch: 66 | Batch_idx: 50 |  Loss_1: (0.0872) | Acc_1: (97.01%) (6333/6528)\n",
      "Epoch: 66 | Batch_idx: 60 |  Loss_1: (0.0857) | Acc_1: (97.05%) (7578/7808)\n",
      "Epoch: 66 | Batch_idx: 70 |  Loss_1: (0.0857) | Acc_1: (97.08%) (8823/9088)\n",
      "Epoch: 66 | Batch_idx: 80 |  Loss_1: (0.0858) | Acc_1: (97.13%) (10070/10368)\n",
      "Epoch: 66 | Batch_idx: 90 |  Loss_1: (0.0874) | Acc_1: (97.07%) (11307/11648)\n",
      "Epoch: 66 | Batch_idx: 100 |  Loss_1: (0.0884) | Acc_1: (96.99%) (12539/12928)\n",
      "Epoch: 66 | Batch_idx: 110 |  Loss_1: (0.0893) | Acc_1: (96.97%) (13777/14208)\n",
      "Epoch: 66 | Batch_idx: 120 |  Loss_1: (0.0910) | Acc_1: (96.91%) (15010/15488)\n",
      "Epoch: 66 | Batch_idx: 130 |  Loss_1: (0.0909) | Acc_1: (96.92%) (16251/16768)\n",
      "Epoch: 66 | Batch_idx: 140 |  Loss_1: (0.0908) | Acc_1: (96.92%) (17493/18048)\n",
      "Epoch: 66 | Batch_idx: 150 |  Loss_1: (0.0905) | Acc_1: (96.95%) (18738/19328)\n",
      "Epoch: 66 | Batch_idx: 160 |  Loss_1: (0.0912) | Acc_1: (96.90%) (19969/20608)\n",
      "Epoch: 66 | Batch_idx: 170 |  Loss_1: (0.0914) | Acc_1: (96.89%) (21207/21888)\n",
      "Epoch: 66 | Batch_idx: 180 |  Loss_1: (0.0905) | Acc_1: (96.91%) (22453/23168)\n",
      "Epoch: 66 | Batch_idx: 190 |  Loss_1: (0.0909) | Acc_1: (96.91%) (23692/24448)\n",
      "Epoch: 66 | Batch_idx: 200 |  Loss_1: (0.0907) | Acc_1: (96.91%) (24934/25728)\n",
      "Epoch: 66 | Batch_idx: 210 |  Loss_1: (0.0915) | Acc_1: (96.88%) (26164/27008)\n",
      "Epoch: 66 | Batch_idx: 220 |  Loss_1: (0.0920) | Acc_1: (96.87%) (27402/28288)\n",
      "Epoch: 66 | Batch_idx: 230 |  Loss_1: (0.0923) | Acc_1: (96.85%) (28637/29568)\n",
      "Epoch: 66 | Batch_idx: 240 |  Loss_1: (0.0929) | Acc_1: (96.85%) (29875/30848)\n",
      "Epoch: 66 | Batch_idx: 250 |  Loss_1: (0.0934) | Acc_1: (96.82%) (31105/32128)\n",
      "Epoch: 66 | Batch_idx: 260 |  Loss_1: (0.0927) | Acc_1: (96.84%) (32353/33408)\n",
      "Epoch: 66 | Batch_idx: 270 |  Loss_1: (0.0928) | Acc_1: (96.84%) (33591/34688)\n",
      "Epoch: 66 | Batch_idx: 280 |  Loss_1: (0.0929) | Acc_1: (96.84%) (34831/35968)\n",
      "Epoch: 66 | Batch_idx: 290 |  Loss_1: (0.0926) | Acc_1: (96.82%) (36064/37248)\n",
      "Epoch: 66 | Batch_idx: 300 |  Loss_1: (0.0928) | Acc_1: (96.82%) (37302/38528)\n",
      "Epoch: 66 | Batch_idx: 310 |  Loss_1: (0.0930) | Acc_1: (96.81%) (38538/39808)\n",
      "Epoch: 66 | Batch_idx: 320 |  Loss_1: (0.0936) | Acc_1: (96.78%) (39766/41088)\n",
      "Epoch: 66 | Batch_idx: 330 |  Loss_1: (0.0935) | Acc_1: (96.79%) (41006/42368)\n",
      "Epoch: 66 | Batch_idx: 340 |  Loss_1: (0.0934) | Acc_1: (96.79%) (42247/43648)\n",
      "Epoch: 66 | Batch_idx: 350 |  Loss_1: (0.0934) | Acc_1: (96.79%) (43485/44928)\n",
      "Epoch: 66 | Batch_idx: 360 |  Loss_1: (0.0935) | Acc_1: (96.79%) (44724/46208)\n",
      "Epoch: 66 | Batch_idx: 370 |  Loss_1: (0.0941) | Acc_1: (96.77%) (45953/47488)\n",
      "Epoch: 66 | Batch_idx: 380 |  Loss_1: (0.0948) | Acc_1: (96.75%) (47182/48768)\n",
      "Epoch: 66 | Batch_idx: 390 |  Loss_1: (0.0948) | Acc_1: (96.74%) (48371/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4155) | Acc: (89.86%) (8986/10000)\n",
      "Epoch: 67 | Batch_idx: 0 |  Loss_1: (0.0779) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 67 | Batch_idx: 10 |  Loss_1: (0.0873) | Acc_1: (97.09%) (1367/1408)\n",
      "Epoch: 67 | Batch_idx: 20 |  Loss_1: (0.0944) | Acc_1: (96.76%) (2601/2688)\n",
      "Epoch: 67 | Batch_idx: 30 |  Loss_1: (0.0964) | Acc_1: (96.75%) (3839/3968)\n",
      "Epoch: 67 | Batch_idx: 40 |  Loss_1: (0.0929) | Acc_1: (96.82%) (5081/5248)\n",
      "Epoch: 67 | Batch_idx: 50 |  Loss_1: (0.0883) | Acc_1: (96.97%) (6330/6528)\n",
      "Epoch: 67 | Batch_idx: 60 |  Loss_1: (0.0898) | Acc_1: (96.98%) (7572/7808)\n",
      "Epoch: 67 | Batch_idx: 70 |  Loss_1: (0.0873) | Acc_1: (97.07%) (8822/9088)\n",
      "Epoch: 67 | Batch_idx: 80 |  Loss_1: (0.0848) | Acc_1: (97.15%) (10072/10368)\n",
      "Epoch: 67 | Batch_idx: 90 |  Loss_1: (0.0854) | Acc_1: (97.10%) (11310/11648)\n",
      "Epoch: 67 | Batch_idx: 100 |  Loss_1: (0.0851) | Acc_1: (97.07%) (12549/12928)\n",
      "Epoch: 67 | Batch_idx: 110 |  Loss_1: (0.0870) | Acc_1: (97.00%) (13782/14208)\n",
      "Epoch: 67 | Batch_idx: 120 |  Loss_1: (0.0879) | Acc_1: (96.97%) (15019/15488)\n",
      "Epoch: 67 | Batch_idx: 130 |  Loss_1: (0.0881) | Acc_1: (96.96%) (16259/16768)\n",
      "Epoch: 67 | Batch_idx: 140 |  Loss_1: (0.0897) | Acc_1: (96.92%) (17493/18048)\n",
      "Epoch: 67 | Batch_idx: 150 |  Loss_1: (0.0906) | Acc_1: (96.92%) (18732/19328)\n",
      "Epoch: 67 | Batch_idx: 160 |  Loss_1: (0.0908) | Acc_1: (96.88%) (19966/20608)\n",
      "Epoch: 67 | Batch_idx: 170 |  Loss_1: (0.0901) | Acc_1: (96.91%) (21212/21888)\n",
      "Epoch: 67 | Batch_idx: 180 |  Loss_1: (0.0899) | Acc_1: (96.91%) (22453/23168)\n",
      "Epoch: 67 | Batch_idx: 190 |  Loss_1: (0.0888) | Acc_1: (96.95%) (23703/24448)\n",
      "Epoch: 67 | Batch_idx: 200 |  Loss_1: (0.0885) | Acc_1: (96.96%) (24947/25728)\n",
      "Epoch: 67 | Batch_idx: 210 |  Loss_1: (0.0883) | Acc_1: (96.97%) (26190/27008)\n",
      "Epoch: 67 | Batch_idx: 220 |  Loss_1: (0.0885) | Acc_1: (96.96%) (27427/28288)\n",
      "Epoch: 67 | Batch_idx: 230 |  Loss_1: (0.0885) | Acc_1: (96.95%) (28666/29568)\n",
      "Epoch: 67 | Batch_idx: 240 |  Loss_1: (0.0891) | Acc_1: (96.93%) (29901/30848)\n",
      "Epoch: 67 | Batch_idx: 250 |  Loss_1: (0.0890) | Acc_1: (96.93%) (31143/32128)\n",
      "Epoch: 67 | Batch_idx: 260 |  Loss_1: (0.0892) | Acc_1: (96.91%) (32376/33408)\n",
      "Epoch: 67 | Batch_idx: 270 |  Loss_1: (0.0898) | Acc_1: (96.87%) (33601/34688)\n",
      "Epoch: 67 | Batch_idx: 280 |  Loss_1: (0.0906) | Acc_1: (96.84%) (34830/35968)\n",
      "Epoch: 67 | Batch_idx: 290 |  Loss_1: (0.0905) | Acc_1: (96.82%) (36065/37248)\n",
      "Epoch: 67 | Batch_idx: 300 |  Loss_1: (0.0904) | Acc_1: (96.82%) (37303/38528)\n",
      "Epoch: 67 | Batch_idx: 310 |  Loss_1: (0.0905) | Acc_1: (96.82%) (38543/39808)\n",
      "Epoch: 67 | Batch_idx: 320 |  Loss_1: (0.0901) | Acc_1: (96.84%) (39788/41088)\n",
      "Epoch: 67 | Batch_idx: 330 |  Loss_1: (0.0905) | Acc_1: (96.82%) (41022/42368)\n",
      "Epoch: 67 | Batch_idx: 340 |  Loss_1: (0.0917) | Acc_1: (96.77%) (42240/43648)\n",
      "Epoch: 67 | Batch_idx: 350 |  Loss_1: (0.0914) | Acc_1: (96.78%) (43483/44928)\n",
      "Epoch: 67 | Batch_idx: 360 |  Loss_1: (0.0913) | Acc_1: (96.79%) (44725/46208)\n",
      "Epoch: 67 | Batch_idx: 370 |  Loss_1: (0.0912) | Acc_1: (96.79%) (45962/47488)\n",
      "Epoch: 67 | Batch_idx: 380 |  Loss_1: (0.0912) | Acc_1: (96.79%) (47203/48768)\n",
      "Epoch: 67 | Batch_idx: 390 |  Loss_1: (0.0915) | Acc_1: (96.77%) (48387/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4450) | Acc: (89.56%) (8956/10000)\n",
      "Epoch: 68 | Batch_idx: 0 |  Loss_1: (0.1154) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 68 | Batch_idx: 10 |  Loss_1: (0.0774) | Acc_1: (96.59%) (1360/1408)\n",
      "Epoch: 68 | Batch_idx: 20 |  Loss_1: (0.0856) | Acc_1: (96.69%) (2599/2688)\n",
      "Epoch: 68 | Batch_idx: 30 |  Loss_1: (0.0822) | Acc_1: (96.75%) (3839/3968)\n",
      "Epoch: 68 | Batch_idx: 40 |  Loss_1: (0.0822) | Acc_1: (96.88%) (5084/5248)\n",
      "Epoch: 68 | Batch_idx: 50 |  Loss_1: (0.0823) | Acc_1: (96.94%) (6328/6528)\n",
      "Epoch: 68 | Batch_idx: 60 |  Loss_1: (0.0828) | Acc_1: (96.98%) (7572/7808)\n",
      "Epoch: 68 | Batch_idx: 70 |  Loss_1: (0.0839) | Acc_1: (96.95%) (8811/9088)\n",
      "Epoch: 68 | Batch_idx: 80 |  Loss_1: (0.0864) | Acc_1: (96.92%) (10049/10368)\n",
      "Epoch: 68 | Batch_idx: 90 |  Loss_1: (0.0859) | Acc_1: (96.95%) (11293/11648)\n",
      "Epoch: 68 | Batch_idx: 100 |  Loss_1: (0.0868) | Acc_1: (96.95%) (12534/12928)\n",
      "Epoch: 68 | Batch_idx: 110 |  Loss_1: (0.0868) | Acc_1: (96.93%) (13772/14208)\n",
      "Epoch: 68 | Batch_idx: 120 |  Loss_1: (0.0857) | Acc_1: (96.98%) (15021/15488)\n",
      "Epoch: 68 | Batch_idx: 130 |  Loss_1: (0.0867) | Acc_1: (96.93%) (16254/16768)\n",
      "Epoch: 68 | Batch_idx: 140 |  Loss_1: (0.0854) | Acc_1: (96.99%) (17504/18048)\n",
      "Epoch: 68 | Batch_idx: 150 |  Loss_1: (0.0860) | Acc_1: (96.97%) (18742/19328)\n",
      "Epoch: 68 | Batch_idx: 160 |  Loss_1: (0.0861) | Acc_1: (96.97%) (19984/20608)\n",
      "Epoch: 68 | Batch_idx: 170 |  Loss_1: (0.0852) | Acc_1: (97.00%) (21231/21888)\n",
      "Epoch: 68 | Batch_idx: 180 |  Loss_1: (0.0842) | Acc_1: (97.03%) (22481/23168)\n",
      "Epoch: 68 | Batch_idx: 190 |  Loss_1: (0.0856) | Acc_1: (96.99%) (23711/24448)\n",
      "Epoch: 68 | Batch_idx: 200 |  Loss_1: (0.0855) | Acc_1: (96.97%) (24949/25728)\n",
      "Epoch: 68 | Batch_idx: 210 |  Loss_1: (0.0851) | Acc_1: (97.01%) (26200/27008)\n",
      "Epoch: 68 | Batch_idx: 220 |  Loss_1: (0.0855) | Acc_1: (97.00%) (27440/28288)\n",
      "Epoch: 68 | Batch_idx: 230 |  Loss_1: (0.0867) | Acc_1: (96.97%) (28673/29568)\n",
      "Epoch: 68 | Batch_idx: 240 |  Loss_1: (0.0872) | Acc_1: (96.96%) (29909/30848)\n",
      "Epoch: 68 | Batch_idx: 250 |  Loss_1: (0.0879) | Acc_1: (96.92%) (31139/32128)\n",
      "Epoch: 68 | Batch_idx: 260 |  Loss_1: (0.0873) | Acc_1: (96.94%) (32385/33408)\n",
      "Epoch: 68 | Batch_idx: 270 |  Loss_1: (0.0869) | Acc_1: (96.95%) (33631/34688)\n",
      "Epoch: 68 | Batch_idx: 280 |  Loss_1: (0.0875) | Acc_1: (96.93%) (34864/35968)\n",
      "Epoch: 68 | Batch_idx: 290 |  Loss_1: (0.0876) | Acc_1: (96.93%) (36105/37248)\n",
      "Epoch: 68 | Batch_idx: 300 |  Loss_1: (0.0884) | Acc_1: (96.90%) (37334/38528)\n",
      "Epoch: 68 | Batch_idx: 310 |  Loss_1: (0.0889) | Acc_1: (96.89%) (38569/39808)\n",
      "Epoch: 68 | Batch_idx: 320 |  Loss_1: (0.0885) | Acc_1: (96.89%) (39812/41088)\n",
      "Epoch: 68 | Batch_idx: 330 |  Loss_1: (0.0885) | Acc_1: (96.90%) (41056/42368)\n",
      "Epoch: 68 | Batch_idx: 340 |  Loss_1: (0.0884) | Acc_1: (96.91%) (42300/43648)\n",
      "Epoch: 68 | Batch_idx: 350 |  Loss_1: (0.0884) | Acc_1: (96.91%) (43538/44928)\n",
      "Epoch: 68 | Batch_idx: 360 |  Loss_1: (0.0890) | Acc_1: (96.89%) (44772/46208)\n",
      "Epoch: 68 | Batch_idx: 370 |  Loss_1: (0.0887) | Acc_1: (96.90%) (46015/47488)\n",
      "Epoch: 68 | Batch_idx: 380 |  Loss_1: (0.0885) | Acc_1: (96.91%) (47261/48768)\n",
      "Epoch: 68 | Batch_idx: 390 |  Loss_1: (0.0881) | Acc_1: (96.91%) (48457/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3931) | Acc: (90.46%) (9046/10000)\n",
      "Epoch: 69 | Batch_idx: 0 |  Loss_1: (0.0823) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 69 | Batch_idx: 10 |  Loss_1: (0.0720) | Acc_1: (97.51%) (1373/1408)\n",
      "Epoch: 69 | Batch_idx: 20 |  Loss_1: (0.0780) | Acc_1: (97.28%) (2615/2688)\n",
      "Epoch: 69 | Batch_idx: 30 |  Loss_1: (0.0801) | Acc_1: (96.85%) (3843/3968)\n",
      "Epoch: 69 | Batch_idx: 40 |  Loss_1: (0.0797) | Acc_1: (96.97%) (5089/5248)\n",
      "Epoch: 69 | Batch_idx: 50 |  Loss_1: (0.0798) | Acc_1: (97.00%) (6332/6528)\n",
      "Epoch: 69 | Batch_idx: 60 |  Loss_1: (0.0824) | Acc_1: (96.94%) (7569/7808)\n",
      "Epoch: 69 | Batch_idx: 70 |  Loss_1: (0.0852) | Acc_1: (96.89%) (8805/9088)\n",
      "Epoch: 69 | Batch_idx: 80 |  Loss_1: (0.0841) | Acc_1: (96.90%) (10047/10368)\n",
      "Epoch: 69 | Batch_idx: 90 |  Loss_1: (0.0834) | Acc_1: (96.94%) (11292/11648)\n",
      "Epoch: 69 | Batch_idx: 100 |  Loss_1: (0.0836) | Acc_1: (96.98%) (12538/12928)\n",
      "Epoch: 69 | Batch_idx: 110 |  Loss_1: (0.0844) | Acc_1: (96.99%) (13780/14208)\n",
      "Epoch: 69 | Batch_idx: 120 |  Loss_1: (0.0828) | Acc_1: (97.06%) (15032/15488)\n",
      "Epoch: 69 | Batch_idx: 130 |  Loss_1: (0.0827) | Acc_1: (97.06%) (16275/16768)\n",
      "Epoch: 69 | Batch_idx: 140 |  Loss_1: (0.0832) | Acc_1: (97.04%) (17513/18048)\n",
      "Epoch: 69 | Batch_idx: 150 |  Loss_1: (0.0832) | Acc_1: (97.07%) (18761/19328)\n",
      "Epoch: 69 | Batch_idx: 160 |  Loss_1: (0.0836) | Acc_1: (97.07%) (20004/20608)\n",
      "Epoch: 69 | Batch_idx: 170 |  Loss_1: (0.0833) | Acc_1: (97.09%) (21252/21888)\n",
      "Epoch: 69 | Batch_idx: 180 |  Loss_1: (0.0837) | Acc_1: (97.09%) (22494/23168)\n",
      "Epoch: 69 | Batch_idx: 190 |  Loss_1: (0.0843) | Acc_1: (97.05%) (23728/24448)\n",
      "Epoch: 69 | Batch_idx: 200 |  Loss_1: (0.0853) | Acc_1: (97.03%) (24965/25728)\n",
      "Epoch: 69 | Batch_idx: 210 |  Loss_1: (0.0859) | Acc_1: (97.00%) (26198/27008)\n",
      "Epoch: 69 | Batch_idx: 220 |  Loss_1: (0.0855) | Acc_1: (97.00%) (27439/28288)\n",
      "Epoch: 69 | Batch_idx: 230 |  Loss_1: (0.0856) | Acc_1: (96.98%) (28676/29568)\n",
      "Epoch: 69 | Batch_idx: 240 |  Loss_1: (0.0850) | Acc_1: (96.99%) (29921/30848)\n",
      "Epoch: 69 | Batch_idx: 250 |  Loss_1: (0.0845) | Acc_1: (97.01%) (31168/32128)\n",
      "Epoch: 69 | Batch_idx: 260 |  Loss_1: (0.0846) | Acc_1: (97.02%) (32413/33408)\n",
      "Epoch: 69 | Batch_idx: 270 |  Loss_1: (0.0854) | Acc_1: (97.00%) (33647/34688)\n",
      "Epoch: 69 | Batch_idx: 280 |  Loss_1: (0.0856) | Acc_1: (96.99%) (34887/35968)\n",
      "Epoch: 69 | Batch_idx: 290 |  Loss_1: (0.0850) | Acc_1: (97.02%) (36139/37248)\n",
      "Epoch: 69 | Batch_idx: 300 |  Loss_1: (0.0844) | Acc_1: (97.04%) (37387/38528)\n",
      "Epoch: 69 | Batch_idx: 310 |  Loss_1: (0.0847) | Acc_1: (97.04%) (38630/39808)\n",
      "Epoch: 69 | Batch_idx: 320 |  Loss_1: (0.0849) | Acc_1: (97.04%) (39871/41088)\n",
      "Epoch: 69 | Batch_idx: 330 |  Loss_1: (0.0858) | Acc_1: (97.02%) (41104/42368)\n",
      "Epoch: 69 | Batch_idx: 340 |  Loss_1: (0.0858) | Acc_1: (97.01%) (42344/43648)\n",
      "Epoch: 69 | Batch_idx: 350 |  Loss_1: (0.0878) | Acc_1: (96.94%) (43554/44928)\n",
      "Epoch: 69 | Batch_idx: 360 |  Loss_1: (0.0881) | Acc_1: (96.94%) (44793/46208)\n",
      "Epoch: 69 | Batch_idx: 370 |  Loss_1: (0.0880) | Acc_1: (96.94%) (46034/47488)\n",
      "Epoch: 69 | Batch_idx: 380 |  Loss_1: (0.0884) | Acc_1: (96.90%) (47256/48768)\n",
      "Epoch: 69 | Batch_idx: 390 |  Loss_1: (0.0893) | Acc_1: (96.88%) (48440/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5173) | Acc: (88.69%) (8869/10000)\n",
      "Epoch: 70 | Batch_idx: 0 |  Loss_1: (0.0796) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 70 | Batch_idx: 10 |  Loss_1: (0.1068) | Acc_1: (96.45%) (1358/1408)\n",
      "Epoch: 70 | Batch_idx: 20 |  Loss_1: (0.1040) | Acc_1: (96.43%) (2592/2688)\n",
      "Epoch: 70 | Batch_idx: 30 |  Loss_1: (0.0978) | Acc_1: (96.60%) (3833/3968)\n",
      "Epoch: 70 | Batch_idx: 40 |  Loss_1: (0.0928) | Acc_1: (96.76%) (5078/5248)\n",
      "Epoch: 70 | Batch_idx: 50 |  Loss_1: (0.0907) | Acc_1: (96.83%) (6321/6528)\n",
      "Epoch: 70 | Batch_idx: 60 |  Loss_1: (0.0867) | Acc_1: (96.96%) (7571/7808)\n",
      "Epoch: 70 | Batch_idx: 70 |  Loss_1: (0.0852) | Acc_1: (97.04%) (8819/9088)\n",
      "Epoch: 70 | Batch_idx: 80 |  Loss_1: (0.0849) | Acc_1: (97.00%) (10057/10368)\n",
      "Epoch: 70 | Batch_idx: 90 |  Loss_1: (0.0854) | Acc_1: (97.02%) (11301/11648)\n",
      "Epoch: 70 | Batch_idx: 100 |  Loss_1: (0.0833) | Acc_1: (97.08%) (12550/12928)\n",
      "Epoch: 70 | Batch_idx: 110 |  Loss_1: (0.0845) | Acc_1: (97.01%) (13783/14208)\n",
      "Epoch: 70 | Batch_idx: 120 |  Loss_1: (0.0829) | Acc_1: (97.12%) (15042/15488)\n",
      "Epoch: 70 | Batch_idx: 130 |  Loss_1: (0.0822) | Acc_1: (97.17%) (16293/16768)\n",
      "Epoch: 70 | Batch_idx: 140 |  Loss_1: (0.0817) | Acc_1: (97.21%) (17544/18048)\n",
      "Epoch: 70 | Batch_idx: 150 |  Loss_1: (0.0825) | Acc_1: (97.18%) (18783/19328)\n",
      "Epoch: 70 | Batch_idx: 160 |  Loss_1: (0.0831) | Acc_1: (97.19%) (20029/20608)\n",
      "Epoch: 70 | Batch_idx: 170 |  Loss_1: (0.0831) | Acc_1: (97.19%) (21272/21888)\n",
      "Epoch: 70 | Batch_idx: 180 |  Loss_1: (0.0822) | Acc_1: (97.23%) (22526/23168)\n",
      "Epoch: 70 | Batch_idx: 190 |  Loss_1: (0.0828) | Acc_1: (97.19%) (23761/24448)\n",
      "Epoch: 70 | Batch_idx: 200 |  Loss_1: (0.0823) | Acc_1: (97.23%) (25015/25728)\n",
      "Epoch: 70 | Batch_idx: 210 |  Loss_1: (0.0821) | Acc_1: (97.24%) (26262/27008)\n",
      "Epoch: 70 | Batch_idx: 220 |  Loss_1: (0.0818) | Acc_1: (97.26%) (27513/28288)\n",
      "Epoch: 70 | Batch_idx: 230 |  Loss_1: (0.0821) | Acc_1: (97.25%) (28754/29568)\n",
      "Epoch: 70 | Batch_idx: 240 |  Loss_1: (0.0823) | Acc_1: (97.24%) (29998/30848)\n",
      "Epoch: 70 | Batch_idx: 250 |  Loss_1: (0.0821) | Acc_1: (97.23%) (31239/32128)\n",
      "Epoch: 70 | Batch_idx: 260 |  Loss_1: (0.0826) | Acc_1: (97.22%) (32479/33408)\n",
      "Epoch: 70 | Batch_idx: 270 |  Loss_1: (0.0833) | Acc_1: (97.20%) (33718/34688)\n",
      "Epoch: 70 | Batch_idx: 280 |  Loss_1: (0.0836) | Acc_1: (97.19%) (34956/35968)\n",
      "Epoch: 70 | Batch_idx: 290 |  Loss_1: (0.0840) | Acc_1: (97.19%) (36201/37248)\n",
      "Epoch: 70 | Batch_idx: 300 |  Loss_1: (0.0839) | Acc_1: (97.20%) (37448/38528)\n",
      "Epoch: 70 | Batch_idx: 310 |  Loss_1: (0.0840) | Acc_1: (97.19%) (38690/39808)\n",
      "Epoch: 70 | Batch_idx: 320 |  Loss_1: (0.0844) | Acc_1: (97.16%) (39923/41088)\n",
      "Epoch: 70 | Batch_idx: 330 |  Loss_1: (0.0844) | Acc_1: (97.16%) (41165/42368)\n",
      "Epoch: 70 | Batch_idx: 340 |  Loss_1: (0.0842) | Acc_1: (97.15%) (42404/43648)\n",
      "Epoch: 70 | Batch_idx: 350 |  Loss_1: (0.0838) | Acc_1: (97.16%) (43652/44928)\n",
      "Epoch: 70 | Batch_idx: 360 |  Loss_1: (0.0839) | Acc_1: (97.16%) (44895/46208)\n",
      "Epoch: 70 | Batch_idx: 370 |  Loss_1: (0.0839) | Acc_1: (97.14%) (46132/47488)\n",
      "Epoch: 70 | Batch_idx: 380 |  Loss_1: (0.0837) | Acc_1: (97.16%) (47384/48768)\n",
      "Epoch: 70 | Batch_idx: 390 |  Loss_1: (0.0832) | Acc_1: (97.18%) (48588/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4618) | Acc: (89.79%) (8979/10000)\n",
      "Epoch: 71 | Batch_idx: 0 |  Loss_1: (0.0575) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 71 | Batch_idx: 10 |  Loss_1: (0.0803) | Acc_1: (97.30%) (1370/1408)\n",
      "Epoch: 71 | Batch_idx: 20 |  Loss_1: (0.0802) | Acc_1: (97.06%) (2609/2688)\n",
      "Epoch: 71 | Batch_idx: 30 |  Loss_1: (0.0844) | Acc_1: (96.75%) (3839/3968)\n",
      "Epoch: 71 | Batch_idx: 40 |  Loss_1: (0.0850) | Acc_1: (96.80%) (5080/5248)\n",
      "Epoch: 71 | Batch_idx: 50 |  Loss_1: (0.0841) | Acc_1: (96.89%) (6325/6528)\n",
      "Epoch: 71 | Batch_idx: 60 |  Loss_1: (0.0856) | Acc_1: (96.85%) (7562/7808)\n",
      "Epoch: 71 | Batch_idx: 70 |  Loss_1: (0.0854) | Acc_1: (96.82%) (8799/9088)\n",
      "Epoch: 71 | Batch_idx: 80 |  Loss_1: (0.0829) | Acc_1: (96.93%) (10050/10368)\n",
      "Epoch: 71 | Batch_idx: 90 |  Loss_1: (0.0836) | Acc_1: (96.94%) (11292/11648)\n",
      "Epoch: 71 | Batch_idx: 100 |  Loss_1: (0.0846) | Acc_1: (96.94%) (12532/12928)\n",
      "Epoch: 71 | Batch_idx: 110 |  Loss_1: (0.0837) | Acc_1: (96.94%) (13773/14208)\n",
      "Epoch: 71 | Batch_idx: 120 |  Loss_1: (0.0853) | Acc_1: (96.93%) (15012/15488)\n",
      "Epoch: 71 | Batch_idx: 130 |  Loss_1: (0.0857) | Acc_1: (96.94%) (16255/16768)\n",
      "Epoch: 71 | Batch_idx: 140 |  Loss_1: (0.0866) | Acc_1: (96.94%) (17496/18048)\n",
      "Epoch: 71 | Batch_idx: 150 |  Loss_1: (0.0885) | Acc_1: (96.86%) (18722/19328)\n",
      "Epoch: 71 | Batch_idx: 160 |  Loss_1: (0.0884) | Acc_1: (96.87%) (19963/20608)\n",
      "Epoch: 71 | Batch_idx: 170 |  Loss_1: (0.0882) | Acc_1: (96.88%) (21206/21888)\n",
      "Epoch: 71 | Batch_idx: 180 |  Loss_1: (0.0880) | Acc_1: (96.91%) (22453/23168)\n",
      "Epoch: 71 | Batch_idx: 190 |  Loss_1: (0.0881) | Acc_1: (96.92%) (23695/24448)\n",
      "Epoch: 71 | Batch_idx: 200 |  Loss_1: (0.0875) | Acc_1: (96.96%) (24945/25728)\n",
      "Epoch: 71 | Batch_idx: 210 |  Loss_1: (0.0873) | Acc_1: (96.97%) (26191/27008)\n",
      "Epoch: 71 | Batch_idx: 220 |  Loss_1: (0.0867) | Acc_1: (96.99%) (27437/28288)\n",
      "Epoch: 71 | Batch_idx: 230 |  Loss_1: (0.0857) | Acc_1: (97.02%) (28687/29568)\n",
      "Epoch: 71 | Batch_idx: 240 |  Loss_1: (0.0854) | Acc_1: (97.01%) (29927/30848)\n",
      "Epoch: 71 | Batch_idx: 250 |  Loss_1: (0.0852) | Acc_1: (97.02%) (31172/32128)\n",
      "Epoch: 71 | Batch_idx: 260 |  Loss_1: (0.0850) | Acc_1: (97.02%) (32413/33408)\n",
      "Epoch: 71 | Batch_idx: 270 |  Loss_1: (0.0856) | Acc_1: (97.01%) (33651/34688)\n",
      "Epoch: 71 | Batch_idx: 280 |  Loss_1: (0.0856) | Acc_1: (97.02%) (34895/35968)\n",
      "Epoch: 71 | Batch_idx: 290 |  Loss_1: (0.0855) | Acc_1: (97.03%) (36140/37248)\n",
      "Epoch: 71 | Batch_idx: 300 |  Loss_1: (0.0853) | Acc_1: (97.03%) (37382/38528)\n",
      "Epoch: 71 | Batch_idx: 310 |  Loss_1: (0.0852) | Acc_1: (97.03%) (38625/39808)\n",
      "Epoch: 71 | Batch_idx: 320 |  Loss_1: (0.0856) | Acc_1: (97.01%) (39861/41088)\n",
      "Epoch: 71 | Batch_idx: 330 |  Loss_1: (0.0850) | Acc_1: (97.04%) (41113/42368)\n",
      "Epoch: 71 | Batch_idx: 340 |  Loss_1: (0.0855) | Acc_1: (97.03%) (42350/43648)\n",
      "Epoch: 71 | Batch_idx: 350 |  Loss_1: (0.0853) | Acc_1: (97.03%) (43595/44928)\n",
      "Epoch: 71 | Batch_idx: 360 |  Loss_1: (0.0853) | Acc_1: (97.02%) (44833/46208)\n",
      "Epoch: 71 | Batch_idx: 370 |  Loss_1: (0.0853) | Acc_1: (97.02%) (46074/47488)\n",
      "Epoch: 71 | Batch_idx: 380 |  Loss_1: (0.0854) | Acc_1: (97.02%) (47313/48768)\n",
      "Epoch: 71 | Batch_idx: 390 |  Loss_1: (0.0850) | Acc_1: (97.02%) (48508/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4337) | Acc: (90.09%) (9009/10000)\n",
      "Epoch: 72 | Batch_idx: 0 |  Loss_1: (0.0696) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 72 | Batch_idx: 10 |  Loss_1: (0.0661) | Acc_1: (97.51%) (1373/1408)\n",
      "Epoch: 72 | Batch_idx: 20 |  Loss_1: (0.0630) | Acc_1: (97.84%) (2630/2688)\n",
      "Epoch: 72 | Batch_idx: 30 |  Loss_1: (0.0688) | Acc_1: (97.63%) (3874/3968)\n",
      "Epoch: 72 | Batch_idx: 40 |  Loss_1: (0.0708) | Acc_1: (97.45%) (5114/5248)\n",
      "Epoch: 72 | Batch_idx: 50 |  Loss_1: (0.0743) | Acc_1: (97.37%) (6356/6528)\n",
      "Epoch: 72 | Batch_idx: 60 |  Loss_1: (0.0753) | Acc_1: (97.32%) (7599/7808)\n",
      "Epoch: 72 | Batch_idx: 70 |  Loss_1: (0.0744) | Acc_1: (97.33%) (8845/9088)\n",
      "Epoch: 72 | Batch_idx: 80 |  Loss_1: (0.0764) | Acc_1: (97.30%) (10088/10368)\n",
      "Epoch: 72 | Batch_idx: 90 |  Loss_1: (0.0774) | Acc_1: (97.24%) (11327/11648)\n",
      "Epoch: 72 | Batch_idx: 100 |  Loss_1: (0.0780) | Acc_1: (97.20%) (12566/12928)\n",
      "Epoch: 72 | Batch_idx: 110 |  Loss_1: (0.0781) | Acc_1: (97.21%) (13811/14208)\n",
      "Epoch: 72 | Batch_idx: 120 |  Loss_1: (0.0787) | Acc_1: (97.20%) (15055/15488)\n",
      "Epoch: 72 | Batch_idx: 130 |  Loss_1: (0.0796) | Acc_1: (97.13%) (16286/16768)\n",
      "Epoch: 72 | Batch_idx: 140 |  Loss_1: (0.0793) | Acc_1: (97.14%) (17532/18048)\n",
      "Epoch: 72 | Batch_idx: 150 |  Loss_1: (0.0788) | Acc_1: (97.15%) (18778/19328)\n",
      "Epoch: 72 | Batch_idx: 160 |  Loss_1: (0.0782) | Acc_1: (97.19%) (20028/20608)\n",
      "Epoch: 72 | Batch_idx: 170 |  Loss_1: (0.0776) | Acc_1: (97.21%) (21278/21888)\n",
      "Epoch: 72 | Batch_idx: 180 |  Loss_1: (0.0786) | Acc_1: (97.18%) (22514/23168)\n",
      "Epoch: 72 | Batch_idx: 190 |  Loss_1: (0.0775) | Acc_1: (97.22%) (23768/24448)\n",
      "Epoch: 72 | Batch_idx: 200 |  Loss_1: (0.0777) | Acc_1: (97.22%) (25012/25728)\n",
      "Epoch: 72 | Batch_idx: 210 |  Loss_1: (0.0771) | Acc_1: (97.24%) (26263/27008)\n",
      "Epoch: 72 | Batch_idx: 220 |  Loss_1: (0.0775) | Acc_1: (97.23%) (27505/28288)\n",
      "Epoch: 72 | Batch_idx: 230 |  Loss_1: (0.0779) | Acc_1: (97.24%) (28751/29568)\n",
      "Epoch: 72 | Batch_idx: 240 |  Loss_1: (0.0783) | Acc_1: (97.23%) (29994/30848)\n",
      "Epoch: 72 | Batch_idx: 250 |  Loss_1: (0.0781) | Acc_1: (97.24%) (31241/32128)\n",
      "Epoch: 72 | Batch_idx: 260 |  Loss_1: (0.0794) | Acc_1: (97.19%) (32470/33408)\n",
      "Epoch: 72 | Batch_idx: 270 |  Loss_1: (0.0794) | Acc_1: (97.19%) (33714/34688)\n",
      "Epoch: 72 | Batch_idx: 280 |  Loss_1: (0.0796) | Acc_1: (97.19%) (34957/35968)\n",
      "Epoch: 72 | Batch_idx: 290 |  Loss_1: (0.0801) | Acc_1: (97.17%) (36195/37248)\n",
      "Epoch: 72 | Batch_idx: 300 |  Loss_1: (0.0803) | Acc_1: (97.15%) (37429/38528)\n",
      "Epoch: 72 | Batch_idx: 310 |  Loss_1: (0.0810) | Acc_1: (97.14%) (38671/39808)\n",
      "Epoch: 72 | Batch_idx: 320 |  Loss_1: (0.0812) | Acc_1: (97.14%) (39913/41088)\n",
      "Epoch: 72 | Batch_idx: 330 |  Loss_1: (0.0816) | Acc_1: (97.16%) (41163/42368)\n",
      "Epoch: 72 | Batch_idx: 340 |  Loss_1: (0.0819) | Acc_1: (97.16%) (42410/43648)\n",
      "Epoch: 72 | Batch_idx: 350 |  Loss_1: (0.0819) | Acc_1: (97.17%) (43656/44928)\n",
      "Epoch: 72 | Batch_idx: 360 |  Loss_1: (0.0814) | Acc_1: (97.18%) (44903/46208)\n",
      "Epoch: 72 | Batch_idx: 370 |  Loss_1: (0.0815) | Acc_1: (97.16%) (46141/47488)\n",
      "Epoch: 72 | Batch_idx: 380 |  Loss_1: (0.0816) | Acc_1: (97.18%) (47391/48768)\n",
      "Epoch: 72 | Batch_idx: 390 |  Loss_1: (0.0816) | Acc_1: (97.19%) (48594/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4261) | Acc: (90.27%) (9027/10000)\n",
      "Epoch: 73 | Batch_idx: 0 |  Loss_1: (0.1584) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 73 | Batch_idx: 10 |  Loss_1: (0.0959) | Acc_1: (97.02%) (1366/1408)\n",
      "Epoch: 73 | Batch_idx: 20 |  Loss_1: (0.0908) | Acc_1: (96.88%) (2604/2688)\n",
      "Epoch: 73 | Batch_idx: 30 |  Loss_1: (0.0839) | Acc_1: (97.18%) (3856/3968)\n",
      "Epoch: 73 | Batch_idx: 40 |  Loss_1: (0.0844) | Acc_1: (97.26%) (5104/5248)\n",
      "Epoch: 73 | Batch_idx: 50 |  Loss_1: (0.0834) | Acc_1: (97.21%) (6346/6528)\n",
      "Epoch: 73 | Batch_idx: 60 |  Loss_1: (0.0801) | Acc_1: (97.30%) (7597/7808)\n",
      "Epoch: 73 | Batch_idx: 70 |  Loss_1: (0.0783) | Acc_1: (97.36%) (8848/9088)\n",
      "Epoch: 73 | Batch_idx: 80 |  Loss_1: (0.0793) | Acc_1: (97.35%) (10093/10368)\n",
      "Epoch: 73 | Batch_idx: 90 |  Loss_1: (0.0783) | Acc_1: (97.42%) (11347/11648)\n",
      "Epoch: 73 | Batch_idx: 100 |  Loss_1: (0.0782) | Acc_1: (97.46%) (12599/12928)\n",
      "Epoch: 73 | Batch_idx: 110 |  Loss_1: (0.0780) | Acc_1: (97.41%) (13840/14208)\n",
      "Epoch: 73 | Batch_idx: 120 |  Loss_1: (0.0782) | Acc_1: (97.39%) (15083/15488)\n",
      "Epoch: 73 | Batch_idx: 130 |  Loss_1: (0.0789) | Acc_1: (97.33%) (16321/16768)\n",
      "Epoch: 73 | Batch_idx: 140 |  Loss_1: (0.0789) | Acc_1: (97.33%) (17567/18048)\n",
      "Epoch: 73 | Batch_idx: 150 |  Loss_1: (0.0784) | Acc_1: (97.32%) (18810/19328)\n",
      "Epoch: 73 | Batch_idx: 160 |  Loss_1: (0.0779) | Acc_1: (97.36%) (20064/20608)\n",
      "Epoch: 73 | Batch_idx: 170 |  Loss_1: (0.0776) | Acc_1: (97.36%) (21311/21888)\n",
      "Epoch: 73 | Batch_idx: 180 |  Loss_1: (0.0773) | Acc_1: (97.38%) (22560/23168)\n",
      "Epoch: 73 | Batch_idx: 190 |  Loss_1: (0.0771) | Acc_1: (97.37%) (23806/24448)\n",
      "Epoch: 73 | Batch_idx: 200 |  Loss_1: (0.0775) | Acc_1: (97.35%) (25046/25728)\n",
      "Epoch: 73 | Batch_idx: 210 |  Loss_1: (0.0778) | Acc_1: (97.34%) (26290/27008)\n",
      "Epoch: 73 | Batch_idx: 220 |  Loss_1: (0.0781) | Acc_1: (97.33%) (27532/28288)\n",
      "Epoch: 73 | Batch_idx: 230 |  Loss_1: (0.0793) | Acc_1: (97.31%) (28772/29568)\n",
      "Epoch: 73 | Batch_idx: 240 |  Loss_1: (0.0786) | Acc_1: (97.33%) (30025/30848)\n",
      "Epoch: 73 | Batch_idx: 250 |  Loss_1: (0.0794) | Acc_1: (97.30%) (31259/32128)\n",
      "Epoch: 73 | Batch_idx: 260 |  Loss_1: (0.0795) | Acc_1: (97.30%) (32507/33408)\n",
      "Epoch: 73 | Batch_idx: 270 |  Loss_1: (0.0801) | Acc_1: (97.28%) (33745/34688)\n",
      "Epoch: 73 | Batch_idx: 280 |  Loss_1: (0.0803) | Acc_1: (97.27%) (34985/35968)\n",
      "Epoch: 73 | Batch_idx: 290 |  Loss_1: (0.0804) | Acc_1: (97.26%) (36227/37248)\n",
      "Epoch: 73 | Batch_idx: 300 |  Loss_1: (0.0798) | Acc_1: (97.28%) (37481/38528)\n",
      "Epoch: 73 | Batch_idx: 310 |  Loss_1: (0.0798) | Acc_1: (97.28%) (38724/39808)\n",
      "Epoch: 73 | Batch_idx: 320 |  Loss_1: (0.0802) | Acc_1: (97.27%) (39965/41088)\n",
      "Epoch: 73 | Batch_idx: 330 |  Loss_1: (0.0800) | Acc_1: (97.26%) (41209/42368)\n",
      "Epoch: 73 | Batch_idx: 340 |  Loss_1: (0.0810) | Acc_1: (97.23%) (42441/43648)\n",
      "Epoch: 73 | Batch_idx: 350 |  Loss_1: (0.0811) | Acc_1: (97.24%) (43687/44928)\n",
      "Epoch: 73 | Batch_idx: 360 |  Loss_1: (0.0814) | Acc_1: (97.23%) (44926/46208)\n",
      "Epoch: 73 | Batch_idx: 370 |  Loss_1: (0.0815) | Acc_1: (97.21%) (46163/47488)\n",
      "Epoch: 73 | Batch_idx: 380 |  Loss_1: (0.0816) | Acc_1: (97.22%) (47410/48768)\n",
      "Epoch: 73 | Batch_idx: 390 |  Loss_1: (0.0812) | Acc_1: (97.22%) (48611/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4162) | Acc: (90.26%) (9026/10000)\n",
      "Epoch: 74 | Batch_idx: 0 |  Loss_1: (0.0322) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 74 | Batch_idx: 10 |  Loss_1: (0.0720) | Acc_1: (97.44%) (1372/1408)\n",
      "Epoch: 74 | Batch_idx: 20 |  Loss_1: (0.0800) | Acc_1: (97.10%) (2610/2688)\n",
      "Epoch: 74 | Batch_idx: 30 |  Loss_1: (0.0788) | Acc_1: (97.10%) (3853/3968)\n",
      "Epoch: 74 | Batch_idx: 40 |  Loss_1: (0.0757) | Acc_1: (97.26%) (5104/5248)\n",
      "Epoch: 74 | Batch_idx: 50 |  Loss_1: (0.0788) | Acc_1: (97.12%) (6340/6528)\n",
      "Epoch: 74 | Batch_idx: 60 |  Loss_1: (0.0814) | Acc_1: (97.11%) (7582/7808)\n",
      "Epoch: 74 | Batch_idx: 70 |  Loss_1: (0.0802) | Acc_1: (97.17%) (8831/9088)\n",
      "Epoch: 74 | Batch_idx: 80 |  Loss_1: (0.0777) | Acc_1: (97.27%) (10085/10368)\n",
      "Epoch: 74 | Batch_idx: 90 |  Loss_1: (0.0783) | Acc_1: (97.20%) (11322/11648)\n",
      "Epoch: 74 | Batch_idx: 100 |  Loss_1: (0.0762) | Acc_1: (97.28%) (12576/12928)\n",
      "Epoch: 74 | Batch_idx: 110 |  Loss_1: (0.0757) | Acc_1: (97.30%) (13824/14208)\n",
      "Epoch: 74 | Batch_idx: 120 |  Loss_1: (0.0753) | Acc_1: (97.29%) (15069/15488)\n",
      "Epoch: 74 | Batch_idx: 130 |  Loss_1: (0.0750) | Acc_1: (97.32%) (16318/16768)\n",
      "Epoch: 74 | Batch_idx: 140 |  Loss_1: (0.0743) | Acc_1: (97.32%) (17565/18048)\n",
      "Epoch: 74 | Batch_idx: 150 |  Loss_1: (0.0748) | Acc_1: (97.33%) (18812/19328)\n",
      "Epoch: 74 | Batch_idx: 160 |  Loss_1: (0.0744) | Acc_1: (97.37%) (20065/20608)\n",
      "Epoch: 74 | Batch_idx: 170 |  Loss_1: (0.0746) | Acc_1: (97.38%) (21315/21888)\n",
      "Epoch: 74 | Batch_idx: 180 |  Loss_1: (0.0750) | Acc_1: (97.35%) (22555/23168)\n",
      "Epoch: 74 | Batch_idx: 190 |  Loss_1: (0.0757) | Acc_1: (97.32%) (23792/24448)\n",
      "Epoch: 74 | Batch_idx: 200 |  Loss_1: (0.0767) | Acc_1: (97.30%) (25033/25728)\n",
      "Epoch: 74 | Batch_idx: 210 |  Loss_1: (0.0763) | Acc_1: (97.32%) (26284/27008)\n",
      "Epoch: 74 | Batch_idx: 220 |  Loss_1: (0.0765) | Acc_1: (97.31%) (27527/28288)\n",
      "Epoch: 74 | Batch_idx: 230 |  Loss_1: (0.0768) | Acc_1: (97.32%) (28775/29568)\n",
      "Epoch: 74 | Batch_idx: 240 |  Loss_1: (0.0774) | Acc_1: (97.30%) (30015/30848)\n",
      "Epoch: 74 | Batch_idx: 250 |  Loss_1: (0.0780) | Acc_1: (97.26%) (31247/32128)\n",
      "Epoch: 74 | Batch_idx: 260 |  Loss_1: (0.0780) | Acc_1: (97.26%) (32493/33408)\n",
      "Epoch: 74 | Batch_idx: 270 |  Loss_1: (0.0783) | Acc_1: (97.25%) (33733/34688)\n",
      "Epoch: 74 | Batch_idx: 280 |  Loss_1: (0.0785) | Acc_1: (97.23%) (34973/35968)\n",
      "Epoch: 74 | Batch_idx: 290 |  Loss_1: (0.0781) | Acc_1: (97.25%) (36222/37248)\n",
      "Epoch: 74 | Batch_idx: 300 |  Loss_1: (0.0784) | Acc_1: (97.23%) (37461/38528)\n",
      "Epoch: 74 | Batch_idx: 310 |  Loss_1: (0.0785) | Acc_1: (97.24%) (38708/39808)\n",
      "Epoch: 74 | Batch_idx: 320 |  Loss_1: (0.0785) | Acc_1: (97.24%) (39954/41088)\n",
      "Epoch: 74 | Batch_idx: 330 |  Loss_1: (0.0783) | Acc_1: (97.25%) (41202/42368)\n",
      "Epoch: 74 | Batch_idx: 340 |  Loss_1: (0.0781) | Acc_1: (97.26%) (42451/43648)\n",
      "Epoch: 74 | Batch_idx: 350 |  Loss_1: (0.0775) | Acc_1: (97.28%) (43705/44928)\n",
      "Epoch: 74 | Batch_idx: 360 |  Loss_1: (0.0772) | Acc_1: (97.28%) (44953/46208)\n",
      "Epoch: 74 | Batch_idx: 370 |  Loss_1: (0.0769) | Acc_1: (97.29%) (46199/47488)\n",
      "Epoch: 74 | Batch_idx: 380 |  Loss_1: (0.0770) | Acc_1: (97.29%) (47445/48768)\n",
      "Epoch: 74 | Batch_idx: 390 |  Loss_1: (0.0769) | Acc_1: (97.29%) (48644/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3976) | Acc: (90.62%) (9062/10000)\n",
      "Epoch: 75 | Batch_idx: 0 |  Loss_1: (0.0511) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 75 | Batch_idx: 10 |  Loss_1: (0.0653) | Acc_1: (97.87%) (1378/1408)\n",
      "Epoch: 75 | Batch_idx: 20 |  Loss_1: (0.0621) | Acc_1: (97.81%) (2629/2688)\n",
      "Epoch: 75 | Batch_idx: 30 |  Loss_1: (0.0621) | Acc_1: (97.81%) (3881/3968)\n",
      "Epoch: 75 | Batch_idx: 40 |  Loss_1: (0.0622) | Acc_1: (97.79%) (5132/5248)\n",
      "Epoch: 75 | Batch_idx: 50 |  Loss_1: (0.0618) | Acc_1: (97.79%) (6384/6528)\n",
      "Epoch: 75 | Batch_idx: 60 |  Loss_1: (0.0639) | Acc_1: (97.75%) (7632/7808)\n",
      "Epoch: 75 | Batch_idx: 70 |  Loss_1: (0.0635) | Acc_1: (97.76%) (8884/9088)\n",
      "Epoch: 75 | Batch_idx: 80 |  Loss_1: (0.0622) | Acc_1: (97.81%) (10141/10368)\n",
      "Epoch: 75 | Batch_idx: 90 |  Loss_1: (0.0612) | Acc_1: (97.85%) (11397/11648)\n",
      "Epoch: 75 | Batch_idx: 100 |  Loss_1: (0.0624) | Acc_1: (97.80%) (12644/12928)\n",
      "Epoch: 75 | Batch_idx: 110 |  Loss_1: (0.0652) | Acc_1: (97.73%) (13886/14208)\n",
      "Epoch: 75 | Batch_idx: 120 |  Loss_1: (0.0660) | Acc_1: (97.71%) (15134/15488)\n",
      "Epoch: 75 | Batch_idx: 130 |  Loss_1: (0.0663) | Acc_1: (97.73%) (16387/16768)\n",
      "Epoch: 75 | Batch_idx: 140 |  Loss_1: (0.0675) | Acc_1: (97.68%) (17629/18048)\n",
      "Epoch: 75 | Batch_idx: 150 |  Loss_1: (0.0676) | Acc_1: (97.66%) (18875/19328)\n",
      "Epoch: 75 | Batch_idx: 160 |  Loss_1: (0.0681) | Acc_1: (97.64%) (20122/20608)\n",
      "Epoch: 75 | Batch_idx: 170 |  Loss_1: (0.0694) | Acc_1: (97.58%) (21358/21888)\n",
      "Epoch: 75 | Batch_idx: 180 |  Loss_1: (0.0706) | Acc_1: (97.54%) (22597/23168)\n",
      "Epoch: 75 | Batch_idx: 190 |  Loss_1: (0.0708) | Acc_1: (97.51%) (23840/24448)\n",
      "Epoch: 75 | Batch_idx: 200 |  Loss_1: (0.0722) | Acc_1: (97.47%) (25076/25728)\n",
      "Epoch: 75 | Batch_idx: 210 |  Loss_1: (0.0723) | Acc_1: (97.45%) (26319/27008)\n",
      "Epoch: 75 | Batch_idx: 220 |  Loss_1: (0.0731) | Acc_1: (97.41%) (27555/28288)\n",
      "Epoch: 75 | Batch_idx: 230 |  Loss_1: (0.0737) | Acc_1: (97.40%) (28799/29568)\n",
      "Epoch: 75 | Batch_idx: 240 |  Loss_1: (0.0742) | Acc_1: (97.38%) (30040/30848)\n",
      "Epoch: 75 | Batch_idx: 250 |  Loss_1: (0.0749) | Acc_1: (97.36%) (31279/32128)\n",
      "Epoch: 75 | Batch_idx: 260 |  Loss_1: (0.0751) | Acc_1: (97.34%) (32521/33408)\n",
      "Epoch: 75 | Batch_idx: 270 |  Loss_1: (0.0759) | Acc_1: (97.32%) (33760/34688)\n",
      "Epoch: 75 | Batch_idx: 280 |  Loss_1: (0.0763) | Acc_1: (97.30%) (34998/35968)\n",
      "Epoch: 75 | Batch_idx: 290 |  Loss_1: (0.0769) | Acc_1: (97.28%) (36234/37248)\n",
      "Epoch: 75 | Batch_idx: 300 |  Loss_1: (0.0768) | Acc_1: (97.27%) (37477/38528)\n",
      "Epoch: 75 | Batch_idx: 310 |  Loss_1: (0.0772) | Acc_1: (97.27%) (38721/39808)\n",
      "Epoch: 75 | Batch_idx: 320 |  Loss_1: (0.0772) | Acc_1: (97.26%) (39962/41088)\n",
      "Epoch: 75 | Batch_idx: 330 |  Loss_1: (0.0770) | Acc_1: (97.27%) (41212/42368)\n",
      "Epoch: 75 | Batch_idx: 340 |  Loss_1: (0.0770) | Acc_1: (97.27%) (42458/43648)\n",
      "Epoch: 75 | Batch_idx: 350 |  Loss_1: (0.0773) | Acc_1: (97.28%) (43706/44928)\n",
      "Epoch: 75 | Batch_idx: 360 |  Loss_1: (0.0777) | Acc_1: (97.28%) (44949/46208)\n",
      "Epoch: 75 | Batch_idx: 370 |  Loss_1: (0.0775) | Acc_1: (97.28%) (46196/47488)\n",
      "Epoch: 75 | Batch_idx: 380 |  Loss_1: (0.0778) | Acc_1: (97.26%) (47430/48768)\n",
      "Epoch: 75 | Batch_idx: 390 |  Loss_1: (0.0774) | Acc_1: (97.26%) (48629/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4417) | Acc: (89.86%) (8986/10000)\n",
      "Epoch: 76 | Batch_idx: 0 |  Loss_1: (0.0331) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 76 | Batch_idx: 10 |  Loss_1: (0.0807) | Acc_1: (97.73%) (1376/1408)\n",
      "Epoch: 76 | Batch_idx: 20 |  Loss_1: (0.0694) | Acc_1: (97.84%) (2630/2688)\n",
      "Epoch: 76 | Batch_idx: 30 |  Loss_1: (0.0702) | Acc_1: (97.86%) (3883/3968)\n",
      "Epoch: 76 | Batch_idx: 40 |  Loss_1: (0.0666) | Acc_1: (97.96%) (5141/5248)\n",
      "Epoch: 76 | Batch_idx: 50 |  Loss_1: (0.0695) | Acc_1: (97.93%) (6393/6528)\n",
      "Epoch: 76 | Batch_idx: 60 |  Loss_1: (0.0679) | Acc_1: (97.93%) (7646/7808)\n",
      "Epoch: 76 | Batch_idx: 70 |  Loss_1: (0.0687) | Acc_1: (97.83%) (8891/9088)\n",
      "Epoch: 76 | Batch_idx: 80 |  Loss_1: (0.0677) | Acc_1: (97.82%) (10142/10368)\n",
      "Epoch: 76 | Batch_idx: 90 |  Loss_1: (0.0691) | Acc_1: (97.77%) (11388/11648)\n",
      "Epoch: 76 | Batch_idx: 100 |  Loss_1: (0.0692) | Acc_1: (97.73%) (12635/12928)\n",
      "Epoch: 76 | Batch_idx: 110 |  Loss_1: (0.0720) | Acc_1: (97.64%) (13873/14208)\n",
      "Epoch: 76 | Batch_idx: 120 |  Loss_1: (0.0725) | Acc_1: (97.62%) (15119/15488)\n",
      "Epoch: 76 | Batch_idx: 130 |  Loss_1: (0.0721) | Acc_1: (97.62%) (16369/16768)\n",
      "Epoch: 76 | Batch_idx: 140 |  Loss_1: (0.0716) | Acc_1: (97.63%) (17620/18048)\n",
      "Epoch: 76 | Batch_idx: 150 |  Loss_1: (0.0717) | Acc_1: (97.61%) (18866/19328)\n",
      "Epoch: 76 | Batch_idx: 160 |  Loss_1: (0.0720) | Acc_1: (97.60%) (20114/20608)\n",
      "Epoch: 76 | Batch_idx: 170 |  Loss_1: (0.0721) | Acc_1: (97.59%) (21360/21888)\n",
      "Epoch: 76 | Batch_idx: 180 |  Loss_1: (0.0729) | Acc_1: (97.56%) (22602/23168)\n",
      "Epoch: 76 | Batch_idx: 190 |  Loss_1: (0.0730) | Acc_1: (97.55%) (23848/24448)\n",
      "Epoch: 76 | Batch_idx: 200 |  Loss_1: (0.0729) | Acc_1: (97.53%) (25093/25728)\n",
      "Epoch: 76 | Batch_idx: 210 |  Loss_1: (0.0734) | Acc_1: (97.49%) (26330/27008)\n",
      "Epoch: 76 | Batch_idx: 220 |  Loss_1: (0.0745) | Acc_1: (97.46%) (27570/28288)\n",
      "Epoch: 76 | Batch_idx: 230 |  Loss_1: (0.0747) | Acc_1: (97.45%) (28814/29568)\n",
      "Epoch: 76 | Batch_idx: 240 |  Loss_1: (0.0751) | Acc_1: (97.45%) (30060/30848)\n",
      "Epoch: 76 | Batch_idx: 250 |  Loss_1: (0.0756) | Acc_1: (97.43%) (31301/32128)\n",
      "Epoch: 76 | Batch_idx: 260 |  Loss_1: (0.0757) | Acc_1: (97.40%) (32541/33408)\n",
      "Epoch: 76 | Batch_idx: 270 |  Loss_1: (0.0759) | Acc_1: (97.40%) (33787/34688)\n",
      "Epoch: 76 | Batch_idx: 280 |  Loss_1: (0.0759) | Acc_1: (97.41%) (35037/35968)\n",
      "Epoch: 76 | Batch_idx: 290 |  Loss_1: (0.0760) | Acc_1: (97.41%) (36282/37248)\n",
      "Epoch: 76 | Batch_idx: 300 |  Loss_1: (0.0761) | Acc_1: (97.39%) (37522/38528)\n",
      "Epoch: 76 | Batch_idx: 310 |  Loss_1: (0.0762) | Acc_1: (97.39%) (38771/39808)\n",
      "Epoch: 76 | Batch_idx: 320 |  Loss_1: (0.0765) | Acc_1: (97.38%) (40013/41088)\n",
      "Epoch: 76 | Batch_idx: 330 |  Loss_1: (0.0765) | Acc_1: (97.38%) (41257/42368)\n",
      "Epoch: 76 | Batch_idx: 340 |  Loss_1: (0.0768) | Acc_1: (97.36%) (42495/43648)\n",
      "Epoch: 76 | Batch_idx: 350 |  Loss_1: (0.0770) | Acc_1: (97.35%) (43736/44928)\n",
      "Epoch: 76 | Batch_idx: 360 |  Loss_1: (0.0775) | Acc_1: (97.34%) (44981/46208)\n",
      "Epoch: 76 | Batch_idx: 370 |  Loss_1: (0.0775) | Acc_1: (97.35%) (46228/47488)\n",
      "Epoch: 76 | Batch_idx: 380 |  Loss_1: (0.0773) | Acc_1: (97.35%) (47474/48768)\n",
      "Epoch: 76 | Batch_idx: 390 |  Loss_1: (0.0771) | Acc_1: (97.35%) (48674/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4076) | Acc: (90.92%) (9092/10000)\n",
      "Epoch: 77 | Batch_idx: 0 |  Loss_1: (0.0516) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 77 | Batch_idx: 10 |  Loss_1: (0.0697) | Acc_1: (97.51%) (1373/1408)\n",
      "Epoch: 77 | Batch_idx: 20 |  Loss_1: (0.0809) | Acc_1: (97.10%) (2610/2688)\n",
      "Epoch: 77 | Batch_idx: 30 |  Loss_1: (0.0772) | Acc_1: (97.28%) (3860/3968)\n",
      "Epoch: 77 | Batch_idx: 40 |  Loss_1: (0.0763) | Acc_1: (97.31%) (5107/5248)\n",
      "Epoch: 77 | Batch_idx: 50 |  Loss_1: (0.0759) | Acc_1: (97.40%) (6358/6528)\n",
      "Epoch: 77 | Batch_idx: 60 |  Loss_1: (0.0761) | Acc_1: (97.43%) (7607/7808)\n",
      "Epoch: 77 | Batch_idx: 70 |  Loss_1: (0.0760) | Acc_1: (97.43%) (8854/9088)\n",
      "Epoch: 77 | Batch_idx: 80 |  Loss_1: (0.0747) | Acc_1: (97.47%) (10106/10368)\n",
      "Epoch: 77 | Batch_idx: 90 |  Loss_1: (0.0725) | Acc_1: (97.56%) (11364/11648)\n",
      "Epoch: 77 | Batch_idx: 100 |  Loss_1: (0.0708) | Acc_1: (97.61%) (12619/12928)\n",
      "Epoch: 77 | Batch_idx: 110 |  Loss_1: (0.0700) | Acc_1: (97.64%) (13872/14208)\n",
      "Epoch: 77 | Batch_idx: 120 |  Loss_1: (0.0723) | Acc_1: (97.59%) (15114/15488)\n",
      "Epoch: 77 | Batch_idx: 130 |  Loss_1: (0.0718) | Acc_1: (97.61%) (16367/16768)\n",
      "Epoch: 77 | Batch_idx: 140 |  Loss_1: (0.0720) | Acc_1: (97.61%) (17617/18048)\n",
      "Epoch: 77 | Batch_idx: 150 |  Loss_1: (0.0731) | Acc_1: (97.58%) (18861/19328)\n",
      "Epoch: 77 | Batch_idx: 160 |  Loss_1: (0.0731) | Acc_1: (97.56%) (20106/20608)\n",
      "Epoch: 77 | Batch_idx: 170 |  Loss_1: (0.0739) | Acc_1: (97.55%) (21351/21888)\n",
      "Epoch: 77 | Batch_idx: 180 |  Loss_1: (0.0732) | Acc_1: (97.56%) (22602/23168)\n",
      "Epoch: 77 | Batch_idx: 190 |  Loss_1: (0.0735) | Acc_1: (97.55%) (23849/24448)\n",
      "Epoch: 77 | Batch_idx: 200 |  Loss_1: (0.0739) | Acc_1: (97.54%) (25095/25728)\n",
      "Epoch: 77 | Batch_idx: 210 |  Loss_1: (0.0735) | Acc_1: (97.55%) (26347/27008)\n",
      "Epoch: 77 | Batch_idx: 220 |  Loss_1: (0.0737) | Acc_1: (97.53%) (27589/28288)\n",
      "Epoch: 77 | Batch_idx: 230 |  Loss_1: (0.0738) | Acc_1: (97.53%) (28837/29568)\n",
      "Epoch: 77 | Batch_idx: 240 |  Loss_1: (0.0735) | Acc_1: (97.55%) (30091/30848)\n",
      "Epoch: 77 | Batch_idx: 250 |  Loss_1: (0.0738) | Acc_1: (97.53%) (31336/32128)\n",
      "Epoch: 77 | Batch_idx: 260 |  Loss_1: (0.0736) | Acc_1: (97.53%) (32584/33408)\n",
      "Epoch: 77 | Batch_idx: 270 |  Loss_1: (0.0739) | Acc_1: (97.51%) (33825/34688)\n",
      "Epoch: 77 | Batch_idx: 280 |  Loss_1: (0.0734) | Acc_1: (97.54%) (35084/35968)\n",
      "Epoch: 77 | Batch_idx: 290 |  Loss_1: (0.0735) | Acc_1: (97.53%) (36329/37248)\n",
      "Epoch: 77 | Batch_idx: 300 |  Loss_1: (0.0735) | Acc_1: (97.54%) (37579/38528)\n",
      "Epoch: 77 | Batch_idx: 310 |  Loss_1: (0.0728) | Acc_1: (97.55%) (38833/39808)\n",
      "Epoch: 77 | Batch_idx: 320 |  Loss_1: (0.0730) | Acc_1: (97.53%) (40074/41088)\n",
      "Epoch: 77 | Batch_idx: 330 |  Loss_1: (0.0733) | Acc_1: (97.51%) (41312/42368)\n",
      "Epoch: 77 | Batch_idx: 340 |  Loss_1: (0.0726) | Acc_1: (97.53%) (42570/43648)\n",
      "Epoch: 77 | Batch_idx: 350 |  Loss_1: (0.0731) | Acc_1: (97.50%) (43806/44928)\n",
      "Epoch: 77 | Batch_idx: 360 |  Loss_1: (0.0732) | Acc_1: (97.50%) (45055/46208)\n",
      "Epoch: 77 | Batch_idx: 370 |  Loss_1: (0.0736) | Acc_1: (97.48%) (46290/47488)\n",
      "Epoch: 77 | Batch_idx: 380 |  Loss_1: (0.0738) | Acc_1: (97.46%) (47531/48768)\n",
      "Epoch: 77 | Batch_idx: 390 |  Loss_1: (0.0740) | Acc_1: (97.48%) (48738/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4319) | Acc: (90.01%) (9001/10000)\n",
      "Epoch: 78 | Batch_idx: 0 |  Loss_1: (0.0862) | Acc_1: (95.31%) (122/128)\n",
      "Epoch: 78 | Batch_idx: 10 |  Loss_1: (0.0830) | Acc_1: (96.88%) (1364/1408)\n",
      "Epoch: 78 | Batch_idx: 20 |  Loss_1: (0.0759) | Acc_1: (97.36%) (2617/2688)\n",
      "Epoch: 78 | Batch_idx: 30 |  Loss_1: (0.0727) | Acc_1: (97.56%) (3871/3968)\n",
      "Epoch: 78 | Batch_idx: 40 |  Loss_1: (0.0683) | Acc_1: (97.68%) (5126/5248)\n",
      "Epoch: 78 | Batch_idx: 50 |  Loss_1: (0.0723) | Acc_1: (97.49%) (6364/6528)\n",
      "Epoch: 78 | Batch_idx: 60 |  Loss_1: (0.0695) | Acc_1: (97.58%) (7619/7808)\n",
      "Epoch: 78 | Batch_idx: 70 |  Loss_1: (0.0714) | Acc_1: (97.60%) (8870/9088)\n",
      "Epoch: 78 | Batch_idx: 80 |  Loss_1: (0.0701) | Acc_1: (97.62%) (10121/10368)\n",
      "Epoch: 78 | Batch_idx: 90 |  Loss_1: (0.0683) | Acc_1: (97.65%) (11374/11648)\n",
      "Epoch: 78 | Batch_idx: 100 |  Loss_1: (0.0693) | Acc_1: (97.58%) (12615/12928)\n",
      "Epoch: 78 | Batch_idx: 110 |  Loss_1: (0.0685) | Acc_1: (97.63%) (13871/14208)\n",
      "Epoch: 78 | Batch_idx: 120 |  Loss_1: (0.0685) | Acc_1: (97.64%) (15122/15488)\n",
      "Epoch: 78 | Batch_idx: 130 |  Loss_1: (0.0683) | Acc_1: (97.65%) (16374/16768)\n",
      "Epoch: 78 | Batch_idx: 140 |  Loss_1: (0.0690) | Acc_1: (97.60%) (17615/18048)\n",
      "Epoch: 78 | Batch_idx: 150 |  Loss_1: (0.0692) | Acc_1: (97.59%) (18863/19328)\n",
      "Epoch: 78 | Batch_idx: 160 |  Loss_1: (0.0699) | Acc_1: (97.56%) (20105/20608)\n",
      "Epoch: 78 | Batch_idx: 170 |  Loss_1: (0.0713) | Acc_1: (97.51%) (21342/21888)\n",
      "Epoch: 78 | Batch_idx: 180 |  Loss_1: (0.0726) | Acc_1: (97.47%) (22581/23168)\n",
      "Epoch: 78 | Batch_idx: 190 |  Loss_1: (0.0725) | Acc_1: (97.47%) (23829/24448)\n",
      "Epoch: 78 | Batch_idx: 200 |  Loss_1: (0.0738) | Acc_1: (97.42%) (25065/25728)\n",
      "Epoch: 78 | Batch_idx: 210 |  Loss_1: (0.0733) | Acc_1: (97.44%) (26317/27008)\n",
      "Epoch: 78 | Batch_idx: 220 |  Loss_1: (0.0739) | Acc_1: (97.43%) (27560/28288)\n",
      "Epoch: 78 | Batch_idx: 230 |  Loss_1: (0.0738) | Acc_1: (97.41%) (28803/29568)\n",
      "Epoch: 78 | Batch_idx: 240 |  Loss_1: (0.0744) | Acc_1: (97.40%) (30047/30848)\n",
      "Epoch: 78 | Batch_idx: 250 |  Loss_1: (0.0736) | Acc_1: (97.44%) (31304/32128)\n",
      "Epoch: 78 | Batch_idx: 260 |  Loss_1: (0.0730) | Acc_1: (97.44%) (32553/33408)\n",
      "Epoch: 78 | Batch_idx: 270 |  Loss_1: (0.0722) | Acc_1: (97.46%) (33808/34688)\n",
      "Epoch: 78 | Batch_idx: 280 |  Loss_1: (0.0727) | Acc_1: (97.45%) (35051/35968)\n",
      "Epoch: 78 | Batch_idx: 290 |  Loss_1: (0.0728) | Acc_1: (97.44%) (36293/37248)\n",
      "Epoch: 78 | Batch_idx: 300 |  Loss_1: (0.0721) | Acc_1: (97.47%) (37555/38528)\n",
      "Epoch: 78 | Batch_idx: 310 |  Loss_1: (0.0729) | Acc_1: (97.44%) (38789/39808)\n",
      "Epoch: 78 | Batch_idx: 320 |  Loss_1: (0.0730) | Acc_1: (97.44%) (40037/41088)\n",
      "Epoch: 78 | Batch_idx: 330 |  Loss_1: (0.0729) | Acc_1: (97.45%) (41289/42368)\n",
      "Epoch: 78 | Batch_idx: 340 |  Loss_1: (0.0729) | Acc_1: (97.45%) (42535/43648)\n",
      "Epoch: 78 | Batch_idx: 350 |  Loss_1: (0.0734) | Acc_1: (97.42%) (43771/44928)\n",
      "Epoch: 78 | Batch_idx: 360 |  Loss_1: (0.0739) | Acc_1: (97.41%) (45012/46208)\n",
      "Epoch: 78 | Batch_idx: 370 |  Loss_1: (0.0743) | Acc_1: (97.41%) (46260/47488)\n",
      "Epoch: 78 | Batch_idx: 380 |  Loss_1: (0.0743) | Acc_1: (97.41%) (47506/48768)\n",
      "Epoch: 78 | Batch_idx: 390 |  Loss_1: (0.0744) | Acc_1: (97.41%) (48707/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4491) | Acc: (89.87%) (8987/10000)\n",
      "Epoch: 79 | Batch_idx: 0 |  Loss_1: (0.1322) | Acc_1: (93.75%) (120/128)\n",
      "Epoch: 79 | Batch_idx: 10 |  Loss_1: (0.0662) | Acc_1: (97.23%) (1369/1408)\n",
      "Epoch: 79 | Batch_idx: 20 |  Loss_1: (0.0753) | Acc_1: (97.10%) (2610/2688)\n",
      "Epoch: 79 | Batch_idx: 30 |  Loss_1: (0.0785) | Acc_1: (96.95%) (3847/3968)\n",
      "Epoch: 79 | Batch_idx: 40 |  Loss_1: (0.0769) | Acc_1: (97.03%) (5092/5248)\n",
      "Epoch: 79 | Batch_idx: 50 |  Loss_1: (0.0741) | Acc_1: (97.17%) (6343/6528)\n",
      "Epoch: 79 | Batch_idx: 60 |  Loss_1: (0.0721) | Acc_1: (97.18%) (7588/7808)\n",
      "Epoch: 79 | Batch_idx: 70 |  Loss_1: (0.0695) | Acc_1: (97.37%) (8849/9088)\n",
      "Epoch: 79 | Batch_idx: 80 |  Loss_1: (0.0701) | Acc_1: (97.39%) (10097/10368)\n",
      "Epoch: 79 | Batch_idx: 90 |  Loss_1: (0.0710) | Acc_1: (97.36%) (11340/11648)\n",
      "Epoch: 79 | Batch_idx: 100 |  Loss_1: (0.0706) | Acc_1: (97.36%) (12587/12928)\n",
      "Epoch: 79 | Batch_idx: 110 |  Loss_1: (0.0703) | Acc_1: (97.39%) (13837/14208)\n",
      "Epoch: 79 | Batch_idx: 120 |  Loss_1: (0.0678) | Acc_1: (97.47%) (15096/15488)\n",
      "Epoch: 79 | Batch_idx: 130 |  Loss_1: (0.0682) | Acc_1: (97.48%) (16345/16768)\n",
      "Epoch: 79 | Batch_idx: 140 |  Loss_1: (0.0688) | Acc_1: (97.48%) (17594/18048)\n",
      "Epoch: 79 | Batch_idx: 150 |  Loss_1: (0.0703) | Acc_1: (97.45%) (18836/19328)\n",
      "Epoch: 79 | Batch_idx: 160 |  Loss_1: (0.0698) | Acc_1: (97.46%) (20084/20608)\n",
      "Epoch: 79 | Batch_idx: 170 |  Loss_1: (0.0698) | Acc_1: (97.45%) (21329/21888)\n",
      "Epoch: 79 | Batch_idx: 180 |  Loss_1: (0.0701) | Acc_1: (97.44%) (22575/23168)\n",
      "Epoch: 79 | Batch_idx: 190 |  Loss_1: (0.0701) | Acc_1: (97.45%) (23824/24448)\n",
      "Epoch: 79 | Batch_idx: 200 |  Loss_1: (0.0708) | Acc_1: (97.43%) (25068/25728)\n",
      "Epoch: 79 | Batch_idx: 210 |  Loss_1: (0.0706) | Acc_1: (97.46%) (26321/27008)\n",
      "Epoch: 79 | Batch_idx: 220 |  Loss_1: (0.0699) | Acc_1: (97.48%) (27576/28288)\n",
      "Epoch: 79 | Batch_idx: 230 |  Loss_1: (0.0697) | Acc_1: (97.49%) (28825/29568)\n",
      "Epoch: 79 | Batch_idx: 240 |  Loss_1: (0.0700) | Acc_1: (97.49%) (30073/30848)\n",
      "Epoch: 79 | Batch_idx: 250 |  Loss_1: (0.0698) | Acc_1: (97.52%) (31330/32128)\n",
      "Epoch: 79 | Batch_idx: 260 |  Loss_1: (0.0700) | Acc_1: (97.50%) (32574/33408)\n",
      "Epoch: 79 | Batch_idx: 270 |  Loss_1: (0.0704) | Acc_1: (97.49%) (33816/34688)\n",
      "Epoch: 79 | Batch_idx: 280 |  Loss_1: (0.0710) | Acc_1: (97.46%) (35053/35968)\n",
      "Epoch: 79 | Batch_idx: 290 |  Loss_1: (0.0708) | Acc_1: (97.47%) (36305/37248)\n",
      "Epoch: 79 | Batch_idx: 300 |  Loss_1: (0.0707) | Acc_1: (97.47%) (37552/38528)\n",
      "Epoch: 79 | Batch_idx: 310 |  Loss_1: (0.0707) | Acc_1: (97.48%) (38805/39808)\n",
      "Epoch: 79 | Batch_idx: 320 |  Loss_1: (0.0703) | Acc_1: (97.50%) (40061/41088)\n",
      "Epoch: 79 | Batch_idx: 330 |  Loss_1: (0.0700) | Acc_1: (97.52%) (41318/42368)\n",
      "Epoch: 79 | Batch_idx: 340 |  Loss_1: (0.0701) | Acc_1: (97.53%) (42569/43648)\n",
      "Epoch: 79 | Batch_idx: 350 |  Loss_1: (0.0701) | Acc_1: (97.51%) (43811/44928)\n",
      "Epoch: 79 | Batch_idx: 360 |  Loss_1: (0.0701) | Acc_1: (97.50%) (45055/46208)\n",
      "Epoch: 79 | Batch_idx: 370 |  Loss_1: (0.0701) | Acc_1: (97.51%) (46305/47488)\n",
      "Epoch: 79 | Batch_idx: 380 |  Loss_1: (0.0708) | Acc_1: (97.49%) (47543/48768)\n",
      "Epoch: 79 | Batch_idx: 390 |  Loss_1: (0.0710) | Acc_1: (97.48%) (48742/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4867) | Acc: (89.65%) (8965/10000)\n",
      "Epoch: 80 | Batch_idx: 0 |  Loss_1: (0.0919) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 80 | Batch_idx: 10 |  Loss_1: (0.0793) | Acc_1: (97.51%) (1373/1408)\n",
      "Epoch: 80 | Batch_idx: 20 |  Loss_1: (0.0729) | Acc_1: (97.69%) (2626/2688)\n",
      "Epoch: 80 | Batch_idx: 30 |  Loss_1: (0.0719) | Acc_1: (97.68%) (3876/3968)\n",
      "Epoch: 80 | Batch_idx: 40 |  Loss_1: (0.0699) | Acc_1: (97.75%) (5130/5248)\n",
      "Epoch: 80 | Batch_idx: 50 |  Loss_1: (0.0670) | Acc_1: (97.84%) (6387/6528)\n",
      "Epoch: 80 | Batch_idx: 60 |  Loss_1: (0.0707) | Acc_1: (97.63%) (7623/7808)\n",
      "Epoch: 80 | Batch_idx: 70 |  Loss_1: (0.0726) | Acc_1: (97.52%) (8863/9088)\n",
      "Epoch: 80 | Batch_idx: 80 |  Loss_1: (0.0712) | Acc_1: (97.50%) (10109/10368)\n",
      "Epoch: 80 | Batch_idx: 90 |  Loss_1: (0.0713) | Acc_1: (97.46%) (11352/11648)\n",
      "Epoch: 80 | Batch_idx: 100 |  Loss_1: (0.0717) | Acc_1: (97.45%) (12598/12928)\n",
      "Epoch: 80 | Batch_idx: 110 |  Loss_1: (0.0719) | Acc_1: (97.51%) (13854/14208)\n",
      "Epoch: 80 | Batch_idx: 120 |  Loss_1: (0.0716) | Acc_1: (97.52%) (15104/15488)\n",
      "Epoch: 80 | Batch_idx: 130 |  Loss_1: (0.0700) | Acc_1: (97.52%) (16352/16768)\n",
      "Epoch: 80 | Batch_idx: 140 |  Loss_1: (0.0706) | Acc_1: (97.52%) (17600/18048)\n",
      "Epoch: 80 | Batch_idx: 150 |  Loss_1: (0.0710) | Acc_1: (97.52%) (18848/19328)\n",
      "Epoch: 80 | Batch_idx: 160 |  Loss_1: (0.0701) | Acc_1: (97.55%) (20103/20608)\n",
      "Epoch: 80 | Batch_idx: 170 |  Loss_1: (0.0701) | Acc_1: (97.53%) (21347/21888)\n",
      "Epoch: 80 | Batch_idx: 180 |  Loss_1: (0.0704) | Acc_1: (97.52%) (22593/23168)\n",
      "Epoch: 80 | Batch_idx: 190 |  Loss_1: (0.0698) | Acc_1: (97.53%) (23844/24448)\n",
      "Epoch: 80 | Batch_idx: 200 |  Loss_1: (0.0690) | Acc_1: (97.54%) (25096/25728)\n",
      "Epoch: 80 | Batch_idx: 210 |  Loss_1: (0.0689) | Acc_1: (97.55%) (26345/27008)\n",
      "Epoch: 80 | Batch_idx: 220 |  Loss_1: (0.0688) | Acc_1: (97.54%) (27592/28288)\n",
      "Epoch: 80 | Batch_idx: 230 |  Loss_1: (0.0682) | Acc_1: (97.57%) (28850/29568)\n",
      "Epoch: 80 | Batch_idx: 240 |  Loss_1: (0.0682) | Acc_1: (97.58%) (30100/30848)\n",
      "Epoch: 80 | Batch_idx: 250 |  Loss_1: (0.0690) | Acc_1: (97.56%) (31343/32128)\n",
      "Epoch: 80 | Batch_idx: 260 |  Loss_1: (0.0696) | Acc_1: (97.53%) (32582/33408)\n",
      "Epoch: 80 | Batch_idx: 270 |  Loss_1: (0.0695) | Acc_1: (97.56%) (33840/34688)\n",
      "Epoch: 80 | Batch_idx: 280 |  Loss_1: (0.0692) | Acc_1: (97.57%) (35093/35968)\n",
      "Epoch: 80 | Batch_idx: 290 |  Loss_1: (0.0693) | Acc_1: (97.55%) (36337/37248)\n",
      "Epoch: 80 | Batch_idx: 300 |  Loss_1: (0.0698) | Acc_1: (97.55%) (37585/38528)\n",
      "Epoch: 80 | Batch_idx: 310 |  Loss_1: (0.0699) | Acc_1: (97.54%) (38828/39808)\n",
      "Epoch: 80 | Batch_idx: 320 |  Loss_1: (0.0698) | Acc_1: (97.55%) (40082/41088)\n",
      "Epoch: 80 | Batch_idx: 330 |  Loss_1: (0.0700) | Acc_1: (97.55%) (41328/42368)\n",
      "Epoch: 80 | Batch_idx: 340 |  Loss_1: (0.0701) | Acc_1: (97.53%) (42570/43648)\n",
      "Epoch: 80 | Batch_idx: 350 |  Loss_1: (0.0702) | Acc_1: (97.54%) (43821/44928)\n",
      "Epoch: 80 | Batch_idx: 360 |  Loss_1: (0.0699) | Acc_1: (97.56%) (45080/46208)\n",
      "Epoch: 80 | Batch_idx: 370 |  Loss_1: (0.0701) | Acc_1: (97.55%) (46323/47488)\n",
      "Epoch: 80 | Batch_idx: 380 |  Loss_1: (0.0698) | Acc_1: (97.56%) (47578/48768)\n",
      "Epoch: 80 | Batch_idx: 390 |  Loss_1: (0.0700) | Acc_1: (97.54%) (48771/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4350) | Acc: (90.01%) (9001/10000)\n",
      "Epoch: 81 | Batch_idx: 0 |  Loss_1: (0.0345) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 81 | Batch_idx: 10 |  Loss_1: (0.0603) | Acc_1: (97.87%) (1378/1408)\n",
      "Epoch: 81 | Batch_idx: 20 |  Loss_1: (0.0639) | Acc_1: (97.84%) (2630/2688)\n",
      "Epoch: 81 | Batch_idx: 30 |  Loss_1: (0.0715) | Acc_1: (97.51%) (3869/3968)\n",
      "Epoch: 81 | Batch_idx: 40 |  Loss_1: (0.0709) | Acc_1: (97.60%) (5122/5248)\n",
      "Epoch: 81 | Batch_idx: 50 |  Loss_1: (0.0704) | Acc_1: (97.58%) (6370/6528)\n",
      "Epoch: 81 | Batch_idx: 60 |  Loss_1: (0.0736) | Acc_1: (97.45%) (7609/7808)\n",
      "Epoch: 81 | Batch_idx: 70 |  Loss_1: (0.0721) | Acc_1: (97.45%) (8856/9088)\n",
      "Epoch: 81 | Batch_idx: 80 |  Loss_1: (0.0716) | Acc_1: (97.46%) (10105/10368)\n",
      "Epoch: 81 | Batch_idx: 90 |  Loss_1: (0.0703) | Acc_1: (97.52%) (11359/11648)\n",
      "Epoch: 81 | Batch_idx: 100 |  Loss_1: (0.0693) | Acc_1: (97.54%) (12610/12928)\n",
      "Epoch: 81 | Batch_idx: 110 |  Loss_1: (0.0689) | Acc_1: (97.54%) (13859/14208)\n",
      "Epoch: 81 | Batch_idx: 120 |  Loss_1: (0.0691) | Acc_1: (97.54%) (15107/15488)\n",
      "Epoch: 81 | Batch_idx: 130 |  Loss_1: (0.0688) | Acc_1: (97.56%) (16359/16768)\n",
      "Epoch: 81 | Batch_idx: 140 |  Loss_1: (0.0693) | Acc_1: (97.57%) (17609/18048)\n",
      "Epoch: 81 | Batch_idx: 150 |  Loss_1: (0.0683) | Acc_1: (97.61%) (18866/19328)\n",
      "Epoch: 81 | Batch_idx: 160 |  Loss_1: (0.0683) | Acc_1: (97.63%) (20119/20608)\n",
      "Epoch: 81 | Batch_idx: 170 |  Loss_1: (0.0671) | Acc_1: (97.69%) (21383/21888)\n",
      "Epoch: 81 | Batch_idx: 180 |  Loss_1: (0.0677) | Acc_1: (97.66%) (22627/23168)\n",
      "Epoch: 81 | Batch_idx: 190 |  Loss_1: (0.0683) | Acc_1: (97.68%) (23880/24448)\n",
      "Epoch: 81 | Batch_idx: 200 |  Loss_1: (0.0684) | Acc_1: (97.68%) (25130/25728)\n",
      "Epoch: 81 | Batch_idx: 210 |  Loss_1: (0.0681) | Acc_1: (97.68%) (26381/27008)\n",
      "Epoch: 81 | Batch_idx: 220 |  Loss_1: (0.0678) | Acc_1: (97.67%) (27630/28288)\n",
      "Epoch: 81 | Batch_idx: 230 |  Loss_1: (0.0673) | Acc_1: (97.68%) (28883/29568)\n",
      "Epoch: 81 | Batch_idx: 240 |  Loss_1: (0.0670) | Acc_1: (97.70%) (30138/30848)\n",
      "Epoch: 81 | Batch_idx: 250 |  Loss_1: (0.0672) | Acc_1: (97.69%) (31385/32128)\n",
      "Epoch: 81 | Batch_idx: 260 |  Loss_1: (0.0677) | Acc_1: (97.67%) (32631/33408)\n",
      "Epoch: 81 | Batch_idx: 270 |  Loss_1: (0.0675) | Acc_1: (97.67%) (33880/34688)\n",
      "Epoch: 81 | Batch_idx: 280 |  Loss_1: (0.0679) | Acc_1: (97.66%) (35127/35968)\n",
      "Epoch: 81 | Batch_idx: 290 |  Loss_1: (0.0681) | Acc_1: (97.65%) (36372/37248)\n",
      "Epoch: 81 | Batch_idx: 300 |  Loss_1: (0.0677) | Acc_1: (97.65%) (37624/38528)\n",
      "Epoch: 81 | Batch_idx: 310 |  Loss_1: (0.0683) | Acc_1: (97.63%) (38863/39808)\n",
      "Epoch: 81 | Batch_idx: 320 |  Loss_1: (0.0688) | Acc_1: (97.62%) (40112/41088)\n",
      "Epoch: 81 | Batch_idx: 330 |  Loss_1: (0.0692) | Acc_1: (97.62%) (41359/42368)\n",
      "Epoch: 81 | Batch_idx: 340 |  Loss_1: (0.0688) | Acc_1: (97.63%) (42613/43648)\n",
      "Epoch: 81 | Batch_idx: 350 |  Loss_1: (0.0683) | Acc_1: (97.64%) (43868/44928)\n",
      "Epoch: 81 | Batch_idx: 360 |  Loss_1: (0.0678) | Acc_1: (97.65%) (45122/46208)\n",
      "Epoch: 81 | Batch_idx: 370 |  Loss_1: (0.0673) | Acc_1: (97.67%) (46383/47488)\n",
      "Epoch: 81 | Batch_idx: 380 |  Loss_1: (0.0672) | Acc_1: (97.68%) (47636/48768)\n",
      "Epoch: 81 | Batch_idx: 390 |  Loss_1: (0.0673) | Acc_1: (97.68%) (48839/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4429) | Acc: (90.21%) (9021/10000)\n",
      "Epoch: 82 | Batch_idx: 0 |  Loss_1: (0.0689) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 82 | Batch_idx: 10 |  Loss_1: (0.0486) | Acc_1: (98.44%) (1386/1408)\n",
      "Epoch: 82 | Batch_idx: 20 |  Loss_1: (0.0586) | Acc_1: (98.10%) (2637/2688)\n",
      "Epoch: 82 | Batch_idx: 30 |  Loss_1: (0.0592) | Acc_1: (98.14%) (3894/3968)\n",
      "Epoch: 82 | Batch_idx: 40 |  Loss_1: (0.0613) | Acc_1: (98.04%) (5145/5248)\n",
      "Epoch: 82 | Batch_idx: 50 |  Loss_1: (0.0614) | Acc_1: (98.07%) (6402/6528)\n",
      "Epoch: 82 | Batch_idx: 60 |  Loss_1: (0.0615) | Acc_1: (98.09%) (7659/7808)\n",
      "Epoch: 82 | Batch_idx: 70 |  Loss_1: (0.0620) | Acc_1: (98.00%) (8906/9088)\n",
      "Epoch: 82 | Batch_idx: 80 |  Loss_1: (0.0601) | Acc_1: (98.03%) (10164/10368)\n",
      "Epoch: 82 | Batch_idx: 90 |  Loss_1: (0.0591) | Acc_1: (98.04%) (11420/11648)\n",
      "Epoch: 82 | Batch_idx: 100 |  Loss_1: (0.0584) | Acc_1: (98.07%) (12679/12928)\n",
      "Epoch: 82 | Batch_idx: 110 |  Loss_1: (0.0589) | Acc_1: (98.06%) (13933/14208)\n",
      "Epoch: 82 | Batch_idx: 120 |  Loss_1: (0.0583) | Acc_1: (98.10%) (15194/15488)\n",
      "Epoch: 82 | Batch_idx: 130 |  Loss_1: (0.0589) | Acc_1: (98.08%) (16446/16768)\n",
      "Epoch: 82 | Batch_idx: 140 |  Loss_1: (0.0591) | Acc_1: (98.04%) (17695/18048)\n",
      "Epoch: 82 | Batch_idx: 150 |  Loss_1: (0.0592) | Acc_1: (98.02%) (18945/19328)\n",
      "Epoch: 82 | Batch_idx: 160 |  Loss_1: (0.0590) | Acc_1: (98.03%) (20201/20608)\n",
      "Epoch: 82 | Batch_idx: 170 |  Loss_1: (0.0608) | Acc_1: (97.95%) (21440/21888)\n",
      "Epoch: 82 | Batch_idx: 180 |  Loss_1: (0.0617) | Acc_1: (97.89%) (22680/23168)\n",
      "Epoch: 82 | Batch_idx: 190 |  Loss_1: (0.0620) | Acc_1: (97.88%) (23929/24448)\n",
      "Epoch: 82 | Batch_idx: 200 |  Loss_1: (0.0628) | Acc_1: (97.84%) (25173/25728)\n",
      "Epoch: 82 | Batch_idx: 210 |  Loss_1: (0.0639) | Acc_1: (97.81%) (26416/27008)\n",
      "Epoch: 82 | Batch_idx: 220 |  Loss_1: (0.0637) | Acc_1: (97.81%) (27668/28288)\n",
      "Epoch: 82 | Batch_idx: 230 |  Loss_1: (0.0633) | Acc_1: (97.81%) (28921/29568)\n",
      "Epoch: 82 | Batch_idx: 240 |  Loss_1: (0.0638) | Acc_1: (97.80%) (30169/30848)\n",
      "Epoch: 82 | Batch_idx: 250 |  Loss_1: (0.0642) | Acc_1: (97.79%) (31418/32128)\n",
      "Epoch: 82 | Batch_idx: 260 |  Loss_1: (0.0644) | Acc_1: (97.77%) (32662/33408)\n",
      "Epoch: 82 | Batch_idx: 270 |  Loss_1: (0.0652) | Acc_1: (97.73%) (33902/34688)\n",
      "Epoch: 82 | Batch_idx: 280 |  Loss_1: (0.0661) | Acc_1: (97.70%) (35140/35968)\n",
      "Epoch: 82 | Batch_idx: 290 |  Loss_1: (0.0661) | Acc_1: (97.69%) (36387/37248)\n",
      "Epoch: 82 | Batch_idx: 300 |  Loss_1: (0.0664) | Acc_1: (97.67%) (37632/38528)\n",
      "Epoch: 82 | Batch_idx: 310 |  Loss_1: (0.0664) | Acc_1: (97.68%) (38883/39808)\n",
      "Epoch: 82 | Batch_idx: 320 |  Loss_1: (0.0665) | Acc_1: (97.67%) (40132/41088)\n",
      "Epoch: 82 | Batch_idx: 330 |  Loss_1: (0.0672) | Acc_1: (97.66%) (41375/42368)\n",
      "Epoch: 82 | Batch_idx: 340 |  Loss_1: (0.0672) | Acc_1: (97.66%) (42628/43648)\n",
      "Epoch: 82 | Batch_idx: 350 |  Loss_1: (0.0673) | Acc_1: (97.66%) (43876/44928)\n",
      "Epoch: 82 | Batch_idx: 360 |  Loss_1: (0.0672) | Acc_1: (97.66%) (45128/46208)\n",
      "Epoch: 82 | Batch_idx: 370 |  Loss_1: (0.0669) | Acc_1: (97.68%) (46385/47488)\n",
      "Epoch: 82 | Batch_idx: 380 |  Loss_1: (0.0671) | Acc_1: (97.66%) (47628/48768)\n",
      "Epoch: 82 | Batch_idx: 390 |  Loss_1: (0.0673) | Acc_1: (97.63%) (48817/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4082) | Acc: (90.48%) (9048/10000)\n",
      "Epoch: 83 | Batch_idx: 0 |  Loss_1: (0.0625) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 83 | Batch_idx: 10 |  Loss_1: (0.0636) | Acc_1: (97.51%) (1373/1408)\n",
      "Epoch: 83 | Batch_idx: 20 |  Loss_1: (0.0670) | Acc_1: (97.54%) (2622/2688)\n",
      "Epoch: 83 | Batch_idx: 30 |  Loss_1: (0.0633) | Acc_1: (97.71%) (3877/3968)\n",
      "Epoch: 83 | Batch_idx: 40 |  Loss_1: (0.0673) | Acc_1: (97.64%) (5124/5248)\n",
      "Epoch: 83 | Batch_idx: 50 |  Loss_1: (0.0701) | Acc_1: (97.59%) (6371/6528)\n",
      "Epoch: 83 | Batch_idx: 60 |  Loss_1: (0.0677) | Acc_1: (97.67%) (7626/7808)\n",
      "Epoch: 83 | Batch_idx: 70 |  Loss_1: (0.0674) | Acc_1: (97.65%) (8874/9088)\n",
      "Epoch: 83 | Batch_idx: 80 |  Loss_1: (0.0670) | Acc_1: (97.64%) (10123/10368)\n",
      "Epoch: 83 | Batch_idx: 90 |  Loss_1: (0.0654) | Acc_1: (97.72%) (11382/11648)\n",
      "Epoch: 83 | Batch_idx: 100 |  Loss_1: (0.0641) | Acc_1: (97.76%) (12639/12928)\n",
      "Epoch: 83 | Batch_idx: 110 |  Loss_1: (0.0631) | Acc_1: (97.82%) (13898/14208)\n",
      "Epoch: 83 | Batch_idx: 120 |  Loss_1: (0.0624) | Acc_1: (97.85%) (15155/15488)\n",
      "Epoch: 83 | Batch_idx: 130 |  Loss_1: (0.0636) | Acc_1: (97.84%) (16406/16768)\n",
      "Epoch: 83 | Batch_idx: 140 |  Loss_1: (0.0644) | Acc_1: (97.79%) (17649/18048)\n",
      "Epoch: 83 | Batch_idx: 150 |  Loss_1: (0.0649) | Acc_1: (97.79%) (18900/19328)\n",
      "Epoch: 83 | Batch_idx: 160 |  Loss_1: (0.0654) | Acc_1: (97.75%) (20145/20608)\n",
      "Epoch: 83 | Batch_idx: 170 |  Loss_1: (0.0661) | Acc_1: (97.75%) (21395/21888)\n",
      "Epoch: 83 | Batch_idx: 180 |  Loss_1: (0.0663) | Acc_1: (97.76%) (22649/23168)\n",
      "Epoch: 83 | Batch_idx: 190 |  Loss_1: (0.0658) | Acc_1: (97.76%) (23900/24448)\n",
      "Epoch: 83 | Batch_idx: 200 |  Loss_1: (0.0667) | Acc_1: (97.71%) (25139/25728)\n",
      "Epoch: 83 | Batch_idx: 210 |  Loss_1: (0.0679) | Acc_1: (97.67%) (26378/27008)\n",
      "Epoch: 83 | Batch_idx: 220 |  Loss_1: (0.0675) | Acc_1: (97.68%) (27633/28288)\n",
      "Epoch: 83 | Batch_idx: 230 |  Loss_1: (0.0676) | Acc_1: (97.69%) (28886/29568)\n",
      "Epoch: 83 | Batch_idx: 240 |  Loss_1: (0.0671) | Acc_1: (97.70%) (30137/30848)\n",
      "Epoch: 83 | Batch_idx: 250 |  Loss_1: (0.0668) | Acc_1: (97.72%) (31394/32128)\n",
      "Epoch: 83 | Batch_idx: 260 |  Loss_1: (0.0672) | Acc_1: (97.71%) (32644/33408)\n",
      "Epoch: 83 | Batch_idx: 270 |  Loss_1: (0.0677) | Acc_1: (97.71%) (33895/34688)\n",
      "Epoch: 83 | Batch_idx: 280 |  Loss_1: (0.0681) | Acc_1: (97.71%) (35144/35968)\n",
      "Epoch: 83 | Batch_idx: 290 |  Loss_1: (0.0684) | Acc_1: (97.70%) (36390/37248)\n",
      "Epoch: 83 | Batch_idx: 300 |  Loss_1: (0.0685) | Acc_1: (97.69%) (37637/38528)\n",
      "Epoch: 83 | Batch_idx: 310 |  Loss_1: (0.0680) | Acc_1: (97.71%) (38897/39808)\n",
      "Epoch: 83 | Batch_idx: 320 |  Loss_1: (0.0683) | Acc_1: (97.68%) (40136/41088)\n",
      "Epoch: 83 | Batch_idx: 330 |  Loss_1: (0.0685) | Acc_1: (97.67%) (41382/42368)\n",
      "Epoch: 83 | Batch_idx: 340 |  Loss_1: (0.0685) | Acc_1: (97.68%) (42636/43648)\n",
      "Epoch: 83 | Batch_idx: 350 |  Loss_1: (0.0680) | Acc_1: (97.69%) (43892/44928)\n",
      "Epoch: 83 | Batch_idx: 360 |  Loss_1: (0.0682) | Acc_1: (97.70%) (45144/46208)\n",
      "Epoch: 83 | Batch_idx: 370 |  Loss_1: (0.0678) | Acc_1: (97.72%) (46405/47488)\n",
      "Epoch: 83 | Batch_idx: 380 |  Loss_1: (0.0682) | Acc_1: (97.72%) (47654/48768)\n",
      "Epoch: 83 | Batch_idx: 390 |  Loss_1: (0.0682) | Acc_1: (97.71%) (48856/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4453) | Acc: (90.15%) (9015/10000)\n",
      "Epoch: 84 | Batch_idx: 0 |  Loss_1: (0.0552) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 84 | Batch_idx: 10 |  Loss_1: (0.0724) | Acc_1: (97.44%) (1372/1408)\n",
      "Epoch: 84 | Batch_idx: 20 |  Loss_1: (0.0707) | Acc_1: (97.58%) (2623/2688)\n",
      "Epoch: 84 | Batch_idx: 30 |  Loss_1: (0.0676) | Acc_1: (97.68%) (3876/3968)\n",
      "Epoch: 84 | Batch_idx: 40 |  Loss_1: (0.0709) | Acc_1: (97.56%) (5120/5248)\n",
      "Epoch: 84 | Batch_idx: 50 |  Loss_1: (0.0672) | Acc_1: (97.73%) (6380/6528)\n",
      "Epoch: 84 | Batch_idx: 60 |  Loss_1: (0.0674) | Acc_1: (97.72%) (7630/7808)\n",
      "Epoch: 84 | Batch_idx: 70 |  Loss_1: (0.0664) | Acc_1: (97.80%) (8888/9088)\n",
      "Epoch: 84 | Batch_idx: 80 |  Loss_1: (0.0659) | Acc_1: (97.77%) (10137/10368)\n",
      "Epoch: 84 | Batch_idx: 90 |  Loss_1: (0.0654) | Acc_1: (97.81%) (11393/11648)\n",
      "Epoch: 84 | Batch_idx: 100 |  Loss_1: (0.0656) | Acc_1: (97.77%) (12640/12928)\n",
      "Epoch: 84 | Batch_idx: 110 |  Loss_1: (0.0639) | Acc_1: (97.85%) (13902/14208)\n",
      "Epoch: 84 | Batch_idx: 120 |  Loss_1: (0.0625) | Acc_1: (97.86%) (15157/15488)\n",
      "Epoch: 84 | Batch_idx: 130 |  Loss_1: (0.0637) | Acc_1: (97.85%) (16407/16768)\n",
      "Epoch: 84 | Batch_idx: 140 |  Loss_1: (0.0647) | Acc_1: (97.82%) (17654/18048)\n",
      "Epoch: 84 | Batch_idx: 150 |  Loss_1: (0.0655) | Acc_1: (97.79%) (18900/19328)\n",
      "Epoch: 84 | Batch_idx: 160 |  Loss_1: (0.0666) | Acc_1: (97.75%) (20144/20608)\n",
      "Epoch: 84 | Batch_idx: 170 |  Loss_1: (0.0661) | Acc_1: (97.75%) (21396/21888)\n",
      "Epoch: 84 | Batch_idx: 180 |  Loss_1: (0.0663) | Acc_1: (97.73%) (22643/23168)\n",
      "Epoch: 84 | Batch_idx: 190 |  Loss_1: (0.0657) | Acc_1: (97.77%) (23902/24448)\n",
      "Epoch: 84 | Batch_idx: 200 |  Loss_1: (0.0649) | Acc_1: (97.81%) (25164/25728)\n",
      "Epoch: 84 | Batch_idx: 210 |  Loss_1: (0.0640) | Acc_1: (97.82%) (26420/27008)\n",
      "Epoch: 84 | Batch_idx: 220 |  Loss_1: (0.0634) | Acc_1: (97.86%) (27682/28288)\n",
      "Epoch: 84 | Batch_idx: 230 |  Loss_1: (0.0628) | Acc_1: (97.86%) (28934/29568)\n",
      "Epoch: 84 | Batch_idx: 240 |  Loss_1: (0.0633) | Acc_1: (97.83%) (30180/30848)\n",
      "Epoch: 84 | Batch_idx: 250 |  Loss_1: (0.0634) | Acc_1: (97.83%) (31431/32128)\n",
      "Epoch: 84 | Batch_idx: 260 |  Loss_1: (0.0638) | Acc_1: (97.81%) (32676/33408)\n",
      "Epoch: 84 | Batch_idx: 270 |  Loss_1: (0.0638) | Acc_1: (97.80%) (33926/34688)\n",
      "Epoch: 84 | Batch_idx: 280 |  Loss_1: (0.0637) | Acc_1: (97.80%) (35176/35968)\n",
      "Epoch: 84 | Batch_idx: 290 |  Loss_1: (0.0640) | Acc_1: (97.79%) (36424/37248)\n",
      "Epoch: 84 | Batch_idx: 300 |  Loss_1: (0.0636) | Acc_1: (97.81%) (37684/38528)\n",
      "Epoch: 84 | Batch_idx: 310 |  Loss_1: (0.0636) | Acc_1: (97.81%) (38935/39808)\n",
      "Epoch: 84 | Batch_idx: 320 |  Loss_1: (0.0634) | Acc_1: (97.82%) (40192/41088)\n",
      "Epoch: 84 | Batch_idx: 330 |  Loss_1: (0.0631) | Acc_1: (97.82%) (41446/42368)\n",
      "Epoch: 84 | Batch_idx: 340 |  Loss_1: (0.0635) | Acc_1: (97.81%) (42691/43648)\n",
      "Epoch: 84 | Batch_idx: 350 |  Loss_1: (0.0639) | Acc_1: (97.79%) (43936/44928)\n",
      "Epoch: 84 | Batch_idx: 360 |  Loss_1: (0.0642) | Acc_1: (97.80%) (45190/46208)\n",
      "Epoch: 84 | Batch_idx: 370 |  Loss_1: (0.0644) | Acc_1: (97.80%) (46444/47488)\n",
      "Epoch: 84 | Batch_idx: 380 |  Loss_1: (0.0647) | Acc_1: (97.79%) (47690/48768)\n",
      "Epoch: 84 | Batch_idx: 390 |  Loss_1: (0.0651) | Acc_1: (97.79%) (48895/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4561) | Acc: (90.42%) (9042/10000)\n",
      "Epoch: 85 | Batch_idx: 0 |  Loss_1: (0.1122) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 85 | Batch_idx: 10 |  Loss_1: (0.0819) | Acc_1: (97.23%) (1369/1408)\n",
      "Epoch: 85 | Batch_idx: 20 |  Loss_1: (0.0851) | Acc_1: (97.14%) (2611/2688)\n",
      "Epoch: 85 | Batch_idx: 30 |  Loss_1: (0.0734) | Acc_1: (97.56%) (3871/3968)\n",
      "Epoch: 85 | Batch_idx: 40 |  Loss_1: (0.0693) | Acc_1: (97.64%) (5124/5248)\n",
      "Epoch: 85 | Batch_idx: 50 |  Loss_1: (0.0679) | Acc_1: (97.64%) (6374/6528)\n",
      "Epoch: 85 | Batch_idx: 60 |  Loss_1: (0.0702) | Acc_1: (97.58%) (7619/7808)\n",
      "Epoch: 85 | Batch_idx: 70 |  Loss_1: (0.0679) | Acc_1: (97.67%) (8876/9088)\n",
      "Epoch: 85 | Batch_idx: 80 |  Loss_1: (0.0643) | Acc_1: (97.80%) (10140/10368)\n",
      "Epoch: 85 | Batch_idx: 90 |  Loss_1: (0.0631) | Acc_1: (97.85%) (11398/11648)\n",
      "Epoch: 85 | Batch_idx: 100 |  Loss_1: (0.0631) | Acc_1: (97.84%) (12649/12928)\n",
      "Epoch: 85 | Batch_idx: 110 |  Loss_1: (0.0633) | Acc_1: (97.80%) (13895/14208)\n",
      "Epoch: 85 | Batch_idx: 120 |  Loss_1: (0.0620) | Acc_1: (97.86%) (15157/15488)\n",
      "Epoch: 85 | Batch_idx: 130 |  Loss_1: (0.0617) | Acc_1: (97.85%) (16408/16768)\n",
      "Epoch: 85 | Batch_idx: 140 |  Loss_1: (0.0615) | Acc_1: (97.86%) (17662/18048)\n",
      "Epoch: 85 | Batch_idx: 150 |  Loss_1: (0.0613) | Acc_1: (97.86%) (18915/19328)\n",
      "Epoch: 85 | Batch_idx: 160 |  Loss_1: (0.0615) | Acc_1: (97.85%) (20164/20608)\n",
      "Epoch: 85 | Batch_idx: 170 |  Loss_1: (0.0615) | Acc_1: (97.85%) (21418/21888)\n",
      "Epoch: 85 | Batch_idx: 180 |  Loss_1: (0.0616) | Acc_1: (97.89%) (22679/23168)\n",
      "Epoch: 85 | Batch_idx: 190 |  Loss_1: (0.0635) | Acc_1: (97.80%) (23911/24448)\n",
      "Epoch: 85 | Batch_idx: 200 |  Loss_1: (0.0641) | Acc_1: (97.78%) (25158/25728)\n",
      "Epoch: 85 | Batch_idx: 210 |  Loss_1: (0.0647) | Acc_1: (97.78%) (26409/27008)\n",
      "Epoch: 85 | Batch_idx: 220 |  Loss_1: (0.0643) | Acc_1: (97.79%) (27662/28288)\n",
      "Epoch: 85 | Batch_idx: 230 |  Loss_1: (0.0640) | Acc_1: (97.79%) (28916/29568)\n",
      "Epoch: 85 | Batch_idx: 240 |  Loss_1: (0.0655) | Acc_1: (97.73%) (30147/30848)\n",
      "Epoch: 85 | Batch_idx: 250 |  Loss_1: (0.0661) | Acc_1: (97.70%) (31389/32128)\n",
      "Epoch: 85 | Batch_idx: 260 |  Loss_1: (0.0663) | Acc_1: (97.71%) (32642/33408)\n",
      "Epoch: 85 | Batch_idx: 270 |  Loss_1: (0.0663) | Acc_1: (97.72%) (33896/34688)\n",
      "Epoch: 85 | Batch_idx: 280 |  Loss_1: (0.0664) | Acc_1: (97.71%) (35145/35968)\n",
      "Epoch: 85 | Batch_idx: 290 |  Loss_1: (0.0662) | Acc_1: (97.72%) (36398/37248)\n",
      "Epoch: 85 | Batch_idx: 300 |  Loss_1: (0.0666) | Acc_1: (97.70%) (37642/38528)\n",
      "Epoch: 85 | Batch_idx: 310 |  Loss_1: (0.0668) | Acc_1: (97.70%) (38892/39808)\n",
      "Epoch: 85 | Batch_idx: 320 |  Loss_1: (0.0670) | Acc_1: (97.68%) (40134/41088)\n",
      "Epoch: 85 | Batch_idx: 330 |  Loss_1: (0.0667) | Acc_1: (97.70%) (41392/42368)\n",
      "Epoch: 85 | Batch_idx: 340 |  Loss_1: (0.0669) | Acc_1: (97.69%) (42640/43648)\n",
      "Epoch: 85 | Batch_idx: 350 |  Loss_1: (0.0670) | Acc_1: (97.69%) (43892/44928)\n",
      "Epoch: 85 | Batch_idx: 360 |  Loss_1: (0.0668) | Acc_1: (97.70%) (45144/46208)\n",
      "Epoch: 85 | Batch_idx: 370 |  Loss_1: (0.0672) | Acc_1: (97.69%) (46392/47488)\n",
      "Epoch: 85 | Batch_idx: 380 |  Loss_1: (0.0671) | Acc_1: (97.69%) (47642/48768)\n",
      "Epoch: 85 | Batch_idx: 390 |  Loss_1: (0.0671) | Acc_1: (97.69%) (48843/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4592) | Acc: (89.93%) (8993/10000)\n",
      "Epoch: 86 | Batch_idx: 0 |  Loss_1: (0.0714) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 86 | Batch_idx: 10 |  Loss_1: (0.0619) | Acc_1: (97.66%) (1375/1408)\n",
      "Epoch: 86 | Batch_idx: 20 |  Loss_1: (0.0588) | Acc_1: (97.92%) (2632/2688)\n",
      "Epoch: 86 | Batch_idx: 30 |  Loss_1: (0.0636) | Acc_1: (97.76%) (3879/3968)\n",
      "Epoch: 86 | Batch_idx: 40 |  Loss_1: (0.0597) | Acc_1: (97.94%) (5140/5248)\n",
      "Epoch: 86 | Batch_idx: 50 |  Loss_1: (0.0600) | Acc_1: (97.99%) (6397/6528)\n",
      "Epoch: 86 | Batch_idx: 60 |  Loss_1: (0.0591) | Acc_1: (98.01%) (7653/7808)\n",
      "Epoch: 86 | Batch_idx: 70 |  Loss_1: (0.0599) | Acc_1: (97.99%) (8905/9088)\n",
      "Epoch: 86 | Batch_idx: 80 |  Loss_1: (0.0585) | Acc_1: (98.00%) (10161/10368)\n",
      "Epoch: 86 | Batch_idx: 90 |  Loss_1: (0.0594) | Acc_1: (97.98%) (11413/11648)\n",
      "Epoch: 86 | Batch_idx: 100 |  Loss_1: (0.0586) | Acc_1: (98.01%) (12671/12928)\n",
      "Epoch: 86 | Batch_idx: 110 |  Loss_1: (0.0582) | Acc_1: (98.03%) (13928/14208)\n",
      "Epoch: 86 | Batch_idx: 120 |  Loss_1: (0.0580) | Acc_1: (98.00%) (15179/15488)\n",
      "Epoch: 86 | Batch_idx: 130 |  Loss_1: (0.0589) | Acc_1: (97.94%) (16422/16768)\n",
      "Epoch: 86 | Batch_idx: 140 |  Loss_1: (0.0587) | Acc_1: (97.95%) (17678/18048)\n",
      "Epoch: 86 | Batch_idx: 150 |  Loss_1: (0.0586) | Acc_1: (97.98%) (18938/19328)\n",
      "Epoch: 86 | Batch_idx: 160 |  Loss_1: (0.0597) | Acc_1: (97.95%) (20186/20608)\n",
      "Epoch: 86 | Batch_idx: 170 |  Loss_1: (0.0592) | Acc_1: (97.97%) (21443/21888)\n",
      "Epoch: 86 | Batch_idx: 180 |  Loss_1: (0.0585) | Acc_1: (97.99%) (22702/23168)\n",
      "Epoch: 86 | Batch_idx: 190 |  Loss_1: (0.0594) | Acc_1: (97.97%) (23951/24448)\n",
      "Epoch: 86 | Batch_idx: 200 |  Loss_1: (0.0599) | Acc_1: (97.95%) (25201/25728)\n",
      "Epoch: 86 | Batch_idx: 210 |  Loss_1: (0.0607) | Acc_1: (97.91%) (26444/27008)\n",
      "Epoch: 86 | Batch_idx: 220 |  Loss_1: (0.0604) | Acc_1: (97.92%) (27699/28288)\n",
      "Epoch: 86 | Batch_idx: 230 |  Loss_1: (0.0605) | Acc_1: (97.92%) (28953/29568)\n",
      "Epoch: 86 | Batch_idx: 240 |  Loss_1: (0.0605) | Acc_1: (97.92%) (30205/30848)\n",
      "Epoch: 86 | Batch_idx: 250 |  Loss_1: (0.0607) | Acc_1: (97.90%) (31453/32128)\n",
      "Epoch: 86 | Batch_idx: 260 |  Loss_1: (0.0618) | Acc_1: (97.87%) (32695/33408)\n",
      "Epoch: 86 | Batch_idx: 270 |  Loss_1: (0.0620) | Acc_1: (97.87%) (33948/34688)\n",
      "Epoch: 86 | Batch_idx: 280 |  Loss_1: (0.0618) | Acc_1: (97.86%) (35200/35968)\n",
      "Epoch: 86 | Batch_idx: 290 |  Loss_1: (0.0629) | Acc_1: (97.83%) (36440/37248)\n",
      "Epoch: 86 | Batch_idx: 300 |  Loss_1: (0.0629) | Acc_1: (97.84%) (37696/38528)\n",
      "Epoch: 86 | Batch_idx: 310 |  Loss_1: (0.0633) | Acc_1: (97.83%) (38946/39808)\n",
      "Epoch: 86 | Batch_idx: 320 |  Loss_1: (0.0636) | Acc_1: (97.81%) (40188/41088)\n",
      "Epoch: 86 | Batch_idx: 330 |  Loss_1: (0.0641) | Acc_1: (97.80%) (41438/42368)\n",
      "Epoch: 86 | Batch_idx: 340 |  Loss_1: (0.0644) | Acc_1: (97.78%) (42680/43648)\n",
      "Epoch: 86 | Batch_idx: 350 |  Loss_1: (0.0643) | Acc_1: (97.77%) (43928/44928)\n",
      "Epoch: 86 | Batch_idx: 360 |  Loss_1: (0.0638) | Acc_1: (97.80%) (45190/46208)\n",
      "Epoch: 86 | Batch_idx: 370 |  Loss_1: (0.0638) | Acc_1: (97.77%) (46430/47488)\n",
      "Epoch: 86 | Batch_idx: 380 |  Loss_1: (0.0640) | Acc_1: (97.77%) (47681/48768)\n",
      "Epoch: 86 | Batch_idx: 390 |  Loss_1: (0.0643) | Acc_1: (97.76%) (48882/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4280) | Acc: (90.99%) (9099/10000)\n",
      "Epoch: 87 | Batch_idx: 0 |  Loss_1: (0.0486) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 87 | Batch_idx: 10 |  Loss_1: (0.0588) | Acc_1: (97.87%) (1378/1408)\n",
      "Epoch: 87 | Batch_idx: 20 |  Loss_1: (0.0615) | Acc_1: (97.62%) (2624/2688)\n",
      "Epoch: 87 | Batch_idx: 30 |  Loss_1: (0.0623) | Acc_1: (97.76%) (3879/3968)\n",
      "Epoch: 87 | Batch_idx: 40 |  Loss_1: (0.0666) | Acc_1: (97.58%) (5121/5248)\n",
      "Epoch: 87 | Batch_idx: 50 |  Loss_1: (0.0652) | Acc_1: (97.72%) (6379/6528)\n",
      "Epoch: 87 | Batch_idx: 60 |  Loss_1: (0.0640) | Acc_1: (97.76%) (7633/7808)\n",
      "Epoch: 87 | Batch_idx: 70 |  Loss_1: (0.0640) | Acc_1: (97.74%) (8883/9088)\n",
      "Epoch: 87 | Batch_idx: 80 |  Loss_1: (0.0663) | Acc_1: (97.65%) (10124/10368)\n",
      "Epoch: 87 | Batch_idx: 90 |  Loss_1: (0.0673) | Acc_1: (97.59%) (11367/11648)\n",
      "Epoch: 87 | Batch_idx: 100 |  Loss_1: (0.0675) | Acc_1: (97.60%) (12618/12928)\n",
      "Epoch: 87 | Batch_idx: 110 |  Loss_1: (0.0674) | Acc_1: (97.59%) (13866/14208)\n",
      "Epoch: 87 | Batch_idx: 120 |  Loss_1: (0.0674) | Acc_1: (97.64%) (15123/15488)\n",
      "Epoch: 87 | Batch_idx: 130 |  Loss_1: (0.0666) | Acc_1: (97.65%) (16374/16768)\n",
      "Epoch: 87 | Batch_idx: 140 |  Loss_1: (0.0656) | Acc_1: (97.68%) (17629/18048)\n",
      "Epoch: 87 | Batch_idx: 150 |  Loss_1: (0.0658) | Acc_1: (97.67%) (18878/19328)\n",
      "Epoch: 87 | Batch_idx: 160 |  Loss_1: (0.0652) | Acc_1: (97.67%) (20128/20608)\n",
      "Epoch: 87 | Batch_idx: 170 |  Loss_1: (0.0644) | Acc_1: (97.71%) (21387/21888)\n",
      "Epoch: 87 | Batch_idx: 180 |  Loss_1: (0.0640) | Acc_1: (97.73%) (22641/23168)\n",
      "Epoch: 87 | Batch_idx: 190 |  Loss_1: (0.0640) | Acc_1: (97.75%) (23897/24448)\n",
      "Epoch: 87 | Batch_idx: 200 |  Loss_1: (0.0638) | Acc_1: (97.77%) (25153/25728)\n",
      "Epoch: 87 | Batch_idx: 210 |  Loss_1: (0.0634) | Acc_1: (97.79%) (26410/27008)\n",
      "Epoch: 87 | Batch_idx: 220 |  Loss_1: (0.0634) | Acc_1: (97.78%) (27660/28288)\n",
      "Epoch: 87 | Batch_idx: 230 |  Loss_1: (0.0643) | Acc_1: (97.75%) (28902/29568)\n",
      "Epoch: 87 | Batch_idx: 240 |  Loss_1: (0.0645) | Acc_1: (97.74%) (30152/30848)\n",
      "Epoch: 87 | Batch_idx: 250 |  Loss_1: (0.0651) | Acc_1: (97.73%) (31400/32128)\n",
      "Epoch: 87 | Batch_idx: 260 |  Loss_1: (0.0651) | Acc_1: (97.72%) (32647/33408)\n",
      "Epoch: 87 | Batch_idx: 270 |  Loss_1: (0.0648) | Acc_1: (97.73%) (33901/34688)\n",
      "Epoch: 87 | Batch_idx: 280 |  Loss_1: (0.0639) | Acc_1: (97.77%) (35166/35968)\n",
      "Epoch: 87 | Batch_idx: 290 |  Loss_1: (0.0635) | Acc_1: (97.78%) (36420/37248)\n",
      "Epoch: 87 | Batch_idx: 300 |  Loss_1: (0.0635) | Acc_1: (97.76%) (37664/38528)\n",
      "Epoch: 87 | Batch_idx: 310 |  Loss_1: (0.0632) | Acc_1: (97.76%) (38916/39808)\n",
      "Epoch: 87 | Batch_idx: 320 |  Loss_1: (0.0634) | Acc_1: (97.76%) (40167/41088)\n",
      "Epoch: 87 | Batch_idx: 330 |  Loss_1: (0.0632) | Acc_1: (97.76%) (41421/42368)\n",
      "Epoch: 87 | Batch_idx: 340 |  Loss_1: (0.0638) | Acc_1: (97.76%) (42671/43648)\n",
      "Epoch: 87 | Batch_idx: 350 |  Loss_1: (0.0629) | Acc_1: (97.79%) (43936/44928)\n",
      "Epoch: 87 | Batch_idx: 360 |  Loss_1: (0.0634) | Acc_1: (97.79%) (45185/46208)\n",
      "Epoch: 87 | Batch_idx: 370 |  Loss_1: (0.0635) | Acc_1: (97.79%) (46437/47488)\n",
      "Epoch: 87 | Batch_idx: 380 |  Loss_1: (0.0638) | Acc_1: (97.78%) (47685/48768)\n",
      "Epoch: 87 | Batch_idx: 390 |  Loss_1: (0.0639) | Acc_1: (97.77%) (48886/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4676) | Acc: (89.93%) (8993/10000)\n",
      "Epoch: 88 | Batch_idx: 0 |  Loss_1: (0.0655) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 88 | Batch_idx: 10 |  Loss_1: (0.0723) | Acc_1: (97.23%) (1369/1408)\n",
      "Epoch: 88 | Batch_idx: 20 |  Loss_1: (0.0619) | Acc_1: (97.81%) (2629/2688)\n",
      "Epoch: 88 | Batch_idx: 30 |  Loss_1: (0.0600) | Acc_1: (97.83%) (3882/3968)\n",
      "Epoch: 88 | Batch_idx: 40 |  Loss_1: (0.0566) | Acc_1: (97.94%) (5140/5248)\n",
      "Epoch: 88 | Batch_idx: 50 |  Loss_1: (0.0576) | Acc_1: (97.96%) (6395/6528)\n",
      "Epoch: 88 | Batch_idx: 60 |  Loss_1: (0.0569) | Acc_1: (98.00%) (7652/7808)\n",
      "Epoch: 88 | Batch_idx: 70 |  Loss_1: (0.0566) | Acc_1: (98.01%) (8907/9088)\n",
      "Epoch: 88 | Batch_idx: 80 |  Loss_1: (0.0579) | Acc_1: (98.00%) (10161/10368)\n",
      "Epoch: 88 | Batch_idx: 90 |  Loss_1: (0.0563) | Acc_1: (98.09%) (11425/11648)\n",
      "Epoch: 88 | Batch_idx: 100 |  Loss_1: (0.0561) | Acc_1: (98.07%) (12678/12928)\n",
      "Epoch: 88 | Batch_idx: 110 |  Loss_1: (0.0570) | Acc_1: (98.03%) (13928/14208)\n",
      "Epoch: 88 | Batch_idx: 120 |  Loss_1: (0.0574) | Acc_1: (97.99%) (15176/15488)\n",
      "Epoch: 88 | Batch_idx: 130 |  Loss_1: (0.0568) | Acc_1: (98.00%) (16433/16768)\n",
      "Epoch: 88 | Batch_idx: 140 |  Loss_1: (0.0564) | Acc_1: (98.02%) (17691/18048)\n",
      "Epoch: 88 | Batch_idx: 150 |  Loss_1: (0.0572) | Acc_1: (98.02%) (18945/19328)\n",
      "Epoch: 88 | Batch_idx: 160 |  Loss_1: (0.0577) | Acc_1: (98.03%) (20203/20608)\n",
      "Epoch: 88 | Batch_idx: 170 |  Loss_1: (0.0578) | Acc_1: (98.03%) (21456/21888)\n",
      "Epoch: 88 | Batch_idx: 180 |  Loss_1: (0.0584) | Acc_1: (97.99%) (22703/23168)\n",
      "Epoch: 88 | Batch_idx: 190 |  Loss_1: (0.0585) | Acc_1: (97.97%) (23951/24448)\n",
      "Epoch: 88 | Batch_idx: 200 |  Loss_1: (0.0589) | Acc_1: (97.96%) (25202/25728)\n",
      "Epoch: 88 | Batch_idx: 210 |  Loss_1: (0.0586) | Acc_1: (97.95%) (26454/27008)\n",
      "Epoch: 88 | Batch_idx: 220 |  Loss_1: (0.0586) | Acc_1: (97.94%) (27704/28288)\n",
      "Epoch: 88 | Batch_idx: 230 |  Loss_1: (0.0583) | Acc_1: (97.95%) (28962/29568)\n",
      "Epoch: 88 | Batch_idx: 240 |  Loss_1: (0.0589) | Acc_1: (97.94%) (30211/30848)\n",
      "Epoch: 88 | Batch_idx: 250 |  Loss_1: (0.0600) | Acc_1: (97.91%) (31458/32128)\n",
      "Epoch: 88 | Batch_idx: 260 |  Loss_1: (0.0601) | Acc_1: (97.91%) (32709/33408)\n",
      "Epoch: 88 | Batch_idx: 270 |  Loss_1: (0.0606) | Acc_1: (97.88%) (33954/34688)\n",
      "Epoch: 88 | Batch_idx: 280 |  Loss_1: (0.0603) | Acc_1: (97.90%) (35211/35968)\n",
      "Epoch: 88 | Batch_idx: 290 |  Loss_1: (0.0598) | Acc_1: (97.92%) (36472/37248)\n",
      "Epoch: 88 | Batch_idx: 300 |  Loss_1: (0.0595) | Acc_1: (97.93%) (37729/38528)\n",
      "Epoch: 88 | Batch_idx: 310 |  Loss_1: (0.0594) | Acc_1: (97.93%) (38982/39808)\n",
      "Epoch: 88 | Batch_idx: 320 |  Loss_1: (0.0598) | Acc_1: (97.91%) (40228/41088)\n",
      "Epoch: 88 | Batch_idx: 330 |  Loss_1: (0.0604) | Acc_1: (97.90%) (41477/42368)\n",
      "Epoch: 88 | Batch_idx: 340 |  Loss_1: (0.0602) | Acc_1: (97.90%) (42733/43648)\n",
      "Epoch: 88 | Batch_idx: 350 |  Loss_1: (0.0607) | Acc_1: (97.89%) (43982/44928)\n",
      "Epoch: 88 | Batch_idx: 360 |  Loss_1: (0.0616) | Acc_1: (97.86%) (45219/46208)\n",
      "Epoch: 88 | Batch_idx: 370 |  Loss_1: (0.0621) | Acc_1: (97.84%) (46464/47488)\n",
      "Epoch: 88 | Batch_idx: 380 |  Loss_1: (0.0623) | Acc_1: (97.83%) (47711/48768)\n",
      "Epoch: 88 | Batch_idx: 390 |  Loss_1: (0.0622) | Acc_1: (97.84%) (48921/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4269) | Acc: (90.67%) (9067/10000)\n",
      "Epoch: 89 | Batch_idx: 0 |  Loss_1: (0.0353) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 89 | Batch_idx: 10 |  Loss_1: (0.0436) | Acc_1: (98.44%) (1386/1408)\n",
      "Epoch: 89 | Batch_idx: 20 |  Loss_1: (0.0585) | Acc_1: (97.99%) (2634/2688)\n",
      "Epoch: 89 | Batch_idx: 30 |  Loss_1: (0.0559) | Acc_1: (98.03%) (3890/3968)\n",
      "Epoch: 89 | Batch_idx: 40 |  Loss_1: (0.0579) | Acc_1: (97.96%) (5141/5248)\n",
      "Epoch: 89 | Batch_idx: 50 |  Loss_1: (0.0592) | Acc_1: (97.98%) (6396/6528)\n",
      "Epoch: 89 | Batch_idx: 60 |  Loss_1: (0.0583) | Acc_1: (98.00%) (7652/7808)\n",
      "Epoch: 89 | Batch_idx: 70 |  Loss_1: (0.0594) | Acc_1: (97.96%) (8903/9088)\n",
      "Epoch: 89 | Batch_idx: 80 |  Loss_1: (0.0615) | Acc_1: (97.86%) (10146/10368)\n",
      "Epoch: 89 | Batch_idx: 90 |  Loss_1: (0.0608) | Acc_1: (97.95%) (11409/11648)\n",
      "Epoch: 89 | Batch_idx: 100 |  Loss_1: (0.0610) | Acc_1: (97.95%) (12663/12928)\n",
      "Epoch: 89 | Batch_idx: 110 |  Loss_1: (0.0608) | Acc_1: (97.97%) (13920/14208)\n",
      "Epoch: 89 | Batch_idx: 120 |  Loss_1: (0.0603) | Acc_1: (97.97%) (15174/15488)\n",
      "Epoch: 89 | Batch_idx: 130 |  Loss_1: (0.0613) | Acc_1: (97.89%) (16414/16768)\n",
      "Epoch: 89 | Batch_idx: 140 |  Loss_1: (0.0619) | Acc_1: (97.89%) (17668/18048)\n",
      "Epoch: 89 | Batch_idx: 150 |  Loss_1: (0.0629) | Acc_1: (97.90%) (18922/19328)\n",
      "Epoch: 89 | Batch_idx: 160 |  Loss_1: (0.0624) | Acc_1: (97.88%) (20172/20608)\n",
      "Epoch: 89 | Batch_idx: 170 |  Loss_1: (0.0626) | Acc_1: (97.90%) (21429/21888)\n",
      "Epoch: 89 | Batch_idx: 180 |  Loss_1: (0.0623) | Acc_1: (97.91%) (22683/23168)\n",
      "Epoch: 89 | Batch_idx: 190 |  Loss_1: (0.0627) | Acc_1: (97.88%) (23930/24448)\n",
      "Epoch: 89 | Batch_idx: 200 |  Loss_1: (0.0639) | Acc_1: (97.86%) (25177/25728)\n",
      "Epoch: 89 | Batch_idx: 210 |  Loss_1: (0.0629) | Acc_1: (97.88%) (26435/27008)\n",
      "Epoch: 89 | Batch_idx: 220 |  Loss_1: (0.0622) | Acc_1: (97.92%) (27699/28288)\n",
      "Epoch: 89 | Batch_idx: 230 |  Loss_1: (0.0623) | Acc_1: (97.92%) (28954/29568)\n",
      "Epoch: 89 | Batch_idx: 240 |  Loss_1: (0.0616) | Acc_1: (97.94%) (30212/30848)\n",
      "Epoch: 89 | Batch_idx: 250 |  Loss_1: (0.0616) | Acc_1: (97.94%) (31466/32128)\n",
      "Epoch: 89 | Batch_idx: 260 |  Loss_1: (0.0613) | Acc_1: (97.94%) (32720/33408)\n",
      "Epoch: 89 | Batch_idx: 270 |  Loss_1: (0.0611) | Acc_1: (97.95%) (33977/34688)\n",
      "Epoch: 89 | Batch_idx: 280 |  Loss_1: (0.0614) | Acc_1: (97.95%) (35230/35968)\n",
      "Epoch: 89 | Batch_idx: 290 |  Loss_1: (0.0613) | Acc_1: (97.95%) (36486/37248)\n",
      "Epoch: 89 | Batch_idx: 300 |  Loss_1: (0.0612) | Acc_1: (97.95%) (37737/38528)\n",
      "Epoch: 89 | Batch_idx: 310 |  Loss_1: (0.0614) | Acc_1: (97.92%) (38980/39808)\n",
      "Epoch: 89 | Batch_idx: 320 |  Loss_1: (0.0614) | Acc_1: (97.91%) (40231/41088)\n",
      "Epoch: 89 | Batch_idx: 330 |  Loss_1: (0.0610) | Acc_1: (97.93%) (41490/42368)\n",
      "Epoch: 89 | Batch_idx: 340 |  Loss_1: (0.0610) | Acc_1: (97.93%) (42743/43648)\n",
      "Epoch: 89 | Batch_idx: 350 |  Loss_1: (0.0608) | Acc_1: (97.94%) (44001/44928)\n",
      "Epoch: 89 | Batch_idx: 360 |  Loss_1: (0.0613) | Acc_1: (97.90%) (45238/46208)\n",
      "Epoch: 89 | Batch_idx: 370 |  Loss_1: (0.0612) | Acc_1: (97.89%) (46486/47488)\n",
      "Epoch: 89 | Batch_idx: 380 |  Loss_1: (0.0611) | Acc_1: (97.90%) (47744/48768)\n",
      "Epoch: 89 | Batch_idx: 390 |  Loss_1: (0.0616) | Acc_1: (97.88%) (48941/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4132) | Acc: (90.83%) (9083/10000)\n",
      "Epoch: 90 | Batch_idx: 0 |  Loss_1: (0.1020) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 90 | Batch_idx: 10 |  Loss_1: (0.0645) | Acc_1: (97.51%) (1373/1408)\n",
      "Epoch: 90 | Batch_idx: 20 |  Loss_1: (0.0673) | Acc_1: (97.73%) (2627/2688)\n",
      "Epoch: 90 | Batch_idx: 30 |  Loss_1: (0.0704) | Acc_1: (97.56%) (3871/3968)\n",
      "Epoch: 90 | Batch_idx: 40 |  Loss_1: (0.0662) | Acc_1: (97.69%) (5127/5248)\n",
      "Epoch: 90 | Batch_idx: 50 |  Loss_1: (0.0655) | Acc_1: (97.75%) (6381/6528)\n",
      "Epoch: 90 | Batch_idx: 60 |  Loss_1: (0.0638) | Acc_1: (97.81%) (7637/7808)\n",
      "Epoch: 90 | Batch_idx: 70 |  Loss_1: (0.0638) | Acc_1: (97.82%) (8890/9088)\n",
      "Epoch: 90 | Batch_idx: 80 |  Loss_1: (0.0620) | Acc_1: (97.86%) (10146/10368)\n",
      "Epoch: 90 | Batch_idx: 90 |  Loss_1: (0.0612) | Acc_1: (97.90%) (11403/11648)\n",
      "Epoch: 90 | Batch_idx: 100 |  Loss_1: (0.0618) | Acc_1: (97.90%) (12656/12928)\n",
      "Epoch: 90 | Batch_idx: 110 |  Loss_1: (0.0619) | Acc_1: (97.92%) (13913/14208)\n",
      "Epoch: 90 | Batch_idx: 120 |  Loss_1: (0.0621) | Acc_1: (97.92%) (15166/15488)\n",
      "Epoch: 90 | Batch_idx: 130 |  Loss_1: (0.0612) | Acc_1: (97.93%) (16421/16768)\n",
      "Epoch: 90 | Batch_idx: 140 |  Loss_1: (0.0602) | Acc_1: (97.97%) (17681/18048)\n",
      "Epoch: 90 | Batch_idx: 150 |  Loss_1: (0.0589) | Acc_1: (98.03%) (18947/19328)\n",
      "Epoch: 90 | Batch_idx: 160 |  Loss_1: (0.0588) | Acc_1: (98.04%) (20204/20608)\n",
      "Epoch: 90 | Batch_idx: 170 |  Loss_1: (0.0588) | Acc_1: (98.01%) (21453/21888)\n",
      "Epoch: 90 | Batch_idx: 180 |  Loss_1: (0.0580) | Acc_1: (98.04%) (22714/23168)\n",
      "Epoch: 90 | Batch_idx: 190 |  Loss_1: (0.0581) | Acc_1: (98.03%) (23967/24448)\n",
      "Epoch: 90 | Batch_idx: 200 |  Loss_1: (0.0585) | Acc_1: (98.01%) (25216/25728)\n",
      "Epoch: 90 | Batch_idx: 210 |  Loss_1: (0.0581) | Acc_1: (98.02%) (26474/27008)\n",
      "Epoch: 90 | Batch_idx: 220 |  Loss_1: (0.0589) | Acc_1: (97.99%) (27720/28288)\n",
      "Epoch: 90 | Batch_idx: 230 |  Loss_1: (0.0594) | Acc_1: (97.99%) (28973/29568)\n",
      "Epoch: 90 | Batch_idx: 240 |  Loss_1: (0.0585) | Acc_1: (98.02%) (30236/30848)\n",
      "Epoch: 90 | Batch_idx: 250 |  Loss_1: (0.0585) | Acc_1: (98.02%) (31491/32128)\n",
      "Epoch: 90 | Batch_idx: 260 |  Loss_1: (0.0584) | Acc_1: (98.02%) (32747/33408)\n",
      "Epoch: 90 | Batch_idx: 270 |  Loss_1: (0.0588) | Acc_1: (98.02%) (34000/34688)\n",
      "Epoch: 90 | Batch_idx: 280 |  Loss_1: (0.0597) | Acc_1: (98.01%) (35251/35968)\n",
      "Epoch: 90 | Batch_idx: 290 |  Loss_1: (0.0595) | Acc_1: (98.01%) (36506/37248)\n",
      "Epoch: 90 | Batch_idx: 300 |  Loss_1: (0.0591) | Acc_1: (98.02%) (37764/38528)\n",
      "Epoch: 90 | Batch_idx: 310 |  Loss_1: (0.0589) | Acc_1: (98.03%) (39022/39808)\n",
      "Epoch: 90 | Batch_idx: 320 |  Loss_1: (0.0586) | Acc_1: (98.04%) (40283/41088)\n",
      "Epoch: 90 | Batch_idx: 330 |  Loss_1: (0.0581) | Acc_1: (98.06%) (41548/42368)\n",
      "Epoch: 90 | Batch_idx: 340 |  Loss_1: (0.0581) | Acc_1: (98.05%) (42798/43648)\n",
      "Epoch: 90 | Batch_idx: 350 |  Loss_1: (0.0578) | Acc_1: (98.06%) (44055/44928)\n",
      "Epoch: 90 | Batch_idx: 360 |  Loss_1: (0.0578) | Acc_1: (98.06%) (45313/46208)\n",
      "Epoch: 90 | Batch_idx: 370 |  Loss_1: (0.0579) | Acc_1: (98.06%) (46565/47488)\n",
      "Epoch: 90 | Batch_idx: 380 |  Loss_1: (0.0580) | Acc_1: (98.05%) (47816/48768)\n",
      "Epoch: 90 | Batch_idx: 390 |  Loss_1: (0.0579) | Acc_1: (98.05%) (49026/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4934) | Acc: (90.18%) (9018/10000)\n",
      "Epoch: 91 | Batch_idx: 0 |  Loss_1: (0.0460) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 91 | Batch_idx: 10 |  Loss_1: (0.0540) | Acc_1: (98.37%) (1385/1408)\n",
      "Epoch: 91 | Batch_idx: 20 |  Loss_1: (0.0594) | Acc_1: (97.99%) (2634/2688)\n",
      "Epoch: 91 | Batch_idx: 30 |  Loss_1: (0.0598) | Acc_1: (97.88%) (3884/3968)\n",
      "Epoch: 91 | Batch_idx: 40 |  Loss_1: (0.0611) | Acc_1: (97.71%) (5128/5248)\n",
      "Epoch: 91 | Batch_idx: 50 |  Loss_1: (0.0645) | Acc_1: (97.63%) (6373/6528)\n",
      "Epoch: 91 | Batch_idx: 60 |  Loss_1: (0.0632) | Acc_1: (97.72%) (7630/7808)\n",
      "Epoch: 91 | Batch_idx: 70 |  Loss_1: (0.0645) | Acc_1: (97.68%) (8877/9088)\n",
      "Epoch: 91 | Batch_idx: 80 |  Loss_1: (0.0629) | Acc_1: (97.71%) (10131/10368)\n",
      "Epoch: 91 | Batch_idx: 90 |  Loss_1: (0.0631) | Acc_1: (97.73%) (11384/11648)\n",
      "Epoch: 91 | Batch_idx: 100 |  Loss_1: (0.0615) | Acc_1: (97.80%) (12643/12928)\n",
      "Epoch: 91 | Batch_idx: 110 |  Loss_1: (0.0627) | Acc_1: (97.78%) (13893/14208)\n",
      "Epoch: 91 | Batch_idx: 120 |  Loss_1: (0.0628) | Acc_1: (97.77%) (15142/15488)\n",
      "Epoch: 91 | Batch_idx: 130 |  Loss_1: (0.0618) | Acc_1: (97.81%) (16400/16768)\n",
      "Epoch: 91 | Batch_idx: 140 |  Loss_1: (0.0613) | Acc_1: (97.81%) (17652/18048)\n",
      "Epoch: 91 | Batch_idx: 150 |  Loss_1: (0.0612) | Acc_1: (97.81%) (18905/19328)\n",
      "Epoch: 91 | Batch_idx: 160 |  Loss_1: (0.0623) | Acc_1: (97.79%) (20152/20608)\n",
      "Epoch: 91 | Batch_idx: 170 |  Loss_1: (0.0622) | Acc_1: (97.79%) (21404/21888)\n",
      "Epoch: 91 | Batch_idx: 180 |  Loss_1: (0.0633) | Acc_1: (97.77%) (22652/23168)\n",
      "Epoch: 91 | Batch_idx: 190 |  Loss_1: (0.0627) | Acc_1: (97.80%) (23911/24448)\n",
      "Epoch: 91 | Batch_idx: 200 |  Loss_1: (0.0628) | Acc_1: (97.79%) (25159/25728)\n",
      "Epoch: 91 | Batch_idx: 210 |  Loss_1: (0.0626) | Acc_1: (97.79%) (26411/27008)\n",
      "Epoch: 91 | Batch_idx: 220 |  Loss_1: (0.0626) | Acc_1: (97.80%) (27666/28288)\n",
      "Epoch: 91 | Batch_idx: 230 |  Loss_1: (0.0630) | Acc_1: (97.79%) (28916/29568)\n",
      "Epoch: 91 | Batch_idx: 240 |  Loss_1: (0.0626) | Acc_1: (97.81%) (30172/30848)\n",
      "Epoch: 91 | Batch_idx: 250 |  Loss_1: (0.0627) | Acc_1: (97.80%) (31420/32128)\n",
      "Epoch: 91 | Batch_idx: 260 |  Loss_1: (0.0629) | Acc_1: (97.79%) (32670/33408)\n",
      "Epoch: 91 | Batch_idx: 270 |  Loss_1: (0.0627) | Acc_1: (97.79%) (33923/34688)\n",
      "Epoch: 91 | Batch_idx: 280 |  Loss_1: (0.0632) | Acc_1: (97.78%) (35171/35968)\n",
      "Epoch: 91 | Batch_idx: 290 |  Loss_1: (0.0635) | Acc_1: (97.78%) (36422/37248)\n",
      "Epoch: 91 | Batch_idx: 300 |  Loss_1: (0.0637) | Acc_1: (97.77%) (37668/38528)\n",
      "Epoch: 91 | Batch_idx: 310 |  Loss_1: (0.0639) | Acc_1: (97.76%) (38918/39808)\n",
      "Epoch: 91 | Batch_idx: 320 |  Loss_1: (0.0641) | Acc_1: (97.77%) (40170/41088)\n",
      "Epoch: 91 | Batch_idx: 330 |  Loss_1: (0.0646) | Acc_1: (97.76%) (41417/42368)\n",
      "Epoch: 91 | Batch_idx: 340 |  Loss_1: (0.0645) | Acc_1: (97.77%) (42675/43648)\n",
      "Epoch: 91 | Batch_idx: 350 |  Loss_1: (0.0647) | Acc_1: (97.76%) (43920/44928)\n",
      "Epoch: 91 | Batch_idx: 360 |  Loss_1: (0.0652) | Acc_1: (97.74%) (45162/46208)\n",
      "Epoch: 91 | Batch_idx: 370 |  Loss_1: (0.0657) | Acc_1: (97.71%) (46402/47488)\n",
      "Epoch: 91 | Batch_idx: 380 |  Loss_1: (0.0654) | Acc_1: (97.72%) (47658/48768)\n",
      "Epoch: 91 | Batch_idx: 390 |  Loss_1: (0.0652) | Acc_1: (97.73%) (48863/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4166) | Acc: (90.85%) (9085/10000)\n",
      "Epoch: 92 | Batch_idx: 0 |  Loss_1: (0.1502) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 92 | Batch_idx: 10 |  Loss_1: (0.0728) | Acc_1: (98.01%) (1380/1408)\n",
      "Epoch: 92 | Batch_idx: 20 |  Loss_1: (0.0706) | Acc_1: (97.81%) (2629/2688)\n",
      "Epoch: 92 | Batch_idx: 30 |  Loss_1: (0.0666) | Acc_1: (97.83%) (3882/3968)\n",
      "Epoch: 92 | Batch_idx: 40 |  Loss_1: (0.0635) | Acc_1: (97.96%) (5141/5248)\n",
      "Epoch: 92 | Batch_idx: 50 |  Loss_1: (0.0618) | Acc_1: (97.93%) (6393/6528)\n",
      "Epoch: 92 | Batch_idx: 60 |  Loss_1: (0.0599) | Acc_1: (98.07%) (7657/7808)\n",
      "Epoch: 92 | Batch_idx: 70 |  Loss_1: (0.0599) | Acc_1: (98.11%) (8916/9088)\n",
      "Epoch: 92 | Batch_idx: 80 |  Loss_1: (0.0595) | Acc_1: (98.11%) (10172/10368)\n",
      "Epoch: 92 | Batch_idx: 90 |  Loss_1: (0.0574) | Acc_1: (98.17%) (11435/11648)\n",
      "Epoch: 92 | Batch_idx: 100 |  Loss_1: (0.0580) | Acc_1: (98.14%) (12688/12928)\n",
      "Epoch: 92 | Batch_idx: 110 |  Loss_1: (0.0559) | Acc_1: (98.20%) (13952/14208)\n",
      "Epoch: 92 | Batch_idx: 120 |  Loss_1: (0.0555) | Acc_1: (98.20%) (15209/15488)\n",
      "Epoch: 92 | Batch_idx: 130 |  Loss_1: (0.0561) | Acc_1: (98.18%) (16463/16768)\n",
      "Epoch: 92 | Batch_idx: 140 |  Loss_1: (0.0561) | Acc_1: (98.16%) (17716/18048)\n",
      "Epoch: 92 | Batch_idx: 150 |  Loss_1: (0.0568) | Acc_1: (98.13%) (18967/19328)\n",
      "Epoch: 92 | Batch_idx: 160 |  Loss_1: (0.0557) | Acc_1: (98.17%) (20231/20608)\n",
      "Epoch: 92 | Batch_idx: 170 |  Loss_1: (0.0563) | Acc_1: (98.15%) (21482/21888)\n",
      "Epoch: 92 | Batch_idx: 180 |  Loss_1: (0.0568) | Acc_1: (98.11%) (22729/23168)\n",
      "Epoch: 92 | Batch_idx: 190 |  Loss_1: (0.0575) | Acc_1: (98.08%) (23978/24448)\n",
      "Epoch: 92 | Batch_idx: 200 |  Loss_1: (0.0581) | Acc_1: (98.04%) (25225/25728)\n",
      "Epoch: 92 | Batch_idx: 210 |  Loss_1: (0.0583) | Acc_1: (98.03%) (26475/27008)\n",
      "Epoch: 92 | Batch_idx: 220 |  Loss_1: (0.0585) | Acc_1: (98.02%) (27729/28288)\n",
      "Epoch: 92 | Batch_idx: 230 |  Loss_1: (0.0587) | Acc_1: (97.98%) (28972/29568)\n",
      "Epoch: 92 | Batch_idx: 240 |  Loss_1: (0.0583) | Acc_1: (97.99%) (30228/30848)\n",
      "Epoch: 92 | Batch_idx: 250 |  Loss_1: (0.0580) | Acc_1: (98.01%) (31488/32128)\n",
      "Epoch: 92 | Batch_idx: 260 |  Loss_1: (0.0577) | Acc_1: (98.01%) (32743/33408)\n",
      "Epoch: 92 | Batch_idx: 270 |  Loss_1: (0.0578) | Acc_1: (98.01%) (33997/34688)\n",
      "Epoch: 92 | Batch_idx: 280 |  Loss_1: (0.0577) | Acc_1: (97.99%) (35246/35968)\n",
      "Epoch: 92 | Batch_idx: 290 |  Loss_1: (0.0585) | Acc_1: (97.96%) (36489/37248)\n",
      "Epoch: 92 | Batch_idx: 300 |  Loss_1: (0.0586) | Acc_1: (97.97%) (37746/38528)\n",
      "Epoch: 92 | Batch_idx: 310 |  Loss_1: (0.0585) | Acc_1: (97.98%) (39003/39808)\n",
      "Epoch: 92 | Batch_idx: 320 |  Loss_1: (0.0582) | Acc_1: (97.99%) (40263/41088)\n",
      "Epoch: 92 | Batch_idx: 330 |  Loss_1: (0.0585) | Acc_1: (97.98%) (41513/42368)\n",
      "Epoch: 92 | Batch_idx: 340 |  Loss_1: (0.0586) | Acc_1: (97.97%) (42763/43648)\n",
      "Epoch: 92 | Batch_idx: 350 |  Loss_1: (0.0587) | Acc_1: (97.97%) (44014/44928)\n",
      "Epoch: 92 | Batch_idx: 360 |  Loss_1: (0.0590) | Acc_1: (97.95%) (45259/46208)\n",
      "Epoch: 92 | Batch_idx: 370 |  Loss_1: (0.0590) | Acc_1: (97.95%) (46516/47488)\n",
      "Epoch: 92 | Batch_idx: 380 |  Loss_1: (0.0593) | Acc_1: (97.93%) (47760/48768)\n",
      "Epoch: 92 | Batch_idx: 390 |  Loss_1: (0.0595) | Acc_1: (97.92%) (48961/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4674) | Acc: (90.43%) (9043/10000)\n",
      "Epoch: 93 | Batch_idx: 0 |  Loss_1: (0.0709) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 93 | Batch_idx: 10 |  Loss_1: (0.0586) | Acc_1: (97.59%) (1374/1408)\n",
      "Epoch: 93 | Batch_idx: 20 |  Loss_1: (0.0563) | Acc_1: (97.84%) (2630/2688)\n",
      "Epoch: 93 | Batch_idx: 30 |  Loss_1: (0.0543) | Acc_1: (97.91%) (3885/3968)\n",
      "Epoch: 93 | Batch_idx: 40 |  Loss_1: (0.0599) | Acc_1: (97.66%) (5125/5248)\n",
      "Epoch: 93 | Batch_idx: 50 |  Loss_1: (0.0634) | Acc_1: (97.59%) (6371/6528)\n",
      "Epoch: 93 | Batch_idx: 60 |  Loss_1: (0.0606) | Acc_1: (97.72%) (7630/7808)\n",
      "Epoch: 93 | Batch_idx: 70 |  Loss_1: (0.0609) | Acc_1: (97.73%) (8882/9088)\n",
      "Epoch: 93 | Batch_idx: 80 |  Loss_1: (0.0601) | Acc_1: (97.77%) (10137/10368)\n",
      "Epoch: 93 | Batch_idx: 90 |  Loss_1: (0.0614) | Acc_1: (97.74%) (11385/11648)\n",
      "Epoch: 93 | Batch_idx: 100 |  Loss_1: (0.0595) | Acc_1: (97.80%) (12644/12928)\n",
      "Epoch: 93 | Batch_idx: 110 |  Loss_1: (0.0588) | Acc_1: (97.83%) (13899/14208)\n",
      "Epoch: 93 | Batch_idx: 120 |  Loss_1: (0.0580) | Acc_1: (97.88%) (15159/15488)\n",
      "Epoch: 93 | Batch_idx: 130 |  Loss_1: (0.0572) | Acc_1: (97.91%) (16417/16768)\n",
      "Epoch: 93 | Batch_idx: 140 |  Loss_1: (0.0562) | Acc_1: (97.94%) (17676/18048)\n",
      "Epoch: 93 | Batch_idx: 150 |  Loss_1: (0.0562) | Acc_1: (97.94%) (18930/19328)\n",
      "Epoch: 93 | Batch_idx: 160 |  Loss_1: (0.0558) | Acc_1: (97.96%) (20188/20608)\n",
      "Epoch: 93 | Batch_idx: 170 |  Loss_1: (0.0554) | Acc_1: (97.97%) (21443/21888)\n",
      "Epoch: 93 | Batch_idx: 180 |  Loss_1: (0.0557) | Acc_1: (97.98%) (22701/23168)\n",
      "Epoch: 93 | Batch_idx: 190 |  Loss_1: (0.0566) | Acc_1: (97.97%) (23951/24448)\n",
      "Epoch: 93 | Batch_idx: 200 |  Loss_1: (0.0567) | Acc_1: (97.94%) (25199/25728)\n",
      "Epoch: 93 | Batch_idx: 210 |  Loss_1: (0.0559) | Acc_1: (97.99%) (26465/27008)\n",
      "Epoch: 93 | Batch_idx: 220 |  Loss_1: (0.0563) | Acc_1: (97.96%) (27711/28288)\n",
      "Epoch: 93 | Batch_idx: 230 |  Loss_1: (0.0564) | Acc_1: (97.95%) (28963/29568)\n",
      "Epoch: 93 | Batch_idx: 240 |  Loss_1: (0.0569) | Acc_1: (97.95%) (30215/30848)\n",
      "Epoch: 93 | Batch_idx: 250 |  Loss_1: (0.0570) | Acc_1: (97.94%) (31466/32128)\n",
      "Epoch: 93 | Batch_idx: 260 |  Loss_1: (0.0575) | Acc_1: (97.94%) (32720/33408)\n",
      "Epoch: 93 | Batch_idx: 270 |  Loss_1: (0.0577) | Acc_1: (97.93%) (33969/34688)\n",
      "Epoch: 93 | Batch_idx: 280 |  Loss_1: (0.0575) | Acc_1: (97.93%) (35222/35968)\n",
      "Epoch: 93 | Batch_idx: 290 |  Loss_1: (0.0572) | Acc_1: (97.92%) (36473/37248)\n",
      "Epoch: 93 | Batch_idx: 300 |  Loss_1: (0.0572) | Acc_1: (97.92%) (37727/38528)\n",
      "Epoch: 93 | Batch_idx: 310 |  Loss_1: (0.0572) | Acc_1: (97.92%) (38980/39808)\n",
      "Epoch: 93 | Batch_idx: 320 |  Loss_1: (0.0580) | Acc_1: (97.90%) (40225/41088)\n",
      "Epoch: 93 | Batch_idx: 330 |  Loss_1: (0.0581) | Acc_1: (97.90%) (41480/42368)\n",
      "Epoch: 93 | Batch_idx: 340 |  Loss_1: (0.0580) | Acc_1: (97.92%) (42738/43648)\n",
      "Epoch: 93 | Batch_idx: 350 |  Loss_1: (0.0583) | Acc_1: (97.91%) (43991/44928)\n",
      "Epoch: 93 | Batch_idx: 360 |  Loss_1: (0.0583) | Acc_1: (97.91%) (45244/46208)\n",
      "Epoch: 93 | Batch_idx: 370 |  Loss_1: (0.0587) | Acc_1: (97.90%) (46493/47488)\n",
      "Epoch: 93 | Batch_idx: 380 |  Loss_1: (0.0587) | Acc_1: (97.91%) (47747/48768)\n",
      "Epoch: 93 | Batch_idx: 390 |  Loss_1: (0.0588) | Acc_1: (97.90%) (48951/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4593) | Acc: (90.41%) (9041/10000)\n",
      "Epoch: 94 | Batch_idx: 0 |  Loss_1: (0.0620) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 94 | Batch_idx: 10 |  Loss_1: (0.0527) | Acc_1: (97.80%) (1377/1408)\n",
      "Epoch: 94 | Batch_idx: 20 |  Loss_1: (0.0483) | Acc_1: (98.18%) (2639/2688)\n",
      "Epoch: 94 | Batch_idx: 30 |  Loss_1: (0.0479) | Acc_1: (98.34%) (3902/3968)\n",
      "Epoch: 94 | Batch_idx: 40 |  Loss_1: (0.0519) | Acc_1: (98.17%) (5152/5248)\n",
      "Epoch: 94 | Batch_idx: 50 |  Loss_1: (0.0492) | Acc_1: (98.28%) (6416/6528)\n",
      "Epoch: 94 | Batch_idx: 60 |  Loss_1: (0.0483) | Acc_1: (98.34%) (7678/7808)\n",
      "Epoch: 94 | Batch_idx: 70 |  Loss_1: (0.0494) | Acc_1: (98.27%) (8931/9088)\n",
      "Epoch: 94 | Batch_idx: 80 |  Loss_1: (0.0487) | Acc_1: (98.31%) (10193/10368)\n",
      "Epoch: 94 | Batch_idx: 90 |  Loss_1: (0.0509) | Acc_1: (98.26%) (11445/11648)\n",
      "Epoch: 94 | Batch_idx: 100 |  Loss_1: (0.0507) | Acc_1: (98.27%) (12704/12928)\n",
      "Epoch: 94 | Batch_idx: 110 |  Loss_1: (0.0518) | Acc_1: (98.25%) (13960/14208)\n",
      "Epoch: 94 | Batch_idx: 120 |  Loss_1: (0.0528) | Acc_1: (98.21%) (15211/15488)\n",
      "Epoch: 94 | Batch_idx: 130 |  Loss_1: (0.0531) | Acc_1: (98.20%) (16467/16768)\n",
      "Epoch: 94 | Batch_idx: 140 |  Loss_1: (0.0544) | Acc_1: (98.17%) (17718/18048)\n",
      "Epoch: 94 | Batch_idx: 150 |  Loss_1: (0.0552) | Acc_1: (98.14%) (18968/19328)\n",
      "Epoch: 94 | Batch_idx: 160 |  Loss_1: (0.0555) | Acc_1: (98.11%) (20219/20608)\n",
      "Epoch: 94 | Batch_idx: 170 |  Loss_1: (0.0549) | Acc_1: (98.14%) (21480/21888)\n",
      "Epoch: 94 | Batch_idx: 180 |  Loss_1: (0.0547) | Acc_1: (98.16%) (22742/23168)\n",
      "Epoch: 94 | Batch_idx: 190 |  Loss_1: (0.0552) | Acc_1: (98.16%) (23999/24448)\n",
      "Epoch: 94 | Batch_idx: 200 |  Loss_1: (0.0551) | Acc_1: (98.16%) (25255/25728)\n",
      "Epoch: 94 | Batch_idx: 210 |  Loss_1: (0.0550) | Acc_1: (98.16%) (26512/27008)\n",
      "Epoch: 94 | Batch_idx: 220 |  Loss_1: (0.0552) | Acc_1: (98.16%) (27768/28288)\n",
      "Epoch: 94 | Batch_idx: 230 |  Loss_1: (0.0548) | Acc_1: (98.16%) (29025/29568)\n",
      "Epoch: 94 | Batch_idx: 240 |  Loss_1: (0.0549) | Acc_1: (98.16%) (30281/30848)\n",
      "Epoch: 94 | Batch_idx: 250 |  Loss_1: (0.0544) | Acc_1: (98.18%) (31542/32128)\n",
      "Epoch: 94 | Batch_idx: 260 |  Loss_1: (0.0545) | Acc_1: (98.19%) (32803/33408)\n",
      "Epoch: 94 | Batch_idx: 270 |  Loss_1: (0.0545) | Acc_1: (98.18%) (34057/34688)\n",
      "Epoch: 94 | Batch_idx: 280 |  Loss_1: (0.0551) | Acc_1: (98.16%) (35305/35968)\n",
      "Epoch: 94 | Batch_idx: 290 |  Loss_1: (0.0553) | Acc_1: (98.15%) (36559/37248)\n",
      "Epoch: 94 | Batch_idx: 300 |  Loss_1: (0.0547) | Acc_1: (98.18%) (37825/38528)\n",
      "Epoch: 94 | Batch_idx: 310 |  Loss_1: (0.0548) | Acc_1: (98.17%) (39080/39808)\n",
      "Epoch: 94 | Batch_idx: 320 |  Loss_1: (0.0547) | Acc_1: (98.17%) (40337/41088)\n",
      "Epoch: 94 | Batch_idx: 330 |  Loss_1: (0.0546) | Acc_1: (98.17%) (41591/42368)\n",
      "Epoch: 94 | Batch_idx: 340 |  Loss_1: (0.0543) | Acc_1: (98.17%) (42849/43648)\n",
      "Epoch: 94 | Batch_idx: 350 |  Loss_1: (0.0543) | Acc_1: (98.17%) (44106/44928)\n",
      "Epoch: 94 | Batch_idx: 360 |  Loss_1: (0.0544) | Acc_1: (98.17%) (45361/46208)\n",
      "Epoch: 94 | Batch_idx: 370 |  Loss_1: (0.0547) | Acc_1: (98.16%) (46615/47488)\n",
      "Epoch: 94 | Batch_idx: 380 |  Loss_1: (0.0547) | Acc_1: (98.17%) (47874/48768)\n",
      "Epoch: 94 | Batch_idx: 390 |  Loss_1: (0.0547) | Acc_1: (98.17%) (49085/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4247) | Acc: (90.86%) (9086/10000)\n",
      "Epoch: 95 | Batch_idx: 0 |  Loss_1: (0.0603) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 95 | Batch_idx: 10 |  Loss_1: (0.0473) | Acc_1: (98.44%) (1386/1408)\n",
      "Epoch: 95 | Batch_idx: 20 |  Loss_1: (0.0537) | Acc_1: (98.10%) (2637/2688)\n",
      "Epoch: 95 | Batch_idx: 30 |  Loss_1: (0.0517) | Acc_1: (98.11%) (3893/3968)\n",
      "Epoch: 95 | Batch_idx: 40 |  Loss_1: (0.0508) | Acc_1: (98.17%) (5152/5248)\n",
      "Epoch: 95 | Batch_idx: 50 |  Loss_1: (0.0536) | Acc_1: (98.12%) (6405/6528)\n",
      "Epoch: 95 | Batch_idx: 60 |  Loss_1: (0.0557) | Acc_1: (98.09%) (7659/7808)\n",
      "Epoch: 95 | Batch_idx: 70 |  Loss_1: (0.0570) | Acc_1: (98.06%) (8912/9088)\n",
      "Epoch: 95 | Batch_idx: 80 |  Loss_1: (0.0560) | Acc_1: (98.11%) (10172/10368)\n",
      "Epoch: 95 | Batch_idx: 90 |  Loss_1: (0.0562) | Acc_1: (98.12%) (11429/11648)\n",
      "Epoch: 95 | Batch_idx: 100 |  Loss_1: (0.0556) | Acc_1: (98.15%) (12689/12928)\n",
      "Epoch: 95 | Batch_idx: 110 |  Loss_1: (0.0556) | Acc_1: (98.13%) (13943/14208)\n",
      "Epoch: 95 | Batch_idx: 120 |  Loss_1: (0.0564) | Acc_1: (98.12%) (15197/15488)\n",
      "Epoch: 95 | Batch_idx: 130 |  Loss_1: (0.0567) | Acc_1: (98.12%) (16452/16768)\n",
      "Epoch: 95 | Batch_idx: 140 |  Loss_1: (0.0576) | Acc_1: (98.07%) (17699/18048)\n",
      "Epoch: 95 | Batch_idx: 150 |  Loss_1: (0.0585) | Acc_1: (98.03%) (18948/19328)\n",
      "Epoch: 95 | Batch_idx: 160 |  Loss_1: (0.0582) | Acc_1: (98.03%) (20201/20608)\n",
      "Epoch: 95 | Batch_idx: 170 |  Loss_1: (0.0576) | Acc_1: (98.05%) (21462/21888)\n",
      "Epoch: 95 | Batch_idx: 180 |  Loss_1: (0.0578) | Acc_1: (98.04%) (22715/23168)\n",
      "Epoch: 95 | Batch_idx: 190 |  Loss_1: (0.0576) | Acc_1: (98.06%) (23974/24448)\n",
      "Epoch: 95 | Batch_idx: 200 |  Loss_1: (0.0576) | Acc_1: (98.05%) (25226/25728)\n",
      "Epoch: 95 | Batch_idx: 210 |  Loss_1: (0.0576) | Acc_1: (98.05%) (26482/27008)\n",
      "Epoch: 95 | Batch_idx: 220 |  Loss_1: (0.0575) | Acc_1: (98.05%) (27736/28288)\n",
      "Epoch: 95 | Batch_idx: 230 |  Loss_1: (0.0573) | Acc_1: (98.05%) (28992/29568)\n",
      "Epoch: 95 | Batch_idx: 240 |  Loss_1: (0.0570) | Acc_1: (98.05%) (30248/30848)\n",
      "Epoch: 95 | Batch_idx: 250 |  Loss_1: (0.0569) | Acc_1: (98.05%) (31501/32128)\n",
      "Epoch: 95 | Batch_idx: 260 |  Loss_1: (0.0570) | Acc_1: (98.05%) (32756/33408)\n",
      "Epoch: 95 | Batch_idx: 270 |  Loss_1: (0.0570) | Acc_1: (98.05%) (34013/34688)\n",
      "Epoch: 95 | Batch_idx: 280 |  Loss_1: (0.0567) | Acc_1: (98.06%) (35271/35968)\n",
      "Epoch: 95 | Batch_idx: 290 |  Loss_1: (0.0563) | Acc_1: (98.07%) (36528/37248)\n",
      "Epoch: 95 | Batch_idx: 300 |  Loss_1: (0.0562) | Acc_1: (98.06%) (37780/38528)\n",
      "Epoch: 95 | Batch_idx: 310 |  Loss_1: (0.0564) | Acc_1: (98.05%) (39033/39808)\n",
      "Epoch: 95 | Batch_idx: 320 |  Loss_1: (0.0562) | Acc_1: (98.06%) (40292/41088)\n",
      "Epoch: 95 | Batch_idx: 330 |  Loss_1: (0.0569) | Acc_1: (98.04%) (41538/42368)\n",
      "Epoch: 95 | Batch_idx: 340 |  Loss_1: (0.0569) | Acc_1: (98.03%) (42789/43648)\n",
      "Epoch: 95 | Batch_idx: 350 |  Loss_1: (0.0568) | Acc_1: (98.03%) (44042/44928)\n",
      "Epoch: 95 | Batch_idx: 360 |  Loss_1: (0.0565) | Acc_1: (98.04%) (45301/46208)\n",
      "Epoch: 95 | Batch_idx: 370 |  Loss_1: (0.0563) | Acc_1: (98.04%) (46558/47488)\n",
      "Epoch: 95 | Batch_idx: 380 |  Loss_1: (0.0564) | Acc_1: (98.04%) (47814/48768)\n",
      "Epoch: 95 | Batch_idx: 390 |  Loss_1: (0.0560) | Acc_1: (98.06%) (49028/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4155) | Acc: (91.37%) (9137/10000)\n",
      "Epoch: 96 | Batch_idx: 0 |  Loss_1: (0.0566) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 96 | Batch_idx: 10 |  Loss_1: (0.0394) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 96 | Batch_idx: 20 |  Loss_1: (0.0467) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 96 | Batch_idx: 30 |  Loss_1: (0.0484) | Acc_1: (98.39%) (3904/3968)\n",
      "Epoch: 96 | Batch_idx: 40 |  Loss_1: (0.0498) | Acc_1: (98.30%) (5159/5248)\n",
      "Epoch: 96 | Batch_idx: 50 |  Loss_1: (0.0512) | Acc_1: (98.25%) (6414/6528)\n",
      "Epoch: 96 | Batch_idx: 60 |  Loss_1: (0.0537) | Acc_1: (98.22%) (7669/7808)\n",
      "Epoch: 96 | Batch_idx: 70 |  Loss_1: (0.0539) | Acc_1: (98.24%) (8928/9088)\n",
      "Epoch: 96 | Batch_idx: 80 |  Loss_1: (0.0536) | Acc_1: (98.28%) (10190/10368)\n",
      "Epoch: 96 | Batch_idx: 90 |  Loss_1: (0.0535) | Acc_1: (98.27%) (11447/11648)\n",
      "Epoch: 96 | Batch_idx: 100 |  Loss_1: (0.0525) | Acc_1: (98.26%) (12703/12928)\n",
      "Epoch: 96 | Batch_idx: 110 |  Loss_1: (0.0519) | Acc_1: (98.25%) (13960/14208)\n",
      "Epoch: 96 | Batch_idx: 120 |  Loss_1: (0.0505) | Acc_1: (98.31%) (15226/15488)\n",
      "Epoch: 96 | Batch_idx: 130 |  Loss_1: (0.0500) | Acc_1: (98.35%) (16491/16768)\n",
      "Epoch: 96 | Batch_idx: 140 |  Loss_1: (0.0508) | Acc_1: (98.33%) (17747/18048)\n",
      "Epoch: 96 | Batch_idx: 150 |  Loss_1: (0.0506) | Acc_1: (98.35%) (19009/19328)\n",
      "Epoch: 96 | Batch_idx: 160 |  Loss_1: (0.0504) | Acc_1: (98.35%) (20267/20608)\n",
      "Epoch: 96 | Batch_idx: 170 |  Loss_1: (0.0510) | Acc_1: (98.31%) (21517/21888)\n",
      "Epoch: 96 | Batch_idx: 180 |  Loss_1: (0.0513) | Acc_1: (98.28%) (22770/23168)\n",
      "Epoch: 96 | Batch_idx: 190 |  Loss_1: (0.0521) | Acc_1: (98.23%) (24016/24448)\n",
      "Epoch: 96 | Batch_idx: 200 |  Loss_1: (0.0525) | Acc_1: (98.22%) (25269/25728)\n",
      "Epoch: 96 | Batch_idx: 210 |  Loss_1: (0.0528) | Acc_1: (98.21%) (26524/27008)\n",
      "Epoch: 96 | Batch_idx: 220 |  Loss_1: (0.0522) | Acc_1: (98.21%) (27783/28288)\n",
      "Epoch: 96 | Batch_idx: 230 |  Loss_1: (0.0532) | Acc_1: (98.19%) (29032/29568)\n",
      "Epoch: 96 | Batch_idx: 240 |  Loss_1: (0.0533) | Acc_1: (98.18%) (30288/30848)\n",
      "Epoch: 96 | Batch_idx: 250 |  Loss_1: (0.0534) | Acc_1: (98.18%) (31544/32128)\n",
      "Epoch: 96 | Batch_idx: 260 |  Loss_1: (0.0539) | Acc_1: (98.17%) (32796/33408)\n",
      "Epoch: 96 | Batch_idx: 270 |  Loss_1: (0.0539) | Acc_1: (98.18%) (34055/34688)\n",
      "Epoch: 96 | Batch_idx: 280 |  Loss_1: (0.0542) | Acc_1: (98.17%) (35308/35968)\n",
      "Epoch: 96 | Batch_idx: 290 |  Loss_1: (0.0540) | Acc_1: (98.18%) (36570/37248)\n",
      "Epoch: 96 | Batch_idx: 300 |  Loss_1: (0.0538) | Acc_1: (98.19%) (37830/38528)\n",
      "Epoch: 96 | Batch_idx: 310 |  Loss_1: (0.0538) | Acc_1: (98.17%) (39081/39808)\n",
      "Epoch: 96 | Batch_idx: 320 |  Loss_1: (0.0537) | Acc_1: (98.17%) (40336/41088)\n",
      "Epoch: 96 | Batch_idx: 330 |  Loss_1: (0.0536) | Acc_1: (98.17%) (41594/42368)\n",
      "Epoch: 96 | Batch_idx: 340 |  Loss_1: (0.0532) | Acc_1: (98.18%) (42855/43648)\n",
      "Epoch: 96 | Batch_idx: 350 |  Loss_1: (0.0533) | Acc_1: (98.18%) (44109/44928)\n",
      "Epoch: 96 | Batch_idx: 360 |  Loss_1: (0.0536) | Acc_1: (98.17%) (45361/46208)\n",
      "Epoch: 96 | Batch_idx: 370 |  Loss_1: (0.0533) | Acc_1: (98.18%) (46624/47488)\n",
      "Epoch: 96 | Batch_idx: 380 |  Loss_1: (0.0535) | Acc_1: (98.17%) (47874/48768)\n",
      "Epoch: 96 | Batch_idx: 390 |  Loss_1: (0.0539) | Acc_1: (98.16%) (49081/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4688) | Acc: (90.46%) (9046/10000)\n",
      "Epoch: 97 | Batch_idx: 0 |  Loss_1: (0.0562) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 97 | Batch_idx: 10 |  Loss_1: (0.0531) | Acc_1: (97.94%) (1379/1408)\n",
      "Epoch: 97 | Batch_idx: 20 |  Loss_1: (0.0474) | Acc_1: (98.07%) (2636/2688)\n",
      "Epoch: 97 | Batch_idx: 30 |  Loss_1: (0.0507) | Acc_1: (97.98%) (3888/3968)\n",
      "Epoch: 97 | Batch_idx: 40 |  Loss_1: (0.0480) | Acc_1: (98.17%) (5152/5248)\n",
      "Epoch: 97 | Batch_idx: 50 |  Loss_1: (0.0478) | Acc_1: (98.25%) (6414/6528)\n",
      "Epoch: 97 | Batch_idx: 60 |  Loss_1: (0.0473) | Acc_1: (98.28%) (7674/7808)\n",
      "Epoch: 97 | Batch_idx: 70 |  Loss_1: (0.0487) | Acc_1: (98.20%) (8924/9088)\n",
      "Epoch: 97 | Batch_idx: 80 |  Loss_1: (0.0497) | Acc_1: (98.18%) (10179/10368)\n",
      "Epoch: 97 | Batch_idx: 90 |  Loss_1: (0.0495) | Acc_1: (98.20%) (11438/11648)\n",
      "Epoch: 97 | Batch_idx: 100 |  Loss_1: (0.0483) | Acc_1: (98.25%) (12702/12928)\n",
      "Epoch: 97 | Batch_idx: 110 |  Loss_1: (0.0479) | Acc_1: (98.28%) (13963/14208)\n",
      "Epoch: 97 | Batch_idx: 120 |  Loss_1: (0.0476) | Acc_1: (98.31%) (15227/15488)\n",
      "Epoch: 97 | Batch_idx: 130 |  Loss_1: (0.0477) | Acc_1: (98.32%) (16486/16768)\n",
      "Epoch: 97 | Batch_idx: 140 |  Loss_1: (0.0478) | Acc_1: (98.31%) (17743/18048)\n",
      "Epoch: 97 | Batch_idx: 150 |  Loss_1: (0.0476) | Acc_1: (98.34%) (19007/19328)\n",
      "Epoch: 97 | Batch_idx: 160 |  Loss_1: (0.0485) | Acc_1: (98.32%) (20261/20608)\n",
      "Epoch: 97 | Batch_idx: 170 |  Loss_1: (0.0491) | Acc_1: (98.31%) (21518/21888)\n",
      "Epoch: 97 | Batch_idx: 180 |  Loss_1: (0.0495) | Acc_1: (98.29%) (22771/23168)\n",
      "Epoch: 97 | Batch_idx: 190 |  Loss_1: (0.0504) | Acc_1: (98.26%) (24022/24448)\n",
      "Epoch: 97 | Batch_idx: 200 |  Loss_1: (0.0504) | Acc_1: (98.25%) (25277/25728)\n",
      "Epoch: 97 | Batch_idx: 210 |  Loss_1: (0.0500) | Acc_1: (98.27%) (26540/27008)\n",
      "Epoch: 97 | Batch_idx: 220 |  Loss_1: (0.0502) | Acc_1: (98.24%) (27791/28288)\n",
      "Epoch: 97 | Batch_idx: 230 |  Loss_1: (0.0497) | Acc_1: (98.27%) (29057/29568)\n",
      "Epoch: 97 | Batch_idx: 240 |  Loss_1: (0.0505) | Acc_1: (98.26%) (30312/30848)\n",
      "Epoch: 97 | Batch_idx: 250 |  Loss_1: (0.0506) | Acc_1: (98.25%) (31567/32128)\n",
      "Epoch: 97 | Batch_idx: 260 |  Loss_1: (0.0503) | Acc_1: (98.26%) (32828/33408)\n",
      "Epoch: 97 | Batch_idx: 270 |  Loss_1: (0.0498) | Acc_1: (98.29%) (34094/34688)\n",
      "Epoch: 97 | Batch_idx: 280 |  Loss_1: (0.0498) | Acc_1: (98.29%) (35352/35968)\n",
      "Epoch: 97 | Batch_idx: 290 |  Loss_1: (0.0495) | Acc_1: (98.30%) (36614/37248)\n",
      "Epoch: 97 | Batch_idx: 300 |  Loss_1: (0.0497) | Acc_1: (98.29%) (37869/38528)\n",
      "Epoch: 97 | Batch_idx: 310 |  Loss_1: (0.0496) | Acc_1: (98.30%) (39132/39808)\n",
      "Epoch: 97 | Batch_idx: 320 |  Loss_1: (0.0499) | Acc_1: (98.29%) (40386/41088)\n",
      "Epoch: 97 | Batch_idx: 330 |  Loss_1: (0.0502) | Acc_1: (98.27%) (41636/42368)\n",
      "Epoch: 97 | Batch_idx: 340 |  Loss_1: (0.0503) | Acc_1: (98.27%) (42892/43648)\n",
      "Epoch: 97 | Batch_idx: 350 |  Loss_1: (0.0501) | Acc_1: (98.28%) (44154/44928)\n",
      "Epoch: 97 | Batch_idx: 360 |  Loss_1: (0.0507) | Acc_1: (98.25%) (45401/46208)\n",
      "Epoch: 97 | Batch_idx: 370 |  Loss_1: (0.0512) | Acc_1: (98.25%) (46657/47488)\n",
      "Epoch: 97 | Batch_idx: 380 |  Loss_1: (0.0512) | Acc_1: (98.25%) (47915/48768)\n",
      "Epoch: 97 | Batch_idx: 390 |  Loss_1: (0.0515) | Acc_1: (98.25%) (49123/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4468) | Acc: (90.89%) (9089/10000)\n",
      "Epoch: 98 | Batch_idx: 0 |  Loss_1: (0.0069) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 98 | Batch_idx: 10 |  Loss_1: (0.0466) | Acc_1: (98.44%) (1386/1408)\n",
      "Epoch: 98 | Batch_idx: 20 |  Loss_1: (0.0576) | Acc_1: (98.10%) (2637/2688)\n",
      "Epoch: 98 | Batch_idx: 30 |  Loss_1: (0.0560) | Acc_1: (98.16%) (3895/3968)\n",
      "Epoch: 98 | Batch_idx: 40 |  Loss_1: (0.0558) | Acc_1: (98.09%) (5148/5248)\n",
      "Epoch: 98 | Batch_idx: 50 |  Loss_1: (0.0551) | Acc_1: (98.15%) (6407/6528)\n",
      "Epoch: 98 | Batch_idx: 60 |  Loss_1: (0.0547) | Acc_1: (98.14%) (7663/7808)\n",
      "Epoch: 98 | Batch_idx: 70 |  Loss_1: (0.0537) | Acc_1: (98.17%) (8922/9088)\n",
      "Epoch: 98 | Batch_idx: 80 |  Loss_1: (0.0525) | Acc_1: (98.20%) (10181/10368)\n",
      "Epoch: 98 | Batch_idx: 90 |  Loss_1: (0.0519) | Acc_1: (98.23%) (11442/11648)\n",
      "Epoch: 98 | Batch_idx: 100 |  Loss_1: (0.0513) | Acc_1: (98.24%) (12701/12928)\n",
      "Epoch: 98 | Batch_idx: 110 |  Loss_1: (0.0514) | Acc_1: (98.20%) (13952/14208)\n",
      "Epoch: 98 | Batch_idx: 120 |  Loss_1: (0.0525) | Acc_1: (98.15%) (15202/15488)\n",
      "Epoch: 98 | Batch_idx: 130 |  Loss_1: (0.0525) | Acc_1: (98.16%) (16460/16768)\n",
      "Epoch: 98 | Batch_idx: 140 |  Loss_1: (0.0517) | Acc_1: (98.19%) (17722/18048)\n",
      "Epoch: 98 | Batch_idx: 150 |  Loss_1: (0.0527) | Acc_1: (98.16%) (18972/19328)\n",
      "Epoch: 98 | Batch_idx: 160 |  Loss_1: (0.0527) | Acc_1: (98.17%) (20230/20608)\n",
      "Epoch: 98 | Batch_idx: 170 |  Loss_1: (0.0527) | Acc_1: (98.15%) (21483/21888)\n",
      "Epoch: 98 | Batch_idx: 180 |  Loss_1: (0.0526) | Acc_1: (98.17%) (22744/23168)\n",
      "Epoch: 98 | Batch_idx: 190 |  Loss_1: (0.0533) | Acc_1: (98.15%) (23996/24448)\n",
      "Epoch: 98 | Batch_idx: 200 |  Loss_1: (0.0528) | Acc_1: (98.16%) (25255/25728)\n",
      "Epoch: 98 | Batch_idx: 210 |  Loss_1: (0.0529) | Acc_1: (98.18%) (26516/27008)\n",
      "Epoch: 98 | Batch_idx: 220 |  Loss_1: (0.0536) | Acc_1: (98.15%) (27764/28288)\n",
      "Epoch: 98 | Batch_idx: 230 |  Loss_1: (0.0542) | Acc_1: (98.13%) (29015/29568)\n",
      "Epoch: 98 | Batch_idx: 240 |  Loss_1: (0.0537) | Acc_1: (98.13%) (30272/30848)\n",
      "Epoch: 98 | Batch_idx: 250 |  Loss_1: (0.0532) | Acc_1: (98.15%) (31534/32128)\n",
      "Epoch: 98 | Batch_idx: 260 |  Loss_1: (0.0538) | Acc_1: (98.13%) (32784/33408)\n",
      "Epoch: 98 | Batch_idx: 270 |  Loss_1: (0.0541) | Acc_1: (98.11%) (34034/34688)\n",
      "Epoch: 98 | Batch_idx: 280 |  Loss_1: (0.0542) | Acc_1: (98.10%) (35285/35968)\n",
      "Epoch: 98 | Batch_idx: 290 |  Loss_1: (0.0541) | Acc_1: (98.11%) (36543/37248)\n",
      "Epoch: 98 | Batch_idx: 300 |  Loss_1: (0.0546) | Acc_1: (98.08%) (37788/38528)\n",
      "Epoch: 98 | Batch_idx: 310 |  Loss_1: (0.0548) | Acc_1: (98.08%) (39042/39808)\n",
      "Epoch: 98 | Batch_idx: 320 |  Loss_1: (0.0544) | Acc_1: (98.09%) (40302/41088)\n",
      "Epoch: 98 | Batch_idx: 330 |  Loss_1: (0.0543) | Acc_1: (98.09%) (41560/42368)\n",
      "Epoch: 98 | Batch_idx: 340 |  Loss_1: (0.0546) | Acc_1: (98.08%) (42810/43648)\n",
      "Epoch: 98 | Batch_idx: 350 |  Loss_1: (0.0544) | Acc_1: (98.09%) (44069/44928)\n",
      "Epoch: 98 | Batch_idx: 360 |  Loss_1: (0.0540) | Acc_1: (98.10%) (45331/46208)\n",
      "Epoch: 98 | Batch_idx: 370 |  Loss_1: (0.0540) | Acc_1: (98.10%) (46587/47488)\n",
      "Epoch: 98 | Batch_idx: 380 |  Loss_1: (0.0543) | Acc_1: (98.10%) (47843/48768)\n",
      "Epoch: 98 | Batch_idx: 390 |  Loss_1: (0.0544) | Acc_1: (98.10%) (49050/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4500) | Acc: (90.86%) (9086/10000)\n",
      "Epoch: 99 | Batch_idx: 0 |  Loss_1: (0.1461) | Acc_1: (94.53%) (121/128)\n",
      "Epoch: 99 | Batch_idx: 10 |  Loss_1: (0.0665) | Acc_1: (97.66%) (1375/1408)\n",
      "Epoch: 99 | Batch_idx: 20 |  Loss_1: (0.0552) | Acc_1: (98.14%) (2638/2688)\n",
      "Epoch: 99 | Batch_idx: 30 |  Loss_1: (0.0525) | Acc_1: (98.21%) (3897/3968)\n",
      "Epoch: 99 | Batch_idx: 40 |  Loss_1: (0.0511) | Acc_1: (98.29%) (5158/5248)\n",
      "Epoch: 99 | Batch_idx: 50 |  Loss_1: (0.0494) | Acc_1: (98.35%) (6420/6528)\n",
      "Epoch: 99 | Batch_idx: 60 |  Loss_1: (0.0475) | Acc_1: (98.40%) (7683/7808)\n",
      "Epoch: 99 | Batch_idx: 70 |  Loss_1: (0.0499) | Acc_1: (98.34%) (8937/9088)\n",
      "Epoch: 99 | Batch_idx: 80 |  Loss_1: (0.0513) | Acc_1: (98.29%) (10191/10368)\n",
      "Epoch: 99 | Batch_idx: 90 |  Loss_1: (0.0517) | Acc_1: (98.29%) (11449/11648)\n",
      "Epoch: 99 | Batch_idx: 100 |  Loss_1: (0.0536) | Acc_1: (98.24%) (12700/12928)\n",
      "Epoch: 99 | Batch_idx: 110 |  Loss_1: (0.0539) | Acc_1: (98.22%) (13955/14208)\n",
      "Epoch: 99 | Batch_idx: 120 |  Loss_1: (0.0548) | Acc_1: (98.19%) (15208/15488)\n",
      "Epoch: 99 | Batch_idx: 130 |  Loss_1: (0.0540) | Acc_1: (98.22%) (16469/16768)\n",
      "Epoch: 99 | Batch_idx: 140 |  Loss_1: (0.0546) | Acc_1: (98.19%) (17722/18048)\n",
      "Epoch: 99 | Batch_idx: 150 |  Loss_1: (0.0543) | Acc_1: (98.22%) (18983/19328)\n",
      "Epoch: 99 | Batch_idx: 160 |  Loss_1: (0.0558) | Acc_1: (98.16%) (20229/20608)\n",
      "Epoch: 99 | Batch_idx: 170 |  Loss_1: (0.0559) | Acc_1: (98.14%) (21481/21888)\n",
      "Epoch: 99 | Batch_idx: 180 |  Loss_1: (0.0546) | Acc_1: (98.19%) (22748/23168)\n",
      "Epoch: 99 | Batch_idx: 190 |  Loss_1: (0.0552) | Acc_1: (98.14%) (23993/24448)\n",
      "Epoch: 99 | Batch_idx: 200 |  Loss_1: (0.0549) | Acc_1: (98.17%) (25257/25728)\n",
      "Epoch: 99 | Batch_idx: 210 |  Loss_1: (0.0556) | Acc_1: (98.17%) (26514/27008)\n",
      "Epoch: 99 | Batch_idx: 220 |  Loss_1: (0.0552) | Acc_1: (98.17%) (27769/28288)\n",
      "Epoch: 99 | Batch_idx: 230 |  Loss_1: (0.0550) | Acc_1: (98.18%) (29030/29568)\n",
      "Epoch: 99 | Batch_idx: 240 |  Loss_1: (0.0548) | Acc_1: (98.19%) (30289/30848)\n",
      "Epoch: 99 | Batch_idx: 250 |  Loss_1: (0.0547) | Acc_1: (98.19%) (31546/32128)\n",
      "Epoch: 99 | Batch_idx: 260 |  Loss_1: (0.0549) | Acc_1: (98.18%) (32799/33408)\n",
      "Epoch: 99 | Batch_idx: 270 |  Loss_1: (0.0546) | Acc_1: (98.17%) (34054/34688)\n",
      "Epoch: 99 | Batch_idx: 280 |  Loss_1: (0.0545) | Acc_1: (98.18%) (35314/35968)\n",
      "Epoch: 99 | Batch_idx: 290 |  Loss_1: (0.0545) | Acc_1: (98.17%) (36567/37248)\n",
      "Epoch: 99 | Batch_idx: 300 |  Loss_1: (0.0547) | Acc_1: (98.17%) (37823/38528)\n",
      "Epoch: 99 | Batch_idx: 310 |  Loss_1: (0.0548) | Acc_1: (98.17%) (39079/39808)\n",
      "Epoch: 99 | Batch_idx: 320 |  Loss_1: (0.0544) | Acc_1: (98.18%) (40339/41088)\n",
      "Epoch: 99 | Batch_idx: 330 |  Loss_1: (0.0540) | Acc_1: (98.19%) (41600/42368)\n",
      "Epoch: 99 | Batch_idx: 340 |  Loss_1: (0.0538) | Acc_1: (98.20%) (42862/43648)\n",
      "Epoch: 99 | Batch_idx: 350 |  Loss_1: (0.0538) | Acc_1: (98.19%) (44117/44928)\n",
      "Epoch: 99 | Batch_idx: 360 |  Loss_1: (0.0532) | Acc_1: (98.21%) (45381/46208)\n",
      "Epoch: 99 | Batch_idx: 370 |  Loss_1: (0.0525) | Acc_1: (98.24%) (46651/47488)\n",
      "Epoch: 99 | Batch_idx: 380 |  Loss_1: (0.0522) | Acc_1: (98.26%) (47919/48768)\n",
      "Epoch: 99 | Batch_idx: 390 |  Loss_1: (0.0522) | Acc_1: (98.25%) (49127/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4450) | Acc: (90.68%) (9068/10000)\n",
      "Epoch: 100 | Batch_idx: 0 |  Loss_1: (0.0175) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 100 | Batch_idx: 10 |  Loss_1: (0.0322) | Acc_1: (98.65%) (1389/1408)\n",
      "Epoch: 100 | Batch_idx: 20 |  Loss_1: (0.0408) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 100 | Batch_idx: 30 |  Loss_1: (0.0467) | Acc_1: (98.24%) (3898/3968)\n",
      "Epoch: 100 | Batch_idx: 40 |  Loss_1: (0.0463) | Acc_1: (98.25%) (5156/5248)\n",
      "Epoch: 100 | Batch_idx: 50 |  Loss_1: (0.0469) | Acc_1: (98.35%) (6420/6528)\n",
      "Epoch: 100 | Batch_idx: 60 |  Loss_1: (0.0473) | Acc_1: (98.37%) (7681/7808)\n",
      "Epoch: 100 | Batch_idx: 70 |  Loss_1: (0.0459) | Acc_1: (98.38%) (8941/9088)\n",
      "Epoch: 100 | Batch_idx: 80 |  Loss_1: (0.0459) | Acc_1: (98.41%) (10203/10368)\n",
      "Epoch: 100 | Batch_idx: 90 |  Loss_1: (0.0454) | Acc_1: (98.42%) (11464/11648)\n",
      "Epoch: 100 | Batch_idx: 100 |  Loss_1: (0.0460) | Acc_1: (98.38%) (12718/12928)\n",
      "Epoch: 100 | Batch_idx: 110 |  Loss_1: (0.0460) | Acc_1: (98.40%) (13981/14208)\n",
      "Epoch: 100 | Batch_idx: 120 |  Loss_1: (0.0461) | Acc_1: (98.41%) (15241/15488)\n",
      "Epoch: 100 | Batch_idx: 130 |  Loss_1: (0.0475) | Acc_1: (98.37%) (16494/16768)\n",
      "Epoch: 100 | Batch_idx: 140 |  Loss_1: (0.0480) | Acc_1: (98.37%) (17753/18048)\n",
      "Epoch: 100 | Batch_idx: 150 |  Loss_1: (0.0477) | Acc_1: (98.37%) (19012/19328)\n",
      "Epoch: 100 | Batch_idx: 160 |  Loss_1: (0.0475) | Acc_1: (98.36%) (20271/20608)\n",
      "Epoch: 100 | Batch_idx: 170 |  Loss_1: (0.0471) | Acc_1: (98.40%) (21538/21888)\n",
      "Epoch: 100 | Batch_idx: 180 |  Loss_1: (0.0479) | Acc_1: (98.39%) (22795/23168)\n",
      "Epoch: 100 | Batch_idx: 190 |  Loss_1: (0.0481) | Acc_1: (98.38%) (24053/24448)\n",
      "Epoch: 100 | Batch_idx: 200 |  Loss_1: (0.0478) | Acc_1: (98.40%) (25316/25728)\n",
      "Epoch: 100 | Batch_idx: 210 |  Loss_1: (0.0477) | Acc_1: (98.39%) (26573/27008)\n",
      "Epoch: 100 | Batch_idx: 220 |  Loss_1: (0.0479) | Acc_1: (98.36%) (27825/28288)\n",
      "Epoch: 100 | Batch_idx: 230 |  Loss_1: (0.0481) | Acc_1: (98.34%) (29076/29568)\n",
      "Epoch: 100 | Batch_idx: 240 |  Loss_1: (0.0487) | Acc_1: (98.31%) (30326/30848)\n",
      "Epoch: 100 | Batch_idx: 250 |  Loss_1: (0.0490) | Acc_1: (98.29%) (31579/32128)\n",
      "Epoch: 100 | Batch_idx: 260 |  Loss_1: (0.0491) | Acc_1: (98.29%) (32838/33408)\n",
      "Epoch: 100 | Batch_idx: 270 |  Loss_1: (0.0495) | Acc_1: (98.28%) (34093/34688)\n",
      "Epoch: 100 | Batch_idx: 280 |  Loss_1: (0.0498) | Acc_1: (98.27%) (35346/35968)\n",
      "Epoch: 100 | Batch_idx: 290 |  Loss_1: (0.0496) | Acc_1: (98.29%) (36610/37248)\n",
      "Epoch: 100 | Batch_idx: 300 |  Loss_1: (0.0496) | Acc_1: (98.28%) (37867/38528)\n",
      "Epoch: 100 | Batch_idx: 310 |  Loss_1: (0.0496) | Acc_1: (98.29%) (39126/39808)\n",
      "Epoch: 100 | Batch_idx: 320 |  Loss_1: (0.0492) | Acc_1: (98.30%) (40391/41088)\n",
      "Epoch: 100 | Batch_idx: 330 |  Loss_1: (0.0492) | Acc_1: (98.32%) (41655/42368)\n",
      "Epoch: 100 | Batch_idx: 340 |  Loss_1: (0.0496) | Acc_1: (98.30%) (42907/43648)\n",
      "Epoch: 100 | Batch_idx: 350 |  Loss_1: (0.0500) | Acc_1: (98.29%) (44159/44928)\n",
      "Epoch: 100 | Batch_idx: 360 |  Loss_1: (0.0503) | Acc_1: (98.28%) (45413/46208)\n",
      "Epoch: 100 | Batch_idx: 370 |  Loss_1: (0.0504) | Acc_1: (98.28%) (46673/47488)\n",
      "Epoch: 100 | Batch_idx: 380 |  Loss_1: (0.0504) | Acc_1: (98.28%) (47929/48768)\n",
      "Epoch: 100 | Batch_idx: 390 |  Loss_1: (0.0506) | Acc_1: (98.27%) (49135/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4923) | Acc: (90.69%) (9069/10000)\n",
      "Epoch: 101 | Batch_idx: 0 |  Loss_1: (0.0452) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 101 | Batch_idx: 10 |  Loss_1: (0.0607) | Acc_1: (97.94%) (1379/1408)\n",
      "Epoch: 101 | Batch_idx: 20 |  Loss_1: (0.0501) | Acc_1: (98.36%) (2644/2688)\n",
      "Epoch: 101 | Batch_idx: 30 |  Loss_1: (0.0537) | Acc_1: (98.31%) (3901/3968)\n",
      "Epoch: 101 | Batch_idx: 40 |  Loss_1: (0.0526) | Acc_1: (98.34%) (5161/5248)\n",
      "Epoch: 101 | Batch_idx: 50 |  Loss_1: (0.0573) | Acc_1: (98.21%) (6411/6528)\n",
      "Epoch: 101 | Batch_idx: 60 |  Loss_1: (0.0571) | Acc_1: (98.17%) (7665/7808)\n",
      "Epoch: 101 | Batch_idx: 70 |  Loss_1: (0.0549) | Acc_1: (98.25%) (8929/9088)\n",
      "Epoch: 101 | Batch_idx: 80 |  Loss_1: (0.0560) | Acc_1: (98.18%) (10179/10368)\n",
      "Epoch: 101 | Batch_idx: 90 |  Loss_1: (0.0555) | Acc_1: (98.14%) (11431/11648)\n",
      "Epoch: 101 | Batch_idx: 100 |  Loss_1: (0.0557) | Acc_1: (98.10%) (12683/12928)\n",
      "Epoch: 101 | Batch_idx: 110 |  Loss_1: (0.0557) | Acc_1: (98.08%) (13935/14208)\n",
      "Epoch: 101 | Batch_idx: 120 |  Loss_1: (0.0565) | Acc_1: (98.04%) (15185/15488)\n",
      "Epoch: 101 | Batch_idx: 130 |  Loss_1: (0.0567) | Acc_1: (98.04%) (16439/16768)\n",
      "Epoch: 101 | Batch_idx: 140 |  Loss_1: (0.0560) | Acc_1: (98.06%) (17698/18048)\n",
      "Epoch: 101 | Batch_idx: 150 |  Loss_1: (0.0560) | Acc_1: (98.05%) (18951/19328)\n",
      "Epoch: 101 | Batch_idx: 160 |  Loss_1: (0.0546) | Acc_1: (98.09%) (20215/20608)\n",
      "Epoch: 101 | Batch_idx: 170 |  Loss_1: (0.0556) | Acc_1: (98.07%) (21466/21888)\n",
      "Epoch: 101 | Batch_idx: 180 |  Loss_1: (0.0556) | Acc_1: (98.06%) (22718/23168)\n",
      "Epoch: 101 | Batch_idx: 190 |  Loss_1: (0.0555) | Acc_1: (98.06%) (23974/24448)\n",
      "Epoch: 101 | Batch_idx: 200 |  Loss_1: (0.0549) | Acc_1: (98.10%) (25239/25728)\n",
      "Epoch: 101 | Batch_idx: 210 |  Loss_1: (0.0546) | Acc_1: (98.12%) (26500/27008)\n",
      "Epoch: 101 | Batch_idx: 220 |  Loss_1: (0.0547) | Acc_1: (98.11%) (27753/28288)\n",
      "Epoch: 101 | Batch_idx: 230 |  Loss_1: (0.0544) | Acc_1: (98.12%) (29012/29568)\n",
      "Epoch: 101 | Batch_idx: 240 |  Loss_1: (0.0539) | Acc_1: (98.13%) (30270/30848)\n",
      "Epoch: 101 | Batch_idx: 250 |  Loss_1: (0.0540) | Acc_1: (98.13%) (31526/32128)\n",
      "Epoch: 101 | Batch_idx: 260 |  Loss_1: (0.0547) | Acc_1: (98.12%) (32780/33408)\n",
      "Epoch: 101 | Batch_idx: 270 |  Loss_1: (0.0548) | Acc_1: (98.12%) (34035/34688)\n",
      "Epoch: 101 | Batch_idx: 280 |  Loss_1: (0.0550) | Acc_1: (98.13%) (35294/35968)\n",
      "Epoch: 101 | Batch_idx: 290 |  Loss_1: (0.0549) | Acc_1: (98.13%) (36550/37248)\n",
      "Epoch: 101 | Batch_idx: 300 |  Loss_1: (0.0547) | Acc_1: (98.14%) (37810/38528)\n",
      "Epoch: 101 | Batch_idx: 310 |  Loss_1: (0.0547) | Acc_1: (98.14%) (39068/39808)\n",
      "Epoch: 101 | Batch_idx: 320 |  Loss_1: (0.0548) | Acc_1: (98.13%) (40321/41088)\n",
      "Epoch: 101 | Batch_idx: 330 |  Loss_1: (0.0551) | Acc_1: (98.13%) (41575/42368)\n",
      "Epoch: 101 | Batch_idx: 340 |  Loss_1: (0.0548) | Acc_1: (98.13%) (42832/43648)\n",
      "Epoch: 101 | Batch_idx: 350 |  Loss_1: (0.0546) | Acc_1: (98.13%) (44090/44928)\n",
      "Epoch: 101 | Batch_idx: 360 |  Loss_1: (0.0540) | Acc_1: (98.15%) (45353/46208)\n",
      "Epoch: 101 | Batch_idx: 370 |  Loss_1: (0.0541) | Acc_1: (98.15%) (46608/47488)\n",
      "Epoch: 101 | Batch_idx: 380 |  Loss_1: (0.0540) | Acc_1: (98.15%) (47866/48768)\n",
      "Epoch: 101 | Batch_idx: 390 |  Loss_1: (0.0541) | Acc_1: (98.16%) (49080/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4652) | Acc: (90.63%) (9063/10000)\n",
      "Epoch: 102 | Batch_idx: 0 |  Loss_1: (0.0133) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 102 | Batch_idx: 10 |  Loss_1: (0.0513) | Acc_1: (98.37%) (1385/1408)\n",
      "Epoch: 102 | Batch_idx: 20 |  Loss_1: (0.0485) | Acc_1: (98.36%) (2644/2688)\n",
      "Epoch: 102 | Batch_idx: 30 |  Loss_1: (0.0509) | Acc_1: (98.26%) (3899/3968)\n",
      "Epoch: 102 | Batch_idx: 40 |  Loss_1: (0.0527) | Acc_1: (98.19%) (5153/5248)\n",
      "Epoch: 102 | Batch_idx: 50 |  Loss_1: (0.0527) | Acc_1: (98.27%) (6415/6528)\n",
      "Epoch: 102 | Batch_idx: 60 |  Loss_1: (0.0532) | Acc_1: (98.22%) (7669/7808)\n",
      "Epoch: 102 | Batch_idx: 70 |  Loss_1: (0.0520) | Acc_1: (98.28%) (8932/9088)\n",
      "Epoch: 102 | Batch_idx: 80 |  Loss_1: (0.0531) | Acc_1: (98.23%) (10184/10368)\n",
      "Epoch: 102 | Batch_idx: 90 |  Loss_1: (0.0533) | Acc_1: (98.22%) (11441/11648)\n",
      "Epoch: 102 | Batch_idx: 100 |  Loss_1: (0.0515) | Acc_1: (98.28%) (12706/12928)\n",
      "Epoch: 102 | Batch_idx: 110 |  Loss_1: (0.0496) | Acc_1: (98.34%) (13972/14208)\n",
      "Epoch: 102 | Batch_idx: 120 |  Loss_1: (0.0493) | Acc_1: (98.35%) (15232/15488)\n",
      "Epoch: 102 | Batch_idx: 130 |  Loss_1: (0.0492) | Acc_1: (98.33%) (16488/16768)\n",
      "Epoch: 102 | Batch_idx: 140 |  Loss_1: (0.0484) | Acc_1: (98.35%) (17750/18048)\n",
      "Epoch: 102 | Batch_idx: 150 |  Loss_1: (0.0475) | Acc_1: (98.38%) (19014/19328)\n",
      "Epoch: 102 | Batch_idx: 160 |  Loss_1: (0.0485) | Acc_1: (98.34%) (20265/20608)\n",
      "Epoch: 102 | Batch_idx: 170 |  Loss_1: (0.0478) | Acc_1: (98.36%) (21529/21888)\n",
      "Epoch: 102 | Batch_idx: 180 |  Loss_1: (0.0480) | Acc_1: (98.35%) (22785/23168)\n",
      "Epoch: 102 | Batch_idx: 190 |  Loss_1: (0.0478) | Acc_1: (98.35%) (24044/24448)\n",
      "Epoch: 102 | Batch_idx: 200 |  Loss_1: (0.0477) | Acc_1: (98.34%) (25300/25728)\n",
      "Epoch: 102 | Batch_idx: 210 |  Loss_1: (0.0484) | Acc_1: (98.33%) (26557/27008)\n",
      "Epoch: 102 | Batch_idx: 220 |  Loss_1: (0.0482) | Acc_1: (98.32%) (27814/28288)\n",
      "Epoch: 102 | Batch_idx: 230 |  Loss_1: (0.0485) | Acc_1: (98.32%) (29071/29568)\n",
      "Epoch: 102 | Batch_idx: 240 |  Loss_1: (0.0482) | Acc_1: (98.33%) (30333/30848)\n",
      "Epoch: 102 | Batch_idx: 250 |  Loss_1: (0.0481) | Acc_1: (98.33%) (31593/32128)\n",
      "Epoch: 102 | Batch_idx: 260 |  Loss_1: (0.0481) | Acc_1: (98.34%) (32854/33408)\n",
      "Epoch: 102 | Batch_idx: 270 |  Loss_1: (0.0482) | Acc_1: (98.33%) (34109/34688)\n",
      "Epoch: 102 | Batch_idx: 280 |  Loss_1: (0.0481) | Acc_1: (98.32%) (35364/35968)\n",
      "Epoch: 102 | Batch_idx: 290 |  Loss_1: (0.0487) | Acc_1: (98.30%) (36615/37248)\n",
      "Epoch: 102 | Batch_idx: 300 |  Loss_1: (0.0492) | Acc_1: (98.29%) (37869/38528)\n",
      "Epoch: 102 | Batch_idx: 310 |  Loss_1: (0.0497) | Acc_1: (98.28%) (39123/39808)\n",
      "Epoch: 102 | Batch_idx: 320 |  Loss_1: (0.0495) | Acc_1: (98.28%) (40382/41088)\n",
      "Epoch: 102 | Batch_idx: 330 |  Loss_1: (0.0498) | Acc_1: (98.28%) (41638/42368)\n",
      "Epoch: 102 | Batch_idx: 340 |  Loss_1: (0.0492) | Acc_1: (98.30%) (42905/43648)\n",
      "Epoch: 102 | Batch_idx: 350 |  Loss_1: (0.0491) | Acc_1: (98.29%) (44160/44928)\n",
      "Epoch: 102 | Batch_idx: 360 |  Loss_1: (0.0491) | Acc_1: (98.29%) (45420/46208)\n",
      "Epoch: 102 | Batch_idx: 370 |  Loss_1: (0.0491) | Acc_1: (98.29%) (46677/47488)\n",
      "Epoch: 102 | Batch_idx: 380 |  Loss_1: (0.0491) | Acc_1: (98.28%) (47931/48768)\n",
      "Epoch: 102 | Batch_idx: 390 |  Loss_1: (0.0493) | Acc_1: (98.27%) (49134/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4343) | Acc: (90.96%) (9096/10000)\n",
      "Epoch: 103 | Batch_idx: 0 |  Loss_1: (0.0680) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 103 | Batch_idx: 10 |  Loss_1: (0.0465) | Acc_1: (98.44%) (1386/1408)\n",
      "Epoch: 103 | Batch_idx: 20 |  Loss_1: (0.0393) | Acc_1: (98.74%) (2654/2688)\n",
      "Epoch: 103 | Batch_idx: 30 |  Loss_1: (0.0439) | Acc_1: (98.51%) (3909/3968)\n",
      "Epoch: 103 | Batch_idx: 40 |  Loss_1: (0.0412) | Acc_1: (98.55%) (5172/5248)\n",
      "Epoch: 103 | Batch_idx: 50 |  Loss_1: (0.0401) | Acc_1: (98.58%) (6435/6528)\n",
      "Epoch: 103 | Batch_idx: 60 |  Loss_1: (0.0402) | Acc_1: (98.58%) (7697/7808)\n",
      "Epoch: 103 | Batch_idx: 70 |  Loss_1: (0.0394) | Acc_1: (98.59%) (8960/9088)\n",
      "Epoch: 103 | Batch_idx: 80 |  Loss_1: (0.0423) | Acc_1: (98.51%) (10214/10368)\n",
      "Epoch: 103 | Batch_idx: 90 |  Loss_1: (0.0447) | Acc_1: (98.43%) (11465/11648)\n",
      "Epoch: 103 | Batch_idx: 100 |  Loss_1: (0.0461) | Acc_1: (98.43%) (12725/12928)\n",
      "Epoch: 103 | Batch_idx: 110 |  Loss_1: (0.0472) | Acc_1: (98.37%) (13977/14208)\n",
      "Epoch: 103 | Batch_idx: 120 |  Loss_1: (0.0462) | Acc_1: (98.41%) (15242/15488)\n",
      "Epoch: 103 | Batch_idx: 130 |  Loss_1: (0.0463) | Acc_1: (98.41%) (16502/16768)\n",
      "Epoch: 103 | Batch_idx: 140 |  Loss_1: (0.0472) | Acc_1: (98.39%) (17757/18048)\n",
      "Epoch: 103 | Batch_idx: 150 |  Loss_1: (0.0464) | Acc_1: (98.41%) (19020/19328)\n",
      "Epoch: 103 | Batch_idx: 160 |  Loss_1: (0.0461) | Acc_1: (98.41%) (20281/20608)\n",
      "Epoch: 103 | Batch_idx: 170 |  Loss_1: (0.0467) | Acc_1: (98.42%) (21542/21888)\n",
      "Epoch: 103 | Batch_idx: 180 |  Loss_1: (0.0462) | Acc_1: (98.41%) (22800/23168)\n",
      "Epoch: 103 | Batch_idx: 190 |  Loss_1: (0.0469) | Acc_1: (98.38%) (24052/24448)\n",
      "Epoch: 103 | Batch_idx: 200 |  Loss_1: (0.0471) | Acc_1: (98.38%) (25311/25728)\n",
      "Epoch: 103 | Batch_idx: 210 |  Loss_1: (0.0475) | Acc_1: (98.37%) (26567/27008)\n",
      "Epoch: 103 | Batch_idx: 220 |  Loss_1: (0.0476) | Acc_1: (98.37%) (27828/28288)\n",
      "Epoch: 103 | Batch_idx: 230 |  Loss_1: (0.0470) | Acc_1: (98.39%) (29091/29568)\n",
      "Epoch: 103 | Batch_idx: 240 |  Loss_1: (0.0470) | Acc_1: (98.39%) (30351/30848)\n",
      "Epoch: 103 | Batch_idx: 250 |  Loss_1: (0.0474) | Acc_1: (98.38%) (31609/32128)\n",
      "Epoch: 103 | Batch_idx: 260 |  Loss_1: (0.0477) | Acc_1: (98.39%) (32869/33408)\n",
      "Epoch: 103 | Batch_idx: 270 |  Loss_1: (0.0478) | Acc_1: (98.38%) (34127/34688)\n",
      "Epoch: 103 | Batch_idx: 280 |  Loss_1: (0.0473) | Acc_1: (98.41%) (35395/35968)\n",
      "Epoch: 103 | Batch_idx: 290 |  Loss_1: (0.0470) | Acc_1: (98.42%) (36658/37248)\n",
      "Epoch: 103 | Batch_idx: 300 |  Loss_1: (0.0466) | Acc_1: (98.43%) (37924/38528)\n",
      "Epoch: 103 | Batch_idx: 310 |  Loss_1: (0.0465) | Acc_1: (98.44%) (39188/39808)\n",
      "Epoch: 103 | Batch_idx: 320 |  Loss_1: (0.0461) | Acc_1: (98.46%) (40454/41088)\n",
      "Epoch: 103 | Batch_idx: 330 |  Loss_1: (0.0458) | Acc_1: (98.47%) (41720/42368)\n",
      "Epoch: 103 | Batch_idx: 340 |  Loss_1: (0.0455) | Acc_1: (98.47%) (42979/43648)\n",
      "Epoch: 103 | Batch_idx: 350 |  Loss_1: (0.0459) | Acc_1: (98.46%) (44235/44928)\n",
      "Epoch: 103 | Batch_idx: 360 |  Loss_1: (0.0458) | Acc_1: (98.46%) (45495/46208)\n",
      "Epoch: 103 | Batch_idx: 370 |  Loss_1: (0.0459) | Acc_1: (98.45%) (46752/47488)\n",
      "Epoch: 103 | Batch_idx: 380 |  Loss_1: (0.0455) | Acc_1: (98.46%) (48015/48768)\n",
      "Epoch: 103 | Batch_idx: 390 |  Loss_1: (0.0456) | Acc_1: (98.46%) (49230/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4543) | Acc: (90.87%) (9087/10000)\n",
      "Epoch: 104 | Batch_idx: 0 |  Loss_1: (0.0188) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 104 | Batch_idx: 10 |  Loss_1: (0.0408) | Acc_1: (98.01%) (1380/1408)\n",
      "Epoch: 104 | Batch_idx: 20 |  Loss_1: (0.0483) | Acc_1: (98.10%) (2637/2688)\n",
      "Epoch: 104 | Batch_idx: 30 |  Loss_1: (0.0435) | Acc_1: (98.41%) (3905/3968)\n",
      "Epoch: 104 | Batch_idx: 40 |  Loss_1: (0.0428) | Acc_1: (98.46%) (5167/5248)\n",
      "Epoch: 104 | Batch_idx: 50 |  Loss_1: (0.0418) | Acc_1: (98.51%) (6431/6528)\n",
      "Epoch: 104 | Batch_idx: 60 |  Loss_1: (0.0419) | Acc_1: (98.50%) (7691/7808)\n",
      "Epoch: 104 | Batch_idx: 70 |  Loss_1: (0.0434) | Acc_1: (98.43%) (8945/9088)\n",
      "Epoch: 104 | Batch_idx: 80 |  Loss_1: (0.0421) | Acc_1: (98.48%) (10210/10368)\n",
      "Epoch: 104 | Batch_idx: 90 |  Loss_1: (0.0428) | Acc_1: (98.50%) (11473/11648)\n",
      "Epoch: 104 | Batch_idx: 100 |  Loss_1: (0.0435) | Acc_1: (98.50%) (12734/12928)\n",
      "Epoch: 104 | Batch_idx: 110 |  Loss_1: (0.0432) | Acc_1: (98.51%) (13996/14208)\n",
      "Epoch: 104 | Batch_idx: 120 |  Loss_1: (0.0431) | Acc_1: (98.53%) (15260/15488)\n",
      "Epoch: 104 | Batch_idx: 130 |  Loss_1: (0.0429) | Acc_1: (98.52%) (16519/16768)\n",
      "Epoch: 104 | Batch_idx: 140 |  Loss_1: (0.0435) | Acc_1: (98.50%) (17777/18048)\n",
      "Epoch: 104 | Batch_idx: 150 |  Loss_1: (0.0425) | Acc_1: (98.53%) (19044/19328)\n",
      "Epoch: 104 | Batch_idx: 160 |  Loss_1: (0.0423) | Acc_1: (98.55%) (20309/20608)\n",
      "Epoch: 104 | Batch_idx: 170 |  Loss_1: (0.0426) | Acc_1: (98.54%) (21569/21888)\n",
      "Epoch: 104 | Batch_idx: 180 |  Loss_1: (0.0433) | Acc_1: (98.52%) (22824/23168)\n",
      "Epoch: 104 | Batch_idx: 190 |  Loss_1: (0.0431) | Acc_1: (98.52%) (24085/24448)\n",
      "Epoch: 104 | Batch_idx: 200 |  Loss_1: (0.0433) | Acc_1: (98.51%) (25344/25728)\n",
      "Epoch: 104 | Batch_idx: 210 |  Loss_1: (0.0440) | Acc_1: (98.47%) (26596/27008)\n",
      "Epoch: 104 | Batch_idx: 220 |  Loss_1: (0.0449) | Acc_1: (98.47%) (27855/28288)\n",
      "Epoch: 104 | Batch_idx: 230 |  Loss_1: (0.0445) | Acc_1: (98.47%) (29116/29568)\n",
      "Epoch: 104 | Batch_idx: 240 |  Loss_1: (0.0447) | Acc_1: (98.47%) (30375/30848)\n",
      "Epoch: 104 | Batch_idx: 250 |  Loss_1: (0.0448) | Acc_1: (98.46%) (31634/32128)\n",
      "Epoch: 104 | Batch_idx: 260 |  Loss_1: (0.0452) | Acc_1: (98.46%) (32892/33408)\n",
      "Epoch: 104 | Batch_idx: 270 |  Loss_1: (0.0447) | Acc_1: (98.46%) (34155/34688)\n",
      "Epoch: 104 | Batch_idx: 280 |  Loss_1: (0.0451) | Acc_1: (98.45%) (35412/35968)\n",
      "Epoch: 104 | Batch_idx: 290 |  Loss_1: (0.0460) | Acc_1: (98.43%) (36664/37248)\n",
      "Epoch: 104 | Batch_idx: 300 |  Loss_1: (0.0462) | Acc_1: (98.42%) (37921/38528)\n",
      "Epoch: 104 | Batch_idx: 310 |  Loss_1: (0.0461) | Acc_1: (98.42%) (39180/39808)\n",
      "Epoch: 104 | Batch_idx: 320 |  Loss_1: (0.0464) | Acc_1: (98.41%) (40435/41088)\n",
      "Epoch: 104 | Batch_idx: 330 |  Loss_1: (0.0466) | Acc_1: (98.41%) (41694/42368)\n",
      "Epoch: 104 | Batch_idx: 340 |  Loss_1: (0.0469) | Acc_1: (98.40%) (42951/43648)\n",
      "Epoch: 104 | Batch_idx: 350 |  Loss_1: (0.0473) | Acc_1: (98.38%) (44202/44928)\n",
      "Epoch: 104 | Batch_idx: 360 |  Loss_1: (0.0475) | Acc_1: (98.38%) (45458/46208)\n",
      "Epoch: 104 | Batch_idx: 370 |  Loss_1: (0.0476) | Acc_1: (98.37%) (46713/47488)\n",
      "Epoch: 104 | Batch_idx: 380 |  Loss_1: (0.0475) | Acc_1: (98.37%) (47971/48768)\n",
      "Epoch: 104 | Batch_idx: 390 |  Loss_1: (0.0483) | Acc_1: (98.33%) (49163/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4893) | Acc: (90.37%) (9037/10000)\n",
      "Epoch: 105 | Batch_idx: 0 |  Loss_1: (0.0112) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 105 | Batch_idx: 10 |  Loss_1: (0.0441) | Acc_1: (98.30%) (1384/1408)\n",
      "Epoch: 105 | Batch_idx: 20 |  Loss_1: (0.0451) | Acc_1: (98.25%) (2641/2688)\n",
      "Epoch: 105 | Batch_idx: 30 |  Loss_1: (0.0443) | Acc_1: (98.26%) (3899/3968)\n",
      "Epoch: 105 | Batch_idx: 40 |  Loss_1: (0.0439) | Acc_1: (98.29%) (5158/5248)\n",
      "Epoch: 105 | Batch_idx: 50 |  Loss_1: (0.0444) | Acc_1: (98.24%) (6413/6528)\n",
      "Epoch: 105 | Batch_idx: 60 |  Loss_1: (0.0469) | Acc_1: (98.17%) (7665/7808)\n",
      "Epoch: 105 | Batch_idx: 70 |  Loss_1: (0.0479) | Acc_1: (98.10%) (8915/9088)\n",
      "Epoch: 105 | Batch_idx: 80 |  Loss_1: (0.0492) | Acc_1: (98.08%) (10169/10368)\n",
      "Epoch: 105 | Batch_idx: 90 |  Loss_1: (0.0488) | Acc_1: (98.11%) (11428/11648)\n",
      "Epoch: 105 | Batch_idx: 100 |  Loss_1: (0.0495) | Acc_1: (98.13%) (12686/12928)\n",
      "Epoch: 105 | Batch_idx: 110 |  Loss_1: (0.0501) | Acc_1: (98.16%) (13947/14208)\n",
      "Epoch: 105 | Batch_idx: 120 |  Loss_1: (0.0508) | Acc_1: (98.17%) (15205/15488)\n",
      "Epoch: 105 | Batch_idx: 130 |  Loss_1: (0.0506) | Acc_1: (98.17%) (16461/16768)\n",
      "Epoch: 105 | Batch_idx: 140 |  Loss_1: (0.0510) | Acc_1: (98.15%) (17714/18048)\n",
      "Epoch: 105 | Batch_idx: 150 |  Loss_1: (0.0510) | Acc_1: (98.15%) (18971/19328)\n",
      "Epoch: 105 | Batch_idx: 160 |  Loss_1: (0.0503) | Acc_1: (98.18%) (20232/20608)\n",
      "Epoch: 105 | Batch_idx: 170 |  Loss_1: (0.0502) | Acc_1: (98.19%) (21492/21888)\n",
      "Epoch: 105 | Batch_idx: 180 |  Loss_1: (0.0501) | Acc_1: (98.19%) (22748/23168)\n",
      "Epoch: 105 | Batch_idx: 190 |  Loss_1: (0.0497) | Acc_1: (98.20%) (24007/24448)\n",
      "Epoch: 105 | Batch_idx: 200 |  Loss_1: (0.0494) | Acc_1: (98.24%) (25275/25728)\n",
      "Epoch: 105 | Batch_idx: 210 |  Loss_1: (0.0496) | Acc_1: (98.25%) (26535/27008)\n",
      "Epoch: 105 | Batch_idx: 220 |  Loss_1: (0.0499) | Acc_1: (98.25%) (27792/28288)\n",
      "Epoch: 105 | Batch_idx: 230 |  Loss_1: (0.0496) | Acc_1: (98.27%) (29056/29568)\n",
      "Epoch: 105 | Batch_idx: 240 |  Loss_1: (0.0489) | Acc_1: (98.29%) (30322/30848)\n",
      "Epoch: 105 | Batch_idx: 250 |  Loss_1: (0.0489) | Acc_1: (98.29%) (31579/32128)\n",
      "Epoch: 105 | Batch_idx: 260 |  Loss_1: (0.0491) | Acc_1: (98.28%) (32834/33408)\n",
      "Epoch: 105 | Batch_idx: 270 |  Loss_1: (0.0489) | Acc_1: (98.28%) (34093/34688)\n",
      "Epoch: 105 | Batch_idx: 280 |  Loss_1: (0.0487) | Acc_1: (98.30%) (35355/35968)\n",
      "Epoch: 105 | Batch_idx: 290 |  Loss_1: (0.0491) | Acc_1: (98.29%) (36611/37248)\n",
      "Epoch: 105 | Batch_idx: 300 |  Loss_1: (0.0488) | Acc_1: (98.29%) (37871/38528)\n",
      "Epoch: 105 | Batch_idx: 310 |  Loss_1: (0.0493) | Acc_1: (98.27%) (39121/39808)\n",
      "Epoch: 105 | Batch_idx: 320 |  Loss_1: (0.0493) | Acc_1: (98.28%) (40383/41088)\n",
      "Epoch: 105 | Batch_idx: 330 |  Loss_1: (0.0493) | Acc_1: (98.29%) (41644/42368)\n",
      "Epoch: 105 | Batch_idx: 340 |  Loss_1: (0.0493) | Acc_1: (98.29%) (42903/43648)\n",
      "Epoch: 105 | Batch_idx: 350 |  Loss_1: (0.0490) | Acc_1: (98.30%) (44166/44928)\n",
      "Epoch: 105 | Batch_idx: 360 |  Loss_1: (0.0489) | Acc_1: (98.30%) (45422/46208)\n",
      "Epoch: 105 | Batch_idx: 370 |  Loss_1: (0.0492) | Acc_1: (98.30%) (46680/47488)\n",
      "Epoch: 105 | Batch_idx: 380 |  Loss_1: (0.0492) | Acc_1: (98.30%) (47940/48768)\n",
      "Epoch: 105 | Batch_idx: 390 |  Loss_1: (0.0492) | Acc_1: (98.29%) (49147/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4557) | Acc: (90.94%) (9094/10000)\n",
      "Epoch: 106 | Batch_idx: 0 |  Loss_1: (0.0724) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 106 | Batch_idx: 10 |  Loss_1: (0.0504) | Acc_1: (98.22%) (1383/1408)\n",
      "Epoch: 106 | Batch_idx: 20 |  Loss_1: (0.0533) | Acc_1: (98.03%) (2635/2688)\n",
      "Epoch: 106 | Batch_idx: 30 |  Loss_1: (0.0530) | Acc_1: (98.19%) (3896/3968)\n",
      "Epoch: 106 | Batch_idx: 40 |  Loss_1: (0.0476) | Acc_1: (98.34%) (5161/5248)\n",
      "Epoch: 106 | Batch_idx: 50 |  Loss_1: (0.0455) | Acc_1: (98.41%) (6424/6528)\n",
      "Epoch: 106 | Batch_idx: 60 |  Loss_1: (0.0435) | Acc_1: (98.51%) (7692/7808)\n",
      "Epoch: 106 | Batch_idx: 70 |  Loss_1: (0.0440) | Acc_1: (98.48%) (8950/9088)\n",
      "Epoch: 106 | Batch_idx: 80 |  Loss_1: (0.0438) | Acc_1: (98.48%) (10210/10368)\n",
      "Epoch: 106 | Batch_idx: 90 |  Loss_1: (0.0449) | Acc_1: (98.41%) (11463/11648)\n",
      "Epoch: 106 | Batch_idx: 100 |  Loss_1: (0.0449) | Acc_1: (98.41%) (12723/12928)\n",
      "Epoch: 106 | Batch_idx: 110 |  Loss_1: (0.0440) | Acc_1: (98.47%) (13990/14208)\n",
      "Epoch: 106 | Batch_idx: 120 |  Loss_1: (0.0429) | Acc_1: (98.50%) (15256/15488)\n",
      "Epoch: 106 | Batch_idx: 130 |  Loss_1: (0.0440) | Acc_1: (98.48%) (16513/16768)\n",
      "Epoch: 106 | Batch_idx: 140 |  Loss_1: (0.0437) | Acc_1: (98.49%) (17775/18048)\n",
      "Epoch: 106 | Batch_idx: 150 |  Loss_1: (0.0436) | Acc_1: (98.49%) (19036/19328)\n",
      "Epoch: 106 | Batch_idx: 160 |  Loss_1: (0.0435) | Acc_1: (98.50%) (20299/20608)\n",
      "Epoch: 106 | Batch_idx: 170 |  Loss_1: (0.0444) | Acc_1: (98.46%) (21551/21888)\n",
      "Epoch: 106 | Batch_idx: 180 |  Loss_1: (0.0450) | Acc_1: (98.45%) (22810/23168)\n",
      "Epoch: 106 | Batch_idx: 190 |  Loss_1: (0.0452) | Acc_1: (98.45%) (24068/24448)\n",
      "Epoch: 106 | Batch_idx: 200 |  Loss_1: (0.0454) | Acc_1: (98.44%) (25327/25728)\n",
      "Epoch: 106 | Batch_idx: 210 |  Loss_1: (0.0454) | Acc_1: (98.44%) (26586/27008)\n",
      "Epoch: 106 | Batch_idx: 220 |  Loss_1: (0.0462) | Acc_1: (98.40%) (27836/28288)\n",
      "Epoch: 106 | Batch_idx: 230 |  Loss_1: (0.0467) | Acc_1: (98.37%) (29087/29568)\n",
      "Epoch: 106 | Batch_idx: 240 |  Loss_1: (0.0466) | Acc_1: (98.39%) (30351/30848)\n",
      "Epoch: 106 | Batch_idx: 250 |  Loss_1: (0.0468) | Acc_1: (98.39%) (31610/32128)\n",
      "Epoch: 106 | Batch_idx: 260 |  Loss_1: (0.0466) | Acc_1: (98.40%) (32872/33408)\n",
      "Epoch: 106 | Batch_idx: 270 |  Loss_1: (0.0463) | Acc_1: (98.41%) (34135/34688)\n",
      "Epoch: 106 | Batch_idx: 280 |  Loss_1: (0.0460) | Acc_1: (98.41%) (35396/35968)\n",
      "Epoch: 106 | Batch_idx: 290 |  Loss_1: (0.0462) | Acc_1: (98.40%) (36652/37248)\n",
      "Epoch: 106 | Batch_idx: 300 |  Loss_1: (0.0461) | Acc_1: (98.40%) (37910/38528)\n",
      "Epoch: 106 | Batch_idx: 310 |  Loss_1: (0.0459) | Acc_1: (98.40%) (39171/39808)\n",
      "Epoch: 106 | Batch_idx: 320 |  Loss_1: (0.0456) | Acc_1: (98.41%) (40435/41088)\n",
      "Epoch: 106 | Batch_idx: 330 |  Loss_1: (0.0456) | Acc_1: (98.41%) (41696/42368)\n",
      "Epoch: 106 | Batch_idx: 340 |  Loss_1: (0.0458) | Acc_1: (98.41%) (42955/43648)\n",
      "Epoch: 106 | Batch_idx: 350 |  Loss_1: (0.0460) | Acc_1: (98.42%) (44219/44928)\n",
      "Epoch: 106 | Batch_idx: 360 |  Loss_1: (0.0455) | Acc_1: (98.44%) (45486/46208)\n",
      "Epoch: 106 | Batch_idx: 370 |  Loss_1: (0.0452) | Acc_1: (98.45%) (46750/47488)\n",
      "Epoch: 106 | Batch_idx: 380 |  Loss_1: (0.0459) | Acc_1: (98.43%) (48001/48768)\n",
      "Epoch: 106 | Batch_idx: 390 |  Loss_1: (0.0460) | Acc_1: (98.43%) (49213/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4654) | Acc: (90.46%) (9046/10000)\n",
      "Epoch: 107 | Batch_idx: 0 |  Loss_1: (0.0324) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 107 | Batch_idx: 10 |  Loss_1: (0.0389) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 107 | Batch_idx: 20 |  Loss_1: (0.0404) | Acc_1: (98.55%) (2649/2688)\n",
      "Epoch: 107 | Batch_idx: 30 |  Loss_1: (0.0408) | Acc_1: (98.54%) (3910/3968)\n",
      "Epoch: 107 | Batch_idx: 40 |  Loss_1: (0.0429) | Acc_1: (98.48%) (5168/5248)\n",
      "Epoch: 107 | Batch_idx: 50 |  Loss_1: (0.0463) | Acc_1: (98.36%) (6421/6528)\n",
      "Epoch: 107 | Batch_idx: 60 |  Loss_1: (0.0434) | Acc_1: (98.51%) (7692/7808)\n",
      "Epoch: 107 | Batch_idx: 70 |  Loss_1: (0.0438) | Acc_1: (98.48%) (8950/9088)\n",
      "Epoch: 107 | Batch_idx: 80 |  Loss_1: (0.0438) | Acc_1: (98.48%) (10210/10368)\n",
      "Epoch: 107 | Batch_idx: 90 |  Loss_1: (0.0438) | Acc_1: (98.50%) (11473/11648)\n",
      "Epoch: 107 | Batch_idx: 100 |  Loss_1: (0.0434) | Acc_1: (98.51%) (12735/12928)\n",
      "Epoch: 107 | Batch_idx: 110 |  Loss_1: (0.0424) | Acc_1: (98.52%) (13998/14208)\n",
      "Epoch: 107 | Batch_idx: 120 |  Loss_1: (0.0423) | Acc_1: (98.53%) (15261/15488)\n",
      "Epoch: 107 | Batch_idx: 130 |  Loss_1: (0.0423) | Acc_1: (98.53%) (16521/16768)\n",
      "Epoch: 107 | Batch_idx: 140 |  Loss_1: (0.0421) | Acc_1: (98.54%) (17785/18048)\n",
      "Epoch: 107 | Batch_idx: 150 |  Loss_1: (0.0417) | Acc_1: (98.56%) (19049/19328)\n",
      "Epoch: 107 | Batch_idx: 160 |  Loss_1: (0.0409) | Acc_1: (98.57%) (20314/20608)\n",
      "Epoch: 107 | Batch_idx: 170 |  Loss_1: (0.0416) | Acc_1: (98.57%) (21574/21888)\n",
      "Epoch: 107 | Batch_idx: 180 |  Loss_1: (0.0415) | Acc_1: (98.57%) (22836/23168)\n",
      "Epoch: 107 | Batch_idx: 190 |  Loss_1: (0.0428) | Acc_1: (98.56%) (24095/24448)\n",
      "Epoch: 107 | Batch_idx: 200 |  Loss_1: (0.0429) | Acc_1: (98.55%) (25356/25728)\n",
      "Epoch: 107 | Batch_idx: 210 |  Loss_1: (0.0435) | Acc_1: (98.53%) (26611/27008)\n",
      "Epoch: 107 | Batch_idx: 220 |  Loss_1: (0.0437) | Acc_1: (98.50%) (27864/28288)\n",
      "Epoch: 107 | Batch_idx: 230 |  Loss_1: (0.0442) | Acc_1: (98.48%) (29118/29568)\n",
      "Epoch: 107 | Batch_idx: 240 |  Loss_1: (0.0436) | Acc_1: (98.50%) (30384/30848)\n",
      "Epoch: 107 | Batch_idx: 250 |  Loss_1: (0.0438) | Acc_1: (98.50%) (31646/32128)\n",
      "Epoch: 107 | Batch_idx: 260 |  Loss_1: (0.0443) | Acc_1: (98.49%) (32905/33408)\n",
      "Epoch: 107 | Batch_idx: 270 |  Loss_1: (0.0440) | Acc_1: (98.50%) (34166/34688)\n",
      "Epoch: 107 | Batch_idx: 280 |  Loss_1: (0.0441) | Acc_1: (98.50%) (35427/35968)\n",
      "Epoch: 107 | Batch_idx: 290 |  Loss_1: (0.0450) | Acc_1: (98.46%) (36673/37248)\n",
      "Epoch: 107 | Batch_idx: 300 |  Loss_1: (0.0457) | Acc_1: (98.43%) (37925/38528)\n",
      "Epoch: 107 | Batch_idx: 310 |  Loss_1: (0.0459) | Acc_1: (98.42%) (39180/39808)\n",
      "Epoch: 107 | Batch_idx: 320 |  Loss_1: (0.0461) | Acc_1: (98.41%) (40436/41088)\n",
      "Epoch: 107 | Batch_idx: 330 |  Loss_1: (0.0462) | Acc_1: (98.41%) (41693/42368)\n",
      "Epoch: 107 | Batch_idx: 340 |  Loss_1: (0.0462) | Acc_1: (98.40%) (42951/43648)\n",
      "Epoch: 107 | Batch_idx: 350 |  Loss_1: (0.0465) | Acc_1: (98.40%) (44207/44928)\n",
      "Epoch: 107 | Batch_idx: 360 |  Loss_1: (0.0470) | Acc_1: (98.38%) (45460/46208)\n",
      "Epoch: 107 | Batch_idx: 370 |  Loss_1: (0.0473) | Acc_1: (98.37%) (46716/47488)\n",
      "Epoch: 107 | Batch_idx: 380 |  Loss_1: (0.0474) | Acc_1: (98.38%) (47976/48768)\n",
      "Epoch: 107 | Batch_idx: 390 |  Loss_1: (0.0476) | Acc_1: (98.37%) (49187/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4552) | Acc: (89.97%) (8997/10000)\n",
      "Epoch: 108 | Batch_idx: 0 |  Loss_1: (0.0547) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 108 | Batch_idx: 10 |  Loss_1: (0.0374) | Acc_1: (98.65%) (1389/1408)\n",
      "Epoch: 108 | Batch_idx: 20 |  Loss_1: (0.0357) | Acc_1: (98.66%) (2652/2688)\n",
      "Epoch: 108 | Batch_idx: 30 |  Loss_1: (0.0360) | Acc_1: (98.64%) (3914/3968)\n",
      "Epoch: 108 | Batch_idx: 40 |  Loss_1: (0.0359) | Acc_1: (98.72%) (5181/5248)\n",
      "Epoch: 108 | Batch_idx: 50 |  Loss_1: (0.0387) | Acc_1: (98.59%) (6436/6528)\n",
      "Epoch: 108 | Batch_idx: 60 |  Loss_1: (0.0401) | Acc_1: (98.54%) (7694/7808)\n",
      "Epoch: 108 | Batch_idx: 70 |  Loss_1: (0.0430) | Acc_1: (98.49%) (8951/9088)\n",
      "Epoch: 108 | Batch_idx: 80 |  Loss_1: (0.0428) | Acc_1: (98.50%) (10212/10368)\n",
      "Epoch: 108 | Batch_idx: 90 |  Loss_1: (0.0435) | Acc_1: (98.46%) (11469/11648)\n",
      "Epoch: 108 | Batch_idx: 100 |  Loss_1: (0.0418) | Acc_1: (98.54%) (12739/12928)\n",
      "Epoch: 108 | Batch_idx: 110 |  Loss_1: (0.0417) | Acc_1: (98.54%) (14001/14208)\n",
      "Epoch: 108 | Batch_idx: 120 |  Loss_1: (0.0425) | Acc_1: (98.49%) (15254/15488)\n",
      "Epoch: 108 | Batch_idx: 130 |  Loss_1: (0.0428) | Acc_1: (98.50%) (16516/16768)\n",
      "Epoch: 108 | Batch_idx: 140 |  Loss_1: (0.0437) | Acc_1: (98.45%) (17768/18048)\n",
      "Epoch: 108 | Batch_idx: 150 |  Loss_1: (0.0440) | Acc_1: (98.45%) (19029/19328)\n",
      "Epoch: 108 | Batch_idx: 160 |  Loss_1: (0.0437) | Acc_1: (98.47%) (20292/20608)\n",
      "Epoch: 108 | Batch_idx: 170 |  Loss_1: (0.0443) | Acc_1: (98.43%) (21545/21888)\n",
      "Epoch: 108 | Batch_idx: 180 |  Loss_1: (0.0442) | Acc_1: (98.43%) (22805/23168)\n",
      "Epoch: 108 | Batch_idx: 190 |  Loss_1: (0.0442) | Acc_1: (98.42%) (24061/24448)\n",
      "Epoch: 108 | Batch_idx: 200 |  Loss_1: (0.0449) | Acc_1: (98.39%) (25313/25728)\n",
      "Epoch: 108 | Batch_idx: 210 |  Loss_1: (0.0450) | Acc_1: (98.38%) (26571/27008)\n",
      "Epoch: 108 | Batch_idx: 220 |  Loss_1: (0.0463) | Acc_1: (98.34%) (27818/28288)\n",
      "Epoch: 108 | Batch_idx: 230 |  Loss_1: (0.0471) | Acc_1: (98.32%) (29071/29568)\n",
      "Epoch: 108 | Batch_idx: 240 |  Loss_1: (0.0478) | Acc_1: (98.29%) (30322/30848)\n",
      "Epoch: 108 | Batch_idx: 250 |  Loss_1: (0.0471) | Acc_1: (98.33%) (31590/32128)\n",
      "Epoch: 108 | Batch_idx: 260 |  Loss_1: (0.0464) | Acc_1: (98.35%) (32858/33408)\n",
      "Epoch: 108 | Batch_idx: 270 |  Loss_1: (0.0467) | Acc_1: (98.34%) (34112/34688)\n",
      "Epoch: 108 | Batch_idx: 280 |  Loss_1: (0.0465) | Acc_1: (98.35%) (35375/35968)\n",
      "Epoch: 108 | Batch_idx: 290 |  Loss_1: (0.0466) | Acc_1: (98.35%) (36633/37248)\n",
      "Epoch: 108 | Batch_idx: 300 |  Loss_1: (0.0468) | Acc_1: (98.34%) (37888/38528)\n",
      "Epoch: 108 | Batch_idx: 310 |  Loss_1: (0.0468) | Acc_1: (98.32%) (39140/39808)\n",
      "Epoch: 108 | Batch_idx: 320 |  Loss_1: (0.0467) | Acc_1: (98.34%) (40405/41088)\n",
      "Epoch: 108 | Batch_idx: 330 |  Loss_1: (0.0469) | Acc_1: (98.34%) (41665/42368)\n",
      "Epoch: 108 | Batch_idx: 340 |  Loss_1: (0.0470) | Acc_1: (98.33%) (42920/43648)\n",
      "Epoch: 108 | Batch_idx: 350 |  Loss_1: (0.0471) | Acc_1: (98.32%) (44173/44928)\n",
      "Epoch: 108 | Batch_idx: 360 |  Loss_1: (0.0475) | Acc_1: (98.30%) (45424/46208)\n",
      "Epoch: 108 | Batch_idx: 370 |  Loss_1: (0.0476) | Acc_1: (98.30%) (46683/47488)\n",
      "Epoch: 108 | Batch_idx: 380 |  Loss_1: (0.0477) | Acc_1: (98.30%) (47937/48768)\n",
      "Epoch: 108 | Batch_idx: 390 |  Loss_1: (0.0475) | Acc_1: (98.30%) (49152/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4747) | Acc: (90.57%) (9057/10000)\n",
      "Epoch: 109 | Batch_idx: 0 |  Loss_1: (0.0170) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 109 | Batch_idx: 10 |  Loss_1: (0.0370) | Acc_1: (98.58%) (1388/1408)\n",
      "Epoch: 109 | Batch_idx: 20 |  Loss_1: (0.0425) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 109 | Batch_idx: 30 |  Loss_1: (0.0439) | Acc_1: (98.54%) (3910/3968)\n",
      "Epoch: 109 | Batch_idx: 40 |  Loss_1: (0.0447) | Acc_1: (98.46%) (5167/5248)\n",
      "Epoch: 109 | Batch_idx: 50 |  Loss_1: (0.0433) | Acc_1: (98.56%) (6434/6528)\n",
      "Epoch: 109 | Batch_idx: 60 |  Loss_1: (0.0436) | Acc_1: (98.54%) (7694/7808)\n",
      "Epoch: 109 | Batch_idx: 70 |  Loss_1: (0.0416) | Acc_1: (98.64%) (8964/9088)\n",
      "Epoch: 109 | Batch_idx: 80 |  Loss_1: (0.0421) | Acc_1: (98.63%) (10226/10368)\n",
      "Epoch: 109 | Batch_idx: 90 |  Loss_1: (0.0433) | Acc_1: (98.57%) (11482/11648)\n",
      "Epoch: 109 | Batch_idx: 100 |  Loss_1: (0.0430) | Acc_1: (98.55%) (12741/12928)\n",
      "Epoch: 109 | Batch_idx: 110 |  Loss_1: (0.0434) | Acc_1: (98.52%) (13998/14208)\n",
      "Epoch: 109 | Batch_idx: 120 |  Loss_1: (0.0440) | Acc_1: (98.51%) (15257/15488)\n",
      "Epoch: 109 | Batch_idx: 130 |  Loss_1: (0.0444) | Acc_1: (98.49%) (16515/16768)\n",
      "Epoch: 109 | Batch_idx: 140 |  Loss_1: (0.0453) | Acc_1: (98.43%) (17764/18048)\n",
      "Epoch: 109 | Batch_idx: 150 |  Loss_1: (0.0454) | Acc_1: (98.40%) (19019/19328)\n",
      "Epoch: 109 | Batch_idx: 160 |  Loss_1: (0.0454) | Acc_1: (98.39%) (20276/20608)\n",
      "Epoch: 109 | Batch_idx: 170 |  Loss_1: (0.0455) | Acc_1: (98.40%) (21537/21888)\n",
      "Epoch: 109 | Batch_idx: 180 |  Loss_1: (0.0479) | Acc_1: (98.31%) (22777/23168)\n",
      "Epoch: 109 | Batch_idx: 190 |  Loss_1: (0.0483) | Acc_1: (98.32%) (24037/24448)\n",
      "Epoch: 109 | Batch_idx: 200 |  Loss_1: (0.0487) | Acc_1: (98.30%) (25291/25728)\n",
      "Epoch: 109 | Batch_idx: 210 |  Loss_1: (0.0501) | Acc_1: (98.27%) (26541/27008)\n",
      "Epoch: 109 | Batch_idx: 220 |  Loss_1: (0.0500) | Acc_1: (98.26%) (27795/28288)\n",
      "Epoch: 109 | Batch_idx: 230 |  Loss_1: (0.0499) | Acc_1: (98.28%) (29059/29568)\n",
      "Epoch: 109 | Batch_idx: 240 |  Loss_1: (0.0498) | Acc_1: (98.29%) (30320/30848)\n",
      "Epoch: 109 | Batch_idx: 250 |  Loss_1: (0.0501) | Acc_1: (98.29%) (31578/32128)\n",
      "Epoch: 109 | Batch_idx: 260 |  Loss_1: (0.0504) | Acc_1: (98.28%) (32832/33408)\n",
      "Epoch: 109 | Batch_idx: 270 |  Loss_1: (0.0506) | Acc_1: (98.26%) (34085/34688)\n",
      "Epoch: 109 | Batch_idx: 280 |  Loss_1: (0.0504) | Acc_1: (98.27%) (35346/35968)\n",
      "Epoch: 109 | Batch_idx: 290 |  Loss_1: (0.0500) | Acc_1: (98.28%) (36608/37248)\n",
      "Epoch: 109 | Batch_idx: 300 |  Loss_1: (0.0499) | Acc_1: (98.28%) (37867/38528)\n",
      "Epoch: 109 | Batch_idx: 310 |  Loss_1: (0.0497) | Acc_1: (98.29%) (39126/39808)\n",
      "Epoch: 109 | Batch_idx: 320 |  Loss_1: (0.0497) | Acc_1: (98.28%) (40383/41088)\n",
      "Epoch: 109 | Batch_idx: 330 |  Loss_1: (0.0492) | Acc_1: (98.30%) (41646/42368)\n",
      "Epoch: 109 | Batch_idx: 340 |  Loss_1: (0.0485) | Acc_1: (98.33%) (42917/43648)\n",
      "Epoch: 109 | Batch_idx: 350 |  Loss_1: (0.0487) | Acc_1: (98.31%) (44170/44928)\n",
      "Epoch: 109 | Batch_idx: 360 |  Loss_1: (0.0489) | Acc_1: (98.30%) (45421/46208)\n",
      "Epoch: 109 | Batch_idx: 370 |  Loss_1: (0.0492) | Acc_1: (98.30%) (46679/47488)\n",
      "Epoch: 109 | Batch_idx: 380 |  Loss_1: (0.0490) | Acc_1: (98.30%) (47940/48768)\n",
      "Epoch: 109 | Batch_idx: 390 |  Loss_1: (0.0490) | Acc_1: (98.31%) (49153/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4438) | Acc: (90.90%) (9090/10000)\n",
      "Epoch: 110 | Batch_idx: 0 |  Loss_1: (0.0220) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 110 | Batch_idx: 10 |  Loss_1: (0.0349) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 110 | Batch_idx: 20 |  Loss_1: (0.0369) | Acc_1: (98.66%) (2652/2688)\n",
      "Epoch: 110 | Batch_idx: 30 |  Loss_1: (0.0375) | Acc_1: (98.64%) (3914/3968)\n",
      "Epoch: 110 | Batch_idx: 40 |  Loss_1: (0.0378) | Acc_1: (98.59%) (5174/5248)\n",
      "Epoch: 110 | Batch_idx: 50 |  Loss_1: (0.0390) | Acc_1: (98.56%) (6434/6528)\n",
      "Epoch: 110 | Batch_idx: 60 |  Loss_1: (0.0376) | Acc_1: (98.60%) (7699/7808)\n",
      "Epoch: 110 | Batch_idx: 70 |  Loss_1: (0.0374) | Acc_1: (98.61%) (8962/9088)\n",
      "Epoch: 110 | Batch_idx: 80 |  Loss_1: (0.0394) | Acc_1: (98.61%) (10224/10368)\n",
      "Epoch: 110 | Batch_idx: 90 |  Loss_1: (0.0399) | Acc_1: (98.62%) (11487/11648)\n",
      "Epoch: 110 | Batch_idx: 100 |  Loss_1: (0.0411) | Acc_1: (98.59%) (12746/12928)\n",
      "Epoch: 110 | Batch_idx: 110 |  Loss_1: (0.0413) | Acc_1: (98.59%) (14007/14208)\n",
      "Epoch: 110 | Batch_idx: 120 |  Loss_1: (0.0415) | Acc_1: (98.57%) (15267/15488)\n",
      "Epoch: 110 | Batch_idx: 130 |  Loss_1: (0.0413) | Acc_1: (98.60%) (16534/16768)\n",
      "Epoch: 110 | Batch_idx: 140 |  Loss_1: (0.0409) | Acc_1: (98.59%) (17793/18048)\n",
      "Epoch: 110 | Batch_idx: 150 |  Loss_1: (0.0414) | Acc_1: (98.57%) (19051/19328)\n",
      "Epoch: 110 | Batch_idx: 160 |  Loss_1: (0.0425) | Acc_1: (98.54%) (20308/20608)\n",
      "Epoch: 110 | Batch_idx: 170 |  Loss_1: (0.0433) | Acc_1: (98.52%) (21565/21888)\n",
      "Epoch: 110 | Batch_idx: 180 |  Loss_1: (0.0431) | Acc_1: (98.54%) (22829/23168)\n",
      "Epoch: 110 | Batch_idx: 190 |  Loss_1: (0.0432) | Acc_1: (98.54%) (24091/24448)\n",
      "Epoch: 110 | Batch_idx: 200 |  Loss_1: (0.0433) | Acc_1: (98.54%) (25352/25728)\n",
      "Epoch: 110 | Batch_idx: 210 |  Loss_1: (0.0432) | Acc_1: (98.53%) (26612/27008)\n",
      "Epoch: 110 | Batch_idx: 220 |  Loss_1: (0.0431) | Acc_1: (98.54%) (27874/28288)\n",
      "Epoch: 110 | Batch_idx: 230 |  Loss_1: (0.0432) | Acc_1: (98.52%) (29131/29568)\n",
      "Epoch: 110 | Batch_idx: 240 |  Loss_1: (0.0432) | Acc_1: (98.51%) (30389/30848)\n",
      "Epoch: 110 | Batch_idx: 250 |  Loss_1: (0.0438) | Acc_1: (98.51%) (31648/32128)\n",
      "Epoch: 110 | Batch_idx: 260 |  Loss_1: (0.0434) | Acc_1: (98.52%) (32912/33408)\n",
      "Epoch: 110 | Batch_idx: 270 |  Loss_1: (0.0431) | Acc_1: (98.53%) (34179/34688)\n",
      "Epoch: 110 | Batch_idx: 280 |  Loss_1: (0.0426) | Acc_1: (98.55%) (35447/35968)\n",
      "Epoch: 110 | Batch_idx: 290 |  Loss_1: (0.0424) | Acc_1: (98.56%) (36712/37248)\n",
      "Epoch: 110 | Batch_idx: 300 |  Loss_1: (0.0425) | Acc_1: (98.56%) (37972/38528)\n",
      "Epoch: 110 | Batch_idx: 310 |  Loss_1: (0.0426) | Acc_1: (98.55%) (39232/39808)\n",
      "Epoch: 110 | Batch_idx: 320 |  Loss_1: (0.0424) | Acc_1: (98.56%) (40496/41088)\n",
      "Epoch: 110 | Batch_idx: 330 |  Loss_1: (0.0426) | Acc_1: (98.55%) (41753/42368)\n",
      "Epoch: 110 | Batch_idx: 340 |  Loss_1: (0.0430) | Acc_1: (98.54%) (43009/43648)\n",
      "Epoch: 110 | Batch_idx: 350 |  Loss_1: (0.0432) | Acc_1: (98.52%) (44263/44928)\n",
      "Epoch: 110 | Batch_idx: 360 |  Loss_1: (0.0432) | Acc_1: (98.52%) (45526/46208)\n",
      "Epoch: 110 | Batch_idx: 370 |  Loss_1: (0.0434) | Acc_1: (98.52%) (46783/47488)\n",
      "Epoch: 110 | Batch_idx: 380 |  Loss_1: (0.0441) | Acc_1: (98.49%) (48032/48768)\n",
      "Epoch: 110 | Batch_idx: 390 |  Loss_1: (0.0445) | Acc_1: (98.48%) (49241/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4856) | Acc: (90.74%) (9074/10000)\n",
      "Epoch: 111 | Batch_idx: 0 |  Loss_1: (0.0176) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 111 | Batch_idx: 10 |  Loss_1: (0.0513) | Acc_1: (98.30%) (1384/1408)\n",
      "Epoch: 111 | Batch_idx: 20 |  Loss_1: (0.0472) | Acc_1: (98.47%) (2647/2688)\n",
      "Epoch: 111 | Batch_idx: 30 |  Loss_1: (0.0453) | Acc_1: (98.51%) (3909/3968)\n",
      "Epoch: 111 | Batch_idx: 40 |  Loss_1: (0.0459) | Acc_1: (98.46%) (5167/5248)\n",
      "Epoch: 111 | Batch_idx: 50 |  Loss_1: (0.0471) | Acc_1: (98.44%) (6426/6528)\n",
      "Epoch: 111 | Batch_idx: 60 |  Loss_1: (0.0473) | Acc_1: (98.45%) (7687/7808)\n",
      "Epoch: 111 | Batch_idx: 70 |  Loss_1: (0.0460) | Acc_1: (98.48%) (8950/9088)\n",
      "Epoch: 111 | Batch_idx: 80 |  Loss_1: (0.0467) | Acc_1: (98.49%) (10211/10368)\n",
      "Epoch: 111 | Batch_idx: 90 |  Loss_1: (0.0452) | Acc_1: (98.51%) (11475/11648)\n",
      "Epoch: 111 | Batch_idx: 100 |  Loss_1: (0.0434) | Acc_1: (98.58%) (12745/12928)\n",
      "Epoch: 111 | Batch_idx: 110 |  Loss_1: (0.0423) | Acc_1: (98.64%) (14015/14208)\n",
      "Epoch: 111 | Batch_idx: 120 |  Loss_1: (0.0419) | Acc_1: (98.65%) (15279/15488)\n",
      "Epoch: 111 | Batch_idx: 130 |  Loss_1: (0.0414) | Acc_1: (98.63%) (16538/16768)\n",
      "Epoch: 111 | Batch_idx: 140 |  Loss_1: (0.0404) | Acc_1: (98.67%) (17808/18048)\n",
      "Epoch: 111 | Batch_idx: 150 |  Loss_1: (0.0405) | Acc_1: (98.66%) (19069/19328)\n",
      "Epoch: 111 | Batch_idx: 160 |  Loss_1: (0.0404) | Acc_1: (98.65%) (20329/20608)\n",
      "Epoch: 111 | Batch_idx: 170 |  Loss_1: (0.0402) | Acc_1: (98.64%) (21591/21888)\n",
      "Epoch: 111 | Batch_idx: 180 |  Loss_1: (0.0409) | Acc_1: (98.62%) (22848/23168)\n",
      "Epoch: 111 | Batch_idx: 190 |  Loss_1: (0.0403) | Acc_1: (98.63%) (24112/24448)\n",
      "Epoch: 111 | Batch_idx: 200 |  Loss_1: (0.0405) | Acc_1: (98.62%) (25373/25728)\n",
      "Epoch: 111 | Batch_idx: 210 |  Loss_1: (0.0407) | Acc_1: (98.60%) (26630/27008)\n",
      "Epoch: 111 | Batch_idx: 220 |  Loss_1: (0.0407) | Acc_1: (98.60%) (27891/28288)\n",
      "Epoch: 111 | Batch_idx: 230 |  Loss_1: (0.0414) | Acc_1: (98.58%) (29147/29568)\n",
      "Epoch: 111 | Batch_idx: 240 |  Loss_1: (0.0419) | Acc_1: (98.57%) (30407/30848)\n",
      "Epoch: 111 | Batch_idx: 250 |  Loss_1: (0.0417) | Acc_1: (98.57%) (31667/32128)\n",
      "Epoch: 111 | Batch_idx: 260 |  Loss_1: (0.0416) | Acc_1: (98.56%) (32927/33408)\n",
      "Epoch: 111 | Batch_idx: 270 |  Loss_1: (0.0419) | Acc_1: (98.54%) (34181/34688)\n",
      "Epoch: 111 | Batch_idx: 280 |  Loss_1: (0.0424) | Acc_1: (98.53%) (35439/35968)\n",
      "Epoch: 111 | Batch_idx: 290 |  Loss_1: (0.0429) | Acc_1: (98.51%) (36693/37248)\n",
      "Epoch: 111 | Batch_idx: 300 |  Loss_1: (0.0429) | Acc_1: (98.52%) (37956/38528)\n",
      "Epoch: 111 | Batch_idx: 310 |  Loss_1: (0.0425) | Acc_1: (98.53%) (39222/39808)\n",
      "Epoch: 111 | Batch_idx: 320 |  Loss_1: (0.0426) | Acc_1: (98.53%) (40485/41088)\n",
      "Epoch: 111 | Batch_idx: 330 |  Loss_1: (0.0426) | Acc_1: (98.53%) (41747/42368)\n",
      "Epoch: 111 | Batch_idx: 340 |  Loss_1: (0.0426) | Acc_1: (98.54%) (43010/43648)\n",
      "Epoch: 111 | Batch_idx: 350 |  Loss_1: (0.0429) | Acc_1: (98.53%) (44269/44928)\n",
      "Epoch: 111 | Batch_idx: 360 |  Loss_1: (0.0431) | Acc_1: (98.53%) (45528/46208)\n",
      "Epoch: 111 | Batch_idx: 370 |  Loss_1: (0.0433) | Acc_1: (98.52%) (46786/47488)\n",
      "Epoch: 111 | Batch_idx: 380 |  Loss_1: (0.0433) | Acc_1: (98.53%) (48049/48768)\n",
      "Epoch: 111 | Batch_idx: 390 |  Loss_1: (0.0433) | Acc_1: (98.52%) (49262/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4706) | Acc: (90.73%) (9073/10000)\n",
      "Epoch: 112 | Batch_idx: 0 |  Loss_1: (0.0459) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 112 | Batch_idx: 10 |  Loss_1: (0.0448) | Acc_1: (98.58%) (1388/1408)\n",
      "Epoch: 112 | Batch_idx: 20 |  Loss_1: (0.0439) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 112 | Batch_idx: 30 |  Loss_1: (0.0394) | Acc_1: (98.69%) (3916/3968)\n",
      "Epoch: 112 | Batch_idx: 40 |  Loss_1: (0.0442) | Acc_1: (98.48%) (5168/5248)\n",
      "Epoch: 112 | Batch_idx: 50 |  Loss_1: (0.0457) | Acc_1: (98.38%) (6422/6528)\n",
      "Epoch: 112 | Batch_idx: 60 |  Loss_1: (0.0452) | Acc_1: (98.41%) (7684/7808)\n",
      "Epoch: 112 | Batch_idx: 70 |  Loss_1: (0.0438) | Acc_1: (98.47%) (8949/9088)\n",
      "Epoch: 112 | Batch_idx: 80 |  Loss_1: (0.0428) | Acc_1: (98.51%) (10213/10368)\n",
      "Epoch: 112 | Batch_idx: 90 |  Loss_1: (0.0435) | Acc_1: (98.51%) (11474/11648)\n",
      "Epoch: 112 | Batch_idx: 100 |  Loss_1: (0.0456) | Acc_1: (98.44%) (12726/12928)\n",
      "Epoch: 112 | Batch_idx: 110 |  Loss_1: (0.0464) | Acc_1: (98.42%) (13983/14208)\n",
      "Epoch: 112 | Batch_idx: 120 |  Loss_1: (0.0457) | Acc_1: (98.45%) (15248/15488)\n",
      "Epoch: 112 | Batch_idx: 130 |  Loss_1: (0.0453) | Acc_1: (98.47%) (16512/16768)\n",
      "Epoch: 112 | Batch_idx: 140 |  Loss_1: (0.0462) | Acc_1: (98.42%) (17762/18048)\n",
      "Epoch: 112 | Batch_idx: 150 |  Loss_1: (0.0467) | Acc_1: (98.41%) (19020/19328)\n",
      "Epoch: 112 | Batch_idx: 160 |  Loss_1: (0.0465) | Acc_1: (98.40%) (20279/20608)\n",
      "Epoch: 112 | Batch_idx: 170 |  Loss_1: (0.0469) | Acc_1: (98.39%) (21535/21888)\n",
      "Epoch: 112 | Batch_idx: 180 |  Loss_1: (0.0469) | Acc_1: (98.41%) (22800/23168)\n",
      "Epoch: 112 | Batch_idx: 190 |  Loss_1: (0.0470) | Acc_1: (98.40%) (24056/24448)\n",
      "Epoch: 112 | Batch_idx: 200 |  Loss_1: (0.0474) | Acc_1: (98.38%) (25311/25728)\n",
      "Epoch: 112 | Batch_idx: 210 |  Loss_1: (0.0474) | Acc_1: (98.38%) (26570/27008)\n",
      "Epoch: 112 | Batch_idx: 220 |  Loss_1: (0.0472) | Acc_1: (98.40%) (27835/28288)\n",
      "Epoch: 112 | Batch_idx: 230 |  Loss_1: (0.0467) | Acc_1: (98.41%) (29097/29568)\n",
      "Epoch: 112 | Batch_idx: 240 |  Loss_1: (0.0465) | Acc_1: (98.39%) (30350/30848)\n",
      "Epoch: 112 | Batch_idx: 250 |  Loss_1: (0.0466) | Acc_1: (98.38%) (31607/32128)\n",
      "Epoch: 112 | Batch_idx: 260 |  Loss_1: (0.0467) | Acc_1: (98.38%) (32866/33408)\n",
      "Epoch: 112 | Batch_idx: 270 |  Loss_1: (0.0470) | Acc_1: (98.37%) (34123/34688)\n",
      "Epoch: 112 | Batch_idx: 280 |  Loss_1: (0.0471) | Acc_1: (98.38%) (35384/35968)\n",
      "Epoch: 112 | Batch_idx: 290 |  Loss_1: (0.0469) | Acc_1: (98.38%) (36645/37248)\n",
      "Epoch: 112 | Batch_idx: 300 |  Loss_1: (0.0467) | Acc_1: (98.39%) (37908/38528)\n",
      "Epoch: 112 | Batch_idx: 310 |  Loss_1: (0.0463) | Acc_1: (98.41%) (39175/39808)\n",
      "Epoch: 112 | Batch_idx: 320 |  Loss_1: (0.0464) | Acc_1: (98.40%) (40432/41088)\n",
      "Epoch: 112 | Batch_idx: 330 |  Loss_1: (0.0464) | Acc_1: (98.40%) (41691/42368)\n",
      "Epoch: 112 | Batch_idx: 340 |  Loss_1: (0.0462) | Acc_1: (98.42%) (42957/43648)\n",
      "Epoch: 112 | Batch_idx: 350 |  Loss_1: (0.0457) | Acc_1: (98.44%) (44225/44928)\n",
      "Epoch: 112 | Batch_idx: 360 |  Loss_1: (0.0453) | Acc_1: (98.45%) (45492/46208)\n",
      "Epoch: 112 | Batch_idx: 370 |  Loss_1: (0.0453) | Acc_1: (98.45%) (46751/47488)\n",
      "Epoch: 112 | Batch_idx: 380 |  Loss_1: (0.0452) | Acc_1: (98.45%) (48014/48768)\n",
      "Epoch: 112 | Batch_idx: 390 |  Loss_1: (0.0449) | Acc_1: (98.47%) (49234/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4524) | Acc: (91.09%) (9109/10000)\n",
      "Epoch: 113 | Batch_idx: 0 |  Loss_1: (0.0233) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 113 | Batch_idx: 10 |  Loss_1: (0.0358) | Acc_1: (99.01%) (1394/1408)\n",
      "Epoch: 113 | Batch_idx: 20 |  Loss_1: (0.0348) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 113 | Batch_idx: 30 |  Loss_1: (0.0381) | Acc_1: (98.87%) (3923/3968)\n",
      "Epoch: 113 | Batch_idx: 40 |  Loss_1: (0.0381) | Acc_1: (98.86%) (5188/5248)\n",
      "Epoch: 113 | Batch_idx: 50 |  Loss_1: (0.0399) | Acc_1: (98.74%) (6446/6528)\n",
      "Epoch: 113 | Batch_idx: 60 |  Loss_1: (0.0370) | Acc_1: (98.86%) (7719/7808)\n",
      "Epoch: 113 | Batch_idx: 70 |  Loss_1: (0.0369) | Acc_1: (98.86%) (8984/9088)\n",
      "Epoch: 113 | Batch_idx: 80 |  Loss_1: (0.0398) | Acc_1: (98.73%) (10236/10368)\n",
      "Epoch: 113 | Batch_idx: 90 |  Loss_1: (0.0391) | Acc_1: (98.74%) (11501/11648)\n",
      "Epoch: 113 | Batch_idx: 100 |  Loss_1: (0.0396) | Acc_1: (98.71%) (12761/12928)\n",
      "Epoch: 113 | Batch_idx: 110 |  Loss_1: (0.0400) | Acc_1: (98.67%) (14019/14208)\n",
      "Epoch: 113 | Batch_idx: 120 |  Loss_1: (0.0396) | Acc_1: (98.68%) (15283/15488)\n",
      "Epoch: 113 | Batch_idx: 130 |  Loss_1: (0.0399) | Acc_1: (98.65%) (16541/16768)\n",
      "Epoch: 113 | Batch_idx: 140 |  Loss_1: (0.0401) | Acc_1: (98.65%) (17805/18048)\n",
      "Epoch: 113 | Batch_idx: 150 |  Loss_1: (0.0401) | Acc_1: (98.68%) (19072/19328)\n",
      "Epoch: 113 | Batch_idx: 160 |  Loss_1: (0.0405) | Acc_1: (98.68%) (20336/20608)\n",
      "Epoch: 113 | Batch_idx: 170 |  Loss_1: (0.0405) | Acc_1: (98.68%) (21599/21888)\n",
      "Epoch: 113 | Batch_idx: 180 |  Loss_1: (0.0408) | Acc_1: (98.67%) (22860/23168)\n",
      "Epoch: 113 | Batch_idx: 190 |  Loss_1: (0.0407) | Acc_1: (98.66%) (24121/24448)\n",
      "Epoch: 113 | Batch_idx: 200 |  Loss_1: (0.0409) | Acc_1: (98.63%) (25376/25728)\n",
      "Epoch: 113 | Batch_idx: 210 |  Loss_1: (0.0409) | Acc_1: (98.63%) (26638/27008)\n",
      "Epoch: 113 | Batch_idx: 220 |  Loss_1: (0.0406) | Acc_1: (98.65%) (27906/28288)\n",
      "Epoch: 113 | Batch_idx: 230 |  Loss_1: (0.0409) | Acc_1: (98.63%) (29164/29568)\n",
      "Epoch: 113 | Batch_idx: 240 |  Loss_1: (0.0415) | Acc_1: (98.61%) (30419/30848)\n",
      "Epoch: 113 | Batch_idx: 250 |  Loss_1: (0.0415) | Acc_1: (98.60%) (31678/32128)\n",
      "Epoch: 113 | Batch_idx: 260 |  Loss_1: (0.0420) | Acc_1: (98.58%) (32932/33408)\n",
      "Epoch: 113 | Batch_idx: 270 |  Loss_1: (0.0422) | Acc_1: (98.57%) (34193/34688)\n",
      "Epoch: 113 | Batch_idx: 280 |  Loss_1: (0.0427) | Acc_1: (98.55%) (35446/35968)\n",
      "Epoch: 113 | Batch_idx: 290 |  Loss_1: (0.0428) | Acc_1: (98.53%) (36702/37248)\n",
      "Epoch: 113 | Batch_idx: 300 |  Loss_1: (0.0432) | Acc_1: (98.52%) (37959/38528)\n",
      "Epoch: 113 | Batch_idx: 310 |  Loss_1: (0.0437) | Acc_1: (98.51%) (39215/39808)\n",
      "Epoch: 113 | Batch_idx: 320 |  Loss_1: (0.0436) | Acc_1: (98.51%) (40474/41088)\n",
      "Epoch: 113 | Batch_idx: 330 |  Loss_1: (0.0438) | Acc_1: (98.50%) (41733/42368)\n",
      "Epoch: 113 | Batch_idx: 340 |  Loss_1: (0.0441) | Acc_1: (98.48%) (42985/43648)\n",
      "Epoch: 113 | Batch_idx: 350 |  Loss_1: (0.0441) | Acc_1: (98.48%) (44247/44928)\n",
      "Epoch: 113 | Batch_idx: 360 |  Loss_1: (0.0443) | Acc_1: (98.48%) (45505/46208)\n",
      "Epoch: 113 | Batch_idx: 370 |  Loss_1: (0.0442) | Acc_1: (98.48%) (46767/47488)\n",
      "Epoch: 113 | Batch_idx: 380 |  Loss_1: (0.0446) | Acc_1: (98.47%) (48024/48768)\n",
      "Epoch: 113 | Batch_idx: 390 |  Loss_1: (0.0448) | Acc_1: (98.46%) (49232/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4513) | Acc: (90.87%) (9087/10000)\n",
      "Epoch: 114 | Batch_idx: 0 |  Loss_1: (0.0461) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 114 | Batch_idx: 10 |  Loss_1: (0.0376) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 114 | Batch_idx: 20 |  Loss_1: (0.0329) | Acc_1: (98.85%) (2657/2688)\n",
      "Epoch: 114 | Batch_idx: 30 |  Loss_1: (0.0328) | Acc_1: (98.79%) (3920/3968)\n",
      "Epoch: 114 | Batch_idx: 40 |  Loss_1: (0.0357) | Acc_1: (98.74%) (5182/5248)\n",
      "Epoch: 114 | Batch_idx: 50 |  Loss_1: (0.0339) | Acc_1: (98.84%) (6452/6528)\n",
      "Epoch: 114 | Batch_idx: 60 |  Loss_1: (0.0346) | Acc_1: (98.83%) (7717/7808)\n",
      "Epoch: 114 | Batch_idx: 70 |  Loss_1: (0.0342) | Acc_1: (98.84%) (8983/9088)\n",
      "Epoch: 114 | Batch_idx: 80 |  Loss_1: (0.0353) | Acc_1: (98.81%) (10245/10368)\n",
      "Epoch: 114 | Batch_idx: 90 |  Loss_1: (0.0363) | Acc_1: (98.81%) (11509/11648)\n",
      "Epoch: 114 | Batch_idx: 100 |  Loss_1: (0.0376) | Acc_1: (98.75%) (12766/12928)\n",
      "Epoch: 114 | Batch_idx: 110 |  Loss_1: (0.0393) | Acc_1: (98.71%) (14025/14208)\n",
      "Epoch: 114 | Batch_idx: 120 |  Loss_1: (0.0396) | Acc_1: (98.68%) (15284/15488)\n",
      "Epoch: 114 | Batch_idx: 130 |  Loss_1: (0.0395) | Acc_1: (98.70%) (16550/16768)\n",
      "Epoch: 114 | Batch_idx: 140 |  Loss_1: (0.0391) | Acc_1: (98.70%) (17814/18048)\n",
      "Epoch: 114 | Batch_idx: 150 |  Loss_1: (0.0394) | Acc_1: (98.70%) (19076/19328)\n",
      "Epoch: 114 | Batch_idx: 160 |  Loss_1: (0.0394) | Acc_1: (98.69%) (20339/20608)\n",
      "Epoch: 114 | Batch_idx: 170 |  Loss_1: (0.0396) | Acc_1: (98.65%) (21593/21888)\n",
      "Epoch: 114 | Batch_idx: 180 |  Loss_1: (0.0397) | Acc_1: (98.64%) (22852/23168)\n",
      "Epoch: 114 | Batch_idx: 190 |  Loss_1: (0.0390) | Acc_1: (98.63%) (24114/24448)\n",
      "Epoch: 114 | Batch_idx: 200 |  Loss_1: (0.0392) | Acc_1: (98.63%) (25376/25728)\n",
      "Epoch: 114 | Batch_idx: 210 |  Loss_1: (0.0389) | Acc_1: (98.65%) (26643/27008)\n",
      "Epoch: 114 | Batch_idx: 220 |  Loss_1: (0.0389) | Acc_1: (98.66%) (27910/28288)\n",
      "Epoch: 114 | Batch_idx: 230 |  Loss_1: (0.0396) | Acc_1: (98.64%) (29165/29568)\n",
      "Epoch: 114 | Batch_idx: 240 |  Loss_1: (0.0401) | Acc_1: (98.62%) (30423/30848)\n",
      "Epoch: 114 | Batch_idx: 250 |  Loss_1: (0.0398) | Acc_1: (98.62%) (31685/32128)\n",
      "Epoch: 114 | Batch_idx: 260 |  Loss_1: (0.0402) | Acc_1: (98.60%) (32941/33408)\n",
      "Epoch: 114 | Batch_idx: 270 |  Loss_1: (0.0399) | Acc_1: (98.62%) (34208/34688)\n",
      "Epoch: 114 | Batch_idx: 280 |  Loss_1: (0.0399) | Acc_1: (98.62%) (35470/35968)\n",
      "Epoch: 114 | Batch_idx: 290 |  Loss_1: (0.0403) | Acc_1: (98.60%) (36727/37248)\n",
      "Epoch: 114 | Batch_idx: 300 |  Loss_1: (0.0412) | Acc_1: (98.59%) (37985/38528)\n",
      "Epoch: 114 | Batch_idx: 310 |  Loss_1: (0.0409) | Acc_1: (98.60%) (39250/39808)\n",
      "Epoch: 114 | Batch_idx: 320 |  Loss_1: (0.0409) | Acc_1: (98.59%) (40510/41088)\n",
      "Epoch: 114 | Batch_idx: 330 |  Loss_1: (0.0410) | Acc_1: (98.59%) (41772/42368)\n",
      "Epoch: 114 | Batch_idx: 340 |  Loss_1: (0.0414) | Acc_1: (98.58%) (43029/43648)\n",
      "Epoch: 114 | Batch_idx: 350 |  Loss_1: (0.0415) | Acc_1: (98.57%) (44285/44928)\n",
      "Epoch: 114 | Batch_idx: 360 |  Loss_1: (0.0416) | Acc_1: (98.56%) (45542/46208)\n",
      "Epoch: 114 | Batch_idx: 370 |  Loss_1: (0.0416) | Acc_1: (98.55%) (46800/47488)\n",
      "Epoch: 114 | Batch_idx: 380 |  Loss_1: (0.0417) | Acc_1: (98.55%) (48061/48768)\n",
      "Epoch: 114 | Batch_idx: 390 |  Loss_1: (0.0415) | Acc_1: (98.57%) (49284/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4631) | Acc: (91.34%) (9134/10000)\n",
      "Epoch: 115 | Batch_idx: 0 |  Loss_1: (0.0351) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 115 | Batch_idx: 10 |  Loss_1: (0.0408) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 115 | Batch_idx: 20 |  Loss_1: (0.0374) | Acc_1: (98.44%) (2646/2688)\n",
      "Epoch: 115 | Batch_idx: 30 |  Loss_1: (0.0332) | Acc_1: (98.74%) (3918/3968)\n",
      "Epoch: 115 | Batch_idx: 40 |  Loss_1: (0.0403) | Acc_1: (98.61%) (5175/5248)\n",
      "Epoch: 115 | Batch_idx: 50 |  Loss_1: (0.0396) | Acc_1: (98.62%) (6438/6528)\n",
      "Epoch: 115 | Batch_idx: 60 |  Loss_1: (0.0387) | Acc_1: (98.63%) (7701/7808)\n",
      "Epoch: 115 | Batch_idx: 70 |  Loss_1: (0.0390) | Acc_1: (98.61%) (8962/9088)\n",
      "Epoch: 115 | Batch_idx: 80 |  Loss_1: (0.0389) | Acc_1: (98.65%) (10228/10368)\n",
      "Epoch: 115 | Batch_idx: 90 |  Loss_1: (0.0385) | Acc_1: (98.66%) (11492/11648)\n",
      "Epoch: 115 | Batch_idx: 100 |  Loss_1: (0.0382) | Acc_1: (98.66%) (12755/12928)\n",
      "Epoch: 115 | Batch_idx: 110 |  Loss_1: (0.0381) | Acc_1: (98.66%) (14018/14208)\n",
      "Epoch: 115 | Batch_idx: 120 |  Loss_1: (0.0378) | Acc_1: (98.68%) (15283/15488)\n",
      "Epoch: 115 | Batch_idx: 130 |  Loss_1: (0.0383) | Acc_1: (98.67%) (16545/16768)\n",
      "Epoch: 115 | Batch_idx: 140 |  Loss_1: (0.0394) | Acc_1: (98.62%) (17799/18048)\n",
      "Epoch: 115 | Batch_idx: 150 |  Loss_1: (0.0397) | Acc_1: (98.60%) (19058/19328)\n",
      "Epoch: 115 | Batch_idx: 160 |  Loss_1: (0.0400) | Acc_1: (98.58%) (20316/20608)\n",
      "Epoch: 115 | Batch_idx: 170 |  Loss_1: (0.0414) | Acc_1: (98.56%) (21573/21888)\n",
      "Epoch: 115 | Batch_idx: 180 |  Loss_1: (0.0409) | Acc_1: (98.58%) (22839/23168)\n",
      "Epoch: 115 | Batch_idx: 190 |  Loss_1: (0.0407) | Acc_1: (98.59%) (24103/24448)\n",
      "Epoch: 115 | Batch_idx: 200 |  Loss_1: (0.0410) | Acc_1: (98.59%) (25364/25728)\n",
      "Epoch: 115 | Batch_idx: 210 |  Loss_1: (0.0402) | Acc_1: (98.63%) (26637/27008)\n",
      "Epoch: 115 | Batch_idx: 220 |  Loss_1: (0.0401) | Acc_1: (98.62%) (27899/28288)\n",
      "Epoch: 115 | Batch_idx: 230 |  Loss_1: (0.0395) | Acc_1: (98.64%) (29166/29568)\n",
      "Epoch: 115 | Batch_idx: 240 |  Loss_1: (0.0396) | Acc_1: (98.64%) (30427/30848)\n",
      "Epoch: 115 | Batch_idx: 250 |  Loss_1: (0.0397) | Acc_1: (98.64%) (31690/32128)\n",
      "Epoch: 115 | Batch_idx: 260 |  Loss_1: (0.0400) | Acc_1: (98.63%) (32951/33408)\n",
      "Epoch: 115 | Batch_idx: 270 |  Loss_1: (0.0404) | Acc_1: (98.61%) (34206/34688)\n",
      "Epoch: 115 | Batch_idx: 280 |  Loss_1: (0.0399) | Acc_1: (98.63%) (35477/35968)\n",
      "Epoch: 115 | Batch_idx: 290 |  Loss_1: (0.0406) | Acc_1: (98.61%) (36730/37248)\n",
      "Epoch: 115 | Batch_idx: 300 |  Loss_1: (0.0411) | Acc_1: (98.58%) (37981/38528)\n",
      "Epoch: 115 | Batch_idx: 310 |  Loss_1: (0.0412) | Acc_1: (98.57%) (39239/39808)\n",
      "Epoch: 115 | Batch_idx: 320 |  Loss_1: (0.0416) | Acc_1: (98.56%) (40497/41088)\n",
      "Epoch: 115 | Batch_idx: 330 |  Loss_1: (0.0416) | Acc_1: (98.55%) (41753/42368)\n",
      "Epoch: 115 | Batch_idx: 340 |  Loss_1: (0.0413) | Acc_1: (98.56%) (43021/43648)\n",
      "Epoch: 115 | Batch_idx: 350 |  Loss_1: (0.0415) | Acc_1: (98.57%) (44285/44928)\n",
      "Epoch: 115 | Batch_idx: 360 |  Loss_1: (0.0415) | Acc_1: (98.56%) (45542/46208)\n",
      "Epoch: 115 | Batch_idx: 370 |  Loss_1: (0.0415) | Acc_1: (98.56%) (46802/47488)\n",
      "Epoch: 115 | Batch_idx: 380 |  Loss_1: (0.0416) | Acc_1: (98.55%) (48060/48768)\n",
      "Epoch: 115 | Batch_idx: 390 |  Loss_1: (0.0416) | Acc_1: (98.54%) (49272/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4490) | Acc: (91.00%) (9100/10000)\n",
      "Epoch: 116 | Batch_idx: 0 |  Loss_1: (0.0442) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 116 | Batch_idx: 10 |  Loss_1: (0.0430) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 116 | Batch_idx: 20 |  Loss_1: (0.0491) | Acc_1: (98.47%) (2647/2688)\n",
      "Epoch: 116 | Batch_idx: 30 |  Loss_1: (0.0488) | Acc_1: (98.51%) (3909/3968)\n",
      "Epoch: 116 | Batch_idx: 40 |  Loss_1: (0.0477) | Acc_1: (98.49%) (5169/5248)\n",
      "Epoch: 116 | Batch_idx: 50 |  Loss_1: (0.0447) | Acc_1: (98.54%) (6433/6528)\n",
      "Epoch: 116 | Batch_idx: 60 |  Loss_1: (0.0437) | Acc_1: (98.63%) (7701/7808)\n",
      "Epoch: 116 | Batch_idx: 70 |  Loss_1: (0.0441) | Acc_1: (98.59%) (8960/9088)\n",
      "Epoch: 116 | Batch_idx: 80 |  Loss_1: (0.0439) | Acc_1: (98.60%) (10223/10368)\n",
      "Epoch: 116 | Batch_idx: 90 |  Loss_1: (0.0442) | Acc_1: (98.60%) (11485/11648)\n",
      "Epoch: 116 | Batch_idx: 100 |  Loss_1: (0.0432) | Acc_1: (98.59%) (12746/12928)\n",
      "Epoch: 116 | Batch_idx: 110 |  Loss_1: (0.0435) | Acc_1: (98.56%) (14004/14208)\n",
      "Epoch: 116 | Batch_idx: 120 |  Loss_1: (0.0442) | Acc_1: (98.51%) (15258/15488)\n",
      "Epoch: 116 | Batch_idx: 130 |  Loss_1: (0.0443) | Acc_1: (98.49%) (16514/16768)\n",
      "Epoch: 116 | Batch_idx: 140 |  Loss_1: (0.0437) | Acc_1: (98.49%) (17775/18048)\n",
      "Epoch: 116 | Batch_idx: 150 |  Loss_1: (0.0442) | Acc_1: (98.47%) (19033/19328)\n",
      "Epoch: 116 | Batch_idx: 160 |  Loss_1: (0.0438) | Acc_1: (98.51%) (20301/20608)\n",
      "Epoch: 116 | Batch_idx: 170 |  Loss_1: (0.0441) | Acc_1: (98.47%) (21554/21888)\n",
      "Epoch: 116 | Batch_idx: 180 |  Loss_1: (0.0435) | Acc_1: (98.48%) (22816/23168)\n",
      "Epoch: 116 | Batch_idx: 190 |  Loss_1: (0.0431) | Acc_1: (98.50%) (24081/24448)\n",
      "Epoch: 116 | Batch_idx: 200 |  Loss_1: (0.0428) | Acc_1: (98.50%) (25342/25728)\n",
      "Epoch: 116 | Batch_idx: 210 |  Loss_1: (0.0426) | Acc_1: (98.50%) (26602/27008)\n",
      "Epoch: 116 | Batch_idx: 220 |  Loss_1: (0.0431) | Acc_1: (98.47%) (27855/28288)\n",
      "Epoch: 116 | Batch_idx: 230 |  Loss_1: (0.0432) | Acc_1: (98.47%) (29116/29568)\n",
      "Epoch: 116 | Batch_idx: 240 |  Loss_1: (0.0439) | Acc_1: (98.47%) (30376/30848)\n",
      "Epoch: 116 | Batch_idx: 250 |  Loss_1: (0.0440) | Acc_1: (98.47%) (31637/32128)\n",
      "Epoch: 116 | Batch_idx: 260 |  Loss_1: (0.0440) | Acc_1: (98.47%) (32898/33408)\n",
      "Epoch: 116 | Batch_idx: 270 |  Loss_1: (0.0438) | Acc_1: (98.47%) (34159/34688)\n",
      "Epoch: 116 | Batch_idx: 280 |  Loss_1: (0.0437) | Acc_1: (98.47%) (35419/35968)\n",
      "Epoch: 116 | Batch_idx: 290 |  Loss_1: (0.0436) | Acc_1: (98.47%) (36678/37248)\n",
      "Epoch: 116 | Batch_idx: 300 |  Loss_1: (0.0437) | Acc_1: (98.47%) (37938/38528)\n",
      "Epoch: 116 | Batch_idx: 310 |  Loss_1: (0.0435) | Acc_1: (98.48%) (39201/39808)\n",
      "Epoch: 116 | Batch_idx: 320 |  Loss_1: (0.0436) | Acc_1: (98.47%) (40458/41088)\n",
      "Epoch: 116 | Batch_idx: 330 |  Loss_1: (0.0438) | Acc_1: (98.46%) (41717/42368)\n",
      "Epoch: 116 | Batch_idx: 340 |  Loss_1: (0.0433) | Acc_1: (98.49%) (42987/43648)\n",
      "Epoch: 116 | Batch_idx: 350 |  Loss_1: (0.0432) | Acc_1: (98.49%) (44248/44928)\n",
      "Epoch: 116 | Batch_idx: 360 |  Loss_1: (0.0433) | Acc_1: (98.49%) (45508/46208)\n",
      "Epoch: 116 | Batch_idx: 370 |  Loss_1: (0.0433) | Acc_1: (98.49%) (46771/47488)\n",
      "Epoch: 116 | Batch_idx: 380 |  Loss_1: (0.0433) | Acc_1: (98.48%) (48028/48768)\n",
      "Epoch: 116 | Batch_idx: 390 |  Loss_1: (0.0429) | Acc_1: (98.50%) (49249/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4466) | Acc: (91.38%) (9138/10000)\n",
      "Epoch: 117 | Batch_idx: 0 |  Loss_1: (0.0530) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 117 | Batch_idx: 10 |  Loss_1: (0.0606) | Acc_1: (97.80%) (1377/1408)\n",
      "Epoch: 117 | Batch_idx: 20 |  Loss_1: (0.0523) | Acc_1: (98.14%) (2638/2688)\n",
      "Epoch: 117 | Batch_idx: 30 |  Loss_1: (0.0517) | Acc_1: (98.21%) (3897/3968)\n",
      "Epoch: 117 | Batch_idx: 40 |  Loss_1: (0.0470) | Acc_1: (98.30%) (5159/5248)\n",
      "Epoch: 117 | Batch_idx: 50 |  Loss_1: (0.0477) | Acc_1: (98.28%) (6416/6528)\n",
      "Epoch: 117 | Batch_idx: 60 |  Loss_1: (0.0495) | Acc_1: (98.26%) (7672/7808)\n",
      "Epoch: 117 | Batch_idx: 70 |  Loss_1: (0.0490) | Acc_1: (98.24%) (8928/9088)\n",
      "Epoch: 117 | Batch_idx: 80 |  Loss_1: (0.0477) | Acc_1: (98.30%) (10192/10368)\n",
      "Epoch: 117 | Batch_idx: 90 |  Loss_1: (0.0465) | Acc_1: (98.38%) (11459/11648)\n",
      "Epoch: 117 | Batch_idx: 100 |  Loss_1: (0.0481) | Acc_1: (98.32%) (12711/12928)\n",
      "Epoch: 117 | Batch_idx: 110 |  Loss_1: (0.0493) | Acc_1: (98.30%) (13967/14208)\n",
      "Epoch: 117 | Batch_idx: 120 |  Loss_1: (0.0487) | Acc_1: (98.31%) (15226/15488)\n",
      "Epoch: 117 | Batch_idx: 130 |  Loss_1: (0.0483) | Acc_1: (98.32%) (16487/16768)\n",
      "Epoch: 117 | Batch_idx: 140 |  Loss_1: (0.0471) | Acc_1: (98.36%) (17752/18048)\n",
      "Epoch: 117 | Batch_idx: 150 |  Loss_1: (0.0471) | Acc_1: (98.36%) (19011/19328)\n",
      "Epoch: 117 | Batch_idx: 160 |  Loss_1: (0.0461) | Acc_1: (98.38%) (20274/20608)\n",
      "Epoch: 117 | Batch_idx: 170 |  Loss_1: (0.0455) | Acc_1: (98.39%) (21535/21888)\n",
      "Epoch: 117 | Batch_idx: 180 |  Loss_1: (0.0458) | Acc_1: (98.38%) (22793/23168)\n",
      "Epoch: 117 | Batch_idx: 190 |  Loss_1: (0.0461) | Acc_1: (98.39%) (24054/24448)\n",
      "Epoch: 117 | Batch_idx: 200 |  Loss_1: (0.0460) | Acc_1: (98.40%) (25316/25728)\n",
      "Epoch: 117 | Batch_idx: 210 |  Loss_1: (0.0453) | Acc_1: (98.43%) (26583/27008)\n",
      "Epoch: 117 | Batch_idx: 220 |  Loss_1: (0.0449) | Acc_1: (98.44%) (27846/28288)\n",
      "Epoch: 117 | Batch_idx: 230 |  Loss_1: (0.0453) | Acc_1: (98.44%) (29107/29568)\n",
      "Epoch: 117 | Batch_idx: 240 |  Loss_1: (0.0448) | Acc_1: (98.46%) (30372/30848)\n",
      "Epoch: 117 | Batch_idx: 250 |  Loss_1: (0.0448) | Acc_1: (98.47%) (31635/32128)\n",
      "Epoch: 117 | Batch_idx: 260 |  Loss_1: (0.0441) | Acc_1: (98.49%) (32903/33408)\n",
      "Epoch: 117 | Batch_idx: 270 |  Loss_1: (0.0442) | Acc_1: (98.48%) (34161/34688)\n",
      "Epoch: 117 | Batch_idx: 280 |  Loss_1: (0.0444) | Acc_1: (98.49%) (35425/35968)\n",
      "Epoch: 117 | Batch_idx: 290 |  Loss_1: (0.0443) | Acc_1: (98.50%) (36690/37248)\n",
      "Epoch: 117 | Batch_idx: 300 |  Loss_1: (0.0438) | Acc_1: (98.51%) (37954/38528)\n",
      "Epoch: 117 | Batch_idx: 310 |  Loss_1: (0.0444) | Acc_1: (98.50%) (39209/39808)\n",
      "Epoch: 117 | Batch_idx: 320 |  Loss_1: (0.0443) | Acc_1: (98.49%) (40469/41088)\n",
      "Epoch: 117 | Batch_idx: 330 |  Loss_1: (0.0447) | Acc_1: (98.46%) (41716/42368)\n",
      "Epoch: 117 | Batch_idx: 340 |  Loss_1: (0.0452) | Acc_1: (98.43%) (42962/43648)\n",
      "Epoch: 117 | Batch_idx: 350 |  Loss_1: (0.0451) | Acc_1: (98.43%) (44221/44928)\n",
      "Epoch: 117 | Batch_idx: 360 |  Loss_1: (0.0452) | Acc_1: (98.42%) (45478/46208)\n",
      "Epoch: 117 | Batch_idx: 370 |  Loss_1: (0.0451) | Acc_1: (98.42%) (46739/47488)\n",
      "Epoch: 117 | Batch_idx: 380 |  Loss_1: (0.0453) | Acc_1: (98.40%) (47990/48768)\n",
      "Epoch: 117 | Batch_idx: 390 |  Loss_1: (0.0454) | Acc_1: (98.40%) (49202/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4662) | Acc: (90.50%) (9050/10000)\n",
      "Epoch: 118 | Batch_idx: 0 |  Loss_1: (0.0287) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 118 | Batch_idx: 10 |  Loss_1: (0.0509) | Acc_1: (98.37%) (1385/1408)\n",
      "Epoch: 118 | Batch_idx: 20 |  Loss_1: (0.0449) | Acc_1: (98.40%) (2645/2688)\n",
      "Epoch: 118 | Batch_idx: 30 |  Loss_1: (0.0437) | Acc_1: (98.59%) (3912/3968)\n",
      "Epoch: 118 | Batch_idx: 40 |  Loss_1: (0.0465) | Acc_1: (98.59%) (5174/5248)\n",
      "Epoch: 118 | Batch_idx: 50 |  Loss_1: (0.0451) | Acc_1: (98.59%) (6436/6528)\n",
      "Epoch: 118 | Batch_idx: 60 |  Loss_1: (0.0443) | Acc_1: (98.55%) (7695/7808)\n",
      "Epoch: 118 | Batch_idx: 70 |  Loss_1: (0.0444) | Acc_1: (98.58%) (8959/9088)\n",
      "Epoch: 118 | Batch_idx: 80 |  Loss_1: (0.0441) | Acc_1: (98.55%) (10218/10368)\n",
      "Epoch: 118 | Batch_idx: 90 |  Loss_1: (0.0450) | Acc_1: (98.53%) (11477/11648)\n",
      "Epoch: 118 | Batch_idx: 100 |  Loss_1: (0.0442) | Acc_1: (98.56%) (12742/12928)\n",
      "Epoch: 118 | Batch_idx: 110 |  Loss_1: (0.0436) | Acc_1: (98.55%) (14002/14208)\n",
      "Epoch: 118 | Batch_idx: 120 |  Loss_1: (0.0444) | Acc_1: (98.51%) (15257/15488)\n",
      "Epoch: 118 | Batch_idx: 130 |  Loss_1: (0.0447) | Acc_1: (98.49%) (16515/16768)\n",
      "Epoch: 118 | Batch_idx: 140 |  Loss_1: (0.0445) | Acc_1: (98.50%) (17778/18048)\n",
      "Epoch: 118 | Batch_idx: 150 |  Loss_1: (0.0439) | Acc_1: (98.52%) (19041/19328)\n",
      "Epoch: 118 | Batch_idx: 160 |  Loss_1: (0.0439) | Acc_1: (98.50%) (20299/20608)\n",
      "Epoch: 118 | Batch_idx: 170 |  Loss_1: (0.0437) | Acc_1: (98.51%) (21562/21888)\n",
      "Epoch: 118 | Batch_idx: 180 |  Loss_1: (0.0438) | Acc_1: (98.49%) (22819/23168)\n",
      "Epoch: 118 | Batch_idx: 190 |  Loss_1: (0.0440) | Acc_1: (98.48%) (24077/24448)\n",
      "Epoch: 118 | Batch_idx: 200 |  Loss_1: (0.0439) | Acc_1: (98.51%) (25345/25728)\n",
      "Epoch: 118 | Batch_idx: 210 |  Loss_1: (0.0436) | Acc_1: (98.52%) (26608/27008)\n",
      "Epoch: 118 | Batch_idx: 220 |  Loss_1: (0.0437) | Acc_1: (98.51%) (27867/28288)\n",
      "Epoch: 118 | Batch_idx: 230 |  Loss_1: (0.0432) | Acc_1: (98.51%) (29127/29568)\n",
      "Epoch: 118 | Batch_idx: 240 |  Loss_1: (0.0433) | Acc_1: (98.51%) (30387/30848)\n",
      "Epoch: 118 | Batch_idx: 250 |  Loss_1: (0.0433) | Acc_1: (98.50%) (31647/32128)\n",
      "Epoch: 118 | Batch_idx: 260 |  Loss_1: (0.0437) | Acc_1: (98.48%) (32901/33408)\n",
      "Epoch: 118 | Batch_idx: 270 |  Loss_1: (0.0441) | Acc_1: (98.49%) (34163/34688)\n",
      "Epoch: 118 | Batch_idx: 280 |  Loss_1: (0.0441) | Acc_1: (98.48%) (35420/35968)\n",
      "Epoch: 118 | Batch_idx: 290 |  Loss_1: (0.0441) | Acc_1: (98.46%) (36676/37248)\n",
      "Epoch: 118 | Batch_idx: 300 |  Loss_1: (0.0436) | Acc_1: (98.48%) (37942/38528)\n",
      "Epoch: 118 | Batch_idx: 310 |  Loss_1: (0.0437) | Acc_1: (98.48%) (39202/39808)\n",
      "Epoch: 118 | Batch_idx: 320 |  Loss_1: (0.0434) | Acc_1: (98.49%) (40466/41088)\n",
      "Epoch: 118 | Batch_idx: 330 |  Loss_1: (0.0434) | Acc_1: (98.49%) (41728/42368)\n",
      "Epoch: 118 | Batch_idx: 340 |  Loss_1: (0.0435) | Acc_1: (98.49%) (42990/43648)\n",
      "Epoch: 118 | Batch_idx: 350 |  Loss_1: (0.0437) | Acc_1: (98.48%) (44247/44928)\n",
      "Epoch: 118 | Batch_idx: 360 |  Loss_1: (0.0436) | Acc_1: (98.49%) (45509/46208)\n",
      "Epoch: 118 | Batch_idx: 370 |  Loss_1: (0.0432) | Acc_1: (98.50%) (46774/47488)\n",
      "Epoch: 118 | Batch_idx: 380 |  Loss_1: (0.0436) | Acc_1: (98.48%) (48026/48768)\n",
      "Epoch: 118 | Batch_idx: 390 |  Loss_1: (0.0439) | Acc_1: (98.47%) (49235/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4691) | Acc: (90.98%) (9098/10000)\n",
      "Epoch: 119 | Batch_idx: 0 |  Loss_1: (0.0553) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 119 | Batch_idx: 10 |  Loss_1: (0.0440) | Acc_1: (98.44%) (1386/1408)\n",
      "Epoch: 119 | Batch_idx: 20 |  Loss_1: (0.0467) | Acc_1: (98.55%) (2649/2688)\n",
      "Epoch: 119 | Batch_idx: 30 |  Loss_1: (0.0395) | Acc_1: (98.79%) (3920/3968)\n",
      "Epoch: 119 | Batch_idx: 40 |  Loss_1: (0.0415) | Acc_1: (98.65%) (5177/5248)\n",
      "Epoch: 119 | Batch_idx: 50 |  Loss_1: (0.0448) | Acc_1: (98.51%) (6431/6528)\n",
      "Epoch: 119 | Batch_idx: 60 |  Loss_1: (0.0435) | Acc_1: (98.54%) (7694/7808)\n",
      "Epoch: 119 | Batch_idx: 70 |  Loss_1: (0.0440) | Acc_1: (98.51%) (8953/9088)\n",
      "Epoch: 119 | Batch_idx: 80 |  Loss_1: (0.0436) | Acc_1: (98.51%) (10214/10368)\n",
      "Epoch: 119 | Batch_idx: 90 |  Loss_1: (0.0427) | Acc_1: (98.57%) (11481/11648)\n",
      "Epoch: 119 | Batch_idx: 100 |  Loss_1: (0.0417) | Acc_1: (98.59%) (12746/12928)\n",
      "Epoch: 119 | Batch_idx: 110 |  Loss_1: (0.0430) | Acc_1: (98.59%) (14007/14208)\n",
      "Epoch: 119 | Batch_idx: 120 |  Loss_1: (0.0443) | Acc_1: (98.55%) (15263/15488)\n",
      "Epoch: 119 | Batch_idx: 130 |  Loss_1: (0.0436) | Acc_1: (98.59%) (16531/16768)\n",
      "Epoch: 119 | Batch_idx: 140 |  Loss_1: (0.0434) | Acc_1: (98.58%) (17792/18048)\n",
      "Epoch: 119 | Batch_idx: 150 |  Loss_1: (0.0438) | Acc_1: (98.56%) (19049/19328)\n",
      "Epoch: 119 | Batch_idx: 160 |  Loss_1: (0.0429) | Acc_1: (98.60%) (20320/20608)\n",
      "Epoch: 119 | Batch_idx: 170 |  Loss_1: (0.0426) | Acc_1: (98.62%) (21587/21888)\n",
      "Epoch: 119 | Batch_idx: 180 |  Loss_1: (0.0423) | Acc_1: (98.62%) (22849/23168)\n",
      "Epoch: 119 | Batch_idx: 190 |  Loss_1: (0.0427) | Acc_1: (98.60%) (24105/24448)\n",
      "Epoch: 119 | Batch_idx: 200 |  Loss_1: (0.0424) | Acc_1: (98.60%) (25369/25728)\n",
      "Epoch: 119 | Batch_idx: 210 |  Loss_1: (0.0419) | Acc_1: (98.60%) (26630/27008)\n",
      "Epoch: 119 | Batch_idx: 220 |  Loss_1: (0.0419) | Acc_1: (98.60%) (27891/28288)\n",
      "Epoch: 119 | Batch_idx: 230 |  Loss_1: (0.0412) | Acc_1: (98.62%) (29161/29568)\n",
      "Epoch: 119 | Batch_idx: 240 |  Loss_1: (0.0416) | Acc_1: (98.60%) (30415/30848)\n",
      "Epoch: 119 | Batch_idx: 250 |  Loss_1: (0.0416) | Acc_1: (98.59%) (31676/32128)\n",
      "Epoch: 119 | Batch_idx: 260 |  Loss_1: (0.0419) | Acc_1: (98.58%) (32932/33408)\n",
      "Epoch: 119 | Batch_idx: 270 |  Loss_1: (0.0413) | Acc_1: (98.59%) (34200/34688)\n",
      "Epoch: 119 | Batch_idx: 280 |  Loss_1: (0.0415) | Acc_1: (98.59%) (35460/35968)\n",
      "Epoch: 119 | Batch_idx: 290 |  Loss_1: (0.0417) | Acc_1: (98.58%) (36718/37248)\n",
      "Epoch: 119 | Batch_idx: 300 |  Loss_1: (0.0413) | Acc_1: (98.59%) (37986/38528)\n",
      "Epoch: 119 | Batch_idx: 310 |  Loss_1: (0.0413) | Acc_1: (98.60%) (39250/39808)\n",
      "Epoch: 119 | Batch_idx: 320 |  Loss_1: (0.0412) | Acc_1: (98.60%) (40513/41088)\n",
      "Epoch: 119 | Batch_idx: 330 |  Loss_1: (0.0410) | Acc_1: (98.61%) (41779/42368)\n",
      "Epoch: 119 | Batch_idx: 340 |  Loss_1: (0.0407) | Acc_1: (98.61%) (43041/43648)\n",
      "Epoch: 119 | Batch_idx: 350 |  Loss_1: (0.0409) | Acc_1: (98.60%) (44297/44928)\n",
      "Epoch: 119 | Batch_idx: 360 |  Loss_1: (0.0412) | Acc_1: (98.58%) (45550/46208)\n",
      "Epoch: 119 | Batch_idx: 370 |  Loss_1: (0.0414) | Acc_1: (98.57%) (46807/47488)\n",
      "Epoch: 119 | Batch_idx: 380 |  Loss_1: (0.0415) | Acc_1: (98.55%) (48062/48768)\n",
      "Epoch: 119 | Batch_idx: 390 |  Loss_1: (0.0420) | Acc_1: (98.54%) (49268/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4605) | Acc: (91.03%) (9103/10000)\n",
      "Epoch: 120 | Batch_idx: 0 |  Loss_1: (0.0240) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 120 | Batch_idx: 10 |  Loss_1: (0.0388) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 120 | Batch_idx: 20 |  Loss_1: (0.0395) | Acc_1: (98.62%) (2651/2688)\n",
      "Epoch: 120 | Batch_idx: 30 |  Loss_1: (0.0430) | Acc_1: (98.54%) (3910/3968)\n",
      "Epoch: 120 | Batch_idx: 40 |  Loss_1: (0.0416) | Acc_1: (98.55%) (5172/5248)\n",
      "Epoch: 120 | Batch_idx: 50 |  Loss_1: (0.0392) | Acc_1: (98.61%) (6437/6528)\n",
      "Epoch: 120 | Batch_idx: 60 |  Loss_1: (0.0392) | Acc_1: (98.62%) (7700/7808)\n",
      "Epoch: 120 | Batch_idx: 70 |  Loss_1: (0.0400) | Acc_1: (98.61%) (8962/9088)\n",
      "Epoch: 120 | Batch_idx: 80 |  Loss_1: (0.0408) | Acc_1: (98.55%) (10218/10368)\n",
      "Epoch: 120 | Batch_idx: 90 |  Loss_1: (0.0391) | Acc_1: (98.63%) (11488/11648)\n",
      "Epoch: 120 | Batch_idx: 100 |  Loss_1: (0.0389) | Acc_1: (98.66%) (12755/12928)\n",
      "Epoch: 120 | Batch_idx: 110 |  Loss_1: (0.0392) | Acc_1: (98.63%) (14013/14208)\n",
      "Epoch: 120 | Batch_idx: 120 |  Loss_1: (0.0398) | Acc_1: (98.61%) (15273/15488)\n",
      "Epoch: 120 | Batch_idx: 130 |  Loss_1: (0.0390) | Acc_1: (98.63%) (16539/16768)\n",
      "Epoch: 120 | Batch_idx: 140 |  Loss_1: (0.0398) | Acc_1: (98.61%) (17798/18048)\n",
      "Epoch: 120 | Batch_idx: 150 |  Loss_1: (0.0398) | Acc_1: (98.63%) (19064/19328)\n",
      "Epoch: 120 | Batch_idx: 160 |  Loss_1: (0.0399) | Acc_1: (98.65%) (20330/20608)\n",
      "Epoch: 120 | Batch_idx: 170 |  Loss_1: (0.0397) | Acc_1: (98.65%) (21593/21888)\n",
      "Epoch: 120 | Batch_idx: 180 |  Loss_1: (0.0400) | Acc_1: (98.65%) (22855/23168)\n",
      "Epoch: 120 | Batch_idx: 190 |  Loss_1: (0.0398) | Acc_1: (98.67%) (24122/24448)\n",
      "Epoch: 120 | Batch_idx: 200 |  Loss_1: (0.0410) | Acc_1: (98.63%) (25376/25728)\n",
      "Epoch: 120 | Batch_idx: 210 |  Loss_1: (0.0412) | Acc_1: (98.59%) (26628/27008)\n",
      "Epoch: 120 | Batch_idx: 220 |  Loss_1: (0.0417) | Acc_1: (98.58%) (27885/28288)\n",
      "Epoch: 120 | Batch_idx: 230 |  Loss_1: (0.0425) | Acc_1: (98.54%) (29136/29568)\n",
      "Epoch: 120 | Batch_idx: 240 |  Loss_1: (0.0425) | Acc_1: (98.53%) (30394/30848)\n",
      "Epoch: 120 | Batch_idx: 250 |  Loss_1: (0.0427) | Acc_1: (98.52%) (31652/32128)\n",
      "Epoch: 120 | Batch_idx: 260 |  Loss_1: (0.0424) | Acc_1: (98.53%) (32916/33408)\n",
      "Epoch: 120 | Batch_idx: 270 |  Loss_1: (0.0423) | Acc_1: (98.53%) (34179/34688)\n",
      "Epoch: 120 | Batch_idx: 280 |  Loss_1: (0.0429) | Acc_1: (98.51%) (35433/35968)\n",
      "Epoch: 120 | Batch_idx: 290 |  Loss_1: (0.0428) | Acc_1: (98.52%) (36695/37248)\n",
      "Epoch: 120 | Batch_idx: 300 |  Loss_1: (0.0431) | Acc_1: (98.50%) (37952/38528)\n",
      "Epoch: 120 | Batch_idx: 310 |  Loss_1: (0.0431) | Acc_1: (98.50%) (39211/39808)\n",
      "Epoch: 120 | Batch_idx: 320 |  Loss_1: (0.0435) | Acc_1: (98.48%) (40465/41088)\n",
      "Epoch: 120 | Batch_idx: 330 |  Loss_1: (0.0437) | Acc_1: (98.47%) (41721/42368)\n",
      "Epoch: 120 | Batch_idx: 340 |  Loss_1: (0.0442) | Acc_1: (98.45%) (42971/43648)\n",
      "Epoch: 120 | Batch_idx: 350 |  Loss_1: (0.0443) | Acc_1: (98.45%) (44231/44928)\n",
      "Epoch: 120 | Batch_idx: 360 |  Loss_1: (0.0443) | Acc_1: (98.44%) (45487/46208)\n",
      "Epoch: 120 | Batch_idx: 370 |  Loss_1: (0.0442) | Acc_1: (98.44%) (46748/47488)\n",
      "Epoch: 120 | Batch_idx: 380 |  Loss_1: (0.0445) | Acc_1: (98.45%) (48010/48768)\n",
      "Epoch: 120 | Batch_idx: 390 |  Loss_1: (0.0444) | Acc_1: (98.45%) (49225/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4859) | Acc: (91.19%) (9119/10000)\n",
      "Epoch: 121 | Batch_idx: 0 |  Loss_1: (0.0420) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 121 | Batch_idx: 10 |  Loss_1: (0.0472) | Acc_1: (98.58%) (1388/1408)\n",
      "Epoch: 121 | Batch_idx: 20 |  Loss_1: (0.0399) | Acc_1: (98.77%) (2655/2688)\n",
      "Epoch: 121 | Batch_idx: 30 |  Loss_1: (0.0427) | Acc_1: (98.66%) (3915/3968)\n",
      "Epoch: 121 | Batch_idx: 40 |  Loss_1: (0.0434) | Acc_1: (98.65%) (5177/5248)\n",
      "Epoch: 121 | Batch_idx: 50 |  Loss_1: (0.0458) | Acc_1: (98.53%) (6432/6528)\n",
      "Epoch: 121 | Batch_idx: 60 |  Loss_1: (0.0471) | Acc_1: (98.46%) (7688/7808)\n",
      "Epoch: 121 | Batch_idx: 70 |  Loss_1: (0.0460) | Acc_1: (98.50%) (8952/9088)\n",
      "Epoch: 121 | Batch_idx: 80 |  Loss_1: (0.0442) | Acc_1: (98.53%) (10216/10368)\n",
      "Epoch: 121 | Batch_idx: 90 |  Loss_1: (0.0424) | Acc_1: (98.59%) (11484/11648)\n",
      "Epoch: 121 | Batch_idx: 100 |  Loss_1: (0.0434) | Acc_1: (98.53%) (12738/12928)\n",
      "Epoch: 121 | Batch_idx: 110 |  Loss_1: (0.0434) | Acc_1: (98.51%) (13996/14208)\n",
      "Epoch: 121 | Batch_idx: 120 |  Loss_1: (0.0437) | Acc_1: (98.50%) (15255/15488)\n",
      "Epoch: 121 | Batch_idx: 130 |  Loss_1: (0.0433) | Acc_1: (98.49%) (16515/16768)\n",
      "Epoch: 121 | Batch_idx: 140 |  Loss_1: (0.0425) | Acc_1: (98.53%) (17782/18048)\n",
      "Epoch: 121 | Batch_idx: 150 |  Loss_1: (0.0431) | Acc_1: (98.49%) (19037/19328)\n",
      "Epoch: 121 | Batch_idx: 160 |  Loss_1: (0.0429) | Acc_1: (98.51%) (20301/20608)\n",
      "Epoch: 121 | Batch_idx: 170 |  Loss_1: (0.0434) | Acc_1: (98.52%) (21564/21888)\n",
      "Epoch: 121 | Batch_idx: 180 |  Loss_1: (0.0432) | Acc_1: (98.55%) (22831/23168)\n",
      "Epoch: 121 | Batch_idx: 190 |  Loss_1: (0.0433) | Acc_1: (98.54%) (24092/24448)\n",
      "Epoch: 121 | Batch_idx: 200 |  Loss_1: (0.0436) | Acc_1: (98.52%) (25346/25728)\n",
      "Epoch: 121 | Batch_idx: 210 |  Loss_1: (0.0441) | Acc_1: (98.49%) (26601/27008)\n",
      "Epoch: 121 | Batch_idx: 220 |  Loss_1: (0.0437) | Acc_1: (98.51%) (27867/28288)\n",
      "Epoch: 121 | Batch_idx: 230 |  Loss_1: (0.0433) | Acc_1: (98.53%) (29132/29568)\n",
      "Epoch: 121 | Batch_idx: 240 |  Loss_1: (0.0428) | Acc_1: (98.54%) (30398/30848)\n",
      "Epoch: 121 | Batch_idx: 250 |  Loss_1: (0.0429) | Acc_1: (98.54%) (31660/32128)\n",
      "Epoch: 121 | Batch_idx: 260 |  Loss_1: (0.0427) | Acc_1: (98.56%) (32926/33408)\n",
      "Epoch: 121 | Batch_idx: 270 |  Loss_1: (0.0425) | Acc_1: (98.55%) (34186/34688)\n",
      "Epoch: 121 | Batch_idx: 280 |  Loss_1: (0.0422) | Acc_1: (98.56%) (35451/35968)\n",
      "Epoch: 121 | Batch_idx: 290 |  Loss_1: (0.0425) | Acc_1: (98.56%) (36711/37248)\n",
      "Epoch: 121 | Batch_idx: 300 |  Loss_1: (0.0421) | Acc_1: (98.58%) (37979/38528)\n",
      "Epoch: 121 | Batch_idx: 310 |  Loss_1: (0.0417) | Acc_1: (98.59%) (39248/39808)\n",
      "Epoch: 121 | Batch_idx: 320 |  Loss_1: (0.0414) | Acc_1: (98.60%) (40513/41088)\n",
      "Epoch: 121 | Batch_idx: 330 |  Loss_1: (0.0414) | Acc_1: (98.59%) (41772/42368)\n",
      "Epoch: 121 | Batch_idx: 340 |  Loss_1: (0.0413) | Acc_1: (98.59%) (43034/43648)\n",
      "Epoch: 121 | Batch_idx: 350 |  Loss_1: (0.0410) | Acc_1: (98.61%) (44302/44928)\n",
      "Epoch: 121 | Batch_idx: 360 |  Loss_1: (0.0407) | Acc_1: (98.62%) (45569/46208)\n",
      "Epoch: 121 | Batch_idx: 370 |  Loss_1: (0.0408) | Acc_1: (98.61%) (46828/47488)\n",
      "Epoch: 121 | Batch_idx: 380 |  Loss_1: (0.0407) | Acc_1: (98.62%) (48093/48768)\n",
      "Epoch: 121 | Batch_idx: 390 |  Loss_1: (0.0411) | Acc_1: (98.60%) (49301/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4525) | Acc: (91.04%) (9104/10000)\n",
      "Epoch: 122 | Batch_idx: 0 |  Loss_1: (0.0826) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 122 | Batch_idx: 10 |  Loss_1: (0.0344) | Acc_1: (99.01%) (1394/1408)\n",
      "Epoch: 122 | Batch_idx: 20 |  Loss_1: (0.0387) | Acc_1: (98.77%) (2655/2688)\n",
      "Epoch: 122 | Batch_idx: 30 |  Loss_1: (0.0378) | Acc_1: (98.71%) (3917/3968)\n",
      "Epoch: 122 | Batch_idx: 40 |  Loss_1: (0.0369) | Acc_1: (98.72%) (5181/5248)\n",
      "Epoch: 122 | Batch_idx: 50 |  Loss_1: (0.0383) | Acc_1: (98.68%) (6442/6528)\n",
      "Epoch: 122 | Batch_idx: 60 |  Loss_1: (0.0368) | Acc_1: (98.76%) (7711/7808)\n",
      "Epoch: 122 | Batch_idx: 70 |  Loss_1: (0.0388) | Acc_1: (98.71%) (8971/9088)\n",
      "Epoch: 122 | Batch_idx: 80 |  Loss_1: (0.0395) | Acc_1: (98.65%) (10228/10368)\n",
      "Epoch: 122 | Batch_idx: 90 |  Loss_1: (0.0403) | Acc_1: (98.62%) (11487/11648)\n",
      "Epoch: 122 | Batch_idx: 100 |  Loss_1: (0.0399) | Acc_1: (98.63%) (12751/12928)\n",
      "Epoch: 122 | Batch_idx: 110 |  Loss_1: (0.0406) | Acc_1: (98.58%) (14006/14208)\n",
      "Epoch: 122 | Batch_idx: 120 |  Loss_1: (0.0410) | Acc_1: (98.55%) (15264/15488)\n",
      "Epoch: 122 | Batch_idx: 130 |  Loss_1: (0.0408) | Acc_1: (98.57%) (16528/16768)\n",
      "Epoch: 122 | Batch_idx: 140 |  Loss_1: (0.0396) | Acc_1: (98.61%) (17798/18048)\n",
      "Epoch: 122 | Batch_idx: 150 |  Loss_1: (0.0393) | Acc_1: (98.63%) (19063/19328)\n",
      "Epoch: 122 | Batch_idx: 160 |  Loss_1: (0.0402) | Acc_1: (98.61%) (20321/20608)\n",
      "Epoch: 122 | Batch_idx: 170 |  Loss_1: (0.0401) | Acc_1: (98.62%) (21586/21888)\n",
      "Epoch: 122 | Batch_idx: 180 |  Loss_1: (0.0393) | Acc_1: (98.64%) (22854/23168)\n",
      "Epoch: 122 | Batch_idx: 190 |  Loss_1: (0.0393) | Acc_1: (98.64%) (24116/24448)\n",
      "Epoch: 122 | Batch_idx: 200 |  Loss_1: (0.0391) | Acc_1: (98.65%) (25380/25728)\n",
      "Epoch: 122 | Batch_idx: 210 |  Loss_1: (0.0388) | Acc_1: (98.65%) (26643/27008)\n",
      "Epoch: 122 | Batch_idx: 220 |  Loss_1: (0.0385) | Acc_1: (98.64%) (27904/28288)\n",
      "Epoch: 122 | Batch_idx: 230 |  Loss_1: (0.0382) | Acc_1: (98.65%) (29170/29568)\n",
      "Epoch: 122 | Batch_idx: 240 |  Loss_1: (0.0384) | Acc_1: (98.64%) (30430/30848)\n",
      "Epoch: 122 | Batch_idx: 250 |  Loss_1: (0.0385) | Acc_1: (98.62%) (31685/32128)\n",
      "Epoch: 122 | Batch_idx: 260 |  Loss_1: (0.0383) | Acc_1: (98.63%) (32951/33408)\n",
      "Epoch: 122 | Batch_idx: 270 |  Loss_1: (0.0381) | Acc_1: (98.63%) (34214/34688)\n",
      "Epoch: 122 | Batch_idx: 280 |  Loss_1: (0.0382) | Acc_1: (98.63%) (35474/35968)\n",
      "Epoch: 122 | Batch_idx: 290 |  Loss_1: (0.0380) | Acc_1: (98.64%) (36740/37248)\n",
      "Epoch: 122 | Batch_idx: 300 |  Loss_1: (0.0382) | Acc_1: (98.64%) (38003/38528)\n",
      "Epoch: 122 | Batch_idx: 310 |  Loss_1: (0.0379) | Acc_1: (98.66%) (39275/39808)\n",
      "Epoch: 122 | Batch_idx: 320 |  Loss_1: (0.0378) | Acc_1: (98.67%) (40540/41088)\n",
      "Epoch: 122 | Batch_idx: 330 |  Loss_1: (0.0377) | Acc_1: (98.67%) (41805/42368)\n",
      "Epoch: 122 | Batch_idx: 340 |  Loss_1: (0.0375) | Acc_1: (98.67%) (43067/43648)\n",
      "Epoch: 122 | Batch_idx: 350 |  Loss_1: (0.0376) | Acc_1: (98.68%) (44333/44928)\n",
      "Epoch: 122 | Batch_idx: 360 |  Loss_1: (0.0376) | Acc_1: (98.67%) (45593/46208)\n",
      "Epoch: 122 | Batch_idx: 370 |  Loss_1: (0.0375) | Acc_1: (98.68%) (46859/47488)\n",
      "Epoch: 122 | Batch_idx: 380 |  Loss_1: (0.0374) | Acc_1: (98.68%) (48126/48768)\n",
      "Epoch: 122 | Batch_idx: 390 |  Loss_1: (0.0372) | Acc_1: (98.69%) (49347/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4641) | Acc: (90.96%) (9096/10000)\n",
      "Epoch: 123 | Batch_idx: 0 |  Loss_1: (0.0412) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 123 | Batch_idx: 10 |  Loss_1: (0.0300) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 123 | Batch_idx: 20 |  Loss_1: (0.0325) | Acc_1: (98.66%) (2652/2688)\n",
      "Epoch: 123 | Batch_idx: 30 |  Loss_1: (0.0344) | Acc_1: (98.71%) (3917/3968)\n",
      "Epoch: 123 | Batch_idx: 40 |  Loss_1: (0.0317) | Acc_1: (98.86%) (5188/5248)\n",
      "Epoch: 123 | Batch_idx: 50 |  Loss_1: (0.0327) | Acc_1: (98.84%) (6452/6528)\n",
      "Epoch: 123 | Batch_idx: 60 |  Loss_1: (0.0335) | Acc_1: (98.83%) (7717/7808)\n",
      "Epoch: 123 | Batch_idx: 70 |  Loss_1: (0.0367) | Acc_1: (98.70%) (8970/9088)\n",
      "Epoch: 123 | Batch_idx: 80 |  Loss_1: (0.0373) | Acc_1: (98.69%) (10232/10368)\n",
      "Epoch: 123 | Batch_idx: 90 |  Loss_1: (0.0364) | Acc_1: (98.71%) (11498/11648)\n",
      "Epoch: 123 | Batch_idx: 100 |  Loss_1: (0.0362) | Acc_1: (98.72%) (12762/12928)\n",
      "Epoch: 123 | Batch_idx: 110 |  Loss_1: (0.0373) | Acc_1: (98.66%) (14018/14208)\n",
      "Epoch: 123 | Batch_idx: 120 |  Loss_1: (0.0374) | Acc_1: (98.67%) (15282/15488)\n",
      "Epoch: 123 | Batch_idx: 130 |  Loss_1: (0.0373) | Acc_1: (98.68%) (16547/16768)\n",
      "Epoch: 123 | Batch_idx: 140 |  Loss_1: (0.0390) | Acc_1: (98.65%) (17804/18048)\n",
      "Epoch: 123 | Batch_idx: 150 |  Loss_1: (0.0391) | Acc_1: (98.63%) (19063/19328)\n",
      "Epoch: 123 | Batch_idx: 160 |  Loss_1: (0.0403) | Acc_1: (98.60%) (20320/20608)\n",
      "Epoch: 123 | Batch_idx: 170 |  Loss_1: (0.0397) | Acc_1: (98.62%) (21586/21888)\n",
      "Epoch: 123 | Batch_idx: 180 |  Loss_1: (0.0395) | Acc_1: (98.61%) (22847/23168)\n",
      "Epoch: 123 | Batch_idx: 190 |  Loss_1: (0.0394) | Acc_1: (98.63%) (24113/24448)\n",
      "Epoch: 123 | Batch_idx: 200 |  Loss_1: (0.0398) | Acc_1: (98.60%) (25369/25728)\n",
      "Epoch: 123 | Batch_idx: 210 |  Loss_1: (0.0396) | Acc_1: (98.60%) (26629/27008)\n",
      "Epoch: 123 | Batch_idx: 220 |  Loss_1: (0.0398) | Acc_1: (98.60%) (27892/28288)\n",
      "Epoch: 123 | Batch_idx: 230 |  Loss_1: (0.0402) | Acc_1: (98.58%) (29147/29568)\n",
      "Epoch: 123 | Batch_idx: 240 |  Loss_1: (0.0406) | Acc_1: (98.57%) (30406/30848)\n",
      "Epoch: 123 | Batch_idx: 250 |  Loss_1: (0.0404) | Acc_1: (98.57%) (31670/32128)\n",
      "Epoch: 123 | Batch_idx: 260 |  Loss_1: (0.0406) | Acc_1: (98.59%) (32937/33408)\n",
      "Epoch: 123 | Batch_idx: 270 |  Loss_1: (0.0405) | Acc_1: (98.60%) (34201/34688)\n",
      "Epoch: 123 | Batch_idx: 280 |  Loss_1: (0.0407) | Acc_1: (98.60%) (35464/35968)\n",
      "Epoch: 123 | Batch_idx: 290 |  Loss_1: (0.0404) | Acc_1: (98.61%) (36729/37248)\n",
      "Epoch: 123 | Batch_idx: 300 |  Loss_1: (0.0399) | Acc_1: (98.62%) (37997/38528)\n",
      "Epoch: 123 | Batch_idx: 310 |  Loss_1: (0.0401) | Acc_1: (98.62%) (39260/39808)\n",
      "Epoch: 123 | Batch_idx: 320 |  Loss_1: (0.0404) | Acc_1: (98.62%) (40519/41088)\n",
      "Epoch: 123 | Batch_idx: 330 |  Loss_1: (0.0410) | Acc_1: (98.61%) (41778/42368)\n",
      "Epoch: 123 | Batch_idx: 340 |  Loss_1: (0.0415) | Acc_1: (98.58%) (43030/43648)\n",
      "Epoch: 123 | Batch_idx: 350 |  Loss_1: (0.0416) | Acc_1: (98.58%) (44292/44928)\n",
      "Epoch: 123 | Batch_idx: 360 |  Loss_1: (0.0418) | Acc_1: (98.57%) (45548/46208)\n",
      "Epoch: 123 | Batch_idx: 370 |  Loss_1: (0.0423) | Acc_1: (98.56%) (46806/47488)\n",
      "Epoch: 123 | Batch_idx: 380 |  Loss_1: (0.0425) | Acc_1: (98.56%) (48066/48768)\n",
      "Epoch: 123 | Batch_idx: 390 |  Loss_1: (0.0425) | Acc_1: (98.56%) (49281/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4847) | Acc: (90.53%) (9053/10000)\n",
      "Epoch: 124 | Batch_idx: 0 |  Loss_1: (0.0808) | Acc_1: (96.88%) (124/128)\n",
      "Epoch: 124 | Batch_idx: 10 |  Loss_1: (0.0446) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 124 | Batch_idx: 20 |  Loss_1: (0.0393) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 124 | Batch_idx: 30 |  Loss_1: (0.0447) | Acc_1: (98.29%) (3900/3968)\n",
      "Epoch: 124 | Batch_idx: 40 |  Loss_1: (0.0421) | Acc_1: (98.38%) (5163/5248)\n",
      "Epoch: 124 | Batch_idx: 50 |  Loss_1: (0.0428) | Acc_1: (98.41%) (6424/6528)\n",
      "Epoch: 124 | Batch_idx: 60 |  Loss_1: (0.0452) | Acc_1: (98.35%) (7679/7808)\n",
      "Epoch: 124 | Batch_idx: 70 |  Loss_1: (0.0432) | Acc_1: (98.45%) (8947/9088)\n",
      "Epoch: 124 | Batch_idx: 80 |  Loss_1: (0.0437) | Acc_1: (98.45%) (10207/10368)\n",
      "Epoch: 124 | Batch_idx: 90 |  Loss_1: (0.0449) | Acc_1: (98.45%) (11467/11648)\n",
      "Epoch: 124 | Batch_idx: 100 |  Loss_1: (0.0434) | Acc_1: (98.51%) (12736/12928)\n",
      "Epoch: 124 | Batch_idx: 110 |  Loss_1: (0.0419) | Acc_1: (98.57%) (14005/14208)\n",
      "Epoch: 124 | Batch_idx: 120 |  Loss_1: (0.0405) | Acc_1: (98.60%) (15271/15488)\n",
      "Epoch: 124 | Batch_idx: 130 |  Loss_1: (0.0399) | Acc_1: (98.62%) (16536/16768)\n",
      "Epoch: 124 | Batch_idx: 140 |  Loss_1: (0.0396) | Acc_1: (98.63%) (17800/18048)\n",
      "Epoch: 124 | Batch_idx: 150 |  Loss_1: (0.0400) | Acc_1: (98.65%) (19068/19328)\n",
      "Epoch: 124 | Batch_idx: 160 |  Loss_1: (0.0403) | Acc_1: (98.61%) (20322/20608)\n",
      "Epoch: 124 | Batch_idx: 170 |  Loss_1: (0.0401) | Acc_1: (98.60%) (21582/21888)\n",
      "Epoch: 124 | Batch_idx: 180 |  Loss_1: (0.0406) | Acc_1: (98.58%) (22840/23168)\n",
      "Epoch: 124 | Batch_idx: 190 |  Loss_1: (0.0405) | Acc_1: (98.59%) (24103/24448)\n",
      "Epoch: 124 | Batch_idx: 200 |  Loss_1: (0.0410) | Acc_1: (98.59%) (25365/25728)\n",
      "Epoch: 124 | Batch_idx: 210 |  Loss_1: (0.0412) | Acc_1: (98.57%) (26622/27008)\n",
      "Epoch: 124 | Batch_idx: 220 |  Loss_1: (0.0413) | Acc_1: (98.56%) (27882/28288)\n",
      "Epoch: 124 | Batch_idx: 230 |  Loss_1: (0.0415) | Acc_1: (98.57%) (29145/29568)\n",
      "Epoch: 124 | Batch_idx: 240 |  Loss_1: (0.0425) | Acc_1: (98.54%) (30399/30848)\n",
      "Epoch: 124 | Batch_idx: 250 |  Loss_1: (0.0423) | Acc_1: (98.55%) (31663/32128)\n",
      "Epoch: 124 | Batch_idx: 260 |  Loss_1: (0.0429) | Acc_1: (98.55%) (32923/33408)\n",
      "Epoch: 124 | Batch_idx: 270 |  Loss_1: (0.0431) | Acc_1: (98.54%) (34182/34688)\n",
      "Epoch: 124 | Batch_idx: 280 |  Loss_1: (0.0430) | Acc_1: (98.54%) (35442/35968)\n",
      "Epoch: 124 | Batch_idx: 290 |  Loss_1: (0.0431) | Acc_1: (98.54%) (36703/37248)\n",
      "Epoch: 124 | Batch_idx: 300 |  Loss_1: (0.0426) | Acc_1: (98.56%) (37973/38528)\n",
      "Epoch: 124 | Batch_idx: 310 |  Loss_1: (0.0427) | Acc_1: (98.56%) (39234/39808)\n",
      "Epoch: 124 | Batch_idx: 320 |  Loss_1: (0.0423) | Acc_1: (98.56%) (40498/41088)\n",
      "Epoch: 124 | Batch_idx: 330 |  Loss_1: (0.0422) | Acc_1: (98.56%) (41760/42368)\n",
      "Epoch: 124 | Batch_idx: 340 |  Loss_1: (0.0419) | Acc_1: (98.56%) (43021/43648)\n",
      "Epoch: 124 | Batch_idx: 350 |  Loss_1: (0.0419) | Acc_1: (98.56%) (44281/44928)\n",
      "Epoch: 124 | Batch_idx: 360 |  Loss_1: (0.0418) | Acc_1: (98.56%) (45541/46208)\n",
      "Epoch: 124 | Batch_idx: 370 |  Loss_1: (0.0421) | Acc_1: (98.55%) (46801/47488)\n",
      "Epoch: 124 | Batch_idx: 380 |  Loss_1: (0.0419) | Acc_1: (98.56%) (48064/48768)\n",
      "Epoch: 124 | Batch_idx: 390 |  Loss_1: (0.0415) | Acc_1: (98.57%) (49283/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4620) | Acc: (91.03%) (9103/10000)\n",
      "Epoch: 125 | Batch_idx: 0 |  Loss_1: (0.0149) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 125 | Batch_idx: 10 |  Loss_1: (0.0330) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 125 | Batch_idx: 20 |  Loss_1: (0.0406) | Acc_1: (98.59%) (2650/2688)\n",
      "Epoch: 125 | Batch_idx: 30 |  Loss_1: (0.0396) | Acc_1: (98.61%) (3913/3968)\n",
      "Epoch: 125 | Batch_idx: 40 |  Loss_1: (0.0371) | Acc_1: (98.67%) (5178/5248)\n",
      "Epoch: 125 | Batch_idx: 50 |  Loss_1: (0.0377) | Acc_1: (98.70%) (6443/6528)\n",
      "Epoch: 125 | Batch_idx: 60 |  Loss_1: (0.0379) | Acc_1: (98.73%) (7709/7808)\n",
      "Epoch: 125 | Batch_idx: 70 |  Loss_1: (0.0376) | Acc_1: (98.75%) (8974/9088)\n",
      "Epoch: 125 | Batch_idx: 80 |  Loss_1: (0.0387) | Acc_1: (98.71%) (10234/10368)\n",
      "Epoch: 125 | Batch_idx: 90 |  Loss_1: (0.0385) | Acc_1: (98.72%) (11499/11648)\n",
      "Epoch: 125 | Batch_idx: 100 |  Loss_1: (0.0379) | Acc_1: (98.75%) (12767/12928)\n",
      "Epoch: 125 | Batch_idx: 110 |  Loss_1: (0.0370) | Acc_1: (98.79%) (14036/14208)\n",
      "Epoch: 125 | Batch_idx: 120 |  Loss_1: (0.0372) | Acc_1: (98.79%) (15300/15488)\n",
      "Epoch: 125 | Batch_idx: 130 |  Loss_1: (0.0373) | Acc_1: (98.77%) (16561/16768)\n",
      "Epoch: 125 | Batch_idx: 140 |  Loss_1: (0.0381) | Acc_1: (98.75%) (17823/18048)\n",
      "Epoch: 125 | Batch_idx: 150 |  Loss_1: (0.0375) | Acc_1: (98.77%) (19090/19328)\n",
      "Epoch: 125 | Batch_idx: 160 |  Loss_1: (0.0367) | Acc_1: (98.79%) (20359/20608)\n",
      "Epoch: 125 | Batch_idx: 170 |  Loss_1: (0.0361) | Acc_1: (98.82%) (21629/21888)\n",
      "Epoch: 125 | Batch_idx: 180 |  Loss_1: (0.0357) | Acc_1: (98.83%) (22896/23168)\n",
      "Epoch: 125 | Batch_idx: 190 |  Loss_1: (0.0360) | Acc_1: (98.83%) (24161/24448)\n",
      "Epoch: 125 | Batch_idx: 200 |  Loss_1: (0.0363) | Acc_1: (98.81%) (25421/25728)\n",
      "Epoch: 125 | Batch_idx: 210 |  Loss_1: (0.0364) | Acc_1: (98.81%) (26687/27008)\n",
      "Epoch: 125 | Batch_idx: 220 |  Loss_1: (0.0364) | Acc_1: (98.81%) (27952/28288)\n",
      "Epoch: 125 | Batch_idx: 230 |  Loss_1: (0.0365) | Acc_1: (98.81%) (29217/29568)\n",
      "Epoch: 125 | Batch_idx: 240 |  Loss_1: (0.0364) | Acc_1: (98.81%) (30482/30848)\n",
      "Epoch: 125 | Batch_idx: 250 |  Loss_1: (0.0363) | Acc_1: (98.82%) (31749/32128)\n",
      "Epoch: 125 | Batch_idx: 260 |  Loss_1: (0.0364) | Acc_1: (98.82%) (33013/33408)\n",
      "Epoch: 125 | Batch_idx: 270 |  Loss_1: (0.0368) | Acc_1: (98.82%) (34277/34688)\n",
      "Epoch: 125 | Batch_idx: 280 |  Loss_1: (0.0363) | Acc_1: (98.84%) (35550/35968)\n",
      "Epoch: 125 | Batch_idx: 290 |  Loss_1: (0.0358) | Acc_1: (98.85%) (36819/37248)\n",
      "Epoch: 125 | Batch_idx: 300 |  Loss_1: (0.0358) | Acc_1: (98.85%) (38084/38528)\n",
      "Epoch: 125 | Batch_idx: 310 |  Loss_1: (0.0354) | Acc_1: (98.85%) (39349/39808)\n",
      "Epoch: 125 | Batch_idx: 320 |  Loss_1: (0.0357) | Acc_1: (98.83%) (40608/41088)\n",
      "Epoch: 125 | Batch_idx: 330 |  Loss_1: (0.0357) | Acc_1: (98.82%) (41869/42368)\n",
      "Epoch: 125 | Batch_idx: 340 |  Loss_1: (0.0357) | Acc_1: (98.82%) (43134/43648)\n",
      "Epoch: 125 | Batch_idx: 350 |  Loss_1: (0.0355) | Acc_1: (98.83%) (44401/44928)\n",
      "Epoch: 125 | Batch_idx: 360 |  Loss_1: (0.0353) | Acc_1: (98.83%) (45667/46208)\n",
      "Epoch: 125 | Batch_idx: 370 |  Loss_1: (0.0354) | Acc_1: (98.82%) (46927/47488)\n",
      "Epoch: 125 | Batch_idx: 380 |  Loss_1: (0.0356) | Acc_1: (98.80%) (48185/48768)\n",
      "Epoch: 125 | Batch_idx: 390 |  Loss_1: (0.0356) | Acc_1: (98.81%) (49403/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4969) | Acc: (90.58%) (9058/10000)\n",
      "Epoch: 126 | Batch_idx: 0 |  Loss_1: (0.0147) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 126 | Batch_idx: 10 |  Loss_1: (0.0317) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 126 | Batch_idx: 20 |  Loss_1: (0.0309) | Acc_1: (99.03%) (2662/2688)\n",
      "Epoch: 126 | Batch_idx: 30 |  Loss_1: (0.0337) | Acc_1: (98.89%) (3924/3968)\n",
      "Epoch: 126 | Batch_idx: 40 |  Loss_1: (0.0323) | Acc_1: (98.91%) (5191/5248)\n",
      "Epoch: 126 | Batch_idx: 50 |  Loss_1: (0.0312) | Acc_1: (98.90%) (6456/6528)\n",
      "Epoch: 126 | Batch_idx: 60 |  Loss_1: (0.0326) | Acc_1: (98.89%) (7721/7808)\n",
      "Epoch: 126 | Batch_idx: 70 |  Loss_1: (0.0326) | Acc_1: (98.90%) (8988/9088)\n",
      "Epoch: 126 | Batch_idx: 80 |  Loss_1: (0.0348) | Acc_1: (98.87%) (10251/10368)\n",
      "Epoch: 126 | Batch_idx: 90 |  Loss_1: (0.0346) | Acc_1: (98.90%) (11520/11648)\n",
      "Epoch: 126 | Batch_idx: 100 |  Loss_1: (0.0358) | Acc_1: (98.85%) (12779/12928)\n",
      "Epoch: 126 | Batch_idx: 110 |  Loss_1: (0.0364) | Acc_1: (98.77%) (14033/14208)\n",
      "Epoch: 126 | Batch_idx: 120 |  Loss_1: (0.0363) | Acc_1: (98.76%) (15296/15488)\n",
      "Epoch: 126 | Batch_idx: 130 |  Loss_1: (0.0375) | Acc_1: (98.72%) (16553/16768)\n",
      "Epoch: 126 | Batch_idx: 140 |  Loss_1: (0.0381) | Acc_1: (98.69%) (17812/18048)\n",
      "Epoch: 126 | Batch_idx: 150 |  Loss_1: (0.0381) | Acc_1: (98.70%) (19076/19328)\n",
      "Epoch: 126 | Batch_idx: 160 |  Loss_1: (0.0380) | Acc_1: (98.71%) (20342/20608)\n",
      "Epoch: 126 | Batch_idx: 170 |  Loss_1: (0.0393) | Acc_1: (98.64%) (21590/21888)\n",
      "Epoch: 126 | Batch_idx: 180 |  Loss_1: (0.0397) | Acc_1: (98.63%) (22851/23168)\n",
      "Epoch: 126 | Batch_idx: 190 |  Loss_1: (0.0398) | Acc_1: (98.63%) (24112/24448)\n",
      "Epoch: 126 | Batch_idx: 200 |  Loss_1: (0.0395) | Acc_1: (98.63%) (25376/25728)\n",
      "Epoch: 126 | Batch_idx: 210 |  Loss_1: (0.0389) | Acc_1: (98.65%) (26643/27008)\n",
      "Epoch: 126 | Batch_idx: 220 |  Loss_1: (0.0389) | Acc_1: (98.64%) (27904/28288)\n",
      "Epoch: 126 | Batch_idx: 230 |  Loss_1: (0.0391) | Acc_1: (98.62%) (29160/29568)\n",
      "Epoch: 126 | Batch_idx: 240 |  Loss_1: (0.0395) | Acc_1: (98.61%) (30420/30848)\n",
      "Epoch: 126 | Batch_idx: 250 |  Loss_1: (0.0390) | Acc_1: (98.62%) (31686/32128)\n",
      "Epoch: 126 | Batch_idx: 260 |  Loss_1: (0.0393) | Acc_1: (98.61%) (32942/33408)\n",
      "Epoch: 126 | Batch_idx: 270 |  Loss_1: (0.0390) | Acc_1: (98.61%) (34207/34688)\n",
      "Epoch: 126 | Batch_idx: 280 |  Loss_1: (0.0396) | Acc_1: (98.60%) (35466/35968)\n",
      "Epoch: 126 | Batch_idx: 290 |  Loss_1: (0.0394) | Acc_1: (98.62%) (36733/37248)\n",
      "Epoch: 126 | Batch_idx: 300 |  Loss_1: (0.0395) | Acc_1: (98.62%) (37995/38528)\n",
      "Epoch: 126 | Batch_idx: 310 |  Loss_1: (0.0392) | Acc_1: (98.63%) (39263/39808)\n",
      "Epoch: 126 | Batch_idx: 320 |  Loss_1: (0.0400) | Acc_1: (98.62%) (40520/41088)\n",
      "Epoch: 126 | Batch_idx: 330 |  Loss_1: (0.0397) | Acc_1: (98.63%) (41786/42368)\n",
      "Epoch: 126 | Batch_idx: 340 |  Loss_1: (0.0396) | Acc_1: (98.63%) (43050/43648)\n",
      "Epoch: 126 | Batch_idx: 350 |  Loss_1: (0.0399) | Acc_1: (98.64%) (44315/44928)\n",
      "Epoch: 126 | Batch_idx: 360 |  Loss_1: (0.0401) | Acc_1: (98.63%) (45574/46208)\n",
      "Epoch: 126 | Batch_idx: 370 |  Loss_1: (0.0402) | Acc_1: (98.63%) (46838/47488)\n",
      "Epoch: 126 | Batch_idx: 380 |  Loss_1: (0.0402) | Acc_1: (98.63%) (48099/48768)\n",
      "Epoch: 126 | Batch_idx: 390 |  Loss_1: (0.0404) | Acc_1: (98.62%) (49311/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4660) | Acc: (90.87%) (9087/10000)\n",
      "Epoch: 127 | Batch_idx: 0 |  Loss_1: (0.0243) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 127 | Batch_idx: 10 |  Loss_1: (0.0317) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 127 | Batch_idx: 20 |  Loss_1: (0.0386) | Acc_1: (98.85%) (2657/2688)\n",
      "Epoch: 127 | Batch_idx: 30 |  Loss_1: (0.0362) | Acc_1: (98.92%) (3925/3968)\n",
      "Epoch: 127 | Batch_idx: 40 |  Loss_1: (0.0345) | Acc_1: (98.97%) (5194/5248)\n",
      "Epoch: 127 | Batch_idx: 50 |  Loss_1: (0.0387) | Acc_1: (98.77%) (6448/6528)\n",
      "Epoch: 127 | Batch_idx: 60 |  Loss_1: (0.0381) | Acc_1: (98.77%) (7712/7808)\n",
      "Epoch: 127 | Batch_idx: 70 |  Loss_1: (0.0378) | Acc_1: (98.77%) (8976/9088)\n",
      "Epoch: 127 | Batch_idx: 80 |  Loss_1: (0.0391) | Acc_1: (98.72%) (10235/10368)\n",
      "Epoch: 127 | Batch_idx: 90 |  Loss_1: (0.0389) | Acc_1: (98.69%) (11495/11648)\n",
      "Epoch: 127 | Batch_idx: 100 |  Loss_1: (0.0385) | Acc_1: (98.71%) (12761/12928)\n",
      "Epoch: 127 | Batch_idx: 110 |  Loss_1: (0.0372) | Acc_1: (98.73%) (14027/14208)\n",
      "Epoch: 127 | Batch_idx: 120 |  Loss_1: (0.0374) | Acc_1: (98.74%) (15293/15488)\n",
      "Epoch: 127 | Batch_idx: 130 |  Loss_1: (0.0375) | Acc_1: (98.70%) (16550/16768)\n",
      "Epoch: 127 | Batch_idx: 140 |  Loss_1: (0.0370) | Acc_1: (98.72%) (17817/18048)\n",
      "Epoch: 127 | Batch_idx: 150 |  Loss_1: (0.0370) | Acc_1: (98.74%) (19085/19328)\n",
      "Epoch: 127 | Batch_idx: 160 |  Loss_1: (0.0372) | Acc_1: (98.73%) (20346/20608)\n",
      "Epoch: 127 | Batch_idx: 170 |  Loss_1: (0.0376) | Acc_1: (98.73%) (21610/21888)\n",
      "Epoch: 127 | Batch_idx: 180 |  Loss_1: (0.0377) | Acc_1: (98.71%) (22869/23168)\n",
      "Epoch: 127 | Batch_idx: 190 |  Loss_1: (0.0376) | Acc_1: (98.70%) (24129/24448)\n",
      "Epoch: 127 | Batch_idx: 200 |  Loss_1: (0.0375) | Acc_1: (98.67%) (25387/25728)\n",
      "Epoch: 127 | Batch_idx: 210 |  Loss_1: (0.0370) | Acc_1: (98.70%) (26656/27008)\n",
      "Epoch: 127 | Batch_idx: 220 |  Loss_1: (0.0366) | Acc_1: (98.71%) (27923/28288)\n",
      "Epoch: 127 | Batch_idx: 230 |  Loss_1: (0.0371) | Acc_1: (98.70%) (29183/29568)\n",
      "Epoch: 127 | Batch_idx: 240 |  Loss_1: (0.0369) | Acc_1: (98.70%) (30448/30848)\n",
      "Epoch: 127 | Batch_idx: 250 |  Loss_1: (0.0370) | Acc_1: (98.70%) (31711/32128)\n",
      "Epoch: 127 | Batch_idx: 260 |  Loss_1: (0.0371) | Acc_1: (98.70%) (32975/33408)\n",
      "Epoch: 127 | Batch_idx: 270 |  Loss_1: (0.0372) | Acc_1: (98.71%) (34239/34688)\n",
      "Epoch: 127 | Batch_idx: 280 |  Loss_1: (0.0378) | Acc_1: (98.69%) (35497/35968)\n",
      "Epoch: 127 | Batch_idx: 290 |  Loss_1: (0.0378) | Acc_1: (98.69%) (36761/37248)\n",
      "Epoch: 127 | Batch_idx: 300 |  Loss_1: (0.0380) | Acc_1: (98.69%) (38024/38528)\n",
      "Epoch: 127 | Batch_idx: 310 |  Loss_1: (0.0375) | Acc_1: (98.71%) (39294/39808)\n",
      "Epoch: 127 | Batch_idx: 320 |  Loss_1: (0.0373) | Acc_1: (98.71%) (40559/41088)\n",
      "Epoch: 127 | Batch_idx: 330 |  Loss_1: (0.0377) | Acc_1: (98.70%) (41819/42368)\n",
      "Epoch: 127 | Batch_idx: 340 |  Loss_1: (0.0374) | Acc_1: (98.71%) (43087/43648)\n",
      "Epoch: 127 | Batch_idx: 350 |  Loss_1: (0.0373) | Acc_1: (98.72%) (44351/44928)\n",
      "Epoch: 127 | Batch_idx: 360 |  Loss_1: (0.0372) | Acc_1: (98.72%) (45615/46208)\n",
      "Epoch: 127 | Batch_idx: 370 |  Loss_1: (0.0369) | Acc_1: (98.72%) (46882/47488)\n",
      "Epoch: 127 | Batch_idx: 380 |  Loss_1: (0.0368) | Acc_1: (98.73%) (48151/48768)\n",
      "Epoch: 127 | Batch_idx: 390 |  Loss_1: (0.0369) | Acc_1: (98.73%) (49366/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4640) | Acc: (91.28%) (9128/10000)\n",
      "Epoch: 128 | Batch_idx: 0 |  Loss_1: (0.0305) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 128 | Batch_idx: 10 |  Loss_1: (0.0308) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 128 | Batch_idx: 20 |  Loss_1: (0.0299) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 128 | Batch_idx: 30 |  Loss_1: (0.0325) | Acc_1: (98.84%) (3922/3968)\n",
      "Epoch: 128 | Batch_idx: 40 |  Loss_1: (0.0302) | Acc_1: (98.93%) (5192/5248)\n",
      "Epoch: 128 | Batch_idx: 50 |  Loss_1: (0.0298) | Acc_1: (98.93%) (6458/6528)\n",
      "Epoch: 128 | Batch_idx: 60 |  Loss_1: (0.0307) | Acc_1: (98.90%) (7722/7808)\n",
      "Epoch: 128 | Batch_idx: 70 |  Loss_1: (0.0324) | Acc_1: (98.84%) (8983/9088)\n",
      "Epoch: 128 | Batch_idx: 80 |  Loss_1: (0.0324) | Acc_1: (98.83%) (10247/10368)\n",
      "Epoch: 128 | Batch_idx: 90 |  Loss_1: (0.0325) | Acc_1: (98.83%) (11512/11648)\n",
      "Epoch: 128 | Batch_idx: 100 |  Loss_1: (0.0316) | Acc_1: (98.86%) (12780/12928)\n",
      "Epoch: 128 | Batch_idx: 110 |  Loss_1: (0.0315) | Acc_1: (98.88%) (14049/14208)\n",
      "Epoch: 128 | Batch_idx: 120 |  Loss_1: (0.0319) | Acc_1: (98.86%) (15312/15488)\n",
      "Epoch: 128 | Batch_idx: 130 |  Loss_1: (0.0314) | Acc_1: (98.88%) (16581/16768)\n",
      "Epoch: 128 | Batch_idx: 140 |  Loss_1: (0.0315) | Acc_1: (98.88%) (17846/18048)\n",
      "Epoch: 128 | Batch_idx: 150 |  Loss_1: (0.0313) | Acc_1: (98.88%) (19112/19328)\n",
      "Epoch: 128 | Batch_idx: 160 |  Loss_1: (0.0314) | Acc_1: (98.88%) (20378/20608)\n",
      "Epoch: 128 | Batch_idx: 170 |  Loss_1: (0.0313) | Acc_1: (98.90%) (21647/21888)\n",
      "Epoch: 128 | Batch_idx: 180 |  Loss_1: (0.0312) | Acc_1: (98.90%) (22914/23168)\n",
      "Epoch: 128 | Batch_idx: 190 |  Loss_1: (0.0313) | Acc_1: (98.91%) (24182/24448)\n",
      "Epoch: 128 | Batch_idx: 200 |  Loss_1: (0.0307) | Acc_1: (98.94%) (25455/25728)\n",
      "Epoch: 128 | Batch_idx: 210 |  Loss_1: (0.0310) | Acc_1: (98.92%) (26717/27008)\n",
      "Epoch: 128 | Batch_idx: 220 |  Loss_1: (0.0312) | Acc_1: (98.91%) (27979/28288)\n",
      "Epoch: 128 | Batch_idx: 230 |  Loss_1: (0.0316) | Acc_1: (98.88%) (29238/29568)\n",
      "Epoch: 128 | Batch_idx: 240 |  Loss_1: (0.0316) | Acc_1: (98.89%) (30505/30848)\n",
      "Epoch: 128 | Batch_idx: 250 |  Loss_1: (0.0323) | Acc_1: (98.85%) (31759/32128)\n",
      "Epoch: 128 | Batch_idx: 260 |  Loss_1: (0.0331) | Acc_1: (98.85%) (33023/33408)\n",
      "Epoch: 128 | Batch_idx: 270 |  Loss_1: (0.0327) | Acc_1: (98.86%) (34292/34688)\n",
      "Epoch: 128 | Batch_idx: 280 |  Loss_1: (0.0327) | Acc_1: (98.86%) (35558/35968)\n",
      "Epoch: 128 | Batch_idx: 290 |  Loss_1: (0.0328) | Acc_1: (98.85%) (36819/37248)\n",
      "Epoch: 128 | Batch_idx: 300 |  Loss_1: (0.0332) | Acc_1: (98.83%) (38078/38528)\n",
      "Epoch: 128 | Batch_idx: 310 |  Loss_1: (0.0336) | Acc_1: (98.83%) (39341/39808)\n",
      "Epoch: 128 | Batch_idx: 320 |  Loss_1: (0.0341) | Acc_1: (98.81%) (40601/41088)\n",
      "Epoch: 128 | Batch_idx: 330 |  Loss_1: (0.0345) | Acc_1: (98.80%) (41859/42368)\n",
      "Epoch: 128 | Batch_idx: 340 |  Loss_1: (0.0356) | Acc_1: (98.76%) (43106/43648)\n",
      "Epoch: 128 | Batch_idx: 350 |  Loss_1: (0.0357) | Acc_1: (98.76%) (44373/44928)\n",
      "Epoch: 128 | Batch_idx: 360 |  Loss_1: (0.0359) | Acc_1: (98.75%) (45632/46208)\n",
      "Epoch: 128 | Batch_idx: 370 |  Loss_1: (0.0364) | Acc_1: (98.73%) (46886/47488)\n",
      "Epoch: 128 | Batch_idx: 380 |  Loss_1: (0.0368) | Acc_1: (98.72%) (48142/48768)\n",
      "Epoch: 128 | Batch_idx: 390 |  Loss_1: (0.0368) | Acc_1: (98.71%) (49353/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4892) | Acc: (90.69%) (9069/10000)\n",
      "Epoch: 129 | Batch_idx: 0 |  Loss_1: (0.0490) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 129 | Batch_idx: 10 |  Loss_1: (0.0464) | Acc_1: (98.22%) (1383/1408)\n",
      "Epoch: 129 | Batch_idx: 20 |  Loss_1: (0.0399) | Acc_1: (98.51%) (2648/2688)\n",
      "Epoch: 129 | Batch_idx: 30 |  Loss_1: (0.0350) | Acc_1: (98.71%) (3917/3968)\n",
      "Epoch: 129 | Batch_idx: 40 |  Loss_1: (0.0357) | Acc_1: (98.69%) (5179/5248)\n",
      "Epoch: 129 | Batch_idx: 50 |  Loss_1: (0.0353) | Acc_1: (98.68%) (6442/6528)\n",
      "Epoch: 129 | Batch_idx: 60 |  Loss_1: (0.0384) | Acc_1: (98.60%) (7699/7808)\n",
      "Epoch: 129 | Batch_idx: 70 |  Loss_1: (0.0393) | Acc_1: (98.62%) (8963/9088)\n",
      "Epoch: 129 | Batch_idx: 80 |  Loss_1: (0.0403) | Acc_1: (98.58%) (10221/10368)\n",
      "Epoch: 129 | Batch_idx: 90 |  Loss_1: (0.0418) | Acc_1: (98.55%) (11479/11648)\n",
      "Epoch: 129 | Batch_idx: 100 |  Loss_1: (0.0427) | Acc_1: (98.55%) (12741/12928)\n",
      "Epoch: 129 | Batch_idx: 110 |  Loss_1: (0.0417) | Acc_1: (98.60%) (14009/14208)\n",
      "Epoch: 129 | Batch_idx: 120 |  Loss_1: (0.0429) | Acc_1: (98.55%) (15264/15488)\n",
      "Epoch: 129 | Batch_idx: 130 |  Loss_1: (0.0431) | Acc_1: (98.54%) (16524/16768)\n",
      "Epoch: 129 | Batch_idx: 140 |  Loss_1: (0.0424) | Acc_1: (98.59%) (17793/18048)\n",
      "Epoch: 129 | Batch_idx: 150 |  Loss_1: (0.0415) | Acc_1: (98.65%) (19067/19328)\n",
      "Epoch: 129 | Batch_idx: 160 |  Loss_1: (0.0406) | Acc_1: (98.69%) (20338/20608)\n",
      "Epoch: 129 | Batch_idx: 170 |  Loss_1: (0.0398) | Acc_1: (98.70%) (21604/21888)\n",
      "Epoch: 129 | Batch_idx: 180 |  Loss_1: (0.0395) | Acc_1: (98.72%) (22872/23168)\n",
      "Epoch: 129 | Batch_idx: 190 |  Loss_1: (0.0397) | Acc_1: (98.70%) (24131/24448)\n",
      "Epoch: 129 | Batch_idx: 200 |  Loss_1: (0.0396) | Acc_1: (98.69%) (25391/25728)\n",
      "Epoch: 129 | Batch_idx: 210 |  Loss_1: (0.0399) | Acc_1: (98.69%) (26653/27008)\n",
      "Epoch: 129 | Batch_idx: 220 |  Loss_1: (0.0395) | Acc_1: (98.70%) (27919/28288)\n",
      "Epoch: 129 | Batch_idx: 230 |  Loss_1: (0.0393) | Acc_1: (98.70%) (29183/29568)\n",
      "Epoch: 129 | Batch_idx: 240 |  Loss_1: (0.0396) | Acc_1: (98.67%) (30437/30848)\n",
      "Epoch: 129 | Batch_idx: 250 |  Loss_1: (0.0397) | Acc_1: (98.67%) (31700/32128)\n",
      "Epoch: 129 | Batch_idx: 260 |  Loss_1: (0.0401) | Acc_1: (98.64%) (32952/33408)\n",
      "Epoch: 129 | Batch_idx: 270 |  Loss_1: (0.0395) | Acc_1: (98.66%) (34224/34688)\n",
      "Epoch: 129 | Batch_idx: 280 |  Loss_1: (0.0393) | Acc_1: (98.68%) (35493/35968)\n",
      "Epoch: 129 | Batch_idx: 290 |  Loss_1: (0.0395) | Acc_1: (98.67%) (36754/37248)\n",
      "Epoch: 129 | Batch_idx: 300 |  Loss_1: (0.0399) | Acc_1: (98.66%) (38012/38528)\n",
      "Epoch: 129 | Batch_idx: 310 |  Loss_1: (0.0398) | Acc_1: (98.67%) (39277/39808)\n",
      "Epoch: 129 | Batch_idx: 320 |  Loss_1: (0.0400) | Acc_1: (98.66%) (40536/41088)\n",
      "Epoch: 129 | Batch_idx: 330 |  Loss_1: (0.0400) | Acc_1: (98.67%) (41805/42368)\n",
      "Epoch: 129 | Batch_idx: 340 |  Loss_1: (0.0396) | Acc_1: (98.69%) (43077/43648)\n",
      "Epoch: 129 | Batch_idx: 350 |  Loss_1: (0.0396) | Acc_1: (98.69%) (44340/44928)\n",
      "Epoch: 129 | Batch_idx: 360 |  Loss_1: (0.0394) | Acc_1: (98.70%) (45607/46208)\n",
      "Epoch: 129 | Batch_idx: 370 |  Loss_1: (0.0396) | Acc_1: (98.69%) (46864/47488)\n",
      "Epoch: 129 | Batch_idx: 380 |  Loss_1: (0.0394) | Acc_1: (98.70%) (48132/48768)\n",
      "Epoch: 129 | Batch_idx: 390 |  Loss_1: (0.0396) | Acc_1: (98.69%) (49343/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4546) | Acc: (91.17%) (9117/10000)\n",
      "Epoch: 130 | Batch_idx: 0 |  Loss_1: (0.0225) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 130 | Batch_idx: 10 |  Loss_1: (0.0213) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 130 | Batch_idx: 20 |  Loss_1: (0.0327) | Acc_1: (98.77%) (2655/2688)\n",
      "Epoch: 130 | Batch_idx: 30 |  Loss_1: (0.0325) | Acc_1: (98.77%) (3919/3968)\n",
      "Epoch: 130 | Batch_idx: 40 |  Loss_1: (0.0349) | Acc_1: (98.72%) (5181/5248)\n",
      "Epoch: 130 | Batch_idx: 50 |  Loss_1: (0.0353) | Acc_1: (98.81%) (6450/6528)\n",
      "Epoch: 130 | Batch_idx: 60 |  Loss_1: (0.0371) | Acc_1: (98.73%) (7709/7808)\n",
      "Epoch: 130 | Batch_idx: 70 |  Loss_1: (0.0374) | Acc_1: (98.77%) (8976/9088)\n",
      "Epoch: 130 | Batch_idx: 80 |  Loss_1: (0.0361) | Acc_1: (98.82%) (10246/10368)\n",
      "Epoch: 130 | Batch_idx: 90 |  Loss_1: (0.0359) | Acc_1: (98.85%) (11514/11648)\n",
      "Epoch: 130 | Batch_idx: 100 |  Loss_1: (0.0355) | Acc_1: (98.86%) (12780/12928)\n",
      "Epoch: 130 | Batch_idx: 110 |  Loss_1: (0.0364) | Acc_1: (98.82%) (14040/14208)\n",
      "Epoch: 130 | Batch_idx: 120 |  Loss_1: (0.0356) | Acc_1: (98.84%) (15308/15488)\n",
      "Epoch: 130 | Batch_idx: 130 |  Loss_1: (0.0350) | Acc_1: (98.85%) (16575/16768)\n",
      "Epoch: 130 | Batch_idx: 140 |  Loss_1: (0.0361) | Acc_1: (98.78%) (17828/18048)\n",
      "Epoch: 130 | Batch_idx: 150 |  Loss_1: (0.0361) | Acc_1: (98.77%) (19091/19328)\n",
      "Epoch: 130 | Batch_idx: 160 |  Loss_1: (0.0360) | Acc_1: (98.76%) (20352/20608)\n",
      "Epoch: 130 | Batch_idx: 170 |  Loss_1: (0.0359) | Acc_1: (98.78%) (21620/21888)\n",
      "Epoch: 130 | Batch_idx: 180 |  Loss_1: (0.0356) | Acc_1: (98.78%) (22886/23168)\n",
      "Epoch: 130 | Batch_idx: 190 |  Loss_1: (0.0364) | Acc_1: (98.75%) (24142/24448)\n",
      "Epoch: 130 | Batch_idx: 200 |  Loss_1: (0.0373) | Acc_1: (98.73%) (25400/25728)\n",
      "Epoch: 130 | Batch_idx: 210 |  Loss_1: (0.0374) | Acc_1: (98.73%) (26664/27008)\n",
      "Epoch: 130 | Batch_idx: 220 |  Loss_1: (0.0379) | Acc_1: (98.70%) (27921/28288)\n",
      "Epoch: 130 | Batch_idx: 230 |  Loss_1: (0.0378) | Acc_1: (98.71%) (29187/29568)\n",
      "Epoch: 130 | Batch_idx: 240 |  Loss_1: (0.0382) | Acc_1: (98.70%) (30446/30848)\n",
      "Epoch: 130 | Batch_idx: 250 |  Loss_1: (0.0377) | Acc_1: (98.71%) (31712/32128)\n",
      "Epoch: 130 | Batch_idx: 260 |  Loss_1: (0.0378) | Acc_1: (98.70%) (32974/33408)\n",
      "Epoch: 130 | Batch_idx: 270 |  Loss_1: (0.0381) | Acc_1: (98.67%) (34227/34688)\n",
      "Epoch: 130 | Batch_idx: 280 |  Loss_1: (0.0385) | Acc_1: (98.66%) (35487/35968)\n",
      "Epoch: 130 | Batch_idx: 290 |  Loss_1: (0.0386) | Acc_1: (98.66%) (36748/37248)\n",
      "Epoch: 130 | Batch_idx: 300 |  Loss_1: (0.0384) | Acc_1: (98.67%) (38014/38528)\n",
      "Epoch: 130 | Batch_idx: 310 |  Loss_1: (0.0388) | Acc_1: (98.66%) (39274/39808)\n",
      "Epoch: 130 | Batch_idx: 320 |  Loss_1: (0.0387) | Acc_1: (98.66%) (40538/41088)\n",
      "Epoch: 130 | Batch_idx: 330 |  Loss_1: (0.0393) | Acc_1: (98.64%) (41790/42368)\n",
      "Epoch: 130 | Batch_idx: 340 |  Loss_1: (0.0393) | Acc_1: (98.63%) (43050/43648)\n",
      "Epoch: 130 | Batch_idx: 350 |  Loss_1: (0.0391) | Acc_1: (98.64%) (44317/44928)\n",
      "Epoch: 130 | Batch_idx: 360 |  Loss_1: (0.0393) | Acc_1: (98.64%) (45579/46208)\n",
      "Epoch: 130 | Batch_idx: 370 |  Loss_1: (0.0395) | Acc_1: (98.63%) (46837/47488)\n",
      "Epoch: 130 | Batch_idx: 380 |  Loss_1: (0.0395) | Acc_1: (98.63%) (48100/48768)\n",
      "Epoch: 130 | Batch_idx: 390 |  Loss_1: (0.0397) | Acc_1: (98.63%) (49313/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4781) | Acc: (90.62%) (9062/10000)\n",
      "Epoch: 131 | Batch_idx: 0 |  Loss_1: (0.0191) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 131 | Batch_idx: 10 |  Loss_1: (0.0406) | Acc_1: (98.65%) (1389/1408)\n",
      "Epoch: 131 | Batch_idx: 20 |  Loss_1: (0.0379) | Acc_1: (98.66%) (2652/2688)\n",
      "Epoch: 131 | Batch_idx: 30 |  Loss_1: (0.0396) | Acc_1: (98.56%) (3911/3968)\n",
      "Epoch: 131 | Batch_idx: 40 |  Loss_1: (0.0367) | Acc_1: (98.70%) (5180/5248)\n",
      "Epoch: 131 | Batch_idx: 50 |  Loss_1: (0.0359) | Acc_1: (98.74%) (6446/6528)\n",
      "Epoch: 131 | Batch_idx: 60 |  Loss_1: (0.0365) | Acc_1: (98.73%) (7709/7808)\n",
      "Epoch: 131 | Batch_idx: 70 |  Loss_1: (0.0365) | Acc_1: (98.72%) (8972/9088)\n",
      "Epoch: 131 | Batch_idx: 80 |  Loss_1: (0.0359) | Acc_1: (98.78%) (10242/10368)\n",
      "Epoch: 131 | Batch_idx: 90 |  Loss_1: (0.0357) | Acc_1: (98.82%) (11510/11648)\n",
      "Epoch: 131 | Batch_idx: 100 |  Loss_1: (0.0353) | Acc_1: (98.81%) (12774/12928)\n",
      "Epoch: 131 | Batch_idx: 110 |  Loss_1: (0.0360) | Acc_1: (98.76%) (14032/14208)\n",
      "Epoch: 131 | Batch_idx: 120 |  Loss_1: (0.0359) | Acc_1: (98.75%) (15294/15488)\n",
      "Epoch: 131 | Batch_idx: 130 |  Loss_1: (0.0358) | Acc_1: (98.77%) (16561/16768)\n",
      "Epoch: 131 | Batch_idx: 140 |  Loss_1: (0.0358) | Acc_1: (98.78%) (17828/18048)\n",
      "Epoch: 131 | Batch_idx: 150 |  Loss_1: (0.0359) | Acc_1: (98.77%) (19090/19328)\n",
      "Epoch: 131 | Batch_idx: 160 |  Loss_1: (0.0352) | Acc_1: (98.78%) (20357/20608)\n",
      "Epoch: 131 | Batch_idx: 170 |  Loss_1: (0.0352) | Acc_1: (98.79%) (21624/21888)\n",
      "Epoch: 131 | Batch_idx: 180 |  Loss_1: (0.0354) | Acc_1: (98.79%) (22887/23168)\n",
      "Epoch: 131 | Batch_idx: 190 |  Loss_1: (0.0356) | Acc_1: (98.77%) (24148/24448)\n",
      "Epoch: 131 | Batch_idx: 200 |  Loss_1: (0.0352) | Acc_1: (98.80%) (25418/25728)\n",
      "Epoch: 131 | Batch_idx: 210 |  Loss_1: (0.0351) | Acc_1: (98.80%) (26684/27008)\n",
      "Epoch: 131 | Batch_idx: 220 |  Loss_1: (0.0353) | Acc_1: (98.80%) (27949/28288)\n",
      "Epoch: 131 | Batch_idx: 230 |  Loss_1: (0.0350) | Acc_1: (98.80%) (29214/29568)\n",
      "Epoch: 131 | Batch_idx: 240 |  Loss_1: (0.0352) | Acc_1: (98.79%) (30475/30848)\n",
      "Epoch: 131 | Batch_idx: 250 |  Loss_1: (0.0354) | Acc_1: (98.79%) (31738/32128)\n",
      "Epoch: 131 | Batch_idx: 260 |  Loss_1: (0.0357) | Acc_1: (98.79%) (33003/33408)\n",
      "Epoch: 131 | Batch_idx: 270 |  Loss_1: (0.0358) | Acc_1: (98.79%) (34268/34688)\n",
      "Epoch: 131 | Batch_idx: 280 |  Loss_1: (0.0357) | Acc_1: (98.78%) (35528/35968)\n",
      "Epoch: 131 | Batch_idx: 290 |  Loss_1: (0.0354) | Acc_1: (98.80%) (36800/37248)\n",
      "Epoch: 131 | Batch_idx: 300 |  Loss_1: (0.0357) | Acc_1: (98.78%) (38059/38528)\n",
      "Epoch: 131 | Batch_idx: 310 |  Loss_1: (0.0367) | Acc_1: (98.76%) (39313/39808)\n",
      "Epoch: 131 | Batch_idx: 320 |  Loss_1: (0.0366) | Acc_1: (98.76%) (40580/41088)\n",
      "Epoch: 131 | Batch_idx: 330 |  Loss_1: (0.0369) | Acc_1: (98.76%) (41842/42368)\n",
      "Epoch: 131 | Batch_idx: 340 |  Loss_1: (0.0367) | Acc_1: (98.77%) (43110/43648)\n",
      "Epoch: 131 | Batch_idx: 350 |  Loss_1: (0.0366) | Acc_1: (98.77%) (44376/44928)\n",
      "Epoch: 131 | Batch_idx: 360 |  Loss_1: (0.0366) | Acc_1: (98.77%) (45641/46208)\n",
      "Epoch: 131 | Batch_idx: 370 |  Loss_1: (0.0367) | Acc_1: (98.78%) (46907/47488)\n",
      "Epoch: 131 | Batch_idx: 380 |  Loss_1: (0.0370) | Acc_1: (98.76%) (48164/48768)\n",
      "Epoch: 131 | Batch_idx: 390 |  Loss_1: (0.0374) | Acc_1: (98.75%) (49374/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4765) | Acc: (90.90%) (9090/10000)\n",
      "Epoch: 132 | Batch_idx: 0 |  Loss_1: (0.0199) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 132 | Batch_idx: 10 |  Loss_1: (0.0303) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 132 | Batch_idx: 20 |  Loss_1: (0.0333) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 132 | Batch_idx: 30 |  Loss_1: (0.0400) | Acc_1: (98.69%) (3916/3968)\n",
      "Epoch: 132 | Batch_idx: 40 |  Loss_1: (0.0407) | Acc_1: (98.65%) (5177/5248)\n",
      "Epoch: 132 | Batch_idx: 50 |  Loss_1: (0.0380) | Acc_1: (98.73%) (6445/6528)\n",
      "Epoch: 132 | Batch_idx: 60 |  Loss_1: (0.0387) | Acc_1: (98.72%) (7708/7808)\n",
      "Epoch: 132 | Batch_idx: 70 |  Loss_1: (0.0390) | Acc_1: (98.71%) (8971/9088)\n",
      "Epoch: 132 | Batch_idx: 80 |  Loss_1: (0.0393) | Acc_1: (98.73%) (10236/10368)\n",
      "Epoch: 132 | Batch_idx: 90 |  Loss_1: (0.0401) | Acc_1: (98.69%) (11495/11648)\n",
      "Epoch: 132 | Batch_idx: 100 |  Loss_1: (0.0391) | Acc_1: (98.71%) (12761/12928)\n",
      "Epoch: 132 | Batch_idx: 110 |  Loss_1: (0.0409) | Acc_1: (98.65%) (14016/14208)\n",
      "Epoch: 132 | Batch_idx: 120 |  Loss_1: (0.0427) | Acc_1: (98.57%) (15266/15488)\n",
      "Epoch: 132 | Batch_idx: 130 |  Loss_1: (0.0424) | Acc_1: (98.57%) (16529/16768)\n",
      "Epoch: 132 | Batch_idx: 140 |  Loss_1: (0.0433) | Acc_1: (98.53%) (17782/18048)\n",
      "Epoch: 132 | Batch_idx: 150 |  Loss_1: (0.0429) | Acc_1: (98.54%) (19045/19328)\n",
      "Epoch: 132 | Batch_idx: 160 |  Loss_1: (0.0428) | Acc_1: (98.53%) (20306/20608)\n",
      "Epoch: 132 | Batch_idx: 170 |  Loss_1: (0.0424) | Acc_1: (98.52%) (21565/21888)\n",
      "Epoch: 132 | Batch_idx: 180 |  Loss_1: (0.0424) | Acc_1: (98.52%) (22824/23168)\n",
      "Epoch: 132 | Batch_idx: 190 |  Loss_1: (0.0424) | Acc_1: (98.54%) (24090/24448)\n",
      "Epoch: 132 | Batch_idx: 200 |  Loss_1: (0.0428) | Acc_1: (98.50%) (25343/25728)\n",
      "Epoch: 132 | Batch_idx: 210 |  Loss_1: (0.0424) | Acc_1: (98.53%) (26610/27008)\n",
      "Epoch: 132 | Batch_idx: 220 |  Loss_1: (0.0421) | Acc_1: (98.53%) (27871/28288)\n",
      "Epoch: 132 | Batch_idx: 230 |  Loss_1: (0.0420) | Acc_1: (98.54%) (29135/29568)\n",
      "Epoch: 132 | Batch_idx: 240 |  Loss_1: (0.0424) | Acc_1: (98.53%) (30396/30848)\n",
      "Epoch: 132 | Batch_idx: 250 |  Loss_1: (0.0421) | Acc_1: (98.55%) (31663/32128)\n",
      "Epoch: 132 | Batch_idx: 260 |  Loss_1: (0.0416) | Acc_1: (98.56%) (32928/33408)\n",
      "Epoch: 132 | Batch_idx: 270 |  Loss_1: (0.0411) | Acc_1: (98.58%) (34196/34688)\n",
      "Epoch: 132 | Batch_idx: 280 |  Loss_1: (0.0410) | Acc_1: (98.58%) (35458/35968)\n",
      "Epoch: 132 | Batch_idx: 290 |  Loss_1: (0.0411) | Acc_1: (98.59%) (36724/37248)\n",
      "Epoch: 132 | Batch_idx: 300 |  Loss_1: (0.0411) | Acc_1: (98.60%) (37989/38528)\n",
      "Epoch: 132 | Batch_idx: 310 |  Loss_1: (0.0409) | Acc_1: (98.61%) (39255/39808)\n",
      "Epoch: 132 | Batch_idx: 320 |  Loss_1: (0.0408) | Acc_1: (98.61%) (40517/41088)\n",
      "Epoch: 132 | Batch_idx: 330 |  Loss_1: (0.0405) | Acc_1: (98.63%) (41787/42368)\n",
      "Epoch: 132 | Batch_idx: 340 |  Loss_1: (0.0401) | Acc_1: (98.64%) (43053/43648)\n",
      "Epoch: 132 | Batch_idx: 350 |  Loss_1: (0.0401) | Acc_1: (98.64%) (44317/44928)\n",
      "Epoch: 132 | Batch_idx: 360 |  Loss_1: (0.0400) | Acc_1: (98.65%) (45583/46208)\n",
      "Epoch: 132 | Batch_idx: 370 |  Loss_1: (0.0396) | Acc_1: (98.66%) (46850/47488)\n",
      "Epoch: 132 | Batch_idx: 380 |  Loss_1: (0.0398) | Acc_1: (98.64%) (48105/48768)\n",
      "Epoch: 132 | Batch_idx: 390 |  Loss_1: (0.0395) | Acc_1: (98.65%) (49323/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5300) | Acc: (90.10%) (9010/10000)\n",
      "Epoch: 133 | Batch_idx: 0 |  Loss_1: (0.0420) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 133 | Batch_idx: 10 |  Loss_1: (0.0414) | Acc_1: (98.30%) (1384/1408)\n",
      "Epoch: 133 | Batch_idx: 20 |  Loss_1: (0.0411) | Acc_1: (98.59%) (2650/2688)\n",
      "Epoch: 133 | Batch_idx: 30 |  Loss_1: (0.0361) | Acc_1: (98.74%) (3918/3968)\n",
      "Epoch: 133 | Batch_idx: 40 |  Loss_1: (0.0357) | Acc_1: (98.76%) (5183/5248)\n",
      "Epoch: 133 | Batch_idx: 50 |  Loss_1: (0.0382) | Acc_1: (98.73%) (6445/6528)\n",
      "Epoch: 133 | Batch_idx: 60 |  Loss_1: (0.0371) | Acc_1: (98.77%) (7712/7808)\n",
      "Epoch: 133 | Batch_idx: 70 |  Loss_1: (0.0367) | Acc_1: (98.81%) (8980/9088)\n",
      "Epoch: 133 | Batch_idx: 80 |  Loss_1: (0.0367) | Acc_1: (98.78%) (10242/10368)\n",
      "Epoch: 133 | Batch_idx: 90 |  Loss_1: (0.0374) | Acc_1: (98.80%) (11508/11648)\n",
      "Epoch: 133 | Batch_idx: 100 |  Loss_1: (0.0368) | Acc_1: (98.83%) (12777/12928)\n",
      "Epoch: 133 | Batch_idx: 110 |  Loss_1: (0.0381) | Acc_1: (98.79%) (14036/14208)\n",
      "Epoch: 133 | Batch_idx: 120 |  Loss_1: (0.0372) | Acc_1: (98.80%) (15302/15488)\n",
      "Epoch: 133 | Batch_idx: 130 |  Loss_1: (0.0367) | Acc_1: (98.81%) (16568/16768)\n",
      "Epoch: 133 | Batch_idx: 140 |  Loss_1: (0.0358) | Acc_1: (98.81%) (17834/18048)\n",
      "Epoch: 133 | Batch_idx: 150 |  Loss_1: (0.0362) | Acc_1: (98.79%) (19095/19328)\n",
      "Epoch: 133 | Batch_idx: 160 |  Loss_1: (0.0357) | Acc_1: (98.81%) (20363/20608)\n",
      "Epoch: 133 | Batch_idx: 170 |  Loss_1: (0.0351) | Acc_1: (98.84%) (21635/21888)\n",
      "Epoch: 133 | Batch_idx: 180 |  Loss_1: (0.0352) | Acc_1: (98.83%) (22897/23168)\n",
      "Epoch: 133 | Batch_idx: 190 |  Loss_1: (0.0349) | Acc_1: (98.83%) (24162/24448)\n",
      "Epoch: 133 | Batch_idx: 200 |  Loss_1: (0.0348) | Acc_1: (98.82%) (25425/25728)\n",
      "Epoch: 133 | Batch_idx: 210 |  Loss_1: (0.0346) | Acc_1: (98.82%) (26690/27008)\n",
      "Epoch: 133 | Batch_idx: 220 |  Loss_1: (0.0347) | Acc_1: (98.82%) (27955/28288)\n",
      "Epoch: 133 | Batch_idx: 230 |  Loss_1: (0.0344) | Acc_1: (98.83%) (29221/29568)\n",
      "Epoch: 133 | Batch_idx: 240 |  Loss_1: (0.0344) | Acc_1: (98.84%) (30489/30848)\n",
      "Epoch: 133 | Batch_idx: 250 |  Loss_1: (0.0342) | Acc_1: (98.83%) (31751/32128)\n",
      "Epoch: 133 | Batch_idx: 260 |  Loss_1: (0.0342) | Acc_1: (98.82%) (33015/33408)\n",
      "Epoch: 133 | Batch_idx: 270 |  Loss_1: (0.0342) | Acc_1: (98.82%) (34280/34688)\n",
      "Epoch: 133 | Batch_idx: 280 |  Loss_1: (0.0346) | Acc_1: (98.81%) (35539/35968)\n",
      "Epoch: 133 | Batch_idx: 290 |  Loss_1: (0.0348) | Acc_1: (98.80%) (36800/37248)\n",
      "Epoch: 133 | Batch_idx: 300 |  Loss_1: (0.0351) | Acc_1: (98.79%) (38061/38528)\n",
      "Epoch: 133 | Batch_idx: 310 |  Loss_1: (0.0354) | Acc_1: (98.78%) (39321/39808)\n",
      "Epoch: 133 | Batch_idx: 320 |  Loss_1: (0.0355) | Acc_1: (98.78%) (40585/41088)\n",
      "Epoch: 133 | Batch_idx: 330 |  Loss_1: (0.0359) | Acc_1: (98.76%) (41844/42368)\n",
      "Epoch: 133 | Batch_idx: 340 |  Loss_1: (0.0361) | Acc_1: (98.75%) (43103/43648)\n",
      "Epoch: 133 | Batch_idx: 350 |  Loss_1: (0.0364) | Acc_1: (98.73%) (44359/44928)\n",
      "Epoch: 133 | Batch_idx: 360 |  Loss_1: (0.0365) | Acc_1: (98.72%) (45618/46208)\n",
      "Epoch: 133 | Batch_idx: 370 |  Loss_1: (0.0365) | Acc_1: (98.72%) (46881/47488)\n",
      "Epoch: 133 | Batch_idx: 380 |  Loss_1: (0.0366) | Acc_1: (98.72%) (48145/48768)\n",
      "Epoch: 133 | Batch_idx: 390 |  Loss_1: (0.0363) | Acc_1: (98.73%) (49363/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4601) | Acc: (90.93%) (9093/10000)\n",
      "Epoch: 134 | Batch_idx: 0 |  Loss_1: (0.0227) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 134 | Batch_idx: 10 |  Loss_1: (0.0495) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 134 | Batch_idx: 20 |  Loss_1: (0.0424) | Acc_1: (98.77%) (2655/2688)\n",
      "Epoch: 134 | Batch_idx: 30 |  Loss_1: (0.0365) | Acc_1: (98.94%) (3926/3968)\n",
      "Epoch: 134 | Batch_idx: 40 |  Loss_1: (0.0385) | Acc_1: (98.78%) (5184/5248)\n",
      "Epoch: 134 | Batch_idx: 50 |  Loss_1: (0.0373) | Acc_1: (98.73%) (6445/6528)\n",
      "Epoch: 134 | Batch_idx: 60 |  Loss_1: (0.0365) | Acc_1: (98.73%) (7709/7808)\n",
      "Epoch: 134 | Batch_idx: 70 |  Loss_1: (0.0372) | Acc_1: (98.77%) (8976/9088)\n",
      "Epoch: 134 | Batch_idx: 80 |  Loss_1: (0.0378) | Acc_1: (98.75%) (10238/10368)\n",
      "Epoch: 134 | Batch_idx: 90 |  Loss_1: (0.0363) | Acc_1: (98.80%) (11508/11648)\n",
      "Epoch: 134 | Batch_idx: 100 |  Loss_1: (0.0369) | Acc_1: (98.75%) (12767/12928)\n",
      "Epoch: 134 | Batch_idx: 110 |  Loss_1: (0.0366) | Acc_1: (98.78%) (14035/14208)\n",
      "Epoch: 134 | Batch_idx: 120 |  Loss_1: (0.0368) | Acc_1: (98.81%) (15303/15488)\n",
      "Epoch: 134 | Batch_idx: 130 |  Loss_1: (0.0386) | Acc_1: (98.75%) (16559/16768)\n",
      "Epoch: 134 | Batch_idx: 140 |  Loss_1: (0.0387) | Acc_1: (98.72%) (17817/18048)\n",
      "Epoch: 134 | Batch_idx: 150 |  Loss_1: (0.0380) | Acc_1: (98.74%) (19084/19328)\n",
      "Epoch: 134 | Batch_idx: 160 |  Loss_1: (0.0372) | Acc_1: (98.76%) (20353/20608)\n",
      "Epoch: 134 | Batch_idx: 170 |  Loss_1: (0.0372) | Acc_1: (98.77%) (21619/21888)\n",
      "Epoch: 134 | Batch_idx: 180 |  Loss_1: (0.0375) | Acc_1: (98.75%) (22879/23168)\n",
      "Epoch: 134 | Batch_idx: 190 |  Loss_1: (0.0371) | Acc_1: (98.75%) (24143/24448)\n",
      "Epoch: 134 | Batch_idx: 200 |  Loss_1: (0.0375) | Acc_1: (98.73%) (25402/25728)\n",
      "Epoch: 134 | Batch_idx: 210 |  Loss_1: (0.0371) | Acc_1: (98.74%) (26669/27008)\n",
      "Epoch: 134 | Batch_idx: 220 |  Loss_1: (0.0371) | Acc_1: (98.75%) (27934/28288)\n",
      "Epoch: 134 | Batch_idx: 230 |  Loss_1: (0.0374) | Acc_1: (98.75%) (29198/29568)\n",
      "Epoch: 134 | Batch_idx: 240 |  Loss_1: (0.0373) | Acc_1: (98.73%) (30457/30848)\n",
      "Epoch: 134 | Batch_idx: 250 |  Loss_1: (0.0369) | Acc_1: (98.75%) (31725/32128)\n",
      "Epoch: 134 | Batch_idx: 260 |  Loss_1: (0.0368) | Acc_1: (98.76%) (32993/33408)\n",
      "Epoch: 134 | Batch_idx: 270 |  Loss_1: (0.0362) | Acc_1: (98.78%) (34265/34688)\n",
      "Epoch: 134 | Batch_idx: 280 |  Loss_1: (0.0360) | Acc_1: (98.79%) (35532/35968)\n",
      "Epoch: 134 | Batch_idx: 290 |  Loss_1: (0.0361) | Acc_1: (98.79%) (36797/37248)\n",
      "Epoch: 134 | Batch_idx: 300 |  Loss_1: (0.0355) | Acc_1: (98.81%) (38069/38528)\n",
      "Epoch: 134 | Batch_idx: 310 |  Loss_1: (0.0354) | Acc_1: (98.82%) (39337/39808)\n",
      "Epoch: 134 | Batch_idx: 320 |  Loss_1: (0.0352) | Acc_1: (98.81%) (40601/41088)\n",
      "Epoch: 134 | Batch_idx: 330 |  Loss_1: (0.0352) | Acc_1: (98.81%) (41864/42368)\n",
      "Epoch: 134 | Batch_idx: 340 |  Loss_1: (0.0351) | Acc_1: (98.80%) (43125/43648)\n",
      "Epoch: 134 | Batch_idx: 350 |  Loss_1: (0.0348) | Acc_1: (98.81%) (44395/44928)\n",
      "Epoch: 134 | Batch_idx: 360 |  Loss_1: (0.0346) | Acc_1: (98.82%) (45662/46208)\n",
      "Epoch: 134 | Batch_idx: 370 |  Loss_1: (0.0345) | Acc_1: (98.81%) (46925/47488)\n",
      "Epoch: 134 | Batch_idx: 380 |  Loss_1: (0.0344) | Acc_1: (98.82%) (48194/48768)\n",
      "Epoch: 134 | Batch_idx: 390 |  Loss_1: (0.0343) | Acc_1: (98.82%) (49409/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4935) | Acc: (90.76%) (9076/10000)\n",
      "Epoch: 135 | Batch_idx: 0 |  Loss_1: (0.0403) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 135 | Batch_idx: 10 |  Loss_1: (0.0255) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 135 | Batch_idx: 20 |  Loss_1: (0.0233) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 135 | Batch_idx: 30 |  Loss_1: (0.0217) | Acc_1: (99.24%) (3938/3968)\n",
      "Epoch: 135 | Batch_idx: 40 |  Loss_1: (0.0231) | Acc_1: (99.09%) (5200/5248)\n",
      "Epoch: 135 | Batch_idx: 50 |  Loss_1: (0.0242) | Acc_1: (99.10%) (6469/6528)\n",
      "Epoch: 135 | Batch_idx: 60 |  Loss_1: (0.0263) | Acc_1: (99.00%) (7730/7808)\n",
      "Epoch: 135 | Batch_idx: 70 |  Loss_1: (0.0259) | Acc_1: (99.06%) (9003/9088)\n",
      "Epoch: 135 | Batch_idx: 80 |  Loss_1: (0.0281) | Acc_1: (98.99%) (10263/10368)\n",
      "Epoch: 135 | Batch_idx: 90 |  Loss_1: (0.0286) | Acc_1: (98.94%) (11524/11648)\n",
      "Epoch: 135 | Batch_idx: 100 |  Loss_1: (0.0284) | Acc_1: (98.96%) (12793/12928)\n",
      "Epoch: 135 | Batch_idx: 110 |  Loss_1: (0.0285) | Acc_1: (98.96%) (14060/14208)\n",
      "Epoch: 135 | Batch_idx: 120 |  Loss_1: (0.0297) | Acc_1: (98.90%) (15317/15488)\n",
      "Epoch: 135 | Batch_idx: 130 |  Loss_1: (0.0314) | Acc_1: (98.83%) (16572/16768)\n",
      "Epoch: 135 | Batch_idx: 140 |  Loss_1: (0.0319) | Acc_1: (98.81%) (17834/18048)\n",
      "Epoch: 135 | Batch_idx: 150 |  Loss_1: (0.0329) | Acc_1: (98.78%) (19092/19328)\n",
      "Epoch: 135 | Batch_idx: 160 |  Loss_1: (0.0333) | Acc_1: (98.79%) (20358/20608)\n",
      "Epoch: 135 | Batch_idx: 170 |  Loss_1: (0.0335) | Acc_1: (98.78%) (21622/21888)\n",
      "Epoch: 135 | Batch_idx: 180 |  Loss_1: (0.0337) | Acc_1: (98.77%) (22884/23168)\n",
      "Epoch: 135 | Batch_idx: 190 |  Loss_1: (0.0335) | Acc_1: (98.78%) (24150/24448)\n",
      "Epoch: 135 | Batch_idx: 200 |  Loss_1: (0.0334) | Acc_1: (98.78%) (25413/25728)\n",
      "Epoch: 135 | Batch_idx: 210 |  Loss_1: (0.0337) | Acc_1: (98.78%) (26678/27008)\n",
      "Epoch: 135 | Batch_idx: 220 |  Loss_1: (0.0334) | Acc_1: (98.81%) (27950/28288)\n",
      "Epoch: 135 | Batch_idx: 230 |  Loss_1: (0.0336) | Acc_1: (98.81%) (29215/29568)\n",
      "Epoch: 135 | Batch_idx: 240 |  Loss_1: (0.0339) | Acc_1: (98.80%) (30479/30848)\n",
      "Epoch: 135 | Batch_idx: 250 |  Loss_1: (0.0335) | Acc_1: (98.83%) (31751/32128)\n",
      "Epoch: 135 | Batch_idx: 260 |  Loss_1: (0.0332) | Acc_1: (98.85%) (33023/33408)\n",
      "Epoch: 135 | Batch_idx: 270 |  Loss_1: (0.0334) | Acc_1: (98.84%) (34286/34688)\n",
      "Epoch: 135 | Batch_idx: 280 |  Loss_1: (0.0332) | Acc_1: (98.85%) (35554/35968)\n",
      "Epoch: 135 | Batch_idx: 290 |  Loss_1: (0.0334) | Acc_1: (98.85%) (36818/37248)\n",
      "Epoch: 135 | Batch_idx: 300 |  Loss_1: (0.0336) | Acc_1: (98.84%) (38081/38528)\n",
      "Epoch: 135 | Batch_idx: 310 |  Loss_1: (0.0335) | Acc_1: (98.84%) (39348/39808)\n",
      "Epoch: 135 | Batch_idx: 320 |  Loss_1: (0.0339) | Acc_1: (98.83%) (40608/41088)\n",
      "Epoch: 135 | Batch_idx: 330 |  Loss_1: (0.0338) | Acc_1: (98.83%) (41871/42368)\n",
      "Epoch: 135 | Batch_idx: 340 |  Loss_1: (0.0338) | Acc_1: (98.82%) (43135/43648)\n",
      "Epoch: 135 | Batch_idx: 350 |  Loss_1: (0.0339) | Acc_1: (98.83%) (44401/44928)\n",
      "Epoch: 135 | Batch_idx: 360 |  Loss_1: (0.0341) | Acc_1: (98.82%) (45664/46208)\n",
      "Epoch: 135 | Batch_idx: 370 |  Loss_1: (0.0340) | Acc_1: (98.83%) (46932/47488)\n",
      "Epoch: 135 | Batch_idx: 380 |  Loss_1: (0.0338) | Acc_1: (98.84%) (48201/48768)\n",
      "Epoch: 135 | Batch_idx: 390 |  Loss_1: (0.0339) | Acc_1: (98.84%) (49419/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4570) | Acc: (91.42%) (9142/10000)\n",
      "Epoch: 136 | Batch_idx: 0 |  Loss_1: (0.0759) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 136 | Batch_idx: 10 |  Loss_1: (0.0401) | Acc_1: (98.51%) (1387/1408)\n",
      "Epoch: 136 | Batch_idx: 20 |  Loss_1: (0.0421) | Acc_1: (98.47%) (2647/2688)\n",
      "Epoch: 136 | Batch_idx: 30 |  Loss_1: (0.0393) | Acc_1: (98.59%) (3912/3968)\n",
      "Epoch: 136 | Batch_idx: 40 |  Loss_1: (0.0364) | Acc_1: (98.72%) (5181/5248)\n",
      "Epoch: 136 | Batch_idx: 50 |  Loss_1: (0.0335) | Acc_1: (98.84%) (6452/6528)\n",
      "Epoch: 136 | Batch_idx: 60 |  Loss_1: (0.0344) | Acc_1: (98.81%) (7715/7808)\n",
      "Epoch: 136 | Batch_idx: 70 |  Loss_1: (0.0345) | Acc_1: (98.82%) (8981/9088)\n",
      "Epoch: 136 | Batch_idx: 80 |  Loss_1: (0.0356) | Acc_1: (98.80%) (10244/10368)\n",
      "Epoch: 136 | Batch_idx: 90 |  Loss_1: (0.0343) | Acc_1: (98.86%) (11515/11648)\n",
      "Epoch: 136 | Batch_idx: 100 |  Loss_1: (0.0336) | Acc_1: (98.88%) (12783/12928)\n",
      "Epoch: 136 | Batch_idx: 110 |  Loss_1: (0.0334) | Acc_1: (98.88%) (14049/14208)\n",
      "Epoch: 136 | Batch_idx: 120 |  Loss_1: (0.0334) | Acc_1: (98.90%) (15317/15488)\n",
      "Epoch: 136 | Batch_idx: 130 |  Loss_1: (0.0335) | Acc_1: (98.90%) (16584/16768)\n",
      "Epoch: 136 | Batch_idx: 140 |  Loss_1: (0.0338) | Acc_1: (98.87%) (17844/18048)\n",
      "Epoch: 136 | Batch_idx: 150 |  Loss_1: (0.0342) | Acc_1: (98.83%) (19102/19328)\n",
      "Epoch: 136 | Batch_idx: 160 |  Loss_1: (0.0349) | Acc_1: (98.80%) (20361/20608)\n",
      "Epoch: 136 | Batch_idx: 170 |  Loss_1: (0.0353) | Acc_1: (98.78%) (21621/21888)\n",
      "Epoch: 136 | Batch_idx: 180 |  Loss_1: (0.0358) | Acc_1: (98.76%) (22880/23168)\n",
      "Epoch: 136 | Batch_idx: 190 |  Loss_1: (0.0354) | Acc_1: (98.76%) (24146/24448)\n",
      "Epoch: 136 | Batch_idx: 200 |  Loss_1: (0.0355) | Acc_1: (98.75%) (25407/25728)\n",
      "Epoch: 136 | Batch_idx: 210 |  Loss_1: (0.0354) | Acc_1: (98.76%) (26673/27008)\n",
      "Epoch: 136 | Batch_idx: 220 |  Loss_1: (0.0354) | Acc_1: (98.77%) (27939/28288)\n",
      "Epoch: 136 | Batch_idx: 230 |  Loss_1: (0.0353) | Acc_1: (98.77%) (29203/29568)\n",
      "Epoch: 136 | Batch_idx: 240 |  Loss_1: (0.0353) | Acc_1: (98.76%) (30467/30848)\n",
      "Epoch: 136 | Batch_idx: 250 |  Loss_1: (0.0360) | Acc_1: (98.76%) (31729/32128)\n",
      "Epoch: 136 | Batch_idx: 260 |  Loss_1: (0.0362) | Acc_1: (98.75%) (32989/33408)\n",
      "Epoch: 136 | Batch_idx: 270 |  Loss_1: (0.0367) | Acc_1: (98.74%) (34250/34688)\n",
      "Epoch: 136 | Batch_idx: 280 |  Loss_1: (0.0369) | Acc_1: (98.73%) (35512/35968)\n",
      "Epoch: 136 | Batch_idx: 290 |  Loss_1: (0.0371) | Acc_1: (98.72%) (36773/37248)\n",
      "Epoch: 136 | Batch_idx: 300 |  Loss_1: (0.0374) | Acc_1: (98.73%) (38037/38528)\n",
      "Epoch: 136 | Batch_idx: 310 |  Loss_1: (0.0371) | Acc_1: (98.73%) (39303/39808)\n",
      "Epoch: 136 | Batch_idx: 320 |  Loss_1: (0.0373) | Acc_1: (98.72%) (40561/41088)\n",
      "Epoch: 136 | Batch_idx: 330 |  Loss_1: (0.0371) | Acc_1: (98.72%) (41826/42368)\n",
      "Epoch: 136 | Batch_idx: 340 |  Loss_1: (0.0369) | Acc_1: (98.73%) (43094/43648)\n",
      "Epoch: 136 | Batch_idx: 350 |  Loss_1: (0.0365) | Acc_1: (98.75%) (44367/44928)\n",
      "Epoch: 136 | Batch_idx: 360 |  Loss_1: (0.0362) | Acc_1: (98.76%) (45634/46208)\n",
      "Epoch: 136 | Batch_idx: 370 |  Loss_1: (0.0363) | Acc_1: (98.76%) (46897/47488)\n",
      "Epoch: 136 | Batch_idx: 380 |  Loss_1: (0.0361) | Acc_1: (98.76%) (48165/48768)\n",
      "Epoch: 136 | Batch_idx: 390 |  Loss_1: (0.0357) | Acc_1: (98.78%) (49391/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4600) | Acc: (91.09%) (9109/10000)\n",
      "Epoch: 137 | Batch_idx: 0 |  Loss_1: (0.0660) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 137 | Batch_idx: 10 |  Loss_1: (0.0286) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 137 | Batch_idx: 20 |  Loss_1: (0.0307) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 137 | Batch_idx: 30 |  Loss_1: (0.0318) | Acc_1: (98.82%) (3921/3968)\n",
      "Epoch: 137 | Batch_idx: 40 |  Loss_1: (0.0319) | Acc_1: (98.86%) (5188/5248)\n",
      "Epoch: 137 | Batch_idx: 50 |  Loss_1: (0.0326) | Acc_1: (98.88%) (6455/6528)\n",
      "Epoch: 137 | Batch_idx: 60 |  Loss_1: (0.0336) | Acc_1: (98.86%) (7719/7808)\n",
      "Epoch: 137 | Batch_idx: 70 |  Loss_1: (0.0330) | Acc_1: (98.91%) (8989/9088)\n",
      "Epoch: 137 | Batch_idx: 80 |  Loss_1: (0.0330) | Acc_1: (98.94%) (10258/10368)\n",
      "Epoch: 137 | Batch_idx: 90 |  Loss_1: (0.0341) | Acc_1: (98.92%) (11522/11648)\n",
      "Epoch: 137 | Batch_idx: 100 |  Loss_1: (0.0329) | Acc_1: (98.96%) (12793/12928)\n",
      "Epoch: 137 | Batch_idx: 110 |  Loss_1: (0.0322) | Acc_1: (98.96%) (14060/14208)\n",
      "Epoch: 137 | Batch_idx: 120 |  Loss_1: (0.0318) | Acc_1: (98.95%) (15326/15488)\n",
      "Epoch: 137 | Batch_idx: 130 |  Loss_1: (0.0315) | Acc_1: (98.97%) (16596/16768)\n",
      "Epoch: 137 | Batch_idx: 140 |  Loss_1: (0.0329) | Acc_1: (98.96%) (17860/18048)\n",
      "Epoch: 137 | Batch_idx: 150 |  Loss_1: (0.0339) | Acc_1: (98.92%) (19120/19328)\n",
      "Epoch: 137 | Batch_idx: 160 |  Loss_1: (0.0344) | Acc_1: (98.90%) (20382/20608)\n",
      "Epoch: 137 | Batch_idx: 170 |  Loss_1: (0.0349) | Acc_1: (98.88%) (21642/21888)\n",
      "Epoch: 137 | Batch_idx: 180 |  Loss_1: (0.0349) | Acc_1: (98.88%) (22908/23168)\n",
      "Epoch: 137 | Batch_idx: 190 |  Loss_1: (0.0344) | Acc_1: (98.90%) (24178/24448)\n",
      "Epoch: 137 | Batch_idx: 200 |  Loss_1: (0.0346) | Acc_1: (98.89%) (25442/25728)\n",
      "Epoch: 137 | Batch_idx: 210 |  Loss_1: (0.0344) | Acc_1: (98.90%) (26711/27008)\n",
      "Epoch: 137 | Batch_idx: 220 |  Loss_1: (0.0342) | Acc_1: (98.90%) (27978/28288)\n",
      "Epoch: 137 | Batch_idx: 230 |  Loss_1: (0.0340) | Acc_1: (98.90%) (29243/29568)\n",
      "Epoch: 137 | Batch_idx: 240 |  Loss_1: (0.0339) | Acc_1: (98.90%) (30510/30848)\n",
      "Epoch: 137 | Batch_idx: 250 |  Loss_1: (0.0336) | Acc_1: (98.91%) (31779/32128)\n",
      "Epoch: 137 | Batch_idx: 260 |  Loss_1: (0.0333) | Acc_1: (98.92%) (33047/33408)\n",
      "Epoch: 137 | Batch_idx: 270 |  Loss_1: (0.0333) | Acc_1: (98.92%) (34313/34688)\n",
      "Epoch: 137 | Batch_idx: 280 |  Loss_1: (0.0334) | Acc_1: (98.91%) (35577/35968)\n",
      "Epoch: 137 | Batch_idx: 290 |  Loss_1: (0.0332) | Acc_1: (98.92%) (36846/37248)\n",
      "Epoch: 137 | Batch_idx: 300 |  Loss_1: (0.0331) | Acc_1: (98.93%) (38116/38528)\n",
      "Epoch: 137 | Batch_idx: 310 |  Loss_1: (0.0331) | Acc_1: (98.93%) (39383/39808)\n",
      "Epoch: 137 | Batch_idx: 320 |  Loss_1: (0.0329) | Acc_1: (98.93%) (40648/41088)\n",
      "Epoch: 137 | Batch_idx: 330 |  Loss_1: (0.0329) | Acc_1: (98.92%) (41909/42368)\n",
      "Epoch: 137 | Batch_idx: 340 |  Loss_1: (0.0325) | Acc_1: (98.93%) (43180/43648)\n",
      "Epoch: 137 | Batch_idx: 350 |  Loss_1: (0.0325) | Acc_1: (98.92%) (44444/44928)\n",
      "Epoch: 137 | Batch_idx: 360 |  Loss_1: (0.0328) | Acc_1: (98.90%) (45701/46208)\n",
      "Epoch: 137 | Batch_idx: 370 |  Loss_1: (0.0336) | Acc_1: (98.88%) (46957/47488)\n",
      "Epoch: 137 | Batch_idx: 380 |  Loss_1: (0.0335) | Acc_1: (98.88%) (48222/48768)\n",
      "Epoch: 137 | Batch_idx: 390 |  Loss_1: (0.0337) | Acc_1: (98.87%) (49435/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4732) | Acc: (91.07%) (9107/10000)\n",
      "Epoch: 138 | Batch_idx: 0 |  Loss_1: (0.0389) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 138 | Batch_idx: 10 |  Loss_1: (0.0310) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 138 | Batch_idx: 20 |  Loss_1: (0.0378) | Acc_1: (98.81%) (2656/2688)\n",
      "Epoch: 138 | Batch_idx: 30 |  Loss_1: (0.0349) | Acc_1: (98.92%) (3925/3968)\n",
      "Epoch: 138 | Batch_idx: 40 |  Loss_1: (0.0345) | Acc_1: (98.93%) (5192/5248)\n",
      "Epoch: 138 | Batch_idx: 50 |  Loss_1: (0.0355) | Acc_1: (98.85%) (6453/6528)\n",
      "Epoch: 138 | Batch_idx: 60 |  Loss_1: (0.0343) | Acc_1: (98.89%) (7721/7808)\n",
      "Epoch: 138 | Batch_idx: 70 |  Loss_1: (0.0354) | Acc_1: (98.80%) (8979/9088)\n",
      "Epoch: 138 | Batch_idx: 80 |  Loss_1: (0.0358) | Acc_1: (98.76%) (10239/10368)\n",
      "Epoch: 138 | Batch_idx: 90 |  Loss_1: (0.0361) | Acc_1: (98.74%) (11501/11648)\n",
      "Epoch: 138 | Batch_idx: 100 |  Loss_1: (0.0365) | Acc_1: (98.72%) (12763/12928)\n",
      "Epoch: 138 | Batch_idx: 110 |  Loss_1: (0.0375) | Acc_1: (98.69%) (14022/14208)\n",
      "Epoch: 138 | Batch_idx: 120 |  Loss_1: (0.0363) | Acc_1: (98.72%) (15290/15488)\n",
      "Epoch: 138 | Batch_idx: 130 |  Loss_1: (0.0361) | Acc_1: (98.71%) (16552/16768)\n",
      "Epoch: 138 | Batch_idx: 140 |  Loss_1: (0.0379) | Acc_1: (98.66%) (17806/18048)\n",
      "Epoch: 138 | Batch_idx: 150 |  Loss_1: (0.0381) | Acc_1: (98.65%) (19068/19328)\n",
      "Epoch: 138 | Batch_idx: 160 |  Loss_1: (0.0376) | Acc_1: (98.68%) (20335/20608)\n",
      "Epoch: 138 | Batch_idx: 170 |  Loss_1: (0.0366) | Acc_1: (98.70%) (21603/21888)\n",
      "Epoch: 138 | Batch_idx: 180 |  Loss_1: (0.0364) | Acc_1: (98.71%) (22870/23168)\n",
      "Epoch: 138 | Batch_idx: 190 |  Loss_1: (0.0355) | Acc_1: (98.75%) (24143/24448)\n",
      "Epoch: 138 | Batch_idx: 200 |  Loss_1: (0.0357) | Acc_1: (98.76%) (25410/25728)\n",
      "Epoch: 138 | Batch_idx: 210 |  Loss_1: (0.0354) | Acc_1: (98.77%) (26676/27008)\n",
      "Epoch: 138 | Batch_idx: 220 |  Loss_1: (0.0350) | Acc_1: (98.78%) (27942/28288)\n",
      "Epoch: 138 | Batch_idx: 230 |  Loss_1: (0.0349) | Acc_1: (98.77%) (29205/29568)\n",
      "Epoch: 138 | Batch_idx: 240 |  Loss_1: (0.0354) | Acc_1: (98.77%) (30468/30848)\n",
      "Epoch: 138 | Batch_idx: 250 |  Loss_1: (0.0351) | Acc_1: (98.78%) (31737/32128)\n",
      "Epoch: 138 | Batch_idx: 260 |  Loss_1: (0.0352) | Acc_1: (98.78%) (33000/33408)\n",
      "Epoch: 138 | Batch_idx: 270 |  Loss_1: (0.0356) | Acc_1: (98.78%) (34264/34688)\n",
      "Epoch: 138 | Batch_idx: 280 |  Loss_1: (0.0356) | Acc_1: (98.79%) (35531/35968)\n",
      "Epoch: 138 | Batch_idx: 290 |  Loss_1: (0.0358) | Acc_1: (98.78%) (36792/37248)\n",
      "Epoch: 138 | Batch_idx: 300 |  Loss_1: (0.0354) | Acc_1: (98.78%) (38059/38528)\n",
      "Epoch: 138 | Batch_idx: 310 |  Loss_1: (0.0352) | Acc_1: (98.79%) (39326/39808)\n",
      "Epoch: 138 | Batch_idx: 320 |  Loss_1: (0.0353) | Acc_1: (98.79%) (40591/41088)\n",
      "Epoch: 138 | Batch_idx: 330 |  Loss_1: (0.0355) | Acc_1: (98.78%) (41852/42368)\n",
      "Epoch: 138 | Batch_idx: 340 |  Loss_1: (0.0354) | Acc_1: (98.79%) (43119/43648)\n",
      "Epoch: 138 | Batch_idx: 350 |  Loss_1: (0.0352) | Acc_1: (98.79%) (44386/44928)\n",
      "Epoch: 138 | Batch_idx: 360 |  Loss_1: (0.0353) | Acc_1: (98.79%) (45650/46208)\n",
      "Epoch: 138 | Batch_idx: 370 |  Loss_1: (0.0353) | Acc_1: (98.79%) (46914/47488)\n",
      "Epoch: 138 | Batch_idx: 380 |  Loss_1: (0.0350) | Acc_1: (98.80%) (48184/48768)\n",
      "Epoch: 138 | Batch_idx: 390 |  Loss_1: (0.0348) | Acc_1: (98.80%) (49402/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4757) | Acc: (91.25%) (9125/10000)\n",
      "Epoch: 139 | Batch_idx: 0 |  Loss_1: (0.0496) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 139 | Batch_idx: 10 |  Loss_1: (0.0253) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 139 | Batch_idx: 20 |  Loss_1: (0.0344) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 139 | Batch_idx: 30 |  Loss_1: (0.0353) | Acc_1: (98.92%) (3925/3968)\n",
      "Epoch: 139 | Batch_idx: 40 |  Loss_1: (0.0319) | Acc_1: (98.97%) (5194/5248)\n",
      "Epoch: 139 | Batch_idx: 50 |  Loss_1: (0.0302) | Acc_1: (98.97%) (6461/6528)\n",
      "Epoch: 139 | Batch_idx: 60 |  Loss_1: (0.0295) | Acc_1: (99.04%) (7733/7808)\n",
      "Epoch: 139 | Batch_idx: 70 |  Loss_1: (0.0304) | Acc_1: (98.99%) (8996/9088)\n",
      "Epoch: 139 | Batch_idx: 80 |  Loss_1: (0.0300) | Acc_1: (99.00%) (10264/10368)\n",
      "Epoch: 139 | Batch_idx: 90 |  Loss_1: (0.0307) | Acc_1: (98.98%) (11529/11648)\n",
      "Epoch: 139 | Batch_idx: 100 |  Loss_1: (0.0302) | Acc_1: (98.99%) (12797/12928)\n",
      "Epoch: 139 | Batch_idx: 110 |  Loss_1: (0.0315) | Acc_1: (98.94%) (14058/14208)\n",
      "Epoch: 139 | Batch_idx: 120 |  Loss_1: (0.0308) | Acc_1: (98.97%) (15328/15488)\n",
      "Epoch: 139 | Batch_idx: 130 |  Loss_1: (0.0309) | Acc_1: (98.94%) (16591/16768)\n",
      "Epoch: 139 | Batch_idx: 140 |  Loss_1: (0.0309) | Acc_1: (98.91%) (17852/18048)\n",
      "Epoch: 139 | Batch_idx: 150 |  Loss_1: (0.0311) | Acc_1: (98.93%) (19121/19328)\n",
      "Epoch: 139 | Batch_idx: 160 |  Loss_1: (0.0308) | Acc_1: (98.94%) (20389/20608)\n",
      "Epoch: 139 | Batch_idx: 170 |  Loss_1: (0.0310) | Acc_1: (98.93%) (21653/21888)\n",
      "Epoch: 139 | Batch_idx: 180 |  Loss_1: (0.0310) | Acc_1: (98.93%) (22920/23168)\n",
      "Epoch: 139 | Batch_idx: 190 |  Loss_1: (0.0308) | Acc_1: (98.94%) (24188/24448)\n",
      "Epoch: 139 | Batch_idx: 200 |  Loss_1: (0.0310) | Acc_1: (98.93%) (25453/25728)\n",
      "Epoch: 139 | Batch_idx: 210 |  Loss_1: (0.0312) | Acc_1: (98.92%) (26717/27008)\n",
      "Epoch: 139 | Batch_idx: 220 |  Loss_1: (0.0311) | Acc_1: (98.94%) (27987/28288)\n",
      "Epoch: 139 | Batch_idx: 230 |  Loss_1: (0.0311) | Acc_1: (98.94%) (29254/29568)\n",
      "Epoch: 139 | Batch_idx: 240 |  Loss_1: (0.0315) | Acc_1: (98.93%) (30518/30848)\n",
      "Epoch: 139 | Batch_idx: 250 |  Loss_1: (0.0316) | Acc_1: (98.92%) (31782/32128)\n",
      "Epoch: 139 | Batch_idx: 260 |  Loss_1: (0.0315) | Acc_1: (98.92%) (33048/33408)\n",
      "Epoch: 139 | Batch_idx: 270 |  Loss_1: (0.0313) | Acc_1: (98.92%) (34315/34688)\n",
      "Epoch: 139 | Batch_idx: 280 |  Loss_1: (0.0310) | Acc_1: (98.94%) (35585/35968)\n",
      "Epoch: 139 | Batch_idx: 290 |  Loss_1: (0.0309) | Acc_1: (98.94%) (36854/37248)\n",
      "Epoch: 139 | Batch_idx: 300 |  Loss_1: (0.0306) | Acc_1: (98.95%) (38123/38528)\n",
      "Epoch: 139 | Batch_idx: 310 |  Loss_1: (0.0303) | Acc_1: (98.96%) (39395/39808)\n",
      "Epoch: 139 | Batch_idx: 320 |  Loss_1: (0.0301) | Acc_1: (98.97%) (40664/41088)\n",
      "Epoch: 139 | Batch_idx: 330 |  Loss_1: (0.0304) | Acc_1: (98.96%) (41927/42368)\n",
      "Epoch: 139 | Batch_idx: 340 |  Loss_1: (0.0303) | Acc_1: (98.96%) (43192/43648)\n",
      "Epoch: 139 | Batch_idx: 350 |  Loss_1: (0.0304) | Acc_1: (98.96%) (44459/44928)\n",
      "Epoch: 139 | Batch_idx: 360 |  Loss_1: (0.0301) | Acc_1: (98.96%) (45728/46208)\n",
      "Epoch: 139 | Batch_idx: 370 |  Loss_1: (0.0300) | Acc_1: (98.97%) (46998/47488)\n",
      "Epoch: 139 | Batch_idx: 380 |  Loss_1: (0.0302) | Acc_1: (98.95%) (48257/48768)\n",
      "Epoch: 139 | Batch_idx: 390 |  Loss_1: (0.0303) | Acc_1: (98.95%) (49477/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4939) | Acc: (91.02%) (9102/10000)\n",
      "Epoch: 140 | Batch_idx: 0 |  Loss_1: (0.0489) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 140 | Batch_idx: 10 |  Loss_1: (0.0364) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 140 | Batch_idx: 20 |  Loss_1: (0.0363) | Acc_1: (98.77%) (2655/2688)\n",
      "Epoch: 140 | Batch_idx: 30 |  Loss_1: (0.0379) | Acc_1: (98.77%) (3919/3968)\n",
      "Epoch: 140 | Batch_idx: 40 |  Loss_1: (0.0385) | Acc_1: (98.70%) (5180/5248)\n",
      "Epoch: 140 | Batch_idx: 50 |  Loss_1: (0.0388) | Acc_1: (98.59%) (6436/6528)\n",
      "Epoch: 140 | Batch_idx: 60 |  Loss_1: (0.0377) | Acc_1: (98.64%) (7702/7808)\n",
      "Epoch: 140 | Batch_idx: 70 |  Loss_1: (0.0375) | Acc_1: (98.61%) (8962/9088)\n",
      "Epoch: 140 | Batch_idx: 80 |  Loss_1: (0.0391) | Acc_1: (98.57%) (10220/10368)\n",
      "Epoch: 140 | Batch_idx: 90 |  Loss_1: (0.0393) | Acc_1: (98.58%) (11483/11648)\n",
      "Epoch: 140 | Batch_idx: 100 |  Loss_1: (0.0389) | Acc_1: (98.59%) (12746/12928)\n",
      "Epoch: 140 | Batch_idx: 110 |  Loss_1: (0.0390) | Acc_1: (98.61%) (14010/14208)\n",
      "Epoch: 140 | Batch_idx: 120 |  Loss_1: (0.0395) | Acc_1: (98.58%) (15268/15488)\n",
      "Epoch: 140 | Batch_idx: 130 |  Loss_1: (0.0394) | Acc_1: (98.56%) (16526/16768)\n",
      "Epoch: 140 | Batch_idx: 140 |  Loss_1: (0.0392) | Acc_1: (98.59%) (17793/18048)\n",
      "Epoch: 140 | Batch_idx: 150 |  Loss_1: (0.0390) | Acc_1: (98.59%) (19056/19328)\n",
      "Epoch: 140 | Batch_idx: 160 |  Loss_1: (0.0382) | Acc_1: (98.61%) (20322/20608)\n",
      "Epoch: 140 | Batch_idx: 170 |  Loss_1: (0.0385) | Acc_1: (98.61%) (21583/21888)\n",
      "Epoch: 140 | Batch_idx: 180 |  Loss_1: (0.0379) | Acc_1: (98.63%) (22851/23168)\n",
      "Epoch: 140 | Batch_idx: 190 |  Loss_1: (0.0381) | Acc_1: (98.61%) (24109/24448)\n",
      "Epoch: 140 | Batch_idx: 200 |  Loss_1: (0.0382) | Acc_1: (98.63%) (25376/25728)\n",
      "Epoch: 140 | Batch_idx: 210 |  Loss_1: (0.0377) | Acc_1: (98.65%) (26644/27008)\n",
      "Epoch: 140 | Batch_idx: 220 |  Loss_1: (0.0377) | Acc_1: (98.65%) (27906/28288)\n",
      "Epoch: 140 | Batch_idx: 230 |  Loss_1: (0.0375) | Acc_1: (98.65%) (29169/29568)\n",
      "Epoch: 140 | Batch_idx: 240 |  Loss_1: (0.0378) | Acc_1: (98.64%) (30430/30848)\n",
      "Epoch: 140 | Batch_idx: 250 |  Loss_1: (0.0377) | Acc_1: (98.64%) (31692/32128)\n",
      "Epoch: 140 | Batch_idx: 260 |  Loss_1: (0.0371) | Acc_1: (98.66%) (32959/33408)\n",
      "Epoch: 140 | Batch_idx: 270 |  Loss_1: (0.0368) | Acc_1: (98.66%) (34222/34688)\n",
      "Epoch: 140 | Batch_idx: 280 |  Loss_1: (0.0367) | Acc_1: (98.66%) (35487/35968)\n",
      "Epoch: 140 | Batch_idx: 290 |  Loss_1: (0.0368) | Acc_1: (98.66%) (36749/37248)\n",
      "Epoch: 140 | Batch_idx: 300 |  Loss_1: (0.0366) | Acc_1: (98.68%) (38019/38528)\n",
      "Epoch: 140 | Batch_idx: 310 |  Loss_1: (0.0365) | Acc_1: (98.68%) (39284/39808)\n",
      "Epoch: 140 | Batch_idx: 320 |  Loss_1: (0.0365) | Acc_1: (98.69%) (40548/41088)\n",
      "Epoch: 140 | Batch_idx: 330 |  Loss_1: (0.0364) | Acc_1: (98.68%) (41810/42368)\n",
      "Epoch: 140 | Batch_idx: 340 |  Loss_1: (0.0364) | Acc_1: (98.69%) (43077/43648)\n",
      "Epoch: 140 | Batch_idx: 350 |  Loss_1: (0.0364) | Acc_1: (98.70%) (44343/44928)\n",
      "Epoch: 140 | Batch_idx: 360 |  Loss_1: (0.0365) | Acc_1: (98.69%) (45602/46208)\n",
      "Epoch: 140 | Batch_idx: 370 |  Loss_1: (0.0360) | Acc_1: (98.71%) (46874/47488)\n",
      "Epoch: 140 | Batch_idx: 380 |  Loss_1: (0.0359) | Acc_1: (98.71%) (48138/48768)\n",
      "Epoch: 140 | Batch_idx: 390 |  Loss_1: (0.0358) | Acc_1: (98.71%) (49355/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4802) | Acc: (91.30%) (9130/10000)\n",
      "Epoch: 141 | Batch_idx: 0 |  Loss_1: (0.0355) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 141 | Batch_idx: 10 |  Loss_1: (0.0371) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 141 | Batch_idx: 20 |  Loss_1: (0.0278) | Acc_1: (99.03%) (2662/2688)\n",
      "Epoch: 141 | Batch_idx: 30 |  Loss_1: (0.0287) | Acc_1: (98.99%) (3928/3968)\n",
      "Epoch: 141 | Batch_idx: 40 |  Loss_1: (0.0290) | Acc_1: (98.97%) (5194/5248)\n",
      "Epoch: 141 | Batch_idx: 50 |  Loss_1: (0.0272) | Acc_1: (99.02%) (6464/6528)\n",
      "Epoch: 141 | Batch_idx: 60 |  Loss_1: (0.0272) | Acc_1: (99.04%) (7733/7808)\n",
      "Epoch: 141 | Batch_idx: 70 |  Loss_1: (0.0275) | Acc_1: (99.03%) (9000/9088)\n",
      "Epoch: 141 | Batch_idx: 80 |  Loss_1: (0.0277) | Acc_1: (99.04%) (10268/10368)\n",
      "Epoch: 141 | Batch_idx: 90 |  Loss_1: (0.0276) | Acc_1: (99.04%) (11536/11648)\n",
      "Epoch: 141 | Batch_idx: 100 |  Loss_1: (0.0289) | Acc_1: (98.99%) (12798/12928)\n",
      "Epoch: 141 | Batch_idx: 110 |  Loss_1: (0.0282) | Acc_1: (99.03%) (14070/14208)\n",
      "Epoch: 141 | Batch_idx: 120 |  Loss_1: (0.0282) | Acc_1: (99.03%) (15338/15488)\n",
      "Epoch: 141 | Batch_idx: 130 |  Loss_1: (0.0281) | Acc_1: (99.02%) (16604/16768)\n",
      "Epoch: 141 | Batch_idx: 140 |  Loss_1: (0.0286) | Acc_1: (99.02%) (17871/18048)\n",
      "Epoch: 141 | Batch_idx: 150 |  Loss_1: (0.0288) | Acc_1: (99.01%) (19136/19328)\n",
      "Epoch: 141 | Batch_idx: 160 |  Loss_1: (0.0284) | Acc_1: (99.02%) (20406/20608)\n",
      "Epoch: 141 | Batch_idx: 170 |  Loss_1: (0.0284) | Acc_1: (99.02%) (21674/21888)\n",
      "Epoch: 141 | Batch_idx: 180 |  Loss_1: (0.0281) | Acc_1: (99.04%) (22945/23168)\n",
      "Epoch: 141 | Batch_idx: 190 |  Loss_1: (0.0285) | Acc_1: (99.03%) (24210/24448)\n",
      "Epoch: 141 | Batch_idx: 200 |  Loss_1: (0.0289) | Acc_1: (99.02%) (25477/25728)\n",
      "Epoch: 141 | Batch_idx: 210 |  Loss_1: (0.0291) | Acc_1: (99.04%) (26748/27008)\n",
      "Epoch: 141 | Batch_idx: 220 |  Loss_1: (0.0289) | Acc_1: (99.04%) (28016/28288)\n",
      "Epoch: 141 | Batch_idx: 230 |  Loss_1: (0.0287) | Acc_1: (99.04%) (29284/29568)\n",
      "Epoch: 141 | Batch_idx: 240 |  Loss_1: (0.0286) | Acc_1: (99.05%) (30555/30848)\n",
      "Epoch: 141 | Batch_idx: 250 |  Loss_1: (0.0287) | Acc_1: (99.05%) (31823/32128)\n",
      "Epoch: 141 | Batch_idx: 260 |  Loss_1: (0.0290) | Acc_1: (99.04%) (33087/33408)\n",
      "Epoch: 141 | Batch_idx: 270 |  Loss_1: (0.0293) | Acc_1: (99.02%) (34349/34688)\n",
      "Epoch: 141 | Batch_idx: 280 |  Loss_1: (0.0295) | Acc_1: (99.01%) (35611/35968)\n",
      "Epoch: 141 | Batch_idx: 290 |  Loss_1: (0.0294) | Acc_1: (99.01%) (36878/37248)\n",
      "Epoch: 141 | Batch_idx: 300 |  Loss_1: (0.0295) | Acc_1: (99.00%) (38141/38528)\n",
      "Epoch: 141 | Batch_idx: 310 |  Loss_1: (0.0298) | Acc_1: (98.99%) (39406/39808)\n",
      "Epoch: 141 | Batch_idx: 320 |  Loss_1: (0.0300) | Acc_1: (98.98%) (40670/41088)\n",
      "Epoch: 141 | Batch_idx: 330 |  Loss_1: (0.0304) | Acc_1: (98.96%) (41928/42368)\n",
      "Epoch: 141 | Batch_idx: 340 |  Loss_1: (0.0306) | Acc_1: (98.96%) (43193/43648)\n",
      "Epoch: 141 | Batch_idx: 350 |  Loss_1: (0.0307) | Acc_1: (98.95%) (44457/44928)\n",
      "Epoch: 141 | Batch_idx: 360 |  Loss_1: (0.0308) | Acc_1: (98.95%) (45723/46208)\n",
      "Epoch: 141 | Batch_idx: 370 |  Loss_1: (0.0309) | Acc_1: (98.94%) (46985/47488)\n",
      "Epoch: 141 | Batch_idx: 380 |  Loss_1: (0.0315) | Acc_1: (98.93%) (48247/48768)\n",
      "Epoch: 141 | Batch_idx: 390 |  Loss_1: (0.0312) | Acc_1: (98.94%) (49471/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5129) | Acc: (91.00%) (9100/10000)\n",
      "Epoch: 142 | Batch_idx: 0 |  Loss_1: (0.0382) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 142 | Batch_idx: 10 |  Loss_1: (0.0352) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 142 | Batch_idx: 20 |  Loss_1: (0.0393) | Acc_1: (98.74%) (2654/2688)\n",
      "Epoch: 142 | Batch_idx: 30 |  Loss_1: (0.0384) | Acc_1: (98.77%) (3919/3968)\n",
      "Epoch: 142 | Batch_idx: 40 |  Loss_1: (0.0353) | Acc_1: (98.88%) (5189/5248)\n",
      "Epoch: 142 | Batch_idx: 50 |  Loss_1: (0.0342) | Acc_1: (98.90%) (6456/6528)\n",
      "Epoch: 142 | Batch_idx: 60 |  Loss_1: (0.0332) | Acc_1: (98.91%) (7723/7808)\n",
      "Epoch: 142 | Batch_idx: 70 |  Loss_1: (0.0329) | Acc_1: (98.93%) (8991/9088)\n",
      "Epoch: 142 | Batch_idx: 80 |  Loss_1: (0.0323) | Acc_1: (98.96%) (10260/10368)\n",
      "Epoch: 142 | Batch_idx: 90 |  Loss_1: (0.0323) | Acc_1: (98.95%) (11526/11648)\n",
      "Epoch: 142 | Batch_idx: 100 |  Loss_1: (0.0315) | Acc_1: (98.96%) (12794/12928)\n",
      "Epoch: 142 | Batch_idx: 110 |  Loss_1: (0.0319) | Acc_1: (98.93%) (14056/14208)\n",
      "Epoch: 142 | Batch_idx: 120 |  Loss_1: (0.0322) | Acc_1: (98.90%) (15317/15488)\n",
      "Epoch: 142 | Batch_idx: 130 |  Loss_1: (0.0319) | Acc_1: (98.90%) (16584/16768)\n",
      "Epoch: 142 | Batch_idx: 140 |  Loss_1: (0.0317) | Acc_1: (98.89%) (17848/18048)\n",
      "Epoch: 142 | Batch_idx: 150 |  Loss_1: (0.0321) | Acc_1: (98.89%) (19113/19328)\n",
      "Epoch: 142 | Batch_idx: 160 |  Loss_1: (0.0325) | Acc_1: (98.86%) (20373/20608)\n",
      "Epoch: 142 | Batch_idx: 170 |  Loss_1: (0.0333) | Acc_1: (98.83%) (21632/21888)\n",
      "Epoch: 142 | Batch_idx: 180 |  Loss_1: (0.0338) | Acc_1: (98.82%) (22895/23168)\n",
      "Epoch: 142 | Batch_idx: 190 |  Loss_1: (0.0346) | Acc_1: (98.80%) (24155/24448)\n",
      "Epoch: 142 | Batch_idx: 200 |  Loss_1: (0.0350) | Acc_1: (98.81%) (25421/25728)\n",
      "Epoch: 142 | Batch_idx: 210 |  Loss_1: (0.0350) | Acc_1: (98.82%) (26689/27008)\n",
      "Epoch: 142 | Batch_idx: 220 |  Loss_1: (0.0348) | Acc_1: (98.81%) (27952/28288)\n",
      "Epoch: 142 | Batch_idx: 230 |  Loss_1: (0.0353) | Acc_1: (98.80%) (29213/29568)\n",
      "Epoch: 142 | Batch_idx: 240 |  Loss_1: (0.0357) | Acc_1: (98.79%) (30474/30848)\n",
      "Epoch: 142 | Batch_idx: 250 |  Loss_1: (0.0356) | Acc_1: (98.78%) (31736/32128)\n",
      "Epoch: 142 | Batch_idx: 260 |  Loss_1: (0.0352) | Acc_1: (98.78%) (33002/33408)\n",
      "Epoch: 142 | Batch_idx: 270 |  Loss_1: (0.0351) | Acc_1: (98.79%) (34268/34688)\n",
      "Epoch: 142 | Batch_idx: 280 |  Loss_1: (0.0350) | Acc_1: (98.80%) (35535/35968)\n",
      "Epoch: 142 | Batch_idx: 290 |  Loss_1: (0.0350) | Acc_1: (98.79%) (36799/37248)\n",
      "Epoch: 142 | Batch_idx: 300 |  Loss_1: (0.0351) | Acc_1: (98.79%) (38060/38528)\n",
      "Epoch: 142 | Batch_idx: 310 |  Loss_1: (0.0354) | Acc_1: (98.78%) (39321/39808)\n",
      "Epoch: 142 | Batch_idx: 320 |  Loss_1: (0.0354) | Acc_1: (98.77%) (40584/41088)\n",
      "Epoch: 142 | Batch_idx: 330 |  Loss_1: (0.0352) | Acc_1: (98.78%) (41852/42368)\n",
      "Epoch: 142 | Batch_idx: 340 |  Loss_1: (0.0358) | Acc_1: (98.77%) (43109/43648)\n",
      "Epoch: 142 | Batch_idx: 350 |  Loss_1: (0.0359) | Acc_1: (98.77%) (44376/44928)\n",
      "Epoch: 142 | Batch_idx: 360 |  Loss_1: (0.0359) | Acc_1: (98.77%) (45639/46208)\n",
      "Epoch: 142 | Batch_idx: 370 |  Loss_1: (0.0358) | Acc_1: (98.76%) (46899/47488)\n",
      "Epoch: 142 | Batch_idx: 380 |  Loss_1: (0.0355) | Acc_1: (98.77%) (48167/48768)\n",
      "Epoch: 142 | Batch_idx: 390 |  Loss_1: (0.0358) | Acc_1: (98.78%) (49388/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4596) | Acc: (91.45%) (9145/10000)\n",
      "Epoch: 143 | Batch_idx: 0 |  Loss_1: (0.0079) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 143 | Batch_idx: 10 |  Loss_1: (0.0449) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 143 | Batch_idx: 20 |  Loss_1: (0.0428) | Acc_1: (98.77%) (2655/2688)\n",
      "Epoch: 143 | Batch_idx: 30 |  Loss_1: (0.0508) | Acc_1: (98.59%) (3912/3968)\n",
      "Epoch: 143 | Batch_idx: 40 |  Loss_1: (0.0457) | Acc_1: (98.61%) (5175/5248)\n",
      "Epoch: 143 | Batch_idx: 50 |  Loss_1: (0.0423) | Acc_1: (98.73%) (6445/6528)\n",
      "Epoch: 143 | Batch_idx: 60 |  Loss_1: (0.0405) | Acc_1: (98.73%) (7709/7808)\n",
      "Epoch: 143 | Batch_idx: 70 |  Loss_1: (0.0413) | Acc_1: (98.69%) (8969/9088)\n",
      "Epoch: 143 | Batch_idx: 80 |  Loss_1: (0.0404) | Acc_1: (98.76%) (10239/10368)\n",
      "Epoch: 143 | Batch_idx: 90 |  Loss_1: (0.0392) | Acc_1: (98.78%) (11506/11648)\n",
      "Epoch: 143 | Batch_idx: 100 |  Loss_1: (0.0377) | Acc_1: (98.83%) (12777/12928)\n",
      "Epoch: 143 | Batch_idx: 110 |  Loss_1: (0.0376) | Acc_1: (98.86%) (14046/14208)\n",
      "Epoch: 143 | Batch_idx: 120 |  Loss_1: (0.0378) | Acc_1: (98.84%) (15308/15488)\n",
      "Epoch: 143 | Batch_idx: 130 |  Loss_1: (0.0369) | Acc_1: (98.87%) (16578/16768)\n",
      "Epoch: 143 | Batch_idx: 140 |  Loss_1: (0.0376) | Acc_1: (98.85%) (17840/18048)\n",
      "Epoch: 143 | Batch_idx: 150 |  Loss_1: (0.0375) | Acc_1: (98.85%) (19105/19328)\n",
      "Epoch: 143 | Batch_idx: 160 |  Loss_1: (0.0378) | Acc_1: (98.82%) (20365/20608)\n",
      "Epoch: 143 | Batch_idx: 170 |  Loss_1: (0.0376) | Acc_1: (98.82%) (21629/21888)\n",
      "Epoch: 143 | Batch_idx: 180 |  Loss_1: (0.0367) | Acc_1: (98.84%) (22899/23168)\n",
      "Epoch: 143 | Batch_idx: 190 |  Loss_1: (0.0361) | Acc_1: (98.85%) (24166/24448)\n",
      "Epoch: 143 | Batch_idx: 200 |  Loss_1: (0.0359) | Acc_1: (98.85%) (25433/25728)\n",
      "Epoch: 143 | Batch_idx: 210 |  Loss_1: (0.0361) | Acc_1: (98.85%) (26697/27008)\n",
      "Epoch: 143 | Batch_idx: 220 |  Loss_1: (0.0362) | Acc_1: (98.85%) (27962/28288)\n",
      "Epoch: 143 | Batch_idx: 230 |  Loss_1: (0.0360) | Acc_1: (98.85%) (29227/29568)\n",
      "Epoch: 143 | Batch_idx: 240 |  Loss_1: (0.0363) | Acc_1: (98.83%) (30486/30848)\n",
      "Epoch: 143 | Batch_idx: 250 |  Loss_1: (0.0361) | Acc_1: (98.83%) (31751/32128)\n",
      "Epoch: 143 | Batch_idx: 260 |  Loss_1: (0.0359) | Acc_1: (98.83%) (33016/33408)\n",
      "Epoch: 143 | Batch_idx: 270 |  Loss_1: (0.0354) | Acc_1: (98.85%) (34288/34688)\n",
      "Epoch: 143 | Batch_idx: 280 |  Loss_1: (0.0348) | Acc_1: (98.87%) (35562/35968)\n",
      "Epoch: 143 | Batch_idx: 290 |  Loss_1: (0.0344) | Acc_1: (98.88%) (36830/37248)\n",
      "Epoch: 143 | Batch_idx: 300 |  Loss_1: (0.0342) | Acc_1: (98.89%) (38099/38528)\n",
      "Epoch: 143 | Batch_idx: 310 |  Loss_1: (0.0340) | Acc_1: (98.89%) (39365/39808)\n",
      "Epoch: 143 | Batch_idx: 320 |  Loss_1: (0.0336) | Acc_1: (98.90%) (40637/41088)\n",
      "Epoch: 143 | Batch_idx: 330 |  Loss_1: (0.0336) | Acc_1: (98.90%) (41902/42368)\n",
      "Epoch: 143 | Batch_idx: 340 |  Loss_1: (0.0333) | Acc_1: (98.92%) (43175/43648)\n",
      "Epoch: 143 | Batch_idx: 350 |  Loss_1: (0.0332) | Acc_1: (98.91%) (44440/44928)\n",
      "Epoch: 143 | Batch_idx: 360 |  Loss_1: (0.0330) | Acc_1: (98.91%) (45706/46208)\n",
      "Epoch: 143 | Batch_idx: 370 |  Loss_1: (0.0334) | Acc_1: (98.90%) (46965/47488)\n",
      "Epoch: 143 | Batch_idx: 380 |  Loss_1: (0.0332) | Acc_1: (98.90%) (48233/48768)\n",
      "Epoch: 143 | Batch_idx: 390 |  Loss_1: (0.0333) | Acc_1: (98.89%) (49444/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4664) | Acc: (91.39%) (9139/10000)\n",
      "Epoch: 144 | Batch_idx: 0 |  Loss_1: (0.0196) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 144 | Batch_idx: 10 |  Loss_1: (0.0450) | Acc_1: (98.22%) (1383/1408)\n",
      "Epoch: 144 | Batch_idx: 20 |  Loss_1: (0.0468) | Acc_1: (98.33%) (2643/2688)\n",
      "Epoch: 144 | Batch_idx: 30 |  Loss_1: (0.0471) | Acc_1: (98.44%) (3906/3968)\n",
      "Epoch: 144 | Batch_idx: 40 |  Loss_1: (0.0456) | Acc_1: (98.49%) (5169/5248)\n",
      "Epoch: 144 | Batch_idx: 50 |  Loss_1: (0.0427) | Acc_1: (98.56%) (6434/6528)\n",
      "Epoch: 144 | Batch_idx: 60 |  Loss_1: (0.0409) | Acc_1: (98.59%) (7698/7808)\n",
      "Epoch: 144 | Batch_idx: 70 |  Loss_1: (0.0402) | Acc_1: (98.61%) (8962/9088)\n",
      "Epoch: 144 | Batch_idx: 80 |  Loss_1: (0.0390) | Acc_1: (98.66%) (10229/10368)\n",
      "Epoch: 144 | Batch_idx: 90 |  Loss_1: (0.0374) | Acc_1: (98.70%) (11497/11648)\n",
      "Epoch: 144 | Batch_idx: 100 |  Loss_1: (0.0363) | Acc_1: (98.76%) (12768/12928)\n",
      "Epoch: 144 | Batch_idx: 110 |  Loss_1: (0.0350) | Acc_1: (98.80%) (14038/14208)\n",
      "Epoch: 144 | Batch_idx: 120 |  Loss_1: (0.0355) | Acc_1: (98.77%) (15298/15488)\n",
      "Epoch: 144 | Batch_idx: 130 |  Loss_1: (0.0353) | Acc_1: (98.78%) (16564/16768)\n",
      "Epoch: 144 | Batch_idx: 140 |  Loss_1: (0.0350) | Acc_1: (98.81%) (17834/18048)\n",
      "Epoch: 144 | Batch_idx: 150 |  Loss_1: (0.0357) | Acc_1: (98.79%) (19095/19328)\n",
      "Epoch: 144 | Batch_idx: 160 |  Loss_1: (0.0352) | Acc_1: (98.80%) (20360/20608)\n",
      "Epoch: 144 | Batch_idx: 170 |  Loss_1: (0.0352) | Acc_1: (98.78%) (21621/21888)\n",
      "Epoch: 144 | Batch_idx: 180 |  Loss_1: (0.0350) | Acc_1: (98.79%) (22888/23168)\n",
      "Epoch: 144 | Batch_idx: 190 |  Loss_1: (0.0344) | Acc_1: (98.81%) (24156/24448)\n",
      "Epoch: 144 | Batch_idx: 200 |  Loss_1: (0.0336) | Acc_1: (98.85%) (25431/25728)\n",
      "Epoch: 144 | Batch_idx: 210 |  Loss_1: (0.0339) | Acc_1: (98.85%) (26697/27008)\n",
      "Epoch: 144 | Batch_idx: 220 |  Loss_1: (0.0341) | Acc_1: (98.84%) (27960/28288)\n",
      "Epoch: 144 | Batch_idx: 230 |  Loss_1: (0.0339) | Acc_1: (98.83%) (29223/29568)\n",
      "Epoch: 144 | Batch_idx: 240 |  Loss_1: (0.0345) | Acc_1: (98.81%) (30481/30848)\n",
      "Epoch: 144 | Batch_idx: 250 |  Loss_1: (0.0347) | Acc_1: (98.79%) (31740/32128)\n",
      "Epoch: 144 | Batch_idx: 260 |  Loss_1: (0.0345) | Acc_1: (98.80%) (33006/33408)\n",
      "Epoch: 144 | Batch_idx: 270 |  Loss_1: (0.0345) | Acc_1: (98.80%) (34273/34688)\n",
      "Epoch: 144 | Batch_idx: 280 |  Loss_1: (0.0344) | Acc_1: (98.82%) (35542/35968)\n",
      "Epoch: 144 | Batch_idx: 290 |  Loss_1: (0.0345) | Acc_1: (98.81%) (36806/37248)\n",
      "Epoch: 144 | Batch_idx: 300 |  Loss_1: (0.0348) | Acc_1: (98.81%) (38071/38528)\n",
      "Epoch: 144 | Batch_idx: 310 |  Loss_1: (0.0347) | Acc_1: (98.82%) (39339/39808)\n",
      "Epoch: 144 | Batch_idx: 320 |  Loss_1: (0.0343) | Acc_1: (98.84%) (40611/41088)\n",
      "Epoch: 144 | Batch_idx: 330 |  Loss_1: (0.0341) | Acc_1: (98.84%) (41877/42368)\n",
      "Epoch: 144 | Batch_idx: 340 |  Loss_1: (0.0337) | Acc_1: (98.85%) (43147/43648)\n",
      "Epoch: 144 | Batch_idx: 350 |  Loss_1: (0.0341) | Acc_1: (98.84%) (44407/44928)\n",
      "Epoch: 144 | Batch_idx: 360 |  Loss_1: (0.0338) | Acc_1: (98.85%) (45676/46208)\n",
      "Epoch: 144 | Batch_idx: 370 |  Loss_1: (0.0342) | Acc_1: (98.84%) (46937/47488)\n",
      "Epoch: 144 | Batch_idx: 380 |  Loss_1: (0.0341) | Acc_1: (98.85%) (48205/48768)\n",
      "Epoch: 144 | Batch_idx: 390 |  Loss_1: (0.0344) | Acc_1: (98.85%) (49423/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5105) | Acc: (90.70%) (9070/10000)\n",
      "Epoch: 145 | Batch_idx: 0 |  Loss_1: (0.0426) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 145 | Batch_idx: 10 |  Loss_1: (0.0306) | Acc_1: (99.01%) (1394/1408)\n",
      "Epoch: 145 | Batch_idx: 20 |  Loss_1: (0.0357) | Acc_1: (98.70%) (2653/2688)\n",
      "Epoch: 145 | Batch_idx: 30 |  Loss_1: (0.0362) | Acc_1: (98.69%) (3916/3968)\n",
      "Epoch: 145 | Batch_idx: 40 |  Loss_1: (0.0347) | Acc_1: (98.78%) (5184/5248)\n",
      "Epoch: 145 | Batch_idx: 50 |  Loss_1: (0.0331) | Acc_1: (98.85%) (6453/6528)\n",
      "Epoch: 145 | Batch_idx: 60 |  Loss_1: (0.0335) | Acc_1: (98.85%) (7718/7808)\n",
      "Epoch: 145 | Batch_idx: 70 |  Loss_1: (0.0355) | Acc_1: (98.82%) (8981/9088)\n",
      "Epoch: 145 | Batch_idx: 80 |  Loss_1: (0.0349) | Acc_1: (98.84%) (10248/10368)\n",
      "Epoch: 145 | Batch_idx: 90 |  Loss_1: (0.0342) | Acc_1: (98.88%) (11518/11648)\n",
      "Epoch: 145 | Batch_idx: 100 |  Loss_1: (0.0362) | Acc_1: (98.84%) (12778/12928)\n",
      "Epoch: 145 | Batch_idx: 110 |  Loss_1: (0.0357) | Acc_1: (98.85%) (14044/14208)\n",
      "Epoch: 145 | Batch_idx: 120 |  Loss_1: (0.0347) | Acc_1: (98.89%) (15316/15488)\n",
      "Epoch: 145 | Batch_idx: 130 |  Loss_1: (0.0346) | Acc_1: (98.87%) (16578/16768)\n",
      "Epoch: 145 | Batch_idx: 140 |  Loss_1: (0.0340) | Acc_1: (98.87%) (17844/18048)\n",
      "Epoch: 145 | Batch_idx: 150 |  Loss_1: (0.0333) | Acc_1: (98.88%) (19111/19328)\n",
      "Epoch: 145 | Batch_idx: 160 |  Loss_1: (0.0327) | Acc_1: (98.90%) (20382/20608)\n",
      "Epoch: 145 | Batch_idx: 170 |  Loss_1: (0.0328) | Acc_1: (98.89%) (21645/21888)\n",
      "Epoch: 145 | Batch_idx: 180 |  Loss_1: (0.0335) | Acc_1: (98.87%) (22907/23168)\n",
      "Epoch: 145 | Batch_idx: 190 |  Loss_1: (0.0345) | Acc_1: (98.86%) (24169/24448)\n",
      "Epoch: 145 | Batch_idx: 200 |  Loss_1: (0.0341) | Acc_1: (98.87%) (25438/25728)\n",
      "Epoch: 145 | Batch_idx: 210 |  Loss_1: (0.0342) | Acc_1: (98.86%) (26700/27008)\n",
      "Epoch: 145 | Batch_idx: 220 |  Loss_1: (0.0349) | Acc_1: (98.85%) (27962/28288)\n",
      "Epoch: 145 | Batch_idx: 230 |  Loss_1: (0.0355) | Acc_1: (98.84%) (29224/29568)\n",
      "Epoch: 145 | Batch_idx: 240 |  Loss_1: (0.0353) | Acc_1: (98.83%) (30487/30848)\n",
      "Epoch: 145 | Batch_idx: 250 |  Loss_1: (0.0349) | Acc_1: (98.85%) (31760/32128)\n",
      "Epoch: 145 | Batch_idx: 260 |  Loss_1: (0.0350) | Acc_1: (98.85%) (33024/33408)\n",
      "Epoch: 145 | Batch_idx: 270 |  Loss_1: (0.0346) | Acc_1: (98.87%) (34295/34688)\n",
      "Epoch: 145 | Batch_idx: 280 |  Loss_1: (0.0346) | Acc_1: (98.85%) (35556/35968)\n",
      "Epoch: 145 | Batch_idx: 290 |  Loss_1: (0.0344) | Acc_1: (98.86%) (36824/37248)\n",
      "Epoch: 145 | Batch_idx: 300 |  Loss_1: (0.0344) | Acc_1: (98.85%) (38085/38528)\n",
      "Epoch: 145 | Batch_idx: 310 |  Loss_1: (0.0344) | Acc_1: (98.86%) (39354/39808)\n",
      "Epoch: 145 | Batch_idx: 320 |  Loss_1: (0.0342) | Acc_1: (98.86%) (40621/41088)\n",
      "Epoch: 145 | Batch_idx: 330 |  Loss_1: (0.0342) | Acc_1: (98.87%) (41889/42368)\n",
      "Epoch: 145 | Batch_idx: 340 |  Loss_1: (0.0345) | Acc_1: (98.86%) (43149/43648)\n",
      "Epoch: 145 | Batch_idx: 350 |  Loss_1: (0.0342) | Acc_1: (98.87%) (44421/44928)\n",
      "Epoch: 145 | Batch_idx: 360 |  Loss_1: (0.0345) | Acc_1: (98.86%) (45682/46208)\n",
      "Epoch: 145 | Batch_idx: 370 |  Loss_1: (0.0342) | Acc_1: (98.87%) (46952/47488)\n",
      "Epoch: 145 | Batch_idx: 380 |  Loss_1: (0.0346) | Acc_1: (98.87%) (48216/48768)\n",
      "Epoch: 145 | Batch_idx: 390 |  Loss_1: (0.0346) | Acc_1: (98.87%) (49436/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4927) | Acc: (91.15%) (9115/10000)\n",
      "Epoch: 146 | Batch_idx: 0 |  Loss_1: (0.0088) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 146 | Batch_idx: 10 |  Loss_1: (0.0275) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 146 | Batch_idx: 20 |  Loss_1: (0.0297) | Acc_1: (99.11%) (2664/2688)\n",
      "Epoch: 146 | Batch_idx: 30 |  Loss_1: (0.0308) | Acc_1: (98.99%) (3928/3968)\n",
      "Epoch: 146 | Batch_idx: 40 |  Loss_1: (0.0318) | Acc_1: (98.93%) (5192/5248)\n",
      "Epoch: 146 | Batch_idx: 50 |  Loss_1: (0.0339) | Acc_1: (98.82%) (6451/6528)\n",
      "Epoch: 146 | Batch_idx: 60 |  Loss_1: (0.0329) | Acc_1: (98.87%) (7720/7808)\n",
      "Epoch: 146 | Batch_idx: 70 |  Loss_1: (0.0325) | Acc_1: (98.86%) (8984/9088)\n",
      "Epoch: 146 | Batch_idx: 80 |  Loss_1: (0.0308) | Acc_1: (98.94%) (10258/10368)\n",
      "Epoch: 146 | Batch_idx: 90 |  Loss_1: (0.0307) | Acc_1: (98.97%) (11528/11648)\n",
      "Epoch: 146 | Batch_idx: 100 |  Loss_1: (0.0306) | Acc_1: (99.00%) (12799/12928)\n",
      "Epoch: 146 | Batch_idx: 110 |  Loss_1: (0.0309) | Acc_1: (99.01%) (14067/14208)\n",
      "Epoch: 146 | Batch_idx: 120 |  Loss_1: (0.0307) | Acc_1: (99.01%) (15335/15488)\n",
      "Epoch: 146 | Batch_idx: 130 |  Loss_1: (0.0302) | Acc_1: (99.03%) (16605/16768)\n",
      "Epoch: 146 | Batch_idx: 140 |  Loss_1: (0.0297) | Acc_1: (99.04%) (17875/18048)\n",
      "Epoch: 146 | Batch_idx: 150 |  Loss_1: (0.0297) | Acc_1: (99.03%) (19140/19328)\n",
      "Epoch: 146 | Batch_idx: 160 |  Loss_1: (0.0301) | Acc_1: (99.03%) (20408/20608)\n",
      "Epoch: 146 | Batch_idx: 170 |  Loss_1: (0.0302) | Acc_1: (99.01%) (21672/21888)\n",
      "Epoch: 146 | Batch_idx: 180 |  Loss_1: (0.0305) | Acc_1: (99.00%) (22937/23168)\n",
      "Epoch: 146 | Batch_idx: 190 |  Loss_1: (0.0308) | Acc_1: (99.00%) (24204/24448)\n",
      "Epoch: 146 | Batch_idx: 200 |  Loss_1: (0.0307) | Acc_1: (99.01%) (25473/25728)\n",
      "Epoch: 146 | Batch_idx: 210 |  Loss_1: (0.0309) | Acc_1: (98.99%) (26735/27008)\n",
      "Epoch: 146 | Batch_idx: 220 |  Loss_1: (0.0307) | Acc_1: (99.00%) (28004/28288)\n",
      "Epoch: 146 | Batch_idx: 230 |  Loss_1: (0.0311) | Acc_1: (98.98%) (29266/29568)\n",
      "Epoch: 146 | Batch_idx: 240 |  Loss_1: (0.0315) | Acc_1: (98.97%) (30531/30848)\n",
      "Epoch: 146 | Batch_idx: 250 |  Loss_1: (0.0312) | Acc_1: (98.98%) (31799/32128)\n",
      "Epoch: 146 | Batch_idx: 260 |  Loss_1: (0.0316) | Acc_1: (98.96%) (33060/33408)\n",
      "Epoch: 146 | Batch_idx: 270 |  Loss_1: (0.0315) | Acc_1: (98.96%) (34326/34688)\n",
      "Epoch: 146 | Batch_idx: 280 |  Loss_1: (0.0318) | Acc_1: (98.94%) (35585/35968)\n",
      "Epoch: 146 | Batch_idx: 290 |  Loss_1: (0.0319) | Acc_1: (98.93%) (36850/37248)\n",
      "Epoch: 146 | Batch_idx: 300 |  Loss_1: (0.0318) | Acc_1: (98.93%) (38115/38528)\n",
      "Epoch: 146 | Batch_idx: 310 |  Loss_1: (0.0317) | Acc_1: (98.92%) (39380/39808)\n",
      "Epoch: 146 | Batch_idx: 320 |  Loss_1: (0.0317) | Acc_1: (98.91%) (40642/41088)\n",
      "Epoch: 146 | Batch_idx: 330 |  Loss_1: (0.0324) | Acc_1: (98.89%) (41897/42368)\n",
      "Epoch: 146 | Batch_idx: 340 |  Loss_1: (0.0323) | Acc_1: (98.89%) (43162/43648)\n",
      "Epoch: 146 | Batch_idx: 350 |  Loss_1: (0.0322) | Acc_1: (98.89%) (44431/44928)\n",
      "Epoch: 146 | Batch_idx: 360 |  Loss_1: (0.0319) | Acc_1: (98.90%) (45699/46208)\n",
      "Epoch: 146 | Batch_idx: 370 |  Loss_1: (0.0319) | Acc_1: (98.90%) (46968/47488)\n",
      "Epoch: 146 | Batch_idx: 380 |  Loss_1: (0.0318) | Acc_1: (98.91%) (48237/48768)\n",
      "Epoch: 146 | Batch_idx: 390 |  Loss_1: (0.0317) | Acc_1: (98.92%) (49460/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4508) | Acc: (90.94%) (9094/10000)\n",
      "Epoch: 147 | Batch_idx: 0 |  Loss_1: (0.0864) | Acc_1: (96.09%) (123/128)\n",
      "Epoch: 147 | Batch_idx: 10 |  Loss_1: (0.0271) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 147 | Batch_idx: 20 |  Loss_1: (0.0229) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 147 | Batch_idx: 30 |  Loss_1: (0.0257) | Acc_1: (99.19%) (3936/3968)\n",
      "Epoch: 147 | Batch_idx: 40 |  Loss_1: (0.0250) | Acc_1: (99.20%) (5206/5248)\n",
      "Epoch: 147 | Batch_idx: 50 |  Loss_1: (0.0254) | Acc_1: (99.14%) (6472/6528)\n",
      "Epoch: 147 | Batch_idx: 60 |  Loss_1: (0.0267) | Acc_1: (99.14%) (7741/7808)\n",
      "Epoch: 147 | Batch_idx: 70 |  Loss_1: (0.0271) | Acc_1: (99.14%) (9010/9088)\n",
      "Epoch: 147 | Batch_idx: 80 |  Loss_1: (0.0263) | Acc_1: (99.17%) (10282/10368)\n",
      "Epoch: 147 | Batch_idx: 90 |  Loss_1: (0.0284) | Acc_1: (99.12%) (11546/11648)\n",
      "Epoch: 147 | Batch_idx: 100 |  Loss_1: (0.0275) | Acc_1: (99.16%) (12819/12928)\n",
      "Epoch: 147 | Batch_idx: 110 |  Loss_1: (0.0278) | Acc_1: (99.16%) (14088/14208)\n",
      "Epoch: 147 | Batch_idx: 120 |  Loss_1: (0.0273) | Acc_1: (99.15%) (15357/15488)\n",
      "Epoch: 147 | Batch_idx: 130 |  Loss_1: (0.0267) | Acc_1: (99.15%) (16626/16768)\n",
      "Epoch: 147 | Batch_idx: 140 |  Loss_1: (0.0268) | Acc_1: (99.14%) (17893/18048)\n",
      "Epoch: 147 | Batch_idx: 150 |  Loss_1: (0.0275) | Acc_1: (99.14%) (19162/19328)\n",
      "Epoch: 147 | Batch_idx: 160 |  Loss_1: (0.0276) | Acc_1: (99.13%) (20428/20608)\n",
      "Epoch: 147 | Batch_idx: 170 |  Loss_1: (0.0281) | Acc_1: (99.12%) (21696/21888)\n",
      "Epoch: 147 | Batch_idx: 180 |  Loss_1: (0.0282) | Acc_1: (99.13%) (22967/23168)\n",
      "Epoch: 147 | Batch_idx: 190 |  Loss_1: (0.0287) | Acc_1: (99.12%) (24233/24448)\n",
      "Epoch: 147 | Batch_idx: 200 |  Loss_1: (0.0290) | Acc_1: (99.11%) (25499/25728)\n",
      "Epoch: 147 | Batch_idx: 210 |  Loss_1: (0.0294) | Acc_1: (99.09%) (26762/27008)\n",
      "Epoch: 147 | Batch_idx: 220 |  Loss_1: (0.0292) | Acc_1: (99.08%) (28029/28288)\n",
      "Epoch: 147 | Batch_idx: 230 |  Loss_1: (0.0289) | Acc_1: (99.09%) (29299/29568)\n",
      "Epoch: 147 | Batch_idx: 240 |  Loss_1: (0.0296) | Acc_1: (99.07%) (30561/30848)\n",
      "Epoch: 147 | Batch_idx: 250 |  Loss_1: (0.0300) | Acc_1: (99.05%) (31823/32128)\n",
      "Epoch: 147 | Batch_idx: 260 |  Loss_1: (0.0299) | Acc_1: (99.06%) (33093/33408)\n",
      "Epoch: 147 | Batch_idx: 270 |  Loss_1: (0.0298) | Acc_1: (99.07%) (34364/34688)\n",
      "Epoch: 147 | Batch_idx: 280 |  Loss_1: (0.0298) | Acc_1: (99.07%) (35634/35968)\n",
      "Epoch: 147 | Batch_idx: 290 |  Loss_1: (0.0298) | Acc_1: (99.06%) (36899/37248)\n",
      "Epoch: 147 | Batch_idx: 300 |  Loss_1: (0.0300) | Acc_1: (99.06%) (38165/38528)\n",
      "Epoch: 147 | Batch_idx: 310 |  Loss_1: (0.0304) | Acc_1: (99.05%) (39429/39808)\n",
      "Epoch: 147 | Batch_idx: 320 |  Loss_1: (0.0301) | Acc_1: (99.06%) (40700/41088)\n",
      "Epoch: 147 | Batch_idx: 330 |  Loss_1: (0.0303) | Acc_1: (99.05%) (41967/42368)\n",
      "Epoch: 147 | Batch_idx: 340 |  Loss_1: (0.0302) | Acc_1: (99.06%) (43238/43648)\n",
      "Epoch: 147 | Batch_idx: 350 |  Loss_1: (0.0300) | Acc_1: (99.07%) (44510/44928)\n",
      "Epoch: 147 | Batch_idx: 360 |  Loss_1: (0.0302) | Acc_1: (99.05%) (45771/46208)\n",
      "Epoch: 147 | Batch_idx: 370 |  Loss_1: (0.0301) | Acc_1: (99.05%) (47038/47488)\n",
      "Epoch: 147 | Batch_idx: 380 |  Loss_1: (0.0305) | Acc_1: (99.04%) (48298/48768)\n",
      "Epoch: 147 | Batch_idx: 390 |  Loss_1: (0.0306) | Acc_1: (99.03%) (49516/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4807) | Acc: (91.43%) (9143/10000)\n",
      "Epoch: 148 | Batch_idx: 0 |  Loss_1: (0.0204) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 148 | Batch_idx: 10 |  Loss_1: (0.0327) | Acc_1: (98.58%) (1388/1408)\n",
      "Epoch: 148 | Batch_idx: 20 |  Loss_1: (0.0385) | Acc_1: (98.47%) (2647/2688)\n",
      "Epoch: 148 | Batch_idx: 30 |  Loss_1: (0.0407) | Acc_1: (98.51%) (3909/3968)\n",
      "Epoch: 148 | Batch_idx: 40 |  Loss_1: (0.0383) | Acc_1: (98.57%) (5173/5248)\n",
      "Epoch: 148 | Batch_idx: 50 |  Loss_1: (0.0369) | Acc_1: (98.62%) (6438/6528)\n",
      "Epoch: 148 | Batch_idx: 60 |  Loss_1: (0.0362) | Acc_1: (98.63%) (7701/7808)\n",
      "Epoch: 148 | Batch_idx: 70 |  Loss_1: (0.0363) | Acc_1: (98.61%) (8962/9088)\n",
      "Epoch: 148 | Batch_idx: 80 |  Loss_1: (0.0370) | Acc_1: (98.63%) (10226/10368)\n",
      "Epoch: 148 | Batch_idx: 90 |  Loss_1: (0.0365) | Acc_1: (98.63%) (11488/11648)\n",
      "Epoch: 148 | Batch_idx: 100 |  Loss_1: (0.0354) | Acc_1: (98.69%) (12759/12928)\n",
      "Epoch: 148 | Batch_idx: 110 |  Loss_1: (0.0357) | Acc_1: (98.69%) (14022/14208)\n",
      "Epoch: 148 | Batch_idx: 120 |  Loss_1: (0.0354) | Acc_1: (98.70%) (15286/15488)\n",
      "Epoch: 148 | Batch_idx: 130 |  Loss_1: (0.0344) | Acc_1: (98.72%) (16553/16768)\n",
      "Epoch: 148 | Batch_idx: 140 |  Loss_1: (0.0357) | Acc_1: (98.69%) (17812/18048)\n",
      "Epoch: 148 | Batch_idx: 150 |  Loss_1: (0.0364) | Acc_1: (98.67%) (19071/19328)\n",
      "Epoch: 148 | Batch_idx: 160 |  Loss_1: (0.0360) | Acc_1: (98.70%) (20340/20608)\n",
      "Epoch: 148 | Batch_idx: 170 |  Loss_1: (0.0361) | Acc_1: (98.70%) (21604/21888)\n",
      "Epoch: 148 | Batch_idx: 180 |  Loss_1: (0.0356) | Acc_1: (98.74%) (22876/23168)\n",
      "Epoch: 148 | Batch_idx: 190 |  Loss_1: (0.0352) | Acc_1: (98.75%) (24142/24448)\n",
      "Epoch: 148 | Batch_idx: 200 |  Loss_1: (0.0348) | Acc_1: (98.77%) (25411/25728)\n",
      "Epoch: 148 | Batch_idx: 210 |  Loss_1: (0.0350) | Acc_1: (98.75%) (26670/27008)\n",
      "Epoch: 148 | Batch_idx: 220 |  Loss_1: (0.0352) | Acc_1: (98.75%) (27935/28288)\n",
      "Epoch: 148 | Batch_idx: 230 |  Loss_1: (0.0348) | Acc_1: (98.77%) (29204/29568)\n",
      "Epoch: 148 | Batch_idx: 240 |  Loss_1: (0.0349) | Acc_1: (98.78%) (30471/30848)\n",
      "Epoch: 148 | Batch_idx: 250 |  Loss_1: (0.0350) | Acc_1: (98.78%) (31735/32128)\n",
      "Epoch: 148 | Batch_idx: 260 |  Loss_1: (0.0351) | Acc_1: (98.78%) (32999/33408)\n",
      "Epoch: 148 | Batch_idx: 270 |  Loss_1: (0.0349) | Acc_1: (98.78%) (34264/34688)\n",
      "Epoch: 148 | Batch_idx: 280 |  Loss_1: (0.0351) | Acc_1: (98.77%) (35527/35968)\n",
      "Epoch: 148 | Batch_idx: 290 |  Loss_1: (0.0352) | Acc_1: (98.77%) (36791/37248)\n",
      "Epoch: 148 | Batch_idx: 300 |  Loss_1: (0.0354) | Acc_1: (98.76%) (38049/38528)\n",
      "Epoch: 148 | Batch_idx: 310 |  Loss_1: (0.0356) | Acc_1: (98.75%) (39311/39808)\n",
      "Epoch: 148 | Batch_idx: 320 |  Loss_1: (0.0353) | Acc_1: (98.76%) (40579/41088)\n",
      "Epoch: 148 | Batch_idx: 330 |  Loss_1: (0.0351) | Acc_1: (98.76%) (41843/42368)\n",
      "Epoch: 148 | Batch_idx: 340 |  Loss_1: (0.0353) | Acc_1: (98.76%) (43106/43648)\n",
      "Epoch: 148 | Batch_idx: 350 |  Loss_1: (0.0354) | Acc_1: (98.75%) (44366/44928)\n",
      "Epoch: 148 | Batch_idx: 360 |  Loss_1: (0.0356) | Acc_1: (98.75%) (45630/46208)\n",
      "Epoch: 148 | Batch_idx: 370 |  Loss_1: (0.0354) | Acc_1: (98.75%) (46893/47488)\n",
      "Epoch: 148 | Batch_idx: 380 |  Loss_1: (0.0354) | Acc_1: (98.75%) (48160/48768)\n",
      "Epoch: 148 | Batch_idx: 390 |  Loss_1: (0.0354) | Acc_1: (98.75%) (49377/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4636) | Acc: (91.14%) (9114/10000)\n",
      "Epoch: 149 | Batch_idx: 0 |  Loss_1: (0.0228) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 149 | Batch_idx: 10 |  Loss_1: (0.0319) | Acc_1: (98.65%) (1389/1408)\n",
      "Epoch: 149 | Batch_idx: 20 |  Loss_1: (0.0319) | Acc_1: (98.66%) (2652/2688)\n",
      "Epoch: 149 | Batch_idx: 30 |  Loss_1: (0.0319) | Acc_1: (98.64%) (3914/3968)\n",
      "Epoch: 149 | Batch_idx: 40 |  Loss_1: (0.0314) | Acc_1: (98.80%) (5185/5248)\n",
      "Epoch: 149 | Batch_idx: 50 |  Loss_1: (0.0296) | Acc_1: (98.91%) (6457/6528)\n",
      "Epoch: 149 | Batch_idx: 60 |  Loss_1: (0.0315) | Acc_1: (98.82%) (7716/7808)\n",
      "Epoch: 149 | Batch_idx: 70 |  Loss_1: (0.0300) | Acc_1: (98.87%) (8985/9088)\n",
      "Epoch: 149 | Batch_idx: 80 |  Loss_1: (0.0304) | Acc_1: (98.88%) (10252/10368)\n",
      "Epoch: 149 | Batch_idx: 90 |  Loss_1: (0.0294) | Acc_1: (98.93%) (11523/11648)\n",
      "Epoch: 149 | Batch_idx: 100 |  Loss_1: (0.0290) | Acc_1: (98.96%) (12794/12928)\n",
      "Epoch: 149 | Batch_idx: 110 |  Loss_1: (0.0290) | Acc_1: (98.96%) (14060/14208)\n",
      "Epoch: 149 | Batch_idx: 120 |  Loss_1: (0.0295) | Acc_1: (98.95%) (15326/15488)\n",
      "Epoch: 149 | Batch_idx: 130 |  Loss_1: (0.0302) | Acc_1: (98.94%) (16591/16768)\n",
      "Epoch: 149 | Batch_idx: 140 |  Loss_1: (0.0297) | Acc_1: (98.97%) (17862/18048)\n",
      "Epoch: 149 | Batch_idx: 150 |  Loss_1: (0.0294) | Acc_1: (98.99%) (19133/19328)\n",
      "Epoch: 149 | Batch_idx: 160 |  Loss_1: (0.0293) | Acc_1: (98.98%) (20398/20608)\n",
      "Epoch: 149 | Batch_idx: 170 |  Loss_1: (0.0294) | Acc_1: (98.98%) (21664/21888)\n",
      "Epoch: 149 | Batch_idx: 180 |  Loss_1: (0.0291) | Acc_1: (98.99%) (22934/23168)\n",
      "Epoch: 149 | Batch_idx: 190 |  Loss_1: (0.0292) | Acc_1: (98.98%) (24198/24448)\n",
      "Epoch: 149 | Batch_idx: 200 |  Loss_1: (0.0295) | Acc_1: (98.96%) (25461/25728)\n",
      "Epoch: 149 | Batch_idx: 210 |  Loss_1: (0.0300) | Acc_1: (98.94%) (26723/27008)\n",
      "Epoch: 149 | Batch_idx: 220 |  Loss_1: (0.0301) | Acc_1: (98.94%) (27989/28288)\n",
      "Epoch: 149 | Batch_idx: 230 |  Loss_1: (0.0303) | Acc_1: (98.95%) (29259/29568)\n",
      "Epoch: 149 | Batch_idx: 240 |  Loss_1: (0.0307) | Acc_1: (98.95%) (30524/30848)\n",
      "Epoch: 149 | Batch_idx: 250 |  Loss_1: (0.0309) | Acc_1: (98.94%) (31787/32128)\n",
      "Epoch: 149 | Batch_idx: 260 |  Loss_1: (0.0313) | Acc_1: (98.93%) (33050/33408)\n",
      "Epoch: 149 | Batch_idx: 270 |  Loss_1: (0.0312) | Acc_1: (98.93%) (34317/34688)\n",
      "Epoch: 149 | Batch_idx: 280 |  Loss_1: (0.0309) | Acc_1: (98.94%) (35587/35968)\n",
      "Epoch: 149 | Batch_idx: 290 |  Loss_1: (0.0312) | Acc_1: (98.93%) (36848/37248)\n",
      "Epoch: 149 | Batch_idx: 300 |  Loss_1: (0.0314) | Acc_1: (98.92%) (38112/38528)\n",
      "Epoch: 149 | Batch_idx: 310 |  Loss_1: (0.0322) | Acc_1: (98.88%) (39362/39808)\n",
      "Epoch: 149 | Batch_idx: 320 |  Loss_1: (0.0322) | Acc_1: (98.88%) (40629/41088)\n",
      "Epoch: 149 | Batch_idx: 330 |  Loss_1: (0.0325) | Acc_1: (98.88%) (41893/42368)\n",
      "Epoch: 149 | Batch_idx: 340 |  Loss_1: (0.0324) | Acc_1: (98.87%) (43156/43648)\n",
      "Epoch: 149 | Batch_idx: 350 |  Loss_1: (0.0326) | Acc_1: (98.86%) (44417/44928)\n",
      "Epoch: 149 | Batch_idx: 360 |  Loss_1: (0.0331) | Acc_1: (98.85%) (45677/46208)\n",
      "Epoch: 149 | Batch_idx: 370 |  Loss_1: (0.0333) | Acc_1: (98.85%) (46941/47488)\n",
      "Epoch: 149 | Batch_idx: 380 |  Loss_1: (0.0335) | Acc_1: (98.85%) (48206/48768)\n",
      "Epoch: 149 | Batch_idx: 390 |  Loss_1: (0.0335) | Acc_1: (98.85%) (49424/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4677) | Acc: (91.22%) (9122/10000)\n",
      "Epoch: 150 | Batch_idx: 0 |  Loss_1: (0.0377) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 150 | Batch_idx: 10 |  Loss_1: (0.0329) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 150 | Batch_idx: 20 |  Loss_1: (0.0276) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 150 | Batch_idx: 30 |  Loss_1: (0.0283) | Acc_1: (98.92%) (3925/3968)\n",
      "Epoch: 150 | Batch_idx: 40 |  Loss_1: (0.0311) | Acc_1: (98.86%) (5188/5248)\n",
      "Epoch: 150 | Batch_idx: 50 |  Loss_1: (0.0279) | Acc_1: (98.97%) (6461/6528)\n",
      "Epoch: 150 | Batch_idx: 60 |  Loss_1: (0.0295) | Acc_1: (98.99%) (7729/7808)\n",
      "Epoch: 150 | Batch_idx: 70 |  Loss_1: (0.0299) | Acc_1: (98.99%) (8996/9088)\n",
      "Epoch: 150 | Batch_idx: 80 |  Loss_1: (0.0319) | Acc_1: (98.96%) (10260/10368)\n",
      "Epoch: 150 | Batch_idx: 90 |  Loss_1: (0.0301) | Acc_1: (99.02%) (11534/11648)\n",
      "Epoch: 150 | Batch_idx: 100 |  Loss_1: (0.0298) | Acc_1: (99.03%) (12802/12928)\n",
      "Epoch: 150 | Batch_idx: 110 |  Loss_1: (0.0303) | Acc_1: (99.01%) (14067/14208)\n",
      "Epoch: 150 | Batch_idx: 120 |  Loss_1: (0.0322) | Acc_1: (98.95%) (15325/15488)\n",
      "Epoch: 150 | Batch_idx: 130 |  Loss_1: (0.0322) | Acc_1: (98.97%) (16595/16768)\n",
      "Epoch: 150 | Batch_idx: 140 |  Loss_1: (0.0331) | Acc_1: (98.93%) (17854/18048)\n",
      "Epoch: 150 | Batch_idx: 150 |  Loss_1: (0.0322) | Acc_1: (98.97%) (19128/19328)\n",
      "Epoch: 150 | Batch_idx: 160 |  Loss_1: (0.0322) | Acc_1: (98.96%) (20393/20608)\n",
      "Epoch: 150 | Batch_idx: 170 |  Loss_1: (0.0321) | Acc_1: (98.95%) (21659/21888)\n",
      "Epoch: 150 | Batch_idx: 180 |  Loss_1: (0.0318) | Acc_1: (98.96%) (22927/23168)\n",
      "Epoch: 150 | Batch_idx: 190 |  Loss_1: (0.0315) | Acc_1: (98.97%) (24196/24448)\n",
      "Epoch: 150 | Batch_idx: 200 |  Loss_1: (0.0315) | Acc_1: (98.97%) (25462/25728)\n",
      "Epoch: 150 | Batch_idx: 210 |  Loss_1: (0.0313) | Acc_1: (98.97%) (26731/27008)\n",
      "Epoch: 150 | Batch_idx: 220 |  Loss_1: (0.0312) | Acc_1: (98.97%) (27998/28288)\n",
      "Epoch: 150 | Batch_idx: 230 |  Loss_1: (0.0311) | Acc_1: (98.98%) (29266/29568)\n",
      "Epoch: 150 | Batch_idx: 240 |  Loss_1: (0.0315) | Acc_1: (98.97%) (30531/30848)\n",
      "Epoch: 150 | Batch_idx: 250 |  Loss_1: (0.0319) | Acc_1: (98.97%) (31796/32128)\n",
      "Epoch: 150 | Batch_idx: 260 |  Loss_1: (0.0316) | Acc_1: (98.98%) (33066/33408)\n",
      "Epoch: 150 | Batch_idx: 270 |  Loss_1: (0.0316) | Acc_1: (98.97%) (34329/34688)\n",
      "Epoch: 150 | Batch_idx: 280 |  Loss_1: (0.0318) | Acc_1: (98.96%) (35595/35968)\n",
      "Epoch: 150 | Batch_idx: 290 |  Loss_1: (0.0315) | Acc_1: (98.97%) (36866/37248)\n",
      "Epoch: 150 | Batch_idx: 300 |  Loss_1: (0.0319) | Acc_1: (98.97%) (38131/38528)\n",
      "Epoch: 150 | Batch_idx: 310 |  Loss_1: (0.0319) | Acc_1: (98.97%) (39399/39808)\n",
      "Epoch: 150 | Batch_idx: 320 |  Loss_1: (0.0322) | Acc_1: (98.96%) (40662/41088)\n",
      "Epoch: 150 | Batch_idx: 330 |  Loss_1: (0.0321) | Acc_1: (98.96%) (41928/42368)\n",
      "Epoch: 150 | Batch_idx: 340 |  Loss_1: (0.0320) | Acc_1: (98.95%) (43191/43648)\n",
      "Epoch: 150 | Batch_idx: 350 |  Loss_1: (0.0318) | Acc_1: (98.96%) (44459/44928)\n",
      "Epoch: 150 | Batch_idx: 360 |  Loss_1: (0.0323) | Acc_1: (98.94%) (45720/46208)\n",
      "Epoch: 150 | Batch_idx: 370 |  Loss_1: (0.0326) | Acc_1: (98.92%) (46977/47488)\n",
      "Epoch: 150 | Batch_idx: 380 |  Loss_1: (0.0325) | Acc_1: (98.93%) (48245/48768)\n",
      "Epoch: 150 | Batch_idx: 390 |  Loss_1: (0.0328) | Acc_1: (98.92%) (49460/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5133) | Acc: (90.81%) (9081/10000)\n",
      "Epoch: 151 | Batch_idx: 0 |  Loss_1: (0.0281) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 151 | Batch_idx: 10 |  Loss_1: (0.0321) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 151 | Batch_idx: 20 |  Loss_1: (0.0279) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 151 | Batch_idx: 30 |  Loss_1: (0.0340) | Acc_1: (98.77%) (3919/3968)\n",
      "Epoch: 151 | Batch_idx: 40 |  Loss_1: (0.0334) | Acc_1: (98.84%) (5187/5248)\n",
      "Epoch: 151 | Batch_idx: 50 |  Loss_1: (0.0341) | Acc_1: (98.85%) (6453/6528)\n",
      "Epoch: 151 | Batch_idx: 60 |  Loss_1: (0.0335) | Acc_1: (98.82%) (7716/7808)\n",
      "Epoch: 151 | Batch_idx: 70 |  Loss_1: (0.0333) | Acc_1: (98.88%) (8986/9088)\n",
      "Epoch: 151 | Batch_idx: 80 |  Loss_1: (0.0321) | Acc_1: (98.92%) (10256/10368)\n",
      "Epoch: 151 | Batch_idx: 90 |  Loss_1: (0.0315) | Acc_1: (98.93%) (11523/11648)\n",
      "Epoch: 151 | Batch_idx: 100 |  Loss_1: (0.0307) | Acc_1: (98.99%) (12797/12928)\n",
      "Epoch: 151 | Batch_idx: 110 |  Loss_1: (0.0293) | Acc_1: (99.01%) (14067/14208)\n",
      "Epoch: 151 | Batch_idx: 120 |  Loss_1: (0.0293) | Acc_1: (99.00%) (15333/15488)\n",
      "Epoch: 151 | Batch_idx: 130 |  Loss_1: (0.0289) | Acc_1: (99.01%) (16602/16768)\n",
      "Epoch: 151 | Batch_idx: 140 |  Loss_1: (0.0288) | Acc_1: (99.00%) (17868/18048)\n",
      "Epoch: 151 | Batch_idx: 150 |  Loss_1: (0.0287) | Acc_1: (99.00%) (19134/19328)\n",
      "Epoch: 151 | Batch_idx: 160 |  Loss_1: (0.0283) | Acc_1: (99.01%) (20405/20608)\n",
      "Epoch: 151 | Batch_idx: 170 |  Loss_1: (0.0280) | Acc_1: (99.02%) (21673/21888)\n",
      "Epoch: 151 | Batch_idx: 180 |  Loss_1: (0.0282) | Acc_1: (99.01%) (22938/23168)\n",
      "Epoch: 151 | Batch_idx: 190 |  Loss_1: (0.0278) | Acc_1: (99.02%) (24209/24448)\n",
      "Epoch: 151 | Batch_idx: 200 |  Loss_1: (0.0278) | Acc_1: (99.03%) (25478/25728)\n",
      "Epoch: 151 | Batch_idx: 210 |  Loss_1: (0.0280) | Acc_1: (99.03%) (26746/27008)\n",
      "Epoch: 151 | Batch_idx: 220 |  Loss_1: (0.0281) | Acc_1: (99.03%) (28013/28288)\n",
      "Epoch: 151 | Batch_idx: 230 |  Loss_1: (0.0282) | Acc_1: (99.03%) (29281/29568)\n",
      "Epoch: 151 | Batch_idx: 240 |  Loss_1: (0.0283) | Acc_1: (99.03%) (30548/30848)\n",
      "Epoch: 151 | Batch_idx: 250 |  Loss_1: (0.0282) | Acc_1: (99.04%) (31820/32128)\n",
      "Epoch: 151 | Batch_idx: 260 |  Loss_1: (0.0281) | Acc_1: (99.05%) (33091/33408)\n",
      "Epoch: 151 | Batch_idx: 270 |  Loss_1: (0.0278) | Acc_1: (99.06%) (34362/34688)\n",
      "Epoch: 151 | Batch_idx: 280 |  Loss_1: (0.0275) | Acc_1: (99.07%) (35633/35968)\n",
      "Epoch: 151 | Batch_idx: 290 |  Loss_1: (0.0274) | Acc_1: (99.07%) (36900/37248)\n",
      "Epoch: 151 | Batch_idx: 300 |  Loss_1: (0.0272) | Acc_1: (99.07%) (38171/38528)\n",
      "Epoch: 151 | Batch_idx: 310 |  Loss_1: (0.0270) | Acc_1: (99.08%) (39443/39808)\n",
      "Epoch: 151 | Batch_idx: 320 |  Loss_1: (0.0272) | Acc_1: (99.08%) (40712/41088)\n",
      "Epoch: 151 | Batch_idx: 330 |  Loss_1: (0.0277) | Acc_1: (99.06%) (41971/42368)\n",
      "Epoch: 151 | Batch_idx: 340 |  Loss_1: (0.0281) | Acc_1: (99.06%) (43238/43648)\n",
      "Epoch: 151 | Batch_idx: 350 |  Loss_1: (0.0286) | Acc_1: (99.04%) (44498/44928)\n",
      "Epoch: 151 | Batch_idx: 360 |  Loss_1: (0.0287) | Acc_1: (99.04%) (45763/46208)\n",
      "Epoch: 151 | Batch_idx: 370 |  Loss_1: (0.0288) | Acc_1: (99.04%) (47033/47488)\n",
      "Epoch: 151 | Batch_idx: 380 |  Loss_1: (0.0288) | Acc_1: (99.04%) (48300/48768)\n",
      "Epoch: 151 | Batch_idx: 390 |  Loss_1: (0.0287) | Acc_1: (99.04%) (49520/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4504) | Acc: (91.19%) (9119/10000)\n",
      "Epoch: 152 | Batch_idx: 0 |  Loss_1: (0.0561) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 152 | Batch_idx: 10 |  Loss_1: (0.0335) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 152 | Batch_idx: 20 |  Loss_1: (0.0304) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 152 | Batch_idx: 30 |  Loss_1: (0.0301) | Acc_1: (99.07%) (3931/3968)\n",
      "Epoch: 152 | Batch_idx: 40 |  Loss_1: (0.0320) | Acc_1: (99.05%) (5198/5248)\n",
      "Epoch: 152 | Batch_idx: 50 |  Loss_1: (0.0305) | Acc_1: (99.08%) (6468/6528)\n",
      "Epoch: 152 | Batch_idx: 60 |  Loss_1: (0.0302) | Acc_1: (99.09%) (7737/7808)\n",
      "Epoch: 152 | Batch_idx: 70 |  Loss_1: (0.0295) | Acc_1: (99.06%) (9003/9088)\n",
      "Epoch: 152 | Batch_idx: 80 |  Loss_1: (0.0275) | Acc_1: (99.15%) (10280/10368)\n",
      "Epoch: 152 | Batch_idx: 90 |  Loss_1: (0.0269) | Acc_1: (99.18%) (11552/11648)\n",
      "Epoch: 152 | Batch_idx: 100 |  Loss_1: (0.0271) | Acc_1: (99.17%) (12821/12928)\n",
      "Epoch: 152 | Batch_idx: 110 |  Loss_1: (0.0277) | Acc_1: (99.15%) (14087/14208)\n",
      "Epoch: 152 | Batch_idx: 120 |  Loss_1: (0.0270) | Acc_1: (99.17%) (15360/15488)\n",
      "Epoch: 152 | Batch_idx: 130 |  Loss_1: (0.0275) | Acc_1: (99.16%) (16627/16768)\n",
      "Epoch: 152 | Batch_idx: 140 |  Loss_1: (0.0270) | Acc_1: (99.19%) (17902/18048)\n",
      "Epoch: 152 | Batch_idx: 150 |  Loss_1: (0.0280) | Acc_1: (99.15%) (19163/19328)\n",
      "Epoch: 152 | Batch_idx: 160 |  Loss_1: (0.0289) | Acc_1: (99.13%) (20429/20608)\n",
      "Epoch: 152 | Batch_idx: 170 |  Loss_1: (0.0286) | Acc_1: (99.13%) (21697/21888)\n",
      "Epoch: 152 | Batch_idx: 180 |  Loss_1: (0.0283) | Acc_1: (99.15%) (22970/23168)\n",
      "Epoch: 152 | Batch_idx: 190 |  Loss_1: (0.0292) | Acc_1: (99.13%) (24235/24448)\n",
      "Epoch: 152 | Batch_idx: 200 |  Loss_1: (0.0296) | Acc_1: (99.11%) (25500/25728)\n",
      "Epoch: 152 | Batch_idx: 210 |  Loss_1: (0.0294) | Acc_1: (99.12%) (26771/27008)\n",
      "Epoch: 152 | Batch_idx: 220 |  Loss_1: (0.0293) | Acc_1: (99.13%) (28041/28288)\n",
      "Epoch: 152 | Batch_idx: 230 |  Loss_1: (0.0294) | Acc_1: (99.12%) (29308/29568)\n",
      "Epoch: 152 | Batch_idx: 240 |  Loss_1: (0.0295) | Acc_1: (99.11%) (30573/30848)\n",
      "Epoch: 152 | Batch_idx: 250 |  Loss_1: (0.0293) | Acc_1: (99.10%) (31840/32128)\n",
      "Epoch: 152 | Batch_idx: 260 |  Loss_1: (0.0294) | Acc_1: (99.09%) (33105/33408)\n",
      "Epoch: 152 | Batch_idx: 270 |  Loss_1: (0.0297) | Acc_1: (99.08%) (34368/34688)\n",
      "Epoch: 152 | Batch_idx: 280 |  Loss_1: (0.0296) | Acc_1: (99.07%) (35635/35968)\n",
      "Epoch: 152 | Batch_idx: 290 |  Loss_1: (0.0295) | Acc_1: (99.07%) (36902/37248)\n",
      "Epoch: 152 | Batch_idx: 300 |  Loss_1: (0.0297) | Acc_1: (99.07%) (38171/38528)\n",
      "Epoch: 152 | Batch_idx: 310 |  Loss_1: (0.0297) | Acc_1: (99.07%) (39438/39808)\n",
      "Epoch: 152 | Batch_idx: 320 |  Loss_1: (0.0297) | Acc_1: (99.06%) (40702/41088)\n",
      "Epoch: 152 | Batch_idx: 330 |  Loss_1: (0.0295) | Acc_1: (99.07%) (41973/42368)\n",
      "Epoch: 152 | Batch_idx: 340 |  Loss_1: (0.0298) | Acc_1: (99.06%) (43236/43648)\n",
      "Epoch: 152 | Batch_idx: 350 |  Loss_1: (0.0297) | Acc_1: (99.06%) (44504/44928)\n",
      "Epoch: 152 | Batch_idx: 360 |  Loss_1: (0.0296) | Acc_1: (99.05%) (45769/46208)\n",
      "Epoch: 152 | Batch_idx: 370 |  Loss_1: (0.0297) | Acc_1: (99.04%) (47033/47488)\n",
      "Epoch: 152 | Batch_idx: 380 |  Loss_1: (0.0299) | Acc_1: (99.03%) (48295/48768)\n",
      "Epoch: 152 | Batch_idx: 390 |  Loss_1: (0.0298) | Acc_1: (99.03%) (49514/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4735) | Acc: (91.37%) (9137/10000)\n",
      "Epoch: 153 | Batch_idx: 0 |  Loss_1: (0.0306) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 153 | Batch_idx: 10 |  Loss_1: (0.0281) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 153 | Batch_idx: 20 |  Loss_1: (0.0259) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 153 | Batch_idx: 30 |  Loss_1: (0.0276) | Acc_1: (98.94%) (3926/3968)\n",
      "Epoch: 153 | Batch_idx: 40 |  Loss_1: (0.0280) | Acc_1: (99.01%) (5196/5248)\n",
      "Epoch: 153 | Batch_idx: 50 |  Loss_1: (0.0309) | Acc_1: (98.97%) (6461/6528)\n",
      "Epoch: 153 | Batch_idx: 60 |  Loss_1: (0.0287) | Acc_1: (99.04%) (7733/7808)\n",
      "Epoch: 153 | Batch_idx: 70 |  Loss_1: (0.0282) | Acc_1: (99.06%) (9003/9088)\n",
      "Epoch: 153 | Batch_idx: 80 |  Loss_1: (0.0297) | Acc_1: (99.04%) (10268/10368)\n",
      "Epoch: 153 | Batch_idx: 90 |  Loss_1: (0.0303) | Acc_1: (99.00%) (11531/11648)\n",
      "Epoch: 153 | Batch_idx: 100 |  Loss_1: (0.0312) | Acc_1: (98.95%) (12792/12928)\n",
      "Epoch: 153 | Batch_idx: 110 |  Loss_1: (0.0308) | Acc_1: (98.98%) (14063/14208)\n",
      "Epoch: 153 | Batch_idx: 120 |  Loss_1: (0.0302) | Acc_1: (99.00%) (15333/15488)\n",
      "Epoch: 153 | Batch_idx: 130 |  Loss_1: (0.0304) | Acc_1: (99.00%) (16601/16768)\n",
      "Epoch: 153 | Batch_idx: 140 |  Loss_1: (0.0300) | Acc_1: (99.00%) (17867/18048)\n",
      "Epoch: 153 | Batch_idx: 150 |  Loss_1: (0.0295) | Acc_1: (99.00%) (19135/19328)\n",
      "Epoch: 153 | Batch_idx: 160 |  Loss_1: (0.0288) | Acc_1: (99.02%) (20407/20608)\n",
      "Epoch: 153 | Batch_idx: 170 |  Loss_1: (0.0283) | Acc_1: (99.05%) (21679/21888)\n",
      "Epoch: 153 | Batch_idx: 180 |  Loss_1: (0.0279) | Acc_1: (99.05%) (22949/23168)\n",
      "Epoch: 153 | Batch_idx: 190 |  Loss_1: (0.0276) | Acc_1: (99.07%) (24220/24448)\n",
      "Epoch: 153 | Batch_idx: 200 |  Loss_1: (0.0276) | Acc_1: (99.08%) (25491/25728)\n",
      "Epoch: 153 | Batch_idx: 210 |  Loss_1: (0.0281) | Acc_1: (99.07%) (26756/27008)\n",
      "Epoch: 153 | Batch_idx: 220 |  Loss_1: (0.0282) | Acc_1: (99.06%) (28022/28288)\n",
      "Epoch: 153 | Batch_idx: 230 |  Loss_1: (0.0280) | Acc_1: (99.07%) (29294/29568)\n",
      "Epoch: 153 | Batch_idx: 240 |  Loss_1: (0.0280) | Acc_1: (99.08%) (30564/30848)\n",
      "Epoch: 153 | Batch_idx: 250 |  Loss_1: (0.0276) | Acc_1: (99.09%) (31836/32128)\n",
      "Epoch: 153 | Batch_idx: 260 |  Loss_1: (0.0272) | Acc_1: (99.10%) (33106/33408)\n",
      "Epoch: 153 | Batch_idx: 270 |  Loss_1: (0.0272) | Acc_1: (99.09%) (34373/34688)\n",
      "Epoch: 153 | Batch_idx: 280 |  Loss_1: (0.0275) | Acc_1: (99.07%) (35633/35968)\n",
      "Epoch: 153 | Batch_idx: 290 |  Loss_1: (0.0279) | Acc_1: (99.05%) (36895/37248)\n",
      "Epoch: 153 | Batch_idx: 300 |  Loss_1: (0.0287) | Acc_1: (99.02%) (38151/38528)\n",
      "Epoch: 153 | Batch_idx: 310 |  Loss_1: (0.0289) | Acc_1: (99.01%) (39413/39808)\n",
      "Epoch: 153 | Batch_idx: 320 |  Loss_1: (0.0294) | Acc_1: (98.99%) (40675/41088)\n",
      "Epoch: 153 | Batch_idx: 330 |  Loss_1: (0.0294) | Acc_1: (98.99%) (41942/42368)\n",
      "Epoch: 153 | Batch_idx: 340 |  Loss_1: (0.0294) | Acc_1: (98.99%) (43209/43648)\n",
      "Epoch: 153 | Batch_idx: 350 |  Loss_1: (0.0295) | Acc_1: (98.99%) (44472/44928)\n",
      "Epoch: 153 | Batch_idx: 360 |  Loss_1: (0.0294) | Acc_1: (98.98%) (45736/46208)\n",
      "Epoch: 153 | Batch_idx: 370 |  Loss_1: (0.0297) | Acc_1: (98.97%) (46999/47488)\n",
      "Epoch: 153 | Batch_idx: 380 |  Loss_1: (0.0296) | Acc_1: (98.97%) (48267/48768)\n",
      "Epoch: 153 | Batch_idx: 390 |  Loss_1: (0.0297) | Acc_1: (98.98%) (49489/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4582) | Acc: (91.07%) (9107/10000)\n",
      "Epoch: 154 | Batch_idx: 0 |  Loss_1: (0.0180) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 154 | Batch_idx: 10 |  Loss_1: (0.0221) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 154 | Batch_idx: 20 |  Loss_1: (0.0279) | Acc_1: (99.07%) (2663/2688)\n",
      "Epoch: 154 | Batch_idx: 30 |  Loss_1: (0.0247) | Acc_1: (99.14%) (3934/3968)\n",
      "Epoch: 154 | Batch_idx: 40 |  Loss_1: (0.0266) | Acc_1: (99.07%) (5199/5248)\n",
      "Epoch: 154 | Batch_idx: 50 |  Loss_1: (0.0282) | Acc_1: (99.03%) (6465/6528)\n",
      "Epoch: 154 | Batch_idx: 60 |  Loss_1: (0.0269) | Acc_1: (99.09%) (7737/7808)\n",
      "Epoch: 154 | Batch_idx: 70 |  Loss_1: (0.0285) | Acc_1: (99.00%) (8997/9088)\n",
      "Epoch: 154 | Batch_idx: 80 |  Loss_1: (0.0284) | Acc_1: (98.99%) (10263/10368)\n",
      "Epoch: 154 | Batch_idx: 90 |  Loss_1: (0.0281) | Acc_1: (99.01%) (11533/11648)\n",
      "Epoch: 154 | Batch_idx: 100 |  Loss_1: (0.0277) | Acc_1: (99.03%) (12802/12928)\n",
      "Epoch: 154 | Batch_idx: 110 |  Loss_1: (0.0283) | Acc_1: (99.00%) (14066/14208)\n",
      "Epoch: 154 | Batch_idx: 120 |  Loss_1: (0.0286) | Acc_1: (98.98%) (15330/15488)\n",
      "Epoch: 154 | Batch_idx: 130 |  Loss_1: (0.0279) | Acc_1: (99.02%) (16603/16768)\n",
      "Epoch: 154 | Batch_idx: 140 |  Loss_1: (0.0276) | Acc_1: (99.02%) (17872/18048)\n",
      "Epoch: 154 | Batch_idx: 150 |  Loss_1: (0.0274) | Acc_1: (99.03%) (19140/19328)\n",
      "Epoch: 154 | Batch_idx: 160 |  Loss_1: (0.0274) | Acc_1: (99.03%) (20409/20608)\n",
      "Epoch: 154 | Batch_idx: 170 |  Loss_1: (0.0277) | Acc_1: (99.01%) (21672/21888)\n",
      "Epoch: 154 | Batch_idx: 180 |  Loss_1: (0.0277) | Acc_1: (99.02%) (22940/23168)\n",
      "Epoch: 154 | Batch_idx: 190 |  Loss_1: (0.0276) | Acc_1: (99.02%) (24209/24448)\n",
      "Epoch: 154 | Batch_idx: 200 |  Loss_1: (0.0276) | Acc_1: (99.02%) (25476/25728)\n",
      "Epoch: 154 | Batch_idx: 210 |  Loss_1: (0.0272) | Acc_1: (99.04%) (26750/27008)\n",
      "Epoch: 154 | Batch_idx: 220 |  Loss_1: (0.0268) | Acc_1: (99.07%) (28025/28288)\n",
      "Epoch: 154 | Batch_idx: 230 |  Loss_1: (0.0270) | Acc_1: (99.05%) (29287/29568)\n",
      "Epoch: 154 | Batch_idx: 240 |  Loss_1: (0.0271) | Acc_1: (99.04%) (30552/30848)\n",
      "Epoch: 154 | Batch_idx: 250 |  Loss_1: (0.0271) | Acc_1: (99.03%) (31817/32128)\n",
      "Epoch: 154 | Batch_idx: 260 |  Loss_1: (0.0274) | Acc_1: (99.03%) (33083/33408)\n",
      "Epoch: 154 | Batch_idx: 270 |  Loss_1: (0.0276) | Acc_1: (99.02%) (34348/34688)\n",
      "Epoch: 154 | Batch_idx: 280 |  Loss_1: (0.0276) | Acc_1: (99.02%) (35614/35968)\n",
      "Epoch: 154 | Batch_idx: 290 |  Loss_1: (0.0274) | Acc_1: (99.03%) (36886/37248)\n",
      "Epoch: 154 | Batch_idx: 300 |  Loss_1: (0.0275) | Acc_1: (99.02%) (38152/38528)\n",
      "Epoch: 154 | Batch_idx: 310 |  Loss_1: (0.0272) | Acc_1: (99.03%) (39421/39808)\n",
      "Epoch: 154 | Batch_idx: 320 |  Loss_1: (0.0269) | Acc_1: (99.03%) (40690/41088)\n",
      "Epoch: 154 | Batch_idx: 330 |  Loss_1: (0.0271) | Acc_1: (99.03%) (41958/42368)\n",
      "Epoch: 154 | Batch_idx: 340 |  Loss_1: (0.0276) | Acc_1: (99.01%) (43217/43648)\n",
      "Epoch: 154 | Batch_idx: 350 |  Loss_1: (0.0277) | Acc_1: (99.01%) (44481/44928)\n",
      "Epoch: 154 | Batch_idx: 360 |  Loss_1: (0.0276) | Acc_1: (99.02%) (45753/46208)\n",
      "Epoch: 154 | Batch_idx: 370 |  Loss_1: (0.0276) | Acc_1: (99.02%) (47022/47488)\n",
      "Epoch: 154 | Batch_idx: 380 |  Loss_1: (0.0274) | Acc_1: (99.03%) (48293/48768)\n",
      "Epoch: 154 | Batch_idx: 390 |  Loss_1: (0.0276) | Acc_1: (99.02%) (49512/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4759) | Acc: (91.40%) (9140/10000)\n",
      "Epoch: 155 | Batch_idx: 0 |  Loss_1: (0.0167) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 155 | Batch_idx: 10 |  Loss_1: (0.0303) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 155 | Batch_idx: 20 |  Loss_1: (0.0298) | Acc_1: (98.88%) (2658/2688)\n",
      "Epoch: 155 | Batch_idx: 30 |  Loss_1: (0.0260) | Acc_1: (99.04%) (3930/3968)\n",
      "Epoch: 155 | Batch_idx: 40 |  Loss_1: (0.0298) | Acc_1: (98.86%) (5188/5248)\n",
      "Epoch: 155 | Batch_idx: 50 |  Loss_1: (0.0284) | Acc_1: (98.91%) (6457/6528)\n",
      "Epoch: 155 | Batch_idx: 60 |  Loss_1: (0.0281) | Acc_1: (98.98%) (7728/7808)\n",
      "Epoch: 155 | Batch_idx: 70 |  Loss_1: (0.0273) | Acc_1: (99.03%) (9000/9088)\n",
      "Epoch: 155 | Batch_idx: 80 |  Loss_1: (0.0274) | Acc_1: (99.04%) (10268/10368)\n",
      "Epoch: 155 | Batch_idx: 90 |  Loss_1: (0.0270) | Acc_1: (99.01%) (11533/11648)\n",
      "Epoch: 155 | Batch_idx: 100 |  Loss_1: (0.0271) | Acc_1: (99.00%) (12799/12928)\n",
      "Epoch: 155 | Batch_idx: 110 |  Loss_1: (0.0264) | Acc_1: (99.02%) (14069/14208)\n",
      "Epoch: 155 | Batch_idx: 120 |  Loss_1: (0.0264) | Acc_1: (99.03%) (15338/15488)\n",
      "Epoch: 155 | Batch_idx: 130 |  Loss_1: (0.0266) | Acc_1: (99.03%) (16606/16768)\n",
      "Epoch: 155 | Batch_idx: 140 |  Loss_1: (0.0261) | Acc_1: (99.06%) (17879/18048)\n",
      "Epoch: 155 | Batch_idx: 150 |  Loss_1: (0.0260) | Acc_1: (99.05%) (19145/19328)\n",
      "Epoch: 155 | Batch_idx: 160 |  Loss_1: (0.0257) | Acc_1: (99.07%) (20417/20608)\n",
      "Epoch: 155 | Batch_idx: 170 |  Loss_1: (0.0254) | Acc_1: (99.09%) (21689/21888)\n",
      "Epoch: 155 | Batch_idx: 180 |  Loss_1: (0.0252) | Acc_1: (99.10%) (22960/23168)\n",
      "Epoch: 155 | Batch_idx: 190 |  Loss_1: (0.0252) | Acc_1: (99.11%) (24230/24448)\n",
      "Epoch: 155 | Batch_idx: 200 |  Loss_1: (0.0250) | Acc_1: (99.12%) (25502/25728)\n",
      "Epoch: 155 | Batch_idx: 210 |  Loss_1: (0.0246) | Acc_1: (99.13%) (26773/27008)\n",
      "Epoch: 155 | Batch_idx: 220 |  Loss_1: (0.0249) | Acc_1: (99.13%) (28042/28288)\n",
      "Epoch: 155 | Batch_idx: 230 |  Loss_1: (0.0256) | Acc_1: (99.10%) (29301/29568)\n",
      "Epoch: 155 | Batch_idx: 240 |  Loss_1: (0.0256) | Acc_1: (99.11%) (30574/30848)\n",
      "Epoch: 155 | Batch_idx: 250 |  Loss_1: (0.0257) | Acc_1: (99.11%) (31841/32128)\n",
      "Epoch: 155 | Batch_idx: 260 |  Loss_1: (0.0260) | Acc_1: (99.10%) (33107/33408)\n",
      "Epoch: 155 | Batch_idx: 270 |  Loss_1: (0.0272) | Acc_1: (99.06%) (34363/34688)\n",
      "Epoch: 155 | Batch_idx: 280 |  Loss_1: (0.0270) | Acc_1: (99.08%) (35637/35968)\n",
      "Epoch: 155 | Batch_idx: 290 |  Loss_1: (0.0271) | Acc_1: (99.08%) (36904/37248)\n",
      "Epoch: 155 | Batch_idx: 300 |  Loss_1: (0.0274) | Acc_1: (99.07%) (38168/38528)\n",
      "Epoch: 155 | Batch_idx: 310 |  Loss_1: (0.0281) | Acc_1: (99.05%) (39428/39808)\n",
      "Epoch: 155 | Batch_idx: 320 |  Loss_1: (0.0282) | Acc_1: (99.04%) (40694/41088)\n",
      "Epoch: 155 | Batch_idx: 330 |  Loss_1: (0.0281) | Acc_1: (99.03%) (41959/42368)\n",
      "Epoch: 155 | Batch_idx: 340 |  Loss_1: (0.0279) | Acc_1: (99.04%) (43231/43648)\n",
      "Epoch: 155 | Batch_idx: 350 |  Loss_1: (0.0278) | Acc_1: (99.05%) (44500/44928)\n",
      "Epoch: 155 | Batch_idx: 360 |  Loss_1: (0.0276) | Acc_1: (99.06%) (45772/46208)\n",
      "Epoch: 155 | Batch_idx: 370 |  Loss_1: (0.0276) | Acc_1: (99.06%) (47040/47488)\n",
      "Epoch: 155 | Batch_idx: 380 |  Loss_1: (0.0279) | Acc_1: (99.04%) (48298/48768)\n",
      "Epoch: 155 | Batch_idx: 390 |  Loss_1: (0.0278) | Acc_1: (99.04%) (49519/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5107) | Acc: (90.63%) (9063/10000)\n",
      "Epoch: 156 | Batch_idx: 0 |  Loss_1: (0.0165) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 156 | Batch_idx: 10 |  Loss_1: (0.0358) | Acc_1: (98.72%) (1390/1408)\n",
      "Epoch: 156 | Batch_idx: 20 |  Loss_1: (0.0317) | Acc_1: (98.92%) (2659/2688)\n",
      "Epoch: 156 | Batch_idx: 30 |  Loss_1: (0.0324) | Acc_1: (98.94%) (3926/3968)\n",
      "Epoch: 156 | Batch_idx: 40 |  Loss_1: (0.0297) | Acc_1: (98.97%) (5194/5248)\n",
      "Epoch: 156 | Batch_idx: 50 |  Loss_1: (0.0295) | Acc_1: (98.94%) (6459/6528)\n",
      "Epoch: 156 | Batch_idx: 60 |  Loss_1: (0.0301) | Acc_1: (98.91%) (7723/7808)\n",
      "Epoch: 156 | Batch_idx: 70 |  Loss_1: (0.0302) | Acc_1: (98.94%) (8992/9088)\n",
      "Epoch: 156 | Batch_idx: 80 |  Loss_1: (0.0296) | Acc_1: (98.95%) (10259/10368)\n",
      "Epoch: 156 | Batch_idx: 90 |  Loss_1: (0.0298) | Acc_1: (98.96%) (11527/11648)\n",
      "Epoch: 156 | Batch_idx: 100 |  Loss_1: (0.0287) | Acc_1: (99.01%) (12800/12928)\n",
      "Epoch: 156 | Batch_idx: 110 |  Loss_1: (0.0279) | Acc_1: (99.03%) (14070/14208)\n",
      "Epoch: 156 | Batch_idx: 120 |  Loss_1: (0.0272) | Acc_1: (99.05%) (15341/15488)\n",
      "Epoch: 156 | Batch_idx: 130 |  Loss_1: (0.0267) | Acc_1: (99.08%) (16613/16768)\n",
      "Epoch: 156 | Batch_idx: 140 |  Loss_1: (0.0268) | Acc_1: (99.06%) (17879/18048)\n",
      "Epoch: 156 | Batch_idx: 150 |  Loss_1: (0.0264) | Acc_1: (99.08%) (19151/19328)\n",
      "Epoch: 156 | Batch_idx: 160 |  Loss_1: (0.0260) | Acc_1: (99.11%) (20424/20608)\n",
      "Epoch: 156 | Batch_idx: 170 |  Loss_1: (0.0264) | Acc_1: (99.09%) (21688/21888)\n",
      "Epoch: 156 | Batch_idx: 180 |  Loss_1: (0.0265) | Acc_1: (99.08%) (22956/23168)\n",
      "Epoch: 156 | Batch_idx: 190 |  Loss_1: (0.0264) | Acc_1: (99.08%) (24224/24448)\n",
      "Epoch: 156 | Batch_idx: 200 |  Loss_1: (0.0263) | Acc_1: (99.09%) (25493/25728)\n",
      "Epoch: 156 | Batch_idx: 210 |  Loss_1: (0.0266) | Acc_1: (99.08%) (26759/27008)\n",
      "Epoch: 156 | Batch_idx: 220 |  Loss_1: (0.0262) | Acc_1: (99.09%) (28031/28288)\n",
      "Epoch: 156 | Batch_idx: 230 |  Loss_1: (0.0267) | Acc_1: (99.08%) (29296/29568)\n",
      "Epoch: 156 | Batch_idx: 240 |  Loss_1: (0.0267) | Acc_1: (99.09%) (30566/30848)\n",
      "Epoch: 156 | Batch_idx: 250 |  Loss_1: (0.0265) | Acc_1: (99.09%) (31836/32128)\n",
      "Epoch: 156 | Batch_idx: 260 |  Loss_1: (0.0268) | Acc_1: (99.08%) (33100/33408)\n",
      "Epoch: 156 | Batch_idx: 270 |  Loss_1: (0.0267) | Acc_1: (99.09%) (34372/34688)\n",
      "Epoch: 156 | Batch_idx: 280 |  Loss_1: (0.0272) | Acc_1: (99.08%) (35637/35968)\n",
      "Epoch: 156 | Batch_idx: 290 |  Loss_1: (0.0282) | Acc_1: (99.05%) (36895/37248)\n",
      "Epoch: 156 | Batch_idx: 300 |  Loss_1: (0.0283) | Acc_1: (99.04%) (38160/38528)\n",
      "Epoch: 156 | Batch_idx: 310 |  Loss_1: (0.0284) | Acc_1: (99.05%) (39429/39808)\n",
      "Epoch: 156 | Batch_idx: 320 |  Loss_1: (0.0286) | Acc_1: (99.04%) (40693/41088)\n",
      "Epoch: 156 | Batch_idx: 330 |  Loss_1: (0.0286) | Acc_1: (99.04%) (41962/42368)\n",
      "Epoch: 156 | Batch_idx: 340 |  Loss_1: (0.0287) | Acc_1: (99.04%) (43229/43648)\n",
      "Epoch: 156 | Batch_idx: 350 |  Loss_1: (0.0291) | Acc_1: (99.03%) (44492/44928)\n",
      "Epoch: 156 | Batch_idx: 360 |  Loss_1: (0.0290) | Acc_1: (99.03%) (45760/46208)\n",
      "Epoch: 156 | Batch_idx: 370 |  Loss_1: (0.0290) | Acc_1: (99.02%) (47023/47488)\n",
      "Epoch: 156 | Batch_idx: 380 |  Loss_1: (0.0294) | Acc_1: (99.01%) (48286/48768)\n",
      "Epoch: 156 | Batch_idx: 390 |  Loss_1: (0.0295) | Acc_1: (99.00%) (49502/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4795) | Acc: (91.28%) (9128/10000)\n",
      "Epoch: 157 | Batch_idx: 0 |  Loss_1: (0.0212) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 157 | Batch_idx: 10 |  Loss_1: (0.0401) | Acc_1: (98.65%) (1389/1408)\n",
      "Epoch: 157 | Batch_idx: 20 |  Loss_1: (0.0328) | Acc_1: (98.88%) (2658/2688)\n",
      "Epoch: 157 | Batch_idx: 30 |  Loss_1: (0.0325) | Acc_1: (98.87%) (3923/3968)\n",
      "Epoch: 157 | Batch_idx: 40 |  Loss_1: (0.0319) | Acc_1: (98.95%) (5193/5248)\n",
      "Epoch: 157 | Batch_idx: 50 |  Loss_1: (0.0348) | Acc_1: (98.90%) (6456/6528)\n",
      "Epoch: 157 | Batch_idx: 60 |  Loss_1: (0.0332) | Acc_1: (98.95%) (7726/7808)\n",
      "Epoch: 157 | Batch_idx: 70 |  Loss_1: (0.0346) | Acc_1: (98.93%) (8991/9088)\n",
      "Epoch: 157 | Batch_idx: 80 |  Loss_1: (0.0339) | Acc_1: (98.94%) (10258/10368)\n",
      "Epoch: 157 | Batch_idx: 90 |  Loss_1: (0.0333) | Acc_1: (98.96%) (11527/11648)\n",
      "Epoch: 157 | Batch_idx: 100 |  Loss_1: (0.0331) | Acc_1: (98.96%) (12793/12928)\n",
      "Epoch: 157 | Batch_idx: 110 |  Loss_1: (0.0338) | Acc_1: (98.95%) (14059/14208)\n",
      "Epoch: 157 | Batch_idx: 120 |  Loss_1: (0.0341) | Acc_1: (98.92%) (15321/15488)\n",
      "Epoch: 157 | Batch_idx: 130 |  Loss_1: (0.0337) | Acc_1: (98.93%) (16589/16768)\n",
      "Epoch: 157 | Batch_idx: 140 |  Loss_1: (0.0332) | Acc_1: (98.94%) (17856/18048)\n",
      "Epoch: 157 | Batch_idx: 150 |  Loss_1: (0.0330) | Acc_1: (98.93%) (19121/19328)\n",
      "Epoch: 157 | Batch_idx: 160 |  Loss_1: (0.0329) | Acc_1: (98.92%) (20385/20608)\n",
      "Epoch: 157 | Batch_idx: 170 |  Loss_1: (0.0330) | Acc_1: (98.93%) (21653/21888)\n",
      "Epoch: 157 | Batch_idx: 180 |  Loss_1: (0.0338) | Acc_1: (98.90%) (22913/23168)\n",
      "Epoch: 157 | Batch_idx: 190 |  Loss_1: (0.0336) | Acc_1: (98.91%) (24182/24448)\n",
      "Epoch: 157 | Batch_idx: 200 |  Loss_1: (0.0329) | Acc_1: (98.93%) (25453/25728)\n",
      "Epoch: 157 | Batch_idx: 210 |  Loss_1: (0.0325) | Acc_1: (98.94%) (26721/27008)\n",
      "Epoch: 157 | Batch_idx: 220 |  Loss_1: (0.0326) | Acc_1: (98.95%) (27991/28288)\n",
      "Epoch: 157 | Batch_idx: 230 |  Loss_1: (0.0322) | Acc_1: (98.97%) (29262/29568)\n",
      "Epoch: 157 | Batch_idx: 240 |  Loss_1: (0.0315) | Acc_1: (98.99%) (30535/30848)\n",
      "Epoch: 157 | Batch_idx: 250 |  Loss_1: (0.0318) | Acc_1: (98.98%) (31800/32128)\n",
      "Epoch: 157 | Batch_idx: 260 |  Loss_1: (0.0317) | Acc_1: (98.98%) (33066/33408)\n",
      "Epoch: 157 | Batch_idx: 270 |  Loss_1: (0.0318) | Acc_1: (98.98%) (34334/34688)\n",
      "Epoch: 157 | Batch_idx: 280 |  Loss_1: (0.0315) | Acc_1: (98.98%) (35601/35968)\n",
      "Epoch: 157 | Batch_idx: 290 |  Loss_1: (0.0318) | Acc_1: (98.95%) (36858/37248)\n",
      "Epoch: 157 | Batch_idx: 300 |  Loss_1: (0.0317) | Acc_1: (98.96%) (38126/38528)\n",
      "Epoch: 157 | Batch_idx: 310 |  Loss_1: (0.0315) | Acc_1: (98.96%) (39394/39808)\n",
      "Epoch: 157 | Batch_idx: 320 |  Loss_1: (0.0315) | Acc_1: (98.96%) (40662/41088)\n",
      "Epoch: 157 | Batch_idx: 330 |  Loss_1: (0.0312) | Acc_1: (98.98%) (41934/42368)\n",
      "Epoch: 157 | Batch_idx: 340 |  Loss_1: (0.0313) | Acc_1: (98.97%) (43199/43648)\n",
      "Epoch: 157 | Batch_idx: 350 |  Loss_1: (0.0310) | Acc_1: (98.98%) (44469/44928)\n",
      "Epoch: 157 | Batch_idx: 360 |  Loss_1: (0.0310) | Acc_1: (98.98%) (45736/46208)\n",
      "Epoch: 157 | Batch_idx: 370 |  Loss_1: (0.0309) | Acc_1: (98.98%) (47005/47488)\n",
      "Epoch: 157 | Batch_idx: 380 |  Loss_1: (0.0307) | Acc_1: (98.99%) (48275/48768)\n",
      "Epoch: 157 | Batch_idx: 390 |  Loss_1: (0.0303) | Acc_1: (99.00%) (49500/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4511) | Acc: (91.74%) (9174/10000)\n",
      "Epoch: 158 | Batch_idx: 0 |  Loss_1: (0.0046) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 158 | Batch_idx: 10 |  Loss_1: (0.0193) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 158 | Batch_idx: 20 |  Loss_1: (0.0231) | Acc_1: (99.11%) (2664/2688)\n",
      "Epoch: 158 | Batch_idx: 30 |  Loss_1: (0.0235) | Acc_1: (99.07%) (3931/3968)\n",
      "Epoch: 158 | Batch_idx: 40 |  Loss_1: (0.0250) | Acc_1: (99.09%) (5200/5248)\n",
      "Epoch: 158 | Batch_idx: 50 |  Loss_1: (0.0235) | Acc_1: (99.17%) (6474/6528)\n",
      "Epoch: 158 | Batch_idx: 60 |  Loss_1: (0.0241) | Acc_1: (99.21%) (7746/7808)\n",
      "Epoch: 158 | Batch_idx: 70 |  Loss_1: (0.0237) | Acc_1: (99.20%) (9015/9088)\n",
      "Epoch: 158 | Batch_idx: 80 |  Loss_1: (0.0233) | Acc_1: (99.20%) (10285/10368)\n",
      "Epoch: 158 | Batch_idx: 90 |  Loss_1: (0.0240) | Acc_1: (99.17%) (11551/11648)\n",
      "Epoch: 158 | Batch_idx: 100 |  Loss_1: (0.0241) | Acc_1: (99.16%) (12819/12928)\n",
      "Epoch: 158 | Batch_idx: 110 |  Loss_1: (0.0251) | Acc_1: (99.13%) (14085/14208)\n",
      "Epoch: 158 | Batch_idx: 120 |  Loss_1: (0.0251) | Acc_1: (99.14%) (15355/15488)\n",
      "Epoch: 158 | Batch_idx: 130 |  Loss_1: (0.0264) | Acc_1: (99.12%) (16621/16768)\n",
      "Epoch: 158 | Batch_idx: 140 |  Loss_1: (0.0268) | Acc_1: (99.10%) (17886/18048)\n",
      "Epoch: 158 | Batch_idx: 150 |  Loss_1: (0.0273) | Acc_1: (99.08%) (19151/19328)\n",
      "Epoch: 158 | Batch_idx: 160 |  Loss_1: (0.0275) | Acc_1: (99.09%) (20421/20608)\n",
      "Epoch: 158 | Batch_idx: 170 |  Loss_1: (0.0270) | Acc_1: (99.10%) (21690/21888)\n",
      "Epoch: 158 | Batch_idx: 180 |  Loss_1: (0.0272) | Acc_1: (99.11%) (22961/23168)\n",
      "Epoch: 158 | Batch_idx: 190 |  Loss_1: (0.0268) | Acc_1: (99.12%) (24233/24448)\n",
      "Epoch: 158 | Batch_idx: 200 |  Loss_1: (0.0271) | Acc_1: (99.11%) (25499/25728)\n",
      "Epoch: 158 | Batch_idx: 210 |  Loss_1: (0.0274) | Acc_1: (99.10%) (26764/27008)\n",
      "Epoch: 158 | Batch_idx: 220 |  Loss_1: (0.0276) | Acc_1: (99.09%) (28031/28288)\n",
      "Epoch: 158 | Batch_idx: 230 |  Loss_1: (0.0274) | Acc_1: (99.10%) (29303/29568)\n",
      "Epoch: 158 | Batch_idx: 240 |  Loss_1: (0.0274) | Acc_1: (99.11%) (30572/30848)\n",
      "Epoch: 158 | Batch_idx: 250 |  Loss_1: (0.0275) | Acc_1: (99.09%) (31836/32128)\n",
      "Epoch: 158 | Batch_idx: 260 |  Loss_1: (0.0282) | Acc_1: (99.08%) (33102/33408)\n",
      "Epoch: 158 | Batch_idx: 270 |  Loss_1: (0.0281) | Acc_1: (99.09%) (34372/34688)\n",
      "Epoch: 158 | Batch_idx: 280 |  Loss_1: (0.0282) | Acc_1: (99.08%) (35638/35968)\n",
      "Epoch: 158 | Batch_idx: 290 |  Loss_1: (0.0280) | Acc_1: (99.10%) (36912/37248)\n",
      "Epoch: 158 | Batch_idx: 300 |  Loss_1: (0.0282) | Acc_1: (99.09%) (38178/38528)\n",
      "Epoch: 158 | Batch_idx: 310 |  Loss_1: (0.0282) | Acc_1: (99.10%) (39448/39808)\n",
      "Epoch: 158 | Batch_idx: 320 |  Loss_1: (0.0278) | Acc_1: (99.10%) (40720/41088)\n",
      "Epoch: 158 | Batch_idx: 330 |  Loss_1: (0.0276) | Acc_1: (99.11%) (41990/42368)\n",
      "Epoch: 158 | Batch_idx: 340 |  Loss_1: (0.0275) | Acc_1: (99.11%) (43259/43648)\n",
      "Epoch: 158 | Batch_idx: 350 |  Loss_1: (0.0277) | Acc_1: (99.09%) (44520/44928)\n",
      "Epoch: 158 | Batch_idx: 360 |  Loss_1: (0.0277) | Acc_1: (99.09%) (45787/46208)\n",
      "Epoch: 158 | Batch_idx: 370 |  Loss_1: (0.0280) | Acc_1: (99.08%) (47053/47488)\n",
      "Epoch: 158 | Batch_idx: 380 |  Loss_1: (0.0280) | Acc_1: (99.08%) (48319/48768)\n",
      "Epoch: 158 | Batch_idx: 390 |  Loss_1: (0.0282) | Acc_1: (99.07%) (49537/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5032) | Acc: (91.05%) (9105/10000)\n",
      "Epoch: 159 | Batch_idx: 0 |  Loss_1: (0.0711) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 159 | Batch_idx: 10 |  Loss_1: (0.0246) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 159 | Batch_idx: 20 |  Loss_1: (0.0281) | Acc_1: (99.07%) (2663/2688)\n",
      "Epoch: 159 | Batch_idx: 30 |  Loss_1: (0.0256) | Acc_1: (99.09%) (3932/3968)\n",
      "Epoch: 159 | Batch_idx: 40 |  Loss_1: (0.0299) | Acc_1: (98.88%) (5189/5248)\n",
      "Epoch: 159 | Batch_idx: 50 |  Loss_1: (0.0283) | Acc_1: (98.93%) (6458/6528)\n",
      "Epoch: 159 | Batch_idx: 60 |  Loss_1: (0.0290) | Acc_1: (98.95%) (7726/7808)\n",
      "Epoch: 159 | Batch_idx: 70 |  Loss_1: (0.0297) | Acc_1: (98.91%) (8989/9088)\n",
      "Epoch: 159 | Batch_idx: 80 |  Loss_1: (0.0295) | Acc_1: (98.94%) (10258/10368)\n",
      "Epoch: 159 | Batch_idx: 90 |  Loss_1: (0.0305) | Acc_1: (98.92%) (11522/11648)\n",
      "Epoch: 159 | Batch_idx: 100 |  Loss_1: (0.0296) | Acc_1: (98.92%) (12789/12928)\n",
      "Epoch: 159 | Batch_idx: 110 |  Loss_1: (0.0294) | Acc_1: (98.94%) (14057/14208)\n",
      "Epoch: 159 | Batch_idx: 120 |  Loss_1: (0.0294) | Acc_1: (98.92%) (15321/15488)\n",
      "Epoch: 159 | Batch_idx: 130 |  Loss_1: (0.0289) | Acc_1: (98.93%) (16588/16768)\n",
      "Epoch: 159 | Batch_idx: 140 |  Loss_1: (0.0295) | Acc_1: (98.92%) (17853/18048)\n",
      "Epoch: 159 | Batch_idx: 150 |  Loss_1: (0.0290) | Acc_1: (98.92%) (19120/19328)\n",
      "Epoch: 159 | Batch_idx: 160 |  Loss_1: (0.0289) | Acc_1: (98.93%) (20388/20608)\n",
      "Epoch: 159 | Batch_idx: 170 |  Loss_1: (0.0288) | Acc_1: (98.94%) (21655/21888)\n",
      "Epoch: 159 | Batch_idx: 180 |  Loss_1: (0.0284) | Acc_1: (98.96%) (22927/23168)\n",
      "Epoch: 159 | Batch_idx: 190 |  Loss_1: (0.0285) | Acc_1: (98.97%) (24195/24448)\n",
      "Epoch: 159 | Batch_idx: 200 |  Loss_1: (0.0282) | Acc_1: (98.97%) (25464/25728)\n",
      "Epoch: 159 | Batch_idx: 210 |  Loss_1: (0.0281) | Acc_1: (98.98%) (26733/27008)\n",
      "Epoch: 159 | Batch_idx: 220 |  Loss_1: (0.0285) | Acc_1: (98.95%) (27992/28288)\n",
      "Epoch: 159 | Batch_idx: 230 |  Loss_1: (0.0285) | Acc_1: (98.96%) (29260/29568)\n",
      "Epoch: 159 | Batch_idx: 240 |  Loss_1: (0.0290) | Acc_1: (98.94%) (30522/30848)\n",
      "Epoch: 159 | Batch_idx: 250 |  Loss_1: (0.0293) | Acc_1: (98.94%) (31787/32128)\n",
      "Epoch: 159 | Batch_idx: 260 |  Loss_1: (0.0296) | Acc_1: (98.94%) (33053/33408)\n",
      "Epoch: 159 | Batch_idx: 270 |  Loss_1: (0.0296) | Acc_1: (98.94%) (34321/34688)\n",
      "Epoch: 159 | Batch_idx: 280 |  Loss_1: (0.0297) | Acc_1: (98.95%) (35589/35968)\n",
      "Epoch: 159 | Batch_idx: 290 |  Loss_1: (0.0293) | Acc_1: (98.96%) (36860/37248)\n",
      "Epoch: 159 | Batch_idx: 300 |  Loss_1: (0.0292) | Acc_1: (98.96%) (38127/38528)\n",
      "Epoch: 159 | Batch_idx: 310 |  Loss_1: (0.0292) | Acc_1: (98.96%) (39395/39808)\n",
      "Epoch: 159 | Batch_idx: 320 |  Loss_1: (0.0296) | Acc_1: (98.95%) (40658/41088)\n",
      "Epoch: 159 | Batch_idx: 330 |  Loss_1: (0.0291) | Acc_1: (98.97%) (41933/42368)\n",
      "Epoch: 159 | Batch_idx: 340 |  Loss_1: (0.0290) | Acc_1: (98.98%) (43202/43648)\n",
      "Epoch: 159 | Batch_idx: 350 |  Loss_1: (0.0288) | Acc_1: (98.98%) (44471/44928)\n",
      "Epoch: 159 | Batch_idx: 360 |  Loss_1: (0.0286) | Acc_1: (98.99%) (45742/46208)\n",
      "Epoch: 159 | Batch_idx: 370 |  Loss_1: (0.0284) | Acc_1: (98.99%) (47010/47488)\n",
      "Epoch: 159 | Batch_idx: 380 |  Loss_1: (0.0283) | Acc_1: (99.01%) (48284/48768)\n",
      "Epoch: 159 | Batch_idx: 390 |  Loss_1: (0.0284) | Acc_1: (99.00%) (49501/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4392) | Acc: (91.49%) (9149/10000)\n",
      "Epoch: 160 | Batch_idx: 0 |  Loss_1: (0.0310) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 160 | Batch_idx: 10 |  Loss_1: (0.0268) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 160 | Batch_idx: 20 |  Loss_1: (0.0332) | Acc_1: (98.81%) (2656/2688)\n",
      "Epoch: 160 | Batch_idx: 30 |  Loss_1: (0.0325) | Acc_1: (98.82%) (3921/3968)\n",
      "Epoch: 160 | Batch_idx: 40 |  Loss_1: (0.0337) | Acc_1: (98.72%) (5181/5248)\n",
      "Epoch: 160 | Batch_idx: 50 |  Loss_1: (0.0320) | Acc_1: (98.84%) (6452/6528)\n",
      "Epoch: 160 | Batch_idx: 60 |  Loss_1: (0.0301) | Acc_1: (98.90%) (7722/7808)\n",
      "Epoch: 160 | Batch_idx: 70 |  Loss_1: (0.0298) | Acc_1: (98.92%) (8990/9088)\n",
      "Epoch: 160 | Batch_idx: 80 |  Loss_1: (0.0308) | Acc_1: (98.88%) (10252/10368)\n",
      "Epoch: 160 | Batch_idx: 90 |  Loss_1: (0.0301) | Acc_1: (98.89%) (11519/11648)\n",
      "Epoch: 160 | Batch_idx: 100 |  Loss_1: (0.0296) | Acc_1: (98.94%) (12791/12928)\n",
      "Epoch: 160 | Batch_idx: 110 |  Loss_1: (0.0289) | Acc_1: (98.97%) (14062/14208)\n",
      "Epoch: 160 | Batch_idx: 120 |  Loss_1: (0.0283) | Acc_1: (98.97%) (15329/15488)\n",
      "Epoch: 160 | Batch_idx: 130 |  Loss_1: (0.0283) | Acc_1: (98.99%) (16598/16768)\n",
      "Epoch: 160 | Batch_idx: 140 |  Loss_1: (0.0277) | Acc_1: (99.01%) (17870/18048)\n",
      "Epoch: 160 | Batch_idx: 150 |  Loss_1: (0.0278) | Acc_1: (99.00%) (19135/19328)\n",
      "Epoch: 160 | Batch_idx: 160 |  Loss_1: (0.0283) | Acc_1: (98.99%) (20400/20608)\n",
      "Epoch: 160 | Batch_idx: 170 |  Loss_1: (0.0281) | Acc_1: (98.99%) (21666/21888)\n",
      "Epoch: 160 | Batch_idx: 180 |  Loss_1: (0.0275) | Acc_1: (99.02%) (22940/23168)\n",
      "Epoch: 160 | Batch_idx: 190 |  Loss_1: (0.0274) | Acc_1: (99.02%) (24209/24448)\n",
      "Epoch: 160 | Batch_idx: 200 |  Loss_1: (0.0281) | Acc_1: (99.00%) (25472/25728)\n",
      "Epoch: 160 | Batch_idx: 210 |  Loss_1: (0.0291) | Acc_1: (98.96%) (26728/27008)\n",
      "Epoch: 160 | Batch_idx: 220 |  Loss_1: (0.0290) | Acc_1: (98.96%) (27993/28288)\n",
      "Epoch: 160 | Batch_idx: 230 |  Loss_1: (0.0286) | Acc_1: (98.98%) (29265/29568)\n",
      "Epoch: 160 | Batch_idx: 240 |  Loss_1: (0.0282) | Acc_1: (99.00%) (30538/30848)\n",
      "Epoch: 160 | Batch_idx: 250 |  Loss_1: (0.0280) | Acc_1: (99.00%) (31806/32128)\n",
      "Epoch: 160 | Batch_idx: 260 |  Loss_1: (0.0279) | Acc_1: (99.00%) (33075/33408)\n",
      "Epoch: 160 | Batch_idx: 270 |  Loss_1: (0.0277) | Acc_1: (99.01%) (34345/34688)\n",
      "Epoch: 160 | Batch_idx: 280 |  Loss_1: (0.0278) | Acc_1: (99.00%) (35608/35968)\n",
      "Epoch: 160 | Batch_idx: 290 |  Loss_1: (0.0278) | Acc_1: (99.00%) (36875/37248)\n",
      "Epoch: 160 | Batch_idx: 300 |  Loss_1: (0.0277) | Acc_1: (99.00%) (38142/38528)\n",
      "Epoch: 160 | Batch_idx: 310 |  Loss_1: (0.0275) | Acc_1: (99.01%) (39414/39808)\n",
      "Epoch: 160 | Batch_idx: 320 |  Loss_1: (0.0275) | Acc_1: (99.02%) (40684/41088)\n",
      "Epoch: 160 | Batch_idx: 330 |  Loss_1: (0.0276) | Acc_1: (99.02%) (41951/42368)\n",
      "Epoch: 160 | Batch_idx: 340 |  Loss_1: (0.0273) | Acc_1: (99.02%) (43220/43648)\n",
      "Epoch: 160 | Batch_idx: 350 |  Loss_1: (0.0273) | Acc_1: (99.03%) (44491/44928)\n",
      "Epoch: 160 | Batch_idx: 360 |  Loss_1: (0.0274) | Acc_1: (99.02%) (45757/46208)\n",
      "Epoch: 160 | Batch_idx: 370 |  Loss_1: (0.0272) | Acc_1: (99.03%) (47029/47488)\n",
      "Epoch: 160 | Batch_idx: 380 |  Loss_1: (0.0273) | Acc_1: (99.04%) (48300/48768)\n",
      "Epoch: 160 | Batch_idx: 390 |  Loss_1: (0.0271) | Acc_1: (99.05%) (49524/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4593) | Acc: (91.50%) (9150/10000)\n",
      "Epoch: 161 | Batch_idx: 0 |  Loss_1: (0.0198) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 161 | Batch_idx: 10 |  Loss_1: (0.0297) | Acc_1: (99.01%) (1394/1408)\n",
      "Epoch: 161 | Batch_idx: 20 |  Loss_1: (0.0249) | Acc_1: (99.14%) (2665/2688)\n",
      "Epoch: 161 | Batch_idx: 30 |  Loss_1: (0.0262) | Acc_1: (99.12%) (3933/3968)\n",
      "Epoch: 161 | Batch_idx: 40 |  Loss_1: (0.0271) | Acc_1: (98.99%) (5195/5248)\n",
      "Epoch: 161 | Batch_idx: 50 |  Loss_1: (0.0261) | Acc_1: (99.07%) (6467/6528)\n",
      "Epoch: 161 | Batch_idx: 60 |  Loss_1: (0.0264) | Acc_1: (99.07%) (7735/7808)\n",
      "Epoch: 161 | Batch_idx: 70 |  Loss_1: (0.0263) | Acc_1: (99.10%) (9006/9088)\n",
      "Epoch: 161 | Batch_idx: 80 |  Loss_1: (0.0274) | Acc_1: (99.05%) (10269/10368)\n",
      "Epoch: 161 | Batch_idx: 90 |  Loss_1: (0.0276) | Acc_1: (99.06%) (11538/11648)\n",
      "Epoch: 161 | Batch_idx: 100 |  Loss_1: (0.0277) | Acc_1: (99.06%) (12807/12928)\n",
      "Epoch: 161 | Batch_idx: 110 |  Loss_1: (0.0284) | Acc_1: (99.01%) (14068/14208)\n",
      "Epoch: 161 | Batch_idx: 120 |  Loss_1: (0.0275) | Acc_1: (99.06%) (15342/15488)\n",
      "Epoch: 161 | Batch_idx: 130 |  Loss_1: (0.0278) | Acc_1: (99.05%) (16608/16768)\n",
      "Epoch: 161 | Batch_idx: 140 |  Loss_1: (0.0274) | Acc_1: (99.07%) (17881/18048)\n",
      "Epoch: 161 | Batch_idx: 150 |  Loss_1: (0.0270) | Acc_1: (99.08%) (19150/19328)\n",
      "Epoch: 161 | Batch_idx: 160 |  Loss_1: (0.0272) | Acc_1: (99.08%) (20418/20608)\n",
      "Epoch: 161 | Batch_idx: 170 |  Loss_1: (0.0278) | Acc_1: (99.05%) (21680/21888)\n",
      "Epoch: 161 | Batch_idx: 180 |  Loss_1: (0.0277) | Acc_1: (99.06%) (22950/23168)\n",
      "Epoch: 161 | Batch_idx: 190 |  Loss_1: (0.0274) | Acc_1: (99.06%) (24218/24448)\n",
      "Epoch: 161 | Batch_idx: 200 |  Loss_1: (0.0271) | Acc_1: (99.06%) (25487/25728)\n",
      "Epoch: 161 | Batch_idx: 210 |  Loss_1: (0.0270) | Acc_1: (99.07%) (26757/27008)\n",
      "Epoch: 161 | Batch_idx: 220 |  Loss_1: (0.0270) | Acc_1: (99.08%) (28027/28288)\n",
      "Epoch: 161 | Batch_idx: 230 |  Loss_1: (0.0270) | Acc_1: (99.08%) (29295/29568)\n",
      "Epoch: 161 | Batch_idx: 240 |  Loss_1: (0.0280) | Acc_1: (99.04%) (30553/30848)\n",
      "Epoch: 161 | Batch_idx: 250 |  Loss_1: (0.0284) | Acc_1: (99.04%) (31821/32128)\n",
      "Epoch: 161 | Batch_idx: 260 |  Loss_1: (0.0288) | Acc_1: (99.03%) (33083/33408)\n",
      "Epoch: 161 | Batch_idx: 270 |  Loss_1: (0.0289) | Acc_1: (99.02%) (34349/34688)\n",
      "Epoch: 161 | Batch_idx: 280 |  Loss_1: (0.0291) | Acc_1: (99.00%) (35610/35968)\n",
      "Epoch: 161 | Batch_idx: 290 |  Loss_1: (0.0295) | Acc_1: (98.99%) (36870/37248)\n",
      "Epoch: 161 | Batch_idx: 300 |  Loss_1: (0.0294) | Acc_1: (98.99%) (38138/38528)\n",
      "Epoch: 161 | Batch_idx: 310 |  Loss_1: (0.0295) | Acc_1: (98.99%) (39407/39808)\n",
      "Epoch: 161 | Batch_idx: 320 |  Loss_1: (0.0295) | Acc_1: (98.99%) (40674/41088)\n",
      "Epoch: 161 | Batch_idx: 330 |  Loss_1: (0.0296) | Acc_1: (98.99%) (41940/42368)\n",
      "Epoch: 161 | Batch_idx: 340 |  Loss_1: (0.0302) | Acc_1: (98.98%) (43203/43648)\n",
      "Epoch: 161 | Batch_idx: 350 |  Loss_1: (0.0300) | Acc_1: (98.99%) (44475/44928)\n",
      "Epoch: 161 | Batch_idx: 360 |  Loss_1: (0.0297) | Acc_1: (99.00%) (45746/46208)\n",
      "Epoch: 161 | Batch_idx: 370 |  Loss_1: (0.0295) | Acc_1: (99.01%) (47019/47488)\n",
      "Epoch: 161 | Batch_idx: 380 |  Loss_1: (0.0291) | Acc_1: (99.03%) (48293/48768)\n",
      "Epoch: 161 | Batch_idx: 390 |  Loss_1: (0.0289) | Acc_1: (99.04%) (49518/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4174) | Acc: (91.88%) (9188/10000)\n",
      "Epoch: 162 | Batch_idx: 0 |  Loss_1: (0.0161) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 162 | Batch_idx: 10 |  Loss_1: (0.0135) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 162 | Batch_idx: 20 |  Loss_1: (0.0196) | Acc_1: (99.37%) (2671/2688)\n",
      "Epoch: 162 | Batch_idx: 30 |  Loss_1: (0.0201) | Acc_1: (99.40%) (3944/3968)\n",
      "Epoch: 162 | Batch_idx: 40 |  Loss_1: (0.0200) | Acc_1: (99.35%) (5214/5248)\n",
      "Epoch: 162 | Batch_idx: 50 |  Loss_1: (0.0198) | Acc_1: (99.36%) (6486/6528)\n",
      "Epoch: 162 | Batch_idx: 60 |  Loss_1: (0.0227) | Acc_1: (99.28%) (7752/7808)\n",
      "Epoch: 162 | Batch_idx: 70 |  Loss_1: (0.0220) | Acc_1: (99.28%) (9023/9088)\n",
      "Epoch: 162 | Batch_idx: 80 |  Loss_1: (0.0238) | Acc_1: (99.21%) (10286/10368)\n",
      "Epoch: 162 | Batch_idx: 90 |  Loss_1: (0.0236) | Acc_1: (99.24%) (11559/11648)\n",
      "Epoch: 162 | Batch_idx: 100 |  Loss_1: (0.0242) | Acc_1: (99.22%) (12827/12928)\n",
      "Epoch: 162 | Batch_idx: 110 |  Loss_1: (0.0242) | Acc_1: (99.20%) (14095/14208)\n",
      "Epoch: 162 | Batch_idx: 120 |  Loss_1: (0.0240) | Acc_1: (99.21%) (15366/15488)\n",
      "Epoch: 162 | Batch_idx: 130 |  Loss_1: (0.0236) | Acc_1: (99.23%) (16639/16768)\n",
      "Epoch: 162 | Batch_idx: 140 |  Loss_1: (0.0228) | Acc_1: (99.25%) (17913/18048)\n",
      "Epoch: 162 | Batch_idx: 150 |  Loss_1: (0.0227) | Acc_1: (99.27%) (19187/19328)\n",
      "Epoch: 162 | Batch_idx: 160 |  Loss_1: (0.0227) | Acc_1: (99.27%) (20458/20608)\n",
      "Epoch: 162 | Batch_idx: 170 |  Loss_1: (0.0233) | Acc_1: (99.24%) (21722/21888)\n",
      "Epoch: 162 | Batch_idx: 180 |  Loss_1: (0.0230) | Acc_1: (99.26%) (22996/23168)\n",
      "Epoch: 162 | Batch_idx: 190 |  Loss_1: (0.0229) | Acc_1: (99.25%) (24264/24448)\n",
      "Epoch: 162 | Batch_idx: 200 |  Loss_1: (0.0230) | Acc_1: (99.25%) (25535/25728)\n",
      "Epoch: 162 | Batch_idx: 210 |  Loss_1: (0.0234) | Acc_1: (99.24%) (26803/27008)\n",
      "Epoch: 162 | Batch_idx: 220 |  Loss_1: (0.0232) | Acc_1: (99.24%) (28074/28288)\n",
      "Epoch: 162 | Batch_idx: 230 |  Loss_1: (0.0235) | Acc_1: (99.23%) (29341/29568)\n",
      "Epoch: 162 | Batch_idx: 240 |  Loss_1: (0.0240) | Acc_1: (99.23%) (30611/30848)\n",
      "Epoch: 162 | Batch_idx: 250 |  Loss_1: (0.0239) | Acc_1: (99.23%) (31880/32128)\n",
      "Epoch: 162 | Batch_idx: 260 |  Loss_1: (0.0243) | Acc_1: (99.22%) (33146/33408)\n",
      "Epoch: 162 | Batch_idx: 270 |  Loss_1: (0.0243) | Acc_1: (99.21%) (34414/34688)\n",
      "Epoch: 162 | Batch_idx: 280 |  Loss_1: (0.0244) | Acc_1: (99.20%) (35682/35968)\n",
      "Epoch: 162 | Batch_idx: 290 |  Loss_1: (0.0248) | Acc_1: (99.19%) (36948/37248)\n",
      "Epoch: 162 | Batch_idx: 300 |  Loss_1: (0.0249) | Acc_1: (99.19%) (38215/38528)\n",
      "Epoch: 162 | Batch_idx: 310 |  Loss_1: (0.0254) | Acc_1: (99.17%) (39478/39808)\n",
      "Epoch: 162 | Batch_idx: 320 |  Loss_1: (0.0261) | Acc_1: (99.15%) (40739/41088)\n",
      "Epoch: 162 | Batch_idx: 330 |  Loss_1: (0.0266) | Acc_1: (99.13%) (42001/42368)\n",
      "Epoch: 162 | Batch_idx: 340 |  Loss_1: (0.0265) | Acc_1: (99.13%) (43270/43648)\n",
      "Epoch: 162 | Batch_idx: 350 |  Loss_1: (0.0270) | Acc_1: (99.12%) (44533/44928)\n",
      "Epoch: 162 | Batch_idx: 360 |  Loss_1: (0.0271) | Acc_1: (99.12%) (45803/46208)\n",
      "Epoch: 162 | Batch_idx: 370 |  Loss_1: (0.0276) | Acc_1: (99.11%) (47063/47488)\n",
      "Epoch: 162 | Batch_idx: 380 |  Loss_1: (0.0278) | Acc_1: (99.09%) (48326/48768)\n",
      "Epoch: 162 | Batch_idx: 390 |  Loss_1: (0.0278) | Acc_1: (99.09%) (49546/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4697) | Acc: (91.05%) (9105/10000)\n",
      "Epoch: 163 | Batch_idx: 0 |  Loss_1: (0.0667) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 163 | Batch_idx: 10 |  Loss_1: (0.0414) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 163 | Batch_idx: 20 |  Loss_1: (0.0441) | Acc_1: (98.81%) (2656/2688)\n",
      "Epoch: 163 | Batch_idx: 30 |  Loss_1: (0.0383) | Acc_1: (98.87%) (3923/3968)\n",
      "Epoch: 163 | Batch_idx: 40 |  Loss_1: (0.0353) | Acc_1: (98.95%) (5193/5248)\n",
      "Epoch: 163 | Batch_idx: 50 |  Loss_1: (0.0363) | Acc_1: (98.96%) (6460/6528)\n",
      "Epoch: 163 | Batch_idx: 60 |  Loss_1: (0.0368) | Acc_1: (98.92%) (7724/7808)\n",
      "Epoch: 163 | Batch_idx: 70 |  Loss_1: (0.0347) | Acc_1: (98.97%) (8994/9088)\n",
      "Epoch: 163 | Batch_idx: 80 |  Loss_1: (0.0344) | Acc_1: (98.97%) (10261/10368)\n",
      "Epoch: 163 | Batch_idx: 90 |  Loss_1: (0.0348) | Acc_1: (98.94%) (11525/11648)\n",
      "Epoch: 163 | Batch_idx: 100 |  Loss_1: (0.0339) | Acc_1: (98.95%) (12792/12928)\n",
      "Epoch: 163 | Batch_idx: 110 |  Loss_1: (0.0333) | Acc_1: (98.97%) (14062/14208)\n",
      "Epoch: 163 | Batch_idx: 120 |  Loss_1: (0.0327) | Acc_1: (98.99%) (15332/15488)\n",
      "Epoch: 163 | Batch_idx: 130 |  Loss_1: (0.0317) | Acc_1: (99.03%) (16606/16768)\n",
      "Epoch: 163 | Batch_idx: 140 |  Loss_1: (0.0315) | Acc_1: (99.05%) (17876/18048)\n",
      "Epoch: 163 | Batch_idx: 150 |  Loss_1: (0.0312) | Acc_1: (99.04%) (19142/19328)\n",
      "Epoch: 163 | Batch_idx: 160 |  Loss_1: (0.0308) | Acc_1: (99.04%) (20410/20608)\n",
      "Epoch: 163 | Batch_idx: 170 |  Loss_1: (0.0303) | Acc_1: (99.05%) (21681/21888)\n",
      "Epoch: 163 | Batch_idx: 180 |  Loss_1: (0.0311) | Acc_1: (99.01%) (22939/23168)\n",
      "Epoch: 163 | Batch_idx: 190 |  Loss_1: (0.0308) | Acc_1: (99.01%) (24207/24448)\n",
      "Epoch: 163 | Batch_idx: 200 |  Loss_1: (0.0309) | Acc_1: (99.01%) (25473/25728)\n",
      "Epoch: 163 | Batch_idx: 210 |  Loss_1: (0.0303) | Acc_1: (99.04%) (26748/27008)\n",
      "Epoch: 163 | Batch_idx: 220 |  Loss_1: (0.0301) | Acc_1: (99.03%) (28015/28288)\n",
      "Epoch: 163 | Batch_idx: 230 |  Loss_1: (0.0306) | Acc_1: (99.02%) (29277/29568)\n",
      "Epoch: 163 | Batch_idx: 240 |  Loss_1: (0.0304) | Acc_1: (99.02%) (30547/30848)\n",
      "Epoch: 163 | Batch_idx: 250 |  Loss_1: (0.0305) | Acc_1: (99.03%) (31815/32128)\n",
      "Epoch: 163 | Batch_idx: 260 |  Loss_1: (0.0304) | Acc_1: (99.02%) (33082/33408)\n",
      "Epoch: 163 | Batch_idx: 270 |  Loss_1: (0.0302) | Acc_1: (99.03%) (34353/34688)\n",
      "Epoch: 163 | Batch_idx: 280 |  Loss_1: (0.0300) | Acc_1: (99.04%) (35622/35968)\n",
      "Epoch: 163 | Batch_idx: 290 |  Loss_1: (0.0297) | Acc_1: (99.04%) (36891/37248)\n",
      "Epoch: 163 | Batch_idx: 300 |  Loss_1: (0.0296) | Acc_1: (99.04%) (38159/38528)\n",
      "Epoch: 163 | Batch_idx: 310 |  Loss_1: (0.0296) | Acc_1: (99.05%) (39430/39808)\n",
      "Epoch: 163 | Batch_idx: 320 |  Loss_1: (0.0295) | Acc_1: (99.05%) (40699/41088)\n",
      "Epoch: 163 | Batch_idx: 330 |  Loss_1: (0.0300) | Acc_1: (99.04%) (41961/42368)\n",
      "Epoch: 163 | Batch_idx: 340 |  Loss_1: (0.0296) | Acc_1: (99.05%) (43234/43648)\n",
      "Epoch: 163 | Batch_idx: 350 |  Loss_1: (0.0297) | Acc_1: (99.05%) (44499/44928)\n",
      "Epoch: 163 | Batch_idx: 360 |  Loss_1: (0.0297) | Acc_1: (99.04%) (45765/46208)\n",
      "Epoch: 163 | Batch_idx: 370 |  Loss_1: (0.0298) | Acc_1: (99.03%) (47029/47488)\n",
      "Epoch: 163 | Batch_idx: 380 |  Loss_1: (0.0297) | Acc_1: (99.04%) (48300/48768)\n",
      "Epoch: 163 | Batch_idx: 390 |  Loss_1: (0.0301) | Acc_1: (99.02%) (49511/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4560) | Acc: (91.67%) (9167/10000)\n",
      "Epoch: 164 | Batch_idx: 0 |  Loss_1: (0.0151) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 164 | Batch_idx: 10 |  Loss_1: (0.0251) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 164 | Batch_idx: 20 |  Loss_1: (0.0218) | Acc_1: (99.26%) (2668/2688)\n",
      "Epoch: 164 | Batch_idx: 30 |  Loss_1: (0.0256) | Acc_1: (99.17%) (3935/3968)\n",
      "Epoch: 164 | Batch_idx: 40 |  Loss_1: (0.0268) | Acc_1: (99.10%) (5201/5248)\n",
      "Epoch: 164 | Batch_idx: 50 |  Loss_1: (0.0279) | Acc_1: (99.05%) (6466/6528)\n",
      "Epoch: 164 | Batch_idx: 60 |  Loss_1: (0.0268) | Acc_1: (99.12%) (7739/7808)\n",
      "Epoch: 164 | Batch_idx: 70 |  Loss_1: (0.0276) | Acc_1: (99.06%) (9003/9088)\n",
      "Epoch: 164 | Batch_idx: 80 |  Loss_1: (0.0282) | Acc_1: (99.06%) (10271/10368)\n",
      "Epoch: 164 | Batch_idx: 90 |  Loss_1: (0.0277) | Acc_1: (99.07%) (11540/11648)\n",
      "Epoch: 164 | Batch_idx: 100 |  Loss_1: (0.0276) | Acc_1: (99.06%) (12806/12928)\n",
      "Epoch: 164 | Batch_idx: 110 |  Loss_1: (0.0281) | Acc_1: (99.03%) (14070/14208)\n",
      "Epoch: 164 | Batch_idx: 120 |  Loss_1: (0.0291) | Acc_1: (99.01%) (15335/15488)\n",
      "Epoch: 164 | Batch_idx: 130 |  Loss_1: (0.0287) | Acc_1: (99.02%) (16604/16768)\n",
      "Epoch: 164 | Batch_idx: 140 |  Loss_1: (0.0285) | Acc_1: (99.03%) (17873/18048)\n",
      "Epoch: 164 | Batch_idx: 150 |  Loss_1: (0.0285) | Acc_1: (99.03%) (19141/19328)\n",
      "Epoch: 164 | Batch_idx: 160 |  Loss_1: (0.0291) | Acc_1: (99.02%) (20406/20608)\n",
      "Epoch: 164 | Batch_idx: 170 |  Loss_1: (0.0288) | Acc_1: (99.02%) (21674/21888)\n",
      "Epoch: 164 | Batch_idx: 180 |  Loss_1: (0.0292) | Acc_1: (99.01%) (22939/23168)\n",
      "Epoch: 164 | Batch_idx: 190 |  Loss_1: (0.0293) | Acc_1: (99.01%) (24205/24448)\n",
      "Epoch: 164 | Batch_idx: 200 |  Loss_1: (0.0292) | Acc_1: (99.01%) (25474/25728)\n",
      "Epoch: 164 | Batch_idx: 210 |  Loss_1: (0.0295) | Acc_1: (99.02%) (26743/27008)\n",
      "Epoch: 164 | Batch_idx: 220 |  Loss_1: (0.0295) | Acc_1: (99.02%) (28010/28288)\n",
      "Epoch: 164 | Batch_idx: 230 |  Loss_1: (0.0296) | Acc_1: (99.02%) (29278/29568)\n",
      "Epoch: 164 | Batch_idx: 240 |  Loss_1: (0.0296) | Acc_1: (99.02%) (30547/30848)\n",
      "Epoch: 164 | Batch_idx: 250 |  Loss_1: (0.0297) | Acc_1: (99.04%) (31818/32128)\n",
      "Epoch: 164 | Batch_idx: 260 |  Loss_1: (0.0294) | Acc_1: (99.05%) (33089/33408)\n",
      "Epoch: 164 | Batch_idx: 270 |  Loss_1: (0.0294) | Acc_1: (99.05%) (34358/34688)\n",
      "Epoch: 164 | Batch_idx: 280 |  Loss_1: (0.0293) | Acc_1: (99.04%) (35624/35968)\n",
      "Epoch: 164 | Batch_idx: 290 |  Loss_1: (0.0290) | Acc_1: (99.05%) (36894/37248)\n",
      "Epoch: 164 | Batch_idx: 300 |  Loss_1: (0.0292) | Acc_1: (99.03%) (38156/38528)\n",
      "Epoch: 164 | Batch_idx: 310 |  Loss_1: (0.0289) | Acc_1: (99.05%) (39429/39808)\n",
      "Epoch: 164 | Batch_idx: 320 |  Loss_1: (0.0290) | Acc_1: (99.04%) (40694/41088)\n",
      "Epoch: 164 | Batch_idx: 330 |  Loss_1: (0.0287) | Acc_1: (99.05%) (41967/42368)\n",
      "Epoch: 164 | Batch_idx: 340 |  Loss_1: (0.0289) | Acc_1: (99.04%) (43231/43648)\n",
      "Epoch: 164 | Batch_idx: 350 |  Loss_1: (0.0292) | Acc_1: (99.03%) (44494/44928)\n",
      "Epoch: 164 | Batch_idx: 360 |  Loss_1: (0.0294) | Acc_1: (99.03%) (45762/46208)\n",
      "Epoch: 164 | Batch_idx: 370 |  Loss_1: (0.0295) | Acc_1: (99.03%) (47028/47488)\n",
      "Epoch: 164 | Batch_idx: 380 |  Loss_1: (0.0295) | Acc_1: (99.02%) (48292/48768)\n",
      "Epoch: 164 | Batch_idx: 390 |  Loss_1: (0.0296) | Acc_1: (99.02%) (49511/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4790) | Acc: (91.15%) (9115/10000)\n",
      "Epoch: 165 | Batch_idx: 0 |  Loss_1: (0.0184) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 165 | Batch_idx: 10 |  Loss_1: (0.0249) | Acc_1: (99.01%) (1394/1408)\n",
      "Epoch: 165 | Batch_idx: 20 |  Loss_1: (0.0245) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 165 | Batch_idx: 30 |  Loss_1: (0.0286) | Acc_1: (99.12%) (3933/3968)\n",
      "Epoch: 165 | Batch_idx: 40 |  Loss_1: (0.0282) | Acc_1: (99.10%) (5201/5248)\n",
      "Epoch: 165 | Batch_idx: 50 |  Loss_1: (0.0309) | Acc_1: (98.94%) (6459/6528)\n",
      "Epoch: 165 | Batch_idx: 60 |  Loss_1: (0.0306) | Acc_1: (99.00%) (7730/7808)\n",
      "Epoch: 165 | Batch_idx: 70 |  Loss_1: (0.0314) | Acc_1: (98.97%) (8994/9088)\n",
      "Epoch: 165 | Batch_idx: 80 |  Loss_1: (0.0304) | Acc_1: (99.00%) (10264/10368)\n",
      "Epoch: 165 | Batch_idx: 90 |  Loss_1: (0.0297) | Acc_1: (99.05%) (11537/11648)\n",
      "Epoch: 165 | Batch_idx: 100 |  Loss_1: (0.0298) | Acc_1: (99.04%) (12804/12928)\n",
      "Epoch: 165 | Batch_idx: 110 |  Loss_1: (0.0289) | Acc_1: (99.09%) (14078/14208)\n",
      "Epoch: 165 | Batch_idx: 120 |  Loss_1: (0.0284) | Acc_1: (99.10%) (15348/15488)\n",
      "Epoch: 165 | Batch_idx: 130 |  Loss_1: (0.0276) | Acc_1: (99.12%) (16621/16768)\n",
      "Epoch: 165 | Batch_idx: 140 |  Loss_1: (0.0284) | Acc_1: (99.09%) (17883/18048)\n",
      "Epoch: 165 | Batch_idx: 150 |  Loss_1: (0.0283) | Acc_1: (99.09%) (19153/19328)\n",
      "Epoch: 165 | Batch_idx: 160 |  Loss_1: (0.0279) | Acc_1: (99.11%) (20425/20608)\n",
      "Epoch: 165 | Batch_idx: 170 |  Loss_1: (0.0279) | Acc_1: (99.11%) (21694/21888)\n",
      "Epoch: 165 | Batch_idx: 180 |  Loss_1: (0.0290) | Acc_1: (99.09%) (22957/23168)\n",
      "Epoch: 165 | Batch_idx: 190 |  Loss_1: (0.0286) | Acc_1: (99.10%) (24227/24448)\n",
      "Epoch: 165 | Batch_idx: 200 |  Loss_1: (0.0283) | Acc_1: (99.08%) (25492/25728)\n",
      "Epoch: 165 | Batch_idx: 210 |  Loss_1: (0.0279) | Acc_1: (99.10%) (26764/27008)\n",
      "Epoch: 165 | Batch_idx: 220 |  Loss_1: (0.0281) | Acc_1: (99.09%) (28031/28288)\n",
      "Epoch: 165 | Batch_idx: 230 |  Loss_1: (0.0283) | Acc_1: (99.09%) (29299/29568)\n",
      "Epoch: 165 | Batch_idx: 240 |  Loss_1: (0.0281) | Acc_1: (99.11%) (30572/30848)\n",
      "Epoch: 165 | Batch_idx: 250 |  Loss_1: (0.0283) | Acc_1: (99.09%) (31835/32128)\n",
      "Epoch: 165 | Batch_idx: 260 |  Loss_1: (0.0279) | Acc_1: (99.11%) (33109/33408)\n",
      "Epoch: 165 | Batch_idx: 270 |  Loss_1: (0.0279) | Acc_1: (99.10%) (34376/34688)\n",
      "Epoch: 165 | Batch_idx: 280 |  Loss_1: (0.0278) | Acc_1: (99.10%) (35645/35968)\n",
      "Epoch: 165 | Batch_idx: 290 |  Loss_1: (0.0281) | Acc_1: (99.09%) (36908/37248)\n",
      "Epoch: 165 | Batch_idx: 300 |  Loss_1: (0.0280) | Acc_1: (99.09%) (38177/38528)\n",
      "Epoch: 165 | Batch_idx: 310 |  Loss_1: (0.0282) | Acc_1: (99.07%) (39439/39808)\n",
      "Epoch: 165 | Batch_idx: 320 |  Loss_1: (0.0282) | Acc_1: (99.07%) (40707/41088)\n",
      "Epoch: 165 | Batch_idx: 330 |  Loss_1: (0.0285) | Acc_1: (99.07%) (41973/42368)\n",
      "Epoch: 165 | Batch_idx: 340 |  Loss_1: (0.0284) | Acc_1: (99.07%) (43244/43648)\n",
      "Epoch: 165 | Batch_idx: 350 |  Loss_1: (0.0285) | Acc_1: (99.07%) (44508/44928)\n",
      "Epoch: 165 | Batch_idx: 360 |  Loss_1: (0.0288) | Acc_1: (99.05%) (45769/46208)\n",
      "Epoch: 165 | Batch_idx: 370 |  Loss_1: (0.0290) | Acc_1: (99.05%) (47035/47488)\n",
      "Epoch: 165 | Batch_idx: 380 |  Loss_1: (0.0294) | Acc_1: (99.03%) (48296/48768)\n",
      "Epoch: 165 | Batch_idx: 390 |  Loss_1: (0.0295) | Acc_1: (99.03%) (49514/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5081) | Acc: (90.60%) (9060/10000)\n",
      "Epoch: 166 | Batch_idx: 0 |  Loss_1: (0.0330) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 166 | Batch_idx: 10 |  Loss_1: (0.0336) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 166 | Batch_idx: 20 |  Loss_1: (0.0340) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 166 | Batch_idx: 30 |  Loss_1: (0.0344) | Acc_1: (98.89%) (3924/3968)\n",
      "Epoch: 166 | Batch_idx: 40 |  Loss_1: (0.0343) | Acc_1: (98.88%) (5189/5248)\n",
      "Epoch: 166 | Batch_idx: 50 |  Loss_1: (0.0337) | Acc_1: (98.87%) (6454/6528)\n",
      "Epoch: 166 | Batch_idx: 60 |  Loss_1: (0.0330) | Acc_1: (98.91%) (7723/7808)\n",
      "Epoch: 166 | Batch_idx: 70 |  Loss_1: (0.0319) | Acc_1: (98.95%) (8993/9088)\n",
      "Epoch: 166 | Batch_idx: 80 |  Loss_1: (0.0307) | Acc_1: (98.97%) (10261/10368)\n",
      "Epoch: 166 | Batch_idx: 90 |  Loss_1: (0.0295) | Acc_1: (99.02%) (11534/11648)\n",
      "Epoch: 166 | Batch_idx: 100 |  Loss_1: (0.0288) | Acc_1: (99.04%) (12804/12928)\n",
      "Epoch: 166 | Batch_idx: 110 |  Loss_1: (0.0287) | Acc_1: (99.04%) (14071/14208)\n",
      "Epoch: 166 | Batch_idx: 120 |  Loss_1: (0.0289) | Acc_1: (99.03%) (15337/15488)\n",
      "Epoch: 166 | Batch_idx: 130 |  Loss_1: (0.0286) | Acc_1: (99.05%) (16609/16768)\n",
      "Epoch: 166 | Batch_idx: 140 |  Loss_1: (0.0285) | Acc_1: (99.05%) (17877/18048)\n",
      "Epoch: 166 | Batch_idx: 150 |  Loss_1: (0.0282) | Acc_1: (99.07%) (19148/19328)\n",
      "Epoch: 166 | Batch_idx: 160 |  Loss_1: (0.0280) | Acc_1: (99.06%) (20414/20608)\n",
      "Epoch: 166 | Batch_idx: 170 |  Loss_1: (0.0283) | Acc_1: (99.03%) (21675/21888)\n",
      "Epoch: 166 | Batch_idx: 180 |  Loss_1: (0.0283) | Acc_1: (99.03%) (22944/23168)\n",
      "Epoch: 166 | Batch_idx: 190 |  Loss_1: (0.0284) | Acc_1: (99.03%) (24210/24448)\n",
      "Epoch: 166 | Batch_idx: 200 |  Loss_1: (0.0286) | Acc_1: (99.02%) (25477/25728)\n",
      "Epoch: 166 | Batch_idx: 210 |  Loss_1: (0.0282) | Acc_1: (99.04%) (26748/27008)\n",
      "Epoch: 166 | Batch_idx: 220 |  Loss_1: (0.0280) | Acc_1: (99.04%) (28017/28288)\n",
      "Epoch: 166 | Batch_idx: 230 |  Loss_1: (0.0280) | Acc_1: (99.04%) (29284/29568)\n",
      "Epoch: 166 | Batch_idx: 240 |  Loss_1: (0.0284) | Acc_1: (99.02%) (30546/30848)\n",
      "Epoch: 166 | Batch_idx: 250 |  Loss_1: (0.0284) | Acc_1: (99.03%) (31815/32128)\n",
      "Epoch: 166 | Batch_idx: 260 |  Loss_1: (0.0285) | Acc_1: (99.02%) (33079/33408)\n",
      "Epoch: 166 | Batch_idx: 270 |  Loss_1: (0.0284) | Acc_1: (99.02%) (34347/34688)\n",
      "Epoch: 166 | Batch_idx: 280 |  Loss_1: (0.0285) | Acc_1: (99.01%) (35613/35968)\n",
      "Epoch: 166 | Batch_idx: 290 |  Loss_1: (0.0288) | Acc_1: (99.00%) (36876/37248)\n",
      "Epoch: 166 | Batch_idx: 300 |  Loss_1: (0.0293) | Acc_1: (98.99%) (38138/38528)\n",
      "Epoch: 166 | Batch_idx: 310 |  Loss_1: (0.0292) | Acc_1: (98.99%) (39404/39808)\n",
      "Epoch: 166 | Batch_idx: 320 |  Loss_1: (0.0294) | Acc_1: (98.98%) (40670/41088)\n",
      "Epoch: 166 | Batch_idx: 330 |  Loss_1: (0.0294) | Acc_1: (98.98%) (41936/42368)\n",
      "Epoch: 166 | Batch_idx: 340 |  Loss_1: (0.0297) | Acc_1: (98.97%) (43200/43648)\n",
      "Epoch: 166 | Batch_idx: 350 |  Loss_1: (0.0297) | Acc_1: (98.97%) (44466/44928)\n",
      "Epoch: 166 | Batch_idx: 360 |  Loss_1: (0.0296) | Acc_1: (98.97%) (45733/46208)\n",
      "Epoch: 166 | Batch_idx: 370 |  Loss_1: (0.0298) | Acc_1: (98.97%) (47000/47488)\n",
      "Epoch: 166 | Batch_idx: 380 |  Loss_1: (0.0297) | Acc_1: (98.98%) (48272/48768)\n",
      "Epoch: 166 | Batch_idx: 390 |  Loss_1: (0.0296) | Acc_1: (98.99%) (49494/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4478) | Acc: (91.57%) (9157/10000)\n",
      "Epoch: 167 | Batch_idx: 0 |  Loss_1: (0.0344) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 167 | Batch_idx: 10 |  Loss_1: (0.0265) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 167 | Batch_idx: 20 |  Loss_1: (0.0222) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 167 | Batch_idx: 30 |  Loss_1: (0.0248) | Acc_1: (99.24%) (3938/3968)\n",
      "Epoch: 167 | Batch_idx: 40 |  Loss_1: (0.0224) | Acc_1: (99.29%) (5211/5248)\n",
      "Epoch: 167 | Batch_idx: 50 |  Loss_1: (0.0210) | Acc_1: (99.33%) (6484/6528)\n",
      "Epoch: 167 | Batch_idx: 60 |  Loss_1: (0.0223) | Acc_1: (99.32%) (7755/7808)\n",
      "Epoch: 167 | Batch_idx: 70 |  Loss_1: (0.0219) | Acc_1: (99.31%) (9025/9088)\n",
      "Epoch: 167 | Batch_idx: 80 |  Loss_1: (0.0228) | Acc_1: (99.27%) (10292/10368)\n",
      "Epoch: 167 | Batch_idx: 90 |  Loss_1: (0.0235) | Acc_1: (99.24%) (11559/11648)\n",
      "Epoch: 167 | Batch_idx: 100 |  Loss_1: (0.0241) | Acc_1: (99.24%) (12830/12928)\n",
      "Epoch: 167 | Batch_idx: 110 |  Loss_1: (0.0244) | Acc_1: (99.23%) (14099/14208)\n",
      "Epoch: 167 | Batch_idx: 120 |  Loss_1: (0.0242) | Acc_1: (99.24%) (15371/15488)\n",
      "Epoch: 167 | Batch_idx: 130 |  Loss_1: (0.0233) | Acc_1: (99.25%) (16643/16768)\n",
      "Epoch: 167 | Batch_idx: 140 |  Loss_1: (0.0242) | Acc_1: (99.20%) (17904/18048)\n",
      "Epoch: 167 | Batch_idx: 150 |  Loss_1: (0.0240) | Acc_1: (99.20%) (19174/19328)\n",
      "Epoch: 167 | Batch_idx: 160 |  Loss_1: (0.0240) | Acc_1: (99.19%) (20441/20608)\n",
      "Epoch: 167 | Batch_idx: 170 |  Loss_1: (0.0244) | Acc_1: (99.16%) (21705/21888)\n",
      "Epoch: 167 | Batch_idx: 180 |  Loss_1: (0.0242) | Acc_1: (99.17%) (22976/23168)\n",
      "Epoch: 167 | Batch_idx: 190 |  Loss_1: (0.0242) | Acc_1: (99.17%) (24245/24448)\n",
      "Epoch: 167 | Batch_idx: 200 |  Loss_1: (0.0246) | Acc_1: (99.15%) (25509/25728)\n",
      "Epoch: 167 | Batch_idx: 210 |  Loss_1: (0.0249) | Acc_1: (99.16%) (26780/27008)\n",
      "Epoch: 167 | Batch_idx: 220 |  Loss_1: (0.0252) | Acc_1: (99.15%) (28047/28288)\n",
      "Epoch: 167 | Batch_idx: 230 |  Loss_1: (0.0251) | Acc_1: (99.15%) (29318/29568)\n",
      "Epoch: 167 | Batch_idx: 240 |  Loss_1: (0.0251) | Acc_1: (99.15%) (30587/30848)\n",
      "Epoch: 167 | Batch_idx: 250 |  Loss_1: (0.0257) | Acc_1: (99.13%) (31850/32128)\n",
      "Epoch: 167 | Batch_idx: 260 |  Loss_1: (0.0257) | Acc_1: (99.13%) (33116/33408)\n",
      "Epoch: 167 | Batch_idx: 270 |  Loss_1: (0.0259) | Acc_1: (99.12%) (34384/34688)\n",
      "Epoch: 167 | Batch_idx: 280 |  Loss_1: (0.0261) | Acc_1: (99.12%) (35651/35968)\n",
      "Epoch: 167 | Batch_idx: 290 |  Loss_1: (0.0260) | Acc_1: (99.12%) (36921/37248)\n",
      "Epoch: 167 | Batch_idx: 300 |  Loss_1: (0.0258) | Acc_1: (99.14%) (38195/38528)\n",
      "Epoch: 167 | Batch_idx: 310 |  Loss_1: (0.0260) | Acc_1: (99.13%) (39460/39808)\n",
      "Epoch: 167 | Batch_idx: 320 |  Loss_1: (0.0262) | Acc_1: (99.12%) (40727/41088)\n",
      "Epoch: 167 | Batch_idx: 330 |  Loss_1: (0.0260) | Acc_1: (99.13%) (41998/42368)\n",
      "Epoch: 167 | Batch_idx: 340 |  Loss_1: (0.0258) | Acc_1: (99.13%) (43268/43648)\n",
      "Epoch: 167 | Batch_idx: 350 |  Loss_1: (0.0255) | Acc_1: (99.13%) (44538/44928)\n",
      "Epoch: 167 | Batch_idx: 360 |  Loss_1: (0.0258) | Acc_1: (99.13%) (45804/46208)\n",
      "Epoch: 167 | Batch_idx: 370 |  Loss_1: (0.0257) | Acc_1: (99.13%) (47076/47488)\n",
      "Epoch: 167 | Batch_idx: 380 |  Loss_1: (0.0257) | Acc_1: (99.13%) (48345/48768)\n",
      "Epoch: 167 | Batch_idx: 390 |  Loss_1: (0.0256) | Acc_1: (99.14%) (49569/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4724) | Acc: (91.48%) (9148/10000)\n",
      "Epoch: 168 | Batch_idx: 0 |  Loss_1: (0.0327) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 168 | Batch_idx: 10 |  Loss_1: (0.0237) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 168 | Batch_idx: 20 |  Loss_1: (0.0282) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 168 | Batch_idx: 30 |  Loss_1: (0.0243) | Acc_1: (99.22%) (3937/3968)\n",
      "Epoch: 168 | Batch_idx: 40 |  Loss_1: (0.0255) | Acc_1: (99.22%) (5207/5248)\n",
      "Epoch: 168 | Batch_idx: 50 |  Loss_1: (0.0228) | Acc_1: (99.33%) (6484/6528)\n",
      "Epoch: 168 | Batch_idx: 60 |  Loss_1: (0.0232) | Acc_1: (99.28%) (7752/7808)\n",
      "Epoch: 168 | Batch_idx: 70 |  Loss_1: (0.0245) | Acc_1: (99.23%) (9018/9088)\n",
      "Epoch: 168 | Batch_idx: 80 |  Loss_1: (0.0246) | Acc_1: (99.23%) (10288/10368)\n",
      "Epoch: 168 | Batch_idx: 90 |  Loss_1: (0.0244) | Acc_1: (99.24%) (11559/11648)\n",
      "Epoch: 168 | Batch_idx: 100 |  Loss_1: (0.0239) | Acc_1: (99.26%) (12832/12928)\n",
      "Epoch: 168 | Batch_idx: 110 |  Loss_1: (0.0253) | Acc_1: (99.24%) (14100/14208)\n",
      "Epoch: 168 | Batch_idx: 120 |  Loss_1: (0.0256) | Acc_1: (99.22%) (15367/15488)\n",
      "Epoch: 168 | Batch_idx: 130 |  Loss_1: (0.0261) | Acc_1: (99.18%) (16630/16768)\n",
      "Epoch: 168 | Batch_idx: 140 |  Loss_1: (0.0270) | Acc_1: (99.14%) (17892/18048)\n",
      "Epoch: 168 | Batch_idx: 150 |  Loss_1: (0.0276) | Acc_1: (99.12%) (19158/19328)\n",
      "Epoch: 168 | Batch_idx: 160 |  Loss_1: (0.0273) | Acc_1: (99.14%) (20430/20608)\n",
      "Epoch: 168 | Batch_idx: 170 |  Loss_1: (0.0270) | Acc_1: (99.15%) (21702/21888)\n",
      "Epoch: 168 | Batch_idx: 180 |  Loss_1: (0.0269) | Acc_1: (99.15%) (22971/23168)\n",
      "Epoch: 168 | Batch_idx: 190 |  Loss_1: (0.0269) | Acc_1: (99.15%) (24239/24448)\n",
      "Epoch: 168 | Batch_idx: 200 |  Loss_1: (0.0268) | Acc_1: (99.15%) (25510/25728)\n",
      "Epoch: 168 | Batch_idx: 210 |  Loss_1: (0.0270) | Acc_1: (99.15%) (26778/27008)\n",
      "Epoch: 168 | Batch_idx: 220 |  Loss_1: (0.0273) | Acc_1: (99.14%) (28045/28288)\n",
      "Epoch: 168 | Batch_idx: 230 |  Loss_1: (0.0272) | Acc_1: (99.14%) (29315/29568)\n",
      "Epoch: 168 | Batch_idx: 240 |  Loss_1: (0.0274) | Acc_1: (99.13%) (30581/30848)\n",
      "Epoch: 168 | Batch_idx: 250 |  Loss_1: (0.0273) | Acc_1: (99.14%) (31851/32128)\n",
      "Epoch: 168 | Batch_idx: 260 |  Loss_1: (0.0274) | Acc_1: (99.13%) (33118/33408)\n",
      "Epoch: 168 | Batch_idx: 270 |  Loss_1: (0.0277) | Acc_1: (99.12%) (34382/34688)\n",
      "Epoch: 168 | Batch_idx: 280 |  Loss_1: (0.0280) | Acc_1: (99.11%) (35648/35968)\n",
      "Epoch: 168 | Batch_idx: 290 |  Loss_1: (0.0281) | Acc_1: (99.11%) (36916/37248)\n",
      "Epoch: 168 | Batch_idx: 300 |  Loss_1: (0.0280) | Acc_1: (99.11%) (38184/38528)\n",
      "Epoch: 168 | Batch_idx: 310 |  Loss_1: (0.0280) | Acc_1: (99.11%) (39452/39808)\n",
      "Epoch: 168 | Batch_idx: 320 |  Loss_1: (0.0279) | Acc_1: (99.11%) (40722/41088)\n",
      "Epoch: 168 | Batch_idx: 330 |  Loss_1: (0.0282) | Acc_1: (99.10%) (41986/42368)\n",
      "Epoch: 168 | Batch_idx: 340 |  Loss_1: (0.0284) | Acc_1: (99.09%) (43252/43648)\n",
      "Epoch: 168 | Batch_idx: 350 |  Loss_1: (0.0283) | Acc_1: (99.10%) (44522/44928)\n",
      "Epoch: 168 | Batch_idx: 360 |  Loss_1: (0.0281) | Acc_1: (99.10%) (45792/46208)\n",
      "Epoch: 168 | Batch_idx: 370 |  Loss_1: (0.0280) | Acc_1: (99.10%) (47061/47488)\n",
      "Epoch: 168 | Batch_idx: 380 |  Loss_1: (0.0280) | Acc_1: (99.10%) (48330/48768)\n",
      "Epoch: 168 | Batch_idx: 390 |  Loss_1: (0.0278) | Acc_1: (99.11%) (49554/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4739) | Acc: (91.30%) (9130/10000)\n",
      "Epoch: 169 | Batch_idx: 0 |  Loss_1: (0.0681) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 169 | Batch_idx: 10 |  Loss_1: (0.0366) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 169 | Batch_idx: 20 |  Loss_1: (0.0342) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 169 | Batch_idx: 30 |  Loss_1: (0.0311) | Acc_1: (99.09%) (3932/3968)\n",
      "Epoch: 169 | Batch_idx: 40 |  Loss_1: (0.0293) | Acc_1: (99.12%) (5202/5248)\n",
      "Epoch: 169 | Batch_idx: 50 |  Loss_1: (0.0306) | Acc_1: (99.07%) (6467/6528)\n",
      "Epoch: 169 | Batch_idx: 60 |  Loss_1: (0.0280) | Acc_1: (99.13%) (7740/7808)\n",
      "Epoch: 169 | Batch_idx: 70 |  Loss_1: (0.0284) | Acc_1: (99.08%) (9004/9088)\n",
      "Epoch: 169 | Batch_idx: 80 |  Loss_1: (0.0271) | Acc_1: (99.11%) (10276/10368)\n",
      "Epoch: 169 | Batch_idx: 90 |  Loss_1: (0.0276) | Acc_1: (99.10%) (11543/11648)\n",
      "Epoch: 169 | Batch_idx: 100 |  Loss_1: (0.0276) | Acc_1: (99.09%) (12811/12928)\n",
      "Epoch: 169 | Batch_idx: 110 |  Loss_1: (0.0275) | Acc_1: (99.09%) (14078/14208)\n",
      "Epoch: 169 | Batch_idx: 120 |  Loss_1: (0.0271) | Acc_1: (99.08%) (15345/15488)\n",
      "Epoch: 169 | Batch_idx: 130 |  Loss_1: (0.0273) | Acc_1: (99.06%) (16610/16768)\n",
      "Epoch: 169 | Batch_idx: 140 |  Loss_1: (0.0268) | Acc_1: (99.07%) (17880/18048)\n",
      "Epoch: 169 | Batch_idx: 150 |  Loss_1: (0.0269) | Acc_1: (99.05%) (19145/19328)\n",
      "Epoch: 169 | Batch_idx: 160 |  Loss_1: (0.0272) | Acc_1: (99.03%) (20409/20608)\n",
      "Epoch: 169 | Batch_idx: 170 |  Loss_1: (0.0276) | Acc_1: (99.00%) (21670/21888)\n",
      "Epoch: 169 | Batch_idx: 180 |  Loss_1: (0.0284) | Acc_1: (98.96%) (22927/23168)\n",
      "Epoch: 169 | Batch_idx: 190 |  Loss_1: (0.0285) | Acc_1: (98.95%) (24192/24448)\n",
      "Epoch: 169 | Batch_idx: 200 |  Loss_1: (0.0293) | Acc_1: (98.94%) (25454/25728)\n",
      "Epoch: 169 | Batch_idx: 210 |  Loss_1: (0.0292) | Acc_1: (98.93%) (26720/27008)\n",
      "Epoch: 169 | Batch_idx: 220 |  Loss_1: (0.0295) | Acc_1: (98.91%) (27981/28288)\n",
      "Epoch: 169 | Batch_idx: 230 |  Loss_1: (0.0303) | Acc_1: (98.90%) (29242/29568)\n",
      "Epoch: 169 | Batch_idx: 240 |  Loss_1: (0.0303) | Acc_1: (98.89%) (30506/30848)\n",
      "Epoch: 169 | Batch_idx: 250 |  Loss_1: (0.0302) | Acc_1: (98.90%) (31774/32128)\n",
      "Epoch: 169 | Batch_idx: 260 |  Loss_1: (0.0297) | Acc_1: (98.92%) (33046/33408)\n",
      "Epoch: 169 | Batch_idx: 270 |  Loss_1: (0.0294) | Acc_1: (98.93%) (34317/34688)\n",
      "Epoch: 169 | Batch_idx: 280 |  Loss_1: (0.0300) | Acc_1: (98.92%) (35578/35968)\n",
      "Epoch: 169 | Batch_idx: 290 |  Loss_1: (0.0300) | Acc_1: (98.92%) (36846/37248)\n",
      "Epoch: 169 | Batch_idx: 300 |  Loss_1: (0.0298) | Acc_1: (98.92%) (38113/38528)\n",
      "Epoch: 169 | Batch_idx: 310 |  Loss_1: (0.0301) | Acc_1: (98.92%) (39379/39808)\n",
      "Epoch: 169 | Batch_idx: 320 |  Loss_1: (0.0299) | Acc_1: (98.92%) (40646/41088)\n",
      "Epoch: 169 | Batch_idx: 330 |  Loss_1: (0.0296) | Acc_1: (98.93%) (41915/42368)\n",
      "Epoch: 169 | Batch_idx: 340 |  Loss_1: (0.0294) | Acc_1: (98.94%) (43187/43648)\n",
      "Epoch: 169 | Batch_idx: 350 |  Loss_1: (0.0296) | Acc_1: (98.94%) (44451/44928)\n",
      "Epoch: 169 | Batch_idx: 360 |  Loss_1: (0.0296) | Acc_1: (98.94%) (45720/46208)\n",
      "Epoch: 169 | Batch_idx: 370 |  Loss_1: (0.0296) | Acc_1: (98.94%) (46983/47488)\n",
      "Epoch: 169 | Batch_idx: 380 |  Loss_1: (0.0297) | Acc_1: (98.93%) (48248/48768)\n",
      "Epoch: 169 | Batch_idx: 390 |  Loss_1: (0.0297) | Acc_1: (98.94%) (49468/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4868) | Acc: (90.95%) (9095/10000)\n",
      "Epoch: 170 | Batch_idx: 0 |  Loss_1: (0.0293) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 170 | Batch_idx: 10 |  Loss_1: (0.0254) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 170 | Batch_idx: 20 |  Loss_1: (0.0229) | Acc_1: (99.18%) (2666/2688)\n",
      "Epoch: 170 | Batch_idx: 30 |  Loss_1: (0.0236) | Acc_1: (99.17%) (3935/3968)\n",
      "Epoch: 170 | Batch_idx: 40 |  Loss_1: (0.0246) | Acc_1: (99.14%) (5203/5248)\n",
      "Epoch: 170 | Batch_idx: 50 |  Loss_1: (0.0248) | Acc_1: (99.14%) (6472/6528)\n",
      "Epoch: 170 | Batch_idx: 60 |  Loss_1: (0.0238) | Acc_1: (99.14%) (7741/7808)\n",
      "Epoch: 170 | Batch_idx: 70 |  Loss_1: (0.0235) | Acc_1: (99.15%) (9011/9088)\n",
      "Epoch: 170 | Batch_idx: 80 |  Loss_1: (0.0246) | Acc_1: (99.12%) (10277/10368)\n",
      "Epoch: 170 | Batch_idx: 90 |  Loss_1: (0.0245) | Acc_1: (99.13%) (11547/11648)\n",
      "Epoch: 170 | Batch_idx: 100 |  Loss_1: (0.0236) | Acc_1: (99.16%) (12820/12928)\n",
      "Epoch: 170 | Batch_idx: 110 |  Loss_1: (0.0237) | Acc_1: (99.14%) (14086/14208)\n",
      "Epoch: 170 | Batch_idx: 120 |  Loss_1: (0.0238) | Acc_1: (99.15%) (15356/15488)\n",
      "Epoch: 170 | Batch_idx: 130 |  Loss_1: (0.0237) | Acc_1: (99.16%) (16627/16768)\n",
      "Epoch: 170 | Batch_idx: 140 |  Loss_1: (0.0246) | Acc_1: (99.14%) (17893/18048)\n",
      "Epoch: 170 | Batch_idx: 150 |  Loss_1: (0.0247) | Acc_1: (99.15%) (19163/19328)\n",
      "Epoch: 170 | Batch_idx: 160 |  Loss_1: (0.0255) | Acc_1: (99.11%) (20425/20608)\n",
      "Epoch: 170 | Batch_idx: 170 |  Loss_1: (0.0251) | Acc_1: (99.14%) (21699/21888)\n",
      "Epoch: 170 | Batch_idx: 180 |  Loss_1: (0.0253) | Acc_1: (99.14%) (22968/23168)\n",
      "Epoch: 170 | Batch_idx: 190 |  Loss_1: (0.0258) | Acc_1: (99.12%) (24232/24448)\n",
      "Epoch: 170 | Batch_idx: 200 |  Loss_1: (0.0256) | Acc_1: (99.13%) (25504/25728)\n",
      "Epoch: 170 | Batch_idx: 210 |  Loss_1: (0.0258) | Acc_1: (99.12%) (26769/27008)\n",
      "Epoch: 170 | Batch_idx: 220 |  Loss_1: (0.0259) | Acc_1: (99.11%) (28037/28288)\n",
      "Epoch: 170 | Batch_idx: 230 |  Loss_1: (0.0259) | Acc_1: (99.11%) (29304/29568)\n",
      "Epoch: 170 | Batch_idx: 240 |  Loss_1: (0.0262) | Acc_1: (99.10%) (30571/30848)\n",
      "Epoch: 170 | Batch_idx: 250 |  Loss_1: (0.0261) | Acc_1: (99.11%) (31841/32128)\n",
      "Epoch: 170 | Batch_idx: 260 |  Loss_1: (0.0262) | Acc_1: (99.10%) (33108/33408)\n",
      "Epoch: 170 | Batch_idx: 270 |  Loss_1: (0.0265) | Acc_1: (99.10%) (34376/34688)\n",
      "Epoch: 170 | Batch_idx: 280 |  Loss_1: (0.0266) | Acc_1: (99.09%) (35640/35968)\n",
      "Epoch: 170 | Batch_idx: 290 |  Loss_1: (0.0272) | Acc_1: (99.06%) (36898/37248)\n",
      "Epoch: 170 | Batch_idx: 300 |  Loss_1: (0.0275) | Acc_1: (99.06%) (38165/38528)\n",
      "Epoch: 170 | Batch_idx: 310 |  Loss_1: (0.0275) | Acc_1: (99.05%) (39428/39808)\n",
      "Epoch: 170 | Batch_idx: 320 |  Loss_1: (0.0275) | Acc_1: (99.04%) (40695/41088)\n",
      "Epoch: 170 | Batch_idx: 330 |  Loss_1: (0.0276) | Acc_1: (99.04%) (41963/42368)\n",
      "Epoch: 170 | Batch_idx: 340 |  Loss_1: (0.0276) | Acc_1: (99.04%) (43229/43648)\n",
      "Epoch: 170 | Batch_idx: 350 |  Loss_1: (0.0276) | Acc_1: (99.05%) (44501/44928)\n",
      "Epoch: 170 | Batch_idx: 360 |  Loss_1: (0.0277) | Acc_1: (99.05%) (45767/46208)\n",
      "Epoch: 170 | Batch_idx: 370 |  Loss_1: (0.0283) | Acc_1: (99.03%) (47025/47488)\n",
      "Epoch: 170 | Batch_idx: 380 |  Loss_1: (0.0284) | Acc_1: (99.02%) (48290/48768)\n",
      "Epoch: 170 | Batch_idx: 390 |  Loss_1: (0.0288) | Acc_1: (99.01%) (49505/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4882) | Acc: (91.26%) (9126/10000)\n",
      "Epoch: 171 | Batch_idx: 0 |  Loss_1: (0.0472) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 171 | Batch_idx: 10 |  Loss_1: (0.0267) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 171 | Batch_idx: 20 |  Loss_1: (0.0229) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 171 | Batch_idx: 30 |  Loss_1: (0.0249) | Acc_1: (99.19%) (3936/3968)\n",
      "Epoch: 171 | Batch_idx: 40 |  Loss_1: (0.0288) | Acc_1: (99.07%) (5199/5248)\n",
      "Epoch: 171 | Batch_idx: 50 |  Loss_1: (0.0272) | Acc_1: (99.07%) (6467/6528)\n",
      "Epoch: 171 | Batch_idx: 60 |  Loss_1: (0.0275) | Acc_1: (99.07%) (7735/7808)\n",
      "Epoch: 171 | Batch_idx: 70 |  Loss_1: (0.0265) | Acc_1: (99.10%) (9006/9088)\n",
      "Epoch: 171 | Batch_idx: 80 |  Loss_1: (0.0270) | Acc_1: (99.08%) (10273/10368)\n",
      "Epoch: 171 | Batch_idx: 90 |  Loss_1: (0.0270) | Acc_1: (99.04%) (11536/11648)\n",
      "Epoch: 171 | Batch_idx: 100 |  Loss_1: (0.0279) | Acc_1: (98.99%) (12797/12928)\n",
      "Epoch: 171 | Batch_idx: 110 |  Loss_1: (0.0280) | Acc_1: (98.99%) (14064/14208)\n",
      "Epoch: 171 | Batch_idx: 120 |  Loss_1: (0.0275) | Acc_1: (99.00%) (15333/15488)\n",
      "Epoch: 171 | Batch_idx: 130 |  Loss_1: (0.0276) | Acc_1: (98.99%) (16598/16768)\n",
      "Epoch: 171 | Batch_idx: 140 |  Loss_1: (0.0274) | Acc_1: (99.01%) (17870/18048)\n",
      "Epoch: 171 | Batch_idx: 150 |  Loss_1: (0.0272) | Acc_1: (99.03%) (19141/19328)\n",
      "Epoch: 171 | Batch_idx: 160 |  Loss_1: (0.0265) | Acc_1: (99.06%) (20414/20608)\n",
      "Epoch: 171 | Batch_idx: 170 |  Loss_1: (0.0263) | Acc_1: (99.08%) (21686/21888)\n",
      "Epoch: 171 | Batch_idx: 180 |  Loss_1: (0.0267) | Acc_1: (99.07%) (22952/23168)\n",
      "Epoch: 171 | Batch_idx: 190 |  Loss_1: (0.0264) | Acc_1: (99.08%) (24224/24448)\n",
      "Epoch: 171 | Batch_idx: 200 |  Loss_1: (0.0265) | Acc_1: (99.07%) (25488/25728)\n",
      "Epoch: 171 | Batch_idx: 210 |  Loss_1: (0.0267) | Acc_1: (99.06%) (26753/27008)\n",
      "Epoch: 171 | Batch_idx: 220 |  Loss_1: (0.0275) | Acc_1: (99.03%) (28015/28288)\n",
      "Epoch: 171 | Batch_idx: 230 |  Loss_1: (0.0276) | Acc_1: (99.04%) (29284/29568)\n",
      "Epoch: 171 | Batch_idx: 240 |  Loss_1: (0.0269) | Acc_1: (99.07%) (30562/30848)\n",
      "Epoch: 171 | Batch_idx: 250 |  Loss_1: (0.0268) | Acc_1: (99.08%) (31831/32128)\n",
      "Epoch: 171 | Batch_idx: 260 |  Loss_1: (0.0265) | Acc_1: (99.09%) (33104/33408)\n",
      "Epoch: 171 | Batch_idx: 270 |  Loss_1: (0.0265) | Acc_1: (99.09%) (34372/34688)\n",
      "Epoch: 171 | Batch_idx: 280 |  Loss_1: (0.0264) | Acc_1: (99.09%) (35642/35968)\n",
      "Epoch: 171 | Batch_idx: 290 |  Loss_1: (0.0263) | Acc_1: (99.10%) (36911/37248)\n",
      "Epoch: 171 | Batch_idx: 300 |  Loss_1: (0.0261) | Acc_1: (99.10%) (38183/38528)\n",
      "Epoch: 171 | Batch_idx: 310 |  Loss_1: (0.0259) | Acc_1: (99.11%) (39455/39808)\n",
      "Epoch: 171 | Batch_idx: 320 |  Loss_1: (0.0261) | Acc_1: (99.11%) (40722/41088)\n",
      "Epoch: 171 | Batch_idx: 330 |  Loss_1: (0.0258) | Acc_1: (99.12%) (41994/42368)\n",
      "Epoch: 171 | Batch_idx: 340 |  Loss_1: (0.0257) | Acc_1: (99.12%) (43265/43648)\n",
      "Epoch: 171 | Batch_idx: 350 |  Loss_1: (0.0254) | Acc_1: (99.13%) (44538/44928)\n",
      "Epoch: 171 | Batch_idx: 360 |  Loss_1: (0.0251) | Acc_1: (99.15%) (45813/46208)\n",
      "Epoch: 171 | Batch_idx: 370 |  Loss_1: (0.0250) | Acc_1: (99.15%) (47083/47488)\n",
      "Epoch: 171 | Batch_idx: 380 |  Loss_1: (0.0251) | Acc_1: (99.14%) (48347/48768)\n",
      "Epoch: 171 | Batch_idx: 390 |  Loss_1: (0.0248) | Acc_1: (99.15%) (49574/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4760) | Acc: (91.23%) (9123/10000)\n",
      "Epoch: 172 | Batch_idx: 0 |  Loss_1: (0.0685) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 172 | Batch_idx: 10 |  Loss_1: (0.0167) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 172 | Batch_idx: 20 |  Loss_1: (0.0183) | Acc_1: (99.37%) (2671/2688)\n",
      "Epoch: 172 | Batch_idx: 30 |  Loss_1: (0.0207) | Acc_1: (99.27%) (3939/3968)\n",
      "Epoch: 172 | Batch_idx: 40 |  Loss_1: (0.0213) | Acc_1: (99.35%) (5214/5248)\n",
      "Epoch: 172 | Batch_idx: 50 |  Loss_1: (0.0224) | Acc_1: (99.34%) (6485/6528)\n",
      "Epoch: 172 | Batch_idx: 60 |  Loss_1: (0.0225) | Acc_1: (99.33%) (7756/7808)\n",
      "Epoch: 172 | Batch_idx: 70 |  Loss_1: (0.0226) | Acc_1: (99.30%) (9024/9088)\n",
      "Epoch: 172 | Batch_idx: 80 |  Loss_1: (0.0219) | Acc_1: (99.31%) (10296/10368)\n",
      "Epoch: 172 | Batch_idx: 90 |  Loss_1: (0.0219) | Acc_1: (99.30%) (11567/11648)\n",
      "Epoch: 172 | Batch_idx: 100 |  Loss_1: (0.0233) | Acc_1: (99.24%) (12830/12928)\n",
      "Epoch: 172 | Batch_idx: 110 |  Loss_1: (0.0231) | Acc_1: (99.28%) (14105/14208)\n",
      "Epoch: 172 | Batch_idx: 120 |  Loss_1: (0.0247) | Acc_1: (99.24%) (15370/15488)\n",
      "Epoch: 172 | Batch_idx: 130 |  Loss_1: (0.0242) | Acc_1: (99.25%) (16642/16768)\n",
      "Epoch: 172 | Batch_idx: 140 |  Loss_1: (0.0251) | Acc_1: (99.22%) (17908/18048)\n",
      "Epoch: 172 | Batch_idx: 150 |  Loss_1: (0.0249) | Acc_1: (99.22%) (19177/19328)\n",
      "Epoch: 172 | Batch_idx: 160 |  Loss_1: (0.0256) | Acc_1: (99.21%) (20445/20608)\n",
      "Epoch: 172 | Batch_idx: 170 |  Loss_1: (0.0257) | Acc_1: (99.20%) (21712/21888)\n",
      "Epoch: 172 | Batch_idx: 180 |  Loss_1: (0.0252) | Acc_1: (99.21%) (22984/23168)\n",
      "Epoch: 172 | Batch_idx: 190 |  Loss_1: (0.0249) | Acc_1: (99.21%) (24255/24448)\n",
      "Epoch: 172 | Batch_idx: 200 |  Loss_1: (0.0249) | Acc_1: (99.20%) (25521/25728)\n",
      "Epoch: 172 | Batch_idx: 210 |  Loss_1: (0.0257) | Acc_1: (99.16%) (26782/27008)\n",
      "Epoch: 172 | Batch_idx: 220 |  Loss_1: (0.0261) | Acc_1: (99.14%) (28045/28288)\n",
      "Epoch: 172 | Batch_idx: 230 |  Loss_1: (0.0259) | Acc_1: (99.14%) (29313/29568)\n",
      "Epoch: 172 | Batch_idx: 240 |  Loss_1: (0.0260) | Acc_1: (99.13%) (30581/30848)\n",
      "Epoch: 172 | Batch_idx: 250 |  Loss_1: (0.0262) | Acc_1: (99.13%) (31847/32128)\n",
      "Epoch: 172 | Batch_idx: 260 |  Loss_1: (0.0263) | Acc_1: (99.12%) (33115/33408)\n",
      "Epoch: 172 | Batch_idx: 270 |  Loss_1: (0.0262) | Acc_1: (99.13%) (34386/34688)\n",
      "Epoch: 172 | Batch_idx: 280 |  Loss_1: (0.0262) | Acc_1: (99.11%) (35649/35968)\n",
      "Epoch: 172 | Batch_idx: 290 |  Loss_1: (0.0268) | Acc_1: (99.10%) (36911/37248)\n",
      "Epoch: 172 | Batch_idx: 300 |  Loss_1: (0.0273) | Acc_1: (99.09%) (38178/38528)\n",
      "Epoch: 172 | Batch_idx: 310 |  Loss_1: (0.0271) | Acc_1: (99.09%) (39447/39808)\n",
      "Epoch: 172 | Batch_idx: 320 |  Loss_1: (0.0271) | Acc_1: (99.09%) (40714/41088)\n",
      "Epoch: 172 | Batch_idx: 330 |  Loss_1: (0.0270) | Acc_1: (99.10%) (41985/42368)\n",
      "Epoch: 172 | Batch_idx: 340 |  Loss_1: (0.0270) | Acc_1: (99.10%) (43253/43648)\n",
      "Epoch: 172 | Batch_idx: 350 |  Loss_1: (0.0272) | Acc_1: (99.09%) (44519/44928)\n",
      "Epoch: 172 | Batch_idx: 360 |  Loss_1: (0.0272) | Acc_1: (99.08%) (45784/46208)\n",
      "Epoch: 172 | Batch_idx: 370 |  Loss_1: (0.0278) | Acc_1: (99.06%) (47041/47488)\n",
      "Epoch: 172 | Batch_idx: 380 |  Loss_1: (0.0279) | Acc_1: (99.06%) (48309/48768)\n",
      "Epoch: 172 | Batch_idx: 390 |  Loss_1: (0.0279) | Acc_1: (99.06%) (49531/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4781) | Acc: (90.96%) (9096/10000)\n",
      "Epoch: 173 | Batch_idx: 0 |  Loss_1: (0.0097) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 173 | Batch_idx: 10 |  Loss_1: (0.0195) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 173 | Batch_idx: 20 |  Loss_1: (0.0193) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 173 | Batch_idx: 30 |  Loss_1: (0.0218) | Acc_1: (99.29%) (3940/3968)\n",
      "Epoch: 173 | Batch_idx: 40 |  Loss_1: (0.0231) | Acc_1: (99.28%) (5210/5248)\n",
      "Epoch: 173 | Batch_idx: 50 |  Loss_1: (0.0253) | Acc_1: (99.25%) (6479/6528)\n",
      "Epoch: 173 | Batch_idx: 60 |  Loss_1: (0.0252) | Acc_1: (99.19%) (7745/7808)\n",
      "Epoch: 173 | Batch_idx: 70 |  Loss_1: (0.0262) | Acc_1: (99.21%) (9016/9088)\n",
      "Epoch: 173 | Batch_idx: 80 |  Loss_1: (0.0252) | Acc_1: (99.24%) (10289/10368)\n",
      "Epoch: 173 | Batch_idx: 90 |  Loss_1: (0.0259) | Acc_1: (99.19%) (11554/11648)\n",
      "Epoch: 173 | Batch_idx: 100 |  Loss_1: (0.0251) | Acc_1: (99.23%) (12828/12928)\n",
      "Epoch: 173 | Batch_idx: 110 |  Loss_1: (0.0247) | Acc_1: (99.20%) (14094/14208)\n",
      "Epoch: 173 | Batch_idx: 120 |  Loss_1: (0.0240) | Acc_1: (99.24%) (15370/15488)\n",
      "Epoch: 173 | Batch_idx: 130 |  Loss_1: (0.0244) | Acc_1: (99.24%) (16641/16768)\n",
      "Epoch: 173 | Batch_idx: 140 |  Loss_1: (0.0243) | Acc_1: (99.25%) (17913/18048)\n",
      "Epoch: 173 | Batch_idx: 150 |  Loss_1: (0.0237) | Acc_1: (99.27%) (19186/19328)\n",
      "Epoch: 173 | Batch_idx: 160 |  Loss_1: (0.0232) | Acc_1: (99.26%) (20456/20608)\n",
      "Epoch: 173 | Batch_idx: 170 |  Loss_1: (0.0236) | Acc_1: (99.25%) (21723/21888)\n",
      "Epoch: 173 | Batch_idx: 180 |  Loss_1: (0.0233) | Acc_1: (99.25%) (22995/23168)\n",
      "Epoch: 173 | Batch_idx: 190 |  Loss_1: (0.0236) | Acc_1: (99.23%) (24260/24448)\n",
      "Epoch: 173 | Batch_idx: 200 |  Loss_1: (0.0236) | Acc_1: (99.23%) (25530/25728)\n",
      "Epoch: 173 | Batch_idx: 210 |  Loss_1: (0.0238) | Acc_1: (99.21%) (26794/27008)\n",
      "Epoch: 173 | Batch_idx: 220 |  Loss_1: (0.0237) | Acc_1: (99.22%) (28066/28288)\n",
      "Epoch: 173 | Batch_idx: 230 |  Loss_1: (0.0235) | Acc_1: (99.23%) (29339/29568)\n",
      "Epoch: 173 | Batch_idx: 240 |  Loss_1: (0.0238) | Acc_1: (99.21%) (30604/30848)\n",
      "Epoch: 173 | Batch_idx: 250 |  Loss_1: (0.0239) | Acc_1: (99.21%) (31873/32128)\n",
      "Epoch: 173 | Batch_idx: 260 |  Loss_1: (0.0240) | Acc_1: (99.21%) (33144/33408)\n",
      "Epoch: 173 | Batch_idx: 270 |  Loss_1: (0.0240) | Acc_1: (99.21%) (34415/34688)\n",
      "Epoch: 173 | Batch_idx: 280 |  Loss_1: (0.0238) | Acc_1: (99.22%) (35687/35968)\n",
      "Epoch: 173 | Batch_idx: 290 |  Loss_1: (0.0238) | Acc_1: (99.21%) (36955/37248)\n",
      "Epoch: 173 | Batch_idx: 300 |  Loss_1: (0.0237) | Acc_1: (99.21%) (38225/38528)\n",
      "Epoch: 173 | Batch_idx: 310 |  Loss_1: (0.0237) | Acc_1: (99.21%) (39493/39808)\n",
      "Epoch: 173 | Batch_idx: 320 |  Loss_1: (0.0239) | Acc_1: (99.21%) (40762/41088)\n",
      "Epoch: 173 | Batch_idx: 330 |  Loss_1: (0.0242) | Acc_1: (99.20%) (42029/42368)\n",
      "Epoch: 173 | Batch_idx: 340 |  Loss_1: (0.0239) | Acc_1: (99.21%) (43303/43648)\n",
      "Epoch: 173 | Batch_idx: 350 |  Loss_1: (0.0239) | Acc_1: (99.21%) (44573/44928)\n",
      "Epoch: 173 | Batch_idx: 360 |  Loss_1: (0.0242) | Acc_1: (99.20%) (45838/46208)\n",
      "Epoch: 173 | Batch_idx: 370 |  Loss_1: (0.0244) | Acc_1: (99.20%) (47107/47488)\n",
      "Epoch: 173 | Batch_idx: 380 |  Loss_1: (0.0249) | Acc_1: (99.18%) (48367/48768)\n",
      "Epoch: 173 | Batch_idx: 390 |  Loss_1: (0.0248) | Acc_1: (99.18%) (49590/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4737) | Acc: (90.99%) (9099/10000)\n",
      "Epoch: 174 | Batch_idx: 0 |  Loss_1: (0.0162) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 174 | Batch_idx: 10 |  Loss_1: (0.0331) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 174 | Batch_idx: 20 |  Loss_1: (0.0285) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 174 | Batch_idx: 30 |  Loss_1: (0.0278) | Acc_1: (99.04%) (3930/3968)\n",
      "Epoch: 174 | Batch_idx: 40 |  Loss_1: (0.0251) | Acc_1: (99.10%) (5201/5248)\n",
      "Epoch: 174 | Batch_idx: 50 |  Loss_1: (0.0274) | Acc_1: (99.03%) (6465/6528)\n",
      "Epoch: 174 | Batch_idx: 60 |  Loss_1: (0.0269) | Acc_1: (99.04%) (7733/7808)\n",
      "Epoch: 174 | Batch_idx: 70 |  Loss_1: (0.0266) | Acc_1: (99.08%) (9004/9088)\n",
      "Epoch: 174 | Batch_idx: 80 |  Loss_1: (0.0263) | Acc_1: (99.06%) (10271/10368)\n",
      "Epoch: 174 | Batch_idx: 90 |  Loss_1: (0.0259) | Acc_1: (99.09%) (11542/11648)\n",
      "Epoch: 174 | Batch_idx: 100 |  Loss_1: (0.0272) | Acc_1: (99.03%) (12803/12928)\n",
      "Epoch: 174 | Batch_idx: 110 |  Loss_1: (0.0264) | Acc_1: (99.06%) (14075/14208)\n",
      "Epoch: 174 | Batch_idx: 120 |  Loss_1: (0.0261) | Acc_1: (99.08%) (15346/15488)\n",
      "Epoch: 174 | Batch_idx: 130 |  Loss_1: (0.0271) | Acc_1: (99.04%) (16607/16768)\n",
      "Epoch: 174 | Batch_idx: 140 |  Loss_1: (0.0269) | Acc_1: (99.05%) (17877/18048)\n",
      "Epoch: 174 | Batch_idx: 150 |  Loss_1: (0.0265) | Acc_1: (99.07%) (19149/19328)\n",
      "Epoch: 174 | Batch_idx: 160 |  Loss_1: (0.0265) | Acc_1: (99.07%) (20416/20608)\n",
      "Epoch: 174 | Batch_idx: 170 |  Loss_1: (0.0271) | Acc_1: (99.05%) (21679/21888)\n",
      "Epoch: 174 | Batch_idx: 180 |  Loss_1: (0.0271) | Acc_1: (99.04%) (22945/23168)\n",
      "Epoch: 174 | Batch_idx: 190 |  Loss_1: (0.0272) | Acc_1: (99.04%) (24214/24448)\n",
      "Epoch: 174 | Batch_idx: 200 |  Loss_1: (0.0273) | Acc_1: (99.02%) (25476/25728)\n",
      "Epoch: 174 | Batch_idx: 210 |  Loss_1: (0.0268) | Acc_1: (99.04%) (26749/27008)\n",
      "Epoch: 174 | Batch_idx: 220 |  Loss_1: (0.0267) | Acc_1: (99.04%) (28016/28288)\n",
      "Epoch: 174 | Batch_idx: 230 |  Loss_1: (0.0267) | Acc_1: (99.05%) (29287/29568)\n",
      "Epoch: 174 | Batch_idx: 240 |  Loss_1: (0.0267) | Acc_1: (99.05%) (30554/30848)\n",
      "Epoch: 174 | Batch_idx: 250 |  Loss_1: (0.0271) | Acc_1: (99.04%) (31821/32128)\n",
      "Epoch: 174 | Batch_idx: 260 |  Loss_1: (0.0269) | Acc_1: (99.05%) (33092/33408)\n",
      "Epoch: 174 | Batch_idx: 270 |  Loss_1: (0.0267) | Acc_1: (99.06%) (34362/34688)\n",
      "Epoch: 174 | Batch_idx: 280 |  Loss_1: (0.0263) | Acc_1: (99.08%) (35636/35968)\n",
      "Epoch: 174 | Batch_idx: 290 |  Loss_1: (0.0261) | Acc_1: (99.08%) (36907/37248)\n",
      "Epoch: 174 | Batch_idx: 300 |  Loss_1: (0.0262) | Acc_1: (99.07%) (38170/38528)\n",
      "Epoch: 174 | Batch_idx: 310 |  Loss_1: (0.0260) | Acc_1: (99.08%) (39442/39808)\n",
      "Epoch: 174 | Batch_idx: 320 |  Loss_1: (0.0264) | Acc_1: (99.06%) (40703/41088)\n",
      "Epoch: 174 | Batch_idx: 330 |  Loss_1: (0.0265) | Acc_1: (99.06%) (41971/42368)\n",
      "Epoch: 174 | Batch_idx: 340 |  Loss_1: (0.0267) | Acc_1: (99.06%) (43236/43648)\n",
      "Epoch: 174 | Batch_idx: 350 |  Loss_1: (0.0266) | Acc_1: (99.06%) (44506/44928)\n",
      "Epoch: 174 | Batch_idx: 360 |  Loss_1: (0.0265) | Acc_1: (99.06%) (45775/46208)\n",
      "Epoch: 174 | Batch_idx: 370 |  Loss_1: (0.0264) | Acc_1: (99.06%) (47043/47488)\n",
      "Epoch: 174 | Batch_idx: 380 |  Loss_1: (0.0263) | Acc_1: (99.07%) (48314/48768)\n",
      "Epoch: 174 | Batch_idx: 390 |  Loss_1: (0.0263) | Acc_1: (99.07%) (49535/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4694) | Acc: (91.53%) (9153/10000)\n",
      "Epoch: 175 | Batch_idx: 0 |  Loss_1: (0.0350) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 175 | Batch_idx: 10 |  Loss_1: (0.0304) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 175 | Batch_idx: 20 |  Loss_1: (0.0242) | Acc_1: (99.03%) (2662/2688)\n",
      "Epoch: 175 | Batch_idx: 30 |  Loss_1: (0.0235) | Acc_1: (99.04%) (3930/3968)\n",
      "Epoch: 175 | Batch_idx: 40 |  Loss_1: (0.0272) | Acc_1: (98.99%) (5195/5248)\n",
      "Epoch: 175 | Batch_idx: 50 |  Loss_1: (0.0270) | Acc_1: (99.02%) (6464/6528)\n",
      "Epoch: 175 | Batch_idx: 60 |  Loss_1: (0.0275) | Acc_1: (99.04%) (7733/7808)\n",
      "Epoch: 175 | Batch_idx: 70 |  Loss_1: (0.0279) | Acc_1: (99.06%) (9003/9088)\n",
      "Epoch: 175 | Batch_idx: 80 |  Loss_1: (0.0281) | Acc_1: (99.07%) (10272/10368)\n",
      "Epoch: 175 | Batch_idx: 90 |  Loss_1: (0.0276) | Acc_1: (99.10%) (11543/11648)\n",
      "Epoch: 175 | Batch_idx: 100 |  Loss_1: (0.0274) | Acc_1: (99.09%) (12810/12928)\n",
      "Epoch: 175 | Batch_idx: 110 |  Loss_1: (0.0271) | Acc_1: (99.09%) (14079/14208)\n",
      "Epoch: 175 | Batch_idx: 120 |  Loss_1: (0.0271) | Acc_1: (99.09%) (15347/15488)\n",
      "Epoch: 175 | Batch_idx: 130 |  Loss_1: (0.0271) | Acc_1: (99.10%) (16617/16768)\n",
      "Epoch: 175 | Batch_idx: 140 |  Loss_1: (0.0268) | Acc_1: (99.11%) (17887/18048)\n",
      "Epoch: 175 | Batch_idx: 150 |  Loss_1: (0.0273) | Acc_1: (99.08%) (19151/19328)\n",
      "Epoch: 175 | Batch_idx: 160 |  Loss_1: (0.0282) | Acc_1: (99.07%) (20416/20608)\n",
      "Epoch: 175 | Batch_idx: 170 |  Loss_1: (0.0281) | Acc_1: (99.08%) (21686/21888)\n",
      "Epoch: 175 | Batch_idx: 180 |  Loss_1: (0.0286) | Acc_1: (99.07%) (22952/23168)\n",
      "Epoch: 175 | Batch_idx: 190 |  Loss_1: (0.0286) | Acc_1: (99.06%) (24217/24448)\n",
      "Epoch: 175 | Batch_idx: 200 |  Loss_1: (0.0289) | Acc_1: (99.05%) (25483/25728)\n",
      "Epoch: 175 | Batch_idx: 210 |  Loss_1: (0.0286) | Acc_1: (99.06%) (26755/27008)\n",
      "Epoch: 175 | Batch_idx: 220 |  Loss_1: (0.0285) | Acc_1: (99.07%) (28026/28288)\n",
      "Epoch: 175 | Batch_idx: 230 |  Loss_1: (0.0283) | Acc_1: (99.08%) (29295/29568)\n",
      "Epoch: 175 | Batch_idx: 240 |  Loss_1: (0.0284) | Acc_1: (99.07%) (30562/30848)\n",
      "Epoch: 175 | Batch_idx: 250 |  Loss_1: (0.0280) | Acc_1: (99.08%) (31832/32128)\n",
      "Epoch: 175 | Batch_idx: 260 |  Loss_1: (0.0277) | Acc_1: (99.10%) (33107/33408)\n",
      "Epoch: 175 | Batch_idx: 270 |  Loss_1: (0.0278) | Acc_1: (99.09%) (34373/34688)\n",
      "Epoch: 175 | Batch_idx: 280 |  Loss_1: (0.0277) | Acc_1: (99.10%) (35644/35968)\n",
      "Epoch: 175 | Batch_idx: 290 |  Loss_1: (0.0280) | Acc_1: (99.09%) (36908/37248)\n",
      "Epoch: 175 | Batch_idx: 300 |  Loss_1: (0.0282) | Acc_1: (99.08%) (38172/38528)\n",
      "Epoch: 175 | Batch_idx: 310 |  Loss_1: (0.0278) | Acc_1: (99.08%) (39443/39808)\n",
      "Epoch: 175 | Batch_idx: 320 |  Loss_1: (0.0273) | Acc_1: (99.10%) (40718/41088)\n",
      "Epoch: 175 | Batch_idx: 330 |  Loss_1: (0.0271) | Acc_1: (99.11%) (41991/42368)\n",
      "Epoch: 175 | Batch_idx: 340 |  Loss_1: (0.0274) | Acc_1: (99.10%) (43255/43648)\n",
      "Epoch: 175 | Batch_idx: 350 |  Loss_1: (0.0275) | Acc_1: (99.11%) (44526/44928)\n",
      "Epoch: 175 | Batch_idx: 360 |  Loss_1: (0.0272) | Acc_1: (99.12%) (45800/46208)\n",
      "Epoch: 175 | Batch_idx: 370 |  Loss_1: (0.0271) | Acc_1: (99.11%) (47067/47488)\n",
      "Epoch: 175 | Batch_idx: 380 |  Loss_1: (0.0270) | Acc_1: (99.13%) (48342/48768)\n",
      "Epoch: 175 | Batch_idx: 390 |  Loss_1: (0.0270) | Acc_1: (99.13%) (49563/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4742) | Acc: (91.26%) (9126/10000)\n",
      "Epoch: 176 | Batch_idx: 0 |  Loss_1: (0.0313) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 176 | Batch_idx: 10 |  Loss_1: (0.0190) | Acc_1: (99.57%) (1402/1408)\n",
      "Epoch: 176 | Batch_idx: 20 |  Loss_1: (0.0240) | Acc_1: (99.29%) (2669/2688)\n",
      "Epoch: 176 | Batch_idx: 30 |  Loss_1: (0.0263) | Acc_1: (99.19%) (3936/3968)\n",
      "Epoch: 176 | Batch_idx: 40 |  Loss_1: (0.0259) | Acc_1: (99.16%) (5204/5248)\n",
      "Epoch: 176 | Batch_idx: 50 |  Loss_1: (0.0265) | Acc_1: (99.13%) (6471/6528)\n",
      "Epoch: 176 | Batch_idx: 60 |  Loss_1: (0.0252) | Acc_1: (99.12%) (7739/7808)\n",
      "Epoch: 176 | Batch_idx: 70 |  Loss_1: (0.0261) | Acc_1: (99.13%) (9009/9088)\n",
      "Epoch: 176 | Batch_idx: 80 |  Loss_1: (0.0271) | Acc_1: (99.08%) (10273/10368)\n",
      "Epoch: 176 | Batch_idx: 90 |  Loss_1: (0.0273) | Acc_1: (99.10%) (11543/11648)\n",
      "Epoch: 176 | Batch_idx: 100 |  Loss_1: (0.0281) | Acc_1: (99.05%) (12805/12928)\n",
      "Epoch: 176 | Batch_idx: 110 |  Loss_1: (0.0274) | Acc_1: (99.05%) (14073/14208)\n",
      "Epoch: 176 | Batch_idx: 120 |  Loss_1: (0.0278) | Acc_1: (99.04%) (15339/15488)\n",
      "Epoch: 176 | Batch_idx: 130 |  Loss_1: (0.0262) | Acc_1: (99.09%) (16616/16768)\n",
      "Epoch: 176 | Batch_idx: 140 |  Loss_1: (0.0270) | Acc_1: (99.07%) (17881/18048)\n",
      "Epoch: 176 | Batch_idx: 150 |  Loss_1: (0.0271) | Acc_1: (99.05%) (19145/19328)\n",
      "Epoch: 176 | Batch_idx: 160 |  Loss_1: (0.0268) | Acc_1: (99.06%) (20415/20608)\n",
      "Epoch: 176 | Batch_idx: 170 |  Loss_1: (0.0268) | Acc_1: (99.05%) (21681/21888)\n",
      "Epoch: 176 | Batch_idx: 180 |  Loss_1: (0.0266) | Acc_1: (99.06%) (22951/23168)\n",
      "Epoch: 176 | Batch_idx: 190 |  Loss_1: (0.0263) | Acc_1: (99.08%) (24222/24448)\n",
      "Epoch: 176 | Batch_idx: 200 |  Loss_1: (0.0259) | Acc_1: (99.10%) (25497/25728)\n",
      "Epoch: 176 | Batch_idx: 210 |  Loss_1: (0.0263) | Acc_1: (99.10%) (26765/27008)\n",
      "Epoch: 176 | Batch_idx: 220 |  Loss_1: (0.0262) | Acc_1: (99.11%) (28035/28288)\n",
      "Epoch: 176 | Batch_idx: 230 |  Loss_1: (0.0259) | Acc_1: (99.11%) (29304/29568)\n",
      "Epoch: 176 | Batch_idx: 240 |  Loss_1: (0.0263) | Acc_1: (99.10%) (30569/30848)\n",
      "Epoch: 176 | Batch_idx: 250 |  Loss_1: (0.0260) | Acc_1: (99.10%) (31840/32128)\n",
      "Epoch: 176 | Batch_idx: 260 |  Loss_1: (0.0258) | Acc_1: (99.11%) (33110/33408)\n",
      "Epoch: 176 | Batch_idx: 270 |  Loss_1: (0.0258) | Acc_1: (99.10%) (34376/34688)\n",
      "Epoch: 176 | Batch_idx: 280 |  Loss_1: (0.0256) | Acc_1: (99.10%) (35646/35968)\n",
      "Epoch: 176 | Batch_idx: 290 |  Loss_1: (0.0261) | Acc_1: (99.10%) (36911/37248)\n",
      "Epoch: 176 | Batch_idx: 300 |  Loss_1: (0.0259) | Acc_1: (99.10%) (38182/38528)\n",
      "Epoch: 176 | Batch_idx: 310 |  Loss_1: (0.0256) | Acc_1: (99.12%) (39456/39808)\n",
      "Epoch: 176 | Batch_idx: 320 |  Loss_1: (0.0257) | Acc_1: (99.11%) (40722/41088)\n",
      "Epoch: 176 | Batch_idx: 330 |  Loss_1: (0.0257) | Acc_1: (99.10%) (41988/42368)\n",
      "Epoch: 176 | Batch_idx: 340 |  Loss_1: (0.0253) | Acc_1: (99.12%) (43264/43648)\n",
      "Epoch: 176 | Batch_idx: 350 |  Loss_1: (0.0257) | Acc_1: (99.10%) (44525/44928)\n",
      "Epoch: 176 | Batch_idx: 360 |  Loss_1: (0.0255) | Acc_1: (99.11%) (45798/46208)\n",
      "Epoch: 176 | Batch_idx: 370 |  Loss_1: (0.0257) | Acc_1: (99.11%) (47064/47488)\n",
      "Epoch: 176 | Batch_idx: 380 |  Loss_1: (0.0259) | Acc_1: (99.11%) (48334/48768)\n",
      "Epoch: 176 | Batch_idx: 390 |  Loss_1: (0.0257) | Acc_1: (99.12%) (49559/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4909) | Acc: (91.07%) (9107/10000)\n",
      "Epoch: 177 | Batch_idx: 0 |  Loss_1: (0.0737) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 177 | Batch_idx: 10 |  Loss_1: (0.0221) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 177 | Batch_idx: 20 |  Loss_1: (0.0270) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 177 | Batch_idx: 30 |  Loss_1: (0.0283) | Acc_1: (99.04%) (3930/3968)\n",
      "Epoch: 177 | Batch_idx: 40 |  Loss_1: (0.0277) | Acc_1: (99.01%) (5196/5248)\n",
      "Epoch: 177 | Batch_idx: 50 |  Loss_1: (0.0277) | Acc_1: (99.00%) (6463/6528)\n",
      "Epoch: 177 | Batch_idx: 60 |  Loss_1: (0.0293) | Acc_1: (98.99%) (7729/7808)\n",
      "Epoch: 177 | Batch_idx: 70 |  Loss_1: (0.0300) | Acc_1: (98.97%) (8994/9088)\n",
      "Epoch: 177 | Batch_idx: 80 |  Loss_1: (0.0289) | Acc_1: (99.02%) (10266/10368)\n",
      "Epoch: 177 | Batch_idx: 90 |  Loss_1: (0.0293) | Acc_1: (99.00%) (11532/11648)\n",
      "Epoch: 177 | Batch_idx: 100 |  Loss_1: (0.0290) | Acc_1: (99.00%) (12799/12928)\n",
      "Epoch: 177 | Batch_idx: 110 |  Loss_1: (0.0286) | Acc_1: (98.99%) (14064/14208)\n",
      "Epoch: 177 | Batch_idx: 120 |  Loss_1: (0.0276) | Acc_1: (99.03%) (15337/15488)\n",
      "Epoch: 177 | Batch_idx: 130 |  Loss_1: (0.0269) | Acc_1: (99.03%) (16605/16768)\n",
      "Epoch: 177 | Batch_idx: 140 |  Loss_1: (0.0272) | Acc_1: (99.04%) (17874/18048)\n",
      "Epoch: 177 | Batch_idx: 150 |  Loss_1: (0.0272) | Acc_1: (99.02%) (19139/19328)\n",
      "Epoch: 177 | Batch_idx: 160 |  Loss_1: (0.0269) | Acc_1: (99.03%) (20409/20608)\n",
      "Epoch: 177 | Batch_idx: 170 |  Loss_1: (0.0269) | Acc_1: (99.04%) (21677/21888)\n",
      "Epoch: 177 | Batch_idx: 180 |  Loss_1: (0.0272) | Acc_1: (99.04%) (22946/23168)\n",
      "Epoch: 177 | Batch_idx: 190 |  Loss_1: (0.0274) | Acc_1: (99.03%) (24210/24448)\n",
      "Epoch: 177 | Batch_idx: 200 |  Loss_1: (0.0279) | Acc_1: (99.03%) (25479/25728)\n",
      "Epoch: 177 | Batch_idx: 210 |  Loss_1: (0.0279) | Acc_1: (99.04%) (26749/27008)\n",
      "Epoch: 177 | Batch_idx: 220 |  Loss_1: (0.0281) | Acc_1: (99.05%) (28018/28288)\n",
      "Epoch: 177 | Batch_idx: 230 |  Loss_1: (0.0275) | Acc_1: (99.07%) (29292/29568)\n",
      "Epoch: 177 | Batch_idx: 240 |  Loss_1: (0.0276) | Acc_1: (99.06%) (30559/30848)\n",
      "Epoch: 177 | Batch_idx: 250 |  Loss_1: (0.0272) | Acc_1: (99.07%) (31829/32128)\n",
      "Epoch: 177 | Batch_idx: 260 |  Loss_1: (0.0269) | Acc_1: (99.09%) (33105/33408)\n",
      "Epoch: 177 | Batch_idx: 270 |  Loss_1: (0.0265) | Acc_1: (99.10%) (34377/34688)\n",
      "Epoch: 177 | Batch_idx: 280 |  Loss_1: (0.0270) | Acc_1: (99.09%) (35639/35968)\n",
      "Epoch: 177 | Batch_idx: 290 |  Loss_1: (0.0269) | Acc_1: (99.09%) (36910/37248)\n",
      "Epoch: 177 | Batch_idx: 300 |  Loss_1: (0.0272) | Acc_1: (99.09%) (38179/38528)\n",
      "Epoch: 177 | Batch_idx: 310 |  Loss_1: (0.0270) | Acc_1: (99.10%) (39451/39808)\n",
      "Epoch: 177 | Batch_idx: 320 |  Loss_1: (0.0270) | Acc_1: (99.10%) (40720/41088)\n",
      "Epoch: 177 | Batch_idx: 330 |  Loss_1: (0.0267) | Acc_1: (99.12%) (41995/42368)\n",
      "Epoch: 177 | Batch_idx: 340 |  Loss_1: (0.0266) | Acc_1: (99.12%) (43265/43648)\n",
      "Epoch: 177 | Batch_idx: 350 |  Loss_1: (0.0265) | Acc_1: (99.12%) (44533/44928)\n",
      "Epoch: 177 | Batch_idx: 360 |  Loss_1: (0.0267) | Acc_1: (99.11%) (45796/46208)\n",
      "Epoch: 177 | Batch_idx: 370 |  Loss_1: (0.0266) | Acc_1: (99.11%) (47066/47488)\n",
      "Epoch: 177 | Batch_idx: 380 |  Loss_1: (0.0264) | Acc_1: (99.11%) (48336/48768)\n",
      "Epoch: 177 | Batch_idx: 390 |  Loss_1: (0.0262) | Acc_1: (99.12%) (49558/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4757) | Acc: (91.36%) (9136/10000)\n",
      "Epoch: 178 | Batch_idx: 0 |  Loss_1: (0.0212) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 178 | Batch_idx: 10 |  Loss_1: (0.0318) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 178 | Batch_idx: 20 |  Loss_1: (0.0266) | Acc_1: (98.96%) (2660/2688)\n",
      "Epoch: 178 | Batch_idx: 30 |  Loss_1: (0.0255) | Acc_1: (99.12%) (3933/3968)\n",
      "Epoch: 178 | Batch_idx: 40 |  Loss_1: (0.0255) | Acc_1: (99.07%) (5199/5248)\n",
      "Epoch: 178 | Batch_idx: 50 |  Loss_1: (0.0269) | Acc_1: (98.97%) (6461/6528)\n",
      "Epoch: 178 | Batch_idx: 60 |  Loss_1: (0.0258) | Acc_1: (99.04%) (7733/7808)\n",
      "Epoch: 178 | Batch_idx: 70 |  Loss_1: (0.0262) | Acc_1: (99.01%) (8998/9088)\n",
      "Epoch: 178 | Batch_idx: 80 |  Loss_1: (0.0270) | Acc_1: (99.03%) (10267/10368)\n",
      "Epoch: 178 | Batch_idx: 90 |  Loss_1: (0.0270) | Acc_1: (99.02%) (11534/11648)\n",
      "Epoch: 178 | Batch_idx: 100 |  Loss_1: (0.0267) | Acc_1: (99.03%) (12802/12928)\n",
      "Epoch: 178 | Batch_idx: 110 |  Loss_1: (0.0269) | Acc_1: (99.00%) (14066/14208)\n",
      "Epoch: 178 | Batch_idx: 120 |  Loss_1: (0.0268) | Acc_1: (99.01%) (15335/15488)\n",
      "Epoch: 178 | Batch_idx: 130 |  Loss_1: (0.0266) | Acc_1: (99.01%) (16602/16768)\n",
      "Epoch: 178 | Batch_idx: 140 |  Loss_1: (0.0258) | Acc_1: (99.04%) (17874/18048)\n",
      "Epoch: 178 | Batch_idx: 150 |  Loss_1: (0.0260) | Acc_1: (99.04%) (19142/19328)\n",
      "Epoch: 178 | Batch_idx: 160 |  Loss_1: (0.0259) | Acc_1: (99.04%) (20411/20608)\n",
      "Epoch: 178 | Batch_idx: 170 |  Loss_1: (0.0263) | Acc_1: (99.03%) (21675/21888)\n",
      "Epoch: 178 | Batch_idx: 180 |  Loss_1: (0.0258) | Acc_1: (99.04%) (22946/23168)\n",
      "Epoch: 178 | Batch_idx: 190 |  Loss_1: (0.0256) | Acc_1: (99.05%) (24216/24448)\n",
      "Epoch: 178 | Batch_idx: 200 |  Loss_1: (0.0259) | Acc_1: (99.04%) (25482/25728)\n",
      "Epoch: 178 | Batch_idx: 210 |  Loss_1: (0.0257) | Acc_1: (99.05%) (26752/27008)\n",
      "Epoch: 178 | Batch_idx: 220 |  Loss_1: (0.0255) | Acc_1: (99.07%) (28024/28288)\n",
      "Epoch: 178 | Batch_idx: 230 |  Loss_1: (0.0255) | Acc_1: (99.07%) (29293/29568)\n",
      "Epoch: 178 | Batch_idx: 240 |  Loss_1: (0.0255) | Acc_1: (99.08%) (30564/30848)\n",
      "Epoch: 178 | Batch_idx: 250 |  Loss_1: (0.0257) | Acc_1: (99.07%) (31830/32128)\n",
      "Epoch: 178 | Batch_idx: 260 |  Loss_1: (0.0259) | Acc_1: (99.08%) (33100/33408)\n",
      "Epoch: 178 | Batch_idx: 270 |  Loss_1: (0.0262) | Acc_1: (99.07%) (34366/34688)\n",
      "Epoch: 178 | Batch_idx: 280 |  Loss_1: (0.0261) | Acc_1: (99.08%) (35636/35968)\n",
      "Epoch: 178 | Batch_idx: 290 |  Loss_1: (0.0265) | Acc_1: (99.06%) (36898/37248)\n",
      "Epoch: 178 | Batch_idx: 300 |  Loss_1: (0.0263) | Acc_1: (99.07%) (38169/38528)\n",
      "Epoch: 178 | Batch_idx: 310 |  Loss_1: (0.0262) | Acc_1: (99.07%) (39439/39808)\n",
      "Epoch: 178 | Batch_idx: 320 |  Loss_1: (0.0263) | Acc_1: (99.07%) (40705/41088)\n",
      "Epoch: 178 | Batch_idx: 330 |  Loss_1: (0.0265) | Acc_1: (99.07%) (41974/42368)\n",
      "Epoch: 178 | Batch_idx: 340 |  Loss_1: (0.0267) | Acc_1: (99.06%) (43238/43648)\n",
      "Epoch: 178 | Batch_idx: 350 |  Loss_1: (0.0268) | Acc_1: (99.05%) (44502/44928)\n",
      "Epoch: 178 | Batch_idx: 360 |  Loss_1: (0.0266) | Acc_1: (99.05%) (45769/46208)\n",
      "Epoch: 178 | Batch_idx: 370 |  Loss_1: (0.0266) | Acc_1: (99.05%) (47037/47488)\n",
      "Epoch: 178 | Batch_idx: 380 |  Loss_1: (0.0267) | Acc_1: (99.05%) (48305/48768)\n",
      "Epoch: 178 | Batch_idx: 390 |  Loss_1: (0.0271) | Acc_1: (99.04%) (49521/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5395) | Acc: (90.87%) (9087/10000)\n",
      "Epoch: 179 | Batch_idx: 0 |  Loss_1: (0.0277) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 179 | Batch_idx: 10 |  Loss_1: (0.0235) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 179 | Batch_idx: 20 |  Loss_1: (0.0257) | Acc_1: (99.14%) (2665/2688)\n",
      "Epoch: 179 | Batch_idx: 30 |  Loss_1: (0.0259) | Acc_1: (99.07%) (3931/3968)\n",
      "Epoch: 179 | Batch_idx: 40 |  Loss_1: (0.0249) | Acc_1: (99.12%) (5202/5248)\n",
      "Epoch: 179 | Batch_idx: 50 |  Loss_1: (0.0255) | Acc_1: (99.13%) (6471/6528)\n",
      "Epoch: 179 | Batch_idx: 60 |  Loss_1: (0.0279) | Acc_1: (99.09%) (7737/7808)\n",
      "Epoch: 179 | Batch_idx: 70 |  Loss_1: (0.0289) | Acc_1: (99.06%) (9003/9088)\n",
      "Epoch: 179 | Batch_idx: 80 |  Loss_1: (0.0289) | Acc_1: (99.03%) (10267/10368)\n",
      "Epoch: 179 | Batch_idx: 90 |  Loss_1: (0.0289) | Acc_1: (99.02%) (11534/11648)\n",
      "Epoch: 179 | Batch_idx: 100 |  Loss_1: (0.0299) | Acc_1: (99.00%) (12799/12928)\n",
      "Epoch: 179 | Batch_idx: 110 |  Loss_1: (0.0297) | Acc_1: (99.00%) (14066/14208)\n",
      "Epoch: 179 | Batch_idx: 120 |  Loss_1: (0.0302) | Acc_1: (98.99%) (15331/15488)\n",
      "Epoch: 179 | Batch_idx: 130 |  Loss_1: (0.0304) | Acc_1: (99.00%) (16600/16768)\n",
      "Epoch: 179 | Batch_idx: 140 |  Loss_1: (0.0311) | Acc_1: (99.00%) (17868/18048)\n",
      "Epoch: 179 | Batch_idx: 150 |  Loss_1: (0.0305) | Acc_1: (99.01%) (19137/19328)\n",
      "Epoch: 179 | Batch_idx: 160 |  Loss_1: (0.0299) | Acc_1: (99.01%) (20404/20608)\n",
      "Epoch: 179 | Batch_idx: 170 |  Loss_1: (0.0304) | Acc_1: (99.02%) (21673/21888)\n",
      "Epoch: 179 | Batch_idx: 180 |  Loss_1: (0.0302) | Acc_1: (99.03%) (22944/23168)\n",
      "Epoch: 179 | Batch_idx: 190 |  Loss_1: (0.0303) | Acc_1: (99.03%) (24210/24448)\n",
      "Epoch: 179 | Batch_idx: 200 |  Loss_1: (0.0297) | Acc_1: (99.05%) (25483/25728)\n",
      "Epoch: 179 | Batch_idx: 210 |  Loss_1: (0.0297) | Acc_1: (99.03%) (26747/27008)\n",
      "Epoch: 179 | Batch_idx: 220 |  Loss_1: (0.0294) | Acc_1: (99.05%) (28018/28288)\n",
      "Epoch: 179 | Batch_idx: 230 |  Loss_1: (0.0289) | Acc_1: (99.06%) (29291/29568)\n",
      "Epoch: 179 | Batch_idx: 240 |  Loss_1: (0.0286) | Acc_1: (99.07%) (30562/30848)\n",
      "Epoch: 179 | Batch_idx: 250 |  Loss_1: (0.0283) | Acc_1: (99.08%) (31832/32128)\n",
      "Epoch: 179 | Batch_idx: 260 |  Loss_1: (0.0287) | Acc_1: (99.07%) (33097/33408)\n",
      "Epoch: 179 | Batch_idx: 270 |  Loss_1: (0.0285) | Acc_1: (99.08%) (34368/34688)\n",
      "Epoch: 179 | Batch_idx: 280 |  Loss_1: (0.0283) | Acc_1: (99.09%) (35640/35968)\n",
      "Epoch: 179 | Batch_idx: 290 |  Loss_1: (0.0285) | Acc_1: (99.08%) (36906/37248)\n",
      "Epoch: 179 | Batch_idx: 300 |  Loss_1: (0.0282) | Acc_1: (99.09%) (38176/38528)\n",
      "Epoch: 179 | Batch_idx: 310 |  Loss_1: (0.0281) | Acc_1: (99.09%) (39446/39808)\n",
      "Epoch: 179 | Batch_idx: 320 |  Loss_1: (0.0280) | Acc_1: (99.09%) (40714/41088)\n",
      "Epoch: 179 | Batch_idx: 330 |  Loss_1: (0.0281) | Acc_1: (99.09%) (41983/42368)\n",
      "Epoch: 179 | Batch_idx: 340 |  Loss_1: (0.0278) | Acc_1: (99.10%) (43256/43648)\n",
      "Epoch: 179 | Batch_idx: 350 |  Loss_1: (0.0275) | Acc_1: (99.10%) (44524/44928)\n",
      "Epoch: 179 | Batch_idx: 360 |  Loss_1: (0.0273) | Acc_1: (99.11%) (45796/46208)\n",
      "Epoch: 179 | Batch_idx: 370 |  Loss_1: (0.0274) | Acc_1: (99.10%) (47059/47488)\n",
      "Epoch: 179 | Batch_idx: 380 |  Loss_1: (0.0275) | Acc_1: (99.10%) (48327/48768)\n",
      "Epoch: 179 | Batch_idx: 390 |  Loss_1: (0.0272) | Acc_1: (99.11%) (49553/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4740) | Acc: (91.29%) (9129/10000)\n",
      "Epoch: 180 | Batch_idx: 0 |  Loss_1: (0.0521) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 180 | Batch_idx: 10 |  Loss_1: (0.0273) | Acc_1: (98.93%) (1393/1408)\n",
      "Epoch: 180 | Batch_idx: 20 |  Loss_1: (0.0214) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 180 | Batch_idx: 30 |  Loss_1: (0.0265) | Acc_1: (99.17%) (3935/3968)\n",
      "Epoch: 180 | Batch_idx: 40 |  Loss_1: (0.0257) | Acc_1: (99.12%) (5202/5248)\n",
      "Epoch: 180 | Batch_idx: 50 |  Loss_1: (0.0250) | Acc_1: (99.16%) (6473/6528)\n",
      "Epoch: 180 | Batch_idx: 60 |  Loss_1: (0.0254) | Acc_1: (99.12%) (7739/7808)\n",
      "Epoch: 180 | Batch_idx: 70 |  Loss_1: (0.0251) | Acc_1: (99.12%) (9008/9088)\n",
      "Epoch: 180 | Batch_idx: 80 |  Loss_1: (0.0252) | Acc_1: (99.14%) (10279/10368)\n",
      "Epoch: 180 | Batch_idx: 90 |  Loss_1: (0.0253) | Acc_1: (99.16%) (11550/11648)\n",
      "Epoch: 180 | Batch_idx: 100 |  Loss_1: (0.0253) | Acc_1: (99.16%) (12820/12928)\n",
      "Epoch: 180 | Batch_idx: 110 |  Loss_1: (0.0255) | Acc_1: (99.15%) (14087/14208)\n",
      "Epoch: 180 | Batch_idx: 120 |  Loss_1: (0.0252) | Acc_1: (99.13%) (15354/15488)\n",
      "Epoch: 180 | Batch_idx: 130 |  Loss_1: (0.0251) | Acc_1: (99.14%) (16624/16768)\n",
      "Epoch: 180 | Batch_idx: 140 |  Loss_1: (0.0250) | Acc_1: (99.12%) (17890/18048)\n",
      "Epoch: 180 | Batch_idx: 150 |  Loss_1: (0.0248) | Acc_1: (99.14%) (19161/19328)\n",
      "Epoch: 180 | Batch_idx: 160 |  Loss_1: (0.0248) | Acc_1: (99.15%) (20432/20608)\n",
      "Epoch: 180 | Batch_idx: 170 |  Loss_1: (0.0254) | Acc_1: (99.13%) (21698/21888)\n",
      "Epoch: 180 | Batch_idx: 180 |  Loss_1: (0.0259) | Acc_1: (99.12%) (22965/23168)\n",
      "Epoch: 180 | Batch_idx: 190 |  Loss_1: (0.0261) | Acc_1: (99.10%) (24228/24448)\n",
      "Epoch: 180 | Batch_idx: 200 |  Loss_1: (0.0265) | Acc_1: (99.08%) (25492/25728)\n",
      "Epoch: 180 | Batch_idx: 210 |  Loss_1: (0.0273) | Acc_1: (99.04%) (26750/27008)\n",
      "Epoch: 180 | Batch_idx: 220 |  Loss_1: (0.0273) | Acc_1: (99.02%) (28012/28288)\n",
      "Epoch: 180 | Batch_idx: 230 |  Loss_1: (0.0271) | Acc_1: (99.03%) (29281/29568)\n",
      "Epoch: 180 | Batch_idx: 240 |  Loss_1: (0.0267) | Acc_1: (99.03%) (30549/30848)\n",
      "Epoch: 180 | Batch_idx: 250 |  Loss_1: (0.0270) | Acc_1: (99.03%) (31815/32128)\n",
      "Epoch: 180 | Batch_idx: 260 |  Loss_1: (0.0266) | Acc_1: (99.05%) (33090/33408)\n",
      "Epoch: 180 | Batch_idx: 270 |  Loss_1: (0.0266) | Acc_1: (99.05%) (34359/34688)\n",
      "Epoch: 180 | Batch_idx: 280 |  Loss_1: (0.0271) | Acc_1: (99.05%) (35627/35968)\n",
      "Epoch: 180 | Batch_idx: 290 |  Loss_1: (0.0271) | Acc_1: (99.05%) (36896/37248)\n",
      "Epoch: 180 | Batch_idx: 300 |  Loss_1: (0.0271) | Acc_1: (99.05%) (38162/38528)\n",
      "Epoch: 180 | Batch_idx: 310 |  Loss_1: (0.0274) | Acc_1: (99.05%) (39428/39808)\n",
      "Epoch: 180 | Batch_idx: 320 |  Loss_1: (0.0276) | Acc_1: (99.05%) (40696/41088)\n",
      "Epoch: 180 | Batch_idx: 330 |  Loss_1: (0.0272) | Acc_1: (99.06%) (41969/42368)\n",
      "Epoch: 180 | Batch_idx: 340 |  Loss_1: (0.0270) | Acc_1: (99.06%) (43237/43648)\n",
      "Epoch: 180 | Batch_idx: 350 |  Loss_1: (0.0271) | Acc_1: (99.06%) (44505/44928)\n",
      "Epoch: 180 | Batch_idx: 360 |  Loss_1: (0.0270) | Acc_1: (99.07%) (45776/46208)\n",
      "Epoch: 180 | Batch_idx: 370 |  Loss_1: (0.0273) | Acc_1: (99.06%) (47041/47488)\n",
      "Epoch: 180 | Batch_idx: 380 |  Loss_1: (0.0273) | Acc_1: (99.06%) (48308/48768)\n",
      "Epoch: 180 | Batch_idx: 390 |  Loss_1: (0.0273) | Acc_1: (99.06%) (49531/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4787) | Acc: (91.01%) (9101/10000)\n",
      "Epoch: 181 | Batch_idx: 0 |  Loss_1: (0.0103) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 181 | Batch_idx: 10 |  Loss_1: (0.0236) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 181 | Batch_idx: 20 |  Loss_1: (0.0226) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 181 | Batch_idx: 30 |  Loss_1: (0.0231) | Acc_1: (99.37%) (3943/3968)\n",
      "Epoch: 181 | Batch_idx: 40 |  Loss_1: (0.0264) | Acc_1: (99.18%) (5205/5248)\n",
      "Epoch: 181 | Batch_idx: 50 |  Loss_1: (0.0260) | Acc_1: (99.16%) (6473/6528)\n",
      "Epoch: 181 | Batch_idx: 60 |  Loss_1: (0.0249) | Acc_1: (99.17%) (7743/7808)\n",
      "Epoch: 181 | Batch_idx: 70 |  Loss_1: (0.0240) | Acc_1: (99.19%) (9014/9088)\n",
      "Epoch: 181 | Batch_idx: 80 |  Loss_1: (0.0242) | Acc_1: (99.20%) (10285/10368)\n",
      "Epoch: 181 | Batch_idx: 90 |  Loss_1: (0.0255) | Acc_1: (99.12%) (11546/11648)\n",
      "Epoch: 181 | Batch_idx: 100 |  Loss_1: (0.0264) | Acc_1: (99.11%) (12813/12928)\n",
      "Epoch: 181 | Batch_idx: 110 |  Loss_1: (0.0259) | Acc_1: (99.11%) (14081/14208)\n",
      "Epoch: 181 | Batch_idx: 120 |  Loss_1: (0.0259) | Acc_1: (99.10%) (15349/15488)\n",
      "Epoch: 181 | Batch_idx: 130 |  Loss_1: (0.0261) | Acc_1: (99.10%) (16617/16768)\n",
      "Epoch: 181 | Batch_idx: 140 |  Loss_1: (0.0253) | Acc_1: (99.12%) (17890/18048)\n",
      "Epoch: 181 | Batch_idx: 150 |  Loss_1: (0.0255) | Acc_1: (99.09%) (19153/19328)\n",
      "Epoch: 181 | Batch_idx: 160 |  Loss_1: (0.0251) | Acc_1: (99.11%) (20424/20608)\n",
      "Epoch: 181 | Batch_idx: 170 |  Loss_1: (0.0250) | Acc_1: (99.09%) (21688/21888)\n",
      "Epoch: 181 | Batch_idx: 180 |  Loss_1: (0.0254) | Acc_1: (99.06%) (22951/23168)\n",
      "Epoch: 181 | Batch_idx: 190 |  Loss_1: (0.0253) | Acc_1: (99.07%) (24220/24448)\n",
      "Epoch: 181 | Batch_idx: 200 |  Loss_1: (0.0252) | Acc_1: (99.06%) (25487/25728)\n",
      "Epoch: 181 | Batch_idx: 210 |  Loss_1: (0.0259) | Acc_1: (99.03%) (26747/27008)\n",
      "Epoch: 181 | Batch_idx: 220 |  Loss_1: (0.0260) | Acc_1: (99.03%) (28014/28288)\n",
      "Epoch: 181 | Batch_idx: 230 |  Loss_1: (0.0260) | Acc_1: (99.02%) (29278/29568)\n",
      "Epoch: 181 | Batch_idx: 240 |  Loss_1: (0.0258) | Acc_1: (99.03%) (30550/30848)\n",
      "Epoch: 181 | Batch_idx: 250 |  Loss_1: (0.0259) | Acc_1: (99.04%) (31819/32128)\n",
      "Epoch: 181 | Batch_idx: 260 |  Loss_1: (0.0257) | Acc_1: (99.05%) (33090/33408)\n",
      "Epoch: 181 | Batch_idx: 270 |  Loss_1: (0.0256) | Acc_1: (99.05%) (34358/34688)\n",
      "Epoch: 181 | Batch_idx: 280 |  Loss_1: (0.0256) | Acc_1: (99.04%) (35622/35968)\n",
      "Epoch: 181 | Batch_idx: 290 |  Loss_1: (0.0257) | Acc_1: (99.04%) (36891/37248)\n",
      "Epoch: 181 | Batch_idx: 300 |  Loss_1: (0.0258) | Acc_1: (99.03%) (38156/38528)\n",
      "Epoch: 181 | Batch_idx: 310 |  Loss_1: (0.0261) | Acc_1: (99.03%) (39423/39808)\n",
      "Epoch: 181 | Batch_idx: 320 |  Loss_1: (0.0260) | Acc_1: (99.04%) (40693/41088)\n",
      "Epoch: 181 | Batch_idx: 330 |  Loss_1: (0.0259) | Acc_1: (99.04%) (41963/42368)\n",
      "Epoch: 181 | Batch_idx: 340 |  Loss_1: (0.0259) | Acc_1: (99.04%) (43231/43648)\n",
      "Epoch: 181 | Batch_idx: 350 |  Loss_1: (0.0258) | Acc_1: (99.05%) (44500/44928)\n",
      "Epoch: 181 | Batch_idx: 360 |  Loss_1: (0.0258) | Acc_1: (99.06%) (45773/46208)\n",
      "Epoch: 181 | Batch_idx: 370 |  Loss_1: (0.0260) | Acc_1: (99.06%) (47042/47488)\n",
      "Epoch: 181 | Batch_idx: 380 |  Loss_1: (0.0258) | Acc_1: (99.07%) (48313/48768)\n",
      "Epoch: 181 | Batch_idx: 390 |  Loss_1: (0.0257) | Acc_1: (99.07%) (49536/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4725) | Acc: (91.74%) (9174/10000)\n",
      "Epoch: 182 | Batch_idx: 0 |  Loss_1: (0.0187) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 182 | Batch_idx: 10 |  Loss_1: (0.0202) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 182 | Batch_idx: 20 |  Loss_1: (0.0281) | Acc_1: (99.07%) (2663/2688)\n",
      "Epoch: 182 | Batch_idx: 30 |  Loss_1: (0.0284) | Acc_1: (99.02%) (3929/3968)\n",
      "Epoch: 182 | Batch_idx: 40 |  Loss_1: (0.0260) | Acc_1: (99.14%) (5203/5248)\n",
      "Epoch: 182 | Batch_idx: 50 |  Loss_1: (0.0280) | Acc_1: (99.03%) (6465/6528)\n",
      "Epoch: 182 | Batch_idx: 60 |  Loss_1: (0.0283) | Acc_1: (98.98%) (7728/7808)\n",
      "Epoch: 182 | Batch_idx: 70 |  Loss_1: (0.0278) | Acc_1: (99.00%) (8997/9088)\n",
      "Epoch: 182 | Batch_idx: 80 |  Loss_1: (0.0275) | Acc_1: (98.99%) (10263/10368)\n",
      "Epoch: 182 | Batch_idx: 90 |  Loss_1: (0.0280) | Acc_1: (98.95%) (11526/11648)\n",
      "Epoch: 182 | Batch_idx: 100 |  Loss_1: (0.0277) | Acc_1: (98.98%) (12796/12928)\n",
      "Epoch: 182 | Batch_idx: 110 |  Loss_1: (0.0287) | Acc_1: (98.97%) (14062/14208)\n",
      "Epoch: 182 | Batch_idx: 120 |  Loss_1: (0.0286) | Acc_1: (98.99%) (15331/15488)\n",
      "Epoch: 182 | Batch_idx: 130 |  Loss_1: (0.0280) | Acc_1: (99.02%) (16604/16768)\n",
      "Epoch: 182 | Batch_idx: 140 |  Loss_1: (0.0286) | Acc_1: (99.01%) (17869/18048)\n",
      "Epoch: 182 | Batch_idx: 150 |  Loss_1: (0.0290) | Acc_1: (98.99%) (19132/19328)\n",
      "Epoch: 182 | Batch_idx: 160 |  Loss_1: (0.0297) | Acc_1: (99.00%) (20401/20608)\n",
      "Epoch: 182 | Batch_idx: 170 |  Loss_1: (0.0297) | Acc_1: (98.99%) (21667/21888)\n",
      "Epoch: 182 | Batch_idx: 180 |  Loss_1: (0.0294) | Acc_1: (99.02%) (22940/23168)\n",
      "Epoch: 182 | Batch_idx: 190 |  Loss_1: (0.0288) | Acc_1: (99.03%) (24212/24448)\n",
      "Epoch: 182 | Batch_idx: 200 |  Loss_1: (0.0284) | Acc_1: (99.05%) (25484/25728)\n",
      "Epoch: 182 | Batch_idx: 210 |  Loss_1: (0.0280) | Acc_1: (99.06%) (26755/27008)\n",
      "Epoch: 182 | Batch_idx: 220 |  Loss_1: (0.0277) | Acc_1: (99.07%) (28025/28288)\n",
      "Epoch: 182 | Batch_idx: 230 |  Loss_1: (0.0275) | Acc_1: (99.07%) (29293/29568)\n",
      "Epoch: 182 | Batch_idx: 240 |  Loss_1: (0.0273) | Acc_1: (99.08%) (30565/30848)\n",
      "Epoch: 182 | Batch_idx: 250 |  Loss_1: (0.0279) | Acc_1: (99.08%) (31832/32128)\n",
      "Epoch: 182 | Batch_idx: 260 |  Loss_1: (0.0280) | Acc_1: (99.07%) (33098/33408)\n",
      "Epoch: 182 | Batch_idx: 270 |  Loss_1: (0.0280) | Acc_1: (99.07%) (34367/34688)\n",
      "Epoch: 182 | Batch_idx: 280 |  Loss_1: (0.0279) | Acc_1: (99.08%) (35638/35968)\n",
      "Epoch: 182 | Batch_idx: 290 |  Loss_1: (0.0282) | Acc_1: (99.07%) (36901/37248)\n",
      "Epoch: 182 | Batch_idx: 300 |  Loss_1: (0.0285) | Acc_1: (99.07%) (38168/38528)\n",
      "Epoch: 182 | Batch_idx: 310 |  Loss_1: (0.0288) | Acc_1: (99.06%) (39433/39808)\n",
      "Epoch: 182 | Batch_idx: 320 |  Loss_1: (0.0284) | Acc_1: (99.07%) (40706/41088)\n",
      "Epoch: 182 | Batch_idx: 330 |  Loss_1: (0.0282) | Acc_1: (99.07%) (41974/42368)\n",
      "Epoch: 182 | Batch_idx: 340 |  Loss_1: (0.0281) | Acc_1: (99.07%) (43241/43648)\n",
      "Epoch: 182 | Batch_idx: 350 |  Loss_1: (0.0280) | Acc_1: (99.07%) (44511/44928)\n",
      "Epoch: 182 | Batch_idx: 360 |  Loss_1: (0.0281) | Acc_1: (99.07%) (45779/46208)\n",
      "Epoch: 182 | Batch_idx: 370 |  Loss_1: (0.0278) | Acc_1: (99.08%) (47052/47488)\n",
      "Epoch: 182 | Batch_idx: 380 |  Loss_1: (0.0277) | Acc_1: (99.08%) (48320/48768)\n",
      "Epoch: 182 | Batch_idx: 390 |  Loss_1: (0.0275) | Acc_1: (99.09%) (49544/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4738) | Acc: (91.51%) (9151/10000)\n",
      "Epoch: 183 | Batch_idx: 0 |  Loss_1: (0.0055) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 183 | Batch_idx: 10 |  Loss_1: (0.0151) | Acc_1: (99.64%) (1403/1408)\n",
      "Epoch: 183 | Batch_idx: 20 |  Loss_1: (0.0189) | Acc_1: (99.55%) (2676/2688)\n",
      "Epoch: 183 | Batch_idx: 30 |  Loss_1: (0.0186) | Acc_1: (99.47%) (3947/3968)\n",
      "Epoch: 183 | Batch_idx: 40 |  Loss_1: (0.0215) | Acc_1: (99.33%) (5213/5248)\n",
      "Epoch: 183 | Batch_idx: 50 |  Loss_1: (0.0217) | Acc_1: (99.34%) (6485/6528)\n",
      "Epoch: 183 | Batch_idx: 60 |  Loss_1: (0.0226) | Acc_1: (99.27%) (7751/7808)\n",
      "Epoch: 183 | Batch_idx: 70 |  Loss_1: (0.0242) | Acc_1: (99.26%) (9021/9088)\n",
      "Epoch: 183 | Batch_idx: 80 |  Loss_1: (0.0239) | Acc_1: (99.27%) (10292/10368)\n",
      "Epoch: 183 | Batch_idx: 90 |  Loss_1: (0.0248) | Acc_1: (99.22%) (11557/11648)\n",
      "Epoch: 183 | Batch_idx: 100 |  Loss_1: (0.0253) | Acc_1: (99.20%) (12825/12928)\n",
      "Epoch: 183 | Batch_idx: 110 |  Loss_1: (0.0245) | Acc_1: (99.21%) (14096/14208)\n",
      "Epoch: 183 | Batch_idx: 120 |  Loss_1: (0.0244) | Acc_1: (99.20%) (15364/15488)\n",
      "Epoch: 183 | Batch_idx: 130 |  Loss_1: (0.0251) | Acc_1: (99.16%) (16627/16768)\n",
      "Epoch: 183 | Batch_idx: 140 |  Loss_1: (0.0256) | Acc_1: (99.15%) (17894/18048)\n",
      "Epoch: 183 | Batch_idx: 150 |  Loss_1: (0.0258) | Acc_1: (99.16%) (19166/19328)\n",
      "Epoch: 183 | Batch_idx: 160 |  Loss_1: (0.0256) | Acc_1: (99.17%) (20436/20608)\n",
      "Epoch: 183 | Batch_idx: 170 |  Loss_1: (0.0254) | Acc_1: (99.16%) (21705/21888)\n",
      "Epoch: 183 | Batch_idx: 180 |  Loss_1: (0.0254) | Acc_1: (99.16%) (22974/23168)\n",
      "Epoch: 183 | Batch_idx: 190 |  Loss_1: (0.0249) | Acc_1: (99.18%) (24247/24448)\n",
      "Epoch: 183 | Batch_idx: 200 |  Loss_1: (0.0246) | Acc_1: (99.20%) (25522/25728)\n",
      "Epoch: 183 | Batch_idx: 210 |  Loss_1: (0.0245) | Acc_1: (99.20%) (26792/27008)\n",
      "Epoch: 183 | Batch_idx: 220 |  Loss_1: (0.0243) | Acc_1: (99.20%) (28061/28288)\n",
      "Epoch: 183 | Batch_idx: 230 |  Loss_1: (0.0240) | Acc_1: (99.20%) (29332/29568)\n",
      "Epoch: 183 | Batch_idx: 240 |  Loss_1: (0.0239) | Acc_1: (99.20%) (30602/30848)\n",
      "Epoch: 183 | Batch_idx: 250 |  Loss_1: (0.0236) | Acc_1: (99.20%) (31870/32128)\n",
      "Epoch: 183 | Batch_idx: 260 |  Loss_1: (0.0239) | Acc_1: (99.18%) (33135/33408)\n",
      "Epoch: 183 | Batch_idx: 270 |  Loss_1: (0.0240) | Acc_1: (99.18%) (34403/34688)\n",
      "Epoch: 183 | Batch_idx: 280 |  Loss_1: (0.0236) | Acc_1: (99.19%) (35677/35968)\n",
      "Epoch: 183 | Batch_idx: 290 |  Loss_1: (0.0239) | Acc_1: (99.19%) (36946/37248)\n",
      "Epoch: 183 | Batch_idx: 300 |  Loss_1: (0.0239) | Acc_1: (99.19%) (38215/38528)\n",
      "Epoch: 183 | Batch_idx: 310 |  Loss_1: (0.0245) | Acc_1: (99.17%) (39479/39808)\n",
      "Epoch: 183 | Batch_idx: 320 |  Loss_1: (0.0242) | Acc_1: (99.18%) (40752/41088)\n",
      "Epoch: 183 | Batch_idx: 330 |  Loss_1: (0.0240) | Acc_1: (99.19%) (42024/42368)\n",
      "Epoch: 183 | Batch_idx: 340 |  Loss_1: (0.0240) | Acc_1: (99.19%) (43295/43648)\n",
      "Epoch: 183 | Batch_idx: 350 |  Loss_1: (0.0242) | Acc_1: (99.19%) (44562/44928)\n",
      "Epoch: 183 | Batch_idx: 360 |  Loss_1: (0.0240) | Acc_1: (99.19%) (45832/46208)\n",
      "Epoch: 183 | Batch_idx: 370 |  Loss_1: (0.0239) | Acc_1: (99.19%) (47102/47488)\n",
      "Epoch: 183 | Batch_idx: 380 |  Loss_1: (0.0242) | Acc_1: (99.17%) (48363/48768)\n",
      "Epoch: 183 | Batch_idx: 390 |  Loss_1: (0.0241) | Acc_1: (99.17%) (49587/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4571) | Acc: (91.03%) (9103/10000)\n",
      "Epoch: 184 | Batch_idx: 0 |  Loss_1: (0.0130) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 184 | Batch_idx: 10 |  Loss_1: (0.0198) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 184 | Batch_idx: 20 |  Loss_1: (0.0215) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 184 | Batch_idx: 30 |  Loss_1: (0.0259) | Acc_1: (99.27%) (3939/3968)\n",
      "Epoch: 184 | Batch_idx: 40 |  Loss_1: (0.0255) | Acc_1: (99.33%) (5213/5248)\n",
      "Epoch: 184 | Batch_idx: 50 |  Loss_1: (0.0263) | Acc_1: (99.28%) (6481/6528)\n",
      "Epoch: 184 | Batch_idx: 60 |  Loss_1: (0.0248) | Acc_1: (99.32%) (7755/7808)\n",
      "Epoch: 184 | Batch_idx: 70 |  Loss_1: (0.0249) | Acc_1: (99.27%) (9022/9088)\n",
      "Epoch: 184 | Batch_idx: 80 |  Loss_1: (0.0252) | Acc_1: (99.23%) (10288/10368)\n",
      "Epoch: 184 | Batch_idx: 90 |  Loss_1: (0.0253) | Acc_1: (99.23%) (11558/11648)\n",
      "Epoch: 184 | Batch_idx: 100 |  Loss_1: (0.0254) | Acc_1: (99.22%) (12827/12928)\n",
      "Epoch: 184 | Batch_idx: 110 |  Loss_1: (0.0259) | Acc_1: (99.21%) (14096/14208)\n",
      "Epoch: 184 | Batch_idx: 120 |  Loss_1: (0.0255) | Acc_1: (99.22%) (15367/15488)\n",
      "Epoch: 184 | Batch_idx: 130 |  Loss_1: (0.0252) | Acc_1: (99.22%) (16638/16768)\n",
      "Epoch: 184 | Batch_idx: 140 |  Loss_1: (0.0252) | Acc_1: (99.22%) (17908/18048)\n",
      "Epoch: 184 | Batch_idx: 150 |  Loss_1: (0.0256) | Acc_1: (99.21%) (19176/19328)\n",
      "Epoch: 184 | Batch_idx: 160 |  Loss_1: (0.0257) | Acc_1: (99.19%) (20442/20608)\n",
      "Epoch: 184 | Batch_idx: 170 |  Loss_1: (0.0263) | Acc_1: (99.17%) (21706/21888)\n",
      "Epoch: 184 | Batch_idx: 180 |  Loss_1: (0.0260) | Acc_1: (99.18%) (22978/23168)\n",
      "Epoch: 184 | Batch_idx: 190 |  Loss_1: (0.0268) | Acc_1: (99.14%) (24237/24448)\n",
      "Epoch: 184 | Batch_idx: 200 |  Loss_1: (0.0262) | Acc_1: (99.16%) (25511/25728)\n",
      "Epoch: 184 | Batch_idx: 210 |  Loss_1: (0.0267) | Acc_1: (99.13%) (26773/27008)\n",
      "Epoch: 184 | Batch_idx: 220 |  Loss_1: (0.0264) | Acc_1: (99.14%) (28045/28288)\n",
      "Epoch: 184 | Batch_idx: 230 |  Loss_1: (0.0261) | Acc_1: (99.14%) (29314/29568)\n",
      "Epoch: 184 | Batch_idx: 240 |  Loss_1: (0.0265) | Acc_1: (99.13%) (30580/30848)\n",
      "Epoch: 184 | Batch_idx: 250 |  Loss_1: (0.0263) | Acc_1: (99.15%) (31854/32128)\n",
      "Epoch: 184 | Batch_idx: 260 |  Loss_1: (0.0265) | Acc_1: (99.13%) (33119/33408)\n",
      "Epoch: 184 | Batch_idx: 270 |  Loss_1: (0.0263) | Acc_1: (99.15%) (34392/34688)\n",
      "Epoch: 184 | Batch_idx: 280 |  Loss_1: (0.0264) | Acc_1: (99.14%) (35659/35968)\n",
      "Epoch: 184 | Batch_idx: 290 |  Loss_1: (0.0263) | Acc_1: (99.14%) (36929/37248)\n",
      "Epoch: 184 | Batch_idx: 300 |  Loss_1: (0.0264) | Acc_1: (99.15%) (38199/38528)\n",
      "Epoch: 184 | Batch_idx: 310 |  Loss_1: (0.0263) | Acc_1: (99.15%) (39469/39808)\n",
      "Epoch: 184 | Batch_idx: 320 |  Loss_1: (0.0263) | Acc_1: (99.15%) (40737/41088)\n",
      "Epoch: 184 | Batch_idx: 330 |  Loss_1: (0.0263) | Acc_1: (99.14%) (42003/42368)\n",
      "Epoch: 184 | Batch_idx: 340 |  Loss_1: (0.0263) | Acc_1: (99.14%) (43271/43648)\n",
      "Epoch: 184 | Batch_idx: 350 |  Loss_1: (0.0262) | Acc_1: (99.13%) (44539/44928)\n",
      "Epoch: 184 | Batch_idx: 360 |  Loss_1: (0.0261) | Acc_1: (99.13%) (45807/46208)\n",
      "Epoch: 184 | Batch_idx: 370 |  Loss_1: (0.0260) | Acc_1: (99.14%) (47078/47488)\n",
      "Epoch: 184 | Batch_idx: 380 |  Loss_1: (0.0262) | Acc_1: (99.13%) (48342/48768)\n",
      "Epoch: 184 | Batch_idx: 390 |  Loss_1: (0.0262) | Acc_1: (99.13%) (49563/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4539) | Acc: (91.59%) (9159/10000)\n",
      "Epoch: 185 | Batch_idx: 0 |  Loss_1: (0.0057) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 185 | Batch_idx: 10 |  Loss_1: (0.0197) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 185 | Batch_idx: 20 |  Loss_1: (0.0220) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 185 | Batch_idx: 30 |  Loss_1: (0.0228) | Acc_1: (99.17%) (3935/3968)\n",
      "Epoch: 185 | Batch_idx: 40 |  Loss_1: (0.0231) | Acc_1: (99.16%) (5204/5248)\n",
      "Epoch: 185 | Batch_idx: 50 |  Loss_1: (0.0252) | Acc_1: (99.10%) (6469/6528)\n",
      "Epoch: 185 | Batch_idx: 60 |  Loss_1: (0.0242) | Acc_1: (99.14%) (7741/7808)\n",
      "Epoch: 185 | Batch_idx: 70 |  Loss_1: (0.0246) | Acc_1: (99.15%) (9011/9088)\n",
      "Epoch: 185 | Batch_idx: 80 |  Loss_1: (0.0249) | Acc_1: (99.11%) (10276/10368)\n",
      "Epoch: 185 | Batch_idx: 90 |  Loss_1: (0.0256) | Acc_1: (99.09%) (11542/11648)\n",
      "Epoch: 185 | Batch_idx: 100 |  Loss_1: (0.0252) | Acc_1: (99.13%) (12815/12928)\n",
      "Epoch: 185 | Batch_idx: 110 |  Loss_1: (0.0252) | Acc_1: (99.13%) (14085/14208)\n",
      "Epoch: 185 | Batch_idx: 120 |  Loss_1: (0.0248) | Acc_1: (99.14%) (15355/15488)\n",
      "Epoch: 185 | Batch_idx: 130 |  Loss_1: (0.0246) | Acc_1: (99.15%) (16626/16768)\n",
      "Epoch: 185 | Batch_idx: 140 |  Loss_1: (0.0252) | Acc_1: (99.14%) (17892/18048)\n",
      "Epoch: 185 | Batch_idx: 150 |  Loss_1: (0.0250) | Acc_1: (99.13%) (19159/19328)\n",
      "Epoch: 185 | Batch_idx: 160 |  Loss_1: (0.0239) | Acc_1: (99.18%) (20438/20608)\n",
      "Epoch: 185 | Batch_idx: 170 |  Loss_1: (0.0239) | Acc_1: (99.16%) (21705/21888)\n",
      "Epoch: 185 | Batch_idx: 180 |  Loss_1: (0.0233) | Acc_1: (99.18%) (22979/23168)\n",
      "Epoch: 185 | Batch_idx: 190 |  Loss_1: (0.0237) | Acc_1: (99.17%) (24245/24448)\n",
      "Epoch: 185 | Batch_idx: 200 |  Loss_1: (0.0238) | Acc_1: (99.17%) (25514/25728)\n",
      "Epoch: 185 | Batch_idx: 210 |  Loss_1: (0.0245) | Acc_1: (99.12%) (26771/27008)\n",
      "Epoch: 185 | Batch_idx: 220 |  Loss_1: (0.0244) | Acc_1: (99.12%) (28038/28288)\n",
      "Epoch: 185 | Batch_idx: 230 |  Loss_1: (0.0244) | Acc_1: (99.12%) (29308/29568)\n",
      "Epoch: 185 | Batch_idx: 240 |  Loss_1: (0.0243) | Acc_1: (99.13%) (30579/30848)\n",
      "Epoch: 185 | Batch_idx: 250 |  Loss_1: (0.0250) | Acc_1: (99.10%) (31840/32128)\n",
      "Epoch: 185 | Batch_idx: 260 |  Loss_1: (0.0251) | Acc_1: (99.10%) (33107/33408)\n",
      "Epoch: 185 | Batch_idx: 270 |  Loss_1: (0.0253) | Acc_1: (99.09%) (34374/34688)\n",
      "Epoch: 185 | Batch_idx: 280 |  Loss_1: (0.0256) | Acc_1: (99.10%) (35643/35968)\n",
      "Epoch: 185 | Batch_idx: 290 |  Loss_1: (0.0252) | Acc_1: (99.11%) (36916/37248)\n",
      "Epoch: 185 | Batch_idx: 300 |  Loss_1: (0.0250) | Acc_1: (99.11%) (38187/38528)\n",
      "Epoch: 185 | Batch_idx: 310 |  Loss_1: (0.0250) | Acc_1: (99.11%) (39455/39808)\n",
      "Epoch: 185 | Batch_idx: 320 |  Loss_1: (0.0252) | Acc_1: (99.11%) (40723/41088)\n",
      "Epoch: 185 | Batch_idx: 330 |  Loss_1: (0.0252) | Acc_1: (99.11%) (41993/42368)\n",
      "Epoch: 185 | Batch_idx: 340 |  Loss_1: (0.0252) | Acc_1: (99.11%) (43260/43648)\n",
      "Epoch: 185 | Batch_idx: 350 |  Loss_1: (0.0253) | Acc_1: (99.11%) (44526/44928)\n",
      "Epoch: 185 | Batch_idx: 360 |  Loss_1: (0.0258) | Acc_1: (99.10%) (45792/46208)\n",
      "Epoch: 185 | Batch_idx: 370 |  Loss_1: (0.0259) | Acc_1: (99.11%) (47063/47488)\n",
      "Epoch: 185 | Batch_idx: 380 |  Loss_1: (0.0259) | Acc_1: (99.11%) (48332/48768)\n",
      "Epoch: 185 | Batch_idx: 390 |  Loss_1: (0.0259) | Acc_1: (99.11%) (49556/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4672) | Acc: (91.26%) (9126/10000)\n",
      "Epoch: 186 | Batch_idx: 0 |  Loss_1: (0.0389) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 186 | Batch_idx: 10 |  Loss_1: (0.0321) | Acc_1: (99.08%) (1395/1408)\n",
      "Epoch: 186 | Batch_idx: 20 |  Loss_1: (0.0307) | Acc_1: (99.00%) (2661/2688)\n",
      "Epoch: 186 | Batch_idx: 30 |  Loss_1: (0.0283) | Acc_1: (99.04%) (3930/3968)\n",
      "Epoch: 186 | Batch_idx: 40 |  Loss_1: (0.0260) | Acc_1: (99.07%) (5199/5248)\n",
      "Epoch: 186 | Batch_idx: 50 |  Loss_1: (0.0266) | Acc_1: (99.11%) (6470/6528)\n",
      "Epoch: 186 | Batch_idx: 60 |  Loss_1: (0.0262) | Acc_1: (99.14%) (7741/7808)\n",
      "Epoch: 186 | Batch_idx: 70 |  Loss_1: (0.0265) | Acc_1: (99.14%) (9010/9088)\n",
      "Epoch: 186 | Batch_idx: 80 |  Loss_1: (0.0250) | Acc_1: (99.19%) (10284/10368)\n",
      "Epoch: 186 | Batch_idx: 90 |  Loss_1: (0.0249) | Acc_1: (99.19%) (11554/11648)\n",
      "Epoch: 186 | Batch_idx: 100 |  Loss_1: (0.0255) | Acc_1: (99.17%) (12821/12928)\n",
      "Epoch: 186 | Batch_idx: 110 |  Loss_1: (0.0256) | Acc_1: (99.16%) (14089/14208)\n",
      "Epoch: 186 | Batch_idx: 120 |  Loss_1: (0.0257) | Acc_1: (99.15%) (15357/15488)\n",
      "Epoch: 186 | Batch_idx: 130 |  Loss_1: (0.0255) | Acc_1: (99.18%) (16630/16768)\n",
      "Epoch: 186 | Batch_idx: 140 |  Loss_1: (0.0259) | Acc_1: (99.16%) (17896/18048)\n",
      "Epoch: 186 | Batch_idx: 150 |  Loss_1: (0.0265) | Acc_1: (99.15%) (19163/19328)\n",
      "Epoch: 186 | Batch_idx: 160 |  Loss_1: (0.0264) | Acc_1: (99.16%) (20434/20608)\n",
      "Epoch: 186 | Batch_idx: 170 |  Loss_1: (0.0266) | Acc_1: (99.15%) (21701/21888)\n",
      "Epoch: 186 | Batch_idx: 180 |  Loss_1: (0.0265) | Acc_1: (99.13%) (22967/23168)\n",
      "Epoch: 186 | Batch_idx: 190 |  Loss_1: (0.0262) | Acc_1: (99.15%) (24239/24448)\n",
      "Epoch: 186 | Batch_idx: 200 |  Loss_1: (0.0263) | Acc_1: (99.13%) (25505/25728)\n",
      "Epoch: 186 | Batch_idx: 210 |  Loss_1: (0.0262) | Acc_1: (99.13%) (26774/27008)\n",
      "Epoch: 186 | Batch_idx: 220 |  Loss_1: (0.0260) | Acc_1: (99.14%) (28046/28288)\n",
      "Epoch: 186 | Batch_idx: 230 |  Loss_1: (0.0263) | Acc_1: (99.14%) (29314/29568)\n",
      "Epoch: 186 | Batch_idx: 240 |  Loss_1: (0.0265) | Acc_1: (99.13%) (30581/30848)\n",
      "Epoch: 186 | Batch_idx: 250 |  Loss_1: (0.0266) | Acc_1: (99.13%) (31850/32128)\n",
      "Epoch: 186 | Batch_idx: 260 |  Loss_1: (0.0266) | Acc_1: (99.14%) (33122/33408)\n",
      "Epoch: 186 | Batch_idx: 270 |  Loss_1: (0.0265) | Acc_1: (99.16%) (34395/34688)\n",
      "Epoch: 186 | Batch_idx: 280 |  Loss_1: (0.0270) | Acc_1: (99.14%) (35660/35968)\n",
      "Epoch: 186 | Batch_idx: 290 |  Loss_1: (0.0271) | Acc_1: (99.14%) (36928/37248)\n",
      "Epoch: 186 | Batch_idx: 300 |  Loss_1: (0.0277) | Acc_1: (99.12%) (38189/38528)\n",
      "Epoch: 186 | Batch_idx: 310 |  Loss_1: (0.0276) | Acc_1: (99.12%) (39456/39808)\n",
      "Epoch: 186 | Batch_idx: 320 |  Loss_1: (0.0276) | Acc_1: (99.11%) (40723/41088)\n",
      "Epoch: 186 | Batch_idx: 330 |  Loss_1: (0.0280) | Acc_1: (99.10%) (41986/42368)\n",
      "Epoch: 186 | Batch_idx: 340 |  Loss_1: (0.0279) | Acc_1: (99.10%) (43253/43648)\n",
      "Epoch: 186 | Batch_idx: 350 |  Loss_1: (0.0279) | Acc_1: (99.09%) (44520/44928)\n",
      "Epoch: 186 | Batch_idx: 360 |  Loss_1: (0.0278) | Acc_1: (99.10%) (45791/46208)\n",
      "Epoch: 186 | Batch_idx: 370 |  Loss_1: (0.0276) | Acc_1: (99.11%) (47064/47488)\n",
      "Epoch: 186 | Batch_idx: 380 |  Loss_1: (0.0274) | Acc_1: (99.11%) (48336/48768)\n",
      "Epoch: 186 | Batch_idx: 390 |  Loss_1: (0.0273) | Acc_1: (99.12%) (49559/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4803) | Acc: (91.46%) (9146/10000)\n",
      "Epoch: 187 | Batch_idx: 0 |  Loss_1: (0.0217) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 187 | Batch_idx: 10 |  Loss_1: (0.0151) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 187 | Batch_idx: 20 |  Loss_1: (0.0184) | Acc_1: (99.11%) (2664/2688)\n",
      "Epoch: 187 | Batch_idx: 30 |  Loss_1: (0.0171) | Acc_1: (99.24%) (3938/3968)\n",
      "Epoch: 187 | Batch_idx: 40 |  Loss_1: (0.0186) | Acc_1: (99.24%) (5208/5248)\n",
      "Epoch: 187 | Batch_idx: 50 |  Loss_1: (0.0191) | Acc_1: (99.26%) (6480/6528)\n",
      "Epoch: 187 | Batch_idx: 60 |  Loss_1: (0.0188) | Acc_1: (99.32%) (7755/7808)\n",
      "Epoch: 187 | Batch_idx: 70 |  Loss_1: (0.0191) | Acc_1: (99.33%) (9027/9088)\n",
      "Epoch: 187 | Batch_idx: 80 |  Loss_1: (0.0195) | Acc_1: (99.33%) (10299/10368)\n",
      "Epoch: 187 | Batch_idx: 90 |  Loss_1: (0.0199) | Acc_1: (99.31%) (11568/11648)\n",
      "Epoch: 187 | Batch_idx: 100 |  Loss_1: (0.0197) | Acc_1: (99.31%) (12839/12928)\n",
      "Epoch: 187 | Batch_idx: 110 |  Loss_1: (0.0200) | Acc_1: (99.29%) (14107/14208)\n",
      "Epoch: 187 | Batch_idx: 120 |  Loss_1: (0.0195) | Acc_1: (99.31%) (15381/15488)\n",
      "Epoch: 187 | Batch_idx: 130 |  Loss_1: (0.0192) | Acc_1: (99.33%) (16655/16768)\n",
      "Epoch: 187 | Batch_idx: 140 |  Loss_1: (0.0193) | Acc_1: (99.33%) (17927/18048)\n",
      "Epoch: 187 | Batch_idx: 150 |  Loss_1: (0.0196) | Acc_1: (99.32%) (19196/19328)\n",
      "Epoch: 187 | Batch_idx: 160 |  Loss_1: (0.0194) | Acc_1: (99.33%) (20469/20608)\n",
      "Epoch: 187 | Batch_idx: 170 |  Loss_1: (0.0192) | Acc_1: (99.33%) (21742/21888)\n",
      "Epoch: 187 | Batch_idx: 180 |  Loss_1: (0.0189) | Acc_1: (99.34%) (23015/23168)\n",
      "Epoch: 187 | Batch_idx: 190 |  Loss_1: (0.0194) | Acc_1: (99.31%) (24279/24448)\n",
      "Epoch: 187 | Batch_idx: 200 |  Loss_1: (0.0189) | Acc_1: (99.33%) (25556/25728)\n",
      "Epoch: 187 | Batch_idx: 210 |  Loss_1: (0.0191) | Acc_1: (99.32%) (26824/27008)\n",
      "Epoch: 187 | Batch_idx: 220 |  Loss_1: (0.0191) | Acc_1: (99.32%) (28096/28288)\n",
      "Epoch: 187 | Batch_idx: 230 |  Loss_1: (0.0191) | Acc_1: (99.32%) (29368/29568)\n",
      "Epoch: 187 | Batch_idx: 240 |  Loss_1: (0.0194) | Acc_1: (99.31%) (30636/30848)\n",
      "Epoch: 187 | Batch_idx: 250 |  Loss_1: (0.0196) | Acc_1: (99.31%) (31906/32128)\n",
      "Epoch: 187 | Batch_idx: 260 |  Loss_1: (0.0198) | Acc_1: (99.30%) (33175/33408)\n",
      "Epoch: 187 | Batch_idx: 270 |  Loss_1: (0.0197) | Acc_1: (99.31%) (34448/34688)\n",
      "Epoch: 187 | Batch_idx: 280 |  Loss_1: (0.0196) | Acc_1: (99.30%) (35718/35968)\n",
      "Epoch: 187 | Batch_idx: 290 |  Loss_1: (0.0200) | Acc_1: (99.29%) (36985/37248)\n",
      "Epoch: 187 | Batch_idx: 300 |  Loss_1: (0.0201) | Acc_1: (99.28%) (38252/38528)\n",
      "Epoch: 187 | Batch_idx: 310 |  Loss_1: (0.0205) | Acc_1: (99.28%) (39520/39808)\n",
      "Epoch: 187 | Batch_idx: 320 |  Loss_1: (0.0210) | Acc_1: (99.26%) (40786/41088)\n",
      "Epoch: 187 | Batch_idx: 330 |  Loss_1: (0.0215) | Acc_1: (99.26%) (42053/42368)\n",
      "Epoch: 187 | Batch_idx: 340 |  Loss_1: (0.0217) | Acc_1: (99.26%) (43324/43648)\n",
      "Epoch: 187 | Batch_idx: 350 |  Loss_1: (0.0216) | Acc_1: (99.27%) (44598/44928)\n",
      "Epoch: 187 | Batch_idx: 360 |  Loss_1: (0.0217) | Acc_1: (99.26%) (45867/46208)\n",
      "Epoch: 187 | Batch_idx: 370 |  Loss_1: (0.0218) | Acc_1: (99.26%) (47136/47488)\n",
      "Epoch: 187 | Batch_idx: 380 |  Loss_1: (0.0218) | Acc_1: (99.26%) (48407/48768)\n",
      "Epoch: 187 | Batch_idx: 390 |  Loss_1: (0.0218) | Acc_1: (99.26%) (49629/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4920) | Acc: (91.58%) (9158/10000)\n",
      "Epoch: 188 | Batch_idx: 0 |  Loss_1: (0.0459) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 188 | Batch_idx: 10 |  Loss_1: (0.0376) | Acc_1: (98.79%) (1391/1408)\n",
      "Epoch: 188 | Batch_idx: 20 |  Loss_1: (0.0346) | Acc_1: (98.85%) (2657/2688)\n",
      "Epoch: 188 | Batch_idx: 30 |  Loss_1: (0.0272) | Acc_1: (99.09%) (3932/3968)\n",
      "Epoch: 188 | Batch_idx: 40 |  Loss_1: (0.0266) | Acc_1: (99.09%) (5200/5248)\n",
      "Epoch: 188 | Batch_idx: 50 |  Loss_1: (0.0252) | Acc_1: (99.11%) (6470/6528)\n",
      "Epoch: 188 | Batch_idx: 60 |  Loss_1: (0.0247) | Acc_1: (99.13%) (7740/7808)\n",
      "Epoch: 188 | Batch_idx: 70 |  Loss_1: (0.0238) | Acc_1: (99.16%) (9012/9088)\n",
      "Epoch: 188 | Batch_idx: 80 |  Loss_1: (0.0229) | Acc_1: (99.20%) (10285/10368)\n",
      "Epoch: 188 | Batch_idx: 90 |  Loss_1: (0.0228) | Acc_1: (99.22%) (11557/11648)\n",
      "Epoch: 188 | Batch_idx: 100 |  Loss_1: (0.0228) | Acc_1: (99.20%) (12825/12928)\n",
      "Epoch: 188 | Batch_idx: 110 |  Loss_1: (0.0238) | Acc_1: (99.16%) (14088/14208)\n",
      "Epoch: 188 | Batch_idx: 120 |  Loss_1: (0.0241) | Acc_1: (99.12%) (15352/15488)\n",
      "Epoch: 188 | Batch_idx: 130 |  Loss_1: (0.0238) | Acc_1: (99.12%) (16621/16768)\n",
      "Epoch: 188 | Batch_idx: 140 |  Loss_1: (0.0243) | Acc_1: (99.11%) (17888/18048)\n",
      "Epoch: 188 | Batch_idx: 150 |  Loss_1: (0.0241) | Acc_1: (99.13%) (19160/19328)\n",
      "Epoch: 188 | Batch_idx: 160 |  Loss_1: (0.0235) | Acc_1: (99.16%) (20435/20608)\n",
      "Epoch: 188 | Batch_idx: 170 |  Loss_1: (0.0236) | Acc_1: (99.16%) (21705/21888)\n",
      "Epoch: 188 | Batch_idx: 180 |  Loss_1: (0.0233) | Acc_1: (99.17%) (22976/23168)\n",
      "Epoch: 188 | Batch_idx: 190 |  Loss_1: (0.0245) | Acc_1: (99.14%) (24238/24448)\n",
      "Epoch: 188 | Batch_idx: 200 |  Loss_1: (0.0239) | Acc_1: (99.17%) (25515/25728)\n",
      "Epoch: 188 | Batch_idx: 210 |  Loss_1: (0.0240) | Acc_1: (99.17%) (26784/27008)\n",
      "Epoch: 188 | Batch_idx: 220 |  Loss_1: (0.0238) | Acc_1: (99.19%) (28058/28288)\n",
      "Epoch: 188 | Batch_idx: 230 |  Loss_1: (0.0237) | Acc_1: (99.18%) (29326/29568)\n",
      "Epoch: 188 | Batch_idx: 240 |  Loss_1: (0.0237) | Acc_1: (99.18%) (30595/30848)\n",
      "Epoch: 188 | Batch_idx: 250 |  Loss_1: (0.0240) | Acc_1: (99.17%) (31860/32128)\n",
      "Epoch: 188 | Batch_idx: 260 |  Loss_1: (0.0241) | Acc_1: (99.16%) (33126/33408)\n",
      "Epoch: 188 | Batch_idx: 270 |  Loss_1: (0.0242) | Acc_1: (99.15%) (34393/34688)\n",
      "Epoch: 188 | Batch_idx: 280 |  Loss_1: (0.0247) | Acc_1: (99.13%) (35656/35968)\n",
      "Epoch: 188 | Batch_idx: 290 |  Loss_1: (0.0247) | Acc_1: (99.12%) (36922/37248)\n",
      "Epoch: 188 | Batch_idx: 300 |  Loss_1: (0.0244) | Acc_1: (99.14%) (38197/38528)\n",
      "Epoch: 188 | Batch_idx: 310 |  Loss_1: (0.0244) | Acc_1: (99.14%) (39464/39808)\n",
      "Epoch: 188 | Batch_idx: 320 |  Loss_1: (0.0245) | Acc_1: (99.14%) (40735/41088)\n",
      "Epoch: 188 | Batch_idx: 330 |  Loss_1: (0.0248) | Acc_1: (99.13%) (42000/42368)\n",
      "Epoch: 188 | Batch_idx: 340 |  Loss_1: (0.0246) | Acc_1: (99.14%) (43273/43648)\n",
      "Epoch: 188 | Batch_idx: 350 |  Loss_1: (0.0246) | Acc_1: (99.14%) (44543/44928)\n",
      "Epoch: 188 | Batch_idx: 360 |  Loss_1: (0.0244) | Acc_1: (99.15%) (45817/46208)\n",
      "Epoch: 188 | Batch_idx: 370 |  Loss_1: (0.0247) | Acc_1: (99.15%) (47082/47488)\n",
      "Epoch: 188 | Batch_idx: 380 |  Loss_1: (0.0244) | Acc_1: (99.16%) (48358/48768)\n",
      "Epoch: 188 | Batch_idx: 390 |  Loss_1: (0.0243) | Acc_1: (99.17%) (49583/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4504) | Acc: (91.35%) (9135/10000)\n",
      "Epoch: 189 | Batch_idx: 0 |  Loss_1: (0.0105) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 189 | Batch_idx: 10 |  Loss_1: (0.0172) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 189 | Batch_idx: 20 |  Loss_1: (0.0222) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 189 | Batch_idx: 30 |  Loss_1: (0.0224) | Acc_1: (99.27%) (3939/3968)\n",
      "Epoch: 189 | Batch_idx: 40 |  Loss_1: (0.0207) | Acc_1: (99.28%) (5210/5248)\n",
      "Epoch: 189 | Batch_idx: 50 |  Loss_1: (0.0221) | Acc_1: (99.16%) (6473/6528)\n",
      "Epoch: 189 | Batch_idx: 60 |  Loss_1: (0.0223) | Acc_1: (99.18%) (7744/7808)\n",
      "Epoch: 189 | Batch_idx: 70 |  Loss_1: (0.0229) | Acc_1: (99.15%) (9011/9088)\n",
      "Epoch: 189 | Batch_idx: 80 |  Loss_1: (0.0225) | Acc_1: (99.18%) (10283/10368)\n",
      "Epoch: 189 | Batch_idx: 90 |  Loss_1: (0.0219) | Acc_1: (99.21%) (11556/11648)\n",
      "Epoch: 189 | Batch_idx: 100 |  Loss_1: (0.0217) | Acc_1: (99.23%) (12829/12928)\n",
      "Epoch: 189 | Batch_idx: 110 |  Loss_1: (0.0217) | Acc_1: (99.23%) (14099/14208)\n",
      "Epoch: 189 | Batch_idx: 120 |  Loss_1: (0.0206) | Acc_1: (99.27%) (15375/15488)\n",
      "Epoch: 189 | Batch_idx: 130 |  Loss_1: (0.0214) | Acc_1: (99.26%) (16644/16768)\n",
      "Epoch: 189 | Batch_idx: 140 |  Loss_1: (0.0206) | Acc_1: (99.29%) (17920/18048)\n",
      "Epoch: 189 | Batch_idx: 150 |  Loss_1: (0.0205) | Acc_1: (99.31%) (19194/19328)\n",
      "Epoch: 189 | Batch_idx: 160 |  Loss_1: (0.0199) | Acc_1: (99.34%) (20472/20608)\n",
      "Epoch: 189 | Batch_idx: 170 |  Loss_1: (0.0200) | Acc_1: (99.34%) (21743/21888)\n",
      "Epoch: 189 | Batch_idx: 180 |  Loss_1: (0.0208) | Acc_1: (99.31%) (23007/23168)\n",
      "Epoch: 189 | Batch_idx: 190 |  Loss_1: (0.0207) | Acc_1: (99.31%) (24280/24448)\n",
      "Epoch: 189 | Batch_idx: 200 |  Loss_1: (0.0214) | Acc_1: (99.30%) (25549/25728)\n",
      "Epoch: 189 | Batch_idx: 210 |  Loss_1: (0.0212) | Acc_1: (99.32%) (26823/27008)\n",
      "Epoch: 189 | Batch_idx: 220 |  Loss_1: (0.0213) | Acc_1: (99.32%) (28096/28288)\n",
      "Epoch: 189 | Batch_idx: 230 |  Loss_1: (0.0221) | Acc_1: (99.30%) (29360/29568)\n",
      "Epoch: 189 | Batch_idx: 240 |  Loss_1: (0.0217) | Acc_1: (99.31%) (30634/30848)\n",
      "Epoch: 189 | Batch_idx: 250 |  Loss_1: (0.0215) | Acc_1: (99.31%) (31906/32128)\n",
      "Epoch: 189 | Batch_idx: 260 |  Loss_1: (0.0212) | Acc_1: (99.32%) (33182/33408)\n",
      "Epoch: 189 | Batch_idx: 270 |  Loss_1: (0.0215) | Acc_1: (99.31%) (34450/34688)\n",
      "Epoch: 189 | Batch_idx: 280 |  Loss_1: (0.0219) | Acc_1: (99.30%) (35715/35968)\n",
      "Epoch: 189 | Batch_idx: 290 |  Loss_1: (0.0219) | Acc_1: (99.29%) (36985/37248)\n",
      "Epoch: 189 | Batch_idx: 300 |  Loss_1: (0.0218) | Acc_1: (99.30%) (38259/38528)\n",
      "Epoch: 189 | Batch_idx: 310 |  Loss_1: (0.0217) | Acc_1: (99.30%) (39531/39808)\n",
      "Epoch: 189 | Batch_idx: 320 |  Loss_1: (0.0222) | Acc_1: (99.29%) (40797/41088)\n",
      "Epoch: 189 | Batch_idx: 330 |  Loss_1: (0.0222) | Acc_1: (99.29%) (42068/42368)\n",
      "Epoch: 189 | Batch_idx: 340 |  Loss_1: (0.0222) | Acc_1: (99.29%) (43339/43648)\n",
      "Epoch: 189 | Batch_idx: 350 |  Loss_1: (0.0222) | Acc_1: (99.30%) (44612/44928)\n",
      "Epoch: 189 | Batch_idx: 360 |  Loss_1: (0.0222) | Acc_1: (99.29%) (45882/46208)\n",
      "Epoch: 189 | Batch_idx: 370 |  Loss_1: (0.0221) | Acc_1: (99.29%) (47153/47488)\n",
      "Epoch: 189 | Batch_idx: 380 |  Loss_1: (0.0221) | Acc_1: (99.29%) (48422/48768)\n",
      "Epoch: 189 | Batch_idx: 390 |  Loss_1: (0.0221) | Acc_1: (99.29%) (49646/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4816) | Acc: (91.22%) (9122/10000)\n",
      "Epoch: 190 | Batch_idx: 0 |  Loss_1: (0.0127) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 190 | Batch_idx: 10 |  Loss_1: (0.0214) | Acc_1: (99.29%) (1398/1408)\n",
      "Epoch: 190 | Batch_idx: 20 |  Loss_1: (0.0203) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 190 | Batch_idx: 30 |  Loss_1: (0.0195) | Acc_1: (99.40%) (3944/3968)\n",
      "Epoch: 190 | Batch_idx: 40 |  Loss_1: (0.0220) | Acc_1: (99.28%) (5210/5248)\n",
      "Epoch: 190 | Batch_idx: 50 |  Loss_1: (0.0210) | Acc_1: (99.31%) (6483/6528)\n",
      "Epoch: 190 | Batch_idx: 60 |  Loss_1: (0.0235) | Acc_1: (99.26%) (7750/7808)\n",
      "Epoch: 190 | Batch_idx: 70 |  Loss_1: (0.0245) | Acc_1: (99.21%) (9016/9088)\n",
      "Epoch: 190 | Batch_idx: 80 |  Loss_1: (0.0245) | Acc_1: (99.20%) (10285/10368)\n",
      "Epoch: 190 | Batch_idx: 90 |  Loss_1: (0.0244) | Acc_1: (99.20%) (11555/11648)\n",
      "Epoch: 190 | Batch_idx: 100 |  Loss_1: (0.0253) | Acc_1: (99.17%) (12821/12928)\n",
      "Epoch: 190 | Batch_idx: 110 |  Loss_1: (0.0243) | Acc_1: (99.20%) (14095/14208)\n",
      "Epoch: 190 | Batch_idx: 120 |  Loss_1: (0.0241) | Acc_1: (99.19%) (15363/15488)\n",
      "Epoch: 190 | Batch_idx: 130 |  Loss_1: (0.0247) | Acc_1: (99.17%) (16629/16768)\n",
      "Epoch: 190 | Batch_idx: 140 |  Loss_1: (0.0253) | Acc_1: (99.16%) (17896/18048)\n",
      "Epoch: 190 | Batch_idx: 150 |  Loss_1: (0.0246) | Acc_1: (99.18%) (19170/19328)\n",
      "Epoch: 190 | Batch_idx: 160 |  Loss_1: (0.0241) | Acc_1: (99.19%) (20442/20608)\n",
      "Epoch: 190 | Batch_idx: 170 |  Loss_1: (0.0242) | Acc_1: (99.16%) (21705/21888)\n",
      "Epoch: 190 | Batch_idx: 180 |  Loss_1: (0.0240) | Acc_1: (99.17%) (22976/23168)\n",
      "Epoch: 190 | Batch_idx: 190 |  Loss_1: (0.0238) | Acc_1: (99.19%) (24250/24448)\n",
      "Epoch: 190 | Batch_idx: 200 |  Loss_1: (0.0236) | Acc_1: (99.20%) (25522/25728)\n",
      "Epoch: 190 | Batch_idx: 210 |  Loss_1: (0.0234) | Acc_1: (99.21%) (26794/27008)\n",
      "Epoch: 190 | Batch_idx: 220 |  Loss_1: (0.0235) | Acc_1: (99.21%) (28065/28288)\n",
      "Epoch: 190 | Batch_idx: 230 |  Loss_1: (0.0235) | Acc_1: (99.21%) (29334/29568)\n",
      "Epoch: 190 | Batch_idx: 240 |  Loss_1: (0.0232) | Acc_1: (99.22%) (30606/30848)\n",
      "Epoch: 190 | Batch_idx: 250 |  Loss_1: (0.0231) | Acc_1: (99.21%) (31875/32128)\n",
      "Epoch: 190 | Batch_idx: 260 |  Loss_1: (0.0231) | Acc_1: (99.21%) (33144/33408)\n",
      "Epoch: 190 | Batch_idx: 270 |  Loss_1: (0.0230) | Acc_1: (99.21%) (34414/34688)\n",
      "Epoch: 190 | Batch_idx: 280 |  Loss_1: (0.0231) | Acc_1: (99.20%) (35682/35968)\n",
      "Epoch: 190 | Batch_idx: 290 |  Loss_1: (0.0230) | Acc_1: (99.21%) (36953/37248)\n",
      "Epoch: 190 | Batch_idx: 300 |  Loss_1: (0.0237) | Acc_1: (99.19%) (38216/38528)\n",
      "Epoch: 190 | Batch_idx: 310 |  Loss_1: (0.0237) | Acc_1: (99.19%) (39485/39808)\n",
      "Epoch: 190 | Batch_idx: 320 |  Loss_1: (0.0238) | Acc_1: (99.19%) (40754/41088)\n",
      "Epoch: 190 | Batch_idx: 330 |  Loss_1: (0.0237) | Acc_1: (99.19%) (42023/42368)\n",
      "Epoch: 190 | Batch_idx: 340 |  Loss_1: (0.0237) | Acc_1: (99.20%) (43297/43648)\n",
      "Epoch: 190 | Batch_idx: 350 |  Loss_1: (0.0236) | Acc_1: (99.20%) (44567/44928)\n",
      "Epoch: 190 | Batch_idx: 360 |  Loss_1: (0.0233) | Acc_1: (99.21%) (45843/46208)\n",
      "Epoch: 190 | Batch_idx: 370 |  Loss_1: (0.0231) | Acc_1: (99.22%) (47116/47488)\n",
      "Epoch: 190 | Batch_idx: 380 |  Loss_1: (0.0233) | Acc_1: (99.21%) (48383/48768)\n",
      "Epoch: 190 | Batch_idx: 390 |  Loss_1: (0.0235) | Acc_1: (99.20%) (49601/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4656) | Acc: (91.46%) (9146/10000)\n",
      "Epoch: 191 | Batch_idx: 0 |  Loss_1: (0.0060) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 191 | Batch_idx: 10 |  Loss_1: (0.0134) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 191 | Batch_idx: 20 |  Loss_1: (0.0187) | Acc_1: (99.33%) (2670/2688)\n",
      "Epoch: 191 | Batch_idx: 30 |  Loss_1: (0.0188) | Acc_1: (99.37%) (3943/3968)\n",
      "Epoch: 191 | Batch_idx: 40 |  Loss_1: (0.0198) | Acc_1: (99.28%) (5210/5248)\n",
      "Epoch: 191 | Batch_idx: 50 |  Loss_1: (0.0216) | Acc_1: (99.22%) (6477/6528)\n",
      "Epoch: 191 | Batch_idx: 60 |  Loss_1: (0.0239) | Acc_1: (99.13%) (7740/7808)\n",
      "Epoch: 191 | Batch_idx: 70 |  Loss_1: (0.0231) | Acc_1: (99.16%) (9012/9088)\n",
      "Epoch: 191 | Batch_idx: 80 |  Loss_1: (0.0231) | Acc_1: (99.18%) (10283/10368)\n",
      "Epoch: 191 | Batch_idx: 90 |  Loss_1: (0.0228) | Acc_1: (99.18%) (11553/11648)\n",
      "Epoch: 191 | Batch_idx: 100 |  Loss_1: (0.0226) | Acc_1: (99.18%) (12822/12928)\n",
      "Epoch: 191 | Batch_idx: 110 |  Loss_1: (0.0231) | Acc_1: (99.17%) (14090/14208)\n",
      "Epoch: 191 | Batch_idx: 120 |  Loss_1: (0.0233) | Acc_1: (99.17%) (15359/15488)\n",
      "Epoch: 191 | Batch_idx: 130 |  Loss_1: (0.0245) | Acc_1: (99.10%) (16617/16768)\n",
      "Epoch: 191 | Batch_idx: 140 |  Loss_1: (0.0245) | Acc_1: (99.11%) (17888/18048)\n",
      "Epoch: 191 | Batch_idx: 150 |  Loss_1: (0.0238) | Acc_1: (99.13%) (19159/19328)\n",
      "Epoch: 191 | Batch_idx: 160 |  Loss_1: (0.0233) | Acc_1: (99.15%) (20432/20608)\n",
      "Epoch: 191 | Batch_idx: 170 |  Loss_1: (0.0237) | Acc_1: (99.14%) (21699/21888)\n",
      "Epoch: 191 | Batch_idx: 180 |  Loss_1: (0.0237) | Acc_1: (99.14%) (22968/23168)\n",
      "Epoch: 191 | Batch_idx: 190 |  Loss_1: (0.0239) | Acc_1: (99.13%) (24236/24448)\n",
      "Epoch: 191 | Batch_idx: 200 |  Loss_1: (0.0237) | Acc_1: (99.14%) (25508/25728)\n",
      "Epoch: 191 | Batch_idx: 210 |  Loss_1: (0.0238) | Acc_1: (99.13%) (26774/27008)\n",
      "Epoch: 191 | Batch_idx: 220 |  Loss_1: (0.0237) | Acc_1: (99.14%) (28045/28288)\n",
      "Epoch: 191 | Batch_idx: 230 |  Loss_1: (0.0240) | Acc_1: (99.14%) (29315/29568)\n",
      "Epoch: 191 | Batch_idx: 240 |  Loss_1: (0.0239) | Acc_1: (99.14%) (30584/30848)\n",
      "Epoch: 191 | Batch_idx: 250 |  Loss_1: (0.0242) | Acc_1: (99.14%) (31851/32128)\n",
      "Epoch: 191 | Batch_idx: 260 |  Loss_1: (0.0250) | Acc_1: (99.13%) (33118/33408)\n",
      "Epoch: 191 | Batch_idx: 270 |  Loss_1: (0.0249) | Acc_1: (99.13%) (34386/34688)\n",
      "Epoch: 191 | Batch_idx: 280 |  Loss_1: (0.0250) | Acc_1: (99.12%) (35653/35968)\n",
      "Epoch: 191 | Batch_idx: 290 |  Loss_1: (0.0252) | Acc_1: (99.12%) (36920/37248)\n",
      "Epoch: 191 | Batch_idx: 300 |  Loss_1: (0.0253) | Acc_1: (99.13%) (38192/38528)\n",
      "Epoch: 191 | Batch_idx: 310 |  Loss_1: (0.0252) | Acc_1: (99.13%) (39460/39808)\n",
      "Epoch: 191 | Batch_idx: 320 |  Loss_1: (0.0250) | Acc_1: (99.14%) (40733/41088)\n",
      "Epoch: 191 | Batch_idx: 330 |  Loss_1: (0.0249) | Acc_1: (99.14%) (42003/42368)\n",
      "Epoch: 191 | Batch_idx: 340 |  Loss_1: (0.0248) | Acc_1: (99.15%) (43275/43648)\n",
      "Epoch: 191 | Batch_idx: 350 |  Loss_1: (0.0250) | Acc_1: (99.13%) (44538/44928)\n",
      "Epoch: 191 | Batch_idx: 360 |  Loss_1: (0.0250) | Acc_1: (99.13%) (45806/46208)\n",
      "Epoch: 191 | Batch_idx: 370 |  Loss_1: (0.0248) | Acc_1: (99.13%) (47076/47488)\n",
      "Epoch: 191 | Batch_idx: 380 |  Loss_1: (0.0246) | Acc_1: (99.14%) (48348/48768)\n",
      "Epoch: 191 | Batch_idx: 390 |  Loss_1: (0.0246) | Acc_1: (99.14%) (49569/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4656) | Acc: (91.60%) (9160/10000)\n",
      "Epoch: 192 | Batch_idx: 0 |  Loss_1: (0.0016) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 192 | Batch_idx: 10 |  Loss_1: (0.0304) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 192 | Batch_idx: 20 |  Loss_1: (0.0287) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 192 | Batch_idx: 30 |  Loss_1: (0.0250) | Acc_1: (99.27%) (3939/3968)\n",
      "Epoch: 192 | Batch_idx: 40 |  Loss_1: (0.0230) | Acc_1: (99.28%) (5210/5248)\n",
      "Epoch: 192 | Batch_idx: 50 |  Loss_1: (0.0234) | Acc_1: (99.28%) (6481/6528)\n",
      "Epoch: 192 | Batch_idx: 60 |  Loss_1: (0.0235) | Acc_1: (99.27%) (7751/7808)\n",
      "Epoch: 192 | Batch_idx: 70 |  Loss_1: (0.0222) | Acc_1: (99.33%) (9027/9088)\n",
      "Epoch: 192 | Batch_idx: 80 |  Loss_1: (0.0212) | Acc_1: (99.35%) (10301/10368)\n",
      "Epoch: 192 | Batch_idx: 90 |  Loss_1: (0.0206) | Acc_1: (99.36%) (11573/11648)\n",
      "Epoch: 192 | Batch_idx: 100 |  Loss_1: (0.0206) | Acc_1: (99.36%) (12845/12928)\n",
      "Epoch: 192 | Batch_idx: 110 |  Loss_1: (0.0205) | Acc_1: (99.35%) (14116/14208)\n",
      "Epoch: 192 | Batch_idx: 120 |  Loss_1: (0.0205) | Acc_1: (99.37%) (15390/15488)\n",
      "Epoch: 192 | Batch_idx: 130 |  Loss_1: (0.0203) | Acc_1: (99.37%) (16662/16768)\n",
      "Epoch: 192 | Batch_idx: 140 |  Loss_1: (0.0200) | Acc_1: (99.36%) (17933/18048)\n",
      "Epoch: 192 | Batch_idx: 150 |  Loss_1: (0.0203) | Acc_1: (99.35%) (19203/19328)\n",
      "Epoch: 192 | Batch_idx: 160 |  Loss_1: (0.0201) | Acc_1: (99.36%) (20477/20608)\n",
      "Epoch: 192 | Batch_idx: 170 |  Loss_1: (0.0199) | Acc_1: (99.37%) (21750/21888)\n",
      "Epoch: 192 | Batch_idx: 180 |  Loss_1: (0.0200) | Acc_1: (99.36%) (23019/23168)\n",
      "Epoch: 192 | Batch_idx: 190 |  Loss_1: (0.0196) | Acc_1: (99.37%) (24294/24448)\n",
      "Epoch: 192 | Batch_idx: 200 |  Loss_1: (0.0197) | Acc_1: (99.37%) (25565/25728)\n",
      "Epoch: 192 | Batch_idx: 210 |  Loss_1: (0.0197) | Acc_1: (99.37%) (26839/27008)\n",
      "Epoch: 192 | Batch_idx: 220 |  Loss_1: (0.0200) | Acc_1: (99.36%) (28106/28288)\n",
      "Epoch: 192 | Batch_idx: 230 |  Loss_1: (0.0205) | Acc_1: (99.34%) (29372/29568)\n",
      "Epoch: 192 | Batch_idx: 240 |  Loss_1: (0.0203) | Acc_1: (99.35%) (30646/30848)\n",
      "Epoch: 192 | Batch_idx: 250 |  Loss_1: (0.0208) | Acc_1: (99.33%) (31912/32128)\n",
      "Epoch: 192 | Batch_idx: 260 |  Loss_1: (0.0211) | Acc_1: (99.32%) (33181/33408)\n",
      "Epoch: 192 | Batch_idx: 270 |  Loss_1: (0.0212) | Acc_1: (99.32%) (34451/34688)\n",
      "Epoch: 192 | Batch_idx: 280 |  Loss_1: (0.0218) | Acc_1: (99.29%) (35712/35968)\n",
      "Epoch: 192 | Batch_idx: 290 |  Loss_1: (0.0218) | Acc_1: (99.29%) (36983/37248)\n",
      "Epoch: 192 | Batch_idx: 300 |  Loss_1: (0.0220) | Acc_1: (99.28%) (38250/38528)\n",
      "Epoch: 192 | Batch_idx: 310 |  Loss_1: (0.0220) | Acc_1: (99.28%) (39520/39808)\n",
      "Epoch: 192 | Batch_idx: 320 |  Loss_1: (0.0221) | Acc_1: (99.27%) (40788/41088)\n",
      "Epoch: 192 | Batch_idx: 330 |  Loss_1: (0.0220) | Acc_1: (99.27%) (42057/42368)\n",
      "Epoch: 192 | Batch_idx: 340 |  Loss_1: (0.0221) | Acc_1: (99.26%) (43323/43648)\n",
      "Epoch: 192 | Batch_idx: 350 |  Loss_1: (0.0217) | Acc_1: (99.27%) (44599/44928)\n",
      "Epoch: 192 | Batch_idx: 360 |  Loss_1: (0.0223) | Acc_1: (99.25%) (45862/46208)\n",
      "Epoch: 192 | Batch_idx: 370 |  Loss_1: (0.0224) | Acc_1: (99.24%) (47129/47488)\n",
      "Epoch: 192 | Batch_idx: 380 |  Loss_1: (0.0222) | Acc_1: (99.26%) (48405/48768)\n",
      "Epoch: 192 | Batch_idx: 390 |  Loss_1: (0.0223) | Acc_1: (99.25%) (49623/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4630) | Acc: (91.39%) (9139/10000)\n",
      "Epoch: 193 | Batch_idx: 0 |  Loss_1: (0.0134) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 193 | Batch_idx: 10 |  Loss_1: (0.0292) | Acc_1: (98.86%) (1392/1408)\n",
      "Epoch: 193 | Batch_idx: 20 |  Loss_1: (0.0238) | Acc_1: (99.14%) (2665/2688)\n",
      "Epoch: 193 | Batch_idx: 30 |  Loss_1: (0.0205) | Acc_1: (99.32%) (3941/3968)\n",
      "Epoch: 193 | Batch_idx: 40 |  Loss_1: (0.0234) | Acc_1: (99.22%) (5207/5248)\n",
      "Epoch: 193 | Batch_idx: 50 |  Loss_1: (0.0225) | Acc_1: (99.25%) (6479/6528)\n",
      "Epoch: 193 | Batch_idx: 60 |  Loss_1: (0.0219) | Acc_1: (99.27%) (7751/7808)\n",
      "Epoch: 193 | Batch_idx: 70 |  Loss_1: (0.0209) | Acc_1: (99.30%) (9024/9088)\n",
      "Epoch: 193 | Batch_idx: 80 |  Loss_1: (0.0214) | Acc_1: (99.27%) (10292/10368)\n",
      "Epoch: 193 | Batch_idx: 90 |  Loss_1: (0.0222) | Acc_1: (99.26%) (11562/11648)\n",
      "Epoch: 193 | Batch_idx: 100 |  Loss_1: (0.0213) | Acc_1: (99.30%) (12837/12928)\n",
      "Epoch: 193 | Batch_idx: 110 |  Loss_1: (0.0220) | Acc_1: (99.29%) (14107/14208)\n",
      "Epoch: 193 | Batch_idx: 120 |  Loss_1: (0.0225) | Acc_1: (99.26%) (15374/15488)\n",
      "Epoch: 193 | Batch_idx: 130 |  Loss_1: (0.0232) | Acc_1: (99.25%) (16643/16768)\n",
      "Epoch: 193 | Batch_idx: 140 |  Loss_1: (0.0228) | Acc_1: (99.26%) (17915/18048)\n",
      "Epoch: 193 | Batch_idx: 150 |  Loss_1: (0.0227) | Acc_1: (99.26%) (19185/19328)\n",
      "Epoch: 193 | Batch_idx: 160 |  Loss_1: (0.0230) | Acc_1: (99.26%) (20456/20608)\n",
      "Epoch: 193 | Batch_idx: 170 |  Loss_1: (0.0230) | Acc_1: (99.26%) (21726/21888)\n",
      "Epoch: 193 | Batch_idx: 180 |  Loss_1: (0.0230) | Acc_1: (99.27%) (22999/23168)\n",
      "Epoch: 193 | Batch_idx: 190 |  Loss_1: (0.0229) | Acc_1: (99.27%) (24269/24448)\n",
      "Epoch: 193 | Batch_idx: 200 |  Loss_1: (0.0236) | Acc_1: (99.25%) (25535/25728)\n",
      "Epoch: 193 | Batch_idx: 210 |  Loss_1: (0.0230) | Acc_1: (99.27%) (26811/27008)\n",
      "Epoch: 193 | Batch_idx: 220 |  Loss_1: (0.0226) | Acc_1: (99.27%) (28082/28288)\n",
      "Epoch: 193 | Batch_idx: 230 |  Loss_1: (0.0223) | Acc_1: (99.29%) (29358/29568)\n",
      "Epoch: 193 | Batch_idx: 240 |  Loss_1: (0.0227) | Acc_1: (99.27%) (30624/30848)\n",
      "Epoch: 193 | Batch_idx: 250 |  Loss_1: (0.0227) | Acc_1: (99.27%) (31894/32128)\n",
      "Epoch: 193 | Batch_idx: 260 |  Loss_1: (0.0223) | Acc_1: (99.29%) (33171/33408)\n",
      "Epoch: 193 | Batch_idx: 270 |  Loss_1: (0.0223) | Acc_1: (99.28%) (34439/34688)\n",
      "Epoch: 193 | Batch_idx: 280 |  Loss_1: (0.0223) | Acc_1: (99.28%) (35710/35968)\n",
      "Epoch: 193 | Batch_idx: 290 |  Loss_1: (0.0222) | Acc_1: (99.29%) (36982/37248)\n",
      "Epoch: 193 | Batch_idx: 300 |  Loss_1: (0.0220) | Acc_1: (99.29%) (38255/38528)\n",
      "Epoch: 193 | Batch_idx: 310 |  Loss_1: (0.0224) | Acc_1: (99.28%) (39521/39808)\n",
      "Epoch: 193 | Batch_idx: 320 |  Loss_1: (0.0224) | Acc_1: (99.28%) (40793/41088)\n",
      "Epoch: 193 | Batch_idx: 330 |  Loss_1: (0.0222) | Acc_1: (99.28%) (42064/42368)\n",
      "Epoch: 193 | Batch_idx: 340 |  Loss_1: (0.0222) | Acc_1: (99.29%) (43338/43648)\n",
      "Epoch: 193 | Batch_idx: 350 |  Loss_1: (0.0223) | Acc_1: (99.28%) (44605/44928)\n",
      "Epoch: 193 | Batch_idx: 360 |  Loss_1: (0.0225) | Acc_1: (99.27%) (45872/46208)\n",
      "Epoch: 193 | Batch_idx: 370 |  Loss_1: (0.0224) | Acc_1: (99.27%) (47139/47488)\n",
      "Epoch: 193 | Batch_idx: 380 |  Loss_1: (0.0226) | Acc_1: (99.26%) (48407/48768)\n",
      "Epoch: 193 | Batch_idx: 390 |  Loss_1: (0.0225) | Acc_1: (99.26%) (49628/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4738) | Acc: (91.89%) (9189/10000)\n",
      "Epoch: 194 | Batch_idx: 0 |  Loss_1: (0.0316) | Acc_1: (97.66%) (125/128)\n",
      "Epoch: 194 | Batch_idx: 10 |  Loss_1: (0.0243) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 194 | Batch_idx: 20 |  Loss_1: (0.0202) | Acc_1: (99.29%) (2669/2688)\n",
      "Epoch: 194 | Batch_idx: 30 |  Loss_1: (0.0200) | Acc_1: (99.34%) (3942/3968)\n",
      "Epoch: 194 | Batch_idx: 40 |  Loss_1: (0.0189) | Acc_1: (99.35%) (5214/5248)\n",
      "Epoch: 194 | Batch_idx: 50 |  Loss_1: (0.0186) | Acc_1: (99.37%) (6487/6528)\n",
      "Epoch: 194 | Batch_idx: 60 |  Loss_1: (0.0181) | Acc_1: (99.40%) (7761/7808)\n",
      "Epoch: 194 | Batch_idx: 70 |  Loss_1: (0.0181) | Acc_1: (99.43%) (9036/9088)\n",
      "Epoch: 194 | Batch_idx: 80 |  Loss_1: (0.0177) | Acc_1: (99.42%) (10308/10368)\n",
      "Epoch: 194 | Batch_idx: 90 |  Loss_1: (0.0179) | Acc_1: (99.42%) (11580/11648)\n",
      "Epoch: 194 | Batch_idx: 100 |  Loss_1: (0.0174) | Acc_1: (99.44%) (12855/12928)\n",
      "Epoch: 194 | Batch_idx: 110 |  Loss_1: (0.0176) | Acc_1: (99.42%) (14126/14208)\n",
      "Epoch: 194 | Batch_idx: 120 |  Loss_1: (0.0173) | Acc_1: (99.44%) (15401/15488)\n",
      "Epoch: 194 | Batch_idx: 130 |  Loss_1: (0.0180) | Acc_1: (99.42%) (16670/16768)\n",
      "Epoch: 194 | Batch_idx: 140 |  Loss_1: (0.0181) | Acc_1: (99.41%) (17942/18048)\n",
      "Epoch: 194 | Batch_idx: 150 |  Loss_1: (0.0187) | Acc_1: (99.39%) (19210/19328)\n",
      "Epoch: 194 | Batch_idx: 160 |  Loss_1: (0.0195) | Acc_1: (99.36%) (20477/20608)\n",
      "Epoch: 194 | Batch_idx: 170 |  Loss_1: (0.0203) | Acc_1: (99.34%) (21743/21888)\n",
      "Epoch: 194 | Batch_idx: 180 |  Loss_1: (0.0205) | Acc_1: (99.34%) (23015/23168)\n",
      "Epoch: 194 | Batch_idx: 190 |  Loss_1: (0.0209) | Acc_1: (99.33%) (24283/24448)\n",
      "Epoch: 194 | Batch_idx: 200 |  Loss_1: (0.0214) | Acc_1: (99.31%) (25551/25728)\n",
      "Epoch: 194 | Batch_idx: 210 |  Loss_1: (0.0218) | Acc_1: (99.29%) (26815/27008)\n",
      "Epoch: 194 | Batch_idx: 220 |  Loss_1: (0.0218) | Acc_1: (99.29%) (28087/28288)\n",
      "Epoch: 194 | Batch_idx: 230 |  Loss_1: (0.0221) | Acc_1: (99.28%) (29356/29568)\n",
      "Epoch: 194 | Batch_idx: 240 |  Loss_1: (0.0218) | Acc_1: (99.28%) (30627/30848)\n",
      "Epoch: 194 | Batch_idx: 250 |  Loss_1: (0.0221) | Acc_1: (99.26%) (31891/32128)\n",
      "Epoch: 194 | Batch_idx: 260 |  Loss_1: (0.0221) | Acc_1: (99.25%) (33157/33408)\n",
      "Epoch: 194 | Batch_idx: 270 |  Loss_1: (0.0219) | Acc_1: (99.26%) (34430/34688)\n",
      "Epoch: 194 | Batch_idx: 280 |  Loss_1: (0.0224) | Acc_1: (99.25%) (35699/35968)\n",
      "Epoch: 194 | Batch_idx: 290 |  Loss_1: (0.0223) | Acc_1: (99.25%) (36967/37248)\n",
      "Epoch: 194 | Batch_idx: 300 |  Loss_1: (0.0220) | Acc_1: (99.25%) (38240/38528)\n",
      "Epoch: 194 | Batch_idx: 310 |  Loss_1: (0.0223) | Acc_1: (99.24%) (39506/39808)\n",
      "Epoch: 194 | Batch_idx: 320 |  Loss_1: (0.0221) | Acc_1: (99.26%) (40783/41088)\n",
      "Epoch: 194 | Batch_idx: 330 |  Loss_1: (0.0222) | Acc_1: (99.26%) (42054/42368)\n",
      "Epoch: 194 | Batch_idx: 340 |  Loss_1: (0.0223) | Acc_1: (99.25%) (43322/43648)\n",
      "Epoch: 194 | Batch_idx: 350 |  Loss_1: (0.0223) | Acc_1: (99.26%) (44595/44928)\n",
      "Epoch: 194 | Batch_idx: 360 |  Loss_1: (0.0223) | Acc_1: (99.26%) (45868/46208)\n",
      "Epoch: 194 | Batch_idx: 370 |  Loss_1: (0.0225) | Acc_1: (99.26%) (47137/47488)\n",
      "Epoch: 194 | Batch_idx: 380 |  Loss_1: (0.0224) | Acc_1: (99.27%) (48410/48768)\n",
      "Epoch: 194 | Batch_idx: 390 |  Loss_1: (0.0222) | Acc_1: (99.27%) (49636/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4905) | Acc: (91.25%) (9125/10000)\n",
      "Epoch: 195 | Batch_idx: 0 |  Loss_1: (0.0166) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 195 | Batch_idx: 10 |  Loss_1: (0.0231) | Acc_1: (99.22%) (1397/1408)\n",
      "Epoch: 195 | Batch_idx: 20 |  Loss_1: (0.0283) | Acc_1: (99.07%) (2663/2688)\n",
      "Epoch: 195 | Batch_idx: 30 |  Loss_1: (0.0291) | Acc_1: (98.94%) (3926/3968)\n",
      "Epoch: 195 | Batch_idx: 40 |  Loss_1: (0.0264) | Acc_1: (98.99%) (5195/5248)\n",
      "Epoch: 195 | Batch_idx: 50 |  Loss_1: (0.0257) | Acc_1: (99.05%) (6466/6528)\n",
      "Epoch: 195 | Batch_idx: 60 |  Loss_1: (0.0243) | Acc_1: (99.12%) (7739/7808)\n",
      "Epoch: 195 | Batch_idx: 70 |  Loss_1: (0.0249) | Acc_1: (99.11%) (9007/9088)\n",
      "Epoch: 195 | Batch_idx: 80 |  Loss_1: (0.0231) | Acc_1: (99.19%) (10284/10368)\n",
      "Epoch: 195 | Batch_idx: 90 |  Loss_1: (0.0237) | Acc_1: (99.16%) (11550/11648)\n",
      "Epoch: 195 | Batch_idx: 100 |  Loss_1: (0.0234) | Acc_1: (99.15%) (12818/12928)\n",
      "Epoch: 195 | Batch_idx: 110 |  Loss_1: (0.0226) | Acc_1: (99.16%) (14089/14208)\n",
      "Epoch: 195 | Batch_idx: 120 |  Loss_1: (0.0226) | Acc_1: (99.17%) (15360/15488)\n",
      "Epoch: 195 | Batch_idx: 130 |  Loss_1: (0.0232) | Acc_1: (99.17%) (16629/16768)\n",
      "Epoch: 195 | Batch_idx: 140 |  Loss_1: (0.0236) | Acc_1: (99.17%) (17899/18048)\n",
      "Epoch: 195 | Batch_idx: 150 |  Loss_1: (0.0228) | Acc_1: (99.20%) (19174/19328)\n",
      "Epoch: 195 | Batch_idx: 160 |  Loss_1: (0.0229) | Acc_1: (99.21%) (20445/20608)\n",
      "Epoch: 195 | Batch_idx: 170 |  Loss_1: (0.0236) | Acc_1: (99.17%) (21706/21888)\n",
      "Epoch: 195 | Batch_idx: 180 |  Loss_1: (0.0234) | Acc_1: (99.18%) (22979/23168)\n",
      "Epoch: 195 | Batch_idx: 190 |  Loss_1: (0.0237) | Acc_1: (99.17%) (24244/24448)\n",
      "Epoch: 195 | Batch_idx: 200 |  Loss_1: (0.0238) | Acc_1: (99.18%) (25517/25728)\n",
      "Epoch: 195 | Batch_idx: 210 |  Loss_1: (0.0238) | Acc_1: (99.17%) (26785/27008)\n",
      "Epoch: 195 | Batch_idx: 220 |  Loss_1: (0.0237) | Acc_1: (99.18%) (28057/28288)\n",
      "Epoch: 195 | Batch_idx: 230 |  Loss_1: (0.0240) | Acc_1: (99.18%) (29325/29568)\n",
      "Epoch: 195 | Batch_idx: 240 |  Loss_1: (0.0241) | Acc_1: (99.18%) (30595/30848)\n",
      "Epoch: 195 | Batch_idx: 250 |  Loss_1: (0.0243) | Acc_1: (99.18%) (31864/32128)\n",
      "Epoch: 195 | Batch_idx: 260 |  Loss_1: (0.0245) | Acc_1: (99.16%) (33129/33408)\n",
      "Epoch: 195 | Batch_idx: 270 |  Loss_1: (0.0248) | Acc_1: (99.16%) (34398/34688)\n",
      "Epoch: 195 | Batch_idx: 280 |  Loss_1: (0.0251) | Acc_1: (99.15%) (35662/35968)\n",
      "Epoch: 195 | Batch_idx: 290 |  Loss_1: (0.0249) | Acc_1: (99.16%) (36935/37248)\n",
      "Epoch: 195 | Batch_idx: 300 |  Loss_1: (0.0248) | Acc_1: (99.17%) (38208/38528)\n",
      "Epoch: 195 | Batch_idx: 310 |  Loss_1: (0.0253) | Acc_1: (99.16%) (39472/39808)\n",
      "Epoch: 195 | Batch_idx: 320 |  Loss_1: (0.0254) | Acc_1: (99.15%) (40740/41088)\n",
      "Epoch: 195 | Batch_idx: 330 |  Loss_1: (0.0254) | Acc_1: (99.15%) (42009/42368)\n",
      "Epoch: 195 | Batch_idx: 340 |  Loss_1: (0.0255) | Acc_1: (99.15%) (43275/43648)\n",
      "Epoch: 195 | Batch_idx: 350 |  Loss_1: (0.0254) | Acc_1: (99.15%) (44544/44928)\n",
      "Epoch: 195 | Batch_idx: 360 |  Loss_1: (0.0258) | Acc_1: (99.13%) (45807/46208)\n",
      "Epoch: 195 | Batch_idx: 370 |  Loss_1: (0.0257) | Acc_1: (99.14%) (47078/47488)\n",
      "Epoch: 195 | Batch_idx: 380 |  Loss_1: (0.0255) | Acc_1: (99.14%) (48351/48768)\n",
      "Epoch: 195 | Batch_idx: 390 |  Loss_1: (0.0254) | Acc_1: (99.14%) (49572/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4915) | Acc: (91.16%) (9116/10000)\n",
      "Epoch: 196 | Batch_idx: 0 |  Loss_1: (0.0284) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 196 | Batch_idx: 10 |  Loss_1: (0.0258) | Acc_1: (99.15%) (1396/1408)\n",
      "Epoch: 196 | Batch_idx: 20 |  Loss_1: (0.0221) | Acc_1: (99.22%) (2667/2688)\n",
      "Epoch: 196 | Batch_idx: 30 |  Loss_1: (0.0221) | Acc_1: (99.29%) (3940/3968)\n",
      "Epoch: 196 | Batch_idx: 40 |  Loss_1: (0.0246) | Acc_1: (99.31%) (5212/5248)\n",
      "Epoch: 196 | Batch_idx: 50 |  Loss_1: (0.0274) | Acc_1: (99.22%) (6477/6528)\n",
      "Epoch: 196 | Batch_idx: 60 |  Loss_1: (0.0266) | Acc_1: (99.19%) (7745/7808)\n",
      "Epoch: 196 | Batch_idx: 70 |  Loss_1: (0.0267) | Acc_1: (99.17%) (9013/9088)\n",
      "Epoch: 196 | Batch_idx: 80 |  Loss_1: (0.0263) | Acc_1: (99.19%) (10284/10368)\n",
      "Epoch: 196 | Batch_idx: 90 |  Loss_1: (0.0258) | Acc_1: (99.19%) (11554/11648)\n",
      "Epoch: 196 | Batch_idx: 100 |  Loss_1: (0.0258) | Acc_1: (99.20%) (12824/12928)\n",
      "Epoch: 196 | Batch_idx: 110 |  Loss_1: (0.0254) | Acc_1: (99.20%) (14094/14208)\n",
      "Epoch: 196 | Batch_idx: 120 |  Loss_1: (0.0246) | Acc_1: (99.22%) (15367/15488)\n",
      "Epoch: 196 | Batch_idx: 130 |  Loss_1: (0.0242) | Acc_1: (99.24%) (16640/16768)\n",
      "Epoch: 196 | Batch_idx: 140 |  Loss_1: (0.0239) | Acc_1: (99.24%) (17911/18048)\n",
      "Epoch: 196 | Batch_idx: 150 |  Loss_1: (0.0234) | Acc_1: (99.25%) (19183/19328)\n",
      "Epoch: 196 | Batch_idx: 160 |  Loss_1: (0.0235) | Acc_1: (99.24%) (20452/20608)\n",
      "Epoch: 196 | Batch_idx: 170 |  Loss_1: (0.0235) | Acc_1: (99.26%) (21725/21888)\n",
      "Epoch: 196 | Batch_idx: 180 |  Loss_1: (0.0237) | Acc_1: (99.23%) (22990/23168)\n",
      "Epoch: 196 | Batch_idx: 190 |  Loss_1: (0.0239) | Acc_1: (99.22%) (24257/24448)\n",
      "Epoch: 196 | Batch_idx: 200 |  Loss_1: (0.0237) | Acc_1: (99.24%) (25532/25728)\n",
      "Epoch: 196 | Batch_idx: 210 |  Loss_1: (0.0240) | Acc_1: (99.23%) (26801/27008)\n",
      "Epoch: 196 | Batch_idx: 220 |  Loss_1: (0.0238) | Acc_1: (99.23%) (28071/28288)\n",
      "Epoch: 196 | Batch_idx: 230 |  Loss_1: (0.0239) | Acc_1: (99.24%) (29342/29568)\n",
      "Epoch: 196 | Batch_idx: 240 |  Loss_1: (0.0236) | Acc_1: (99.24%) (30613/30848)\n",
      "Epoch: 196 | Batch_idx: 250 |  Loss_1: (0.0236) | Acc_1: (99.24%) (31884/32128)\n",
      "Epoch: 196 | Batch_idx: 260 |  Loss_1: (0.0236) | Acc_1: (99.23%) (33152/33408)\n",
      "Epoch: 196 | Batch_idx: 270 |  Loss_1: (0.0237) | Acc_1: (99.23%) (34421/34688)\n",
      "Epoch: 196 | Batch_idx: 280 |  Loss_1: (0.0239) | Acc_1: (99.23%) (35690/35968)\n",
      "Epoch: 196 | Batch_idx: 290 |  Loss_1: (0.0241) | Acc_1: (99.22%) (36956/37248)\n",
      "Epoch: 196 | Batch_idx: 300 |  Loss_1: (0.0245) | Acc_1: (99.21%) (38223/38528)\n",
      "Epoch: 196 | Batch_idx: 310 |  Loss_1: (0.0243) | Acc_1: (99.21%) (39494/39808)\n",
      "Epoch: 196 | Batch_idx: 320 |  Loss_1: (0.0242) | Acc_1: (99.21%) (40764/41088)\n",
      "Epoch: 196 | Batch_idx: 330 |  Loss_1: (0.0243) | Acc_1: (99.21%) (42034/42368)\n",
      "Epoch: 196 | Batch_idx: 340 |  Loss_1: (0.0240) | Acc_1: (99.22%) (43307/43648)\n",
      "Epoch: 196 | Batch_idx: 350 |  Loss_1: (0.0239) | Acc_1: (99.22%) (44578/44928)\n",
      "Epoch: 196 | Batch_idx: 360 |  Loss_1: (0.0239) | Acc_1: (99.22%) (45848/46208)\n",
      "Epoch: 196 | Batch_idx: 370 |  Loss_1: (0.0239) | Acc_1: (99.22%) (47119/47488)\n",
      "Epoch: 196 | Batch_idx: 380 |  Loss_1: (0.0239) | Acc_1: (99.23%) (48391/48768)\n",
      "Epoch: 196 | Batch_idx: 390 |  Loss_1: (0.0241) | Acc_1: (99.23%) (49614/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5183) | Acc: (91.13%) (9113/10000)\n",
      "Epoch: 197 | Batch_idx: 0 |  Loss_1: (0.0277) | Acc_1: (98.44%) (126/128)\n",
      "Epoch: 197 | Batch_idx: 10 |  Loss_1: (0.0199) | Acc_1: (99.43%) (1400/1408)\n",
      "Epoch: 197 | Batch_idx: 20 |  Loss_1: (0.0224) | Acc_1: (99.44%) (2673/2688)\n",
      "Epoch: 197 | Batch_idx: 30 |  Loss_1: (0.0219) | Acc_1: (99.40%) (3944/3968)\n",
      "Epoch: 197 | Batch_idx: 40 |  Loss_1: (0.0214) | Acc_1: (99.33%) (5213/5248)\n",
      "Epoch: 197 | Batch_idx: 50 |  Loss_1: (0.0207) | Acc_1: (99.33%) (6484/6528)\n",
      "Epoch: 197 | Batch_idx: 60 |  Loss_1: (0.0220) | Acc_1: (99.28%) (7752/7808)\n",
      "Epoch: 197 | Batch_idx: 70 |  Loss_1: (0.0208) | Acc_1: (99.30%) (9024/9088)\n",
      "Epoch: 197 | Batch_idx: 80 |  Loss_1: (0.0216) | Acc_1: (99.32%) (10297/10368)\n",
      "Epoch: 197 | Batch_idx: 90 |  Loss_1: (0.0212) | Acc_1: (99.31%) (11568/11648)\n",
      "Epoch: 197 | Batch_idx: 100 |  Loss_1: (0.0216) | Acc_1: (99.30%) (12837/12928)\n",
      "Epoch: 197 | Batch_idx: 110 |  Loss_1: (0.0213) | Acc_1: (99.30%) (14109/14208)\n",
      "Epoch: 197 | Batch_idx: 120 |  Loss_1: (0.0209) | Acc_1: (99.32%) (15382/15488)\n",
      "Epoch: 197 | Batch_idx: 130 |  Loss_1: (0.0206) | Acc_1: (99.31%) (16653/16768)\n",
      "Epoch: 197 | Batch_idx: 140 |  Loss_1: (0.0207) | Acc_1: (99.32%) (17926/18048)\n",
      "Epoch: 197 | Batch_idx: 150 |  Loss_1: (0.0208) | Acc_1: (99.30%) (19193/19328)\n",
      "Epoch: 197 | Batch_idx: 160 |  Loss_1: (0.0209) | Acc_1: (99.29%) (20462/20608)\n",
      "Epoch: 197 | Batch_idx: 170 |  Loss_1: (0.0208) | Acc_1: (99.30%) (21734/21888)\n",
      "Epoch: 197 | Batch_idx: 180 |  Loss_1: (0.0207) | Acc_1: (99.30%) (23006/23168)\n",
      "Epoch: 197 | Batch_idx: 190 |  Loss_1: (0.0206) | Acc_1: (99.31%) (24280/24448)\n",
      "Epoch: 197 | Batch_idx: 200 |  Loss_1: (0.0207) | Acc_1: (99.32%) (25552/25728)\n",
      "Epoch: 197 | Batch_idx: 210 |  Loss_1: (0.0212) | Acc_1: (99.28%) (26813/27008)\n",
      "Epoch: 197 | Batch_idx: 220 |  Loss_1: (0.0211) | Acc_1: (99.27%) (28082/28288)\n",
      "Epoch: 197 | Batch_idx: 230 |  Loss_1: (0.0208) | Acc_1: (99.29%) (29357/29568)\n",
      "Epoch: 197 | Batch_idx: 240 |  Loss_1: (0.0206) | Acc_1: (99.30%) (30631/30848)\n",
      "Epoch: 197 | Batch_idx: 250 |  Loss_1: (0.0206) | Acc_1: (99.30%) (31904/32128)\n",
      "Epoch: 197 | Batch_idx: 260 |  Loss_1: (0.0206) | Acc_1: (99.31%) (33177/33408)\n",
      "Epoch: 197 | Batch_idx: 270 |  Loss_1: (0.0207) | Acc_1: (99.30%) (34446/34688)\n",
      "Epoch: 197 | Batch_idx: 280 |  Loss_1: (0.0208) | Acc_1: (99.29%) (35712/35968)\n",
      "Epoch: 197 | Batch_idx: 290 |  Loss_1: (0.0207) | Acc_1: (99.29%) (36984/37248)\n",
      "Epoch: 197 | Batch_idx: 300 |  Loss_1: (0.0208) | Acc_1: (99.29%) (38253/38528)\n",
      "Epoch: 197 | Batch_idx: 310 |  Loss_1: (0.0209) | Acc_1: (99.28%) (39522/39808)\n",
      "Epoch: 197 | Batch_idx: 320 |  Loss_1: (0.0208) | Acc_1: (99.28%) (40793/41088)\n",
      "Epoch: 197 | Batch_idx: 330 |  Loss_1: (0.0208) | Acc_1: (99.28%) (42065/42368)\n",
      "Epoch: 197 | Batch_idx: 340 |  Loss_1: (0.0209) | Acc_1: (99.28%) (43334/43648)\n",
      "Epoch: 197 | Batch_idx: 350 |  Loss_1: (0.0210) | Acc_1: (99.28%) (44603/44928)\n",
      "Epoch: 197 | Batch_idx: 360 |  Loss_1: (0.0211) | Acc_1: (99.28%) (45873/46208)\n",
      "Epoch: 197 | Batch_idx: 370 |  Loss_1: (0.0212) | Acc_1: (99.27%) (47143/47488)\n",
      "Epoch: 197 | Batch_idx: 380 |  Loss_1: (0.0214) | Acc_1: (99.26%) (48409/48768)\n",
      "Epoch: 197 | Batch_idx: 390 |  Loss_1: (0.0216) | Acc_1: (99.26%) (49628/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4999) | Acc: (91.30%) (9130/10000)\n",
      "Epoch: 198 | Batch_idx: 0 |  Loss_1: (0.0163) | Acc_1: (99.22%) (127/128)\n",
      "Epoch: 198 | Batch_idx: 10 |  Loss_1: (0.0178) | Acc_1: (99.36%) (1399/1408)\n",
      "Epoch: 198 | Batch_idx: 20 |  Loss_1: (0.0257) | Acc_1: (99.26%) (2668/2688)\n",
      "Epoch: 198 | Batch_idx: 30 |  Loss_1: (0.0247) | Acc_1: (99.19%) (3936/3968)\n",
      "Epoch: 198 | Batch_idx: 40 |  Loss_1: (0.0285) | Acc_1: (99.07%) (5199/5248)\n",
      "Epoch: 198 | Batch_idx: 50 |  Loss_1: (0.0277) | Acc_1: (99.03%) (6465/6528)\n",
      "Epoch: 198 | Batch_idx: 60 |  Loss_1: (0.0255) | Acc_1: (99.08%) (7736/7808)\n",
      "Epoch: 198 | Batch_idx: 70 |  Loss_1: (0.0259) | Acc_1: (99.08%) (9004/9088)\n",
      "Epoch: 198 | Batch_idx: 80 |  Loss_1: (0.0253) | Acc_1: (99.12%) (10277/10368)\n",
      "Epoch: 198 | Batch_idx: 90 |  Loss_1: (0.0244) | Acc_1: (99.18%) (11552/11648)\n",
      "Epoch: 198 | Batch_idx: 100 |  Loss_1: (0.0240) | Acc_1: (99.18%) (12822/12928)\n",
      "Epoch: 198 | Batch_idx: 110 |  Loss_1: (0.0242) | Acc_1: (99.18%) (14091/14208)\n",
      "Epoch: 198 | Batch_idx: 120 |  Loss_1: (0.0242) | Acc_1: (99.19%) (15362/15488)\n",
      "Epoch: 198 | Batch_idx: 130 |  Loss_1: (0.0243) | Acc_1: (99.18%) (16631/16768)\n",
      "Epoch: 198 | Batch_idx: 140 |  Loss_1: (0.0239) | Acc_1: (99.20%) (17904/18048)\n",
      "Epoch: 198 | Batch_idx: 150 |  Loss_1: (0.0241) | Acc_1: (99.20%) (19173/19328)\n",
      "Epoch: 198 | Batch_idx: 160 |  Loss_1: (0.0251) | Acc_1: (99.18%) (20440/20608)\n",
      "Epoch: 198 | Batch_idx: 170 |  Loss_1: (0.0249) | Acc_1: (99.19%) (21711/21888)\n",
      "Epoch: 198 | Batch_idx: 180 |  Loss_1: (0.0250) | Acc_1: (99.19%) (22980/23168)\n",
      "Epoch: 198 | Batch_idx: 190 |  Loss_1: (0.0245) | Acc_1: (99.21%) (24254/24448)\n",
      "Epoch: 198 | Batch_idx: 200 |  Loss_1: (0.0246) | Acc_1: (99.19%) (25520/25728)\n",
      "Epoch: 198 | Batch_idx: 210 |  Loss_1: (0.0246) | Acc_1: (99.20%) (26791/27008)\n",
      "Epoch: 198 | Batch_idx: 220 |  Loss_1: (0.0250) | Acc_1: (99.17%) (28054/28288)\n",
      "Epoch: 198 | Batch_idx: 230 |  Loss_1: (0.0251) | Acc_1: (99.17%) (29323/29568)\n",
      "Epoch: 198 | Batch_idx: 240 |  Loss_1: (0.0258) | Acc_1: (99.14%) (30582/30848)\n",
      "Epoch: 198 | Batch_idx: 250 |  Loss_1: (0.0256) | Acc_1: (99.15%) (31854/32128)\n",
      "Epoch: 198 | Batch_idx: 260 |  Loss_1: (0.0254) | Acc_1: (99.15%) (33123/33408)\n",
      "Epoch: 198 | Batch_idx: 270 |  Loss_1: (0.0256) | Acc_1: (99.14%) (34389/34688)\n",
      "Epoch: 198 | Batch_idx: 280 |  Loss_1: (0.0257) | Acc_1: (99.14%) (35660/35968)\n",
      "Epoch: 198 | Batch_idx: 290 |  Loss_1: (0.0254) | Acc_1: (99.15%) (36931/37248)\n",
      "Epoch: 198 | Batch_idx: 300 |  Loss_1: (0.0252) | Acc_1: (99.16%) (38204/38528)\n",
      "Epoch: 198 | Batch_idx: 310 |  Loss_1: (0.0251) | Acc_1: (99.17%) (39476/39808)\n",
      "Epoch: 198 | Batch_idx: 320 |  Loss_1: (0.0248) | Acc_1: (99.18%) (40752/41088)\n",
      "Epoch: 198 | Batch_idx: 330 |  Loss_1: (0.0246) | Acc_1: (99.20%) (42027/42368)\n",
      "Epoch: 198 | Batch_idx: 340 |  Loss_1: (0.0244) | Acc_1: (99.21%) (43301/43648)\n",
      "Epoch: 198 | Batch_idx: 350 |  Loss_1: (0.0240) | Acc_1: (99.22%) (44577/44928)\n",
      "Epoch: 198 | Batch_idx: 360 |  Loss_1: (0.0240) | Acc_1: (99.22%) (45848/46208)\n",
      "Epoch: 198 | Batch_idx: 370 |  Loss_1: (0.0241) | Acc_1: (99.22%) (47116/47488)\n",
      "Epoch: 198 | Batch_idx: 380 |  Loss_1: (0.0239) | Acc_1: (99.22%) (48389/48768)\n",
      "Epoch: 198 | Batch_idx: 390 |  Loss_1: (0.0239) | Acc_1: (99.22%) (49608/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5239) | Acc: (91.12%) (9112/10000)\n",
      "Epoch: 199 | Batch_idx: 0 |  Loss_1: (0.0061) | Acc_1: (100.00%) (128/128)\n",
      "Epoch: 199 | Batch_idx: 10 |  Loss_1: (0.0225) | Acc_1: (99.50%) (1401/1408)\n",
      "Epoch: 199 | Batch_idx: 20 |  Loss_1: (0.0238) | Acc_1: (99.40%) (2672/2688)\n",
      "Epoch: 199 | Batch_idx: 30 |  Loss_1: (0.0230) | Acc_1: (99.37%) (3943/3968)\n",
      "Epoch: 199 | Batch_idx: 40 |  Loss_1: (0.0207) | Acc_1: (99.45%) (5219/5248)\n",
      "Epoch: 199 | Batch_idx: 50 |  Loss_1: (0.0195) | Acc_1: (99.46%) (6493/6528)\n",
      "Epoch: 199 | Batch_idx: 60 |  Loss_1: (0.0199) | Acc_1: (99.41%) (7762/7808)\n",
      "Epoch: 199 | Batch_idx: 70 |  Loss_1: (0.0199) | Acc_1: (99.43%) (9036/9088)\n",
      "Epoch: 199 | Batch_idx: 80 |  Loss_1: (0.0199) | Acc_1: (99.42%) (10308/10368)\n",
      "Epoch: 199 | Batch_idx: 90 |  Loss_1: (0.0196) | Acc_1: (99.41%) (11579/11648)\n",
      "Epoch: 199 | Batch_idx: 100 |  Loss_1: (0.0191) | Acc_1: (99.40%) (12851/12928)\n",
      "Epoch: 199 | Batch_idx: 110 |  Loss_1: (0.0207) | Acc_1: (99.38%) (14120/14208)\n",
      "Epoch: 199 | Batch_idx: 120 |  Loss_1: (0.0211) | Acc_1: (99.35%) (15387/15488)\n",
      "Epoch: 199 | Batch_idx: 130 |  Loss_1: (0.0205) | Acc_1: (99.36%) (16661/16768)\n",
      "Epoch: 199 | Batch_idx: 140 |  Loss_1: (0.0206) | Acc_1: (99.36%) (17932/18048)\n",
      "Epoch: 199 | Batch_idx: 150 |  Loss_1: (0.0206) | Acc_1: (99.35%) (19203/19328)\n",
      "Epoch: 199 | Batch_idx: 160 |  Loss_1: (0.0211) | Acc_1: (99.34%) (20472/20608)\n",
      "Epoch: 199 | Batch_idx: 170 |  Loss_1: (0.0211) | Acc_1: (99.33%) (21741/21888)\n",
      "Epoch: 199 | Batch_idx: 180 |  Loss_1: (0.0221) | Acc_1: (99.30%) (23006/23168)\n",
      "Epoch: 199 | Batch_idx: 190 |  Loss_1: (0.0224) | Acc_1: (99.29%) (24275/24448)\n",
      "Epoch: 199 | Batch_idx: 200 |  Loss_1: (0.0221) | Acc_1: (99.29%) (25546/25728)\n",
      "Epoch: 199 | Batch_idx: 210 |  Loss_1: (0.0218) | Acc_1: (99.30%) (26819/27008)\n",
      "Epoch: 199 | Batch_idx: 220 |  Loss_1: (0.0218) | Acc_1: (99.29%) (28087/28288)\n",
      "Epoch: 199 | Batch_idx: 230 |  Loss_1: (0.0217) | Acc_1: (99.29%) (29358/29568)\n",
      "Epoch: 199 | Batch_idx: 240 |  Loss_1: (0.0220) | Acc_1: (99.28%) (30626/30848)\n",
      "Epoch: 199 | Batch_idx: 250 |  Loss_1: (0.0217) | Acc_1: (99.29%) (31901/32128)\n",
      "Epoch: 199 | Batch_idx: 260 |  Loss_1: (0.0219) | Acc_1: (99.28%) (33169/33408)\n",
      "Epoch: 199 | Batch_idx: 270 |  Loss_1: (0.0226) | Acc_1: (99.28%) (34437/34688)\n",
      "Epoch: 199 | Batch_idx: 280 |  Loss_1: (0.0225) | Acc_1: (99.27%) (35707/35968)\n",
      "Epoch: 199 | Batch_idx: 290 |  Loss_1: (0.0229) | Acc_1: (99.25%) (36969/37248)\n",
      "Epoch: 199 | Batch_idx: 300 |  Loss_1: (0.0229) | Acc_1: (99.24%) (38235/38528)\n",
      "Epoch: 199 | Batch_idx: 310 |  Loss_1: (0.0227) | Acc_1: (99.25%) (39509/39808)\n",
      "Epoch: 199 | Batch_idx: 320 |  Loss_1: (0.0227) | Acc_1: (99.25%) (40781/41088)\n",
      "Epoch: 199 | Batch_idx: 330 |  Loss_1: (0.0227) | Acc_1: (99.26%) (42054/42368)\n",
      "Epoch: 199 | Batch_idx: 340 |  Loss_1: (0.0227) | Acc_1: (99.26%) (43324/43648)\n",
      "Epoch: 199 | Batch_idx: 350 |  Loss_1: (0.0227) | Acc_1: (99.25%) (44593/44928)\n",
      "Epoch: 199 | Batch_idx: 360 |  Loss_1: (0.0228) | Acc_1: (99.25%) (45861/46208)\n",
      "Epoch: 199 | Batch_idx: 370 |  Loss_1: (0.0230) | Acc_1: (99.25%) (47130/47488)\n",
      "Epoch: 199 | Batch_idx: 380 |  Loss_1: (0.0232) | Acc_1: (99.24%) (48399/48768)\n",
      "Epoch: 199 | Batch_idx: 390 |  Loss_1: (0.0231) | Acc_1: (99.24%) (49622/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5102) | Acc: (91.50%) (9150/10000)\n",
      "4 hours 34 mins 23 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='resnet_50_colojitter.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 200):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='resnet_50_colojitter.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8766a17f2584f17ae9875767170f3464b2a051bfe2b6423fb227ac503acbc200"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hw2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
