{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.008\n",
    "default_directory = './save_models'\n",
    "writer = SummaryWriter('./log/vgg19_prelu_dropblock_cosine_1') #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformer_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    #transforms.ColorJitter(brightness=(0.2, 2), contrast=(0.3, 2), saturation=(0.2, 2), hue=(-0.3, 0.3)),\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "training_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_2 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "#training_dataset_3 = datasets.CIFAR10('./data', train=True, download=True, transform=transformer_train)\n",
    "validation_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_2 = torch.utils.data.DataLoader(dataset=training_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "#training_loader_3 = torch.utils.data.DataLoader(dataset=training_dataset_3, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlock2D(nn.Module):\n",
    " \n",
    "    def __init__(self, drop_prob, block_size):\n",
    "        super(DropBlock2D, self).__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        assert x.dim() == 4, \\\n",
    "            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n",
    "\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        else:\n",
    "            # get gamma value\n",
    "            gamma = self._compute_gamma(x)\n",
    "\n",
    "            # sample mask\n",
    "            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n",
    "\n",
    "            # place mask on input device\n",
    "            mask = mask.to(x.device)\n",
    "\n",
    "            # compute block mask\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "\n",
    "            # apply block mask\n",
    "            out = x * block_mask[:, None, :, :]\n",
    "\n",
    "            # scale output\n",
    "            out = out * block_mask.numel() / block_mask.sum()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n",
    "                                  kernel_size=(self.block_size, self.block_size),\n",
    "                                  stride=(1, 1),\n",
    "                                  padding=self.block_size // 2)\n",
    "\n",
    "        if self.block_size % 2 == 0:\n",
    "            block_mask = block_mask[:, :, :-1, :-1]\n",
    "\n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "\n",
    "        return block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob / (self.block_size ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(nn.Module):\n",
    "    def __init__(self, dropblock, start_value, stop_value, nr_steps):\n",
    "        super(LinearScheduler, self).__init__()\n",
    "        self.dropblock = dropblock\n",
    "        self.i = 0\n",
    "        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=int(nr_steps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropblock(x)\n",
    "\n",
    "    def step(self):\n",
    "        if self.i < len(self.drop_values):\n",
    "            self.dropblock.drop_prob = self.drop_values[self.i]\n",
    "\n",
    "        self.i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_types = {\n",
    "                'VGG11' : [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "                'VGG13' : [64, 64 ,'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "                'VGG16' : [64, 64 ,'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "                'VGG19' : [64, 64 ,'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "                \n",
    "                }\n",
    "\n",
    "class VGGnet(nn.Module):\n",
    "    def __init__(self, model, in_channels=3, num_classes=10, init_weights=True):\n",
    "        super(VGGnet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        #self.conv_layers = self.create_conv_laters(vgg_types[model])\n",
    "\n",
    "        self.conv_layers_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "\n",
    "        self.conv_layers_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "\n",
    "        self.conv_layers_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "\n",
    "        self.conv_layers_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "\n",
    "        self.conv_layers_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "\n",
    "        self.dropblock = LinearScheduler(DropBlock2D(drop_prob=0.35, block_size=4), start_value=0.35, stop_value=0.55, nr_steps=60000)\n",
    "           \n",
    "        \n",
    "            \n",
    "\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropblock.step()\n",
    "        x = self.conv_layers_1(x)\n",
    "        x = self.conv_layers_2(x)\n",
    "        x = self.dropblock(x)\n",
    "        x = self.conv_layers_3(x)\n",
    "        x = self.conv_layers_4(x)\n",
    "        x = self.dropblock(x)\n",
    "        x = self.conv_layers_5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# creat VGGnet object\n",
    "model = VGGnet('VGG19', in_channels=3, num_classes=10, init_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE 1 GPUs!\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "             PReLU-3           [-1, 64, 32, 32]               1\n",
      "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "             PReLU-6           [-1, 64, 32, 32]               1\n",
      "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "            PReLU-10          [-1, 128, 16, 16]               1\n",
      "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
      "            PReLU-13          [-1, 128, 16, 16]               1\n",
      "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
      "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
      "            PReLU-17            [-1, 256, 8, 8]               1\n",
      "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
      "            PReLU-20            [-1, 256, 8, 8]               1\n",
      "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
      "            PReLU-23            [-1, 256, 8, 8]               1\n",
      "           Conv2d-24            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-25            [-1, 256, 8, 8]             512\n",
      "            PReLU-26            [-1, 256, 8, 8]               1\n",
      "        MaxPool2d-27            [-1, 256, 4, 4]               0\n",
      "           Conv2d-28            [-1, 512, 4, 4]       1,180,160\n",
      "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
      "              ELU-30            [-1, 512, 4, 4]               0\n",
      "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
      "            PReLU-33            [-1, 512, 4, 4]               1\n",
      "           Conv2d-34            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-35            [-1, 512, 4, 4]           1,024\n",
      "            PReLU-36            [-1, 512, 4, 4]               1\n",
      "           Conv2d-37            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
      "            PReLU-39            [-1, 512, 4, 4]               1\n",
      "        MaxPool2d-40            [-1, 512, 2, 2]               0\n",
      "      DropBlock2D-41            [-1, 512, 2, 2]               0\n",
      "  LinearScheduler-42            [-1, 512, 2, 2]               0\n",
      "           Conv2d-43            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-44            [-1, 512, 2, 2]           1,024\n",
      "            PReLU-45            [-1, 512, 2, 2]               1\n",
      "           Conv2d-46            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-47            [-1, 512, 2, 2]           1,024\n",
      "            PReLU-48            [-1, 512, 2, 2]               1\n",
      "           Conv2d-49            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-50            [-1, 512, 2, 2]           1,024\n",
      "            PReLU-51            [-1, 512, 2, 2]               1\n",
      "           Conv2d-52            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-53            [-1, 512, 2, 2]           1,024\n",
      "            PReLU-54            [-1, 512, 2, 2]               1\n",
      "        MaxPool2d-55            [-1, 512, 1, 1]               0\n",
      "           Linear-56                 [-1, 4096]       2,101,248\n",
      "            PReLU-57                 [-1, 4096]               1\n",
      "          Dropout-58                 [-1, 4096]               0\n",
      "           Linear-59                 [-1, 4096]      16,781,312\n",
      "            PReLU-60                 [-1, 4096]               1\n",
      "          Dropout-61                 [-1, 4096]               0\n",
      "           Linear-62                   [-1, 10]          40,970\n",
      "           VGGnet-63                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 38,958,939\n",
      "Trainable params: 38,958,939\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 7.39\n",
      "Params size (MB): 148.62\n",
      "Estimated Total Size (MB): 156.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "summary(model, (3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)             \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=3, eta_min=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    iters = len(training_loader)\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + batch_idx / iters)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss_1: ({:.4f}) | Acc_1: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(training_loader) + batch_idx) #!#\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(training_loader) + batch_idx) #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(validation_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(validation_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(directory, state, filename='latest_1.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest_1.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss_1: (2.3088) | Acc_1: (6.25%) (8/128)\n",
      "Epoch: 0 | Batch_idx: 10 |  Loss_1: (2.3040) | Acc_1: (10.16%) (143/1408)\n",
      "Epoch: 0 | Batch_idx: 20 |  Loss_1: (2.2791) | Acc_1: (13.47%) (362/2688)\n",
      "Epoch: 0 | Batch_idx: 30 |  Loss_1: (2.2272) | Acc_1: (15.50%) (615/3968)\n",
      "Epoch: 0 | Batch_idx: 40 |  Loss_1: (2.1677) | Acc_1: (17.47%) (917/5248)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss_1: (2.1165) | Acc_1: (19.13%) (1249/6528)\n",
      "Epoch: 0 | Batch_idx: 60 |  Loss_1: (2.0682) | Acc_1: (20.68%) (1615/7808)\n",
      "Epoch: 0 | Batch_idx: 70 |  Loss_1: (2.0284) | Acc_1: (22.06%) (2005/9088)\n",
      "Epoch: 0 | Batch_idx: 80 |  Loss_1: (2.0012) | Acc_1: (22.98%) (2383/10368)\n",
      "Epoch: 0 | Batch_idx: 90 |  Loss_1: (1.9756) | Acc_1: (24.03%) (2799/11648)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss_1: (1.9497) | Acc_1: (25.24%) (3263/12928)\n",
      "Epoch: 0 | Batch_idx: 110 |  Loss_1: (1.9303) | Acc_1: (26.11%) (3709/14208)\n",
      "Epoch: 0 | Batch_idx: 120 |  Loss_1: (1.9135) | Acc_1: (26.87%) (4161/15488)\n",
      "Epoch: 0 | Batch_idx: 130 |  Loss_1: (1.8976) | Acc_1: (27.58%) (4624/16768)\n",
      "Epoch: 0 | Batch_idx: 140 |  Loss_1: (1.8797) | Acc_1: (28.34%) (5115/18048)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss_1: (1.8614) | Acc_1: (29.10%) (5624/19328)\n",
      "Epoch: 0 | Batch_idx: 160 |  Loss_1: (1.8458) | Acc_1: (29.73%) (6126/20608)\n",
      "Epoch: 0 | Batch_idx: 170 |  Loss_1: (1.8288) | Acc_1: (30.41%) (6656/21888)\n",
      "Epoch: 0 | Batch_idx: 180 |  Loss_1: (1.8141) | Acc_1: (31.03%) (7190/23168)\n",
      "Epoch: 0 | Batch_idx: 190 |  Loss_1: (1.8021) | Acc_1: (31.59%) (7722/24448)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss_1: (1.7872) | Acc_1: (32.25%) (8297/25728)\n",
      "Epoch: 0 | Batch_idx: 210 |  Loss_1: (1.7733) | Acc_1: (32.82%) (8865/27008)\n",
      "Epoch: 0 | Batch_idx: 220 |  Loss_1: (1.7603) | Acc_1: (33.38%) (9442/28288)\n",
      "Epoch: 0 | Batch_idx: 230 |  Loss_1: (1.7490) | Acc_1: (33.84%) (10005/29568)\n",
      "Epoch: 0 | Batch_idx: 240 |  Loss_1: (1.7376) | Acc_1: (34.27%) (10571/30848)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss_1: (1.7277) | Acc_1: (34.70%) (11149/32128)\n",
      "Epoch: 0 | Batch_idx: 260 |  Loss_1: (1.7176) | Acc_1: (35.13%) (11736/33408)\n",
      "Epoch: 0 | Batch_idx: 270 |  Loss_1: (1.7070) | Acc_1: (35.58%) (12342/34688)\n",
      "Epoch: 0 | Batch_idx: 280 |  Loss_1: (1.6958) | Acc_1: (36.02%) (12956/35968)\n",
      "Epoch: 0 | Batch_idx: 290 |  Loss_1: (1.6879) | Acc_1: (36.39%) (13556/37248)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss_1: (1.6780) | Acc_1: (36.77%) (14168/38528)\n",
      "Epoch: 0 | Batch_idx: 310 |  Loss_1: (1.6691) | Acc_1: (37.17%) (14796/39808)\n",
      "Epoch: 0 | Batch_idx: 320 |  Loss_1: (1.6597) | Acc_1: (37.50%) (15407/41088)\n",
      "Epoch: 0 | Batch_idx: 330 |  Loss_1: (1.6527) | Acc_1: (37.84%) (16033/42368)\n",
      "Epoch: 0 | Batch_idx: 340 |  Loss_1: (1.6435) | Acc_1: (38.22%) (16683/43648)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss_1: (1.6347) | Acc_1: (38.59%) (17339/44928)\n",
      "Epoch: 0 | Batch_idx: 360 |  Loss_1: (1.6266) | Acc_1: (38.91%) (17980/46208)\n",
      "Epoch: 0 | Batch_idx: 370 |  Loss_1: (1.6199) | Acc_1: (39.21%) (18618/47488)\n",
      "Epoch: 0 | Batch_idx: 380 |  Loss_1: (1.6126) | Acc_1: (39.50%) (19262/48768)\n",
      "Epoch: 0 | Batch_idx: 390 |  Loss_1: (1.6042) | Acc_1: (39.87%) (19936/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1489) | Acc: (59.39%) (5939/10000)\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss_1: (1.2366) | Acc_1: (58.59%) (75/128)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss_1: (1.2843) | Acc_1: (53.48%) (753/1408)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss_1: (1.2827) | Acc_1: (53.05%) (1426/2688)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss_1: (1.2698) | Acc_1: (53.33%) (2116/3968)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss_1: (1.2844) | Acc_1: (53.03%) (2783/5248)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss_1: (1.2794) | Acc_1: (53.16%) (3470/6528)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss_1: (1.2744) | Acc_1: (53.51%) (4178/7808)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss_1: (1.2783) | Acc_1: (53.17%) (4832/9088)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss_1: (1.2762) | Acc_1: (53.15%) (5511/10368)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss_1: (1.2707) | Acc_1: (53.31%) (6209/11648)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss_1: (1.2681) | Acc_1: (53.36%) (6899/12928)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss_1: (1.2660) | Acc_1: (53.60%) (7616/14208)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss_1: (1.2676) | Acc_1: (53.58%) (8298/15488)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss_1: (1.2660) | Acc_1: (53.53%) (8976/16768)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss_1: (1.2645) | Acc_1: (53.78%) (9707/18048)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss_1: (1.2598) | Acc_1: (53.95%) (10428/19328)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss_1: (1.2554) | Acc_1: (54.15%) (11160/20608)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss_1: (1.2504) | Acc_1: (54.39%) (11904/21888)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss_1: (1.2459) | Acc_1: (54.57%) (12642/23168)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss_1: (1.2430) | Acc_1: (54.63%) (13356/24448)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss_1: (1.2379) | Acc_1: (54.87%) (14118/25728)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss_1: (1.2354) | Acc_1: (54.93%) (14836/27008)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss_1: (1.2350) | Acc_1: (54.98%) (15552/28288)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss_1: (1.2310) | Acc_1: (55.10%) (16291/29568)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss_1: (1.2273) | Acc_1: (55.27%) (17051/30848)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss_1: (1.2245) | Acc_1: (55.35%) (17784/32128)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss_1: (1.2217) | Acc_1: (55.44%) (18520/33408)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss_1: (1.2187) | Acc_1: (55.53%) (19263/34688)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss_1: (1.2156) | Acc_1: (55.69%) (20032/35968)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss_1: (1.2118) | Acc_1: (55.83%) (20795/37248)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss_1: (1.2081) | Acc_1: (55.96%) (21562/38528)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss_1: (1.2056) | Acc_1: (56.07%) (22319/39808)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss_1: (1.2023) | Acc_1: (56.19%) (23086/41088)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss_1: (1.2007) | Acc_1: (56.24%) (23829/42368)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss_1: (1.1972) | Acc_1: (56.39%) (24611/43648)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss_1: (1.1949) | Acc_1: (56.47%) (25369/44928)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss_1: (1.1926) | Acc_1: (56.58%) (26143/46208)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss_1: (1.1910) | Acc_1: (56.62%) (26889/47488)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss_1: (1.1889) | Acc_1: (56.68%) (27641/48768)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss_1: (1.1861) | Acc_1: (56.79%) (28395/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9337) | Acc: (68.05%) (6805/10000)\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss_1: (0.9567) | Acc_1: (68.75%) (88/128)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss_1: (1.0650) | Acc_1: (61.29%) (863/1408)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss_1: (1.0445) | Acc_1: (62.80%) (1688/2688)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss_1: (1.0564) | Acc_1: (61.97%) (2459/3968)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss_1: (1.0613) | Acc_1: (61.79%) (3243/5248)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss_1: (1.0676) | Acc_1: (61.38%) (4007/6528)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss_1: (1.0616) | Acc_1: (61.68%) (4816/7808)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss_1: (1.0626) | Acc_1: (61.52%) (5591/9088)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss_1: (1.0637) | Acc_1: (61.58%) (6385/10368)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss_1: (1.0655) | Acc_1: (61.52%) (7166/11648)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss_1: (1.0618) | Acc_1: (61.68%) (7974/12928)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss_1: (1.0620) | Acc_1: (61.68%) (8763/14208)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss_1: (1.0612) | Acc_1: (61.63%) (9546/15488)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss_1: (1.0631) | Acc_1: (61.56%) (10322/16768)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss_1: (1.0613) | Acc_1: (61.69%) (11134/18048)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss_1: (1.0573) | Acc_1: (61.82%) (11948/19328)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss_1: (1.0555) | Acc_1: (61.89%) (12755/20608)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss_1: (1.0507) | Acc_1: (62.03%) (13577/21888)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss_1: (1.0507) | Acc_1: (62.03%) (14372/23168)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss_1: (1.0489) | Acc_1: (62.05%) (15171/24448)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss_1: (1.0455) | Acc_1: (62.18%) (15998/25728)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss_1: (1.0455) | Acc_1: (62.21%) (16801/27008)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss_1: (1.0462) | Acc_1: (62.22%) (17601/28288)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss_1: (1.0461) | Acc_1: (62.21%) (18393/29568)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss_1: (1.0453) | Acc_1: (62.24%) (19200/30848)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss_1: (1.0441) | Acc_1: (62.34%) (20028/32128)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss_1: (1.0417) | Acc_1: (62.44%) (20859/33408)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss_1: (1.0416) | Acc_1: (62.45%) (21662/34688)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss_1: (1.0416) | Acc_1: (62.49%) (22478/35968)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss_1: (1.0416) | Acc_1: (62.48%) (23272/37248)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss_1: (1.0387) | Acc_1: (62.61%) (24124/38528)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss_1: (1.0356) | Acc_1: (62.71%) (24962/39808)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss_1: (1.0353) | Acc_1: (62.72%) (25769/41088)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss_1: (1.0347) | Acc_1: (62.76%) (26591/42368)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss_1: (1.0325) | Acc_1: (62.81%) (27416/43648)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss_1: (1.0291) | Acc_1: (62.97%) (28289/44928)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss_1: (1.0300) | Acc_1: (62.91%) (29070/46208)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss_1: (1.0290) | Acc_1: (62.94%) (29891/47488)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss_1: (1.0264) | Acc_1: (63.04%) (30744/48768)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss_1: (1.0243) | Acc_1: (63.13%) (31564/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7904) | Acc: (72.68%) (7268/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss_1: (1.0113) | Acc_1: (65.62%) (84/128)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss_1: (0.9185) | Acc_1: (65.98%) (929/1408)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss_1: (0.9078) | Acc_1: (66.59%) (1790/2688)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss_1: (0.9187) | Acc_1: (66.78%) (2650/3968)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss_1: (0.9310) | Acc_1: (66.31%) (3480/5248)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss_1: (0.9375) | Acc_1: (66.33%) (4330/6528)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss_1: (0.9372) | Acc_1: (66.44%) (5188/7808)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss_1: (0.9354) | Acc_1: (66.48%) (6042/9088)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss_1: (0.9334) | Acc_1: (66.54%) (6899/10368)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss_1: (0.9335) | Acc_1: (66.42%) (7737/11648)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss_1: (0.9362) | Acc_1: (66.22%) (8561/12928)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss_1: (0.9365) | Acc_1: (66.16%) (9400/14208)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss_1: (0.9363) | Acc_1: (66.19%) (10252/15488)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss_1: (0.9326) | Acc_1: (66.28%) (11114/16768)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss_1: (0.9295) | Acc_1: (66.36%) (11976/18048)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss_1: (0.9293) | Acc_1: (66.36%) (12827/19328)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss_1: (0.9292) | Acc_1: (66.32%) (13667/20608)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss_1: (0.9274) | Acc_1: (66.44%) (14543/21888)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss_1: (0.9283) | Acc_1: (66.36%) (15375/23168)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss_1: (0.9259) | Acc_1: (66.46%) (16248/24448)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss_1: (0.9275) | Acc_1: (66.42%) (17089/25728)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss_1: (0.9276) | Acc_1: (66.41%) (17935/27008)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss_1: (0.9271) | Acc_1: (66.40%) (18782/28288)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss_1: (0.9273) | Acc_1: (66.42%) (19640/29568)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss_1: (0.9275) | Acc_1: (66.40%) (20484/30848)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss_1: (0.9311) | Acc_1: (66.28%) (21294/32128)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss_1: (0.9320) | Acc_1: (66.25%) (22132/33408)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss_1: (0.9301) | Acc_1: (66.26%) (22985/34688)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss_1: (0.9303) | Acc_1: (66.32%) (23854/35968)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss_1: (0.9309) | Acc_1: (66.26%) (24679/37248)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss_1: (0.9300) | Acc_1: (66.32%) (25551/38528)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss_1: (0.9293) | Acc_1: (66.36%) (26417/39808)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss_1: (0.9283) | Acc_1: (66.38%) (27276/41088)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss_1: (0.9301) | Acc_1: (66.30%) (28091/42368)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss_1: (0.9306) | Acc_1: (66.27%) (28927/43648)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss_1: (0.9300) | Acc_1: (66.29%) (29785/44928)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss_1: (0.9287) | Acc_1: (66.37%) (30668/46208)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss_1: (0.9291) | Acc_1: (66.33%) (31500/47488)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss_1: (0.9271) | Acc_1: (66.43%) (32398/48768)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss_1: (0.9261) | Acc_1: (66.47%) (33234/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7300) | Acc: (75.15%) (7515/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss_1: (0.8732) | Acc_1: (71.09%) (91/128)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss_1: (0.8579) | Acc_1: (69.25%) (975/1408)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss_1: (0.8967) | Acc_1: (67.19%) (1806/2688)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss_1: (0.8740) | Acc_1: (68.15%) (2704/3968)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss_1: (0.8752) | Acc_1: (68.16%) (3577/5248)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss_1: (0.8818) | Acc_1: (67.63%) (4415/6528)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss_1: (0.8775) | Acc_1: (67.84%) (5297/7808)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss_1: (0.8799) | Acc_1: (67.79%) (6161/9088)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss_1: (0.8742) | Acc_1: (68.09%) (7060/10368)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss_1: (0.8706) | Acc_1: (68.29%) (7954/11648)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss_1: (0.8716) | Acc_1: (68.26%) (8825/12928)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss_1: (0.8721) | Acc_1: (68.23%) (9694/14208)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss_1: (0.8692) | Acc_1: (68.38%) (10591/15488)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss_1: (0.8712) | Acc_1: (68.30%) (11453/16768)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss_1: (0.8698) | Acc_1: (68.34%) (12334/18048)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss_1: (0.8692) | Acc_1: (68.34%) (13209/19328)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss_1: (0.8686) | Acc_1: (68.36%) (14087/20608)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss_1: (0.8681) | Acc_1: (68.42%) (14976/21888)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss_1: (0.8676) | Acc_1: (68.49%) (15868/23168)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss_1: (0.8670) | Acc_1: (68.50%) (16747/24448)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss_1: (0.8676) | Acc_1: (68.47%) (17616/25728)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss_1: (0.8668) | Acc_1: (68.55%) (18515/27008)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss_1: (0.8665) | Acc_1: (68.58%) (19400/28288)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss_1: (0.8660) | Acc_1: (68.58%) (20277/29568)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss_1: (0.8654) | Acc_1: (68.62%) (21168/30848)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss_1: (0.8665) | Acc_1: (68.58%) (22034/32128)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss_1: (0.8653) | Acc_1: (68.63%) (22929/33408)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss_1: (0.8644) | Acc_1: (68.70%) (23832/34688)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss_1: (0.8648) | Acc_1: (68.72%) (24716/35968)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss_1: (0.8653) | Acc_1: (68.69%) (25586/37248)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss_1: (0.8651) | Acc_1: (68.68%) (26460/38528)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss_1: (0.8635) | Acc_1: (68.74%) (27363/39808)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss_1: (0.8623) | Acc_1: (68.78%) (28259/41088)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss_1: (0.8624) | Acc_1: (68.75%) (29127/42368)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss_1: (0.8607) | Acc_1: (68.81%) (30034/43648)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss_1: (0.8611) | Acc_1: (68.80%) (30911/44928)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss_1: (0.8611) | Acc_1: (68.80%) (31793/46208)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss_1: (0.8621) | Acc_1: (68.80%) (32673/47488)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss_1: (0.8615) | Acc_1: (68.82%) (33564/48768)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss_1: (0.8614) | Acc_1: (68.85%) (34425/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6114) | Acc: (79.14%) (7914/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss_1: (0.8193) | Acc_1: (67.97%) (87/128)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss_1: (0.7778) | Acc_1: (71.59%) (1008/1408)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss_1: (0.8002) | Acc_1: (70.98%) (1908/2688)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss_1: (0.8118) | Acc_1: (70.69%) (2805/3968)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss_1: (0.8179) | Acc_1: (70.39%) (3694/5248)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss_1: (0.8197) | Acc_1: (70.48%) (4601/6528)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss_1: (0.8181) | Acc_1: (70.81%) (5529/7808)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss_1: (0.8182) | Acc_1: (70.80%) (6434/9088)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss_1: (0.8195) | Acc_1: (70.66%) (7326/10368)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss_1: (0.8191) | Acc_1: (70.58%) (8221/11648)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss_1: (0.8171) | Acc_1: (70.63%) (9131/12928)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss_1: (0.8202) | Acc_1: (70.56%) (10025/14208)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss_1: (0.8223) | Acc_1: (70.47%) (10914/15488)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss_1: (0.8230) | Acc_1: (70.52%) (11825/16768)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss_1: (0.8213) | Acc_1: (70.52%) (12727/18048)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss_1: (0.8176) | Acc_1: (70.67%) (13659/19328)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss_1: (0.8177) | Acc_1: (70.67%) (14563/20608)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss_1: (0.8152) | Acc_1: (70.73%) (15482/21888)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss_1: (0.8179) | Acc_1: (70.59%) (16354/23168)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss_1: (0.8164) | Acc_1: (70.63%) (17267/24448)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss_1: (0.8160) | Acc_1: (70.69%) (18186/25728)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss_1: (0.8139) | Acc_1: (70.75%) (19109/27008)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss_1: (0.8166) | Acc_1: (70.59%) (19969/28288)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss_1: (0.8146) | Acc_1: (70.63%) (20885/29568)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss_1: (0.8145) | Acc_1: (70.66%) (21798/30848)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss_1: (0.8135) | Acc_1: (70.70%) (22716/32128)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss_1: (0.8142) | Acc_1: (70.68%) (23612/33408)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss_1: (0.8141) | Acc_1: (70.67%) (24514/34688)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss_1: (0.8148) | Acc_1: (70.64%) (25407/35968)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss_1: (0.8136) | Acc_1: (70.70%) (26333/37248)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss_1: (0.8146) | Acc_1: (70.65%) (27220/38528)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss_1: (0.8143) | Acc_1: (70.69%) (28139/39808)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss_1: (0.8121) | Acc_1: (70.77%) (29077/41088)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss_1: (0.8116) | Acc_1: (70.78%) (29986/42368)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss_1: (0.8116) | Acc_1: (70.76%) (30885/43648)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss_1: (0.8129) | Acc_1: (70.74%) (31781/44928)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss_1: (0.8134) | Acc_1: (70.73%) (32685/46208)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss_1: (0.8146) | Acc_1: (70.68%) (33566/47488)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss_1: (0.8130) | Acc_1: (70.75%) (34505/48768)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss_1: (0.8128) | Acc_1: (70.79%) (35394/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6721) | Acc: (76.42%) (7642/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss_1: (0.7802) | Acc_1: (70.31%) (90/128)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss_1: (0.7797) | Acc_1: (71.52%) (1007/1408)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss_1: (0.8034) | Acc_1: (71.32%) (1917/2688)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss_1: (0.7943) | Acc_1: (71.37%) (2832/3968)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss_1: (0.7860) | Acc_1: (71.68%) (3762/5248)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss_1: (0.7808) | Acc_1: (71.88%) (4692/6528)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss_1: (0.7840) | Acc_1: (71.71%) (5599/7808)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss_1: (0.7856) | Acc_1: (71.61%) (6508/9088)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss_1: (0.7886) | Acc_1: (71.58%) (7421/10368)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss_1: (0.7915) | Acc_1: (71.35%) (8311/11648)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss_1: (0.7889) | Acc_1: (71.53%) (9248/12928)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss_1: (0.7862) | Acc_1: (71.62%) (10176/14208)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss_1: (0.7826) | Acc_1: (71.81%) (11122/15488)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss_1: (0.7789) | Acc_1: (71.90%) (12057/16768)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss_1: (0.7789) | Acc_1: (71.86%) (12969/18048)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss_1: (0.7781) | Acc_1: (71.93%) (13902/19328)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss_1: (0.7785) | Acc_1: (71.94%) (14826/20608)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss_1: (0.7769) | Acc_1: (71.98%) (15756/21888)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss_1: (0.7745) | Acc_1: (72.12%) (16709/23168)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss_1: (0.7741) | Acc_1: (72.08%) (17623/24448)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss_1: (0.7767) | Acc_1: (71.98%) (18518/25728)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss_1: (0.7767) | Acc_1: (71.90%) (19419/27008)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss_1: (0.7762) | Acc_1: (72.00%) (20367/28288)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss_1: (0.7775) | Acc_1: (71.93%) (21269/29568)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss_1: (0.7811) | Acc_1: (71.79%) (22147/30848)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss_1: (0.7821) | Acc_1: (71.76%) (23056/32128)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss_1: (0.7811) | Acc_1: (71.82%) (23995/33408)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss_1: (0.7814) | Acc_1: (71.80%) (24905/34688)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss_1: (0.7796) | Acc_1: (71.84%) (25841/35968)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss_1: (0.7769) | Acc_1: (71.95%) (26801/37248)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss_1: (0.7774) | Acc_1: (71.95%) (27719/38528)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss_1: (0.7773) | Acc_1: (71.93%) (28634/39808)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss_1: (0.7780) | Acc_1: (71.89%) (29538/41088)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss_1: (0.7769) | Acc_1: (71.89%) (30460/42368)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss_1: (0.7771) | Acc_1: (71.88%) (31375/43648)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss_1: (0.7766) | Acc_1: (71.94%) (32322/44928)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss_1: (0.7767) | Acc_1: (71.94%) (33242/46208)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss_1: (0.7771) | Acc_1: (71.91%) (34150/47488)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss_1: (0.7770) | Acc_1: (71.93%) (35079/48768)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss_1: (0.7758) | Acc_1: (71.97%) (35984/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5394) | Acc: (81.44%) (8144/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss_1: (0.6016) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss_1: (0.7536) | Acc_1: (72.66%) (1023/1408)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss_1: (0.7305) | Acc_1: (73.74%) (1982/2688)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss_1: (0.7337) | Acc_1: (73.69%) (2924/3968)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss_1: (0.7302) | Acc_1: (73.57%) (3861/5248)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss_1: (0.7340) | Acc_1: (73.61%) (4805/6528)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss_1: (0.7427) | Acc_1: (73.27%) (5721/7808)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss_1: (0.7410) | Acc_1: (73.28%) (6660/9088)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss_1: (0.7393) | Acc_1: (73.31%) (7601/10368)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss_1: (0.7390) | Acc_1: (73.38%) (8547/11648)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss_1: (0.7441) | Acc_1: (73.14%) (9455/12928)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss_1: (0.7449) | Acc_1: (73.13%) (10391/14208)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss_1: (0.7442) | Acc_1: (73.19%) (11336/15488)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss_1: (0.7401) | Acc_1: (73.36%) (12301/16768)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss_1: (0.7433) | Acc_1: (73.30%) (13230/18048)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss_1: (0.7376) | Acc_1: (73.53%) (14211/19328)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss_1: (0.7340) | Acc_1: (73.64%) (15176/20608)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss_1: (0.7361) | Acc_1: (73.63%) (16116/21888)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss_1: (0.7354) | Acc_1: (73.69%) (17072/23168)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss_1: (0.7400) | Acc_1: (73.55%) (17981/24448)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss_1: (0.7413) | Acc_1: (73.53%) (18918/25728)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss_1: (0.7410) | Acc_1: (73.54%) (19862/27008)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss_1: (0.7387) | Acc_1: (73.66%) (20836/28288)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss_1: (0.7348) | Acc_1: (73.85%) (21835/29568)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss_1: (0.7348) | Acc_1: (73.81%) (22768/30848)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss_1: (0.7356) | Acc_1: (73.74%) (23691/32128)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss_1: (0.7364) | Acc_1: (73.66%) (24610/33408)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss_1: (0.7378) | Acc_1: (73.61%) (25535/34688)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss_1: (0.7384) | Acc_1: (73.58%) (26465/35968)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss_1: (0.7380) | Acc_1: (73.61%) (27420/37248)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss_1: (0.7367) | Acc_1: (73.63%) (28370/38528)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss_1: (0.7377) | Acc_1: (73.58%) (29291/39808)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss_1: (0.7383) | Acc_1: (73.55%) (30219/41088)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss_1: (0.7378) | Acc_1: (73.56%) (31166/42368)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss_1: (0.7383) | Acc_1: (73.55%) (32104/43648)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss_1: (0.7382) | Acc_1: (73.54%) (33038/44928)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss_1: (0.7390) | Acc_1: (73.51%) (33966/46208)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss_1: (0.7379) | Acc_1: (73.55%) (34928/47488)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss_1: (0.7372) | Acc_1: (73.55%) (35871/48768)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss_1: (0.7368) | Acc_1: (73.56%) (36780/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6271) | Acc: (78.72%) (7872/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss_1: (0.7969) | Acc_1: (68.75%) (88/128)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss_1: (0.7198) | Acc_1: (73.44%) (1034/1408)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss_1: (0.7080) | Acc_1: (74.11%) (1992/2688)\n",
      "Epoch: 8 | Batch_idx: 30 |  Loss_1: (0.7163) | Acc_1: (73.56%) (2919/3968)\n",
      "Epoch: 8 | Batch_idx: 40 |  Loss_1: (0.7178) | Acc_1: (73.65%) (3865/5248)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss_1: (0.7224) | Acc_1: (73.50%) (4798/6528)\n",
      "Epoch: 8 | Batch_idx: 60 |  Loss_1: (0.7205) | Acc_1: (73.49%) (5738/7808)\n",
      "Epoch: 8 | Batch_idx: 70 |  Loss_1: (0.7189) | Acc_1: (73.65%) (6693/9088)\n",
      "Epoch: 8 | Batch_idx: 80 |  Loss_1: (0.7213) | Acc_1: (73.68%) (7639/10368)\n",
      "Epoch: 8 | Batch_idx: 90 |  Loss_1: (0.7156) | Acc_1: (73.88%) (8606/11648)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss_1: (0.7136) | Acc_1: (74.01%) (9568/12928)\n",
      "Epoch: 8 | Batch_idx: 110 |  Loss_1: (0.7109) | Acc_1: (74.00%) (10514/14208)\n",
      "Epoch: 8 | Batch_idx: 120 |  Loss_1: (0.7109) | Acc_1: (73.95%) (11454/15488)\n",
      "Epoch: 8 | Batch_idx: 130 |  Loss_1: (0.7118) | Acc_1: (73.87%) (12387/16768)\n",
      "Epoch: 8 | Batch_idx: 140 |  Loss_1: (0.7087) | Acc_1: (74.03%) (13361/18048)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss_1: (0.7104) | Acc_1: (74.00%) (14302/19328)\n",
      "Epoch: 8 | Batch_idx: 160 |  Loss_1: (0.7119) | Acc_1: (73.96%) (15241/20608)\n",
      "Epoch: 8 | Batch_idx: 170 |  Loss_1: (0.7119) | Acc_1: (73.97%) (16190/21888)\n",
      "Epoch: 8 | Batch_idx: 180 |  Loss_1: (0.7126) | Acc_1: (73.95%) (17133/23168)\n",
      "Epoch: 8 | Batch_idx: 190 |  Loss_1: (0.7131) | Acc_1: (73.90%) (18067/24448)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss_1: (0.7121) | Acc_1: (73.91%) (19016/25728)\n",
      "Epoch: 8 | Batch_idx: 210 |  Loss_1: (0.7125) | Acc_1: (73.86%) (19948/27008)\n",
      "Epoch: 8 | Batch_idx: 220 |  Loss_1: (0.7106) | Acc_1: (73.95%) (20919/28288)\n",
      "Epoch: 8 | Batch_idx: 230 |  Loss_1: (0.7097) | Acc_1: (73.99%) (21877/29568)\n",
      "Epoch: 8 | Batch_idx: 240 |  Loss_1: (0.7089) | Acc_1: (74.02%) (22834/30848)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss_1: (0.7080) | Acc_1: (74.06%) (23795/32128)\n",
      "Epoch: 8 | Batch_idx: 260 |  Loss_1: (0.7074) | Acc_1: (74.06%) (24743/33408)\n",
      "Epoch: 8 | Batch_idx: 270 |  Loss_1: (0.7083) | Acc_1: (74.01%) (25671/34688)\n",
      "Epoch: 8 | Batch_idx: 280 |  Loss_1: (0.7086) | Acc_1: (74.00%) (26618/35968)\n",
      "Epoch: 8 | Batch_idx: 290 |  Loss_1: (0.7093) | Acc_1: (73.98%) (27557/37248)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss_1: (0.7075) | Acc_1: (74.06%) (28533/38528)\n",
      "Epoch: 8 | Batch_idx: 310 |  Loss_1: (0.7098) | Acc_1: (74.02%) (29465/39808)\n",
      "Epoch: 8 | Batch_idx: 320 |  Loss_1: (0.7101) | Acc_1: (74.03%) (30416/41088)\n",
      "Epoch: 8 | Batch_idx: 330 |  Loss_1: (0.7096) | Acc_1: (74.04%) (31369/42368)\n",
      "Epoch: 8 | Batch_idx: 340 |  Loss_1: (0.7100) | Acc_1: (74.04%) (32315/43648)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss_1: (0.7098) | Acc_1: (74.08%) (33281/44928)\n",
      "Epoch: 8 | Batch_idx: 360 |  Loss_1: (0.7102) | Acc_1: (74.10%) (34241/46208)\n",
      "Epoch: 8 | Batch_idx: 370 |  Loss_1: (0.7093) | Acc_1: (74.13%) (35201/47488)\n",
      "Epoch: 8 | Batch_idx: 380 |  Loss_1: (0.7112) | Acc_1: (74.05%) (36113/48768)\n",
      "Epoch: 8 | Batch_idx: 390 |  Loss_1: (0.7100) | Acc_1: (74.10%) (37050/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5680) | Acc: (80.56%) (8056/10000)\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss_1: (0.6933) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 9 | Batch_idx: 10 |  Loss_1: (0.6683) | Acc_1: (75.99%) (1070/1408)\n",
      "Epoch: 9 | Batch_idx: 20 |  Loss_1: (0.6591) | Acc_1: (76.30%) (2051/2688)\n",
      "Epoch: 9 | Batch_idx: 30 |  Loss_1: (0.6628) | Acc_1: (76.08%) (3019/3968)\n",
      "Epoch: 9 | Batch_idx: 40 |  Loss_1: (0.6594) | Acc_1: (76.35%) (4007/5248)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss_1: (0.6656) | Acc_1: (76.01%) (4962/6528)\n",
      "Epoch: 9 | Batch_idx: 60 |  Loss_1: (0.6577) | Acc_1: (76.34%) (5961/7808)\n",
      "Epoch: 9 | Batch_idx: 70 |  Loss_1: (0.6618) | Acc_1: (76.18%) (6923/9088)\n",
      "Epoch: 9 | Batch_idx: 80 |  Loss_1: (0.6665) | Acc_1: (75.95%) (7875/10368)\n",
      "Epoch: 9 | Batch_idx: 90 |  Loss_1: (0.6706) | Acc_1: (75.92%) (8843/11648)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss_1: (0.6687) | Acc_1: (75.91%) (9814/12928)\n",
      "Epoch: 9 | Batch_idx: 110 |  Loss_1: (0.6684) | Acc_1: (75.84%) (10776/14208)\n",
      "Epoch: 9 | Batch_idx: 120 |  Loss_1: (0.6703) | Acc_1: (75.85%) (11748/15488)\n",
      "Epoch: 9 | Batch_idx: 130 |  Loss_1: (0.6699) | Acc_1: (75.94%) (12733/16768)\n",
      "Epoch: 9 | Batch_idx: 140 |  Loss_1: (0.6707) | Acc_1: (75.79%) (13679/18048)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss_1: (0.6698) | Acc_1: (75.81%) (14653/19328)\n",
      "Epoch: 9 | Batch_idx: 160 |  Loss_1: (0.6687) | Acc_1: (75.83%) (15628/20608)\n",
      "Epoch: 9 | Batch_idx: 170 |  Loss_1: (0.6671) | Acc_1: (75.79%) (16588/21888)\n",
      "Epoch: 9 | Batch_idx: 180 |  Loss_1: (0.6693) | Acc_1: (75.69%) (17535/23168)\n",
      "Epoch: 9 | Batch_idx: 190 |  Loss_1: (0.6697) | Acc_1: (75.68%) (18502/24448)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss_1: (0.6707) | Acc_1: (75.65%) (19464/25728)\n",
      "Epoch: 9 | Batch_idx: 210 |  Loss_1: (0.6691) | Acc_1: (75.68%) (20441/27008)\n",
      "Epoch: 9 | Batch_idx: 220 |  Loss_1: (0.6694) | Acc_1: (75.63%) (21395/28288)\n",
      "Epoch: 9 | Batch_idx: 230 |  Loss_1: (0.6700) | Acc_1: (75.64%) (22364/29568)\n",
      "Epoch: 9 | Batch_idx: 240 |  Loss_1: (0.6733) | Acc_1: (75.53%) (23300/30848)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss_1: (0.6748) | Acc_1: (75.51%) (24259/32128)\n",
      "Epoch: 9 | Batch_idx: 260 |  Loss_1: (0.6750) | Acc_1: (75.53%) (25232/33408)\n",
      "Epoch: 9 | Batch_idx: 270 |  Loss_1: (0.6771) | Acc_1: (75.46%) (26174/34688)\n",
      "Epoch: 9 | Batch_idx: 280 |  Loss_1: (0.6765) | Acc_1: (75.50%) (27157/35968)\n",
      "Epoch: 9 | Batch_idx: 290 |  Loss_1: (0.6765) | Acc_1: (75.51%) (28127/37248)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss_1: (0.6769) | Acc_1: (75.53%) (29100/38528)\n",
      "Epoch: 9 | Batch_idx: 310 |  Loss_1: (0.6782) | Acc_1: (75.49%) (30052/39808)\n",
      "Epoch: 9 | Batch_idx: 320 |  Loss_1: (0.6783) | Acc_1: (75.49%) (31019/41088)\n",
      "Epoch: 9 | Batch_idx: 330 |  Loss_1: (0.6792) | Acc_1: (75.45%) (31967/42368)\n",
      "Epoch: 9 | Batch_idx: 340 |  Loss_1: (0.6790) | Acc_1: (75.45%) (32933/43648)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss_1: (0.6790) | Acc_1: (75.42%) (33884/44928)\n",
      "Epoch: 9 | Batch_idx: 360 |  Loss_1: (0.6778) | Acc_1: (75.46%) (34870/46208)\n",
      "Epoch: 9 | Batch_idx: 370 |  Loss_1: (0.6779) | Acc_1: (75.47%) (35837/47488)\n",
      "Epoch: 9 | Batch_idx: 380 |  Loss_1: (0.6770) | Acc_1: (75.48%) (36812/48768)\n",
      "Epoch: 9 | Batch_idx: 390 |  Loss_1: (0.6764) | Acc_1: (75.49%) (37747/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5338) | Acc: (81.49%) (8149/10000)\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss_1: (0.7084) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 10 | Batch_idx: 10 |  Loss_1: (0.6516) | Acc_1: (76.78%) (1081/1408)\n",
      "Epoch: 10 | Batch_idx: 20 |  Loss_1: (0.6396) | Acc_1: (76.82%) (2065/2688)\n",
      "Epoch: 10 | Batch_idx: 30 |  Loss_1: (0.6468) | Acc_1: (76.59%) (3039/3968)\n",
      "Epoch: 10 | Batch_idx: 40 |  Loss_1: (0.6479) | Acc_1: (76.87%) (4034/5248)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss_1: (0.6649) | Acc_1: (76.15%) (4971/6528)\n",
      "Epoch: 10 | Batch_idx: 60 |  Loss_1: (0.6551) | Acc_1: (76.52%) (5975/7808)\n",
      "Epoch: 10 | Batch_idx: 70 |  Loss_1: (0.6512) | Acc_1: (76.75%) (6975/9088)\n",
      "Epoch: 10 | Batch_idx: 80 |  Loss_1: (0.6541) | Acc_1: (76.73%) (7955/10368)\n",
      "Epoch: 10 | Batch_idx: 90 |  Loss_1: (0.6518) | Acc_1: (76.89%) (8956/11648)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss_1: (0.6491) | Acc_1: (76.92%) (9944/12928)\n",
      "Epoch: 10 | Batch_idx: 110 |  Loss_1: (0.6495) | Acc_1: (76.89%) (10924/14208)\n",
      "Epoch: 10 | Batch_idx: 120 |  Loss_1: (0.6487) | Acc_1: (76.87%) (11906/15488)\n",
      "Epoch: 10 | Batch_idx: 130 |  Loss_1: (0.6489) | Acc_1: (76.87%) (12890/16768)\n",
      "Epoch: 10 | Batch_idx: 140 |  Loss_1: (0.6534) | Acc_1: (76.71%) (13844/18048)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss_1: (0.6505) | Acc_1: (76.70%) (14825/19328)\n",
      "Epoch: 10 | Batch_idx: 160 |  Loss_1: (0.6508) | Acc_1: (76.71%) (15808/20608)\n",
      "Epoch: 10 | Batch_idx: 170 |  Loss_1: (0.6525) | Acc_1: (76.66%) (16780/21888)\n",
      "Epoch: 10 | Batch_idx: 180 |  Loss_1: (0.6544) | Acc_1: (76.51%) (17725/23168)\n",
      "Epoch: 10 | Batch_idx: 190 |  Loss_1: (0.6538) | Acc_1: (76.48%) (18698/24448)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss_1: (0.6534) | Acc_1: (76.52%) (19687/25728)\n",
      "Epoch: 10 | Batch_idx: 210 |  Loss_1: (0.6533) | Acc_1: (76.48%) (20657/27008)\n",
      "Epoch: 10 | Batch_idx: 220 |  Loss_1: (0.6542) | Acc_1: (76.49%) (21638/28288)\n",
      "Epoch: 10 | Batch_idx: 230 |  Loss_1: (0.6556) | Acc_1: (76.45%) (22606/29568)\n",
      "Epoch: 10 | Batch_idx: 240 |  Loss_1: (0.6551) | Acc_1: (76.42%) (23573/30848)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss_1: (0.6542) | Acc_1: (76.42%) (24551/32128)\n",
      "Epoch: 10 | Batch_idx: 260 |  Loss_1: (0.6547) | Acc_1: (76.40%) (25525/33408)\n",
      "Epoch: 10 | Batch_idx: 270 |  Loss_1: (0.6578) | Acc_1: (76.25%) (26450/34688)\n",
      "Epoch: 10 | Batch_idx: 280 |  Loss_1: (0.6584) | Acc_1: (76.23%) (27417/35968)\n",
      "Epoch: 10 | Batch_idx: 290 |  Loss_1: (0.6591) | Acc_1: (76.21%) (28386/37248)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss_1: (0.6596) | Acc_1: (76.22%) (29365/38528)\n",
      "Epoch: 10 | Batch_idx: 310 |  Loss_1: (0.6607) | Acc_1: (76.18%) (30326/39808)\n",
      "Epoch: 10 | Batch_idx: 320 |  Loss_1: (0.6605) | Acc_1: (76.19%) (31304/41088)\n",
      "Epoch: 10 | Batch_idx: 330 |  Loss_1: (0.6586) | Acc_1: (76.27%) (32316/42368)\n",
      "Epoch: 10 | Batch_idx: 340 |  Loss_1: (0.6595) | Acc_1: (76.25%) (33281/43648)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss_1: (0.6604) | Acc_1: (76.22%) (34244/44928)\n",
      "Epoch: 10 | Batch_idx: 360 |  Loss_1: (0.6602) | Acc_1: (76.22%) (35222/46208)\n",
      "Epoch: 10 | Batch_idx: 370 |  Loss_1: (0.6614) | Acc_1: (76.19%) (36182/47488)\n",
      "Epoch: 10 | Batch_idx: 380 |  Loss_1: (0.6609) | Acc_1: (76.19%) (37158/48768)\n",
      "Epoch: 10 | Batch_idx: 390 |  Loss_1: (0.6603) | Acc_1: (76.20%) (38099/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4916) | Acc: (83.25%) (8325/10000)\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss_1: (0.7540) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 11 | Batch_idx: 10 |  Loss_1: (0.6411) | Acc_1: (75.57%) (1064/1408)\n",
      "Epoch: 11 | Batch_idx: 20 |  Loss_1: (0.6577) | Acc_1: (75.41%) (2027/2688)\n",
      "Epoch: 11 | Batch_idx: 30 |  Loss_1: (0.6465) | Acc_1: (75.83%) (3009/3968)\n",
      "Epoch: 11 | Batch_idx: 40 |  Loss_1: (0.6350) | Acc_1: (76.41%) (4010/5248)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss_1: (0.6402) | Acc_1: (76.38%) (4986/6528)\n",
      "Epoch: 11 | Batch_idx: 60 |  Loss_1: (0.6378) | Acc_1: (76.58%) (5979/7808)\n",
      "Epoch: 11 | Batch_idx: 70 |  Loss_1: (0.6331) | Acc_1: (76.85%) (6984/9088)\n",
      "Epoch: 11 | Batch_idx: 80 |  Loss_1: (0.6331) | Acc_1: (77.06%) (7990/10368)\n",
      "Epoch: 11 | Batch_idx: 90 |  Loss_1: (0.6299) | Acc_1: (77.11%) (8982/11648)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss_1: (0.6348) | Acc_1: (77.07%) (9963/12928)\n",
      "Epoch: 11 | Batch_idx: 110 |  Loss_1: (0.6341) | Acc_1: (77.01%) (10942/14208)\n",
      "Epoch: 11 | Batch_idx: 120 |  Loss_1: (0.6349) | Acc_1: (77.00%) (11925/15488)\n",
      "Epoch: 11 | Batch_idx: 130 |  Loss_1: (0.6343) | Acc_1: (76.99%) (12910/16768)\n",
      "Epoch: 11 | Batch_idx: 140 |  Loss_1: (0.6354) | Acc_1: (76.91%) (13881/18048)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss_1: (0.6359) | Acc_1: (76.92%) (14867/19328)\n",
      "Epoch: 11 | Batch_idx: 160 |  Loss_1: (0.6352) | Acc_1: (76.94%) (15856/20608)\n",
      "Epoch: 11 | Batch_idx: 170 |  Loss_1: (0.6344) | Acc_1: (76.95%) (16842/21888)\n",
      "Epoch: 11 | Batch_idx: 180 |  Loss_1: (0.6330) | Acc_1: (77.01%) (17841/23168)\n",
      "Epoch: 11 | Batch_idx: 190 |  Loss_1: (0.6327) | Acc_1: (76.98%) (18821/24448)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss_1: (0.6340) | Acc_1: (76.87%) (19776/25728)\n",
      "Epoch: 11 | Batch_idx: 210 |  Loss_1: (0.6317) | Acc_1: (76.97%) (20789/27008)\n",
      "Epoch: 11 | Batch_idx: 220 |  Loss_1: (0.6328) | Acc_1: (76.95%) (21767/28288)\n",
      "Epoch: 11 | Batch_idx: 230 |  Loss_1: (0.6335) | Acc_1: (76.89%) (22736/29568)\n",
      "Epoch: 11 | Batch_idx: 240 |  Loss_1: (0.6327) | Acc_1: (76.92%) (23727/30848)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss_1: (0.6330) | Acc_1: (76.95%) (24721/32128)\n",
      "Epoch: 11 | Batch_idx: 260 |  Loss_1: (0.6328) | Acc_1: (76.94%) (25703/33408)\n",
      "Epoch: 11 | Batch_idx: 270 |  Loss_1: (0.6348) | Acc_1: (76.86%) (26660/34688)\n",
      "Epoch: 11 | Batch_idx: 280 |  Loss_1: (0.6350) | Acc_1: (76.85%) (27640/35968)\n",
      "Epoch: 11 | Batch_idx: 290 |  Loss_1: (0.6343) | Acc_1: (76.87%) (28631/37248)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss_1: (0.6344) | Acc_1: (76.85%) (29610/38528)\n",
      "Epoch: 11 | Batch_idx: 310 |  Loss_1: (0.6346) | Acc_1: (76.82%) (30582/39808)\n",
      "Epoch: 11 | Batch_idx: 320 |  Loss_1: (0.6354) | Acc_1: (76.80%) (31557/41088)\n",
      "Epoch: 11 | Batch_idx: 330 |  Loss_1: (0.6352) | Acc_1: (76.78%) (32529/42368)\n",
      "Epoch: 11 | Batch_idx: 340 |  Loss_1: (0.6351) | Acc_1: (76.80%) (33520/43648)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss_1: (0.6344) | Acc_1: (76.81%) (34508/44928)\n",
      "Epoch: 11 | Batch_idx: 360 |  Loss_1: (0.6351) | Acc_1: (76.80%) (35490/46208)\n",
      "Epoch: 11 | Batch_idx: 370 |  Loss_1: (0.6347) | Acc_1: (76.83%) (36483/47488)\n",
      "Epoch: 11 | Batch_idx: 380 |  Loss_1: (0.6355) | Acc_1: (76.81%) (37459/48768)\n",
      "Epoch: 11 | Batch_idx: 390 |  Loss_1: (0.6359) | Acc_1: (76.79%) (38395/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5118) | Acc: (82.73%) (8273/10000)\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss_1: (0.5755) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 12 | Batch_idx: 10 |  Loss_1: (0.6293) | Acc_1: (76.99%) (1084/1408)\n",
      "Epoch: 12 | Batch_idx: 20 |  Loss_1: (0.6065) | Acc_1: (77.75%) (2090/2688)\n",
      "Epoch: 12 | Batch_idx: 30 |  Loss_1: (0.6159) | Acc_1: (77.22%) (3064/3968)\n",
      "Epoch: 12 | Batch_idx: 40 |  Loss_1: (0.6278) | Acc_1: (77.04%) (4043/5248)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss_1: (0.6201) | Acc_1: (77.37%) (5051/6528)\n",
      "Epoch: 12 | Batch_idx: 60 |  Loss_1: (0.6211) | Acc_1: (77.38%) (6042/7808)\n",
      "Epoch: 12 | Batch_idx: 70 |  Loss_1: (0.6176) | Acc_1: (77.41%) (7035/9088)\n",
      "Epoch: 12 | Batch_idx: 80 |  Loss_1: (0.6205) | Acc_1: (77.36%) (8021/10368)\n",
      "Epoch: 12 | Batch_idx: 90 |  Loss_1: (0.6241) | Acc_1: (77.33%) (9007/11648)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss_1: (0.6252) | Acc_1: (77.34%) (9998/12928)\n",
      "Epoch: 12 | Batch_idx: 110 |  Loss_1: (0.6212) | Acc_1: (77.47%) (11007/14208)\n",
      "Epoch: 12 | Batch_idx: 120 |  Loss_1: (0.6214) | Acc_1: (77.43%) (11992/15488)\n",
      "Epoch: 12 | Batch_idx: 130 |  Loss_1: (0.6183) | Acc_1: (77.58%) (13008/16768)\n",
      "Epoch: 12 | Batch_idx: 140 |  Loss_1: (0.6148) | Acc_1: (77.67%) (14018/18048)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss_1: (0.6149) | Acc_1: (77.66%) (15011/19328)\n",
      "Epoch: 12 | Batch_idx: 160 |  Loss_1: (0.6183) | Acc_1: (77.60%) (15992/20608)\n",
      "Epoch: 12 | Batch_idx: 170 |  Loss_1: (0.6200) | Acc_1: (77.46%) (16954/21888)\n",
      "Epoch: 12 | Batch_idx: 180 |  Loss_1: (0.6178) | Acc_1: (77.47%) (17949/23168)\n",
      "Epoch: 12 | Batch_idx: 190 |  Loss_1: (0.6188) | Acc_1: (77.41%) (18926/24448)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss_1: (0.6204) | Acc_1: (77.38%) (19909/25728)\n",
      "Epoch: 12 | Batch_idx: 210 |  Loss_1: (0.6195) | Acc_1: (77.40%) (20904/27008)\n",
      "Epoch: 12 | Batch_idx: 220 |  Loss_1: (0.6172) | Acc_1: (77.45%) (21910/28288)\n",
      "Epoch: 12 | Batch_idx: 230 |  Loss_1: (0.6155) | Acc_1: (77.51%) (22918/29568)\n",
      "Epoch: 12 | Batch_idx: 240 |  Loss_1: (0.6148) | Acc_1: (77.55%) (23923/30848)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss_1: (0.6162) | Acc_1: (77.47%) (24890/32128)\n",
      "Epoch: 12 | Batch_idx: 260 |  Loss_1: (0.6179) | Acc_1: (77.38%) (25852/33408)\n",
      "Epoch: 12 | Batch_idx: 270 |  Loss_1: (0.6168) | Acc_1: (77.45%) (26866/34688)\n",
      "Epoch: 12 | Batch_idx: 280 |  Loss_1: (0.6180) | Acc_1: (77.40%) (27840/35968)\n",
      "Epoch: 12 | Batch_idx: 290 |  Loss_1: (0.6178) | Acc_1: (77.41%) (28835/37248)\n",
      "Epoch: 12 | Batch_idx: 300 |  Loss_1: (0.6172) | Acc_1: (77.41%) (29826/38528)\n",
      "Epoch: 12 | Batch_idx: 310 |  Loss_1: (0.6156) | Acc_1: (77.49%) (30849/39808)\n",
      "Epoch: 12 | Batch_idx: 320 |  Loss_1: (0.6159) | Acc_1: (77.49%) (31840/41088)\n",
      "Epoch: 12 | Batch_idx: 330 |  Loss_1: (0.6165) | Acc_1: (77.47%) (32824/42368)\n",
      "Epoch: 12 | Batch_idx: 340 |  Loss_1: (0.6167) | Acc_1: (77.47%) (33812/43648)\n",
      "Epoch: 12 | Batch_idx: 350 |  Loss_1: (0.6165) | Acc_1: (77.47%) (34806/44928)\n",
      "Epoch: 12 | Batch_idx: 360 |  Loss_1: (0.6166) | Acc_1: (77.47%) (35799/46208)\n",
      "Epoch: 12 | Batch_idx: 370 |  Loss_1: (0.6167) | Acc_1: (77.49%) (36798/47488)\n",
      "Epoch: 12 | Batch_idx: 380 |  Loss_1: (0.6172) | Acc_1: (77.48%) (37786/48768)\n",
      "Epoch: 12 | Batch_idx: 390 |  Loss_1: (0.6169) | Acc_1: (77.52%) (38760/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5052) | Acc: (82.50%) (8250/10000)\n",
      "Epoch: 13 | Batch_idx: 0 |  Loss_1: (0.5212) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 13 | Batch_idx: 10 |  Loss_1: (0.6146) | Acc_1: (77.34%) (1089/1408)\n",
      "Epoch: 13 | Batch_idx: 20 |  Loss_1: (0.6002) | Acc_1: (77.86%) (2093/2688)\n",
      "Epoch: 13 | Batch_idx: 30 |  Loss_1: (0.5968) | Acc_1: (78.00%) (3095/3968)\n",
      "Epoch: 13 | Batch_idx: 40 |  Loss_1: (0.5807) | Acc_1: (78.72%) (4131/5248)\n",
      "Epoch: 13 | Batch_idx: 50 |  Loss_1: (0.5781) | Acc_1: (79.01%) (5158/6528)\n",
      "Epoch: 13 | Batch_idx: 60 |  Loss_1: (0.5869) | Acc_1: (78.57%) (6135/7808)\n",
      "Epoch: 13 | Batch_idx: 70 |  Loss_1: (0.5916) | Acc_1: (78.39%) (7124/9088)\n",
      "Epoch: 13 | Batch_idx: 80 |  Loss_1: (0.5886) | Acc_1: (78.42%) (8131/10368)\n",
      "Epoch: 13 | Batch_idx: 90 |  Loss_1: (0.5878) | Acc_1: (78.31%) (9122/11648)\n",
      "Epoch: 13 | Batch_idx: 100 |  Loss_1: (0.5880) | Acc_1: (78.34%) (10128/12928)\n",
      "Epoch: 13 | Batch_idx: 110 |  Loss_1: (0.5868) | Acc_1: (78.37%) (11135/14208)\n",
      "Epoch: 13 | Batch_idx: 120 |  Loss_1: (0.5848) | Acc_1: (78.47%) (12153/15488)\n",
      "Epoch: 13 | Batch_idx: 130 |  Loss_1: (0.5883) | Acc_1: (78.34%) (13136/16768)\n",
      "Epoch: 13 | Batch_idx: 140 |  Loss_1: (0.5884) | Acc_1: (78.30%) (14132/18048)\n",
      "Epoch: 13 | Batch_idx: 150 |  Loss_1: (0.5902) | Acc_1: (78.22%) (15119/19328)\n",
      "Epoch: 13 | Batch_idx: 160 |  Loss_1: (0.5897) | Acc_1: (78.29%) (16134/20608)\n",
      "Epoch: 13 | Batch_idx: 170 |  Loss_1: (0.5901) | Acc_1: (78.28%) (17135/21888)\n",
      "Epoch: 13 | Batch_idx: 180 |  Loss_1: (0.5913) | Acc_1: (78.31%) (18143/23168)\n",
      "Epoch: 13 | Batch_idx: 190 |  Loss_1: (0.5900) | Acc_1: (78.40%) (19167/24448)\n",
      "Epoch: 13 | Batch_idx: 200 |  Loss_1: (0.5906) | Acc_1: (78.39%) (20167/25728)\n",
      "Epoch: 13 | Batch_idx: 210 |  Loss_1: (0.5931) | Acc_1: (78.24%) (21132/27008)\n",
      "Epoch: 13 | Batch_idx: 220 |  Loss_1: (0.5899) | Acc_1: (78.31%) (22152/28288)\n",
      "Epoch: 13 | Batch_idx: 230 |  Loss_1: (0.5919) | Acc_1: (78.25%) (23136/29568)\n",
      "Epoch: 13 | Batch_idx: 240 |  Loss_1: (0.5931) | Acc_1: (78.23%) (24132/30848)\n",
      "Epoch: 13 | Batch_idx: 250 |  Loss_1: (0.5932) | Acc_1: (78.25%) (25141/32128)\n",
      "Epoch: 13 | Batch_idx: 260 |  Loss_1: (0.5932) | Acc_1: (78.30%) (26159/33408)\n",
      "Epoch: 13 | Batch_idx: 270 |  Loss_1: (0.5943) | Acc_1: (78.25%) (27143/34688)\n",
      "Epoch: 13 | Batch_idx: 280 |  Loss_1: (0.5957) | Acc_1: (78.20%) (28127/35968)\n",
      "Epoch: 13 | Batch_idx: 290 |  Loss_1: (0.5952) | Acc_1: (78.19%) (29126/37248)\n",
      "Epoch: 13 | Batch_idx: 300 |  Loss_1: (0.5951) | Acc_1: (78.20%) (30129/38528)\n",
      "Epoch: 13 | Batch_idx: 310 |  Loss_1: (0.5952) | Acc_1: (78.21%) (31133/39808)\n",
      "Epoch: 13 | Batch_idx: 320 |  Loss_1: (0.5957) | Acc_1: (78.19%) (32127/41088)\n",
      "Epoch: 13 | Batch_idx: 330 |  Loss_1: (0.5964) | Acc_1: (78.20%) (33131/42368)\n",
      "Epoch: 13 | Batch_idx: 340 |  Loss_1: (0.5963) | Acc_1: (78.22%) (34141/43648)\n",
      "Epoch: 13 | Batch_idx: 350 |  Loss_1: (0.5963) | Acc_1: (78.19%) (35128/44928)\n",
      "Epoch: 13 | Batch_idx: 360 |  Loss_1: (0.5977) | Acc_1: (78.14%) (36106/46208)\n",
      "Epoch: 13 | Batch_idx: 370 |  Loss_1: (0.5984) | Acc_1: (78.12%) (37098/47488)\n",
      "Epoch: 13 | Batch_idx: 380 |  Loss_1: (0.5977) | Acc_1: (78.14%) (38106/48768)\n",
      "Epoch: 13 | Batch_idx: 390 |  Loss_1: (0.5994) | Acc_1: (78.08%) (39042/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4189) | Acc: (85.84%) (8584/10000)\n",
      "Epoch: 14 | Batch_idx: 0 |  Loss_1: (0.5457) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 14 | Batch_idx: 10 |  Loss_1: (0.5676) | Acc_1: (80.26%) (1130/1408)\n",
      "Epoch: 14 | Batch_idx: 20 |  Loss_1: (0.5466) | Acc_1: (80.65%) (2168/2688)\n",
      "Epoch: 14 | Batch_idx: 30 |  Loss_1: (0.5598) | Acc_1: (80.32%) (3187/3968)\n",
      "Epoch: 14 | Batch_idx: 40 |  Loss_1: (0.5621) | Acc_1: (80.01%) (4199/5248)\n",
      "Epoch: 14 | Batch_idx: 50 |  Loss_1: (0.5649) | Acc_1: (79.67%) (5201/6528)\n",
      "Epoch: 14 | Batch_idx: 60 |  Loss_1: (0.5728) | Acc_1: (79.41%) (6200/7808)\n",
      "Epoch: 14 | Batch_idx: 70 |  Loss_1: (0.5733) | Acc_1: (79.37%) (7213/9088)\n",
      "Epoch: 14 | Batch_idx: 80 |  Loss_1: (0.5747) | Acc_1: (79.23%) (8215/10368)\n",
      "Epoch: 14 | Batch_idx: 90 |  Loss_1: (0.5686) | Acc_1: (79.34%) (9241/11648)\n",
      "Epoch: 14 | Batch_idx: 100 |  Loss_1: (0.5683) | Acc_1: (79.36%) (10260/12928)\n",
      "Epoch: 14 | Batch_idx: 110 |  Loss_1: (0.5684) | Acc_1: (79.36%) (11275/14208)\n",
      "Epoch: 14 | Batch_idx: 120 |  Loss_1: (0.5688) | Acc_1: (79.32%) (12285/15488)\n",
      "Epoch: 14 | Batch_idx: 130 |  Loss_1: (0.5675) | Acc_1: (79.41%) (13315/16768)\n",
      "Epoch: 14 | Batch_idx: 140 |  Loss_1: (0.5695) | Acc_1: (79.33%) (14318/18048)\n",
      "Epoch: 14 | Batch_idx: 150 |  Loss_1: (0.5725) | Acc_1: (79.19%) (15306/19328)\n",
      "Epoch: 14 | Batch_idx: 160 |  Loss_1: (0.5744) | Acc_1: (79.09%) (16299/20608)\n",
      "Epoch: 14 | Batch_idx: 170 |  Loss_1: (0.5729) | Acc_1: (79.16%) (17327/21888)\n",
      "Epoch: 14 | Batch_idx: 180 |  Loss_1: (0.5762) | Acc_1: (79.05%) (18314/23168)\n",
      "Epoch: 14 | Batch_idx: 190 |  Loss_1: (0.5772) | Acc_1: (79.06%) (19328/24448)\n",
      "Epoch: 14 | Batch_idx: 200 |  Loss_1: (0.5805) | Acc_1: (78.96%) (20316/25728)\n",
      "Epoch: 14 | Batch_idx: 210 |  Loss_1: (0.5789) | Acc_1: (79.01%) (21340/27008)\n",
      "Epoch: 14 | Batch_idx: 220 |  Loss_1: (0.5766) | Acc_1: (79.12%) (22382/28288)\n",
      "Epoch: 14 | Batch_idx: 230 |  Loss_1: (0.5758) | Acc_1: (79.15%) (23404/29568)\n",
      "Epoch: 14 | Batch_idx: 240 |  Loss_1: (0.5776) | Acc_1: (79.08%) (24395/30848)\n",
      "Epoch: 14 | Batch_idx: 250 |  Loss_1: (0.5795) | Acc_1: (78.96%) (25369/32128)\n",
      "Epoch: 14 | Batch_idx: 260 |  Loss_1: (0.5795) | Acc_1: (78.98%) (26386/33408)\n",
      "Epoch: 14 | Batch_idx: 270 |  Loss_1: (0.5821) | Acc_1: (78.92%) (27375/34688)\n",
      "Epoch: 14 | Batch_idx: 280 |  Loss_1: (0.5841) | Acc_1: (78.86%) (28366/35968)\n",
      "Epoch: 14 | Batch_idx: 290 |  Loss_1: (0.5840) | Acc_1: (78.88%) (29381/37248)\n",
      "Epoch: 14 | Batch_idx: 300 |  Loss_1: (0.5836) | Acc_1: (78.86%) (30385/38528)\n",
      "Epoch: 14 | Batch_idx: 310 |  Loss_1: (0.5839) | Acc_1: (78.82%) (31376/39808)\n",
      "Epoch: 14 | Batch_idx: 320 |  Loss_1: (0.5850) | Acc_1: (78.77%) (32367/41088)\n",
      "Epoch: 14 | Batch_idx: 330 |  Loss_1: (0.5855) | Acc_1: (78.75%) (33364/42368)\n",
      "Epoch: 14 | Batch_idx: 340 |  Loss_1: (0.5865) | Acc_1: (78.72%) (34360/43648)\n",
      "Epoch: 14 | Batch_idx: 350 |  Loss_1: (0.5868) | Acc_1: (78.72%) (35368/44928)\n",
      "Epoch: 14 | Batch_idx: 360 |  Loss_1: (0.5861) | Acc_1: (78.74%) (36386/46208)\n",
      "Epoch: 14 | Batch_idx: 370 |  Loss_1: (0.5876) | Acc_1: (78.70%) (37371/47488)\n",
      "Epoch: 14 | Batch_idx: 380 |  Loss_1: (0.5887) | Acc_1: (78.64%) (38351/48768)\n",
      "Epoch: 14 | Batch_idx: 390 |  Loss_1: (0.5883) | Acc_1: (78.65%) (39327/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4389) | Acc: (85.46%) (8546/10000)\n",
      "Epoch: 15 | Batch_idx: 0 |  Loss_1: (0.5345) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 15 | Batch_idx: 10 |  Loss_1: (0.5237) | Acc_1: (81.25%) (1144/1408)\n",
      "Epoch: 15 | Batch_idx: 20 |  Loss_1: (0.5345) | Acc_1: (80.32%) (2159/2688)\n",
      "Epoch: 15 | Batch_idx: 30 |  Loss_1: (0.5361) | Acc_1: (80.54%) (3196/3968)\n",
      "Epoch: 15 | Batch_idx: 40 |  Loss_1: (0.5396) | Acc_1: (80.56%) (4228/5248)\n",
      "Epoch: 15 | Batch_idx: 50 |  Loss_1: (0.5437) | Acc_1: (80.13%) (5231/6528)\n",
      "Epoch: 15 | Batch_idx: 60 |  Loss_1: (0.5553) | Acc_1: (79.74%) (6226/7808)\n",
      "Epoch: 15 | Batch_idx: 70 |  Loss_1: (0.5626) | Acc_1: (79.52%) (7227/9088)\n",
      "Epoch: 15 | Batch_idx: 80 |  Loss_1: (0.5630) | Acc_1: (79.51%) (8244/10368)\n",
      "Epoch: 15 | Batch_idx: 90 |  Loss_1: (0.5659) | Acc_1: (79.45%) (9254/11648)\n",
      "Epoch: 15 | Batch_idx: 100 |  Loss_1: (0.5687) | Acc_1: (79.32%) (10255/12928)\n",
      "Epoch: 15 | Batch_idx: 110 |  Loss_1: (0.5714) | Acc_1: (79.20%) (11253/14208)\n",
      "Epoch: 15 | Batch_idx: 120 |  Loss_1: (0.5695) | Acc_1: (79.29%) (12281/15488)\n",
      "Epoch: 15 | Batch_idx: 130 |  Loss_1: (0.5743) | Acc_1: (79.14%) (13271/16768)\n",
      "Epoch: 15 | Batch_idx: 140 |  Loss_1: (0.5745) | Acc_1: (79.12%) (14280/18048)\n",
      "Epoch: 15 | Batch_idx: 150 |  Loss_1: (0.5753) | Acc_1: (79.06%) (15281/19328)\n",
      "Epoch: 15 | Batch_idx: 160 |  Loss_1: (0.5745) | Acc_1: (79.11%) (16303/20608)\n",
      "Epoch: 15 | Batch_idx: 170 |  Loss_1: (0.5734) | Acc_1: (79.16%) (17326/21888)\n",
      "Epoch: 15 | Batch_idx: 180 |  Loss_1: (0.5750) | Acc_1: (79.06%) (18316/23168)\n",
      "Epoch: 15 | Batch_idx: 190 |  Loss_1: (0.5747) | Acc_1: (79.06%) (19328/24448)\n",
      "Epoch: 15 | Batch_idx: 200 |  Loss_1: (0.5745) | Acc_1: (79.03%) (20334/25728)\n",
      "Epoch: 15 | Batch_idx: 210 |  Loss_1: (0.5756) | Acc_1: (78.97%) (21328/27008)\n",
      "Epoch: 15 | Batch_idx: 220 |  Loss_1: (0.5749) | Acc_1: (79.00%) (22348/28288)\n",
      "Epoch: 15 | Batch_idx: 230 |  Loss_1: (0.5755) | Acc_1: (78.97%) (23349/29568)\n",
      "Epoch: 15 | Batch_idx: 240 |  Loss_1: (0.5769) | Acc_1: (78.90%) (24339/30848)\n",
      "Epoch: 15 | Batch_idx: 250 |  Loss_1: (0.5760) | Acc_1: (78.92%) (25355/32128)\n",
      "Epoch: 15 | Batch_idx: 260 |  Loss_1: (0.5756) | Acc_1: (78.97%) (26382/33408)\n",
      "Epoch: 15 | Batch_idx: 270 |  Loss_1: (0.5746) | Acc_1: (78.98%) (27396/34688)\n",
      "Epoch: 15 | Batch_idx: 280 |  Loss_1: (0.5744) | Acc_1: (78.99%) (28412/35968)\n",
      "Epoch: 15 | Batch_idx: 290 |  Loss_1: (0.5750) | Acc_1: (78.94%) (29402/37248)\n",
      "Epoch: 15 | Batch_idx: 300 |  Loss_1: (0.5749) | Acc_1: (78.95%) (30416/38528)\n",
      "Epoch: 15 | Batch_idx: 310 |  Loss_1: (0.5755) | Acc_1: (78.88%) (31400/39808)\n",
      "Epoch: 15 | Batch_idx: 320 |  Loss_1: (0.5755) | Acc_1: (78.88%) (32410/41088)\n",
      "Epoch: 15 | Batch_idx: 330 |  Loss_1: (0.5734) | Acc_1: (78.94%) (33447/42368)\n",
      "Epoch: 15 | Batch_idx: 340 |  Loss_1: (0.5737) | Acc_1: (78.96%) (34464/43648)\n",
      "Epoch: 15 | Batch_idx: 350 |  Loss_1: (0.5728) | Acc_1: (78.98%) (35484/44928)\n",
      "Epoch: 15 | Batch_idx: 360 |  Loss_1: (0.5726) | Acc_1: (78.97%) (36492/46208)\n",
      "Epoch: 15 | Batch_idx: 370 |  Loss_1: (0.5739) | Acc_1: (78.93%) (37483/47488)\n",
      "Epoch: 15 | Batch_idx: 380 |  Loss_1: (0.5740) | Acc_1: (78.92%) (38489/48768)\n",
      "Epoch: 15 | Batch_idx: 390 |  Loss_1: (0.5731) | Acc_1: (78.98%) (39488/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4658) | Acc: (84.22%) (8422/10000)\n",
      "Epoch: 16 | Batch_idx: 0 |  Loss_1: (0.5492) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 16 | Batch_idx: 10 |  Loss_1: (0.5866) | Acc_1: (78.84%) (1110/1408)\n",
      "Epoch: 16 | Batch_idx: 20 |  Loss_1: (0.5652) | Acc_1: (79.58%) (2139/2688)\n",
      "Epoch: 16 | Batch_idx: 30 |  Loss_1: (0.5826) | Acc_1: (78.93%) (3132/3968)\n",
      "Epoch: 16 | Batch_idx: 40 |  Loss_1: (0.5891) | Acc_1: (78.66%) (4128/5248)\n",
      "Epoch: 16 | Batch_idx: 50 |  Loss_1: (0.5822) | Acc_1: (78.83%) (5146/6528)\n",
      "Epoch: 16 | Batch_idx: 60 |  Loss_1: (0.5791) | Acc_1: (78.75%) (6149/7808)\n",
      "Epoch: 16 | Batch_idx: 70 |  Loss_1: (0.5742) | Acc_1: (78.96%) (7176/9088)\n",
      "Epoch: 16 | Batch_idx: 80 |  Loss_1: (0.5715) | Acc_1: (79.10%) (8201/10368)\n",
      "Epoch: 16 | Batch_idx: 90 |  Loss_1: (0.5694) | Acc_1: (79.22%) (9228/11648)\n",
      "Epoch: 16 | Batch_idx: 100 |  Loss_1: (0.5655) | Acc_1: (79.39%) (10263/12928)\n",
      "Epoch: 16 | Batch_idx: 110 |  Loss_1: (0.5656) | Acc_1: (79.45%) (11288/14208)\n",
      "Epoch: 16 | Batch_idx: 120 |  Loss_1: (0.5630) | Acc_1: (79.52%) (12316/15488)\n",
      "Epoch: 16 | Batch_idx: 130 |  Loss_1: (0.5645) | Acc_1: (79.41%) (13316/16768)\n",
      "Epoch: 16 | Batch_idx: 140 |  Loss_1: (0.5667) | Acc_1: (79.36%) (14322/18048)\n",
      "Epoch: 16 | Batch_idx: 150 |  Loss_1: (0.5684) | Acc_1: (79.31%) (15330/19328)\n",
      "Epoch: 16 | Batch_idx: 160 |  Loss_1: (0.5656) | Acc_1: (79.40%) (16362/20608)\n",
      "Epoch: 16 | Batch_idx: 170 |  Loss_1: (0.5655) | Acc_1: (79.41%) (17381/21888)\n",
      "Epoch: 16 | Batch_idx: 180 |  Loss_1: (0.5660) | Acc_1: (79.35%) (18383/23168)\n",
      "Epoch: 16 | Batch_idx: 190 |  Loss_1: (0.5665) | Acc_1: (79.29%) (19384/24448)\n",
      "Epoch: 16 | Batch_idx: 200 |  Loss_1: (0.5674) | Acc_1: (79.22%) (20383/25728)\n",
      "Epoch: 16 | Batch_idx: 210 |  Loss_1: (0.5685) | Acc_1: (79.15%) (21377/27008)\n",
      "Epoch: 16 | Batch_idx: 220 |  Loss_1: (0.5685) | Acc_1: (79.11%) (22380/28288)\n",
      "Epoch: 16 | Batch_idx: 230 |  Loss_1: (0.5669) | Acc_1: (79.14%) (23400/29568)\n",
      "Epoch: 16 | Batch_idx: 240 |  Loss_1: (0.5665) | Acc_1: (79.15%) (24415/30848)\n",
      "Epoch: 16 | Batch_idx: 250 |  Loss_1: (0.5660) | Acc_1: (79.14%) (25426/32128)\n",
      "Epoch: 16 | Batch_idx: 260 |  Loss_1: (0.5636) | Acc_1: (79.27%) (26482/33408)\n",
      "Epoch: 16 | Batch_idx: 270 |  Loss_1: (0.5642) | Acc_1: (79.23%) (27482/34688)\n",
      "Epoch: 16 | Batch_idx: 280 |  Loss_1: (0.5627) | Acc_1: (79.26%) (28509/35968)\n",
      "Epoch: 16 | Batch_idx: 290 |  Loss_1: (0.5642) | Acc_1: (79.23%) (29510/37248)\n",
      "Epoch: 16 | Batch_idx: 300 |  Loss_1: (0.5636) | Acc_1: (79.26%) (30539/38528)\n",
      "Epoch: 16 | Batch_idx: 310 |  Loss_1: (0.5635) | Acc_1: (79.27%) (31557/39808)\n",
      "Epoch: 16 | Batch_idx: 320 |  Loss_1: (0.5639) | Acc_1: (79.27%) (32572/41088)\n",
      "Epoch: 16 | Batch_idx: 330 |  Loss_1: (0.5643) | Acc_1: (79.25%) (33578/42368)\n",
      "Epoch: 16 | Batch_idx: 340 |  Loss_1: (0.5637) | Acc_1: (79.27%) (34599/43648)\n",
      "Epoch: 16 | Batch_idx: 350 |  Loss_1: (0.5639) | Acc_1: (79.26%) (35609/44928)\n",
      "Epoch: 16 | Batch_idx: 360 |  Loss_1: (0.5645) | Acc_1: (79.24%) (36617/46208)\n",
      "Epoch: 16 | Batch_idx: 370 |  Loss_1: (0.5647) | Acc_1: (79.25%) (37636/47488)\n",
      "Epoch: 16 | Batch_idx: 380 |  Loss_1: (0.5644) | Acc_1: (79.26%) (38655/48768)\n",
      "Epoch: 16 | Batch_idx: 390 |  Loss_1: (0.5645) | Acc_1: (79.29%) (39644/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4378) | Acc: (85.38%) (8538/10000)\n",
      "Epoch: 17 | Batch_idx: 0 |  Loss_1: (0.6292) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 17 | Batch_idx: 10 |  Loss_1: (0.5564) | Acc_1: (79.47%) (1119/1408)\n",
      "Epoch: 17 | Batch_idx: 20 |  Loss_1: (0.5448) | Acc_1: (79.91%) (2148/2688)\n",
      "Epoch: 17 | Batch_idx: 30 |  Loss_1: (0.5269) | Acc_1: (80.54%) (3196/3968)\n",
      "Epoch: 17 | Batch_idx: 40 |  Loss_1: (0.5301) | Acc_1: (80.22%) (4210/5248)\n",
      "Epoch: 17 | Batch_idx: 50 |  Loss_1: (0.5268) | Acc_1: (80.41%) (5249/6528)\n",
      "Epoch: 17 | Batch_idx: 60 |  Loss_1: (0.5314) | Acc_1: (80.25%) (6266/7808)\n",
      "Epoch: 17 | Batch_idx: 70 |  Loss_1: (0.5320) | Acc_1: (80.24%) (7292/9088)\n",
      "Epoch: 17 | Batch_idx: 80 |  Loss_1: (0.5280) | Acc_1: (80.36%) (8332/10368)\n",
      "Epoch: 17 | Batch_idx: 90 |  Loss_1: (0.5297) | Acc_1: (80.28%) (9351/11648)\n",
      "Epoch: 17 | Batch_idx: 100 |  Loss_1: (0.5293) | Acc_1: (80.33%) (10385/12928)\n",
      "Epoch: 17 | Batch_idx: 110 |  Loss_1: (0.5328) | Acc_1: (80.17%) (11391/14208)\n",
      "Epoch: 17 | Batch_idx: 120 |  Loss_1: (0.5336) | Acc_1: (80.10%) (12406/15488)\n",
      "Epoch: 17 | Batch_idx: 130 |  Loss_1: (0.5346) | Acc_1: (80.15%) (13439/16768)\n",
      "Epoch: 17 | Batch_idx: 140 |  Loss_1: (0.5367) | Acc_1: (80.13%) (14462/18048)\n",
      "Epoch: 17 | Batch_idx: 150 |  Loss_1: (0.5367) | Acc_1: (80.12%) (15486/19328)\n",
      "Epoch: 17 | Batch_idx: 160 |  Loss_1: (0.5380) | Acc_1: (80.10%) (16507/20608)\n",
      "Epoch: 17 | Batch_idx: 170 |  Loss_1: (0.5390) | Acc_1: (80.08%) (17527/21888)\n",
      "Epoch: 17 | Batch_idx: 180 |  Loss_1: (0.5387) | Acc_1: (80.05%) (18547/23168)\n",
      "Epoch: 17 | Batch_idx: 190 |  Loss_1: (0.5395) | Acc_1: (80.07%) (19576/24448)\n",
      "Epoch: 17 | Batch_idx: 200 |  Loss_1: (0.5386) | Acc_1: (80.13%) (20615/25728)\n",
      "Epoch: 17 | Batch_idx: 210 |  Loss_1: (0.5398) | Acc_1: (80.08%) (21627/27008)\n",
      "Epoch: 17 | Batch_idx: 220 |  Loss_1: (0.5414) | Acc_1: (80.04%) (22641/28288)\n",
      "Epoch: 17 | Batch_idx: 230 |  Loss_1: (0.5403) | Acc_1: (80.06%) (23671/29568)\n",
      "Epoch: 17 | Batch_idx: 240 |  Loss_1: (0.5401) | Acc_1: (80.07%) (24700/30848)\n",
      "Epoch: 17 | Batch_idx: 250 |  Loss_1: (0.5404) | Acc_1: (80.07%) (25724/32128)\n",
      "Epoch: 17 | Batch_idx: 260 |  Loss_1: (0.5392) | Acc_1: (80.13%) (26771/33408)\n",
      "Epoch: 17 | Batch_idx: 270 |  Loss_1: (0.5392) | Acc_1: (80.10%) (27784/34688)\n",
      "Epoch: 17 | Batch_idx: 280 |  Loss_1: (0.5410) | Acc_1: (80.05%) (28794/35968)\n",
      "Epoch: 17 | Batch_idx: 290 |  Loss_1: (0.5410) | Acc_1: (80.03%) (29810/37248)\n",
      "Epoch: 17 | Batch_idx: 300 |  Loss_1: (0.5420) | Acc_1: (80.00%) (30823/38528)\n",
      "Epoch: 17 | Batch_idx: 310 |  Loss_1: (0.5419) | Acc_1: (79.99%) (31841/39808)\n",
      "Epoch: 17 | Batch_idx: 320 |  Loss_1: (0.5423) | Acc_1: (79.98%) (32861/41088)\n",
      "Epoch: 17 | Batch_idx: 330 |  Loss_1: (0.5438) | Acc_1: (79.95%) (33873/42368)\n",
      "Epoch: 17 | Batch_idx: 340 |  Loss_1: (0.5434) | Acc_1: (79.98%) (34911/43648)\n",
      "Epoch: 17 | Batch_idx: 350 |  Loss_1: (0.5432) | Acc_1: (79.99%) (35937/44928)\n",
      "Epoch: 17 | Batch_idx: 360 |  Loss_1: (0.5434) | Acc_1: (79.98%) (36957/46208)\n",
      "Epoch: 17 | Batch_idx: 370 |  Loss_1: (0.5419) | Acc_1: (80.03%) (38006/47488)\n",
      "Epoch: 17 | Batch_idx: 380 |  Loss_1: (0.5416) | Acc_1: (80.05%) (39037/48768)\n",
      "Epoch: 17 | Batch_idx: 390 |  Loss_1: (0.5413) | Acc_1: (80.08%) (40041/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3978) | Acc: (86.87%) (8687/10000)\n",
      "Epoch: 18 | Batch_idx: 0 |  Loss_1: (0.4193) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 18 | Batch_idx: 10 |  Loss_1: (0.5479) | Acc_1: (78.76%) (1109/1408)\n",
      "Epoch: 18 | Batch_idx: 20 |  Loss_1: (0.5324) | Acc_1: (79.80%) (2145/2688)\n",
      "Epoch: 18 | Batch_idx: 30 |  Loss_1: (0.5032) | Acc_1: (81.02%) (3215/3968)\n",
      "Epoch: 18 | Batch_idx: 40 |  Loss_1: (0.5134) | Acc_1: (80.75%) (4238/5248)\n",
      "Epoch: 18 | Batch_idx: 50 |  Loss_1: (0.5089) | Acc_1: (81.04%) (5290/6528)\n",
      "Epoch: 18 | Batch_idx: 60 |  Loss_1: (0.5151) | Acc_1: (80.93%) (6319/7808)\n",
      "Epoch: 18 | Batch_idx: 70 |  Loss_1: (0.5163) | Acc_1: (80.82%) (7345/9088)\n",
      "Epoch: 18 | Batch_idx: 80 |  Loss_1: (0.5218) | Acc_1: (80.60%) (8357/10368)\n",
      "Epoch: 18 | Batch_idx: 90 |  Loss_1: (0.5227) | Acc_1: (80.56%) (9384/11648)\n",
      "Epoch: 18 | Batch_idx: 100 |  Loss_1: (0.5264) | Acc_1: (80.44%) (10399/12928)\n",
      "Epoch: 18 | Batch_idx: 110 |  Loss_1: (0.5300) | Acc_1: (80.37%) (11419/14208)\n",
      "Epoch: 18 | Batch_idx: 120 |  Loss_1: (0.5307) | Acc_1: (80.35%) (12445/15488)\n",
      "Epoch: 18 | Batch_idx: 130 |  Loss_1: (0.5286) | Acc_1: (80.47%) (13493/16768)\n",
      "Epoch: 18 | Batch_idx: 140 |  Loss_1: (0.5287) | Acc_1: (80.46%) (14522/18048)\n",
      "Epoch: 18 | Batch_idx: 150 |  Loss_1: (0.5274) | Acc_1: (80.52%) (15562/19328)\n",
      "Epoch: 18 | Batch_idx: 160 |  Loss_1: (0.5315) | Acc_1: (80.38%) (16565/20608)\n",
      "Epoch: 18 | Batch_idx: 170 |  Loss_1: (0.5335) | Acc_1: (80.30%) (17575/21888)\n",
      "Epoch: 18 | Batch_idx: 180 |  Loss_1: (0.5336) | Acc_1: (80.29%) (18601/23168)\n",
      "Epoch: 18 | Batch_idx: 190 |  Loss_1: (0.5346) | Acc_1: (80.24%) (19617/24448)\n",
      "Epoch: 18 | Batch_idx: 200 |  Loss_1: (0.5342) | Acc_1: (80.22%) (20639/25728)\n",
      "Epoch: 18 | Batch_idx: 210 |  Loss_1: (0.5360) | Acc_1: (80.15%) (21647/27008)\n",
      "Epoch: 18 | Batch_idx: 220 |  Loss_1: (0.5357) | Acc_1: (80.15%) (22674/28288)\n",
      "Epoch: 18 | Batch_idx: 230 |  Loss_1: (0.5365) | Acc_1: (80.16%) (23702/29568)\n",
      "Epoch: 18 | Batch_idx: 240 |  Loss_1: (0.5367) | Acc_1: (80.15%) (24726/30848)\n",
      "Epoch: 18 | Batch_idx: 250 |  Loss_1: (0.5365) | Acc_1: (80.17%) (25757/32128)\n",
      "Epoch: 18 | Batch_idx: 260 |  Loss_1: (0.5379) | Acc_1: (80.13%) (26770/33408)\n",
      "Epoch: 18 | Batch_idx: 270 |  Loss_1: (0.5380) | Acc_1: (80.11%) (27789/34688)\n",
      "Epoch: 18 | Batch_idx: 280 |  Loss_1: (0.5396) | Acc_1: (80.07%) (28798/35968)\n",
      "Epoch: 18 | Batch_idx: 290 |  Loss_1: (0.5393) | Acc_1: (80.06%) (29820/37248)\n",
      "Epoch: 18 | Batch_idx: 300 |  Loss_1: (0.5392) | Acc_1: (80.04%) (30838/38528)\n",
      "Epoch: 18 | Batch_idx: 310 |  Loss_1: (0.5381) | Acc_1: (80.09%) (31883/39808)\n",
      "Epoch: 18 | Batch_idx: 320 |  Loss_1: (0.5382) | Acc_1: (80.08%) (32905/41088)\n",
      "Epoch: 18 | Batch_idx: 330 |  Loss_1: (0.5375) | Acc_1: (80.12%) (33947/42368)\n",
      "Epoch: 18 | Batch_idx: 340 |  Loss_1: (0.5373) | Acc_1: (80.14%) (34979/43648)\n",
      "Epoch: 18 | Batch_idx: 350 |  Loss_1: (0.5390) | Acc_1: (80.06%) (35970/44928)\n",
      "Epoch: 18 | Batch_idx: 360 |  Loss_1: (0.5396) | Acc_1: (80.04%) (36985/46208)\n",
      "Epoch: 18 | Batch_idx: 370 |  Loss_1: (0.5392) | Acc_1: (80.05%) (38015/47488)\n",
      "Epoch: 18 | Batch_idx: 380 |  Loss_1: (0.5392) | Acc_1: (80.06%) (39043/48768)\n",
      "Epoch: 18 | Batch_idx: 390 |  Loss_1: (0.5401) | Acc_1: (80.04%) (40019/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4002) | Acc: (87.08%) (8708/10000)\n",
      "Epoch: 19 | Batch_idx: 0 |  Loss_1: (0.5997) | Acc_1: (75.00%) (96/128)\n",
      "Epoch: 19 | Batch_idx: 10 |  Loss_1: (0.4932) | Acc_1: (80.68%) (1136/1408)\n",
      "Epoch: 19 | Batch_idx: 20 |  Loss_1: (0.5072) | Acc_1: (80.69%) (2169/2688)\n",
      "Epoch: 19 | Batch_idx: 30 |  Loss_1: (0.5000) | Acc_1: (81.28%) (3225/3968)\n",
      "Epoch: 19 | Batch_idx: 40 |  Loss_1: (0.5058) | Acc_1: (81.33%) (4268/5248)\n",
      "Epoch: 19 | Batch_idx: 50 |  Loss_1: (0.5023) | Acc_1: (81.42%) (5315/6528)\n",
      "Epoch: 19 | Batch_idx: 60 |  Loss_1: (0.5044) | Acc_1: (81.34%) (6351/7808)\n",
      "Epoch: 19 | Batch_idx: 70 |  Loss_1: (0.5046) | Acc_1: (81.35%) (7393/9088)\n",
      "Epoch: 19 | Batch_idx: 80 |  Loss_1: (0.4991) | Acc_1: (81.51%) (8451/10368)\n",
      "Epoch: 19 | Batch_idx: 90 |  Loss_1: (0.4891) | Acc_1: (81.89%) (9539/11648)\n",
      "Epoch: 19 | Batch_idx: 100 |  Loss_1: (0.4935) | Acc_1: (81.78%) (10573/12928)\n",
      "Epoch: 19 | Batch_idx: 110 |  Loss_1: (0.4924) | Acc_1: (81.79%) (11621/14208)\n",
      "Epoch: 19 | Batch_idx: 120 |  Loss_1: (0.4918) | Acc_1: (81.81%) (12670/15488)\n",
      "Epoch: 19 | Batch_idx: 130 |  Loss_1: (0.4932) | Acc_1: (81.71%) (13701/16768)\n",
      "Epoch: 19 | Batch_idx: 140 |  Loss_1: (0.4945) | Acc_1: (81.64%) (14735/18048)\n",
      "Epoch: 19 | Batch_idx: 150 |  Loss_1: (0.4962) | Acc_1: (81.61%) (15774/19328)\n",
      "Epoch: 19 | Batch_idx: 160 |  Loss_1: (0.4967) | Acc_1: (81.64%) (16824/20608)\n",
      "Epoch: 19 | Batch_idx: 170 |  Loss_1: (0.4990) | Acc_1: (81.52%) (17843/21888)\n",
      "Epoch: 19 | Batch_idx: 180 |  Loss_1: (0.4981) | Acc_1: (81.54%) (18891/23168)\n",
      "Epoch: 19 | Batch_idx: 190 |  Loss_1: (0.4989) | Acc_1: (81.54%) (19934/24448)\n",
      "Epoch: 19 | Batch_idx: 200 |  Loss_1: (0.5008) | Acc_1: (81.43%) (20950/25728)\n",
      "Epoch: 19 | Batch_idx: 210 |  Loss_1: (0.5012) | Acc_1: (81.39%) (21982/27008)\n",
      "Epoch: 19 | Batch_idx: 220 |  Loss_1: (0.5034) | Acc_1: (81.35%) (23012/28288)\n",
      "Epoch: 19 | Batch_idx: 230 |  Loss_1: (0.5049) | Acc_1: (81.30%) (24040/29568)\n",
      "Epoch: 19 | Batch_idx: 240 |  Loss_1: (0.5069) | Acc_1: (81.25%) (25064/30848)\n",
      "Epoch: 19 | Batch_idx: 250 |  Loss_1: (0.5077) | Acc_1: (81.24%) (26100/32128)\n",
      "Epoch: 19 | Batch_idx: 260 |  Loss_1: (0.5098) | Acc_1: (81.15%) (27112/33408)\n",
      "Epoch: 19 | Batch_idx: 270 |  Loss_1: (0.5119) | Acc_1: (81.05%) (28116/34688)\n",
      "Epoch: 19 | Batch_idx: 280 |  Loss_1: (0.5111) | Acc_1: (81.08%) (29164/35968)\n",
      "Epoch: 19 | Batch_idx: 290 |  Loss_1: (0.5123) | Acc_1: (81.05%) (30189/37248)\n",
      "Epoch: 19 | Batch_idx: 300 |  Loss_1: (0.5136) | Acc_1: (80.99%) (31203/38528)\n",
      "Epoch: 19 | Batch_idx: 310 |  Loss_1: (0.5140) | Acc_1: (80.94%) (32219/39808)\n",
      "Epoch: 19 | Batch_idx: 320 |  Loss_1: (0.5150) | Acc_1: (80.89%) (33237/41088)\n",
      "Epoch: 19 | Batch_idx: 330 |  Loss_1: (0.5153) | Acc_1: (80.88%) (34269/42368)\n",
      "Epoch: 19 | Batch_idx: 340 |  Loss_1: (0.5163) | Acc_1: (80.85%) (35290/43648)\n",
      "Epoch: 19 | Batch_idx: 350 |  Loss_1: (0.5173) | Acc_1: (80.80%) (36302/44928)\n",
      "Epoch: 19 | Batch_idx: 360 |  Loss_1: (0.5181) | Acc_1: (80.77%) (37322/46208)\n",
      "Epoch: 19 | Batch_idx: 370 |  Loss_1: (0.5183) | Acc_1: (80.74%) (38342/47488)\n",
      "Epoch: 19 | Batch_idx: 380 |  Loss_1: (0.5191) | Acc_1: (80.73%) (39370/48768)\n",
      "Epoch: 19 | Batch_idx: 390 |  Loss_1: (0.5201) | Acc_1: (80.69%) (40347/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4150) | Acc: (86.56%) (8656/10000)\n",
      "Epoch: 20 | Batch_idx: 0 |  Loss_1: (0.4954) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 20 | Batch_idx: 10 |  Loss_1: (0.5423) | Acc_1: (79.62%) (1121/1408)\n",
      "Epoch: 20 | Batch_idx: 20 |  Loss_1: (0.5329) | Acc_1: (80.47%) (2163/2688)\n",
      "Epoch: 20 | Batch_idx: 30 |  Loss_1: (0.5278) | Acc_1: (80.37%) (3189/3968)\n",
      "Epoch: 20 | Batch_idx: 40 |  Loss_1: (0.5274) | Acc_1: (80.45%) (4222/5248)\n",
      "Epoch: 20 | Batch_idx: 50 |  Loss_1: (0.5122) | Acc_1: (80.96%) (5285/6528)\n",
      "Epoch: 20 | Batch_idx: 60 |  Loss_1: (0.5098) | Acc_1: (81.16%) (6337/7808)\n",
      "Epoch: 20 | Batch_idx: 70 |  Loss_1: (0.5089) | Acc_1: (81.16%) (7376/9088)\n",
      "Epoch: 20 | Batch_idx: 80 |  Loss_1: (0.5088) | Acc_1: (81.12%) (8411/10368)\n",
      "Epoch: 20 | Batch_idx: 90 |  Loss_1: (0.5095) | Acc_1: (81.05%) (9441/11648)\n",
      "Epoch: 20 | Batch_idx: 100 |  Loss_1: (0.5158) | Acc_1: (80.88%) (10456/12928)\n",
      "Epoch: 20 | Batch_idx: 110 |  Loss_1: (0.5114) | Acc_1: (81.11%) (11524/14208)\n",
      "Epoch: 20 | Batch_idx: 120 |  Loss_1: (0.5149) | Acc_1: (81.01%) (12547/15488)\n",
      "Epoch: 20 | Batch_idx: 130 |  Loss_1: (0.5132) | Acc_1: (81.06%) (13592/16768)\n",
      "Epoch: 20 | Batch_idx: 140 |  Loss_1: (0.5117) | Acc_1: (81.08%) (14634/18048)\n",
      "Epoch: 20 | Batch_idx: 150 |  Loss_1: (0.5146) | Acc_1: (81.02%) (15660/19328)\n",
      "Epoch: 20 | Batch_idx: 160 |  Loss_1: (0.5157) | Acc_1: (80.95%) (16682/20608)\n",
      "Epoch: 20 | Batch_idx: 170 |  Loss_1: (0.5148) | Acc_1: (81.00%) (17729/21888)\n",
      "Epoch: 20 | Batch_idx: 180 |  Loss_1: (0.5146) | Acc_1: (81.06%) (18779/23168)\n",
      "Epoch: 20 | Batch_idx: 190 |  Loss_1: (0.5169) | Acc_1: (81.01%) (19805/24448)\n",
      "Epoch: 20 | Batch_idx: 200 |  Loss_1: (0.5170) | Acc_1: (80.99%) (20837/25728)\n",
      "Epoch: 20 | Batch_idx: 210 |  Loss_1: (0.5162) | Acc_1: (80.99%) (21875/27008)\n",
      "Epoch: 20 | Batch_idx: 220 |  Loss_1: (0.5172) | Acc_1: (80.95%) (22898/28288)\n",
      "Epoch: 20 | Batch_idx: 230 |  Loss_1: (0.5165) | Acc_1: (80.97%) (23940/29568)\n",
      "Epoch: 20 | Batch_idx: 240 |  Loss_1: (0.5179) | Acc_1: (80.90%) (24956/30848)\n",
      "Epoch: 20 | Batch_idx: 250 |  Loss_1: (0.5193) | Acc_1: (80.84%) (25972/32128)\n",
      "Epoch: 20 | Batch_idx: 260 |  Loss_1: (0.5177) | Acc_1: (80.91%) (27031/33408)\n",
      "Epoch: 20 | Batch_idx: 270 |  Loss_1: (0.5175) | Acc_1: (80.95%) (28081/34688)\n",
      "Epoch: 20 | Batch_idx: 280 |  Loss_1: (0.5186) | Acc_1: (80.92%) (29104/35968)\n",
      "Epoch: 20 | Batch_idx: 290 |  Loss_1: (0.5197) | Acc_1: (80.91%) (30138/37248)\n",
      "Epoch: 20 | Batch_idx: 300 |  Loss_1: (0.5200) | Acc_1: (80.92%) (31176/38528)\n",
      "Epoch: 20 | Batch_idx: 310 |  Loss_1: (0.5198) | Acc_1: (80.91%) (32208/39808)\n",
      "Epoch: 20 | Batch_idx: 320 |  Loss_1: (0.5188) | Acc_1: (80.94%) (33255/41088)\n",
      "Epoch: 20 | Batch_idx: 330 |  Loss_1: (0.5200) | Acc_1: (80.90%) (34276/42368)\n",
      "Epoch: 20 | Batch_idx: 340 |  Loss_1: (0.5196) | Acc_1: (80.91%) (35317/43648)\n",
      "Epoch: 20 | Batch_idx: 350 |  Loss_1: (0.5195) | Acc_1: (80.92%) (36354/44928)\n",
      "Epoch: 20 | Batch_idx: 360 |  Loss_1: (0.5201) | Acc_1: (80.87%) (37367/46208)\n",
      "Epoch: 20 | Batch_idx: 370 |  Loss_1: (0.5193) | Acc_1: (80.90%) (38419/47488)\n",
      "Epoch: 20 | Batch_idx: 380 |  Loss_1: (0.5200) | Acc_1: (80.86%) (39436/48768)\n",
      "Epoch: 20 | Batch_idx: 390 |  Loss_1: (0.5188) | Acc_1: (80.91%) (40457/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3879) | Acc: (87.22%) (8722/10000)\n",
      "Epoch: 21 | Batch_idx: 0 |  Loss_1: (0.4184) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 21 | Batch_idx: 10 |  Loss_1: (0.4709) | Acc_1: (82.46%) (1161/1408)\n",
      "Epoch: 21 | Batch_idx: 20 |  Loss_1: (0.5017) | Acc_1: (81.58%) (2193/2688)\n",
      "Epoch: 21 | Batch_idx: 30 |  Loss_1: (0.5016) | Acc_1: (81.50%) (3234/3968)\n",
      "Epoch: 21 | Batch_idx: 40 |  Loss_1: (0.4956) | Acc_1: (81.82%) (4294/5248)\n",
      "Epoch: 21 | Batch_idx: 50 |  Loss_1: (0.4972) | Acc_1: (81.72%) (5335/6528)\n",
      "Epoch: 21 | Batch_idx: 60 |  Loss_1: (0.4981) | Acc_1: (81.52%) (6365/7808)\n",
      "Epoch: 21 | Batch_idx: 70 |  Loss_1: (0.4989) | Acc_1: (81.47%) (7404/9088)\n",
      "Epoch: 21 | Batch_idx: 80 |  Loss_1: (0.5017) | Acc_1: (81.41%) (8441/10368)\n",
      "Epoch: 21 | Batch_idx: 90 |  Loss_1: (0.5027) | Acc_1: (81.40%) (9481/11648)\n",
      "Epoch: 21 | Batch_idx: 100 |  Loss_1: (0.5040) | Acc_1: (81.35%) (10517/12928)\n",
      "Epoch: 21 | Batch_idx: 110 |  Loss_1: (0.5063) | Acc_1: (81.28%) (11548/14208)\n",
      "Epoch: 21 | Batch_idx: 120 |  Loss_1: (0.5076) | Acc_1: (81.20%) (12576/15488)\n",
      "Epoch: 21 | Batch_idx: 130 |  Loss_1: (0.5057) | Acc_1: (81.35%) (13640/16768)\n",
      "Epoch: 21 | Batch_idx: 140 |  Loss_1: (0.5008) | Acc_1: (81.49%) (14707/18048)\n",
      "Epoch: 21 | Batch_idx: 150 |  Loss_1: (0.5010) | Acc_1: (81.43%) (15738/19328)\n",
      "Epoch: 21 | Batch_idx: 160 |  Loss_1: (0.5000) | Acc_1: (81.47%) (16790/20608)\n",
      "Epoch: 21 | Batch_idx: 170 |  Loss_1: (0.5009) | Acc_1: (81.46%) (17829/21888)\n",
      "Epoch: 21 | Batch_idx: 180 |  Loss_1: (0.5032) | Acc_1: (81.35%) (18848/23168)\n",
      "Epoch: 21 | Batch_idx: 190 |  Loss_1: (0.5022) | Acc_1: (81.37%) (19894/24448)\n",
      "Epoch: 21 | Batch_idx: 200 |  Loss_1: (0.5039) | Acc_1: (81.34%) (20927/25728)\n",
      "Epoch: 21 | Batch_idx: 210 |  Loss_1: (0.5062) | Acc_1: (81.22%) (21937/27008)\n",
      "Epoch: 21 | Batch_idx: 220 |  Loss_1: (0.5062) | Acc_1: (81.22%) (22975/28288)\n",
      "Epoch: 21 | Batch_idx: 230 |  Loss_1: (0.5065) | Acc_1: (81.20%) (24009/29568)\n",
      "Epoch: 21 | Batch_idx: 240 |  Loss_1: (0.5064) | Acc_1: (81.19%) (25046/30848)\n",
      "Epoch: 21 | Batch_idx: 250 |  Loss_1: (0.5069) | Acc_1: (81.16%) (26075/32128)\n",
      "Epoch: 21 | Batch_idx: 260 |  Loss_1: (0.5068) | Acc_1: (81.19%) (27125/33408)\n",
      "Epoch: 21 | Batch_idx: 270 |  Loss_1: (0.5095) | Acc_1: (81.14%) (28146/34688)\n",
      "Epoch: 21 | Batch_idx: 280 |  Loss_1: (0.5084) | Acc_1: (81.20%) (29207/35968)\n",
      "Epoch: 21 | Batch_idx: 290 |  Loss_1: (0.5077) | Acc_1: (81.24%) (30259/37248)\n",
      "Epoch: 21 | Batch_idx: 300 |  Loss_1: (0.5090) | Acc_1: (81.18%) (31277/38528)\n",
      "Epoch: 21 | Batch_idx: 310 |  Loss_1: (0.5081) | Acc_1: (81.21%) (32330/39808)\n",
      "Epoch: 21 | Batch_idx: 320 |  Loss_1: (0.5068) | Acc_1: (81.28%) (33395/41088)\n",
      "Epoch: 21 | Batch_idx: 330 |  Loss_1: (0.5054) | Acc_1: (81.33%) (34459/42368)\n",
      "Epoch: 21 | Batch_idx: 340 |  Loss_1: (0.5065) | Acc_1: (81.30%) (35487/43648)\n",
      "Epoch: 21 | Batch_idx: 350 |  Loss_1: (0.5067) | Acc_1: (81.31%) (36532/44928)\n",
      "Epoch: 21 | Batch_idx: 360 |  Loss_1: (0.5075) | Acc_1: (81.29%) (37562/46208)\n",
      "Epoch: 21 | Batch_idx: 370 |  Loss_1: (0.5087) | Acc_1: (81.25%) (38583/47488)\n",
      "Epoch: 21 | Batch_idx: 380 |  Loss_1: (0.5084) | Acc_1: (81.27%) (39636/48768)\n",
      "Epoch: 21 | Batch_idx: 390 |  Loss_1: (0.5073) | Acc_1: (81.32%) (40658/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4104) | Acc: (86.48%) (8648/10000)\n",
      "Epoch: 22 | Batch_idx: 0 |  Loss_1: (0.4439) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 22 | Batch_idx: 10 |  Loss_1: (0.5288) | Acc_1: (80.89%) (1139/1408)\n",
      "Epoch: 22 | Batch_idx: 20 |  Loss_1: (0.5067) | Acc_1: (81.36%) (2187/2688)\n",
      "Epoch: 22 | Batch_idx: 30 |  Loss_1: (0.5164) | Acc_1: (81.02%) (3215/3968)\n",
      "Epoch: 22 | Batch_idx: 40 |  Loss_1: (0.5220) | Acc_1: (80.64%) (4232/5248)\n",
      "Epoch: 22 | Batch_idx: 50 |  Loss_1: (0.5116) | Acc_1: (81.02%) (5289/6528)\n",
      "Epoch: 22 | Batch_idx: 60 |  Loss_1: (0.5094) | Acc_1: (81.06%) (6329/7808)\n",
      "Epoch: 22 | Batch_idx: 70 |  Loss_1: (0.5105) | Acc_1: (81.01%) (7362/9088)\n",
      "Epoch: 22 | Batch_idx: 80 |  Loss_1: (0.5046) | Acc_1: (81.30%) (8429/10368)\n",
      "Epoch: 22 | Batch_idx: 90 |  Loss_1: (0.5080) | Acc_1: (81.22%) (9460/11648)\n",
      "Epoch: 22 | Batch_idx: 100 |  Loss_1: (0.5128) | Acc_1: (81.10%) (10484/12928)\n",
      "Epoch: 22 | Batch_idx: 110 |  Loss_1: (0.5147) | Acc_1: (80.98%) (11506/14208)\n",
      "Epoch: 22 | Batch_idx: 120 |  Loss_1: (0.5127) | Acc_1: (81.05%) (12553/15488)\n",
      "Epoch: 22 | Batch_idx: 130 |  Loss_1: (0.5127) | Acc_1: (81.05%) (13590/16768)\n",
      "Epoch: 22 | Batch_idx: 140 |  Loss_1: (0.5118) | Acc_1: (81.09%) (14635/18048)\n",
      "Epoch: 22 | Batch_idx: 150 |  Loss_1: (0.5100) | Acc_1: (81.15%) (15684/19328)\n",
      "Epoch: 22 | Batch_idx: 160 |  Loss_1: (0.5102) | Acc_1: (81.16%) (16726/20608)\n",
      "Epoch: 22 | Batch_idx: 170 |  Loss_1: (0.5073) | Acc_1: (81.25%) (17784/21888)\n",
      "Epoch: 22 | Batch_idx: 180 |  Loss_1: (0.5064) | Acc_1: (81.29%) (18834/23168)\n",
      "Epoch: 22 | Batch_idx: 190 |  Loss_1: (0.5068) | Acc_1: (81.31%) (19878/24448)\n",
      "Epoch: 22 | Batch_idx: 200 |  Loss_1: (0.5067) | Acc_1: (81.32%) (20923/25728)\n",
      "Epoch: 22 | Batch_idx: 210 |  Loss_1: (0.5063) | Acc_1: (81.33%) (21965/27008)\n",
      "Epoch: 22 | Batch_idx: 220 |  Loss_1: (0.5034) | Acc_1: (81.47%) (23047/28288)\n",
      "Epoch: 22 | Batch_idx: 230 |  Loss_1: (0.5023) | Acc_1: (81.52%) (24105/29568)\n",
      "Epoch: 22 | Batch_idx: 240 |  Loss_1: (0.5006) | Acc_1: (81.58%) (25166/30848)\n",
      "Epoch: 22 | Batch_idx: 250 |  Loss_1: (0.5028) | Acc_1: (81.51%) (26189/32128)\n",
      "Epoch: 22 | Batch_idx: 260 |  Loss_1: (0.5024) | Acc_1: (81.52%) (27234/33408)\n",
      "Epoch: 22 | Batch_idx: 270 |  Loss_1: (0.5028) | Acc_1: (81.49%) (28268/34688)\n",
      "Epoch: 22 | Batch_idx: 280 |  Loss_1: (0.5031) | Acc_1: (81.50%) (29313/35968)\n",
      "Epoch: 22 | Batch_idx: 290 |  Loss_1: (0.5024) | Acc_1: (81.51%) (30360/37248)\n",
      "Epoch: 22 | Batch_idx: 300 |  Loss_1: (0.5021) | Acc_1: (81.51%) (31406/38528)\n",
      "Epoch: 22 | Batch_idx: 310 |  Loss_1: (0.5026) | Acc_1: (81.50%) (32444/39808)\n",
      "Epoch: 22 | Batch_idx: 320 |  Loss_1: (0.5033) | Acc_1: (81.47%) (33473/41088)\n",
      "Epoch: 22 | Batch_idx: 330 |  Loss_1: (0.5046) | Acc_1: (81.43%) (34499/42368)\n",
      "Epoch: 22 | Batch_idx: 340 |  Loss_1: (0.5033) | Acc_1: (81.47%) (35562/43648)\n",
      "Epoch: 22 | Batch_idx: 350 |  Loss_1: (0.5025) | Acc_1: (81.52%) (36625/44928)\n",
      "Epoch: 22 | Batch_idx: 360 |  Loss_1: (0.5020) | Acc_1: (81.53%) (37672/46208)\n",
      "Epoch: 22 | Batch_idx: 370 |  Loss_1: (0.5016) | Acc_1: (81.55%) (38726/47488)\n",
      "Epoch: 22 | Batch_idx: 380 |  Loss_1: (0.5039) | Acc_1: (81.47%) (39729/48768)\n",
      "Epoch: 22 | Batch_idx: 390 |  Loss_1: (0.5041) | Acc_1: (81.46%) (40730/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4444) | Acc: (85.89%) (8589/10000)\n",
      "Epoch: 23 | Batch_idx: 0 |  Loss_1: (0.4063) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 23 | Batch_idx: 10 |  Loss_1: (0.4775) | Acc_1: (83.17%) (1171/1408)\n",
      "Epoch: 23 | Batch_idx: 20 |  Loss_1: (0.4822) | Acc_1: (82.70%) (2223/2688)\n",
      "Epoch: 23 | Batch_idx: 30 |  Loss_1: (0.4900) | Acc_1: (82.11%) (3258/3968)\n",
      "Epoch: 23 | Batch_idx: 40 |  Loss_1: (0.4790) | Acc_1: (82.47%) (4328/5248)\n",
      "Epoch: 23 | Batch_idx: 50 |  Loss_1: (0.4837) | Acc_1: (82.18%) (5365/6528)\n",
      "Epoch: 23 | Batch_idx: 60 |  Loss_1: (0.4834) | Acc_1: (82.34%) (6429/7808)\n",
      "Epoch: 23 | Batch_idx: 70 |  Loss_1: (0.4826) | Acc_1: (82.35%) (7484/9088)\n",
      "Epoch: 23 | Batch_idx: 80 |  Loss_1: (0.4895) | Acc_1: (82.10%) (8512/10368)\n",
      "Epoch: 23 | Batch_idx: 90 |  Loss_1: (0.4926) | Acc_1: (82.04%) (9556/11648)\n",
      "Epoch: 23 | Batch_idx: 100 |  Loss_1: (0.4907) | Acc_1: (82.09%) (10613/12928)\n",
      "Epoch: 23 | Batch_idx: 110 |  Loss_1: (0.4926) | Acc_1: (81.95%) (11643/14208)\n",
      "Epoch: 23 | Batch_idx: 120 |  Loss_1: (0.4926) | Acc_1: (81.95%) (12693/15488)\n",
      "Epoch: 23 | Batch_idx: 130 |  Loss_1: (0.4909) | Acc_1: (82.04%) (13756/16768)\n",
      "Epoch: 23 | Batch_idx: 140 |  Loss_1: (0.4944) | Acc_1: (81.96%) (14793/18048)\n",
      "Epoch: 23 | Batch_idx: 150 |  Loss_1: (0.4966) | Acc_1: (81.89%) (15827/19328)\n",
      "Epoch: 23 | Batch_idx: 160 |  Loss_1: (0.4973) | Acc_1: (81.86%) (16869/20608)\n",
      "Epoch: 23 | Batch_idx: 170 |  Loss_1: (0.4962) | Acc_1: (81.88%) (17921/21888)\n",
      "Epoch: 23 | Batch_idx: 180 |  Loss_1: (0.4978) | Acc_1: (81.81%) (18954/23168)\n",
      "Epoch: 23 | Batch_idx: 190 |  Loss_1: (0.4957) | Acc_1: (81.84%) (20008/24448)\n",
      "Epoch: 23 | Batch_idx: 200 |  Loss_1: (0.4942) | Acc_1: (81.87%) (21063/25728)\n",
      "Epoch: 23 | Batch_idx: 210 |  Loss_1: (0.4912) | Acc_1: (81.95%) (22134/27008)\n",
      "Epoch: 23 | Batch_idx: 220 |  Loss_1: (0.4912) | Acc_1: (81.99%) (23192/28288)\n",
      "Epoch: 23 | Batch_idx: 230 |  Loss_1: (0.4923) | Acc_1: (81.94%) (24228/29568)\n",
      "Epoch: 23 | Batch_idx: 240 |  Loss_1: (0.4922) | Acc_1: (81.94%) (25277/30848)\n",
      "Epoch: 23 | Batch_idx: 250 |  Loss_1: (0.4929) | Acc_1: (81.93%) (26321/32128)\n",
      "Epoch: 23 | Batch_idx: 260 |  Loss_1: (0.4931) | Acc_1: (81.90%) (27362/33408)\n",
      "Epoch: 23 | Batch_idx: 270 |  Loss_1: (0.4933) | Acc_1: (81.90%) (28411/34688)\n",
      "Epoch: 23 | Batch_idx: 280 |  Loss_1: (0.4946) | Acc_1: (81.86%) (29445/35968)\n",
      "Epoch: 23 | Batch_idx: 290 |  Loss_1: (0.4947) | Acc_1: (81.87%) (30494/37248)\n",
      "Epoch: 23 | Batch_idx: 300 |  Loss_1: (0.4916) | Acc_1: (81.99%) (31588/38528)\n",
      "Epoch: 23 | Batch_idx: 310 |  Loss_1: (0.4928) | Acc_1: (81.95%) (32622/39808)\n",
      "Epoch: 23 | Batch_idx: 320 |  Loss_1: (0.4917) | Acc_1: (81.99%) (33687/41088)\n",
      "Epoch: 23 | Batch_idx: 330 |  Loss_1: (0.4907) | Acc_1: (82.02%) (34749/42368)\n",
      "Epoch: 23 | Batch_idx: 340 |  Loss_1: (0.4920) | Acc_1: (81.98%) (35782/43648)\n",
      "Epoch: 23 | Batch_idx: 350 |  Loss_1: (0.4916) | Acc_1: (82.00%) (36840/44928)\n",
      "Epoch: 23 | Batch_idx: 360 |  Loss_1: (0.4929) | Acc_1: (81.95%) (37866/46208)\n",
      "Epoch: 23 | Batch_idx: 370 |  Loss_1: (0.4927) | Acc_1: (81.95%) (38916/47488)\n",
      "Epoch: 23 | Batch_idx: 380 |  Loss_1: (0.4932) | Acc_1: (81.94%) (39960/48768)\n",
      "Epoch: 23 | Batch_idx: 390 |  Loss_1: (0.4940) | Acc_1: (81.93%) (40963/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4235) | Acc: (86.23%) (8623/10000)\n",
      "Epoch: 24 | Batch_idx: 0 |  Loss_1: (0.3667) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 24 | Batch_idx: 10 |  Loss_1: (0.4635) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 24 | Batch_idx: 20 |  Loss_1: (0.4621) | Acc_1: (83.15%) (2235/2688)\n",
      "Epoch: 24 | Batch_idx: 30 |  Loss_1: (0.4720) | Acc_1: (82.71%) (3282/3968)\n",
      "Epoch: 24 | Batch_idx: 40 |  Loss_1: (0.4816) | Acc_1: (82.34%) (4321/5248)\n",
      "Epoch: 24 | Batch_idx: 50 |  Loss_1: (0.4827) | Acc_1: (82.44%) (5382/6528)\n",
      "Epoch: 24 | Batch_idx: 60 |  Loss_1: (0.4840) | Acc_1: (82.40%) (6434/7808)\n",
      "Epoch: 24 | Batch_idx: 70 |  Loss_1: (0.4783) | Acc_1: (82.59%) (7506/9088)\n",
      "Epoch: 24 | Batch_idx: 80 |  Loss_1: (0.4770) | Acc_1: (82.58%) (8562/10368)\n",
      "Epoch: 24 | Batch_idx: 90 |  Loss_1: (0.4787) | Acc_1: (82.55%) (9615/11648)\n",
      "Epoch: 24 | Batch_idx: 100 |  Loss_1: (0.4789) | Acc_1: (82.54%) (10671/12928)\n",
      "Epoch: 24 | Batch_idx: 110 |  Loss_1: (0.4820) | Acc_1: (82.47%) (11718/14208)\n",
      "Epoch: 24 | Batch_idx: 120 |  Loss_1: (0.4813) | Acc_1: (82.47%) (12773/15488)\n",
      "Epoch: 24 | Batch_idx: 130 |  Loss_1: (0.4825) | Acc_1: (82.42%) (13820/16768)\n",
      "Epoch: 24 | Batch_idx: 140 |  Loss_1: (0.4835) | Acc_1: (82.36%) (14864/18048)\n",
      "Epoch: 24 | Batch_idx: 150 |  Loss_1: (0.4824) | Acc_1: (82.39%) (15925/19328)\n",
      "Epoch: 24 | Batch_idx: 160 |  Loss_1: (0.4846) | Acc_1: (82.34%) (16968/20608)\n",
      "Epoch: 24 | Batch_idx: 170 |  Loss_1: (0.4830) | Acc_1: (82.36%) (18027/21888)\n",
      "Epoch: 24 | Batch_idx: 180 |  Loss_1: (0.4835) | Acc_1: (82.35%) (19080/23168)\n",
      "Epoch: 24 | Batch_idx: 190 |  Loss_1: (0.4838) | Acc_1: (82.35%) (20133/24448)\n",
      "Epoch: 24 | Batch_idx: 200 |  Loss_1: (0.4838) | Acc_1: (82.31%) (21178/25728)\n",
      "Epoch: 24 | Batch_idx: 210 |  Loss_1: (0.4832) | Acc_1: (82.29%) (22225/27008)\n",
      "Epoch: 24 | Batch_idx: 220 |  Loss_1: (0.4834) | Acc_1: (82.31%) (23283/28288)\n",
      "Epoch: 24 | Batch_idx: 230 |  Loss_1: (0.4821) | Acc_1: (82.33%) (24343/29568)\n",
      "Epoch: 24 | Batch_idx: 240 |  Loss_1: (0.4829) | Acc_1: (82.28%) (25382/30848)\n",
      "Epoch: 24 | Batch_idx: 250 |  Loss_1: (0.4851) | Acc_1: (82.21%) (26411/32128)\n",
      "Epoch: 24 | Batch_idx: 260 |  Loss_1: (0.4878) | Acc_1: (82.13%) (27437/33408)\n",
      "Epoch: 24 | Batch_idx: 270 |  Loss_1: (0.4861) | Acc_1: (82.21%) (28516/34688)\n",
      "Epoch: 24 | Batch_idx: 280 |  Loss_1: (0.4869) | Acc_1: (82.14%) (29545/35968)\n",
      "Epoch: 24 | Batch_idx: 290 |  Loss_1: (0.4872) | Acc_1: (82.14%) (30595/37248)\n",
      "Epoch: 24 | Batch_idx: 300 |  Loss_1: (0.4868) | Acc_1: (82.13%) (31644/38528)\n",
      "Epoch: 24 | Batch_idx: 310 |  Loss_1: (0.4864) | Acc_1: (82.13%) (32693/39808)\n",
      "Epoch: 24 | Batch_idx: 320 |  Loss_1: (0.4855) | Acc_1: (82.17%) (33764/41088)\n",
      "Epoch: 24 | Batch_idx: 330 |  Loss_1: (0.4863) | Acc_1: (82.11%) (34790/42368)\n",
      "Epoch: 24 | Batch_idx: 340 |  Loss_1: (0.4862) | Acc_1: (82.13%) (35849/43648)\n",
      "Epoch: 24 | Batch_idx: 350 |  Loss_1: (0.4864) | Acc_1: (82.15%) (36907/44928)\n",
      "Epoch: 24 | Batch_idx: 360 |  Loss_1: (0.4856) | Acc_1: (82.18%) (37972/46208)\n",
      "Epoch: 24 | Batch_idx: 370 |  Loss_1: (0.4863) | Acc_1: (82.16%) (39014/47488)\n",
      "Epoch: 24 | Batch_idx: 380 |  Loss_1: (0.4873) | Acc_1: (82.10%) (40040/48768)\n",
      "Epoch: 24 | Batch_idx: 390 |  Loss_1: (0.4869) | Acc_1: (82.15%) (41074/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4579) | Acc: (86.22%) (8622/10000)\n",
      "Epoch: 25 | Batch_idx: 0 |  Loss_1: (0.4839) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 25 | Batch_idx: 10 |  Loss_1: (0.4918) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 25 | Batch_idx: 20 |  Loss_1: (0.4911) | Acc_1: (81.70%) (2196/2688)\n",
      "Epoch: 25 | Batch_idx: 30 |  Loss_1: (0.4826) | Acc_1: (82.01%) (3254/3968)\n",
      "Epoch: 25 | Batch_idx: 40 |  Loss_1: (0.4847) | Acc_1: (81.78%) (4292/5248)\n",
      "Epoch: 25 | Batch_idx: 50 |  Loss_1: (0.4890) | Acc_1: (81.74%) (5336/6528)\n",
      "Epoch: 25 | Batch_idx: 60 |  Loss_1: (0.4835) | Acc_1: (81.99%) (6402/7808)\n",
      "Epoch: 25 | Batch_idx: 70 |  Loss_1: (0.4826) | Acc_1: (82.10%) (7461/9088)\n",
      "Epoch: 25 | Batch_idx: 80 |  Loss_1: (0.4790) | Acc_1: (82.20%) (8522/10368)\n",
      "Epoch: 25 | Batch_idx: 90 |  Loss_1: (0.4812) | Acc_1: (82.13%) (9567/11648)\n",
      "Epoch: 25 | Batch_idx: 100 |  Loss_1: (0.4804) | Acc_1: (82.12%) (10617/12928)\n",
      "Epoch: 25 | Batch_idx: 110 |  Loss_1: (0.4832) | Acc_1: (82.03%) (11655/14208)\n",
      "Epoch: 25 | Batch_idx: 120 |  Loss_1: (0.4884) | Acc_1: (81.82%) (12672/15488)\n",
      "Epoch: 25 | Batch_idx: 130 |  Loss_1: (0.4914) | Acc_1: (81.72%) (13702/16768)\n",
      "Epoch: 25 | Batch_idx: 140 |  Loss_1: (0.4869) | Acc_1: (81.89%) (14780/18048)\n",
      "Epoch: 25 | Batch_idx: 150 |  Loss_1: (0.4875) | Acc_1: (81.86%) (15822/19328)\n",
      "Epoch: 25 | Batch_idx: 160 |  Loss_1: (0.4838) | Acc_1: (81.97%) (16892/20608)\n",
      "Epoch: 25 | Batch_idx: 170 |  Loss_1: (0.4855) | Acc_1: (81.91%) (17928/21888)\n",
      "Epoch: 25 | Batch_idx: 180 |  Loss_1: (0.4852) | Acc_1: (81.94%) (18983/23168)\n",
      "Epoch: 25 | Batch_idx: 190 |  Loss_1: (0.4875) | Acc_1: (81.84%) (20008/24448)\n",
      "Epoch: 25 | Batch_idx: 200 |  Loss_1: (0.4870) | Acc_1: (81.86%) (21062/25728)\n",
      "Epoch: 25 | Batch_idx: 210 |  Loss_1: (0.4882) | Acc_1: (81.83%) (22100/27008)\n",
      "Epoch: 25 | Batch_idx: 220 |  Loss_1: (0.4890) | Acc_1: (81.79%) (23138/28288)\n",
      "Epoch: 25 | Batch_idx: 230 |  Loss_1: (0.4891) | Acc_1: (81.81%) (24190/29568)\n",
      "Epoch: 25 | Batch_idx: 240 |  Loss_1: (0.4888) | Acc_1: (81.80%) (25235/30848)\n",
      "Epoch: 25 | Batch_idx: 250 |  Loss_1: (0.4866) | Acc_1: (81.91%) (26316/32128)\n",
      "Epoch: 25 | Batch_idx: 260 |  Loss_1: (0.4858) | Acc_1: (81.96%) (27380/33408)\n",
      "Epoch: 25 | Batch_idx: 270 |  Loss_1: (0.4864) | Acc_1: (81.93%) (28419/34688)\n",
      "Epoch: 25 | Batch_idx: 280 |  Loss_1: (0.4869) | Acc_1: (81.88%) (29449/35968)\n",
      "Epoch: 25 | Batch_idx: 290 |  Loss_1: (0.4867) | Acc_1: (81.91%) (30509/37248)\n",
      "Epoch: 25 | Batch_idx: 300 |  Loss_1: (0.4877) | Acc_1: (81.89%) (31550/38528)\n",
      "Epoch: 25 | Batch_idx: 310 |  Loss_1: (0.4867) | Acc_1: (81.92%) (32609/39808)\n",
      "Epoch: 25 | Batch_idx: 320 |  Loss_1: (0.4856) | Acc_1: (81.94%) (33666/41088)\n",
      "Epoch: 25 | Batch_idx: 330 |  Loss_1: (0.4850) | Acc_1: (81.97%) (34729/42368)\n",
      "Epoch: 25 | Batch_idx: 340 |  Loss_1: (0.4855) | Acc_1: (81.96%) (35775/43648)\n",
      "Epoch: 25 | Batch_idx: 350 |  Loss_1: (0.4844) | Acc_1: (82.00%) (36842/44928)\n",
      "Epoch: 25 | Batch_idx: 360 |  Loss_1: (0.4843) | Acc_1: (82.02%) (37899/46208)\n",
      "Epoch: 25 | Batch_idx: 370 |  Loss_1: (0.4844) | Acc_1: (82.01%) (38944/47488)\n",
      "Epoch: 25 | Batch_idx: 380 |  Loss_1: (0.4844) | Acc_1: (81.99%) (39987/48768)\n",
      "Epoch: 25 | Batch_idx: 390 |  Loss_1: (0.4848) | Acc_1: (81.97%) (40984/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4355) | Acc: (86.76%) (8676/10000)\n",
      "Epoch: 26 | Batch_idx: 0 |  Loss_1: (0.7114) | Acc_1: (75.00%) (96/128)\n",
      "Epoch: 26 | Batch_idx: 10 |  Loss_1: (0.4775) | Acc_1: (82.74%) (1165/1408)\n",
      "Epoch: 26 | Batch_idx: 20 |  Loss_1: (0.4539) | Acc_1: (83.52%) (2245/2688)\n",
      "Epoch: 26 | Batch_idx: 30 |  Loss_1: (0.4608) | Acc_1: (82.91%) (3290/3968)\n",
      "Epoch: 26 | Batch_idx: 40 |  Loss_1: (0.4578) | Acc_1: (82.96%) (4354/5248)\n",
      "Epoch: 26 | Batch_idx: 50 |  Loss_1: (0.4538) | Acc_1: (83.27%) (5436/6528)\n",
      "Epoch: 26 | Batch_idx: 60 |  Loss_1: (0.4560) | Acc_1: (83.17%) (6494/7808)\n",
      "Epoch: 26 | Batch_idx: 70 |  Loss_1: (0.4579) | Acc_1: (83.11%) (7553/9088)\n",
      "Epoch: 26 | Batch_idx: 80 |  Loss_1: (0.4567) | Acc_1: (83.20%) (8626/10368)\n",
      "Epoch: 26 | Batch_idx: 90 |  Loss_1: (0.4581) | Acc_1: (83.10%) (9680/11648)\n",
      "Epoch: 26 | Batch_idx: 100 |  Loss_1: (0.4577) | Acc_1: (83.12%) (10746/12928)\n",
      "Epoch: 26 | Batch_idx: 110 |  Loss_1: (0.4575) | Acc_1: (83.16%) (11816/14208)\n",
      "Epoch: 26 | Batch_idx: 120 |  Loss_1: (0.4572) | Acc_1: (83.20%) (12886/15488)\n",
      "Epoch: 26 | Batch_idx: 130 |  Loss_1: (0.4575) | Acc_1: (83.19%) (13950/16768)\n",
      "Epoch: 26 | Batch_idx: 140 |  Loss_1: (0.4590) | Acc_1: (83.15%) (15007/18048)\n",
      "Epoch: 26 | Batch_idx: 150 |  Loss_1: (0.4593) | Acc_1: (83.15%) (16071/19328)\n",
      "Epoch: 26 | Batch_idx: 160 |  Loss_1: (0.4578) | Acc_1: (83.24%) (17154/20608)\n",
      "Epoch: 26 | Batch_idx: 170 |  Loss_1: (0.4614) | Acc_1: (83.05%) (18178/21888)\n",
      "Epoch: 26 | Batch_idx: 180 |  Loss_1: (0.4647) | Acc_1: (82.92%) (19210/23168)\n",
      "Epoch: 26 | Batch_idx: 190 |  Loss_1: (0.4664) | Acc_1: (82.88%) (20263/24448)\n",
      "Epoch: 26 | Batch_idx: 200 |  Loss_1: (0.4700) | Acc_1: (82.72%) (21283/25728)\n",
      "Epoch: 26 | Batch_idx: 210 |  Loss_1: (0.4705) | Acc_1: (82.71%) (22338/27008)\n",
      "Epoch: 26 | Batch_idx: 220 |  Loss_1: (0.4709) | Acc_1: (82.71%) (23398/28288)\n",
      "Epoch: 26 | Batch_idx: 230 |  Loss_1: (0.4718) | Acc_1: (82.65%) (24439/29568)\n",
      "Epoch: 26 | Batch_idx: 240 |  Loss_1: (0.4714) | Acc_1: (82.65%) (25496/30848)\n",
      "Epoch: 26 | Batch_idx: 250 |  Loss_1: (0.4720) | Acc_1: (82.67%) (26559/32128)\n",
      "Epoch: 26 | Batch_idx: 260 |  Loss_1: (0.4727) | Acc_1: (82.62%) (27601/33408)\n",
      "Epoch: 26 | Batch_idx: 270 |  Loss_1: (0.4714) | Acc_1: (82.68%) (28679/34688)\n",
      "Epoch: 26 | Batch_idx: 280 |  Loss_1: (0.4719) | Acc_1: (82.63%) (29721/35968)\n",
      "Epoch: 26 | Batch_idx: 290 |  Loss_1: (0.4728) | Acc_1: (82.60%) (30768/37248)\n",
      "Epoch: 26 | Batch_idx: 300 |  Loss_1: (0.4733) | Acc_1: (82.59%) (31819/38528)\n",
      "Epoch: 26 | Batch_idx: 310 |  Loss_1: (0.4740) | Acc_1: (82.54%) (32858/39808)\n",
      "Epoch: 26 | Batch_idx: 320 |  Loss_1: (0.4758) | Acc_1: (82.47%) (33886/41088)\n",
      "Epoch: 26 | Batch_idx: 330 |  Loss_1: (0.4755) | Acc_1: (82.47%) (34942/42368)\n",
      "Epoch: 26 | Batch_idx: 340 |  Loss_1: (0.4755) | Acc_1: (82.47%) (35997/43648)\n",
      "Epoch: 26 | Batch_idx: 350 |  Loss_1: (0.4770) | Acc_1: (82.43%) (37036/44928)\n",
      "Epoch: 26 | Batch_idx: 360 |  Loss_1: (0.4760) | Acc_1: (82.46%) (38101/46208)\n",
      "Epoch: 26 | Batch_idx: 370 |  Loss_1: (0.4773) | Acc_1: (82.40%) (39131/47488)\n",
      "Epoch: 26 | Batch_idx: 380 |  Loss_1: (0.4777) | Acc_1: (82.39%) (40180/48768)\n",
      "Epoch: 26 | Batch_idx: 390 |  Loss_1: (0.4796) | Acc_1: (82.32%) (41159/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4001) | Acc: (87.75%) (8775/10000)\n",
      "Epoch: 27 | Batch_idx: 0 |  Loss_1: (0.6399) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 27 | Batch_idx: 10 |  Loss_1: (0.5014) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 27 | Batch_idx: 20 |  Loss_1: (0.4873) | Acc_1: (82.33%) (2213/2688)\n",
      "Epoch: 27 | Batch_idx: 30 |  Loss_1: (0.4920) | Acc_1: (81.93%) (3251/3968)\n",
      "Epoch: 27 | Batch_idx: 40 |  Loss_1: (0.4885) | Acc_1: (81.94%) (4300/5248)\n",
      "Epoch: 27 | Batch_idx: 50 |  Loss_1: (0.4895) | Acc_1: (82.05%) (5356/6528)\n",
      "Epoch: 27 | Batch_idx: 60 |  Loss_1: (0.4811) | Acc_1: (82.39%) (6433/7808)\n",
      "Epoch: 27 | Batch_idx: 70 |  Loss_1: (0.4780) | Acc_1: (82.41%) (7489/9088)\n",
      "Epoch: 27 | Batch_idx: 80 |  Loss_1: (0.4753) | Acc_1: (82.48%) (8552/10368)\n",
      "Epoch: 27 | Batch_idx: 90 |  Loss_1: (0.4720) | Acc_1: (82.70%) (9633/11648)\n",
      "Epoch: 27 | Batch_idx: 100 |  Loss_1: (0.4753) | Acc_1: (82.58%) (10676/12928)\n",
      "Epoch: 27 | Batch_idx: 110 |  Loss_1: (0.4728) | Acc_1: (82.64%) (11741/14208)\n",
      "Epoch: 27 | Batch_idx: 120 |  Loss_1: (0.4732) | Acc_1: (82.63%) (12798/15488)\n",
      "Epoch: 27 | Batch_idx: 130 |  Loss_1: (0.4720) | Acc_1: (82.65%) (13858/16768)\n",
      "Epoch: 27 | Batch_idx: 140 |  Loss_1: (0.4731) | Acc_1: (82.59%) (14905/18048)\n",
      "Epoch: 27 | Batch_idx: 150 |  Loss_1: (0.4720) | Acc_1: (82.65%) (15975/19328)\n",
      "Epoch: 27 | Batch_idx: 160 |  Loss_1: (0.4722) | Acc_1: (82.60%) (17023/20608)\n",
      "Epoch: 27 | Batch_idx: 170 |  Loss_1: (0.4742) | Acc_1: (82.50%) (18057/21888)\n",
      "Epoch: 27 | Batch_idx: 180 |  Loss_1: (0.4726) | Acc_1: (82.54%) (19123/23168)\n",
      "Epoch: 27 | Batch_idx: 190 |  Loss_1: (0.4754) | Acc_1: (82.45%) (20158/24448)\n",
      "Epoch: 27 | Batch_idx: 200 |  Loss_1: (0.4759) | Acc_1: (82.47%) (21217/25728)\n",
      "Epoch: 27 | Batch_idx: 210 |  Loss_1: (0.4763) | Acc_1: (82.46%) (22270/27008)\n",
      "Epoch: 27 | Batch_idx: 220 |  Loss_1: (0.4757) | Acc_1: (82.46%) (23325/28288)\n",
      "Epoch: 27 | Batch_idx: 230 |  Loss_1: (0.4754) | Acc_1: (82.42%) (24371/29568)\n",
      "Epoch: 27 | Batch_idx: 240 |  Loss_1: (0.4743) | Acc_1: (82.47%) (25441/30848)\n",
      "Epoch: 27 | Batch_idx: 250 |  Loss_1: (0.4754) | Acc_1: (82.45%) (26491/32128)\n",
      "Epoch: 27 | Batch_idx: 260 |  Loss_1: (0.4767) | Acc_1: (82.43%) (27538/33408)\n",
      "Epoch: 27 | Batch_idx: 270 |  Loss_1: (0.4765) | Acc_1: (82.40%) (28584/34688)\n",
      "Epoch: 27 | Batch_idx: 280 |  Loss_1: (0.4742) | Acc_1: (82.48%) (29668/35968)\n",
      "Epoch: 27 | Batch_idx: 290 |  Loss_1: (0.4741) | Acc_1: (82.48%) (30721/37248)\n",
      "Epoch: 27 | Batch_idx: 300 |  Loss_1: (0.4733) | Acc_1: (82.50%) (31784/38528)\n",
      "Epoch: 27 | Batch_idx: 310 |  Loss_1: (0.4723) | Acc_1: (82.52%) (32848/39808)\n",
      "Epoch: 27 | Batch_idx: 320 |  Loss_1: (0.4722) | Acc_1: (82.54%) (33915/41088)\n",
      "Epoch: 27 | Batch_idx: 330 |  Loss_1: (0.4708) | Acc_1: (82.60%) (34995/42368)\n",
      "Epoch: 27 | Batch_idx: 340 |  Loss_1: (0.4705) | Acc_1: (82.62%) (36061/43648)\n",
      "Epoch: 27 | Batch_idx: 350 |  Loss_1: (0.4709) | Acc_1: (82.60%) (37110/44928)\n",
      "Epoch: 27 | Batch_idx: 360 |  Loss_1: (0.4719) | Acc_1: (82.55%) (38145/46208)\n",
      "Epoch: 27 | Batch_idx: 370 |  Loss_1: (0.4720) | Acc_1: (82.54%) (39195/47488)\n",
      "Epoch: 27 | Batch_idx: 380 |  Loss_1: (0.4716) | Acc_1: (82.55%) (40256/48768)\n",
      "Epoch: 27 | Batch_idx: 390 |  Loss_1: (0.4730) | Acc_1: (82.49%) (41246/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4179) | Acc: (87.96%) (8796/10000)\n",
      "Epoch: 28 | Batch_idx: 0 |  Loss_1: (0.6298) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 28 | Batch_idx: 10 |  Loss_1: (0.4593) | Acc_1: (82.24%) (1158/1408)\n",
      "Epoch: 28 | Batch_idx: 20 |  Loss_1: (0.4658) | Acc_1: (82.51%) (2218/2688)\n",
      "Epoch: 28 | Batch_idx: 30 |  Loss_1: (0.4611) | Acc_1: (82.56%) (3276/3968)\n",
      "Epoch: 28 | Batch_idx: 40 |  Loss_1: (0.4604) | Acc_1: (82.60%) (4335/5248)\n",
      "Epoch: 28 | Batch_idx: 50 |  Loss_1: (0.4531) | Acc_1: (83.03%) (5420/6528)\n",
      "Epoch: 28 | Batch_idx: 60 |  Loss_1: (0.4627) | Acc_1: (82.67%) (6455/7808)\n",
      "Epoch: 28 | Batch_idx: 70 |  Loss_1: (0.4573) | Acc_1: (82.88%) (7532/9088)\n",
      "Epoch: 28 | Batch_idx: 80 |  Loss_1: (0.4566) | Acc_1: (82.91%) (8596/10368)\n",
      "Epoch: 28 | Batch_idx: 90 |  Loss_1: (0.4490) | Acc_1: (83.25%) (9697/11648)\n",
      "Epoch: 28 | Batch_idx: 100 |  Loss_1: (0.4520) | Acc_1: (83.21%) (10757/12928)\n",
      "Epoch: 28 | Batch_idx: 110 |  Loss_1: (0.4555) | Acc_1: (83.17%) (11817/14208)\n",
      "Epoch: 28 | Batch_idx: 120 |  Loss_1: (0.4586) | Acc_1: (83.06%) (12865/15488)\n",
      "Epoch: 28 | Batch_idx: 130 |  Loss_1: (0.4569) | Acc_1: (83.13%) (13940/16768)\n",
      "Epoch: 28 | Batch_idx: 140 |  Loss_1: (0.4570) | Acc_1: (83.14%) (15005/18048)\n",
      "Epoch: 28 | Batch_idx: 150 |  Loss_1: (0.4550) | Acc_1: (83.26%) (16093/19328)\n",
      "Epoch: 28 | Batch_idx: 160 |  Loss_1: (0.4577) | Acc_1: (83.20%) (17145/20608)\n",
      "Epoch: 28 | Batch_idx: 170 |  Loss_1: (0.4578) | Acc_1: (83.19%) (18209/21888)\n",
      "Epoch: 28 | Batch_idx: 180 |  Loss_1: (0.4584) | Acc_1: (83.17%) (19269/23168)\n",
      "Epoch: 28 | Batch_idx: 190 |  Loss_1: (0.4603) | Acc_1: (83.06%) (20306/24448)\n",
      "Epoch: 28 | Batch_idx: 200 |  Loss_1: (0.4593) | Acc_1: (83.09%) (21377/25728)\n",
      "Epoch: 28 | Batch_idx: 210 |  Loss_1: (0.4598) | Acc_1: (83.03%) (22425/27008)\n",
      "Epoch: 28 | Batch_idx: 220 |  Loss_1: (0.4585) | Acc_1: (83.06%) (23496/28288)\n",
      "Epoch: 28 | Batch_idx: 230 |  Loss_1: (0.4579) | Acc_1: (83.10%) (24572/29568)\n",
      "Epoch: 28 | Batch_idx: 240 |  Loss_1: (0.4591) | Acc_1: (83.06%) (25621/30848)\n",
      "Epoch: 28 | Batch_idx: 250 |  Loss_1: (0.4586) | Acc_1: (83.07%) (26689/32128)\n",
      "Epoch: 28 | Batch_idx: 260 |  Loss_1: (0.4589) | Acc_1: (83.05%) (27745/33408)\n",
      "Epoch: 28 | Batch_idx: 270 |  Loss_1: (0.4588) | Acc_1: (83.04%) (28806/34688)\n",
      "Epoch: 28 | Batch_idx: 280 |  Loss_1: (0.4585) | Acc_1: (83.05%) (29870/35968)\n",
      "Epoch: 28 | Batch_idx: 290 |  Loss_1: (0.4583) | Acc_1: (83.06%) (30939/37248)\n",
      "Epoch: 28 | Batch_idx: 300 |  Loss_1: (0.4590) | Acc_1: (83.03%) (31990/38528)\n",
      "Epoch: 28 | Batch_idx: 310 |  Loss_1: (0.4595) | Acc_1: (83.02%) (33048/39808)\n",
      "Epoch: 28 | Batch_idx: 320 |  Loss_1: (0.4594) | Acc_1: (83.03%) (34114/41088)\n",
      "Epoch: 28 | Batch_idx: 330 |  Loss_1: (0.4597) | Acc_1: (82.98%) (35159/42368)\n",
      "Epoch: 28 | Batch_idx: 340 |  Loss_1: (0.4602) | Acc_1: (82.97%) (36215/43648)\n",
      "Epoch: 28 | Batch_idx: 350 |  Loss_1: (0.4601) | Acc_1: (82.97%) (37275/44928)\n",
      "Epoch: 28 | Batch_idx: 360 |  Loss_1: (0.4606) | Acc_1: (82.94%) (38323/46208)\n",
      "Epoch: 28 | Batch_idx: 370 |  Loss_1: (0.4608) | Acc_1: (82.95%) (39389/47488)\n",
      "Epoch: 28 | Batch_idx: 380 |  Loss_1: (0.4604) | Acc_1: (82.97%) (40461/48768)\n",
      "Epoch: 28 | Batch_idx: 390 |  Loss_1: (0.4608) | Acc_1: (82.94%) (41471/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.3954) | Acc: (88.16%) (8816/10000)\n",
      "Epoch: 29 | Batch_idx: 0 |  Loss_1: (0.4606) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 29 | Batch_idx: 10 |  Loss_1: (0.4249) | Acc_1: (84.66%) (1192/1408)\n",
      "Epoch: 29 | Batch_idx: 20 |  Loss_1: (0.4272) | Acc_1: (84.30%) (2266/2688)\n",
      "Epoch: 29 | Batch_idx: 30 |  Loss_1: (0.4343) | Acc_1: (83.85%) (3327/3968)\n",
      "Epoch: 29 | Batch_idx: 40 |  Loss_1: (0.4502) | Acc_1: (83.23%) (4368/5248)\n",
      "Epoch: 29 | Batch_idx: 50 |  Loss_1: (0.4528) | Acc_1: (83.16%) (5429/6528)\n",
      "Epoch: 29 | Batch_idx: 60 |  Loss_1: (0.4564) | Acc_1: (83.16%) (6493/7808)\n",
      "Epoch: 29 | Batch_idx: 70 |  Loss_1: (0.4632) | Acc_1: (82.89%) (7533/9088)\n",
      "Epoch: 29 | Batch_idx: 80 |  Loss_1: (0.4652) | Acc_1: (82.89%) (8594/10368)\n",
      "Epoch: 29 | Batch_idx: 90 |  Loss_1: (0.4667) | Acc_1: (82.83%) (9648/11648)\n",
      "Epoch: 29 | Batch_idx: 100 |  Loss_1: (0.4639) | Acc_1: (82.85%) (10711/12928)\n",
      "Epoch: 29 | Batch_idx: 110 |  Loss_1: (0.4662) | Acc_1: (82.73%) (11754/14208)\n",
      "Epoch: 29 | Batch_idx: 120 |  Loss_1: (0.4635) | Acc_1: (82.84%) (12830/15488)\n",
      "Epoch: 29 | Batch_idx: 130 |  Loss_1: (0.4607) | Acc_1: (82.91%) (13903/16768)\n",
      "Epoch: 29 | Batch_idx: 140 |  Loss_1: (0.4589) | Acc_1: (82.95%) (14971/18048)\n",
      "Epoch: 29 | Batch_idx: 150 |  Loss_1: (0.4580) | Acc_1: (82.97%) (16037/19328)\n",
      "Epoch: 29 | Batch_idx: 160 |  Loss_1: (0.4597) | Acc_1: (82.93%) (17091/20608)\n",
      "Epoch: 29 | Batch_idx: 170 |  Loss_1: (0.4608) | Acc_1: (82.87%) (18138/21888)\n",
      "Epoch: 29 | Batch_idx: 180 |  Loss_1: (0.4617) | Acc_1: (82.86%) (19198/23168)\n",
      "Epoch: 29 | Batch_idx: 190 |  Loss_1: (0.4616) | Acc_1: (82.86%) (20257/24448)\n",
      "Epoch: 29 | Batch_idx: 200 |  Loss_1: (0.4616) | Acc_1: (82.84%) (21314/25728)\n",
      "Epoch: 29 | Batch_idx: 210 |  Loss_1: (0.4619) | Acc_1: (82.85%) (22377/27008)\n",
      "Epoch: 29 | Batch_idx: 220 |  Loss_1: (0.4615) | Acc_1: (82.87%) (23443/28288)\n",
      "Epoch: 29 | Batch_idx: 230 |  Loss_1: (0.4615) | Acc_1: (82.86%) (24501/29568)\n",
      "Epoch: 29 | Batch_idx: 240 |  Loss_1: (0.4603) | Acc_1: (82.92%) (25578/30848)\n",
      "Epoch: 29 | Batch_idx: 250 |  Loss_1: (0.4590) | Acc_1: (82.98%) (26661/32128)\n",
      "Epoch: 29 | Batch_idx: 260 |  Loss_1: (0.4584) | Acc_1: (83.03%) (27738/33408)\n",
      "Epoch: 29 | Batch_idx: 270 |  Loss_1: (0.4584) | Acc_1: (83.01%) (28796/34688)\n",
      "Epoch: 29 | Batch_idx: 280 |  Loss_1: (0.4577) | Acc_1: (83.04%) (29869/35968)\n",
      "Epoch: 29 | Batch_idx: 290 |  Loss_1: (0.4567) | Acc_1: (83.10%) (30953/37248)\n",
      "Epoch: 29 | Batch_idx: 300 |  Loss_1: (0.4579) | Acc_1: (83.08%) (32009/38528)\n",
      "Epoch: 29 | Batch_idx: 310 |  Loss_1: (0.4571) | Acc_1: (83.09%) (33076/39808)\n",
      "Epoch: 29 | Batch_idx: 320 |  Loss_1: (0.4578) | Acc_1: (83.07%) (34133/41088)\n",
      "Epoch: 29 | Batch_idx: 330 |  Loss_1: (0.4578) | Acc_1: (83.08%) (35200/42368)\n",
      "Epoch: 29 | Batch_idx: 340 |  Loss_1: (0.4581) | Acc_1: (83.06%) (36252/43648)\n",
      "Epoch: 29 | Batch_idx: 350 |  Loss_1: (0.4585) | Acc_1: (83.03%) (37305/44928)\n",
      "Epoch: 29 | Batch_idx: 360 |  Loss_1: (0.4582) | Acc_1: (83.06%) (38381/46208)\n",
      "Epoch: 29 | Batch_idx: 370 |  Loss_1: (0.4575) | Acc_1: (83.06%) (39444/47488)\n",
      "Epoch: 29 | Batch_idx: 380 |  Loss_1: (0.4582) | Acc_1: (83.05%) (40502/48768)\n",
      "Epoch: 29 | Batch_idx: 390 |  Loss_1: (0.4587) | Acc_1: (83.06%) (41528/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4122) | Acc: (87.88%) (8788/10000)\n",
      "Epoch: 30 | Batch_idx: 0 |  Loss_1: (0.5596) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 30 | Batch_idx: 10 |  Loss_1: (0.4313) | Acc_1: (83.03%) (1169/1408)\n",
      "Epoch: 30 | Batch_idx: 20 |  Loss_1: (0.4457) | Acc_1: (82.89%) (2228/2688)\n",
      "Epoch: 30 | Batch_idx: 30 |  Loss_1: (0.4530) | Acc_1: (82.84%) (3287/3968)\n",
      "Epoch: 30 | Batch_idx: 40 |  Loss_1: (0.4543) | Acc_1: (83.00%) (4356/5248)\n",
      "Epoch: 30 | Batch_idx: 50 |  Loss_1: (0.4568) | Acc_1: (82.98%) (5417/6528)\n",
      "Epoch: 30 | Batch_idx: 60 |  Loss_1: (0.4557) | Acc_1: (82.91%) (6474/7808)\n",
      "Epoch: 30 | Batch_idx: 70 |  Loss_1: (0.4572) | Acc_1: (82.90%) (7534/9088)\n",
      "Epoch: 30 | Batch_idx: 80 |  Loss_1: (0.4539) | Acc_1: (82.95%) (8600/10368)\n",
      "Epoch: 30 | Batch_idx: 90 |  Loss_1: (0.4550) | Acc_1: (82.87%) (9653/11648)\n",
      "Epoch: 30 | Batch_idx: 100 |  Loss_1: (0.4559) | Acc_1: (82.85%) (10711/12928)\n",
      "Epoch: 30 | Batch_idx: 110 |  Loss_1: (0.4611) | Acc_1: (82.62%) (11738/14208)\n",
      "Epoch: 30 | Batch_idx: 120 |  Loss_1: (0.4637) | Acc_1: (82.62%) (12796/15488)\n",
      "Epoch: 30 | Batch_idx: 130 |  Loss_1: (0.4631) | Acc_1: (82.65%) (13859/16768)\n",
      "Epoch: 30 | Batch_idx: 140 |  Loss_1: (0.4629) | Acc_1: (82.65%) (14916/18048)\n",
      "Epoch: 30 | Batch_idx: 150 |  Loss_1: (0.4633) | Acc_1: (82.66%) (15977/19328)\n",
      "Epoch: 30 | Batch_idx: 160 |  Loss_1: (0.4608) | Acc_1: (82.76%) (17055/20608)\n",
      "Epoch: 30 | Batch_idx: 170 |  Loss_1: (0.4578) | Acc_1: (82.89%) (18142/21888)\n",
      "Epoch: 30 | Batch_idx: 180 |  Loss_1: (0.4575) | Acc_1: (82.91%) (19209/23168)\n",
      "Epoch: 30 | Batch_idx: 190 |  Loss_1: (0.4546) | Acc_1: (83.07%) (20308/24448)\n",
      "Epoch: 30 | Batch_idx: 200 |  Loss_1: (0.4534) | Acc_1: (83.09%) (21377/25728)\n",
      "Epoch: 30 | Batch_idx: 210 |  Loss_1: (0.4529) | Acc_1: (83.13%) (22452/27008)\n",
      "Epoch: 30 | Batch_idx: 220 |  Loss_1: (0.4553) | Acc_1: (83.06%) (23497/28288)\n",
      "Epoch: 30 | Batch_idx: 230 |  Loss_1: (0.4567) | Acc_1: (83.02%) (24546/29568)\n",
      "Epoch: 30 | Batch_idx: 240 |  Loss_1: (0.4563) | Acc_1: (83.02%) (25611/30848)\n",
      "Epoch: 30 | Batch_idx: 250 |  Loss_1: (0.4556) | Acc_1: (83.03%) (26677/32128)\n",
      "Epoch: 30 | Batch_idx: 260 |  Loss_1: (0.4549) | Acc_1: (83.05%) (27747/33408)\n",
      "Epoch: 30 | Batch_idx: 270 |  Loss_1: (0.4541) | Acc_1: (83.11%) (28828/34688)\n",
      "Epoch: 30 | Batch_idx: 280 |  Loss_1: (0.4536) | Acc_1: (83.12%) (29896/35968)\n",
      "Epoch: 30 | Batch_idx: 290 |  Loss_1: (0.4532) | Acc_1: (83.13%) (30966/37248)\n",
      "Epoch: 30 | Batch_idx: 300 |  Loss_1: (0.4544) | Acc_1: (83.12%) (32024/38528)\n",
      "Epoch: 30 | Batch_idx: 310 |  Loss_1: (0.4543) | Acc_1: (83.12%) (33089/39808)\n",
      "Epoch: 30 | Batch_idx: 320 |  Loss_1: (0.4539) | Acc_1: (83.13%) (34157/41088)\n",
      "Epoch: 30 | Batch_idx: 330 |  Loss_1: (0.4547) | Acc_1: (83.11%) (35213/42368)\n",
      "Epoch: 30 | Batch_idx: 340 |  Loss_1: (0.4550) | Acc_1: (83.08%) (36264/43648)\n",
      "Epoch: 30 | Batch_idx: 350 |  Loss_1: (0.4558) | Acc_1: (83.08%) (37325/44928)\n",
      "Epoch: 30 | Batch_idx: 360 |  Loss_1: (0.4557) | Acc_1: (83.06%) (38380/46208)\n",
      "Epoch: 30 | Batch_idx: 370 |  Loss_1: (0.4558) | Acc_1: (83.07%) (39449/47488)\n",
      "Epoch: 30 | Batch_idx: 380 |  Loss_1: (0.4546) | Acc_1: (83.12%) (40536/48768)\n",
      "Epoch: 30 | Batch_idx: 390 |  Loss_1: (0.4547) | Acc_1: (83.12%) (41558/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4988) | Acc: (86.10%) (8610/10000)\n",
      "Epoch: 31 | Batch_idx: 0 |  Loss_1: (0.5830) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 31 | Batch_idx: 10 |  Loss_1: (0.4768) | Acc_1: (82.24%) (1158/1408)\n",
      "Epoch: 31 | Batch_idx: 20 |  Loss_1: (0.4535) | Acc_1: (82.92%) (2229/2688)\n",
      "Epoch: 31 | Batch_idx: 30 |  Loss_1: (0.4612) | Acc_1: (82.71%) (3282/3968)\n",
      "Epoch: 31 | Batch_idx: 40 |  Loss_1: (0.4562) | Acc_1: (83.08%) (4360/5248)\n",
      "Epoch: 31 | Batch_idx: 50 |  Loss_1: (0.4520) | Acc_1: (83.23%) (5433/6528)\n",
      "Epoch: 31 | Batch_idx: 60 |  Loss_1: (0.4512) | Acc_1: (83.31%) (6505/7808)\n",
      "Epoch: 31 | Batch_idx: 70 |  Loss_1: (0.4498) | Acc_1: (83.34%) (7574/9088)\n",
      "Epoch: 31 | Batch_idx: 80 |  Loss_1: (0.4575) | Acc_1: (83.13%) (8619/10368)\n",
      "Epoch: 31 | Batch_idx: 90 |  Loss_1: (0.4612) | Acc_1: (82.97%) (9664/11648)\n",
      "Epoch: 31 | Batch_idx: 100 |  Loss_1: (0.4585) | Acc_1: (83.06%) (10738/12928)\n",
      "Epoch: 31 | Batch_idx: 110 |  Loss_1: (0.4551) | Acc_1: (83.27%) (11831/14208)\n",
      "Epoch: 31 | Batch_idx: 120 |  Loss_1: (0.4564) | Acc_1: (83.25%) (12894/15488)\n",
      "Epoch: 31 | Batch_idx: 130 |  Loss_1: (0.4586) | Acc_1: (83.18%) (13948/16768)\n",
      "Epoch: 31 | Batch_idx: 140 |  Loss_1: (0.4566) | Acc_1: (83.21%) (15017/18048)\n",
      "Epoch: 31 | Batch_idx: 150 |  Loss_1: (0.4547) | Acc_1: (83.26%) (16093/19328)\n",
      "Epoch: 31 | Batch_idx: 160 |  Loss_1: (0.4553) | Acc_1: (83.24%) (17154/20608)\n",
      "Epoch: 31 | Batch_idx: 170 |  Loss_1: (0.4543) | Acc_1: (83.32%) (18237/21888)\n",
      "Epoch: 31 | Batch_idx: 180 |  Loss_1: (0.4533) | Acc_1: (83.34%) (19309/23168)\n",
      "Epoch: 31 | Batch_idx: 190 |  Loss_1: (0.4541) | Acc_1: (83.30%) (20364/24448)\n",
      "Epoch: 31 | Batch_idx: 200 |  Loss_1: (0.4535) | Acc_1: (83.34%) (21443/25728)\n",
      "Epoch: 31 | Batch_idx: 210 |  Loss_1: (0.4530) | Acc_1: (83.30%) (22497/27008)\n",
      "Epoch: 31 | Batch_idx: 220 |  Loss_1: (0.4510) | Acc_1: (83.37%) (23585/28288)\n",
      "Epoch: 31 | Batch_idx: 230 |  Loss_1: (0.4509) | Acc_1: (83.40%) (24659/29568)\n",
      "Epoch: 31 | Batch_idx: 240 |  Loss_1: (0.4506) | Acc_1: (83.39%) (25725/30848)\n",
      "Epoch: 31 | Batch_idx: 250 |  Loss_1: (0.4493) | Acc_1: (83.42%) (26802/32128)\n",
      "Epoch: 31 | Batch_idx: 260 |  Loss_1: (0.4501) | Acc_1: (83.40%) (27862/33408)\n",
      "Epoch: 31 | Batch_idx: 270 |  Loss_1: (0.4508) | Acc_1: (83.37%) (28920/34688)\n",
      "Epoch: 31 | Batch_idx: 280 |  Loss_1: (0.4514) | Acc_1: (83.34%) (29974/35968)\n",
      "Epoch: 31 | Batch_idx: 290 |  Loss_1: (0.4517) | Acc_1: (83.34%) (31041/37248)\n",
      "Epoch: 31 | Batch_idx: 300 |  Loss_1: (0.4530) | Acc_1: (83.27%) (32083/38528)\n",
      "Epoch: 31 | Batch_idx: 310 |  Loss_1: (0.4542) | Acc_1: (83.22%) (33129/39808)\n",
      "Epoch: 31 | Batch_idx: 320 |  Loss_1: (0.4537) | Acc_1: (83.27%) (34213/41088)\n",
      "Epoch: 31 | Batch_idx: 330 |  Loss_1: (0.4544) | Acc_1: (83.23%) (35263/42368)\n",
      "Epoch: 31 | Batch_idx: 340 |  Loss_1: (0.4540) | Acc_1: (83.25%) (36337/43648)\n",
      "Epoch: 31 | Batch_idx: 350 |  Loss_1: (0.4544) | Acc_1: (83.24%) (37396/44928)\n",
      "Epoch: 31 | Batch_idx: 360 |  Loss_1: (0.4539) | Acc_1: (83.25%) (38466/46208)\n",
      "Epoch: 31 | Batch_idx: 370 |  Loss_1: (0.4525) | Acc_1: (83.29%) (39551/47488)\n",
      "Epoch: 31 | Batch_idx: 380 |  Loss_1: (0.4526) | Acc_1: (83.29%) (40617/48768)\n",
      "Epoch: 31 | Batch_idx: 390 |  Loss_1: (0.4528) | Acc_1: (83.30%) (41650/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4171) | Acc: (87.79%) (8779/10000)\n",
      "Epoch: 32 | Batch_idx: 0 |  Loss_1: (0.4494) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 32 | Batch_idx: 10 |  Loss_1: (0.4193) | Acc_1: (84.45%) (1189/1408)\n",
      "Epoch: 32 | Batch_idx: 20 |  Loss_1: (0.4215) | Acc_1: (84.30%) (2266/2688)\n",
      "Epoch: 32 | Batch_idx: 30 |  Loss_1: (0.4390) | Acc_1: (83.80%) (3325/3968)\n",
      "Epoch: 32 | Batch_idx: 40 |  Loss_1: (0.4381) | Acc_1: (83.73%) (4394/5248)\n",
      "Epoch: 32 | Batch_idx: 50 |  Loss_1: (0.4346) | Acc_1: (83.98%) (5482/6528)\n",
      "Epoch: 32 | Batch_idx: 60 |  Loss_1: (0.4376) | Acc_1: (83.75%) (6539/7808)\n",
      "Epoch: 32 | Batch_idx: 70 |  Loss_1: (0.4396) | Acc_1: (83.70%) (7607/9088)\n",
      "Epoch: 32 | Batch_idx: 80 |  Loss_1: (0.4408) | Acc_1: (83.62%) (8670/10368)\n",
      "Epoch: 32 | Batch_idx: 90 |  Loss_1: (0.4388) | Acc_1: (83.77%) (9757/11648)\n",
      "Epoch: 32 | Batch_idx: 100 |  Loss_1: (0.4441) | Acc_1: (83.62%) (10810/12928)\n",
      "Epoch: 32 | Batch_idx: 110 |  Loss_1: (0.4474) | Acc_1: (83.45%) (11856/14208)\n",
      "Epoch: 32 | Batch_idx: 120 |  Loss_1: (0.4481) | Acc_1: (83.42%) (12920/15488)\n",
      "Epoch: 32 | Batch_idx: 130 |  Loss_1: (0.4449) | Acc_1: (83.57%) (14013/16768)\n",
      "Epoch: 32 | Batch_idx: 140 |  Loss_1: (0.4438) | Acc_1: (83.65%) (15098/18048)\n",
      "Epoch: 32 | Batch_idx: 150 |  Loss_1: (0.4454) | Acc_1: (83.58%) (16155/19328)\n",
      "Epoch: 32 | Batch_idx: 160 |  Loss_1: (0.4445) | Acc_1: (83.64%) (17237/20608)\n",
      "Epoch: 32 | Batch_idx: 170 |  Loss_1: (0.4405) | Acc_1: (83.79%) (18341/21888)\n",
      "Epoch: 32 | Batch_idx: 180 |  Loss_1: (0.4411) | Acc_1: (83.75%) (19404/23168)\n",
      "Epoch: 32 | Batch_idx: 190 |  Loss_1: (0.4428) | Acc_1: (83.71%) (20465/24448)\n",
      "Epoch: 32 | Batch_idx: 200 |  Loss_1: (0.4429) | Acc_1: (83.71%) (21536/25728)\n",
      "Epoch: 32 | Batch_idx: 210 |  Loss_1: (0.4425) | Acc_1: (83.71%) (22609/27008)\n",
      "Epoch: 32 | Batch_idx: 220 |  Loss_1: (0.4425) | Acc_1: (83.71%) (23679/28288)\n",
      "Epoch: 32 | Batch_idx: 230 |  Loss_1: (0.4433) | Acc_1: (83.66%) (24737/29568)\n",
      "Epoch: 32 | Batch_idx: 240 |  Loss_1: (0.4446) | Acc_1: (83.60%) (25789/30848)\n",
      "Epoch: 32 | Batch_idx: 250 |  Loss_1: (0.4439) | Acc_1: (83.61%) (26863/32128)\n",
      "Epoch: 32 | Batch_idx: 260 |  Loss_1: (0.4450) | Acc_1: (83.58%) (27922/33408)\n",
      "Epoch: 32 | Batch_idx: 270 |  Loss_1: (0.4453) | Acc_1: (83.59%) (28994/34688)\n",
      "Epoch: 32 | Batch_idx: 280 |  Loss_1: (0.4445) | Acc_1: (83.62%) (30076/35968)\n",
      "Epoch: 32 | Batch_idx: 290 |  Loss_1: (0.4469) | Acc_1: (83.51%) (31104/37248)\n",
      "Epoch: 32 | Batch_idx: 300 |  Loss_1: (0.4460) | Acc_1: (83.55%) (32190/38528)\n",
      "Epoch: 32 | Batch_idx: 310 |  Loss_1: (0.4477) | Acc_1: (83.48%) (33233/39808)\n",
      "Epoch: 32 | Batch_idx: 320 |  Loss_1: (0.4470) | Acc_1: (83.51%) (34311/41088)\n",
      "Epoch: 32 | Batch_idx: 330 |  Loss_1: (0.4478) | Acc_1: (83.49%) (35373/42368)\n",
      "Epoch: 32 | Batch_idx: 340 |  Loss_1: (0.4497) | Acc_1: (83.41%) (36406/43648)\n",
      "Epoch: 32 | Batch_idx: 350 |  Loss_1: (0.4500) | Acc_1: (83.38%) (37463/44928)\n",
      "Epoch: 32 | Batch_idx: 360 |  Loss_1: (0.4507) | Acc_1: (83.36%) (38519/46208)\n",
      "Epoch: 32 | Batch_idx: 370 |  Loss_1: (0.4516) | Acc_1: (83.33%) (39570/47488)\n",
      "Epoch: 32 | Batch_idx: 380 |  Loss_1: (0.4530) | Acc_1: (83.27%) (40607/48768)\n",
      "Epoch: 32 | Batch_idx: 390 |  Loss_1: (0.4526) | Acc_1: (83.28%) (41638/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4015) | Acc: (88.60%) (8860/10000)\n",
      "Epoch: 33 | Batch_idx: 0 |  Loss_1: (0.4731) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 33 | Batch_idx: 10 |  Loss_1: (0.4403) | Acc_1: (82.95%) (1168/1408)\n",
      "Epoch: 33 | Batch_idx: 20 |  Loss_1: (0.4598) | Acc_1: (82.74%) (2224/2688)\n",
      "Epoch: 33 | Batch_idx: 30 |  Loss_1: (0.4507) | Acc_1: (83.24%) (3303/3968)\n",
      "Epoch: 33 | Batch_idx: 40 |  Loss_1: (0.4501) | Acc_1: (83.27%) (4370/5248)\n",
      "Epoch: 33 | Batch_idx: 50 |  Loss_1: (0.4584) | Acc_1: (83.01%) (5419/6528)\n",
      "Epoch: 33 | Batch_idx: 60 |  Loss_1: (0.4567) | Acc_1: (83.11%) (6489/7808)\n",
      "Epoch: 33 | Batch_idx: 70 |  Loss_1: (0.4578) | Acc_1: (83.15%) (7557/9088)\n",
      "Epoch: 33 | Batch_idx: 80 |  Loss_1: (0.4543) | Acc_1: (83.21%) (8627/10368)\n",
      "Epoch: 33 | Batch_idx: 90 |  Loss_1: (0.4540) | Acc_1: (83.20%) (9691/11648)\n",
      "Epoch: 33 | Batch_idx: 100 |  Loss_1: (0.4517) | Acc_1: (83.28%) (10766/12928)\n",
      "Epoch: 33 | Batch_idx: 110 |  Loss_1: (0.4522) | Acc_1: (83.22%) (11824/14208)\n",
      "Epoch: 33 | Batch_idx: 120 |  Loss_1: (0.4521) | Acc_1: (83.23%) (12891/15488)\n",
      "Epoch: 33 | Batch_idx: 130 |  Loss_1: (0.4499) | Acc_1: (83.31%) (13969/16768)\n",
      "Epoch: 33 | Batch_idx: 140 |  Loss_1: (0.4489) | Acc_1: (83.36%) (15045/18048)\n",
      "Epoch: 33 | Batch_idx: 150 |  Loss_1: (0.4497) | Acc_1: (83.36%) (16111/19328)\n",
      "Epoch: 33 | Batch_idx: 160 |  Loss_1: (0.4480) | Acc_1: (83.44%) (17195/20608)\n",
      "Epoch: 33 | Batch_idx: 170 |  Loss_1: (0.4486) | Acc_1: (83.45%) (18265/21888)\n",
      "Epoch: 33 | Batch_idx: 180 |  Loss_1: (0.4477) | Acc_1: (83.49%) (19343/23168)\n",
      "Epoch: 33 | Batch_idx: 190 |  Loss_1: (0.4468) | Acc_1: (83.50%) (20414/24448)\n",
      "Epoch: 33 | Batch_idx: 200 |  Loss_1: (0.4466) | Acc_1: (83.49%) (21481/25728)\n",
      "Epoch: 33 | Batch_idx: 210 |  Loss_1: (0.4463) | Acc_1: (83.50%) (22551/27008)\n",
      "Epoch: 33 | Batch_idx: 220 |  Loss_1: (0.4468) | Acc_1: (83.45%) (23605/28288)\n",
      "Epoch: 33 | Batch_idx: 230 |  Loss_1: (0.4482) | Acc_1: (83.39%) (24657/29568)\n",
      "Epoch: 33 | Batch_idx: 240 |  Loss_1: (0.4462) | Acc_1: (83.45%) (25744/30848)\n",
      "Epoch: 33 | Batch_idx: 250 |  Loss_1: (0.4466) | Acc_1: (83.46%) (26813/32128)\n",
      "Epoch: 33 | Batch_idx: 260 |  Loss_1: (0.4443) | Acc_1: (83.55%) (27914/33408)\n",
      "Epoch: 33 | Batch_idx: 270 |  Loss_1: (0.4440) | Acc_1: (83.57%) (28990/34688)\n",
      "Epoch: 33 | Batch_idx: 280 |  Loss_1: (0.4457) | Acc_1: (83.54%) (30047/35968)\n",
      "Epoch: 33 | Batch_idx: 290 |  Loss_1: (0.4464) | Acc_1: (83.51%) (31105/37248)\n",
      "Epoch: 33 | Batch_idx: 300 |  Loss_1: (0.4472) | Acc_1: (83.49%) (32166/38528)\n",
      "Epoch: 33 | Batch_idx: 310 |  Loss_1: (0.4476) | Acc_1: (83.47%) (33226/39808)\n",
      "Epoch: 33 | Batch_idx: 320 |  Loss_1: (0.4472) | Acc_1: (83.48%) (34300/41088)\n",
      "Epoch: 33 | Batch_idx: 330 |  Loss_1: (0.4469) | Acc_1: (83.49%) (35374/42368)\n",
      "Epoch: 33 | Batch_idx: 340 |  Loss_1: (0.4467) | Acc_1: (83.49%) (36441/43648)\n",
      "Epoch: 33 | Batch_idx: 350 |  Loss_1: (0.4461) | Acc_1: (83.51%) (37518/44928)\n",
      "Epoch: 33 | Batch_idx: 360 |  Loss_1: (0.4464) | Acc_1: (83.50%) (38583/46208)\n",
      "Epoch: 33 | Batch_idx: 370 |  Loss_1: (0.4475) | Acc_1: (83.45%) (39628/47488)\n",
      "Epoch: 33 | Batch_idx: 380 |  Loss_1: (0.4478) | Acc_1: (83.47%) (40708/48768)\n",
      "Epoch: 33 | Batch_idx: 390 |  Loss_1: (0.4467) | Acc_1: (83.53%) (41765/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4269) | Acc: (88.16%) (8816/10000)\n",
      "Epoch: 34 | Batch_idx: 0 |  Loss_1: (0.4615) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 34 | Batch_idx: 10 |  Loss_1: (0.4580) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 34 | Batch_idx: 20 |  Loss_1: (0.4498) | Acc_1: (83.52%) (2245/2688)\n",
      "Epoch: 34 | Batch_idx: 30 |  Loss_1: (0.4281) | Acc_1: (84.25%) (3343/3968)\n",
      "Epoch: 34 | Batch_idx: 40 |  Loss_1: (0.4232) | Acc_1: (84.41%) (4430/5248)\n",
      "Epoch: 34 | Batch_idx: 50 |  Loss_1: (0.4195) | Acc_1: (84.48%) (5515/6528)\n",
      "Epoch: 34 | Batch_idx: 60 |  Loss_1: (0.4317) | Acc_1: (83.95%) (6555/7808)\n",
      "Epoch: 34 | Batch_idx: 70 |  Loss_1: (0.4331) | Acc_1: (83.91%) (7626/9088)\n",
      "Epoch: 34 | Batch_idx: 80 |  Loss_1: (0.4327) | Acc_1: (83.84%) (8693/10368)\n",
      "Epoch: 34 | Batch_idx: 90 |  Loss_1: (0.4304) | Acc_1: (83.98%) (9782/11648)\n",
      "Epoch: 34 | Batch_idx: 100 |  Loss_1: (0.4355) | Acc_1: (83.81%) (10835/12928)\n",
      "Epoch: 34 | Batch_idx: 110 |  Loss_1: (0.4341) | Acc_1: (83.88%) (11918/14208)\n",
      "Epoch: 34 | Batch_idx: 120 |  Loss_1: (0.4332) | Acc_1: (83.93%) (12999/15488)\n",
      "Epoch: 34 | Batch_idx: 130 |  Loss_1: (0.4334) | Acc_1: (83.93%) (14074/16768)\n",
      "Epoch: 34 | Batch_idx: 140 |  Loss_1: (0.4338) | Acc_1: (83.95%) (15151/18048)\n",
      "Epoch: 34 | Batch_idx: 150 |  Loss_1: (0.4339) | Acc_1: (83.95%) (16225/19328)\n",
      "Epoch: 34 | Batch_idx: 160 |  Loss_1: (0.4351) | Acc_1: (83.92%) (17295/20608)\n",
      "Epoch: 34 | Batch_idx: 170 |  Loss_1: (0.4344) | Acc_1: (83.96%) (18377/21888)\n",
      "Epoch: 34 | Batch_idx: 180 |  Loss_1: (0.4331) | Acc_1: (84.03%) (19468/23168)\n",
      "Epoch: 34 | Batch_idx: 190 |  Loss_1: (0.4341) | Acc_1: (83.95%) (20523/24448)\n",
      "Epoch: 34 | Batch_idx: 200 |  Loss_1: (0.4362) | Acc_1: (83.86%) (21575/25728)\n",
      "Epoch: 34 | Batch_idx: 210 |  Loss_1: (0.4375) | Acc_1: (83.83%) (22640/27008)\n",
      "Epoch: 34 | Batch_idx: 220 |  Loss_1: (0.4366) | Acc_1: (83.85%) (23720/28288)\n",
      "Epoch: 34 | Batch_idx: 230 |  Loss_1: (0.4366) | Acc_1: (83.85%) (24794/29568)\n",
      "Epoch: 34 | Batch_idx: 240 |  Loss_1: (0.4377) | Acc_1: (83.79%) (25846/30848)\n",
      "Epoch: 34 | Batch_idx: 250 |  Loss_1: (0.4391) | Acc_1: (83.76%) (26909/32128)\n",
      "Epoch: 34 | Batch_idx: 260 |  Loss_1: (0.4377) | Acc_1: (83.83%) (28006/33408)\n",
      "Epoch: 34 | Batch_idx: 270 |  Loss_1: (0.4379) | Acc_1: (83.81%) (29073/34688)\n",
      "Epoch: 34 | Batch_idx: 280 |  Loss_1: (0.4382) | Acc_1: (83.81%) (30145/35968)\n",
      "Epoch: 34 | Batch_idx: 290 |  Loss_1: (0.4381) | Acc_1: (83.80%) (31212/37248)\n",
      "Epoch: 34 | Batch_idx: 300 |  Loss_1: (0.4388) | Acc_1: (83.78%) (32280/38528)\n",
      "Epoch: 34 | Batch_idx: 310 |  Loss_1: (0.4386) | Acc_1: (83.79%) (33354/39808)\n",
      "Epoch: 34 | Batch_idx: 320 |  Loss_1: (0.4389) | Acc_1: (83.78%) (34425/41088)\n",
      "Epoch: 34 | Batch_idx: 330 |  Loss_1: (0.4398) | Acc_1: (83.75%) (35482/42368)\n",
      "Epoch: 34 | Batch_idx: 340 |  Loss_1: (0.4402) | Acc_1: (83.73%) (36547/43648)\n",
      "Epoch: 34 | Batch_idx: 350 |  Loss_1: (0.4401) | Acc_1: (83.75%) (37629/44928)\n",
      "Epoch: 34 | Batch_idx: 360 |  Loss_1: (0.4388) | Acc_1: (83.81%) (38729/46208)\n",
      "Epoch: 34 | Batch_idx: 370 |  Loss_1: (0.4386) | Acc_1: (83.83%) (39809/47488)\n",
      "Epoch: 34 | Batch_idx: 380 |  Loss_1: (0.4392) | Acc_1: (83.81%) (40874/48768)\n",
      "Epoch: 34 | Batch_idx: 390 |  Loss_1: (0.4401) | Acc_1: (83.77%) (41886/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4305) | Acc: (88.42%) (8842/10000)\n",
      "Epoch: 35 | Batch_idx: 0 |  Loss_1: (0.4468) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 35 | Batch_idx: 10 |  Loss_1: (0.4623) | Acc_1: (82.95%) (1168/1408)\n",
      "Epoch: 35 | Batch_idx: 20 |  Loss_1: (0.4533) | Acc_1: (82.70%) (2223/2688)\n",
      "Epoch: 35 | Batch_idx: 30 |  Loss_1: (0.4546) | Acc_1: (82.99%) (3293/3968)\n",
      "Epoch: 35 | Batch_idx: 40 |  Loss_1: (0.4517) | Acc_1: (83.16%) (4364/5248)\n",
      "Epoch: 35 | Batch_idx: 50 |  Loss_1: (0.4427) | Acc_1: (83.46%) (5448/6528)\n",
      "Epoch: 35 | Batch_idx: 60 |  Loss_1: (0.4369) | Acc_1: (83.66%) (6532/7808)\n",
      "Epoch: 35 | Batch_idx: 70 |  Loss_1: (0.4395) | Acc_1: (83.53%) (7591/9088)\n",
      "Epoch: 35 | Batch_idx: 80 |  Loss_1: (0.4436) | Acc_1: (83.42%) (8649/10368)\n",
      "Epoch: 35 | Batch_idx: 90 |  Loss_1: (0.4447) | Acc_1: (83.37%) (9711/11648)\n",
      "Epoch: 35 | Batch_idx: 100 |  Loss_1: (0.4452) | Acc_1: (83.34%) (10774/12928)\n",
      "Epoch: 35 | Batch_idx: 110 |  Loss_1: (0.4436) | Acc_1: (83.43%) (11854/14208)\n",
      "Epoch: 35 | Batch_idx: 120 |  Loss_1: (0.4428) | Acc_1: (83.45%) (12925/15488)\n",
      "Epoch: 35 | Batch_idx: 130 |  Loss_1: (0.4424) | Acc_1: (83.44%) (13991/16768)\n",
      "Epoch: 35 | Batch_idx: 140 |  Loss_1: (0.4414) | Acc_1: (83.52%) (15073/18048)\n",
      "Epoch: 35 | Batch_idx: 150 |  Loss_1: (0.4422) | Acc_1: (83.47%) (16133/19328)\n",
      "Epoch: 35 | Batch_idx: 160 |  Loss_1: (0.4437) | Acc_1: (83.39%) (17184/20608)\n",
      "Epoch: 35 | Batch_idx: 170 |  Loss_1: (0.4425) | Acc_1: (83.43%) (18262/21888)\n",
      "Epoch: 35 | Batch_idx: 180 |  Loss_1: (0.4416) | Acc_1: (83.48%) (19340/23168)\n",
      "Epoch: 35 | Batch_idx: 190 |  Loss_1: (0.4437) | Acc_1: (83.40%) (20389/24448)\n",
      "Epoch: 35 | Batch_idx: 200 |  Loss_1: (0.4399) | Acc_1: (83.55%) (21495/25728)\n",
      "Epoch: 35 | Batch_idx: 210 |  Loss_1: (0.4403) | Acc_1: (83.54%) (22562/27008)\n",
      "Epoch: 35 | Batch_idx: 220 |  Loss_1: (0.4405) | Acc_1: (83.55%) (23636/28288)\n",
      "Epoch: 35 | Batch_idx: 230 |  Loss_1: (0.4403) | Acc_1: (83.58%) (24712/29568)\n",
      "Epoch: 35 | Batch_idx: 240 |  Loss_1: (0.4390) | Acc_1: (83.66%) (25807/30848)\n",
      "Epoch: 35 | Batch_idx: 250 |  Loss_1: (0.4397) | Acc_1: (83.62%) (26866/32128)\n",
      "Epoch: 35 | Batch_idx: 260 |  Loss_1: (0.4389) | Acc_1: (83.65%) (27945/33408)\n",
      "Epoch: 35 | Batch_idx: 270 |  Loss_1: (0.4405) | Acc_1: (83.59%) (28996/34688)\n",
      "Epoch: 35 | Batch_idx: 280 |  Loss_1: (0.4406) | Acc_1: (83.60%) (30071/35968)\n",
      "Epoch: 35 | Batch_idx: 290 |  Loss_1: (0.4398) | Acc_1: (83.65%) (31157/37248)\n",
      "Epoch: 35 | Batch_idx: 300 |  Loss_1: (0.4395) | Acc_1: (83.67%) (32235/38528)\n",
      "Epoch: 35 | Batch_idx: 310 |  Loss_1: (0.4384) | Acc_1: (83.70%) (33318/39808)\n",
      "Epoch: 35 | Batch_idx: 320 |  Loss_1: (0.4381) | Acc_1: (83.72%) (34400/41088)\n",
      "Epoch: 35 | Batch_idx: 330 |  Loss_1: (0.4378) | Acc_1: (83.73%) (35475/42368)\n",
      "Epoch: 35 | Batch_idx: 340 |  Loss_1: (0.4381) | Acc_1: (83.72%) (36542/43648)\n",
      "Epoch: 35 | Batch_idx: 350 |  Loss_1: (0.4384) | Acc_1: (83.71%) (37610/44928)\n",
      "Epoch: 35 | Batch_idx: 360 |  Loss_1: (0.4388) | Acc_1: (83.70%) (38676/46208)\n",
      "Epoch: 35 | Batch_idx: 370 |  Loss_1: (0.4396) | Acc_1: (83.68%) (39736/47488)\n",
      "Epoch: 35 | Batch_idx: 380 |  Loss_1: (0.4389) | Acc_1: (83.69%) (40815/48768)\n",
      "Epoch: 35 | Batch_idx: 390 |  Loss_1: (0.4404) | Acc_1: (83.63%) (41815/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4354) | Acc: (88.01%) (8801/10000)\n",
      "Epoch: 36 | Batch_idx: 0 |  Loss_1: (0.3428) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 36 | Batch_idx: 10 |  Loss_1: (0.4110) | Acc_1: (84.80%) (1194/1408)\n",
      "Epoch: 36 | Batch_idx: 20 |  Loss_1: (0.4265) | Acc_1: (84.41%) (2269/2688)\n",
      "Epoch: 36 | Batch_idx: 30 |  Loss_1: (0.4381) | Acc_1: (83.95%) (3331/3968)\n",
      "Epoch: 36 | Batch_idx: 40 |  Loss_1: (0.4394) | Acc_1: (83.78%) (4397/5248)\n",
      "Epoch: 36 | Batch_idx: 50 |  Loss_1: (0.4449) | Acc_1: (83.69%) (5463/6528)\n",
      "Epoch: 36 | Batch_idx: 60 |  Loss_1: (0.4488) | Acc_1: (83.75%) (6539/7808)\n",
      "Epoch: 36 | Batch_idx: 70 |  Loss_1: (0.4432) | Acc_1: (83.91%) (7626/9088)\n",
      "Epoch: 36 | Batch_idx: 80 |  Loss_1: (0.4426) | Acc_1: (83.84%) (8693/10368)\n",
      "Epoch: 36 | Batch_idx: 90 |  Loss_1: (0.4430) | Acc_1: (83.78%) (9759/11648)\n",
      "Epoch: 36 | Batch_idx: 100 |  Loss_1: (0.4440) | Acc_1: (83.70%) (10821/12928)\n",
      "Epoch: 36 | Batch_idx: 110 |  Loss_1: (0.4418) | Acc_1: (83.84%) (11912/14208)\n",
      "Epoch: 36 | Batch_idx: 120 |  Loss_1: (0.4427) | Acc_1: (83.78%) (12976/15488)\n",
      "Epoch: 36 | Batch_idx: 130 |  Loss_1: (0.4404) | Acc_1: (83.84%) (14058/16768)\n",
      "Epoch: 36 | Batch_idx: 140 |  Loss_1: (0.4405) | Acc_1: (83.79%) (15122/18048)\n",
      "Epoch: 36 | Batch_idx: 150 |  Loss_1: (0.4409) | Acc_1: (83.78%) (16193/19328)\n",
      "Epoch: 36 | Batch_idx: 160 |  Loss_1: (0.4420) | Acc_1: (83.78%) (17265/20608)\n",
      "Epoch: 36 | Batch_idx: 170 |  Loss_1: (0.4434) | Acc_1: (83.70%) (18321/21888)\n",
      "Epoch: 36 | Batch_idx: 180 |  Loss_1: (0.4427) | Acc_1: (83.73%) (19399/23168)\n",
      "Epoch: 36 | Batch_idx: 190 |  Loss_1: (0.4428) | Acc_1: (83.75%) (20474/24448)\n",
      "Epoch: 36 | Batch_idx: 200 |  Loss_1: (0.4416) | Acc_1: (83.83%) (21569/25728)\n",
      "Epoch: 36 | Batch_idx: 210 |  Loss_1: (0.4422) | Acc_1: (83.79%) (22630/27008)\n",
      "Epoch: 36 | Batch_idx: 220 |  Loss_1: (0.4427) | Acc_1: (83.75%) (23690/28288)\n",
      "Epoch: 36 | Batch_idx: 230 |  Loss_1: (0.4422) | Acc_1: (83.76%) (24765/29568)\n",
      "Epoch: 36 | Batch_idx: 240 |  Loss_1: (0.4424) | Acc_1: (83.74%) (25833/30848)\n",
      "Epoch: 36 | Batch_idx: 250 |  Loss_1: (0.4407) | Acc_1: (83.80%) (26924/32128)\n",
      "Epoch: 36 | Batch_idx: 260 |  Loss_1: (0.4418) | Acc_1: (83.77%) (27985/33408)\n",
      "Epoch: 36 | Batch_idx: 270 |  Loss_1: (0.4400) | Acc_1: (83.85%) (29085/34688)\n",
      "Epoch: 36 | Batch_idx: 280 |  Loss_1: (0.4417) | Acc_1: (83.80%) (30141/35968)\n",
      "Epoch: 36 | Batch_idx: 290 |  Loss_1: (0.4426) | Acc_1: (83.77%) (31204/37248)\n",
      "Epoch: 36 | Batch_idx: 300 |  Loss_1: (0.4421) | Acc_1: (83.77%) (32276/38528)\n",
      "Epoch: 36 | Batch_idx: 310 |  Loss_1: (0.4416) | Acc_1: (83.78%) (33352/39808)\n",
      "Epoch: 36 | Batch_idx: 320 |  Loss_1: (0.4401) | Acc_1: (83.84%) (34447/41088)\n",
      "Epoch: 36 | Batch_idx: 330 |  Loss_1: (0.4394) | Acc_1: (83.87%) (35535/42368)\n",
      "Epoch: 36 | Batch_idx: 340 |  Loss_1: (0.4391) | Acc_1: (83.88%) (36612/43648)\n",
      "Epoch: 36 | Batch_idx: 350 |  Loss_1: (0.4393) | Acc_1: (83.88%) (37684/44928)\n",
      "Epoch: 36 | Batch_idx: 360 |  Loss_1: (0.4390) | Acc_1: (83.89%) (38765/46208)\n",
      "Epoch: 36 | Batch_idx: 370 |  Loss_1: (0.4388) | Acc_1: (83.89%) (39836/47488)\n",
      "Epoch: 36 | Batch_idx: 380 |  Loss_1: (0.4386) | Acc_1: (83.89%) (40912/48768)\n",
      "Epoch: 36 | Batch_idx: 390 |  Loss_1: (0.4394) | Acc_1: (83.87%) (41937/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4343) | Acc: (88.19%) (8819/10000)\n",
      "Epoch: 37 | Batch_idx: 0 |  Loss_1: (0.3864) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 37 | Batch_idx: 10 |  Loss_1: (0.3965) | Acc_1: (86.08%) (1212/1408)\n",
      "Epoch: 37 | Batch_idx: 20 |  Loss_1: (0.4027) | Acc_1: (85.68%) (2303/2688)\n",
      "Epoch: 37 | Batch_idx: 30 |  Loss_1: (0.4110) | Acc_1: (85.08%) (3376/3968)\n",
      "Epoch: 37 | Batch_idx: 40 |  Loss_1: (0.4143) | Acc_1: (84.81%) (4451/5248)\n",
      "Epoch: 37 | Batch_idx: 50 |  Loss_1: (0.4170) | Acc_1: (84.73%) (5531/6528)\n",
      "Epoch: 37 | Batch_idx: 60 |  Loss_1: (0.4185) | Acc_1: (84.62%) (6607/7808)\n",
      "Epoch: 37 | Batch_idx: 70 |  Loss_1: (0.4239) | Acc_1: (84.41%) (7671/9088)\n",
      "Epoch: 37 | Batch_idx: 80 |  Loss_1: (0.4245) | Acc_1: (84.34%) (8744/10368)\n",
      "Epoch: 37 | Batch_idx: 90 |  Loss_1: (0.4268) | Acc_1: (84.23%) (9811/11648)\n",
      "Epoch: 37 | Batch_idx: 100 |  Loss_1: (0.4264) | Acc_1: (84.20%) (10886/12928)\n",
      "Epoch: 37 | Batch_idx: 110 |  Loss_1: (0.4264) | Acc_1: (84.23%) (11968/14208)\n",
      "Epoch: 37 | Batch_idx: 120 |  Loss_1: (0.4250) | Acc_1: (84.25%) (13049/15488)\n",
      "Epoch: 37 | Batch_idx: 130 |  Loss_1: (0.4233) | Acc_1: (84.26%) (14129/16768)\n",
      "Epoch: 37 | Batch_idx: 140 |  Loss_1: (0.4261) | Acc_1: (84.11%) (15180/18048)\n",
      "Epoch: 37 | Batch_idx: 150 |  Loss_1: (0.4274) | Acc_1: (84.05%) (16246/19328)\n",
      "Epoch: 37 | Batch_idx: 160 |  Loss_1: (0.4272) | Acc_1: (84.11%) (17334/20608)\n",
      "Epoch: 37 | Batch_idx: 170 |  Loss_1: (0.4273) | Acc_1: (84.14%) (18417/21888)\n",
      "Epoch: 37 | Batch_idx: 180 |  Loss_1: (0.4276) | Acc_1: (84.16%) (19499/23168)\n",
      "Epoch: 37 | Batch_idx: 190 |  Loss_1: (0.4310) | Acc_1: (84.05%) (20549/24448)\n",
      "Epoch: 37 | Batch_idx: 200 |  Loss_1: (0.4316) | Acc_1: (84.01%) (21613/25728)\n",
      "Epoch: 37 | Batch_idx: 210 |  Loss_1: (0.4315) | Acc_1: (84.02%) (22691/27008)\n",
      "Epoch: 37 | Batch_idx: 220 |  Loss_1: (0.4317) | Acc_1: (84.00%) (23761/28288)\n",
      "Epoch: 37 | Batch_idx: 230 |  Loss_1: (0.4322) | Acc_1: (83.96%) (24824/29568)\n",
      "Epoch: 37 | Batch_idx: 240 |  Loss_1: (0.4333) | Acc_1: (83.93%) (25891/30848)\n",
      "Epoch: 37 | Batch_idx: 250 |  Loss_1: (0.4334) | Acc_1: (83.96%) (26975/32128)\n",
      "Epoch: 37 | Batch_idx: 260 |  Loss_1: (0.4344) | Acc_1: (83.93%) (28039/33408)\n",
      "Epoch: 37 | Batch_idx: 270 |  Loss_1: (0.4351) | Acc_1: (83.91%) (29105/34688)\n",
      "Epoch: 37 | Batch_idx: 280 |  Loss_1: (0.4345) | Acc_1: (83.94%) (30190/35968)\n",
      "Epoch: 37 | Batch_idx: 290 |  Loss_1: (0.4340) | Acc_1: (83.95%) (31268/37248)\n",
      "Epoch: 37 | Batch_idx: 300 |  Loss_1: (0.4335) | Acc_1: (83.96%) (32349/38528)\n",
      "Epoch: 37 | Batch_idx: 310 |  Loss_1: (0.4337) | Acc_1: (83.94%) (33415/39808)\n",
      "Epoch: 37 | Batch_idx: 320 |  Loss_1: (0.4353) | Acc_1: (83.88%) (34463/41088)\n",
      "Epoch: 37 | Batch_idx: 330 |  Loss_1: (0.4346) | Acc_1: (83.90%) (35545/42368)\n",
      "Epoch: 37 | Batch_idx: 340 |  Loss_1: (0.4356) | Acc_1: (83.84%) (36596/43648)\n",
      "Epoch: 37 | Batch_idx: 350 |  Loss_1: (0.4356) | Acc_1: (83.84%) (37667/44928)\n",
      "Epoch: 37 | Batch_idx: 360 |  Loss_1: (0.4351) | Acc_1: (83.88%) (38757/46208)\n",
      "Epoch: 37 | Batch_idx: 370 |  Loss_1: (0.4358) | Acc_1: (83.84%) (39813/47488)\n",
      "Epoch: 37 | Batch_idx: 380 |  Loss_1: (0.4376) | Acc_1: (83.78%) (40858/48768)\n",
      "Epoch: 37 | Batch_idx: 390 |  Loss_1: (0.4392) | Acc_1: (83.74%) (41870/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4476) | Acc: (87.56%) (8756/10000)\n",
      "Epoch: 38 | Batch_idx: 0 |  Loss_1: (0.3138) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 38 | Batch_idx: 10 |  Loss_1: (0.4309) | Acc_1: (83.52%) (1176/1408)\n",
      "Epoch: 38 | Batch_idx: 20 |  Loss_1: (0.4005) | Acc_1: (85.04%) (2286/2688)\n",
      "Epoch: 38 | Batch_idx: 30 |  Loss_1: (0.4064) | Acc_1: (85.01%) (3373/3968)\n",
      "Epoch: 38 | Batch_idx: 40 |  Loss_1: (0.4059) | Acc_1: (85.00%) (4461/5248)\n",
      "Epoch: 38 | Batch_idx: 50 |  Loss_1: (0.4090) | Acc_1: (84.88%) (5541/6528)\n",
      "Epoch: 38 | Batch_idx: 60 |  Loss_1: (0.4226) | Acc_1: (84.54%) (6601/7808)\n",
      "Epoch: 38 | Batch_idx: 70 |  Loss_1: (0.4306) | Acc_1: (84.20%) (7652/9088)\n",
      "Epoch: 38 | Batch_idx: 80 |  Loss_1: (0.4321) | Acc_1: (84.03%) (8712/10368)\n",
      "Epoch: 38 | Batch_idx: 90 |  Loss_1: (0.4322) | Acc_1: (83.99%) (9783/11648)\n",
      "Epoch: 38 | Batch_idx: 100 |  Loss_1: (0.4282) | Acc_1: (84.20%) (10886/12928)\n",
      "Epoch: 38 | Batch_idx: 110 |  Loss_1: (0.4299) | Acc_1: (84.14%) (11955/14208)\n",
      "Epoch: 38 | Batch_idx: 120 |  Loss_1: (0.4296) | Acc_1: (84.17%) (13037/15488)\n",
      "Epoch: 38 | Batch_idx: 130 |  Loss_1: (0.4279) | Acc_1: (84.24%) (14125/16768)\n",
      "Epoch: 38 | Batch_idx: 140 |  Loss_1: (0.4261) | Acc_1: (84.29%) (15213/18048)\n",
      "Epoch: 38 | Batch_idx: 150 |  Loss_1: (0.4240) | Acc_1: (84.38%) (16308/19328)\n",
      "Epoch: 38 | Batch_idx: 160 |  Loss_1: (0.4216) | Acc_1: (84.45%) (17403/20608)\n",
      "Epoch: 38 | Batch_idx: 170 |  Loss_1: (0.4209) | Acc_1: (84.48%) (18491/21888)\n",
      "Epoch: 38 | Batch_idx: 180 |  Loss_1: (0.4222) | Acc_1: (84.44%) (19563/23168)\n",
      "Epoch: 38 | Batch_idx: 190 |  Loss_1: (0.4234) | Acc_1: (84.38%) (20630/24448)\n",
      "Epoch: 38 | Batch_idx: 200 |  Loss_1: (0.4233) | Acc_1: (84.40%) (21714/25728)\n",
      "Epoch: 38 | Batch_idx: 210 |  Loss_1: (0.4220) | Acc_1: (84.46%) (22812/27008)\n",
      "Epoch: 38 | Batch_idx: 220 |  Loss_1: (0.4219) | Acc_1: (84.44%) (23887/28288)\n",
      "Epoch: 38 | Batch_idx: 230 |  Loss_1: (0.4226) | Acc_1: (84.39%) (24953/29568)\n",
      "Epoch: 38 | Batch_idx: 240 |  Loss_1: (0.4213) | Acc_1: (84.43%) (26045/30848)\n",
      "Epoch: 38 | Batch_idx: 250 |  Loss_1: (0.4200) | Acc_1: (84.46%) (27136/32128)\n",
      "Epoch: 38 | Batch_idx: 260 |  Loss_1: (0.4214) | Acc_1: (84.43%) (28206/33408)\n",
      "Epoch: 38 | Batch_idx: 270 |  Loss_1: (0.4202) | Acc_1: (84.48%) (29306/34688)\n",
      "Epoch: 38 | Batch_idx: 280 |  Loss_1: (0.4204) | Acc_1: (84.49%) (30389/35968)\n",
      "Epoch: 38 | Batch_idx: 290 |  Loss_1: (0.4208) | Acc_1: (84.49%) (31470/37248)\n",
      "Epoch: 38 | Batch_idx: 300 |  Loss_1: (0.4223) | Acc_1: (84.43%) (32530/38528)\n",
      "Epoch: 38 | Batch_idx: 310 |  Loss_1: (0.4224) | Acc_1: (84.43%) (33610/39808)\n",
      "Epoch: 38 | Batch_idx: 320 |  Loss_1: (0.4230) | Acc_1: (84.41%) (34682/41088)\n",
      "Epoch: 38 | Batch_idx: 330 |  Loss_1: (0.4240) | Acc_1: (84.38%) (35748/42368)\n",
      "Epoch: 38 | Batch_idx: 340 |  Loss_1: (0.4247) | Acc_1: (84.36%) (36822/43648)\n",
      "Epoch: 38 | Batch_idx: 350 |  Loss_1: (0.4247) | Acc_1: (84.36%) (37901/44928)\n",
      "Epoch: 38 | Batch_idx: 360 |  Loss_1: (0.4243) | Acc_1: (84.38%) (38988/46208)\n",
      "Epoch: 38 | Batch_idx: 370 |  Loss_1: (0.4252) | Acc_1: (84.35%) (40058/47488)\n",
      "Epoch: 38 | Batch_idx: 380 |  Loss_1: (0.4255) | Acc_1: (84.33%) (41126/48768)\n",
      "Epoch: 38 | Batch_idx: 390 |  Loss_1: (0.4256) | Acc_1: (84.32%) (42160/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4516) | Acc: (87.77%) (8777/10000)\n",
      "Epoch: 39 | Batch_idx: 0 |  Loss_1: (0.4675) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 39 | Batch_idx: 10 |  Loss_1: (0.4302) | Acc_1: (83.24%) (1172/1408)\n",
      "Epoch: 39 | Batch_idx: 20 |  Loss_1: (0.4331) | Acc_1: (83.56%) (2246/2688)\n",
      "Epoch: 39 | Batch_idx: 30 |  Loss_1: (0.4284) | Acc_1: (83.95%) (3331/3968)\n",
      "Epoch: 39 | Batch_idx: 40 |  Loss_1: (0.4298) | Acc_1: (83.86%) (4401/5248)\n",
      "Epoch: 39 | Batch_idx: 50 |  Loss_1: (0.4179) | Acc_1: (84.38%) (5508/6528)\n",
      "Epoch: 39 | Batch_idx: 60 |  Loss_1: (0.4170) | Acc_1: (84.41%) (6591/7808)\n",
      "Epoch: 39 | Batch_idx: 70 |  Loss_1: (0.4153) | Acc_1: (84.53%) (7682/9088)\n",
      "Epoch: 39 | Batch_idx: 80 |  Loss_1: (0.4183) | Acc_1: (84.46%) (8757/10368)\n",
      "Epoch: 39 | Batch_idx: 90 |  Loss_1: (0.4195) | Acc_1: (84.38%) (9828/11648)\n",
      "Epoch: 39 | Batch_idx: 100 |  Loss_1: (0.4205) | Acc_1: (84.34%) (10904/12928)\n",
      "Epoch: 39 | Batch_idx: 110 |  Loss_1: (0.4190) | Acc_1: (84.42%) (11994/14208)\n",
      "Epoch: 39 | Batch_idx: 120 |  Loss_1: (0.4159) | Acc_1: (84.52%) (13091/15488)\n",
      "Epoch: 39 | Batch_idx: 130 |  Loss_1: (0.4148) | Acc_1: (84.52%) (14173/16768)\n",
      "Epoch: 39 | Batch_idx: 140 |  Loss_1: (0.4193) | Acc_1: (84.38%) (15228/18048)\n",
      "Epoch: 39 | Batch_idx: 150 |  Loss_1: (0.4203) | Acc_1: (84.33%) (16299/19328)\n",
      "Epoch: 39 | Batch_idx: 160 |  Loss_1: (0.4185) | Acc_1: (84.42%) (17397/20608)\n",
      "Epoch: 39 | Batch_idx: 170 |  Loss_1: (0.4202) | Acc_1: (84.38%) (18469/21888)\n",
      "Epoch: 39 | Batch_idx: 180 |  Loss_1: (0.4231) | Acc_1: (84.30%) (19530/23168)\n",
      "Epoch: 39 | Batch_idx: 190 |  Loss_1: (0.4231) | Acc_1: (84.31%) (20612/24448)\n",
      "Epoch: 39 | Batch_idx: 200 |  Loss_1: (0.4230) | Acc_1: (84.32%) (21695/25728)\n",
      "Epoch: 39 | Batch_idx: 210 |  Loss_1: (0.4234) | Acc_1: (84.35%) (22782/27008)\n",
      "Epoch: 39 | Batch_idx: 220 |  Loss_1: (0.4260) | Acc_1: (84.28%) (23841/28288)\n",
      "Epoch: 39 | Batch_idx: 230 |  Loss_1: (0.4233) | Acc_1: (84.38%) (24948/29568)\n",
      "Epoch: 39 | Batch_idx: 240 |  Loss_1: (0.4245) | Acc_1: (84.34%) (26016/30848)\n",
      "Epoch: 39 | Batch_idx: 250 |  Loss_1: (0.4258) | Acc_1: (84.27%) (27074/32128)\n",
      "Epoch: 39 | Batch_idx: 260 |  Loss_1: (0.4261) | Acc_1: (84.26%) (28150/33408)\n",
      "Epoch: 39 | Batch_idx: 270 |  Loss_1: (0.4253) | Acc_1: (84.27%) (29233/34688)\n",
      "Epoch: 39 | Batch_idx: 280 |  Loss_1: (0.4250) | Acc_1: (84.29%) (30316/35968)\n",
      "Epoch: 39 | Batch_idx: 290 |  Loss_1: (0.4248) | Acc_1: (84.29%) (31398/37248)\n",
      "Epoch: 39 | Batch_idx: 300 |  Loss_1: (0.4261) | Acc_1: (84.26%) (32463/38528)\n",
      "Epoch: 39 | Batch_idx: 310 |  Loss_1: (0.4269) | Acc_1: (84.21%) (33524/39808)\n",
      "Epoch: 39 | Batch_idx: 320 |  Loss_1: (0.4271) | Acc_1: (84.21%) (34601/41088)\n",
      "Epoch: 39 | Batch_idx: 330 |  Loss_1: (0.4263) | Acc_1: (84.22%) (35684/42368)\n",
      "Epoch: 39 | Batch_idx: 340 |  Loss_1: (0.4268) | Acc_1: (84.18%) (36742/43648)\n",
      "Epoch: 39 | Batch_idx: 350 |  Loss_1: (0.4275) | Acc_1: (84.15%) (37805/44928)\n",
      "Epoch: 39 | Batch_idx: 360 |  Loss_1: (0.4279) | Acc_1: (84.14%) (38878/46208)\n",
      "Epoch: 39 | Batch_idx: 370 |  Loss_1: (0.4274) | Acc_1: (84.18%) (39974/47488)\n",
      "Epoch: 39 | Batch_idx: 380 |  Loss_1: (0.4273) | Acc_1: (84.17%) (41050/48768)\n",
      "Epoch: 39 | Batch_idx: 390 |  Loss_1: (0.4261) | Acc_1: (84.23%) (42116/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4617) | Acc: (88.47%) (8847/10000)\n",
      "Epoch: 40 | Batch_idx: 0 |  Loss_1: (0.3113) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 40 | Batch_idx: 10 |  Loss_1: (0.3697) | Acc_1: (86.36%) (1216/1408)\n",
      "Epoch: 40 | Batch_idx: 20 |  Loss_1: (0.3957) | Acc_1: (85.19%) (2290/2688)\n",
      "Epoch: 40 | Batch_idx: 30 |  Loss_1: (0.4163) | Acc_1: (84.58%) (3356/3968)\n",
      "Epoch: 40 | Batch_idx: 40 |  Loss_1: (0.4116) | Acc_1: (84.72%) (4446/5248)\n",
      "Epoch: 40 | Batch_idx: 50 |  Loss_1: (0.4197) | Acc_1: (84.47%) (5514/6528)\n",
      "Epoch: 40 | Batch_idx: 60 |  Loss_1: (0.4243) | Acc_1: (84.39%) (6589/7808)\n",
      "Epoch: 40 | Batch_idx: 70 |  Loss_1: (0.4255) | Acc_1: (84.29%) (7660/9088)\n",
      "Epoch: 40 | Batch_idx: 80 |  Loss_1: (0.4227) | Acc_1: (84.33%) (8743/10368)\n",
      "Epoch: 40 | Batch_idx: 90 |  Loss_1: (0.4234) | Acc_1: (84.31%) (9821/11648)\n",
      "Epoch: 40 | Batch_idx: 100 |  Loss_1: (0.4209) | Acc_1: (84.45%) (10918/12928)\n",
      "Epoch: 40 | Batch_idx: 110 |  Loss_1: (0.4203) | Acc_1: (84.45%) (11998/14208)\n",
      "Epoch: 40 | Batch_idx: 120 |  Loss_1: (0.4244) | Acc_1: (84.29%) (13055/15488)\n",
      "Epoch: 40 | Batch_idx: 130 |  Loss_1: (0.4265) | Acc_1: (84.23%) (14124/16768)\n",
      "Epoch: 40 | Batch_idx: 140 |  Loss_1: (0.4259) | Acc_1: (84.21%) (15198/18048)\n",
      "Epoch: 40 | Batch_idx: 150 |  Loss_1: (0.4263) | Acc_1: (84.22%) (16278/19328)\n",
      "Epoch: 40 | Batch_idx: 160 |  Loss_1: (0.4242) | Acc_1: (84.30%) (17373/20608)\n",
      "Epoch: 40 | Batch_idx: 170 |  Loss_1: (0.4249) | Acc_1: (84.23%) (18437/21888)\n",
      "Epoch: 40 | Batch_idx: 180 |  Loss_1: (0.4259) | Acc_1: (84.21%) (19510/23168)\n",
      "Epoch: 40 | Batch_idx: 190 |  Loss_1: (0.4262) | Acc_1: (84.20%) (20584/24448)\n",
      "Epoch: 40 | Batch_idx: 200 |  Loss_1: (0.4279) | Acc_1: (84.14%) (21647/25728)\n",
      "Epoch: 40 | Batch_idx: 210 |  Loss_1: (0.4283) | Acc_1: (84.11%) (22716/27008)\n",
      "Epoch: 40 | Batch_idx: 220 |  Loss_1: (0.4282) | Acc_1: (84.10%) (23791/28288)\n",
      "Epoch: 40 | Batch_idx: 230 |  Loss_1: (0.4272) | Acc_1: (84.14%) (24878/29568)\n",
      "Epoch: 40 | Batch_idx: 240 |  Loss_1: (0.4279) | Acc_1: (84.14%) (25955/30848)\n",
      "Epoch: 40 | Batch_idx: 250 |  Loss_1: (0.4274) | Acc_1: (84.14%) (27034/32128)\n",
      "Epoch: 40 | Batch_idx: 260 |  Loss_1: (0.4269) | Acc_1: (84.16%) (28115/33408)\n",
      "Epoch: 40 | Batch_idx: 270 |  Loss_1: (0.4280) | Acc_1: (84.13%) (29183/34688)\n",
      "Epoch: 40 | Batch_idx: 280 |  Loss_1: (0.4295) | Acc_1: (84.06%) (30234/35968)\n",
      "Epoch: 40 | Batch_idx: 290 |  Loss_1: (0.4280) | Acc_1: (84.11%) (31329/37248)\n",
      "Epoch: 40 | Batch_idx: 300 |  Loss_1: (0.4292) | Acc_1: (84.06%) (32386/38528)\n",
      "Epoch: 40 | Batch_idx: 310 |  Loss_1: (0.4291) | Acc_1: (84.06%) (33463/39808)\n",
      "Epoch: 40 | Batch_idx: 320 |  Loss_1: (0.4288) | Acc_1: (84.09%) (34551/41088)\n",
      "Epoch: 40 | Batch_idx: 330 |  Loss_1: (0.4281) | Acc_1: (84.10%) (35632/42368)\n",
      "Epoch: 40 | Batch_idx: 340 |  Loss_1: (0.4283) | Acc_1: (84.09%) (36703/43648)\n",
      "Epoch: 40 | Batch_idx: 350 |  Loss_1: (0.4281) | Acc_1: (84.10%) (37783/44928)\n",
      "Epoch: 40 | Batch_idx: 360 |  Loss_1: (0.4294) | Acc_1: (84.05%) (38836/46208)\n",
      "Epoch: 40 | Batch_idx: 370 |  Loss_1: (0.4297) | Acc_1: (84.06%) (39917/47488)\n",
      "Epoch: 40 | Batch_idx: 380 |  Loss_1: (0.4297) | Acc_1: (84.07%) (40997/48768)\n",
      "Epoch: 40 | Batch_idx: 390 |  Loss_1: (0.4292) | Acc_1: (84.07%) (42034/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4689) | Acc: (88.27%) (8827/10000)\n",
      "Epoch: 41 | Batch_idx: 0 |  Loss_1: (0.4769) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 41 | Batch_idx: 10 |  Loss_1: (0.4337) | Acc_1: (84.30%) (1187/1408)\n",
      "Epoch: 41 | Batch_idx: 20 |  Loss_1: (0.4384) | Acc_1: (84.08%) (2260/2688)\n",
      "Epoch: 41 | Batch_idx: 30 |  Loss_1: (0.4430) | Acc_1: (83.59%) (3317/3968)\n",
      "Epoch: 41 | Batch_idx: 40 |  Loss_1: (0.4323) | Acc_1: (83.96%) (4406/5248)\n",
      "Epoch: 41 | Batch_idx: 50 |  Loss_1: (0.4244) | Acc_1: (84.31%) (5504/6528)\n",
      "Epoch: 41 | Batch_idx: 60 |  Loss_1: (0.4195) | Acc_1: (84.53%) (6600/7808)\n",
      "Epoch: 41 | Batch_idx: 70 |  Loss_1: (0.4194) | Acc_1: (84.51%) (7680/9088)\n",
      "Epoch: 41 | Batch_idx: 80 |  Loss_1: (0.4243) | Acc_1: (84.24%) (8734/10368)\n",
      "Epoch: 41 | Batch_idx: 90 |  Loss_1: (0.4234) | Acc_1: (84.26%) (9815/11648)\n",
      "Epoch: 41 | Batch_idx: 100 |  Loss_1: (0.4194) | Acc_1: (84.39%) (10910/12928)\n",
      "Epoch: 41 | Batch_idx: 110 |  Loss_1: (0.4193) | Acc_1: (84.42%) (11994/14208)\n",
      "Epoch: 41 | Batch_idx: 120 |  Loss_1: (0.4181) | Acc_1: (84.45%) (13079/15488)\n",
      "Epoch: 41 | Batch_idx: 130 |  Loss_1: (0.4173) | Acc_1: (84.46%) (14162/16768)\n",
      "Epoch: 41 | Batch_idx: 140 |  Loss_1: (0.4214) | Acc_1: (84.30%) (15214/18048)\n",
      "Epoch: 41 | Batch_idx: 150 |  Loss_1: (0.4226) | Acc_1: (84.21%) (16276/19328)\n",
      "Epoch: 41 | Batch_idx: 160 |  Loss_1: (0.4254) | Acc_1: (84.14%) (17340/20608)\n",
      "Epoch: 41 | Batch_idx: 170 |  Loss_1: (0.4246) | Acc_1: (84.21%) (18432/21888)\n",
      "Epoch: 41 | Batch_idx: 180 |  Loss_1: (0.4277) | Acc_1: (84.11%) (19486/23168)\n",
      "Epoch: 41 | Batch_idx: 190 |  Loss_1: (0.4281) | Acc_1: (84.11%) (20562/24448)\n",
      "Epoch: 41 | Batch_idx: 200 |  Loss_1: (0.4268) | Acc_1: (84.16%) (21652/25728)\n",
      "Epoch: 41 | Batch_idx: 210 |  Loss_1: (0.4248) | Acc_1: (84.23%) (22749/27008)\n",
      "Epoch: 41 | Batch_idx: 220 |  Loss_1: (0.4246) | Acc_1: (84.25%) (23832/28288)\n",
      "Epoch: 41 | Batch_idx: 230 |  Loss_1: (0.4245) | Acc_1: (84.23%) (24905/29568)\n",
      "Epoch: 41 | Batch_idx: 240 |  Loss_1: (0.4222) | Acc_1: (84.32%) (26012/30848)\n",
      "Epoch: 41 | Batch_idx: 250 |  Loss_1: (0.4237) | Acc_1: (84.26%) (27071/32128)\n",
      "Epoch: 41 | Batch_idx: 260 |  Loss_1: (0.4238) | Acc_1: (84.25%) (28145/33408)\n",
      "Epoch: 41 | Batch_idx: 270 |  Loss_1: (0.4232) | Acc_1: (84.26%) (29228/34688)\n",
      "Epoch: 41 | Batch_idx: 280 |  Loss_1: (0.4237) | Acc_1: (84.23%) (30297/35968)\n",
      "Epoch: 41 | Batch_idx: 290 |  Loss_1: (0.4245) | Acc_1: (84.20%) (31363/37248)\n",
      "Epoch: 41 | Batch_idx: 300 |  Loss_1: (0.4233) | Acc_1: (84.22%) (32448/38528)\n",
      "Epoch: 41 | Batch_idx: 310 |  Loss_1: (0.4238) | Acc_1: (84.20%) (33519/39808)\n",
      "Epoch: 41 | Batch_idx: 320 |  Loss_1: (0.4241) | Acc_1: (84.19%) (34590/41088)\n",
      "Epoch: 41 | Batch_idx: 330 |  Loss_1: (0.4232) | Acc_1: (84.22%) (35682/42368)\n",
      "Epoch: 41 | Batch_idx: 340 |  Loss_1: (0.4230) | Acc_1: (84.24%) (36767/43648)\n",
      "Epoch: 41 | Batch_idx: 350 |  Loss_1: (0.4231) | Acc_1: (84.25%) (37853/44928)\n",
      "Epoch: 41 | Batch_idx: 360 |  Loss_1: (0.4233) | Acc_1: (84.25%) (38929/46208)\n",
      "Epoch: 41 | Batch_idx: 370 |  Loss_1: (0.4239) | Acc_1: (84.23%) (39998/47488)\n",
      "Epoch: 41 | Batch_idx: 380 |  Loss_1: (0.4225) | Acc_1: (84.28%) (41103/48768)\n",
      "Epoch: 41 | Batch_idx: 390 |  Loss_1: (0.4243) | Acc_1: (84.22%) (42110/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4731) | Acc: (88.66%) (8866/10000)\n",
      "Epoch: 42 | Batch_idx: 0 |  Loss_1: (0.4768) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 42 | Batch_idx: 10 |  Loss_1: (0.3805) | Acc_1: (86.65%) (1220/1408)\n",
      "Epoch: 42 | Batch_idx: 20 |  Loss_1: (0.4003) | Acc_1: (85.38%) (2295/2688)\n",
      "Epoch: 42 | Batch_idx: 30 |  Loss_1: (0.4115) | Acc_1: (84.93%) (3370/3968)\n",
      "Epoch: 42 | Batch_idx: 40 |  Loss_1: (0.4116) | Acc_1: (84.79%) (4450/5248)\n",
      "Epoch: 42 | Batch_idx: 50 |  Loss_1: (0.4215) | Acc_1: (84.51%) (5517/6528)\n",
      "Epoch: 42 | Batch_idx: 60 |  Loss_1: (0.4223) | Acc_1: (84.46%) (6595/7808)\n",
      "Epoch: 42 | Batch_idx: 70 |  Loss_1: (0.4258) | Acc_1: (84.29%) (7660/9088)\n",
      "Epoch: 42 | Batch_idx: 80 |  Loss_1: (0.4197) | Acc_1: (84.48%) (8759/10368)\n",
      "Epoch: 42 | Batch_idx: 90 |  Loss_1: (0.4196) | Acc_1: (84.51%) (9844/11648)\n",
      "Epoch: 42 | Batch_idx: 100 |  Loss_1: (0.4175) | Acc_1: (84.59%) (10936/12928)\n",
      "Epoch: 42 | Batch_idx: 110 |  Loss_1: (0.4159) | Acc_1: (84.66%) (12028/14208)\n",
      "Epoch: 42 | Batch_idx: 120 |  Loss_1: (0.4150) | Acc_1: (84.68%) (13116/15488)\n",
      "Epoch: 42 | Batch_idx: 130 |  Loss_1: (0.4141) | Acc_1: (84.70%) (14202/16768)\n",
      "Epoch: 42 | Batch_idx: 140 |  Loss_1: (0.4145) | Acc_1: (84.69%) (15284/18048)\n",
      "Epoch: 42 | Batch_idx: 150 |  Loss_1: (0.4162) | Acc_1: (84.61%) (16353/19328)\n",
      "Epoch: 42 | Batch_idx: 160 |  Loss_1: (0.4152) | Acc_1: (84.65%) (17445/20608)\n",
      "Epoch: 42 | Batch_idx: 170 |  Loss_1: (0.4154) | Acc_1: (84.71%) (18541/21888)\n",
      "Epoch: 42 | Batch_idx: 180 |  Loss_1: (0.4162) | Acc_1: (84.69%) (19622/23168)\n",
      "Epoch: 42 | Batch_idx: 190 |  Loss_1: (0.4148) | Acc_1: (84.70%) (20708/24448)\n",
      "Epoch: 42 | Batch_idx: 200 |  Loss_1: (0.4152) | Acc_1: (84.68%) (21787/25728)\n",
      "Epoch: 42 | Batch_idx: 210 |  Loss_1: (0.4125) | Acc_1: (84.80%) (22903/27008)\n",
      "Epoch: 42 | Batch_idx: 220 |  Loss_1: (0.4128) | Acc_1: (84.78%) (23983/28288)\n",
      "Epoch: 42 | Batch_idx: 230 |  Loss_1: (0.4130) | Acc_1: (84.78%) (25067/29568)\n",
      "Epoch: 42 | Batch_idx: 240 |  Loss_1: (0.4126) | Acc_1: (84.79%) (26157/30848)\n",
      "Epoch: 42 | Batch_idx: 250 |  Loss_1: (0.4124) | Acc_1: (84.79%) (27241/32128)\n",
      "Epoch: 42 | Batch_idx: 260 |  Loss_1: (0.4133) | Acc_1: (84.75%) (28312/33408)\n",
      "Epoch: 42 | Batch_idx: 270 |  Loss_1: (0.4139) | Acc_1: (84.72%) (29388/34688)\n",
      "Epoch: 42 | Batch_idx: 280 |  Loss_1: (0.4142) | Acc_1: (84.71%) (30469/35968)\n",
      "Epoch: 42 | Batch_idx: 290 |  Loss_1: (0.4150) | Acc_1: (84.68%) (31543/37248)\n",
      "Epoch: 42 | Batch_idx: 300 |  Loss_1: (0.4161) | Acc_1: (84.66%) (32616/38528)\n",
      "Epoch: 42 | Batch_idx: 310 |  Loss_1: (0.4179) | Acc_1: (84.60%) (33676/39808)\n",
      "Epoch: 42 | Batch_idx: 320 |  Loss_1: (0.4193) | Acc_1: (84.56%) (34742/41088)\n",
      "Epoch: 42 | Batch_idx: 330 |  Loss_1: (0.4180) | Acc_1: (84.61%) (35847/42368)\n",
      "Epoch: 42 | Batch_idx: 340 |  Loss_1: (0.4184) | Acc_1: (84.58%) (36918/43648)\n",
      "Epoch: 42 | Batch_idx: 350 |  Loss_1: (0.4189) | Acc_1: (84.57%) (37995/44928)\n",
      "Epoch: 42 | Batch_idx: 360 |  Loss_1: (0.4182) | Acc_1: (84.59%) (39088/46208)\n",
      "Epoch: 42 | Batch_idx: 370 |  Loss_1: (0.4185) | Acc_1: (84.59%) (40170/47488)\n",
      "Epoch: 42 | Batch_idx: 380 |  Loss_1: (0.4189) | Acc_1: (84.57%) (41241/48768)\n",
      "Epoch: 42 | Batch_idx: 390 |  Loss_1: (0.4196) | Acc_1: (84.53%) (42267/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4779) | Acc: (88.54%) (8854/10000)\n",
      "Epoch: 43 | Batch_idx: 0 |  Loss_1: (0.3953) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 43 | Batch_idx: 10 |  Loss_1: (0.4506) | Acc_1: (83.45%) (1175/1408)\n",
      "Epoch: 43 | Batch_idx: 20 |  Loss_1: (0.4652) | Acc_1: (82.81%) (2226/2688)\n",
      "Epoch: 43 | Batch_idx: 30 |  Loss_1: (0.4582) | Acc_1: (83.34%) (3307/3968)\n",
      "Epoch: 43 | Batch_idx: 40 |  Loss_1: (0.4565) | Acc_1: (83.40%) (4377/5248)\n",
      "Epoch: 43 | Batch_idx: 50 |  Loss_1: (0.4516) | Acc_1: (83.53%) (5453/6528)\n",
      "Epoch: 43 | Batch_idx: 60 |  Loss_1: (0.4442) | Acc_1: (83.71%) (6536/7808)\n",
      "Epoch: 43 | Batch_idx: 70 |  Loss_1: (0.4404) | Acc_1: (83.85%) (7620/9088)\n",
      "Epoch: 43 | Batch_idx: 80 |  Loss_1: (0.4432) | Acc_1: (83.65%) (8673/10368)\n",
      "Epoch: 43 | Batch_idx: 90 |  Loss_1: (0.4448) | Acc_1: (83.66%) (9745/11648)\n",
      "Epoch: 43 | Batch_idx: 100 |  Loss_1: (0.4419) | Acc_1: (83.68%) (10818/12928)\n",
      "Epoch: 43 | Batch_idx: 110 |  Loss_1: (0.4390) | Acc_1: (83.78%) (11904/14208)\n",
      "Epoch: 43 | Batch_idx: 120 |  Loss_1: (0.4415) | Acc_1: (83.72%) (12966/15488)\n",
      "Epoch: 43 | Batch_idx: 130 |  Loss_1: (0.4394) | Acc_1: (83.83%) (14057/16768)\n",
      "Epoch: 43 | Batch_idx: 140 |  Loss_1: (0.4369) | Acc_1: (83.94%) (15149/18048)\n",
      "Epoch: 43 | Batch_idx: 150 |  Loss_1: (0.4348) | Acc_1: (83.96%) (16228/19328)\n",
      "Epoch: 43 | Batch_idx: 160 |  Loss_1: (0.4349) | Acc_1: (83.94%) (17298/20608)\n",
      "Epoch: 43 | Batch_idx: 170 |  Loss_1: (0.4383) | Acc_1: (83.77%) (18336/21888)\n",
      "Epoch: 43 | Batch_idx: 180 |  Loss_1: (0.4343) | Acc_1: (83.92%) (19443/23168)\n",
      "Epoch: 43 | Batch_idx: 190 |  Loss_1: (0.4339) | Acc_1: (83.94%) (20521/24448)\n",
      "Epoch: 43 | Batch_idx: 200 |  Loss_1: (0.4340) | Acc_1: (83.93%) (21593/25728)\n",
      "Epoch: 43 | Batch_idx: 210 |  Loss_1: (0.4339) | Acc_1: (83.94%) (22670/27008)\n",
      "Epoch: 43 | Batch_idx: 220 |  Loss_1: (0.4332) | Acc_1: (83.95%) (23749/28288)\n",
      "Epoch: 43 | Batch_idx: 230 |  Loss_1: (0.4310) | Acc_1: (84.06%) (24855/29568)\n",
      "Epoch: 43 | Batch_idx: 240 |  Loss_1: (0.4301) | Acc_1: (84.07%) (25935/30848)\n",
      "Epoch: 43 | Batch_idx: 250 |  Loss_1: (0.4300) | Acc_1: (84.09%) (27018/32128)\n",
      "Epoch: 43 | Batch_idx: 260 |  Loss_1: (0.4306) | Acc_1: (84.09%) (28093/33408)\n",
      "Epoch: 43 | Batch_idx: 270 |  Loss_1: (0.4311) | Acc_1: (84.09%) (29168/34688)\n",
      "Epoch: 43 | Batch_idx: 280 |  Loss_1: (0.4299) | Acc_1: (84.13%) (30261/35968)\n",
      "Epoch: 43 | Batch_idx: 290 |  Loss_1: (0.4302) | Acc_1: (84.11%) (31330/37248)\n",
      "Epoch: 43 | Batch_idx: 300 |  Loss_1: (0.4295) | Acc_1: (84.14%) (32417/38528)\n",
      "Epoch: 43 | Batch_idx: 310 |  Loss_1: (0.4299) | Acc_1: (84.12%) (33487/39808)\n",
      "Epoch: 43 | Batch_idx: 320 |  Loss_1: (0.4297) | Acc_1: (84.16%) (34579/41088)\n",
      "Epoch: 43 | Batch_idx: 330 |  Loss_1: (0.4302) | Acc_1: (84.15%) (35654/42368)\n",
      "Epoch: 43 | Batch_idx: 340 |  Loss_1: (0.4306) | Acc_1: (84.14%) (36726/43648)\n",
      "Epoch: 43 | Batch_idx: 350 |  Loss_1: (0.4308) | Acc_1: (84.13%) (37799/44928)\n",
      "Epoch: 43 | Batch_idx: 360 |  Loss_1: (0.4304) | Acc_1: (84.16%) (38890/46208)\n",
      "Epoch: 43 | Batch_idx: 370 |  Loss_1: (0.4291) | Acc_1: (84.22%) (39994/47488)\n",
      "Epoch: 43 | Batch_idx: 380 |  Loss_1: (0.4299) | Acc_1: (84.17%) (41049/48768)\n",
      "Epoch: 43 | Batch_idx: 390 |  Loss_1: (0.4305) | Acc_1: (84.14%) (42072/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4566) | Acc: (88.59%) (8859/10000)\n",
      "Epoch: 44 | Batch_idx: 0 |  Loss_1: (0.5304) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 44 | Batch_idx: 10 |  Loss_1: (0.4022) | Acc_1: (85.09%) (1198/1408)\n",
      "Epoch: 44 | Batch_idx: 20 |  Loss_1: (0.3992) | Acc_1: (85.31%) (2293/2688)\n",
      "Epoch: 44 | Batch_idx: 30 |  Loss_1: (0.4037) | Acc_1: (85.03%) (3374/3968)\n",
      "Epoch: 44 | Batch_idx: 40 |  Loss_1: (0.4068) | Acc_1: (85.04%) (4463/5248)\n",
      "Epoch: 44 | Batch_idx: 50 |  Loss_1: (0.4160) | Acc_1: (84.71%) (5530/6528)\n",
      "Epoch: 44 | Batch_idx: 60 |  Loss_1: (0.4177) | Acc_1: (84.57%) (6603/7808)\n",
      "Epoch: 44 | Batch_idx: 70 |  Loss_1: (0.4166) | Acc_1: (84.67%) (7695/9088)\n",
      "Epoch: 44 | Batch_idx: 80 |  Loss_1: (0.4161) | Acc_1: (84.72%) (8784/10368)\n",
      "Epoch: 44 | Batch_idx: 90 |  Loss_1: (0.4195) | Acc_1: (84.56%) (9850/11648)\n",
      "Epoch: 44 | Batch_idx: 100 |  Loss_1: (0.4177) | Acc_1: (84.59%) (10936/12928)\n",
      "Epoch: 44 | Batch_idx: 110 |  Loss_1: (0.4165) | Acc_1: (84.61%) (12022/14208)\n",
      "Epoch: 44 | Batch_idx: 120 |  Loss_1: (0.4161) | Acc_1: (84.63%) (13107/15488)\n",
      "Epoch: 44 | Batch_idx: 130 |  Loss_1: (0.4142) | Acc_1: (84.71%) (14204/16768)\n",
      "Epoch: 44 | Batch_idx: 140 |  Loss_1: (0.4125) | Acc_1: (84.76%) (15298/18048)\n",
      "Epoch: 44 | Batch_idx: 150 |  Loss_1: (0.4135) | Acc_1: (84.72%) (16375/19328)\n",
      "Epoch: 44 | Batch_idx: 160 |  Loss_1: (0.4121) | Acc_1: (84.80%) (17476/20608)\n",
      "Epoch: 44 | Batch_idx: 170 |  Loss_1: (0.4143) | Acc_1: (84.70%) (18540/21888)\n",
      "Epoch: 44 | Batch_idx: 180 |  Loss_1: (0.4145) | Acc_1: (84.69%) (19621/23168)\n",
      "Epoch: 44 | Batch_idx: 190 |  Loss_1: (0.4135) | Acc_1: (84.70%) (20708/24448)\n",
      "Epoch: 44 | Batch_idx: 200 |  Loss_1: (0.4141) | Acc_1: (84.70%) (21791/25728)\n",
      "Epoch: 44 | Batch_idx: 210 |  Loss_1: (0.4143) | Acc_1: (84.72%) (22880/27008)\n",
      "Epoch: 44 | Batch_idx: 220 |  Loss_1: (0.4141) | Acc_1: (84.73%) (23968/28288)\n",
      "Epoch: 44 | Batch_idx: 230 |  Loss_1: (0.4165) | Acc_1: (84.62%) (25020/29568)\n",
      "Epoch: 44 | Batch_idx: 240 |  Loss_1: (0.4170) | Acc_1: (84.61%) (26100/30848)\n",
      "Epoch: 44 | Batch_idx: 250 |  Loss_1: (0.4175) | Acc_1: (84.60%) (27180/32128)\n",
      "Epoch: 44 | Batch_idx: 260 |  Loss_1: (0.4157) | Acc_1: (84.66%) (28284/33408)\n",
      "Epoch: 44 | Batch_idx: 270 |  Loss_1: (0.4166) | Acc_1: (84.65%) (29362/34688)\n",
      "Epoch: 44 | Batch_idx: 280 |  Loss_1: (0.4151) | Acc_1: (84.71%) (30467/35968)\n",
      "Epoch: 44 | Batch_idx: 290 |  Loss_1: (0.4173) | Acc_1: (84.64%) (31527/37248)\n",
      "Epoch: 44 | Batch_idx: 300 |  Loss_1: (0.4179) | Acc_1: (84.61%) (32598/38528)\n",
      "Epoch: 44 | Batch_idx: 310 |  Loss_1: (0.4166) | Acc_1: (84.65%) (33698/39808)\n",
      "Epoch: 44 | Batch_idx: 320 |  Loss_1: (0.4164) | Acc_1: (84.66%) (34785/41088)\n",
      "Epoch: 44 | Batch_idx: 330 |  Loss_1: (0.4164) | Acc_1: (84.66%) (35870/42368)\n",
      "Epoch: 44 | Batch_idx: 340 |  Loss_1: (0.4171) | Acc_1: (84.63%) (36940/43648)\n",
      "Epoch: 44 | Batch_idx: 350 |  Loss_1: (0.4181) | Acc_1: (84.58%) (37998/44928)\n",
      "Epoch: 44 | Batch_idx: 360 |  Loss_1: (0.4196) | Acc_1: (84.52%) (39057/46208)\n",
      "Epoch: 44 | Batch_idx: 370 |  Loss_1: (0.4201) | Acc_1: (84.50%) (40129/47488)\n",
      "Epoch: 44 | Batch_idx: 380 |  Loss_1: (0.4199) | Acc_1: (84.52%) (41217/48768)\n",
      "Epoch: 44 | Batch_idx: 390 |  Loss_1: (0.4201) | Acc_1: (84.51%) (42254/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4434) | Acc: (89.17%) (8917/10000)\n",
      "Epoch: 45 | Batch_idx: 0 |  Loss_1: (0.2789) | Acc_1: (88.28%) (113/128)\n",
      "Epoch: 45 | Batch_idx: 10 |  Loss_1: (0.4161) | Acc_1: (84.45%) (1189/1408)\n",
      "Epoch: 45 | Batch_idx: 20 |  Loss_1: (0.4050) | Acc_1: (84.93%) (2283/2688)\n",
      "Epoch: 45 | Batch_idx: 30 |  Loss_1: (0.4017) | Acc_1: (85.13%) (3378/3968)\n",
      "Epoch: 45 | Batch_idx: 40 |  Loss_1: (0.4125) | Acc_1: (84.68%) (4444/5248)\n",
      "Epoch: 45 | Batch_idx: 50 |  Loss_1: (0.4167) | Acc_1: (84.56%) (5520/6528)\n",
      "Epoch: 45 | Batch_idx: 60 |  Loss_1: (0.4134) | Acc_1: (84.72%) (6615/7808)\n",
      "Epoch: 45 | Batch_idx: 70 |  Loss_1: (0.4155) | Acc_1: (84.68%) (7696/9088)\n",
      "Epoch: 45 | Batch_idx: 80 |  Loss_1: (0.4153) | Acc_1: (84.63%) (8774/10368)\n",
      "Epoch: 45 | Batch_idx: 90 |  Loss_1: (0.4166) | Acc_1: (84.53%) (9846/11648)\n",
      "Epoch: 45 | Batch_idx: 100 |  Loss_1: (0.4153) | Acc_1: (84.58%) (10934/12928)\n",
      "Epoch: 45 | Batch_idx: 110 |  Loss_1: (0.4120) | Acc_1: (84.71%) (12035/14208)\n",
      "Epoch: 45 | Batch_idx: 120 |  Loss_1: (0.4148) | Acc_1: (84.60%) (13103/15488)\n",
      "Epoch: 45 | Batch_idx: 130 |  Loss_1: (0.4159) | Acc_1: (84.55%) (14178/16768)\n",
      "Epoch: 45 | Batch_idx: 140 |  Loss_1: (0.4156) | Acc_1: (84.60%) (15269/18048)\n",
      "Epoch: 45 | Batch_idx: 150 |  Loss_1: (0.4169) | Acc_1: (84.61%) (16353/19328)\n",
      "Epoch: 45 | Batch_idx: 160 |  Loss_1: (0.4174) | Acc_1: (84.57%) (17428/20608)\n",
      "Epoch: 45 | Batch_idx: 170 |  Loss_1: (0.4203) | Acc_1: (84.45%) (18484/21888)\n",
      "Epoch: 45 | Batch_idx: 180 |  Loss_1: (0.4203) | Acc_1: (84.43%) (19561/23168)\n",
      "Epoch: 45 | Batch_idx: 190 |  Loss_1: (0.4225) | Acc_1: (84.37%) (20627/24448)\n",
      "Epoch: 45 | Batch_idx: 200 |  Loss_1: (0.4251) | Acc_1: (84.29%) (21685/25728)\n",
      "Epoch: 45 | Batch_idx: 210 |  Loss_1: (0.4276) | Acc_1: (84.22%) (22747/27008)\n",
      "Epoch: 45 | Batch_idx: 220 |  Loss_1: (0.4260) | Acc_1: (84.28%) (23841/28288)\n",
      "Epoch: 45 | Batch_idx: 230 |  Loss_1: (0.4265) | Acc_1: (84.27%) (24916/29568)\n",
      "Epoch: 45 | Batch_idx: 240 |  Loss_1: (0.4251) | Acc_1: (84.31%) (26007/30848)\n",
      "Epoch: 45 | Batch_idx: 250 |  Loss_1: (0.4237) | Acc_1: (84.34%) (27097/32128)\n",
      "Epoch: 45 | Batch_idx: 260 |  Loss_1: (0.4237) | Acc_1: (84.33%) (28173/33408)\n",
      "Epoch: 45 | Batch_idx: 270 |  Loss_1: (0.4238) | Acc_1: (84.31%) (29247/34688)\n",
      "Epoch: 45 | Batch_idx: 280 |  Loss_1: (0.4237) | Acc_1: (84.30%) (30322/35968)\n",
      "Epoch: 45 | Batch_idx: 290 |  Loss_1: (0.4234) | Acc_1: (84.30%) (31400/37248)\n",
      "Epoch: 45 | Batch_idx: 300 |  Loss_1: (0.4229) | Acc_1: (84.32%) (32487/38528)\n",
      "Epoch: 45 | Batch_idx: 310 |  Loss_1: (0.4237) | Acc_1: (84.30%) (33558/39808)\n",
      "Epoch: 45 | Batch_idx: 320 |  Loss_1: (0.4232) | Acc_1: (84.33%) (34648/41088)\n",
      "Epoch: 45 | Batch_idx: 330 |  Loss_1: (0.4234) | Acc_1: (84.32%) (35724/42368)\n",
      "Epoch: 45 | Batch_idx: 340 |  Loss_1: (0.4238) | Acc_1: (84.30%) (36795/43648)\n",
      "Epoch: 45 | Batch_idx: 350 |  Loss_1: (0.4226) | Acc_1: (84.35%) (37898/44928)\n",
      "Epoch: 45 | Batch_idx: 360 |  Loss_1: (0.4222) | Acc_1: (84.36%) (38983/46208)\n",
      "Epoch: 45 | Batch_idx: 370 |  Loss_1: (0.4225) | Acc_1: (84.35%) (40057/47488)\n",
      "Epoch: 45 | Batch_idx: 380 |  Loss_1: (0.4216) | Acc_1: (84.39%) (41157/48768)\n",
      "Epoch: 45 | Batch_idx: 390 |  Loss_1: (0.4212) | Acc_1: (84.41%) (42205/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4874) | Acc: (88.84%) (8884/10000)\n",
      "Epoch: 46 | Batch_idx: 0 |  Loss_1: (0.4202) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 46 | Batch_idx: 10 |  Loss_1: (0.4316) | Acc_1: (84.38%) (1188/1408)\n",
      "Epoch: 46 | Batch_idx: 20 |  Loss_1: (0.4291) | Acc_1: (84.15%) (2262/2688)\n",
      "Epoch: 46 | Batch_idx: 30 |  Loss_1: (0.4352) | Acc_1: (84.15%) (3339/3968)\n",
      "Epoch: 46 | Batch_idx: 40 |  Loss_1: (0.4328) | Acc_1: (84.15%) (4416/5248)\n",
      "Epoch: 46 | Batch_idx: 50 |  Loss_1: (0.4310) | Acc_1: (84.30%) (5503/6528)\n",
      "Epoch: 46 | Batch_idx: 60 |  Loss_1: (0.4305) | Acc_1: (84.22%) (6576/7808)\n",
      "Epoch: 46 | Batch_idx: 70 |  Loss_1: (0.4243) | Acc_1: (84.51%) (7680/9088)\n",
      "Epoch: 46 | Batch_idx: 80 |  Loss_1: (0.4243) | Acc_1: (84.46%) (8757/10368)\n",
      "Epoch: 46 | Batch_idx: 90 |  Loss_1: (0.4240) | Acc_1: (84.46%) (9838/11648)\n",
      "Epoch: 46 | Batch_idx: 100 |  Loss_1: (0.4232) | Acc_1: (84.51%) (10926/12928)\n",
      "Epoch: 46 | Batch_idx: 110 |  Loss_1: (0.4233) | Acc_1: (84.49%) (12004/14208)\n",
      "Epoch: 46 | Batch_idx: 120 |  Loss_1: (0.4204) | Acc_1: (84.59%) (13102/15488)\n",
      "Epoch: 46 | Batch_idx: 130 |  Loss_1: (0.4198) | Acc_1: (84.63%) (14190/16768)\n",
      "Epoch: 46 | Batch_idx: 140 |  Loss_1: (0.4190) | Acc_1: (84.64%) (15275/18048)\n",
      "Epoch: 46 | Batch_idx: 150 |  Loss_1: (0.4175) | Acc_1: (84.69%) (16368/19328)\n",
      "Epoch: 46 | Batch_idx: 160 |  Loss_1: (0.4187) | Acc_1: (84.63%) (17440/20608)\n",
      "Epoch: 46 | Batch_idx: 170 |  Loss_1: (0.4180) | Acc_1: (84.66%) (18531/21888)\n",
      "Epoch: 46 | Batch_idx: 180 |  Loss_1: (0.4165) | Acc_1: (84.70%) (19623/23168)\n",
      "Epoch: 46 | Batch_idx: 190 |  Loss_1: (0.4161) | Acc_1: (84.75%) (20719/24448)\n",
      "Epoch: 46 | Batch_idx: 200 |  Loss_1: (0.4158) | Acc_1: (84.77%) (21810/25728)\n",
      "Epoch: 46 | Batch_idx: 210 |  Loss_1: (0.4168) | Acc_1: (84.72%) (22881/27008)\n",
      "Epoch: 46 | Batch_idx: 220 |  Loss_1: (0.4176) | Acc_1: (84.68%) (23955/28288)\n",
      "Epoch: 46 | Batch_idx: 230 |  Loss_1: (0.4180) | Acc_1: (84.65%) (25028/29568)\n",
      "Epoch: 46 | Batch_idx: 240 |  Loss_1: (0.4195) | Acc_1: (84.60%) (26096/30848)\n",
      "Epoch: 46 | Batch_idx: 250 |  Loss_1: (0.4202) | Acc_1: (84.56%) (27169/32128)\n",
      "Epoch: 46 | Batch_idx: 260 |  Loss_1: (0.4212) | Acc_1: (84.52%) (28236/33408)\n",
      "Epoch: 46 | Batch_idx: 270 |  Loss_1: (0.4204) | Acc_1: (84.57%) (29335/34688)\n",
      "Epoch: 46 | Batch_idx: 280 |  Loss_1: (0.4211) | Acc_1: (84.57%) (30419/35968)\n",
      "Epoch: 46 | Batch_idx: 290 |  Loss_1: (0.4208) | Acc_1: (84.58%) (31503/37248)\n",
      "Epoch: 46 | Batch_idx: 300 |  Loss_1: (0.4202) | Acc_1: (84.59%) (32592/38528)\n",
      "Epoch: 46 | Batch_idx: 310 |  Loss_1: (0.4208) | Acc_1: (84.56%) (33663/39808)\n",
      "Epoch: 46 | Batch_idx: 320 |  Loss_1: (0.4208) | Acc_1: (84.57%) (34749/41088)\n",
      "Epoch: 46 | Batch_idx: 330 |  Loss_1: (0.4215) | Acc_1: (84.54%) (35819/42368)\n",
      "Epoch: 46 | Batch_idx: 340 |  Loss_1: (0.4223) | Acc_1: (84.51%) (36888/43648)\n",
      "Epoch: 46 | Batch_idx: 350 |  Loss_1: (0.4231) | Acc_1: (84.49%) (37960/44928)\n",
      "Epoch: 46 | Batch_idx: 360 |  Loss_1: (0.4222) | Acc_1: (84.52%) (39054/46208)\n",
      "Epoch: 46 | Batch_idx: 370 |  Loss_1: (0.4223) | Acc_1: (84.52%) (40139/47488)\n",
      "Epoch: 46 | Batch_idx: 380 |  Loss_1: (0.4232) | Acc_1: (84.49%) (41205/48768)\n",
      "Epoch: 46 | Batch_idx: 390 |  Loss_1: (0.4235) | Acc_1: (84.48%) (42239/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4508) | Acc: (88.68%) (8868/10000)\n",
      "Epoch: 47 | Batch_idx: 0 |  Loss_1: (0.4760) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 47 | Batch_idx: 10 |  Loss_1: (0.4257) | Acc_1: (84.23%) (1186/1408)\n",
      "Epoch: 47 | Batch_idx: 20 |  Loss_1: (0.4201) | Acc_1: (84.34%) (2267/2688)\n",
      "Epoch: 47 | Batch_idx: 30 |  Loss_1: (0.4157) | Acc_1: (84.55%) (3355/3968)\n",
      "Epoch: 47 | Batch_idx: 40 |  Loss_1: (0.4164) | Acc_1: (84.60%) (4440/5248)\n",
      "Epoch: 47 | Batch_idx: 50 |  Loss_1: (0.4180) | Acc_1: (84.70%) (5529/6528)\n",
      "Epoch: 47 | Batch_idx: 60 |  Loss_1: (0.4304) | Acc_1: (84.25%) (6578/7808)\n",
      "Epoch: 47 | Batch_idx: 70 |  Loss_1: (0.4249) | Acc_1: (84.39%) (7669/9088)\n",
      "Epoch: 47 | Batch_idx: 80 |  Loss_1: (0.4213) | Acc_1: (84.59%) (8770/10368)\n",
      "Epoch: 47 | Batch_idx: 90 |  Loss_1: (0.4202) | Acc_1: (84.65%) (9860/11648)\n",
      "Epoch: 47 | Batch_idx: 100 |  Loss_1: (0.4191) | Acc_1: (84.65%) (10944/12928)\n",
      "Epoch: 47 | Batch_idx: 110 |  Loss_1: (0.4160) | Acc_1: (84.78%) (12046/14208)\n",
      "Epoch: 47 | Batch_idx: 120 |  Loss_1: (0.4149) | Acc_1: (84.79%) (13133/15488)\n",
      "Epoch: 47 | Batch_idx: 130 |  Loss_1: (0.4167) | Acc_1: (84.72%) (14206/16768)\n",
      "Epoch: 47 | Batch_idx: 140 |  Loss_1: (0.4158) | Acc_1: (84.73%) (15292/18048)\n",
      "Epoch: 47 | Batch_idx: 150 |  Loss_1: (0.4144) | Acc_1: (84.74%) (16379/19328)\n",
      "Epoch: 47 | Batch_idx: 160 |  Loss_1: (0.4137) | Acc_1: (84.75%) (17465/20608)\n",
      "Epoch: 47 | Batch_idx: 170 |  Loss_1: (0.4178) | Acc_1: (84.58%) (18513/21888)\n",
      "Epoch: 47 | Batch_idx: 180 |  Loss_1: (0.4166) | Acc_1: (84.62%) (19605/23168)\n",
      "Epoch: 47 | Batch_idx: 190 |  Loss_1: (0.4191) | Acc_1: (84.53%) (20667/24448)\n",
      "Epoch: 47 | Batch_idx: 200 |  Loss_1: (0.4196) | Acc_1: (84.51%) (21743/25728)\n",
      "Epoch: 47 | Batch_idx: 210 |  Loss_1: (0.4185) | Acc_1: (84.56%) (22839/27008)\n",
      "Epoch: 47 | Batch_idx: 220 |  Loss_1: (0.4196) | Acc_1: (84.54%) (23916/28288)\n",
      "Epoch: 47 | Batch_idx: 230 |  Loss_1: (0.4195) | Acc_1: (84.54%) (24997/29568)\n",
      "Epoch: 47 | Batch_idx: 240 |  Loss_1: (0.4193) | Acc_1: (84.56%) (26084/30848)\n",
      "Epoch: 47 | Batch_idx: 250 |  Loss_1: (0.4200) | Acc_1: (84.52%) (27153/32128)\n",
      "Epoch: 47 | Batch_idx: 260 |  Loss_1: (0.4224) | Acc_1: (84.41%) (28200/33408)\n",
      "Epoch: 47 | Batch_idx: 270 |  Loss_1: (0.4238) | Acc_1: (84.36%) (29263/34688)\n",
      "Epoch: 47 | Batch_idx: 280 |  Loss_1: (0.4249) | Acc_1: (84.33%) (30333/35968)\n",
      "Epoch: 47 | Batch_idx: 290 |  Loss_1: (0.4261) | Acc_1: (84.26%) (31387/37248)\n",
      "Epoch: 47 | Batch_idx: 300 |  Loss_1: (0.4254) | Acc_1: (84.29%) (32477/38528)\n",
      "Epoch: 47 | Batch_idx: 310 |  Loss_1: (0.4244) | Acc_1: (84.31%) (33564/39808)\n",
      "Epoch: 47 | Batch_idx: 320 |  Loss_1: (0.4232) | Acc_1: (84.35%) (34659/41088)\n",
      "Epoch: 47 | Batch_idx: 330 |  Loss_1: (0.4225) | Acc_1: (84.39%) (35754/42368)\n",
      "Epoch: 47 | Batch_idx: 340 |  Loss_1: (0.4229) | Acc_1: (84.38%) (36830/43648)\n",
      "Epoch: 47 | Batch_idx: 350 |  Loss_1: (0.4234) | Acc_1: (84.37%) (37905/44928)\n",
      "Epoch: 47 | Batch_idx: 360 |  Loss_1: (0.4244) | Acc_1: (84.33%) (38965/46208)\n",
      "Epoch: 47 | Batch_idx: 370 |  Loss_1: (0.4252) | Acc_1: (84.29%) (40026/47488)\n",
      "Epoch: 47 | Batch_idx: 380 |  Loss_1: (0.4255) | Acc_1: (84.27%) (41096/48768)\n",
      "Epoch: 47 | Batch_idx: 390 |  Loss_1: (0.4251) | Acc_1: (84.28%) (42138/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4872) | Acc: (88.66%) (8866/10000)\n",
      "Epoch: 48 | Batch_idx: 0 |  Loss_1: (0.4573) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 48 | Batch_idx: 10 |  Loss_1: (0.3993) | Acc_1: (85.37%) (1202/1408)\n",
      "Epoch: 48 | Batch_idx: 20 |  Loss_1: (0.4013) | Acc_1: (85.31%) (2293/2688)\n",
      "Epoch: 48 | Batch_idx: 30 |  Loss_1: (0.4038) | Acc_1: (85.11%) (3377/3968)\n",
      "Epoch: 48 | Batch_idx: 40 |  Loss_1: (0.4125) | Acc_1: (84.78%) (4449/5248)\n",
      "Epoch: 48 | Batch_idx: 50 |  Loss_1: (0.4156) | Acc_1: (84.70%) (5529/6528)\n",
      "Epoch: 48 | Batch_idx: 60 |  Loss_1: (0.4196) | Acc_1: (84.50%) (6598/7808)\n",
      "Epoch: 48 | Batch_idx: 70 |  Loss_1: (0.4220) | Acc_1: (84.44%) (7674/9088)\n",
      "Epoch: 48 | Batch_idx: 80 |  Loss_1: (0.4219) | Acc_1: (84.50%) (8761/10368)\n",
      "Epoch: 48 | Batch_idx: 90 |  Loss_1: (0.4218) | Acc_1: (84.54%) (9847/11648)\n",
      "Epoch: 48 | Batch_idx: 100 |  Loss_1: (0.4240) | Acc_1: (84.43%) (10915/12928)\n",
      "Epoch: 48 | Batch_idx: 110 |  Loss_1: (0.4224) | Acc_1: (84.47%) (12002/14208)\n",
      "Epoch: 48 | Batch_idx: 120 |  Loss_1: (0.4208) | Acc_1: (84.56%) (13097/15488)\n",
      "Epoch: 48 | Batch_idx: 130 |  Loss_1: (0.4235) | Acc_1: (84.49%) (14167/16768)\n",
      "Epoch: 48 | Batch_idx: 140 |  Loss_1: (0.4226) | Acc_1: (84.53%) (15256/18048)\n",
      "Epoch: 48 | Batch_idx: 150 |  Loss_1: (0.4247) | Acc_1: (84.44%) (16321/19328)\n",
      "Epoch: 48 | Batch_idx: 160 |  Loss_1: (0.4245) | Acc_1: (84.46%) (17406/20608)\n",
      "Epoch: 48 | Batch_idx: 170 |  Loss_1: (0.4228) | Acc_1: (84.55%) (18507/21888)\n",
      "Epoch: 48 | Batch_idx: 180 |  Loss_1: (0.4233) | Acc_1: (84.51%) (19579/23168)\n",
      "Epoch: 48 | Batch_idx: 190 |  Loss_1: (0.4228) | Acc_1: (84.55%) (20670/24448)\n",
      "Epoch: 48 | Batch_idx: 200 |  Loss_1: (0.4233) | Acc_1: (84.51%) (21744/25728)\n",
      "Epoch: 48 | Batch_idx: 210 |  Loss_1: (0.4216) | Acc_1: (84.56%) (22837/27008)\n",
      "Epoch: 48 | Batch_idx: 220 |  Loss_1: (0.4210) | Acc_1: (84.58%) (23926/28288)\n",
      "Epoch: 48 | Batch_idx: 230 |  Loss_1: (0.4222) | Acc_1: (84.54%) (24997/29568)\n",
      "Epoch: 48 | Batch_idx: 240 |  Loss_1: (0.4219) | Acc_1: (84.55%) (26082/30848)\n",
      "Epoch: 48 | Batch_idx: 250 |  Loss_1: (0.4213) | Acc_1: (84.57%) (27171/32128)\n",
      "Epoch: 48 | Batch_idx: 260 |  Loss_1: (0.4218) | Acc_1: (84.56%) (28249/33408)\n",
      "Epoch: 48 | Batch_idx: 270 |  Loss_1: (0.4237) | Acc_1: (84.48%) (29303/34688)\n",
      "Epoch: 48 | Batch_idx: 280 |  Loss_1: (0.4229) | Acc_1: (84.51%) (30396/35968)\n",
      "Epoch: 48 | Batch_idx: 290 |  Loss_1: (0.4224) | Acc_1: (84.51%) (31480/37248)\n",
      "Epoch: 48 | Batch_idx: 300 |  Loss_1: (0.4217) | Acc_1: (84.53%) (32568/38528)\n",
      "Epoch: 48 | Batch_idx: 310 |  Loss_1: (0.4217) | Acc_1: (84.53%) (33649/39808)\n",
      "Epoch: 48 | Batch_idx: 320 |  Loss_1: (0.4205) | Acc_1: (84.57%) (34748/41088)\n",
      "Epoch: 48 | Batch_idx: 330 |  Loss_1: (0.4201) | Acc_1: (84.59%) (35837/42368)\n",
      "Epoch: 48 | Batch_idx: 340 |  Loss_1: (0.4207) | Acc_1: (84.54%) (36901/43648)\n",
      "Epoch: 48 | Batch_idx: 350 |  Loss_1: (0.4204) | Acc_1: (84.55%) (37986/44928)\n",
      "Epoch: 48 | Batch_idx: 360 |  Loss_1: (0.4203) | Acc_1: (84.53%) (39058/46208)\n",
      "Epoch: 48 | Batch_idx: 370 |  Loss_1: (0.4212) | Acc_1: (84.50%) (40125/47488)\n",
      "Epoch: 48 | Batch_idx: 380 |  Loss_1: (0.4217) | Acc_1: (84.47%) (41194/48768)\n",
      "Epoch: 48 | Batch_idx: 390 |  Loss_1: (0.4209) | Acc_1: (84.50%) (42252/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4763) | Acc: (89.36%) (8936/10000)\n",
      "Epoch: 49 | Batch_idx: 0 |  Loss_1: (0.4829) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 49 | Batch_idx: 10 |  Loss_1: (0.4490) | Acc_1: (83.45%) (1175/1408)\n",
      "Epoch: 49 | Batch_idx: 20 |  Loss_1: (0.4443) | Acc_1: (83.52%) (2245/2688)\n",
      "Epoch: 49 | Batch_idx: 30 |  Loss_1: (0.4496) | Acc_1: (83.19%) (3301/3968)\n",
      "Epoch: 49 | Batch_idx: 40 |  Loss_1: (0.4340) | Acc_1: (83.71%) (4393/5248)\n",
      "Epoch: 49 | Batch_idx: 50 |  Loss_1: (0.4310) | Acc_1: (83.92%) (5478/6528)\n",
      "Epoch: 49 | Batch_idx: 60 |  Loss_1: (0.4240) | Acc_1: (84.23%) (6577/7808)\n",
      "Epoch: 49 | Batch_idx: 70 |  Loss_1: (0.4242) | Acc_1: (84.30%) (7661/9088)\n",
      "Epoch: 49 | Batch_idx: 80 |  Loss_1: (0.4247) | Acc_1: (84.27%) (8737/10368)\n",
      "Epoch: 49 | Batch_idx: 90 |  Loss_1: (0.4260) | Acc_1: (84.25%) (9814/11648)\n",
      "Epoch: 49 | Batch_idx: 100 |  Loss_1: (0.4230) | Acc_1: (84.37%) (10907/12928)\n",
      "Epoch: 49 | Batch_idx: 110 |  Loss_1: (0.4219) | Acc_1: (84.35%) (11985/14208)\n",
      "Epoch: 49 | Batch_idx: 120 |  Loss_1: (0.4171) | Acc_1: (84.59%) (13101/15488)\n",
      "Epoch: 49 | Batch_idx: 130 |  Loss_1: (0.4147) | Acc_1: (84.65%) (14194/16768)\n",
      "Epoch: 49 | Batch_idx: 140 |  Loss_1: (0.4130) | Acc_1: (84.74%) (15294/18048)\n",
      "Epoch: 49 | Batch_idx: 150 |  Loss_1: (0.4133) | Acc_1: (84.73%) (16376/19328)\n",
      "Epoch: 49 | Batch_idx: 160 |  Loss_1: (0.4137) | Acc_1: (84.74%) (17463/20608)\n",
      "Epoch: 49 | Batch_idx: 170 |  Loss_1: (0.4149) | Acc_1: (84.75%) (18550/21888)\n",
      "Epoch: 49 | Batch_idx: 180 |  Loss_1: (0.4162) | Acc_1: (84.71%) (19626/23168)\n",
      "Epoch: 49 | Batch_idx: 190 |  Loss_1: (0.4156) | Acc_1: (84.71%) (20710/24448)\n",
      "Epoch: 49 | Batch_idx: 200 |  Loss_1: (0.4136) | Acc_1: (84.81%) (21821/25728)\n",
      "Epoch: 49 | Batch_idx: 210 |  Loss_1: (0.4134) | Acc_1: (84.84%) (22913/27008)\n",
      "Epoch: 49 | Batch_idx: 220 |  Loss_1: (0.4146) | Acc_1: (84.81%) (23992/28288)\n",
      "Epoch: 49 | Batch_idx: 230 |  Loss_1: (0.4165) | Acc_1: (84.72%) (25049/29568)\n",
      "Epoch: 49 | Batch_idx: 240 |  Loss_1: (0.4167) | Acc_1: (84.71%) (26130/30848)\n",
      "Epoch: 49 | Batch_idx: 250 |  Loss_1: (0.4179) | Acc_1: (84.69%) (27210/32128)\n",
      "Epoch: 49 | Batch_idx: 260 |  Loss_1: (0.4194) | Acc_1: (84.64%) (28276/33408)\n",
      "Epoch: 49 | Batch_idx: 270 |  Loss_1: (0.4197) | Acc_1: (84.61%) (29351/34688)\n",
      "Epoch: 49 | Batch_idx: 280 |  Loss_1: (0.4199) | Acc_1: (84.61%) (30434/35968)\n",
      "Epoch: 49 | Batch_idx: 290 |  Loss_1: (0.4188) | Acc_1: (84.63%) (31524/37248)\n",
      "Epoch: 49 | Batch_idx: 300 |  Loss_1: (0.4185) | Acc_1: (84.64%) (32610/38528)\n",
      "Epoch: 49 | Batch_idx: 310 |  Loss_1: (0.4190) | Acc_1: (84.62%) (33686/39808)\n",
      "Epoch: 49 | Batch_idx: 320 |  Loss_1: (0.4197) | Acc_1: (84.59%) (34757/41088)\n",
      "Epoch: 49 | Batch_idx: 330 |  Loss_1: (0.4194) | Acc_1: (84.60%) (35844/42368)\n",
      "Epoch: 49 | Batch_idx: 340 |  Loss_1: (0.4203) | Acc_1: (84.57%) (36911/43648)\n",
      "Epoch: 49 | Batch_idx: 350 |  Loss_1: (0.4195) | Acc_1: (84.60%) (38010/44928)\n",
      "Epoch: 49 | Batch_idx: 360 |  Loss_1: (0.4184) | Acc_1: (84.64%) (39111/46208)\n",
      "Epoch: 49 | Batch_idx: 370 |  Loss_1: (0.4181) | Acc_1: (84.65%) (40197/47488)\n",
      "Epoch: 49 | Batch_idx: 380 |  Loss_1: (0.4195) | Acc_1: (84.60%) (41257/48768)\n",
      "Epoch: 49 | Batch_idx: 390 |  Loss_1: (0.4197) | Acc_1: (84.59%) (42296/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4584) | Acc: (89.16%) (8916/10000)\n",
      "Epoch: 50 | Batch_idx: 0 |  Loss_1: (0.4279) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 50 | Batch_idx: 10 |  Loss_1: (0.4550) | Acc_1: (83.38%) (1174/1408)\n",
      "Epoch: 50 | Batch_idx: 20 |  Loss_1: (0.4651) | Acc_1: (83.04%) (2232/2688)\n",
      "Epoch: 50 | Batch_idx: 30 |  Loss_1: (0.4436) | Acc_1: (83.85%) (3327/3968)\n",
      "Epoch: 50 | Batch_idx: 40 |  Loss_1: (0.4468) | Acc_1: (83.78%) (4397/5248)\n",
      "Epoch: 50 | Batch_idx: 50 |  Loss_1: (0.4371) | Acc_1: (84.18%) (5495/6528)\n",
      "Epoch: 50 | Batch_idx: 60 |  Loss_1: (0.4367) | Acc_1: (84.21%) (6575/7808)\n",
      "Epoch: 50 | Batch_idx: 70 |  Loss_1: (0.4378) | Acc_1: (84.19%) (7651/9088)\n",
      "Epoch: 50 | Batch_idx: 80 |  Loss_1: (0.4370) | Acc_1: (84.10%) (8720/10368)\n",
      "Epoch: 50 | Batch_idx: 90 |  Loss_1: (0.4352) | Acc_1: (84.11%) (9797/11648)\n",
      "Epoch: 50 | Batch_idx: 100 |  Loss_1: (0.4379) | Acc_1: (84.03%) (10863/12928)\n",
      "Epoch: 50 | Batch_idx: 110 |  Loss_1: (0.4429) | Acc_1: (83.83%) (11911/14208)\n",
      "Epoch: 50 | Batch_idx: 120 |  Loss_1: (0.4452) | Acc_1: (83.77%) (12974/15488)\n",
      "Epoch: 50 | Batch_idx: 130 |  Loss_1: (0.4490) | Acc_1: (83.57%) (14013/16768)\n",
      "Epoch: 50 | Batch_idx: 140 |  Loss_1: (0.4487) | Acc_1: (83.57%) (15082/18048)\n",
      "Epoch: 50 | Batch_idx: 150 |  Loss_1: (0.4491) | Acc_1: (83.55%) (16148/19328)\n",
      "Epoch: 50 | Batch_idx: 160 |  Loss_1: (0.4523) | Acc_1: (83.41%) (17189/20608)\n",
      "Epoch: 50 | Batch_idx: 170 |  Loss_1: (0.4532) | Acc_1: (83.36%) (18246/21888)\n",
      "Epoch: 50 | Batch_idx: 180 |  Loss_1: (0.4559) | Acc_1: (83.25%) (19288/23168)\n",
      "Epoch: 50 | Batch_idx: 190 |  Loss_1: (0.4546) | Acc_1: (83.30%) (20365/24448)\n",
      "Epoch: 50 | Batch_idx: 200 |  Loss_1: (0.4543) | Acc_1: (83.34%) (21441/25728)\n",
      "Epoch: 50 | Batch_idx: 210 |  Loss_1: (0.4543) | Acc_1: (83.35%) (22512/27008)\n",
      "Epoch: 50 | Batch_idx: 220 |  Loss_1: (0.4549) | Acc_1: (83.31%) (23568/28288)\n",
      "Epoch: 50 | Batch_idx: 230 |  Loss_1: (0.4568) | Acc_1: (83.20%) (24600/29568)\n",
      "Epoch: 50 | Batch_idx: 240 |  Loss_1: (0.4573) | Acc_1: (83.16%) (25654/30848)\n",
      "Epoch: 50 | Batch_idx: 250 |  Loss_1: (0.4565) | Acc_1: (83.20%) (26730/32128)\n",
      "Epoch: 50 | Batch_idx: 260 |  Loss_1: (0.4583) | Acc_1: (83.14%) (27777/33408)\n",
      "Epoch: 50 | Batch_idx: 270 |  Loss_1: (0.4583) | Acc_1: (83.15%) (28842/34688)\n",
      "Epoch: 50 | Batch_idx: 280 |  Loss_1: (0.4568) | Acc_1: (83.18%) (29917/35968)\n",
      "Epoch: 50 | Batch_idx: 290 |  Loss_1: (0.4577) | Acc_1: (83.13%) (30965/37248)\n",
      "Epoch: 50 | Batch_idx: 300 |  Loss_1: (0.4593) | Acc_1: (83.06%) (32000/38528)\n",
      "Epoch: 50 | Batch_idx: 310 |  Loss_1: (0.4590) | Acc_1: (83.07%) (33068/39808)\n",
      "Epoch: 50 | Batch_idx: 320 |  Loss_1: (0.4593) | Acc_1: (83.08%) (34134/41088)\n",
      "Epoch: 50 | Batch_idx: 330 |  Loss_1: (0.4591) | Acc_1: (83.08%) (35199/42368)\n",
      "Epoch: 50 | Batch_idx: 340 |  Loss_1: (0.4582) | Acc_1: (83.12%) (36281/43648)\n",
      "Epoch: 50 | Batch_idx: 350 |  Loss_1: (0.4583) | Acc_1: (83.12%) (37344/44928)\n",
      "Epoch: 50 | Batch_idx: 360 |  Loss_1: (0.4588) | Acc_1: (83.10%) (38397/46208)\n",
      "Epoch: 50 | Batch_idx: 370 |  Loss_1: (0.4599) | Acc_1: (83.07%) (39448/47488)\n",
      "Epoch: 50 | Batch_idx: 380 |  Loss_1: (0.4600) | Acc_1: (83.07%) (40512/48768)\n",
      "Epoch: 50 | Batch_idx: 390 |  Loss_1: (0.4603) | Acc_1: (83.06%) (41528/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4781) | Acc: (87.03%) (8703/10000)\n",
      "Epoch: 51 | Batch_idx: 0 |  Loss_1: (0.5509) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 51 | Batch_idx: 10 |  Loss_1: (0.4871) | Acc_1: (81.68%) (1150/1408)\n",
      "Epoch: 51 | Batch_idx: 20 |  Loss_1: (0.4849) | Acc_1: (81.73%) (2197/2688)\n",
      "Epoch: 51 | Batch_idx: 30 |  Loss_1: (0.4653) | Acc_1: (82.43%) (3271/3968)\n",
      "Epoch: 51 | Batch_idx: 40 |  Loss_1: (0.4647) | Acc_1: (82.62%) (4336/5248)\n",
      "Epoch: 51 | Batch_idx: 50 |  Loss_1: (0.4563) | Acc_1: (83.01%) (5419/6528)\n",
      "Epoch: 51 | Batch_idx: 60 |  Loss_1: (0.4616) | Acc_1: (82.88%) (6471/7808)\n",
      "Epoch: 51 | Batch_idx: 70 |  Loss_1: (0.4575) | Acc_1: (83.02%) (7545/9088)\n",
      "Epoch: 51 | Batch_idx: 80 |  Loss_1: (0.4581) | Acc_1: (83.02%) (8608/10368)\n",
      "Epoch: 51 | Batch_idx: 90 |  Loss_1: (0.4605) | Acc_1: (83.01%) (9669/11648)\n",
      "Epoch: 51 | Batch_idx: 100 |  Loss_1: (0.4618) | Acc_1: (82.92%) (10720/12928)\n",
      "Epoch: 51 | Batch_idx: 110 |  Loss_1: (0.4579) | Acc_1: (83.09%) (11806/14208)\n",
      "Epoch: 51 | Batch_idx: 120 |  Loss_1: (0.4562) | Acc_1: (83.16%) (12880/15488)\n",
      "Epoch: 51 | Batch_idx: 130 |  Loss_1: (0.4568) | Acc_1: (83.16%) (13944/16768)\n",
      "Epoch: 51 | Batch_idx: 140 |  Loss_1: (0.4598) | Acc_1: (83.07%) (14992/18048)\n",
      "Epoch: 51 | Batch_idx: 150 |  Loss_1: (0.4615) | Acc_1: (83.00%) (16043/19328)\n",
      "Epoch: 51 | Batch_idx: 160 |  Loss_1: (0.4653) | Acc_1: (82.86%) (17076/20608)\n",
      "Epoch: 51 | Batch_idx: 170 |  Loss_1: (0.4685) | Acc_1: (82.73%) (18107/21888)\n",
      "Epoch: 51 | Batch_idx: 180 |  Loss_1: (0.4715) | Acc_1: (82.65%) (19148/23168)\n",
      "Epoch: 51 | Batch_idx: 190 |  Loss_1: (0.4711) | Acc_1: (82.65%) (20207/24448)\n",
      "Epoch: 51 | Batch_idx: 200 |  Loss_1: (0.4691) | Acc_1: (82.73%) (21285/25728)\n",
      "Epoch: 51 | Batch_idx: 210 |  Loss_1: (0.4709) | Acc_1: (82.66%) (22326/27008)\n",
      "Epoch: 51 | Batch_idx: 220 |  Loss_1: (0.4713) | Acc_1: (82.63%) (23375/28288)\n",
      "Epoch: 51 | Batch_idx: 230 |  Loss_1: (0.4709) | Acc_1: (82.63%) (24432/29568)\n",
      "Epoch: 51 | Batch_idx: 240 |  Loss_1: (0.4714) | Acc_1: (82.62%) (25488/30848)\n",
      "Epoch: 51 | Batch_idx: 250 |  Loss_1: (0.4725) | Acc_1: (82.62%) (26544/32128)\n",
      "Epoch: 51 | Batch_idx: 260 |  Loss_1: (0.4743) | Acc_1: (82.56%) (27580/33408)\n",
      "Epoch: 51 | Batch_idx: 270 |  Loss_1: (0.4735) | Acc_1: (82.61%) (28657/34688)\n",
      "Epoch: 51 | Batch_idx: 280 |  Loss_1: (0.4731) | Acc_1: (82.64%) (29723/35968)\n",
      "Epoch: 51 | Batch_idx: 290 |  Loss_1: (0.4730) | Acc_1: (82.63%) (30778/37248)\n",
      "Epoch: 51 | Batch_idx: 300 |  Loss_1: (0.4735) | Acc_1: (82.63%) (31834/38528)\n",
      "Epoch: 51 | Batch_idx: 310 |  Loss_1: (0.4731) | Acc_1: (82.65%) (32902/39808)\n",
      "Epoch: 51 | Batch_idx: 320 |  Loss_1: (0.4740) | Acc_1: (82.62%) (33948/41088)\n",
      "Epoch: 51 | Batch_idx: 330 |  Loss_1: (0.4734) | Acc_1: (82.64%) (35013/42368)\n",
      "Epoch: 51 | Batch_idx: 340 |  Loss_1: (0.4740) | Acc_1: (82.61%) (36057/43648)\n",
      "Epoch: 51 | Batch_idx: 350 |  Loss_1: (0.4736) | Acc_1: (82.62%) (37121/44928)\n",
      "Epoch: 51 | Batch_idx: 360 |  Loss_1: (0.4740) | Acc_1: (82.62%) (38175/46208)\n",
      "Epoch: 51 | Batch_idx: 370 |  Loss_1: (0.4737) | Acc_1: (82.62%) (39234/47488)\n",
      "Epoch: 51 | Batch_idx: 380 |  Loss_1: (0.4735) | Acc_1: (82.62%) (40294/48768)\n",
      "Epoch: 51 | Batch_idx: 390 |  Loss_1: (0.4753) | Acc_1: (82.57%) (41283/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4676) | Acc: (87.25%) (8725/10000)\n",
      "Epoch: 52 | Batch_idx: 0 |  Loss_1: (0.4238) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 52 | Batch_idx: 10 |  Loss_1: (0.4911) | Acc_1: (81.46%) (1147/1408)\n",
      "Epoch: 52 | Batch_idx: 20 |  Loss_1: (0.4835) | Acc_1: (81.92%) (2202/2688)\n",
      "Epoch: 52 | Batch_idx: 30 |  Loss_1: (0.4837) | Acc_1: (81.96%) (3252/3968)\n",
      "Epoch: 52 | Batch_idx: 40 |  Loss_1: (0.4820) | Acc_1: (82.03%) (4305/5248)\n",
      "Epoch: 52 | Batch_idx: 50 |  Loss_1: (0.4789) | Acc_1: (82.25%) (5369/6528)\n",
      "Epoch: 52 | Batch_idx: 60 |  Loss_1: (0.4813) | Acc_1: (82.29%) (6425/7808)\n",
      "Epoch: 52 | Batch_idx: 70 |  Loss_1: (0.4812) | Acc_1: (82.25%) (7475/9088)\n",
      "Epoch: 52 | Batch_idx: 80 |  Loss_1: (0.4854) | Acc_1: (82.10%) (8512/10368)\n",
      "Epoch: 52 | Batch_idx: 90 |  Loss_1: (0.4811) | Acc_1: (82.25%) (9581/11648)\n",
      "Epoch: 52 | Batch_idx: 100 |  Loss_1: (0.4777) | Acc_1: (82.38%) (10650/12928)\n",
      "Epoch: 52 | Batch_idx: 110 |  Loss_1: (0.4773) | Acc_1: (82.36%) (11702/14208)\n",
      "Epoch: 52 | Batch_idx: 120 |  Loss_1: (0.4821) | Acc_1: (82.17%) (12726/15488)\n",
      "Epoch: 52 | Batch_idx: 130 |  Loss_1: (0.4828) | Acc_1: (82.17%) (13779/16768)\n",
      "Epoch: 52 | Batch_idx: 140 |  Loss_1: (0.4852) | Acc_1: (82.12%) (14821/18048)\n",
      "Epoch: 52 | Batch_idx: 150 |  Loss_1: (0.4832) | Acc_1: (82.19%) (15885/19328)\n",
      "Epoch: 52 | Batch_idx: 160 |  Loss_1: (0.4842) | Acc_1: (82.15%) (16930/20608)\n",
      "Epoch: 52 | Batch_idx: 170 |  Loss_1: (0.4817) | Acc_1: (82.23%) (17998/21888)\n",
      "Epoch: 52 | Batch_idx: 180 |  Loss_1: (0.4806) | Acc_1: (82.32%) (19071/23168)\n",
      "Epoch: 52 | Batch_idx: 190 |  Loss_1: (0.4786) | Acc_1: (82.41%) (20147/24448)\n",
      "Epoch: 52 | Batch_idx: 200 |  Loss_1: (0.4783) | Acc_1: (82.40%) (21201/25728)\n",
      "Epoch: 52 | Batch_idx: 210 |  Loss_1: (0.4766) | Acc_1: (82.48%) (22275/27008)\n",
      "Epoch: 52 | Batch_idx: 220 |  Loss_1: (0.4769) | Acc_1: (82.43%) (23318/28288)\n",
      "Epoch: 52 | Batch_idx: 230 |  Loss_1: (0.4769) | Acc_1: (82.45%) (24378/29568)\n",
      "Epoch: 52 | Batch_idx: 240 |  Loss_1: (0.4794) | Acc_1: (82.33%) (25396/30848)\n",
      "Epoch: 52 | Batch_idx: 250 |  Loss_1: (0.4782) | Acc_1: (82.37%) (26464/32128)\n",
      "Epoch: 52 | Batch_idx: 260 |  Loss_1: (0.4768) | Acc_1: (82.44%) (27542/33408)\n",
      "Epoch: 52 | Batch_idx: 270 |  Loss_1: (0.4773) | Acc_1: (82.44%) (28597/34688)\n",
      "Epoch: 52 | Batch_idx: 280 |  Loss_1: (0.4779) | Acc_1: (82.42%) (29646/35968)\n",
      "Epoch: 52 | Batch_idx: 290 |  Loss_1: (0.4764) | Acc_1: (82.49%) (30727/37248)\n",
      "Epoch: 52 | Batch_idx: 300 |  Loss_1: (0.4764) | Acc_1: (82.49%) (31782/38528)\n",
      "Epoch: 52 | Batch_idx: 310 |  Loss_1: (0.4776) | Acc_1: (82.44%) (32817/39808)\n",
      "Epoch: 52 | Batch_idx: 320 |  Loss_1: (0.4765) | Acc_1: (82.47%) (33884/41088)\n",
      "Epoch: 52 | Batch_idx: 330 |  Loss_1: (0.4768) | Acc_1: (82.43%) (34926/42368)\n",
      "Epoch: 52 | Batch_idx: 340 |  Loss_1: (0.4768) | Acc_1: (82.42%) (35975/43648)\n",
      "Epoch: 52 | Batch_idx: 350 |  Loss_1: (0.4764) | Acc_1: (82.43%) (37036/44928)\n",
      "Epoch: 52 | Batch_idx: 360 |  Loss_1: (0.4756) | Acc_1: (82.46%) (38101/46208)\n",
      "Epoch: 52 | Batch_idx: 370 |  Loss_1: (0.4758) | Acc_1: (82.45%) (39153/47488)\n",
      "Epoch: 52 | Batch_idx: 380 |  Loss_1: (0.4758) | Acc_1: (82.45%) (40208/48768)\n",
      "Epoch: 52 | Batch_idx: 390 |  Loss_1: (0.4755) | Acc_1: (82.46%) (41228/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4595) | Acc: (87.69%) (8769/10000)\n",
      "Epoch: 53 | Batch_idx: 0 |  Loss_1: (0.3804) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 53 | Batch_idx: 10 |  Loss_1: (0.4817) | Acc_1: (82.88%) (1167/1408)\n",
      "Epoch: 53 | Batch_idx: 20 |  Loss_1: (0.4912) | Acc_1: (82.18%) (2209/2688)\n",
      "Epoch: 53 | Batch_idx: 30 |  Loss_1: (0.4900) | Acc_1: (82.13%) (3259/3968)\n",
      "Epoch: 53 | Batch_idx: 40 |  Loss_1: (0.4917) | Acc_1: (82.05%) (4306/5248)\n",
      "Epoch: 53 | Batch_idx: 50 |  Loss_1: (0.4933) | Acc_1: (82.00%) (5353/6528)\n",
      "Epoch: 53 | Batch_idx: 60 |  Loss_1: (0.4933) | Acc_1: (81.97%) (6400/7808)\n",
      "Epoch: 53 | Batch_idx: 70 |  Loss_1: (0.4805) | Acc_1: (82.48%) (7496/9088)\n",
      "Epoch: 53 | Batch_idx: 80 |  Loss_1: (0.4764) | Acc_1: (82.54%) (8558/10368)\n",
      "Epoch: 53 | Batch_idx: 90 |  Loss_1: (0.4728) | Acc_1: (82.69%) (9632/11648)\n",
      "Epoch: 53 | Batch_idx: 100 |  Loss_1: (0.4723) | Acc_1: (82.74%) (10697/12928)\n",
      "Epoch: 53 | Batch_idx: 110 |  Loss_1: (0.4742) | Acc_1: (82.64%) (11742/14208)\n",
      "Epoch: 53 | Batch_idx: 120 |  Loss_1: (0.4776) | Acc_1: (82.47%) (12773/15488)\n",
      "Epoch: 53 | Batch_idx: 130 |  Loss_1: (0.4762) | Acc_1: (82.48%) (13830/16768)\n",
      "Epoch: 53 | Batch_idx: 140 |  Loss_1: (0.4765) | Acc_1: (82.51%) (14891/18048)\n",
      "Epoch: 53 | Batch_idx: 150 |  Loss_1: (0.4761) | Acc_1: (82.52%) (15949/19328)\n",
      "Epoch: 53 | Batch_idx: 160 |  Loss_1: (0.4740) | Acc_1: (82.62%) (17027/20608)\n",
      "Epoch: 53 | Batch_idx: 170 |  Loss_1: (0.4744) | Acc_1: (82.59%) (18077/21888)\n",
      "Epoch: 53 | Batch_idx: 180 |  Loss_1: (0.4757) | Acc_1: (82.52%) (19118/23168)\n",
      "Epoch: 53 | Batch_idx: 190 |  Loss_1: (0.4753) | Acc_1: (82.54%) (20180/24448)\n",
      "Epoch: 53 | Batch_idx: 200 |  Loss_1: (0.4748) | Acc_1: (82.55%) (21238/25728)\n",
      "Epoch: 53 | Batch_idx: 210 |  Loss_1: (0.4779) | Acc_1: (82.44%) (22265/27008)\n",
      "Epoch: 53 | Batch_idx: 220 |  Loss_1: (0.4761) | Acc_1: (82.50%) (23337/28288)\n",
      "Epoch: 53 | Batch_idx: 230 |  Loss_1: (0.4749) | Acc_1: (82.53%) (24402/29568)\n",
      "Epoch: 53 | Batch_idx: 240 |  Loss_1: (0.4756) | Acc_1: (82.49%) (25448/30848)\n",
      "Epoch: 53 | Batch_idx: 250 |  Loss_1: (0.4749) | Acc_1: (82.53%) (26514/32128)\n",
      "Epoch: 53 | Batch_idx: 260 |  Loss_1: (0.4755) | Acc_1: (82.48%) (27555/33408)\n",
      "Epoch: 53 | Batch_idx: 270 |  Loss_1: (0.4754) | Acc_1: (82.49%) (28614/34688)\n",
      "Epoch: 53 | Batch_idx: 280 |  Loss_1: (0.4742) | Acc_1: (82.50%) (29673/35968)\n",
      "Epoch: 53 | Batch_idx: 290 |  Loss_1: (0.4744) | Acc_1: (82.50%) (30731/37248)\n",
      "Epoch: 53 | Batch_idx: 300 |  Loss_1: (0.4751) | Acc_1: (82.49%) (31783/38528)\n",
      "Epoch: 53 | Batch_idx: 310 |  Loss_1: (0.4753) | Acc_1: (82.48%) (32835/39808)\n",
      "Epoch: 53 | Batch_idx: 320 |  Loss_1: (0.4733) | Acc_1: (82.56%) (33921/41088)\n",
      "Epoch: 53 | Batch_idx: 330 |  Loss_1: (0.4732) | Acc_1: (82.57%) (34982/42368)\n",
      "Epoch: 53 | Batch_idx: 340 |  Loss_1: (0.4736) | Acc_1: (82.55%) (36031/43648)\n",
      "Epoch: 53 | Batch_idx: 350 |  Loss_1: (0.4737) | Acc_1: (82.55%) (37090/44928)\n",
      "Epoch: 53 | Batch_idx: 360 |  Loss_1: (0.4747) | Acc_1: (82.51%) (38125/46208)\n",
      "Epoch: 53 | Batch_idx: 370 |  Loss_1: (0.4750) | Acc_1: (82.49%) (39173/47488)\n",
      "Epoch: 53 | Batch_idx: 380 |  Loss_1: (0.4742) | Acc_1: (82.53%) (40247/48768)\n",
      "Epoch: 53 | Batch_idx: 390 |  Loss_1: (0.4739) | Acc_1: (82.55%) (41274/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4277) | Acc: (87.45%) (8745/10000)\n",
      "Epoch: 54 | Batch_idx: 0 |  Loss_1: (0.4348) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 54 | Batch_idx: 10 |  Loss_1: (0.4517) | Acc_1: (82.88%) (1167/1408)\n",
      "Epoch: 54 | Batch_idx: 20 |  Loss_1: (0.4659) | Acc_1: (82.59%) (2220/2688)\n",
      "Epoch: 54 | Batch_idx: 30 |  Loss_1: (0.4742) | Acc_1: (82.43%) (3271/3968)\n",
      "Epoch: 54 | Batch_idx: 40 |  Loss_1: (0.4713) | Acc_1: (82.66%) (4338/5248)\n",
      "Epoch: 54 | Batch_idx: 50 |  Loss_1: (0.4672) | Acc_1: (82.75%) (5402/6528)\n",
      "Epoch: 54 | Batch_idx: 60 |  Loss_1: (0.4644) | Acc_1: (82.88%) (6471/7808)\n",
      "Epoch: 54 | Batch_idx: 70 |  Loss_1: (0.4618) | Acc_1: (83.08%) (7550/9088)\n",
      "Epoch: 54 | Batch_idx: 80 |  Loss_1: (0.4579) | Acc_1: (83.27%) (8633/10368)\n",
      "Epoch: 54 | Batch_idx: 90 |  Loss_1: (0.4626) | Acc_1: (83.06%) (9675/11648)\n",
      "Epoch: 54 | Batch_idx: 100 |  Loss_1: (0.4627) | Acc_1: (82.98%) (10728/12928)\n",
      "Epoch: 54 | Batch_idx: 110 |  Loss_1: (0.4567) | Acc_1: (83.23%) (11825/14208)\n",
      "Epoch: 54 | Batch_idx: 120 |  Loss_1: (0.4549) | Acc_1: (83.30%) (12902/15488)\n",
      "Epoch: 54 | Batch_idx: 130 |  Loss_1: (0.4562) | Acc_1: (83.24%) (13958/16768)\n",
      "Epoch: 54 | Batch_idx: 140 |  Loss_1: (0.4550) | Acc_1: (83.30%) (15034/18048)\n",
      "Epoch: 54 | Batch_idx: 150 |  Loss_1: (0.4585) | Acc_1: (83.19%) (16079/19328)\n",
      "Epoch: 54 | Batch_idx: 160 |  Loss_1: (0.4556) | Acc_1: (83.31%) (17169/20608)\n",
      "Epoch: 54 | Batch_idx: 170 |  Loss_1: (0.4575) | Acc_1: (83.23%) (18217/21888)\n",
      "Epoch: 54 | Batch_idx: 180 |  Loss_1: (0.4583) | Acc_1: (83.19%) (19273/23168)\n",
      "Epoch: 54 | Batch_idx: 190 |  Loss_1: (0.4582) | Acc_1: (83.17%) (20334/24448)\n",
      "Epoch: 54 | Batch_idx: 200 |  Loss_1: (0.4597) | Acc_1: (83.13%) (21387/25728)\n",
      "Epoch: 54 | Batch_idx: 210 |  Loss_1: (0.4605) | Acc_1: (83.12%) (22449/27008)\n",
      "Epoch: 54 | Batch_idx: 220 |  Loss_1: (0.4616) | Acc_1: (83.02%) (23484/28288)\n",
      "Epoch: 54 | Batch_idx: 230 |  Loss_1: (0.4619) | Acc_1: (83.01%) (24543/29568)\n",
      "Epoch: 54 | Batch_idx: 240 |  Loss_1: (0.4641) | Acc_1: (82.92%) (25578/30848)\n",
      "Epoch: 54 | Batch_idx: 250 |  Loss_1: (0.4632) | Acc_1: (82.95%) (26651/32128)\n",
      "Epoch: 54 | Batch_idx: 260 |  Loss_1: (0.4656) | Acc_1: (82.89%) (27693/33408)\n",
      "Epoch: 54 | Batch_idx: 270 |  Loss_1: (0.4664) | Acc_1: (82.86%) (28742/34688)\n",
      "Epoch: 54 | Batch_idx: 280 |  Loss_1: (0.4672) | Acc_1: (82.84%) (29797/35968)\n",
      "Epoch: 54 | Batch_idx: 290 |  Loss_1: (0.4686) | Acc_1: (82.80%) (30843/37248)\n",
      "Epoch: 54 | Batch_idx: 300 |  Loss_1: (0.4686) | Acc_1: (82.81%) (31905/38528)\n",
      "Epoch: 54 | Batch_idx: 310 |  Loss_1: (0.4690) | Acc_1: (82.80%) (32961/39808)\n",
      "Epoch: 54 | Batch_idx: 320 |  Loss_1: (0.4711) | Acc_1: (82.70%) (33978/41088)\n",
      "Epoch: 54 | Batch_idx: 330 |  Loss_1: (0.4718) | Acc_1: (82.68%) (35030/42368)\n",
      "Epoch: 54 | Batch_idx: 340 |  Loss_1: (0.4720) | Acc_1: (82.68%) (36088/43648)\n",
      "Epoch: 54 | Batch_idx: 350 |  Loss_1: (0.4712) | Acc_1: (82.71%) (37158/44928)\n",
      "Epoch: 54 | Batch_idx: 360 |  Loss_1: (0.4713) | Acc_1: (82.68%) (38205/46208)\n",
      "Epoch: 54 | Batch_idx: 370 |  Loss_1: (0.4717) | Acc_1: (82.67%) (39258/47488)\n",
      "Epoch: 54 | Batch_idx: 380 |  Loss_1: (0.4709) | Acc_1: (82.71%) (40334/48768)\n",
      "Epoch: 54 | Batch_idx: 390 |  Loss_1: (0.4703) | Acc_1: (82.70%) (41351/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4307) | Acc: (87.72%) (8772/10000)\n",
      "Epoch: 55 | Batch_idx: 0 |  Loss_1: (0.5019) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 55 | Batch_idx: 10 |  Loss_1: (0.4583) | Acc_1: (83.52%) (1176/1408)\n",
      "Epoch: 55 | Batch_idx: 20 |  Loss_1: (0.4589) | Acc_1: (83.18%) (2236/2688)\n",
      "Epoch: 55 | Batch_idx: 30 |  Loss_1: (0.4522) | Acc_1: (83.32%) (3306/3968)\n",
      "Epoch: 55 | Batch_idx: 40 |  Loss_1: (0.4519) | Acc_1: (83.33%) (4373/5248)\n",
      "Epoch: 55 | Batch_idx: 50 |  Loss_1: (0.4500) | Acc_1: (83.38%) (5443/6528)\n",
      "Epoch: 55 | Batch_idx: 60 |  Loss_1: (0.4544) | Acc_1: (83.21%) (6497/7808)\n",
      "Epoch: 55 | Batch_idx: 70 |  Loss_1: (0.4505) | Acc_1: (83.32%) (7572/9088)\n",
      "Epoch: 55 | Batch_idx: 80 |  Loss_1: (0.4495) | Acc_1: (83.37%) (8644/10368)\n",
      "Epoch: 55 | Batch_idx: 90 |  Loss_1: (0.4429) | Acc_1: (83.60%) (9738/11648)\n",
      "Epoch: 55 | Batch_idx: 100 |  Loss_1: (0.4421) | Acc_1: (83.63%) (10812/12928)\n",
      "Epoch: 55 | Batch_idx: 110 |  Loss_1: (0.4443) | Acc_1: (83.53%) (11868/14208)\n",
      "Epoch: 55 | Batch_idx: 120 |  Loss_1: (0.4491) | Acc_1: (83.35%) (12910/15488)\n",
      "Epoch: 55 | Batch_idx: 130 |  Loss_1: (0.4500) | Acc_1: (83.36%) (13977/16768)\n",
      "Epoch: 55 | Batch_idx: 140 |  Loss_1: (0.4498) | Acc_1: (83.37%) (15046/18048)\n",
      "Epoch: 55 | Batch_idx: 150 |  Loss_1: (0.4504) | Acc_1: (83.32%) (16105/19328)\n",
      "Epoch: 55 | Batch_idx: 160 |  Loss_1: (0.4512) | Acc_1: (83.33%) (17173/20608)\n",
      "Epoch: 55 | Batch_idx: 170 |  Loss_1: (0.4526) | Acc_1: (83.25%) (18221/21888)\n",
      "Epoch: 55 | Batch_idx: 180 |  Loss_1: (0.4530) | Acc_1: (83.23%) (19283/23168)\n",
      "Epoch: 55 | Batch_idx: 190 |  Loss_1: (0.4531) | Acc_1: (83.25%) (20354/24448)\n",
      "Epoch: 55 | Batch_idx: 200 |  Loss_1: (0.4533) | Acc_1: (83.26%) (21421/25728)\n",
      "Epoch: 55 | Batch_idx: 210 |  Loss_1: (0.4553) | Acc_1: (83.19%) (22469/27008)\n",
      "Epoch: 55 | Batch_idx: 220 |  Loss_1: (0.4546) | Acc_1: (83.23%) (23545/28288)\n",
      "Epoch: 55 | Batch_idx: 230 |  Loss_1: (0.4561) | Acc_1: (83.18%) (24594/29568)\n",
      "Epoch: 55 | Batch_idx: 240 |  Loss_1: (0.4555) | Acc_1: (83.22%) (25671/30848)\n",
      "Epoch: 55 | Batch_idx: 250 |  Loss_1: (0.4541) | Acc_1: (83.28%) (26756/32128)\n",
      "Epoch: 55 | Batch_idx: 260 |  Loss_1: (0.4535) | Acc_1: (83.31%) (27831/33408)\n",
      "Epoch: 55 | Batch_idx: 270 |  Loss_1: (0.4554) | Acc_1: (83.25%) (28879/34688)\n",
      "Epoch: 55 | Batch_idx: 280 |  Loss_1: (0.4558) | Acc_1: (83.21%) (29930/35968)\n",
      "Epoch: 55 | Batch_idx: 290 |  Loss_1: (0.4568) | Acc_1: (83.17%) (30979/37248)\n",
      "Epoch: 55 | Batch_idx: 300 |  Loss_1: (0.4566) | Acc_1: (83.17%) (32042/38528)\n",
      "Epoch: 55 | Batch_idx: 310 |  Loss_1: (0.4569) | Acc_1: (83.16%) (33105/39808)\n",
      "Epoch: 55 | Batch_idx: 320 |  Loss_1: (0.4575) | Acc_1: (83.15%) (34166/41088)\n",
      "Epoch: 55 | Batch_idx: 330 |  Loss_1: (0.4575) | Acc_1: (83.16%) (35232/42368)\n",
      "Epoch: 55 | Batch_idx: 340 |  Loss_1: (0.4585) | Acc_1: (83.13%) (36284/43648)\n",
      "Epoch: 55 | Batch_idx: 350 |  Loss_1: (0.4594) | Acc_1: (83.10%) (37334/44928)\n",
      "Epoch: 55 | Batch_idx: 360 |  Loss_1: (0.4605) | Acc_1: (83.03%) (38365/46208)\n",
      "Epoch: 55 | Batch_idx: 370 |  Loss_1: (0.4599) | Acc_1: (83.07%) (39447/47488)\n",
      "Epoch: 55 | Batch_idx: 380 |  Loss_1: (0.4611) | Acc_1: (83.02%) (40488/48768)\n",
      "Epoch: 55 | Batch_idx: 390 |  Loss_1: (0.4603) | Acc_1: (83.05%) (41523/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4799) | Acc: (87.04%) (8704/10000)\n",
      "Epoch: 56 | Batch_idx: 0 |  Loss_1: (0.5670) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 56 | Batch_idx: 10 |  Loss_1: (0.4622) | Acc_1: (82.67%) (1164/1408)\n",
      "Epoch: 56 | Batch_idx: 20 |  Loss_1: (0.4774) | Acc_1: (82.03%) (2205/2688)\n",
      "Epoch: 56 | Batch_idx: 30 |  Loss_1: (0.4713) | Acc_1: (82.18%) (3261/3968)\n",
      "Epoch: 56 | Batch_idx: 40 |  Loss_1: (0.4637) | Acc_1: (82.47%) (4328/5248)\n",
      "Epoch: 56 | Batch_idx: 50 |  Loss_1: (0.4601) | Acc_1: (82.75%) (5402/6528)\n",
      "Epoch: 56 | Batch_idx: 60 |  Loss_1: (0.4581) | Acc_1: (82.97%) (6478/7808)\n",
      "Epoch: 56 | Batch_idx: 70 |  Loss_1: (0.4581) | Acc_1: (83.08%) (7550/9088)\n",
      "Epoch: 56 | Batch_idx: 80 |  Loss_1: (0.4618) | Acc_1: (82.97%) (8602/10368)\n",
      "Epoch: 56 | Batch_idx: 90 |  Loss_1: (0.4632) | Acc_1: (82.98%) (9665/11648)\n",
      "Epoch: 56 | Batch_idx: 100 |  Loss_1: (0.4615) | Acc_1: (83.02%) (10733/12928)\n",
      "Epoch: 56 | Batch_idx: 110 |  Loss_1: (0.4622) | Acc_1: (82.99%) (11791/14208)\n",
      "Epoch: 56 | Batch_idx: 120 |  Loss_1: (0.4634) | Acc_1: (82.97%) (12850/15488)\n",
      "Epoch: 56 | Batch_idx: 130 |  Loss_1: (0.4646) | Acc_1: (82.93%) (13906/16768)\n",
      "Epoch: 56 | Batch_idx: 140 |  Loss_1: (0.4639) | Acc_1: (82.91%) (14964/18048)\n",
      "Epoch: 56 | Batch_idx: 150 |  Loss_1: (0.4630) | Acc_1: (82.97%) (16036/19328)\n",
      "Epoch: 56 | Batch_idx: 160 |  Loss_1: (0.4616) | Acc_1: (83.02%) (17109/20608)\n",
      "Epoch: 56 | Batch_idx: 170 |  Loss_1: (0.4636) | Acc_1: (82.94%) (18155/21888)\n",
      "Epoch: 56 | Batch_idx: 180 |  Loss_1: (0.4665) | Acc_1: (82.83%) (19190/23168)\n",
      "Epoch: 56 | Batch_idx: 190 |  Loss_1: (0.4653) | Acc_1: (82.87%) (20259/24448)\n",
      "Epoch: 56 | Batch_idx: 200 |  Loss_1: (0.4657) | Acc_1: (82.82%) (21309/25728)\n",
      "Epoch: 56 | Batch_idx: 210 |  Loss_1: (0.4658) | Acc_1: (82.85%) (22375/27008)\n",
      "Epoch: 56 | Batch_idx: 220 |  Loss_1: (0.4664) | Acc_1: (82.80%) (23423/28288)\n",
      "Epoch: 56 | Batch_idx: 230 |  Loss_1: (0.4674) | Acc_1: (82.77%) (24474/29568)\n",
      "Epoch: 56 | Batch_idx: 240 |  Loss_1: (0.4672) | Acc_1: (82.77%) (25532/30848)\n",
      "Epoch: 56 | Batch_idx: 250 |  Loss_1: (0.4664) | Acc_1: (82.81%) (26606/32128)\n",
      "Epoch: 56 | Batch_idx: 260 |  Loss_1: (0.4661) | Acc_1: (82.84%) (27675/33408)\n",
      "Epoch: 56 | Batch_idx: 270 |  Loss_1: (0.4668) | Acc_1: (82.79%) (28719/34688)\n",
      "Epoch: 56 | Batch_idx: 280 |  Loss_1: (0.4675) | Acc_1: (82.75%) (29765/35968)\n",
      "Epoch: 56 | Batch_idx: 290 |  Loss_1: (0.4678) | Acc_1: (82.74%) (30820/37248)\n",
      "Epoch: 56 | Batch_idx: 300 |  Loss_1: (0.4680) | Acc_1: (82.76%) (31884/38528)\n",
      "Epoch: 56 | Batch_idx: 310 |  Loss_1: (0.4676) | Acc_1: (82.77%) (32948/39808)\n",
      "Epoch: 56 | Batch_idx: 320 |  Loss_1: (0.4685) | Acc_1: (82.73%) (33993/41088)\n",
      "Epoch: 56 | Batch_idx: 330 |  Loss_1: (0.4685) | Acc_1: (82.73%) (35051/42368)\n",
      "Epoch: 56 | Batch_idx: 340 |  Loss_1: (0.4684) | Acc_1: (82.76%) (36121/43648)\n",
      "Epoch: 56 | Batch_idx: 350 |  Loss_1: (0.4673) | Acc_1: (82.78%) (37190/44928)\n",
      "Epoch: 56 | Batch_idx: 360 |  Loss_1: (0.4678) | Acc_1: (82.76%) (38243/46208)\n",
      "Epoch: 56 | Batch_idx: 370 |  Loss_1: (0.4699) | Acc_1: (82.68%) (39262/47488)\n",
      "Epoch: 56 | Batch_idx: 380 |  Loss_1: (0.4697) | Acc_1: (82.67%) (40316/48768)\n",
      "Epoch: 56 | Batch_idx: 390 |  Loss_1: (0.4711) | Acc_1: (82.63%) (41317/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4255) | Acc: (87.10%) (8710/10000)\n",
      "Epoch: 57 | Batch_idx: 0 |  Loss_1: (0.5444) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 57 | Batch_idx: 10 |  Loss_1: (0.4603) | Acc_1: (83.38%) (1174/1408)\n",
      "Epoch: 57 | Batch_idx: 20 |  Loss_1: (0.4561) | Acc_1: (83.67%) (2249/2688)\n",
      "Epoch: 57 | Batch_idx: 30 |  Loss_1: (0.4687) | Acc_1: (82.89%) (3289/3968)\n",
      "Epoch: 57 | Batch_idx: 40 |  Loss_1: (0.4650) | Acc_1: (83.04%) (4358/5248)\n",
      "Epoch: 57 | Batch_idx: 50 |  Loss_1: (0.4619) | Acc_1: (83.27%) (5436/6528)\n",
      "Epoch: 57 | Batch_idx: 60 |  Loss_1: (0.4608) | Acc_1: (83.27%) (6502/7808)\n",
      "Epoch: 57 | Batch_idx: 70 |  Loss_1: (0.4660) | Acc_1: (83.07%) (7549/9088)\n",
      "Epoch: 57 | Batch_idx: 80 |  Loss_1: (0.4648) | Acc_1: (83.03%) (8609/10368)\n",
      "Epoch: 57 | Batch_idx: 90 |  Loss_1: (0.4610) | Acc_1: (83.21%) (9692/11648)\n",
      "Epoch: 57 | Batch_idx: 100 |  Loss_1: (0.4600) | Acc_1: (83.19%) (10755/12928)\n",
      "Epoch: 57 | Batch_idx: 110 |  Loss_1: (0.4627) | Acc_1: (83.09%) (11805/14208)\n",
      "Epoch: 57 | Batch_idx: 120 |  Loss_1: (0.4647) | Acc_1: (82.98%) (12852/15488)\n",
      "Epoch: 57 | Batch_idx: 130 |  Loss_1: (0.4683) | Acc_1: (82.86%) (13894/16768)\n",
      "Epoch: 57 | Batch_idx: 140 |  Loss_1: (0.4720) | Acc_1: (82.69%) (14923/18048)\n",
      "Epoch: 57 | Batch_idx: 150 |  Loss_1: (0.4722) | Acc_1: (82.69%) (15983/19328)\n",
      "Epoch: 57 | Batch_idx: 160 |  Loss_1: (0.4746) | Acc_1: (82.58%) (17019/20608)\n",
      "Epoch: 57 | Batch_idx: 170 |  Loss_1: (0.4757) | Acc_1: (82.55%) (18068/21888)\n",
      "Epoch: 57 | Batch_idx: 180 |  Loss_1: (0.4736) | Acc_1: (82.65%) (19148/23168)\n",
      "Epoch: 57 | Batch_idx: 190 |  Loss_1: (0.4730) | Acc_1: (82.68%) (20214/24448)\n",
      "Epoch: 57 | Batch_idx: 200 |  Loss_1: (0.4714) | Acc_1: (82.75%) (21291/25728)\n",
      "Epoch: 57 | Batch_idx: 210 |  Loss_1: (0.4702) | Acc_1: (82.76%) (22352/27008)\n",
      "Epoch: 57 | Batch_idx: 220 |  Loss_1: (0.4718) | Acc_1: (82.71%) (23396/28288)\n",
      "Epoch: 57 | Batch_idx: 230 |  Loss_1: (0.4730) | Acc_1: (82.68%) (24448/29568)\n",
      "Epoch: 57 | Batch_idx: 240 |  Loss_1: (0.4721) | Acc_1: (82.73%) (25521/30848)\n",
      "Epoch: 57 | Batch_idx: 250 |  Loss_1: (0.4736) | Acc_1: (82.68%) (26562/32128)\n",
      "Epoch: 57 | Batch_idx: 260 |  Loss_1: (0.4743) | Acc_1: (82.67%) (27617/33408)\n",
      "Epoch: 57 | Batch_idx: 270 |  Loss_1: (0.4742) | Acc_1: (82.67%) (28677/34688)\n",
      "Epoch: 57 | Batch_idx: 280 |  Loss_1: (0.4742) | Acc_1: (82.68%) (29739/35968)\n",
      "Epoch: 57 | Batch_idx: 290 |  Loss_1: (0.4747) | Acc_1: (82.64%) (30781/37248)\n",
      "Epoch: 57 | Batch_idx: 300 |  Loss_1: (0.4749) | Acc_1: (82.62%) (31832/38528)\n",
      "Epoch: 57 | Batch_idx: 310 |  Loss_1: (0.4767) | Acc_1: (82.57%) (32870/39808)\n",
      "Epoch: 57 | Batch_idx: 320 |  Loss_1: (0.4774) | Acc_1: (82.53%) (33911/41088)\n",
      "Epoch: 57 | Batch_idx: 330 |  Loss_1: (0.4774) | Acc_1: (82.53%) (34966/42368)\n",
      "Epoch: 57 | Batch_idx: 340 |  Loss_1: (0.4786) | Acc_1: (82.50%) (36008/43648)\n",
      "Epoch: 57 | Batch_idx: 350 |  Loss_1: (0.4784) | Acc_1: (82.48%) (37058/44928)\n",
      "Epoch: 57 | Batch_idx: 360 |  Loss_1: (0.4797) | Acc_1: (82.43%) (38088/46208)\n",
      "Epoch: 57 | Batch_idx: 370 |  Loss_1: (0.4788) | Acc_1: (82.47%) (39163/47488)\n",
      "Epoch: 57 | Batch_idx: 380 |  Loss_1: (0.4783) | Acc_1: (82.49%) (40230/48768)\n",
      "Epoch: 57 | Batch_idx: 390 |  Loss_1: (0.4782) | Acc_1: (82.48%) (41242/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4444) | Acc: (88.34%) (8834/10000)\n",
      "Epoch: 58 | Batch_idx: 0 |  Loss_1: (0.5426) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 58 | Batch_idx: 10 |  Loss_1: (0.4397) | Acc_1: (83.95%) (1182/1408)\n",
      "Epoch: 58 | Batch_idx: 20 |  Loss_1: (0.4443) | Acc_1: (83.67%) (2249/2688)\n",
      "Epoch: 58 | Batch_idx: 30 |  Loss_1: (0.4457) | Acc_1: (83.57%) (3316/3968)\n",
      "Epoch: 58 | Batch_idx: 40 |  Loss_1: (0.4543) | Acc_1: (83.33%) (4373/5248)\n",
      "Epoch: 58 | Batch_idx: 50 |  Loss_1: (0.4458) | Acc_1: (83.70%) (5464/6528)\n",
      "Epoch: 58 | Batch_idx: 60 |  Loss_1: (0.4542) | Acc_1: (83.43%) (6514/7808)\n",
      "Epoch: 58 | Batch_idx: 70 |  Loss_1: (0.4603) | Acc_1: (83.12%) (7554/9088)\n",
      "Epoch: 58 | Batch_idx: 80 |  Loss_1: (0.4576) | Acc_1: (83.20%) (8626/10368)\n",
      "Epoch: 58 | Batch_idx: 90 |  Loss_1: (0.4582) | Acc_1: (83.17%) (9688/11648)\n",
      "Epoch: 58 | Batch_idx: 100 |  Loss_1: (0.4604) | Acc_1: (83.07%) (10739/12928)\n",
      "Epoch: 58 | Batch_idx: 110 |  Loss_1: (0.4585) | Acc_1: (83.13%) (11811/14208)\n",
      "Epoch: 58 | Batch_idx: 120 |  Loss_1: (0.4574) | Acc_1: (83.21%) (12887/15488)\n",
      "Epoch: 58 | Batch_idx: 130 |  Loss_1: (0.4601) | Acc_1: (83.17%) (13946/16768)\n",
      "Epoch: 58 | Batch_idx: 140 |  Loss_1: (0.4618) | Acc_1: (83.09%) (14996/18048)\n",
      "Epoch: 58 | Batch_idx: 150 |  Loss_1: (0.4629) | Acc_1: (83.05%) (16052/19328)\n",
      "Epoch: 58 | Batch_idx: 160 |  Loss_1: (0.4647) | Acc_1: (83.01%) (17107/20608)\n",
      "Epoch: 58 | Batch_idx: 170 |  Loss_1: (0.4647) | Acc_1: (83.01%) (18170/21888)\n",
      "Epoch: 58 | Batch_idx: 180 |  Loss_1: (0.4634) | Acc_1: (83.04%) (19239/23168)\n",
      "Epoch: 58 | Batch_idx: 190 |  Loss_1: (0.4629) | Acc_1: (83.02%) (20296/24448)\n",
      "Epoch: 58 | Batch_idx: 200 |  Loss_1: (0.4644) | Acc_1: (82.99%) (21351/25728)\n",
      "Epoch: 58 | Batch_idx: 210 |  Loss_1: (0.4641) | Acc_1: (82.99%) (22415/27008)\n",
      "Epoch: 58 | Batch_idx: 220 |  Loss_1: (0.4635) | Acc_1: (83.01%) (23482/28288)\n",
      "Epoch: 58 | Batch_idx: 230 |  Loss_1: (0.4644) | Acc_1: (82.94%) (24524/29568)\n",
      "Epoch: 58 | Batch_idx: 240 |  Loss_1: (0.4643) | Acc_1: (82.94%) (25586/30848)\n",
      "Epoch: 58 | Batch_idx: 250 |  Loss_1: (0.4642) | Acc_1: (82.92%) (26641/32128)\n",
      "Epoch: 58 | Batch_idx: 260 |  Loss_1: (0.4651) | Acc_1: (82.93%) (27704/33408)\n",
      "Epoch: 58 | Batch_idx: 270 |  Loss_1: (0.4668) | Acc_1: (82.86%) (28742/34688)\n",
      "Epoch: 58 | Batch_idx: 280 |  Loss_1: (0.4668) | Acc_1: (82.85%) (29800/35968)\n",
      "Epoch: 58 | Batch_idx: 290 |  Loss_1: (0.4671) | Acc_1: (82.83%) (30852/37248)\n",
      "Epoch: 58 | Batch_idx: 300 |  Loss_1: (0.4669) | Acc_1: (82.83%) (31913/38528)\n",
      "Epoch: 58 | Batch_idx: 310 |  Loss_1: (0.4678) | Acc_1: (82.81%) (32964/39808)\n",
      "Epoch: 58 | Batch_idx: 320 |  Loss_1: (0.4688) | Acc_1: (82.79%) (34015/41088)\n",
      "Epoch: 58 | Batch_idx: 330 |  Loss_1: (0.4687) | Acc_1: (82.78%) (35074/42368)\n",
      "Epoch: 58 | Batch_idx: 340 |  Loss_1: (0.4695) | Acc_1: (82.77%) (36126/43648)\n",
      "Epoch: 58 | Batch_idx: 350 |  Loss_1: (0.4694) | Acc_1: (82.78%) (37191/44928)\n",
      "Epoch: 58 | Batch_idx: 360 |  Loss_1: (0.4707) | Acc_1: (82.74%) (38231/46208)\n",
      "Epoch: 58 | Batch_idx: 370 |  Loss_1: (0.4704) | Acc_1: (82.76%) (39300/47488)\n",
      "Epoch: 58 | Batch_idx: 380 |  Loss_1: (0.4704) | Acc_1: (82.76%) (40358/48768)\n",
      "Epoch: 58 | Batch_idx: 390 |  Loss_1: (0.4696) | Acc_1: (82.78%) (41388/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4353) | Acc: (88.41%) (8841/10000)\n",
      "Epoch: 59 | Batch_idx: 0 |  Loss_1: (0.3677) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 59 | Batch_idx: 10 |  Loss_1: (0.4902) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 59 | Batch_idx: 20 |  Loss_1: (0.4759) | Acc_1: (82.33%) (2213/2688)\n",
      "Epoch: 59 | Batch_idx: 30 |  Loss_1: (0.4621) | Acc_1: (82.74%) (3283/3968)\n",
      "Epoch: 59 | Batch_idx: 40 |  Loss_1: (0.4608) | Acc_1: (82.85%) (4348/5248)\n",
      "Epoch: 59 | Batch_idx: 50 |  Loss_1: (0.4582) | Acc_1: (82.90%) (5412/6528)\n",
      "Epoch: 59 | Batch_idx: 60 |  Loss_1: (0.4589) | Acc_1: (82.93%) (6475/7808)\n",
      "Epoch: 59 | Batch_idx: 70 |  Loss_1: (0.4599) | Acc_1: (82.88%) (7532/9088)\n",
      "Epoch: 59 | Batch_idx: 80 |  Loss_1: (0.4604) | Acc_1: (82.87%) (8592/10368)\n",
      "Epoch: 59 | Batch_idx: 90 |  Loss_1: (0.4612) | Acc_1: (82.91%) (9657/11648)\n",
      "Epoch: 59 | Batch_idx: 100 |  Loss_1: (0.4604) | Acc_1: (82.95%) (10724/12928)\n",
      "Epoch: 59 | Batch_idx: 110 |  Loss_1: (0.4600) | Acc_1: (82.98%) (11790/14208)\n",
      "Epoch: 59 | Batch_idx: 120 |  Loss_1: (0.4612) | Acc_1: (82.92%) (12843/15488)\n",
      "Epoch: 59 | Batch_idx: 130 |  Loss_1: (0.4614) | Acc_1: (82.93%) (13905/16768)\n",
      "Epoch: 59 | Batch_idx: 140 |  Loss_1: (0.4602) | Acc_1: (83.00%) (14980/18048)\n",
      "Epoch: 59 | Batch_idx: 150 |  Loss_1: (0.4613) | Acc_1: (82.98%) (16038/19328)\n",
      "Epoch: 59 | Batch_idx: 160 |  Loss_1: (0.4614) | Acc_1: (82.92%) (17089/20608)\n",
      "Epoch: 59 | Batch_idx: 170 |  Loss_1: (0.4588) | Acc_1: (83.02%) (18172/21888)\n",
      "Epoch: 59 | Batch_idx: 180 |  Loss_1: (0.4584) | Acc_1: (83.04%) (19238/23168)\n",
      "Epoch: 59 | Batch_idx: 190 |  Loss_1: (0.4611) | Acc_1: (82.92%) (20272/24448)\n",
      "Epoch: 59 | Batch_idx: 200 |  Loss_1: (0.4612) | Acc_1: (82.94%) (21339/25728)\n",
      "Epoch: 59 | Batch_idx: 210 |  Loss_1: (0.4617) | Acc_1: (82.90%) (22390/27008)\n",
      "Epoch: 59 | Batch_idx: 220 |  Loss_1: (0.4622) | Acc_1: (82.89%) (23448/28288)\n",
      "Epoch: 59 | Batch_idx: 230 |  Loss_1: (0.4641) | Acc_1: (82.80%) (24482/29568)\n",
      "Epoch: 59 | Batch_idx: 240 |  Loss_1: (0.4646) | Acc_1: (82.75%) (25528/30848)\n",
      "Epoch: 59 | Batch_idx: 250 |  Loss_1: (0.4643) | Acc_1: (82.76%) (26590/32128)\n",
      "Epoch: 59 | Batch_idx: 260 |  Loss_1: (0.4646) | Acc_1: (82.76%) (27650/33408)\n",
      "Epoch: 59 | Batch_idx: 270 |  Loss_1: (0.4654) | Acc_1: (82.74%) (28701/34688)\n",
      "Epoch: 59 | Batch_idx: 280 |  Loss_1: (0.4645) | Acc_1: (82.78%) (29776/35968)\n",
      "Epoch: 59 | Batch_idx: 290 |  Loss_1: (0.4657) | Acc_1: (82.77%) (30831/37248)\n",
      "Epoch: 59 | Batch_idx: 300 |  Loss_1: (0.4676) | Acc_1: (82.72%) (31871/38528)\n",
      "Epoch: 59 | Batch_idx: 310 |  Loss_1: (0.4689) | Acc_1: (82.69%) (32919/39808)\n",
      "Epoch: 59 | Batch_idx: 320 |  Loss_1: (0.4689) | Acc_1: (82.69%) (33975/41088)\n",
      "Epoch: 59 | Batch_idx: 330 |  Loss_1: (0.4686) | Acc_1: (82.70%) (35039/42368)\n",
      "Epoch: 59 | Batch_idx: 340 |  Loss_1: (0.4681) | Acc_1: (82.71%) (36101/43648)\n",
      "Epoch: 59 | Batch_idx: 350 |  Loss_1: (0.4690) | Acc_1: (82.67%) (37142/44928)\n",
      "Epoch: 59 | Batch_idx: 360 |  Loss_1: (0.4696) | Acc_1: (82.65%) (38192/46208)\n",
      "Epoch: 59 | Batch_idx: 370 |  Loss_1: (0.4684) | Acc_1: (82.72%) (39282/47488)\n",
      "Epoch: 59 | Batch_idx: 380 |  Loss_1: (0.4685) | Acc_1: (82.71%) (40338/48768)\n",
      "Epoch: 59 | Batch_idx: 390 |  Loss_1: (0.4681) | Acc_1: (82.73%) (41365/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4756) | Acc: (87.72%) (8772/10000)\n",
      "Epoch: 60 | Batch_idx: 0 |  Loss_1: (0.4567) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 60 | Batch_idx: 10 |  Loss_1: (0.4608) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 60 | Batch_idx: 20 |  Loss_1: (0.4583) | Acc_1: (83.37%) (2241/2688)\n",
      "Epoch: 60 | Batch_idx: 30 |  Loss_1: (0.4616) | Acc_1: (83.22%) (3302/3968)\n",
      "Epoch: 60 | Batch_idx: 40 |  Loss_1: (0.4754) | Acc_1: (82.66%) (4338/5248)\n",
      "Epoch: 60 | Batch_idx: 50 |  Loss_1: (0.4760) | Acc_1: (82.66%) (5396/6528)\n",
      "Epoch: 60 | Batch_idx: 60 |  Loss_1: (0.4688) | Acc_1: (82.93%) (6475/7808)\n",
      "Epoch: 60 | Batch_idx: 70 |  Loss_1: (0.4649) | Acc_1: (83.08%) (7550/9088)\n",
      "Epoch: 60 | Batch_idx: 80 |  Loss_1: (0.4659) | Acc_1: (83.05%) (8611/10368)\n",
      "Epoch: 60 | Batch_idx: 90 |  Loss_1: (0.4619) | Acc_1: (83.20%) (9691/11648)\n",
      "Epoch: 60 | Batch_idx: 100 |  Loss_1: (0.4627) | Acc_1: (83.11%) (10745/12928)\n",
      "Epoch: 60 | Batch_idx: 110 |  Loss_1: (0.4579) | Acc_1: (83.33%) (11840/14208)\n",
      "Epoch: 60 | Batch_idx: 120 |  Loss_1: (0.4574) | Acc_1: (83.32%) (12904/15488)\n",
      "Epoch: 60 | Batch_idx: 130 |  Loss_1: (0.4552) | Acc_1: (83.41%) (13987/16768)\n",
      "Epoch: 60 | Batch_idx: 140 |  Loss_1: (0.4510) | Acc_1: (83.53%) (15076/18048)\n",
      "Epoch: 60 | Batch_idx: 150 |  Loss_1: (0.4523) | Acc_1: (83.46%) (16132/19328)\n",
      "Epoch: 60 | Batch_idx: 160 |  Loss_1: (0.4490) | Acc_1: (83.60%) (17229/20608)\n",
      "Epoch: 60 | Batch_idx: 170 |  Loss_1: (0.4507) | Acc_1: (83.57%) (18291/21888)\n",
      "Epoch: 60 | Batch_idx: 180 |  Loss_1: (0.4532) | Acc_1: (83.46%) (19335/23168)\n",
      "Epoch: 60 | Batch_idx: 190 |  Loss_1: (0.4523) | Acc_1: (83.49%) (20412/24448)\n",
      "Epoch: 60 | Batch_idx: 200 |  Loss_1: (0.4526) | Acc_1: (83.46%) (21473/25728)\n",
      "Epoch: 60 | Batch_idx: 210 |  Loss_1: (0.4543) | Acc_1: (83.40%) (22524/27008)\n",
      "Epoch: 60 | Batch_idx: 220 |  Loss_1: (0.4560) | Acc_1: (83.33%) (23571/28288)\n",
      "Epoch: 60 | Batch_idx: 230 |  Loss_1: (0.4590) | Acc_1: (83.24%) (24612/29568)\n",
      "Epoch: 60 | Batch_idx: 240 |  Loss_1: (0.4597) | Acc_1: (83.20%) (25667/30848)\n",
      "Epoch: 60 | Batch_idx: 250 |  Loss_1: (0.4609) | Acc_1: (83.15%) (26713/32128)\n",
      "Epoch: 60 | Batch_idx: 260 |  Loss_1: (0.4603) | Acc_1: (83.17%) (27785/33408)\n",
      "Epoch: 60 | Batch_idx: 270 |  Loss_1: (0.4616) | Acc_1: (83.11%) (28829/34688)\n",
      "Epoch: 60 | Batch_idx: 280 |  Loss_1: (0.4637) | Acc_1: (83.03%) (29863/35968)\n",
      "Epoch: 60 | Batch_idx: 290 |  Loss_1: (0.4638) | Acc_1: (83.02%) (30923/37248)\n",
      "Epoch: 60 | Batch_idx: 300 |  Loss_1: (0.4646) | Acc_1: (82.99%) (31976/38528)\n",
      "Epoch: 60 | Batch_idx: 310 |  Loss_1: (0.4641) | Acc_1: (83.00%) (33040/39808)\n",
      "Epoch: 60 | Batch_idx: 320 |  Loss_1: (0.4642) | Acc_1: (82.99%) (34099/41088)\n",
      "Epoch: 60 | Batch_idx: 330 |  Loss_1: (0.4632) | Acc_1: (83.02%) (35174/42368)\n",
      "Epoch: 60 | Batch_idx: 340 |  Loss_1: (0.4633) | Acc_1: (83.02%) (36237/43648)\n",
      "Epoch: 60 | Batch_idx: 350 |  Loss_1: (0.4644) | Acc_1: (83.00%) (37288/44928)\n",
      "Epoch: 60 | Batch_idx: 360 |  Loss_1: (0.4644) | Acc_1: (82.98%) (38345/46208)\n",
      "Epoch: 60 | Batch_idx: 370 |  Loss_1: (0.4653) | Acc_1: (82.93%) (39383/47488)\n",
      "Epoch: 60 | Batch_idx: 380 |  Loss_1: (0.4655) | Acc_1: (82.93%) (40442/48768)\n",
      "Epoch: 60 | Batch_idx: 390 |  Loss_1: (0.4665) | Acc_1: (82.90%) (41449/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4705) | Acc: (87.71%) (8771/10000)\n",
      "Epoch: 61 | Batch_idx: 0 |  Loss_1: (0.3273) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 61 | Batch_idx: 10 |  Loss_1: (0.4271) | Acc_1: (83.59%) (1177/1408)\n",
      "Epoch: 61 | Batch_idx: 20 |  Loss_1: (0.4352) | Acc_1: (83.56%) (2246/2688)\n",
      "Epoch: 61 | Batch_idx: 30 |  Loss_1: (0.4472) | Acc_1: (83.09%) (3297/3968)\n",
      "Epoch: 61 | Batch_idx: 40 |  Loss_1: (0.4559) | Acc_1: (82.98%) (4355/5248)\n",
      "Epoch: 61 | Batch_idx: 50 |  Loss_1: (0.4631) | Acc_1: (82.83%) (5407/6528)\n",
      "Epoch: 61 | Batch_idx: 60 |  Loss_1: (0.4585) | Acc_1: (83.03%) (6483/7808)\n",
      "Epoch: 61 | Batch_idx: 70 |  Loss_1: (0.4595) | Acc_1: (83.07%) (7549/9088)\n",
      "Epoch: 61 | Batch_idx: 80 |  Loss_1: (0.4621) | Acc_1: (82.93%) (8598/10368)\n",
      "Epoch: 61 | Batch_idx: 90 |  Loss_1: (0.4631) | Acc_1: (82.93%) (9660/11648)\n",
      "Epoch: 61 | Batch_idx: 100 |  Loss_1: (0.4611) | Acc_1: (82.95%) (10724/12928)\n",
      "Epoch: 61 | Batch_idx: 110 |  Loss_1: (0.4612) | Acc_1: (83.00%) (11792/14208)\n",
      "Epoch: 61 | Batch_idx: 120 |  Loss_1: (0.4601) | Acc_1: (83.01%) (12856/15488)\n",
      "Epoch: 61 | Batch_idx: 130 |  Loss_1: (0.4576) | Acc_1: (83.13%) (13939/16768)\n",
      "Epoch: 61 | Batch_idx: 140 |  Loss_1: (0.4584) | Acc_1: (83.13%) (15003/18048)\n",
      "Epoch: 61 | Batch_idx: 150 |  Loss_1: (0.4598) | Acc_1: (83.03%) (16049/19328)\n",
      "Epoch: 61 | Batch_idx: 160 |  Loss_1: (0.4581) | Acc_1: (83.07%) (17120/20608)\n",
      "Epoch: 61 | Batch_idx: 170 |  Loss_1: (0.4589) | Acc_1: (83.05%) (18178/21888)\n",
      "Epoch: 61 | Batch_idx: 180 |  Loss_1: (0.4586) | Acc_1: (83.07%) (19245/23168)\n",
      "Epoch: 61 | Batch_idx: 190 |  Loss_1: (0.4598) | Acc_1: (83.03%) (20298/24448)\n",
      "Epoch: 61 | Batch_idx: 200 |  Loss_1: (0.4598) | Acc_1: (83.06%) (21369/25728)\n",
      "Epoch: 61 | Batch_idx: 210 |  Loss_1: (0.4631) | Acc_1: (82.94%) (22401/27008)\n",
      "Epoch: 61 | Batch_idx: 220 |  Loss_1: (0.4650) | Acc_1: (82.86%) (23440/28288)\n",
      "Epoch: 61 | Batch_idx: 230 |  Loss_1: (0.4640) | Acc_1: (82.89%) (24509/29568)\n",
      "Epoch: 61 | Batch_idx: 240 |  Loss_1: (0.4630) | Acc_1: (82.91%) (25576/30848)\n",
      "Epoch: 61 | Batch_idx: 250 |  Loss_1: (0.4625) | Acc_1: (82.91%) (26637/32128)\n",
      "Epoch: 61 | Batch_idx: 260 |  Loss_1: (0.4614) | Acc_1: (82.96%) (27716/33408)\n",
      "Epoch: 61 | Batch_idx: 270 |  Loss_1: (0.4616) | Acc_1: (82.96%) (28778/34688)\n",
      "Epoch: 61 | Batch_idx: 280 |  Loss_1: (0.4606) | Acc_1: (83.01%) (29858/35968)\n",
      "Epoch: 61 | Batch_idx: 290 |  Loss_1: (0.4626) | Acc_1: (82.95%) (30898/37248)\n",
      "Epoch: 61 | Batch_idx: 300 |  Loss_1: (0.4617) | Acc_1: (83.00%) (31978/38528)\n",
      "Epoch: 61 | Batch_idx: 310 |  Loss_1: (0.4610) | Acc_1: (83.01%) (33044/39808)\n",
      "Epoch: 61 | Batch_idx: 320 |  Loss_1: (0.4611) | Acc_1: (83.00%) (34101/41088)\n",
      "Epoch: 61 | Batch_idx: 330 |  Loss_1: (0.4609) | Acc_1: (82.98%) (35156/42368)\n",
      "Epoch: 61 | Batch_idx: 340 |  Loss_1: (0.4639) | Acc_1: (82.89%) (36180/43648)\n",
      "Epoch: 61 | Batch_idx: 350 |  Loss_1: (0.4638) | Acc_1: (82.88%) (37238/44928)\n",
      "Epoch: 61 | Batch_idx: 360 |  Loss_1: (0.4642) | Acc_1: (82.88%) (38299/46208)\n",
      "Epoch: 61 | Batch_idx: 370 |  Loss_1: (0.4633) | Acc_1: (82.91%) (39373/47488)\n",
      "Epoch: 61 | Batch_idx: 380 |  Loss_1: (0.4628) | Acc_1: (82.93%) (40444/48768)\n",
      "Epoch: 61 | Batch_idx: 390 |  Loss_1: (0.4633) | Acc_1: (82.91%) (41453/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4248) | Acc: (88.35%) (8835/10000)\n",
      "Epoch: 62 | Batch_idx: 0 |  Loss_1: (0.5133) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 62 | Batch_idx: 10 |  Loss_1: (0.4511) | Acc_1: (83.45%) (1175/1408)\n",
      "Epoch: 62 | Batch_idx: 20 |  Loss_1: (0.4579) | Acc_1: (82.81%) (2226/2688)\n",
      "Epoch: 62 | Batch_idx: 30 |  Loss_1: (0.4570) | Acc_1: (83.01%) (3294/3968)\n",
      "Epoch: 62 | Batch_idx: 40 |  Loss_1: (0.4533) | Acc_1: (83.06%) (4359/5248)\n",
      "Epoch: 62 | Batch_idx: 50 |  Loss_1: (0.4417) | Acc_1: (83.66%) (5461/6528)\n",
      "Epoch: 62 | Batch_idx: 60 |  Loss_1: (0.4447) | Acc_1: (83.52%) (6521/7808)\n",
      "Epoch: 62 | Batch_idx: 70 |  Loss_1: (0.4502) | Acc_1: (83.37%) (7577/9088)\n",
      "Epoch: 62 | Batch_idx: 80 |  Loss_1: (0.4567) | Acc_1: (83.17%) (8623/10368)\n",
      "Epoch: 62 | Batch_idx: 90 |  Loss_1: (0.4528) | Acc_1: (83.41%) (9716/11648)\n",
      "Epoch: 62 | Batch_idx: 100 |  Loss_1: (0.4481) | Acc_1: (83.57%) (10804/12928)\n",
      "Epoch: 62 | Batch_idx: 110 |  Loss_1: (0.4562) | Acc_1: (83.31%) (11836/14208)\n",
      "Epoch: 62 | Batch_idx: 120 |  Loss_1: (0.4562) | Acc_1: (83.29%) (12900/15488)\n",
      "Epoch: 62 | Batch_idx: 130 |  Loss_1: (0.4553) | Acc_1: (83.33%) (13973/16768)\n",
      "Epoch: 62 | Batch_idx: 140 |  Loss_1: (0.4548) | Acc_1: (83.33%) (15040/18048)\n",
      "Epoch: 62 | Batch_idx: 150 |  Loss_1: (0.4538) | Acc_1: (83.37%) (16114/19328)\n",
      "Epoch: 62 | Batch_idx: 160 |  Loss_1: (0.4546) | Acc_1: (83.36%) (17178/20608)\n",
      "Epoch: 62 | Batch_idx: 170 |  Loss_1: (0.4563) | Acc_1: (83.33%) (18240/21888)\n",
      "Epoch: 62 | Batch_idx: 180 |  Loss_1: (0.4572) | Acc_1: (83.30%) (19300/23168)\n",
      "Epoch: 62 | Batch_idx: 190 |  Loss_1: (0.4575) | Acc_1: (83.28%) (20360/24448)\n",
      "Epoch: 62 | Batch_idx: 200 |  Loss_1: (0.4575) | Acc_1: (83.27%) (21424/25728)\n",
      "Epoch: 62 | Batch_idx: 210 |  Loss_1: (0.4580) | Acc_1: (83.26%) (22486/27008)\n",
      "Epoch: 62 | Batch_idx: 220 |  Loss_1: (0.4575) | Acc_1: (83.26%) (23552/28288)\n",
      "Epoch: 62 | Batch_idx: 230 |  Loss_1: (0.4574) | Acc_1: (83.27%) (24621/29568)\n",
      "Epoch: 62 | Batch_idx: 240 |  Loss_1: (0.4588) | Acc_1: (83.22%) (25671/30848)\n",
      "Epoch: 62 | Batch_idx: 250 |  Loss_1: (0.4605) | Acc_1: (83.13%) (26709/32128)\n",
      "Epoch: 62 | Batch_idx: 260 |  Loss_1: (0.4590) | Acc_1: (83.17%) (27787/33408)\n",
      "Epoch: 62 | Batch_idx: 270 |  Loss_1: (0.4581) | Acc_1: (83.20%) (28861/34688)\n",
      "Epoch: 62 | Batch_idx: 280 |  Loss_1: (0.4576) | Acc_1: (83.22%) (29931/35968)\n",
      "Epoch: 62 | Batch_idx: 290 |  Loss_1: (0.4587) | Acc_1: (83.15%) (30973/37248)\n",
      "Epoch: 62 | Batch_idx: 300 |  Loss_1: (0.4604) | Acc_1: (83.09%) (32013/38528)\n",
      "Epoch: 62 | Batch_idx: 310 |  Loss_1: (0.4605) | Acc_1: (83.10%) (33079/39808)\n",
      "Epoch: 62 | Batch_idx: 320 |  Loss_1: (0.4592) | Acc_1: (83.16%) (34168/41088)\n",
      "Epoch: 62 | Batch_idx: 330 |  Loss_1: (0.4602) | Acc_1: (83.11%) (35211/42368)\n",
      "Epoch: 62 | Batch_idx: 340 |  Loss_1: (0.4598) | Acc_1: (83.12%) (36281/43648)\n",
      "Epoch: 62 | Batch_idx: 350 |  Loss_1: (0.4594) | Acc_1: (83.14%) (37352/44928)\n",
      "Epoch: 62 | Batch_idx: 360 |  Loss_1: (0.4594) | Acc_1: (83.14%) (38418/46208)\n",
      "Epoch: 62 | Batch_idx: 370 |  Loss_1: (0.4598) | Acc_1: (83.13%) (39479/47488)\n",
      "Epoch: 62 | Batch_idx: 380 |  Loss_1: (0.4595) | Acc_1: (83.14%) (40548/48768)\n",
      "Epoch: 62 | Batch_idx: 390 |  Loss_1: (0.4588) | Acc_1: (83.16%) (41582/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4203) | Acc: (88.60%) (8860/10000)\n",
      "Epoch: 63 | Batch_idx: 0 |  Loss_1: (0.4146) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 63 | Batch_idx: 10 |  Loss_1: (0.4034) | Acc_1: (85.37%) (1202/1408)\n",
      "Epoch: 63 | Batch_idx: 20 |  Loss_1: (0.4289) | Acc_1: (84.26%) (2265/2688)\n",
      "Epoch: 63 | Batch_idx: 30 |  Loss_1: (0.4371) | Acc_1: (83.92%) (3330/3968)\n",
      "Epoch: 63 | Batch_idx: 40 |  Loss_1: (0.4342) | Acc_1: (84.07%) (4412/5248)\n",
      "Epoch: 63 | Batch_idx: 50 |  Loss_1: (0.4322) | Acc_1: (84.24%) (5499/6528)\n",
      "Epoch: 63 | Batch_idx: 60 |  Loss_1: (0.4420) | Acc_1: (83.95%) (6555/7808)\n",
      "Epoch: 63 | Batch_idx: 70 |  Loss_1: (0.4429) | Acc_1: (83.88%) (7623/9088)\n",
      "Epoch: 63 | Batch_idx: 80 |  Loss_1: (0.4442) | Acc_1: (83.79%) (8687/10368)\n",
      "Epoch: 63 | Batch_idx: 90 |  Loss_1: (0.4422) | Acc_1: (83.82%) (9763/11648)\n",
      "Epoch: 63 | Batch_idx: 100 |  Loss_1: (0.4441) | Acc_1: (83.75%) (10827/12928)\n",
      "Epoch: 63 | Batch_idx: 110 |  Loss_1: (0.4470) | Acc_1: (83.59%) (11877/14208)\n",
      "Epoch: 63 | Batch_idx: 120 |  Loss_1: (0.4469) | Acc_1: (83.59%) (12947/15488)\n",
      "Epoch: 63 | Batch_idx: 130 |  Loss_1: (0.4471) | Acc_1: (83.58%) (14015/16768)\n",
      "Epoch: 63 | Batch_idx: 140 |  Loss_1: (0.4456) | Acc_1: (83.64%) (15096/18048)\n",
      "Epoch: 63 | Batch_idx: 150 |  Loss_1: (0.4481) | Acc_1: (83.56%) (16150/19328)\n",
      "Epoch: 63 | Batch_idx: 160 |  Loss_1: (0.4495) | Acc_1: (83.50%) (17207/20608)\n",
      "Epoch: 63 | Batch_idx: 170 |  Loss_1: (0.4471) | Acc_1: (83.57%) (18291/21888)\n",
      "Epoch: 63 | Batch_idx: 180 |  Loss_1: (0.4459) | Acc_1: (83.60%) (19368/23168)\n",
      "Epoch: 63 | Batch_idx: 190 |  Loss_1: (0.4494) | Acc_1: (83.46%) (20405/24448)\n",
      "Epoch: 63 | Batch_idx: 200 |  Loss_1: (0.4502) | Acc_1: (83.45%) (21470/25728)\n",
      "Epoch: 63 | Batch_idx: 210 |  Loss_1: (0.4524) | Acc_1: (83.35%) (22510/27008)\n",
      "Epoch: 63 | Batch_idx: 220 |  Loss_1: (0.4529) | Acc_1: (83.32%) (23569/28288)\n",
      "Epoch: 63 | Batch_idx: 230 |  Loss_1: (0.4527) | Acc_1: (83.34%) (24641/29568)\n",
      "Epoch: 63 | Batch_idx: 240 |  Loss_1: (0.4519) | Acc_1: (83.36%) (25716/30848)\n",
      "Epoch: 63 | Batch_idx: 250 |  Loss_1: (0.4537) | Acc_1: (83.32%) (26768/32128)\n",
      "Epoch: 63 | Batch_idx: 260 |  Loss_1: (0.4532) | Acc_1: (83.35%) (27847/33408)\n",
      "Epoch: 63 | Batch_idx: 270 |  Loss_1: (0.4552) | Acc_1: (83.29%) (28891/34688)\n",
      "Epoch: 63 | Batch_idx: 280 |  Loss_1: (0.4558) | Acc_1: (83.27%) (29950/35968)\n",
      "Epoch: 63 | Batch_idx: 290 |  Loss_1: (0.4579) | Acc_1: (83.16%) (30975/37248)\n",
      "Epoch: 63 | Batch_idx: 300 |  Loss_1: (0.4593) | Acc_1: (83.13%) (32027/38528)\n",
      "Epoch: 63 | Batch_idx: 310 |  Loss_1: (0.4598) | Acc_1: (83.11%) (33085/39808)\n",
      "Epoch: 63 | Batch_idx: 320 |  Loss_1: (0.4598) | Acc_1: (83.11%) (34148/41088)\n",
      "Epoch: 63 | Batch_idx: 330 |  Loss_1: (0.4604) | Acc_1: (83.11%) (35213/42368)\n",
      "Epoch: 63 | Batch_idx: 340 |  Loss_1: (0.4601) | Acc_1: (83.14%) (36289/43648)\n",
      "Epoch: 63 | Batch_idx: 350 |  Loss_1: (0.4618) | Acc_1: (83.09%) (37330/44928)\n",
      "Epoch: 63 | Batch_idx: 360 |  Loss_1: (0.4621) | Acc_1: (83.09%) (38396/46208)\n",
      "Epoch: 63 | Batch_idx: 370 |  Loss_1: (0.4620) | Acc_1: (83.08%) (39455/47488)\n",
      "Epoch: 63 | Batch_idx: 380 |  Loss_1: (0.4614) | Acc_1: (83.10%) (40527/48768)\n",
      "Epoch: 63 | Batch_idx: 390 |  Loss_1: (0.4600) | Acc_1: (83.15%) (41575/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4179) | Acc: (89.01%) (8901/10000)\n",
      "Epoch: 64 | Batch_idx: 0 |  Loss_1: (0.4302) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 64 | Batch_idx: 10 |  Loss_1: (0.4585) | Acc_1: (83.17%) (1171/1408)\n",
      "Epoch: 64 | Batch_idx: 20 |  Loss_1: (0.4492) | Acc_1: (83.48%) (2244/2688)\n",
      "Epoch: 64 | Batch_idx: 30 |  Loss_1: (0.4504) | Acc_1: (83.37%) (3308/3968)\n",
      "Epoch: 64 | Batch_idx: 40 |  Loss_1: (0.4502) | Acc_1: (83.37%) (4375/5248)\n",
      "Epoch: 64 | Batch_idx: 50 |  Loss_1: (0.4487) | Acc_1: (83.49%) (5450/6528)\n",
      "Epoch: 64 | Batch_idx: 60 |  Loss_1: (0.4570) | Acc_1: (83.17%) (6494/7808)\n",
      "Epoch: 64 | Batch_idx: 70 |  Loss_1: (0.4590) | Acc_1: (83.21%) (7562/9088)\n",
      "Epoch: 64 | Batch_idx: 80 |  Loss_1: (0.4547) | Acc_1: (83.40%) (8647/10368)\n",
      "Epoch: 64 | Batch_idx: 90 |  Loss_1: (0.4537) | Acc_1: (83.38%) (9712/11648)\n",
      "Epoch: 64 | Batch_idx: 100 |  Loss_1: (0.4532) | Acc_1: (83.37%) (10778/12928)\n",
      "Epoch: 64 | Batch_idx: 110 |  Loss_1: (0.4548) | Acc_1: (83.33%) (11839/14208)\n",
      "Epoch: 64 | Batch_idx: 120 |  Loss_1: (0.4580) | Acc_1: (83.20%) (12886/15488)\n",
      "Epoch: 64 | Batch_idx: 130 |  Loss_1: (0.4587) | Acc_1: (83.19%) (13950/16768)\n",
      "Epoch: 64 | Batch_idx: 140 |  Loss_1: (0.4588) | Acc_1: (83.23%) (15021/18048)\n",
      "Epoch: 64 | Batch_idx: 150 |  Loss_1: (0.4622) | Acc_1: (83.10%) (16062/19328)\n",
      "Epoch: 64 | Batch_idx: 160 |  Loss_1: (0.4592) | Acc_1: (83.19%) (17144/20608)\n",
      "Epoch: 64 | Batch_idx: 170 |  Loss_1: (0.4559) | Acc_1: (83.32%) (18237/21888)\n",
      "Epoch: 64 | Batch_idx: 180 |  Loss_1: (0.4568) | Acc_1: (83.27%) (19292/23168)\n",
      "Epoch: 64 | Batch_idx: 190 |  Loss_1: (0.4578) | Acc_1: (83.22%) (20345/24448)\n",
      "Epoch: 64 | Batch_idx: 200 |  Loss_1: (0.4583) | Acc_1: (83.21%) (21407/25728)\n",
      "Epoch: 64 | Batch_idx: 210 |  Loss_1: (0.4580) | Acc_1: (83.24%) (22481/27008)\n",
      "Epoch: 64 | Batch_idx: 220 |  Loss_1: (0.4570) | Acc_1: (83.25%) (23550/28288)\n",
      "Epoch: 64 | Batch_idx: 230 |  Loss_1: (0.4575) | Acc_1: (83.25%) (24616/29568)\n",
      "Epoch: 64 | Batch_idx: 240 |  Loss_1: (0.4596) | Acc_1: (83.18%) (25660/30848)\n",
      "Epoch: 64 | Batch_idx: 250 |  Loss_1: (0.4608) | Acc_1: (83.14%) (26711/32128)\n",
      "Epoch: 64 | Batch_idx: 260 |  Loss_1: (0.4621) | Acc_1: (83.07%) (27753/33408)\n",
      "Epoch: 64 | Batch_idx: 270 |  Loss_1: (0.4611) | Acc_1: (83.12%) (28832/34688)\n",
      "Epoch: 64 | Batch_idx: 280 |  Loss_1: (0.4614) | Acc_1: (83.12%) (29897/35968)\n",
      "Epoch: 64 | Batch_idx: 290 |  Loss_1: (0.4617) | Acc_1: (83.11%) (30955/37248)\n",
      "Epoch: 64 | Batch_idx: 300 |  Loss_1: (0.4624) | Acc_1: (83.08%) (32008/38528)\n",
      "Epoch: 64 | Batch_idx: 310 |  Loss_1: (0.4620) | Acc_1: (83.09%) (33078/39808)\n",
      "Epoch: 64 | Batch_idx: 320 |  Loss_1: (0.4612) | Acc_1: (83.12%) (34154/41088)\n",
      "Epoch: 64 | Batch_idx: 330 |  Loss_1: (0.4608) | Acc_1: (83.13%) (35219/42368)\n",
      "Epoch: 64 | Batch_idx: 340 |  Loss_1: (0.4597) | Acc_1: (83.15%) (36295/43648)\n",
      "Epoch: 64 | Batch_idx: 350 |  Loss_1: (0.4599) | Acc_1: (83.17%) (37365/44928)\n",
      "Epoch: 64 | Batch_idx: 360 |  Loss_1: (0.4591) | Acc_1: (83.20%) (38445/46208)\n",
      "Epoch: 64 | Batch_idx: 370 |  Loss_1: (0.4603) | Acc_1: (83.16%) (39493/47488)\n",
      "Epoch: 64 | Batch_idx: 380 |  Loss_1: (0.4618) | Acc_1: (83.11%) (40529/48768)\n",
      "Epoch: 64 | Batch_idx: 390 |  Loss_1: (0.4625) | Acc_1: (83.09%) (41545/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4395) | Acc: (88.31%) (8831/10000)\n",
      "Epoch: 65 | Batch_idx: 0 |  Loss_1: (0.4084) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 65 | Batch_idx: 10 |  Loss_1: (0.4492) | Acc_1: (82.95%) (1168/1408)\n",
      "Epoch: 65 | Batch_idx: 20 |  Loss_1: (0.4639) | Acc_1: (82.92%) (2229/2688)\n",
      "Epoch: 65 | Batch_idx: 30 |  Loss_1: (0.4469) | Acc_1: (83.44%) (3311/3968)\n",
      "Epoch: 65 | Batch_idx: 40 |  Loss_1: (0.4560) | Acc_1: (83.12%) (4362/5248)\n",
      "Epoch: 65 | Batch_idx: 50 |  Loss_1: (0.4560) | Acc_1: (82.97%) (5416/6528)\n",
      "Epoch: 65 | Batch_idx: 60 |  Loss_1: (0.4532) | Acc_1: (83.11%) (6489/7808)\n",
      "Epoch: 65 | Batch_idx: 70 |  Loss_1: (0.4542) | Acc_1: (83.13%) (7555/9088)\n",
      "Epoch: 65 | Batch_idx: 80 |  Loss_1: (0.4546) | Acc_1: (83.12%) (8618/10368)\n",
      "Epoch: 65 | Batch_idx: 90 |  Loss_1: (0.4582) | Acc_1: (83.04%) (9672/11648)\n",
      "Epoch: 65 | Batch_idx: 100 |  Loss_1: (0.4618) | Acc_1: (82.97%) (10726/12928)\n",
      "Epoch: 65 | Batch_idx: 110 |  Loss_1: (0.4611) | Acc_1: (82.98%) (11790/14208)\n",
      "Epoch: 65 | Batch_idx: 120 |  Loss_1: (0.4636) | Acc_1: (82.91%) (12841/15488)\n",
      "Epoch: 65 | Batch_idx: 130 |  Loss_1: (0.4616) | Acc_1: (83.02%) (13921/16768)\n",
      "Epoch: 65 | Batch_idx: 140 |  Loss_1: (0.4617) | Acc_1: (83.05%) (14988/18048)\n",
      "Epoch: 65 | Batch_idx: 150 |  Loss_1: (0.4643) | Acc_1: (82.95%) (16032/19328)\n",
      "Epoch: 65 | Batch_idx: 160 |  Loss_1: (0.4616) | Acc_1: (83.05%) (17115/20608)\n",
      "Epoch: 65 | Batch_idx: 170 |  Loss_1: (0.4608) | Acc_1: (83.06%) (18181/21888)\n",
      "Epoch: 65 | Batch_idx: 180 |  Loss_1: (0.4601) | Acc_1: (83.11%) (19255/23168)\n",
      "Epoch: 65 | Batch_idx: 190 |  Loss_1: (0.4607) | Acc_1: (83.07%) (20309/24448)\n",
      "Epoch: 65 | Batch_idx: 200 |  Loss_1: (0.4618) | Acc_1: (83.04%) (21365/25728)\n",
      "Epoch: 65 | Batch_idx: 210 |  Loss_1: (0.4616) | Acc_1: (83.04%) (22428/27008)\n",
      "Epoch: 65 | Batch_idx: 220 |  Loss_1: (0.4618) | Acc_1: (83.05%) (23492/28288)\n",
      "Epoch: 65 | Batch_idx: 230 |  Loss_1: (0.4645) | Acc_1: (82.94%) (24525/29568)\n",
      "Epoch: 65 | Batch_idx: 240 |  Loss_1: (0.4629) | Acc_1: (83.00%) (25605/30848)\n",
      "Epoch: 65 | Batch_idx: 250 |  Loss_1: (0.4618) | Acc_1: (83.04%) (26679/32128)\n",
      "Epoch: 65 | Batch_idx: 260 |  Loss_1: (0.4626) | Acc_1: (83.01%) (27732/33408)\n",
      "Epoch: 65 | Batch_idx: 270 |  Loss_1: (0.4634) | Acc_1: (82.97%) (28779/34688)\n",
      "Epoch: 65 | Batch_idx: 280 |  Loss_1: (0.4631) | Acc_1: (82.98%) (29848/35968)\n",
      "Epoch: 65 | Batch_idx: 290 |  Loss_1: (0.4638) | Acc_1: (82.93%) (30890/37248)\n",
      "Epoch: 65 | Batch_idx: 300 |  Loss_1: (0.4641) | Acc_1: (82.93%) (31952/38528)\n",
      "Epoch: 65 | Batch_idx: 310 |  Loss_1: (0.4635) | Acc_1: (82.97%) (33027/39808)\n",
      "Epoch: 65 | Batch_idx: 320 |  Loss_1: (0.4626) | Acc_1: (82.98%) (34096/41088)\n",
      "Epoch: 65 | Batch_idx: 330 |  Loss_1: (0.4630) | Acc_1: (82.98%) (35158/42368)\n",
      "Epoch: 65 | Batch_idx: 340 |  Loss_1: (0.4634) | Acc_1: (82.97%) (36215/43648)\n",
      "Epoch: 65 | Batch_idx: 350 |  Loss_1: (0.4642) | Acc_1: (82.95%) (37267/44928)\n",
      "Epoch: 65 | Batch_idx: 360 |  Loss_1: (0.4642) | Acc_1: (82.96%) (38335/46208)\n",
      "Epoch: 65 | Batch_idx: 370 |  Loss_1: (0.4641) | Acc_1: (82.96%) (39394/47488)\n",
      "Epoch: 65 | Batch_idx: 380 |  Loss_1: (0.4652) | Acc_1: (82.94%) (40448/48768)\n",
      "Epoch: 65 | Batch_idx: 390 |  Loss_1: (0.4647) | Acc_1: (82.93%) (41465/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4248) | Acc: (88.54%) (8854/10000)\n",
      "Epoch: 66 | Batch_idx: 0 |  Loss_1: (0.4308) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 66 | Batch_idx: 10 |  Loss_1: (0.3998) | Acc_1: (85.23%) (1200/1408)\n",
      "Epoch: 66 | Batch_idx: 20 |  Loss_1: (0.4136) | Acc_1: (84.82%) (2280/2688)\n",
      "Epoch: 66 | Batch_idx: 30 |  Loss_1: (0.4193) | Acc_1: (84.40%) (3349/3968)\n",
      "Epoch: 66 | Batch_idx: 40 |  Loss_1: (0.4327) | Acc_1: (83.94%) (4405/5248)\n",
      "Epoch: 66 | Batch_idx: 50 |  Loss_1: (0.4372) | Acc_1: (83.76%) (5468/6528)\n",
      "Epoch: 66 | Batch_idx: 60 |  Loss_1: (0.4450) | Acc_1: (83.62%) (6529/7808)\n",
      "Epoch: 66 | Batch_idx: 70 |  Loss_1: (0.4400) | Acc_1: (83.77%) (7613/9088)\n",
      "Epoch: 66 | Batch_idx: 80 |  Loss_1: (0.4365) | Acc_1: (83.82%) (8690/10368)\n",
      "Epoch: 66 | Batch_idx: 90 |  Loss_1: (0.4382) | Acc_1: (83.81%) (9762/11648)\n",
      "Epoch: 66 | Batch_idx: 100 |  Loss_1: (0.4388) | Acc_1: (83.84%) (10839/12928)\n",
      "Epoch: 66 | Batch_idx: 110 |  Loss_1: (0.4419) | Acc_1: (83.72%) (11895/14208)\n",
      "Epoch: 66 | Batch_idx: 120 |  Loss_1: (0.4432) | Acc_1: (83.66%) (12958/15488)\n",
      "Epoch: 66 | Batch_idx: 130 |  Loss_1: (0.4443) | Acc_1: (83.60%) (14018/16768)\n",
      "Epoch: 66 | Batch_idx: 140 |  Loss_1: (0.4459) | Acc_1: (83.52%) (15074/18048)\n",
      "Epoch: 66 | Batch_idx: 150 |  Loss_1: (0.4489) | Acc_1: (83.40%) (16120/19328)\n",
      "Epoch: 66 | Batch_idx: 160 |  Loss_1: (0.4533) | Acc_1: (83.26%) (17159/20608)\n",
      "Epoch: 66 | Batch_idx: 170 |  Loss_1: (0.4542) | Acc_1: (83.22%) (18216/21888)\n",
      "Epoch: 66 | Batch_idx: 180 |  Loss_1: (0.4550) | Acc_1: (83.19%) (19274/23168)\n",
      "Epoch: 66 | Batch_idx: 190 |  Loss_1: (0.4553) | Acc_1: (83.16%) (20330/24448)\n",
      "Epoch: 66 | Batch_idx: 200 |  Loss_1: (0.4535) | Acc_1: (83.20%) (21406/25728)\n",
      "Epoch: 66 | Batch_idx: 210 |  Loss_1: (0.4546) | Acc_1: (83.18%) (22464/27008)\n",
      "Epoch: 66 | Batch_idx: 220 |  Loss_1: (0.4553) | Acc_1: (83.14%) (23520/28288)\n",
      "Epoch: 66 | Batch_idx: 230 |  Loss_1: (0.4548) | Acc_1: (83.15%) (24585/29568)\n",
      "Epoch: 66 | Batch_idx: 240 |  Loss_1: (0.4543) | Acc_1: (83.17%) (25656/30848)\n",
      "Epoch: 66 | Batch_idx: 250 |  Loss_1: (0.4547) | Acc_1: (83.16%) (26719/32128)\n",
      "Epoch: 66 | Batch_idx: 260 |  Loss_1: (0.4537) | Acc_1: (83.21%) (27800/33408)\n",
      "Epoch: 66 | Batch_idx: 270 |  Loss_1: (0.4552) | Acc_1: (83.16%) (28845/34688)\n",
      "Epoch: 66 | Batch_idx: 280 |  Loss_1: (0.4551) | Acc_1: (83.19%) (29921/35968)\n",
      "Epoch: 66 | Batch_idx: 290 |  Loss_1: (0.4547) | Acc_1: (83.20%) (30992/37248)\n",
      "Epoch: 66 | Batch_idx: 300 |  Loss_1: (0.4551) | Acc_1: (83.20%) (32055/38528)\n",
      "Epoch: 66 | Batch_idx: 310 |  Loss_1: (0.4555) | Acc_1: (83.18%) (33111/39808)\n",
      "Epoch: 66 | Batch_idx: 320 |  Loss_1: (0.4554) | Acc_1: (83.18%) (34177/41088)\n",
      "Epoch: 66 | Batch_idx: 330 |  Loss_1: (0.4559) | Acc_1: (83.17%) (35238/42368)\n",
      "Epoch: 66 | Batch_idx: 340 |  Loss_1: (0.4572) | Acc_1: (83.13%) (36285/43648)\n",
      "Epoch: 66 | Batch_idx: 350 |  Loss_1: (0.4577) | Acc_1: (83.11%) (37338/44928)\n",
      "Epoch: 66 | Batch_idx: 360 |  Loss_1: (0.4577) | Acc_1: (83.11%) (38403/46208)\n",
      "Epoch: 66 | Batch_idx: 370 |  Loss_1: (0.4583) | Acc_1: (83.09%) (39457/47488)\n",
      "Epoch: 66 | Batch_idx: 380 |  Loss_1: (0.4582) | Acc_1: (83.11%) (40532/48768)\n",
      "Epoch: 66 | Batch_idx: 390 |  Loss_1: (0.4580) | Acc_1: (83.11%) (41554/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4415) | Acc: (87.84%) (8784/10000)\n",
      "Epoch: 67 | Batch_idx: 0 |  Loss_1: (0.4399) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 67 | Batch_idx: 10 |  Loss_1: (0.4806) | Acc_1: (81.96%) (1154/1408)\n",
      "Epoch: 67 | Batch_idx: 20 |  Loss_1: (0.4416) | Acc_1: (83.56%) (2246/2688)\n",
      "Epoch: 67 | Batch_idx: 30 |  Loss_1: (0.4359) | Acc_1: (83.97%) (3332/3968)\n",
      "Epoch: 67 | Batch_idx: 40 |  Loss_1: (0.4405) | Acc_1: (83.71%) (4393/5248)\n",
      "Epoch: 67 | Batch_idx: 50 |  Loss_1: (0.4424) | Acc_1: (83.64%) (5460/6528)\n",
      "Epoch: 67 | Batch_idx: 60 |  Loss_1: (0.4400) | Acc_1: (83.80%) (6543/7808)\n",
      "Epoch: 67 | Batch_idx: 70 |  Loss_1: (0.4383) | Acc_1: (83.91%) (7626/9088)\n",
      "Epoch: 67 | Batch_idx: 80 |  Loss_1: (0.4363) | Acc_1: (84.03%) (8712/10368)\n",
      "Epoch: 67 | Batch_idx: 90 |  Loss_1: (0.4435) | Acc_1: (83.67%) (9746/11648)\n",
      "Epoch: 67 | Batch_idx: 100 |  Loss_1: (0.4414) | Acc_1: (83.78%) (10831/12928)\n",
      "Epoch: 67 | Batch_idx: 110 |  Loss_1: (0.4430) | Acc_1: (83.73%) (11897/14208)\n",
      "Epoch: 67 | Batch_idx: 120 |  Loss_1: (0.4432) | Acc_1: (83.77%) (12974/15488)\n",
      "Epoch: 67 | Batch_idx: 130 |  Loss_1: (0.4425) | Acc_1: (83.77%) (14047/16768)\n",
      "Epoch: 67 | Batch_idx: 140 |  Loss_1: (0.4438) | Acc_1: (83.73%) (15111/18048)\n",
      "Epoch: 67 | Batch_idx: 150 |  Loss_1: (0.4412) | Acc_1: (83.83%) (16202/19328)\n",
      "Epoch: 67 | Batch_idx: 160 |  Loss_1: (0.4414) | Acc_1: (83.84%) (17277/20608)\n",
      "Epoch: 67 | Batch_idx: 170 |  Loss_1: (0.4396) | Acc_1: (83.93%) (18370/21888)\n",
      "Epoch: 67 | Batch_idx: 180 |  Loss_1: (0.4425) | Acc_1: (83.80%) (19415/23168)\n",
      "Epoch: 67 | Batch_idx: 190 |  Loss_1: (0.4431) | Acc_1: (83.74%) (20473/24448)\n",
      "Epoch: 67 | Batch_idx: 200 |  Loss_1: (0.4429) | Acc_1: (83.75%) (21547/25728)\n",
      "Epoch: 67 | Batch_idx: 210 |  Loss_1: (0.4450) | Acc_1: (83.64%) (22589/27008)\n",
      "Epoch: 67 | Batch_idx: 220 |  Loss_1: (0.4451) | Acc_1: (83.61%) (23651/28288)\n",
      "Epoch: 67 | Batch_idx: 230 |  Loss_1: (0.4461) | Acc_1: (83.57%) (24711/29568)\n",
      "Epoch: 67 | Batch_idx: 240 |  Loss_1: (0.4477) | Acc_1: (83.50%) (25758/30848)\n",
      "Epoch: 67 | Batch_idx: 250 |  Loss_1: (0.4488) | Acc_1: (83.48%) (26820/32128)\n",
      "Epoch: 67 | Batch_idx: 260 |  Loss_1: (0.4497) | Acc_1: (83.43%) (27872/33408)\n",
      "Epoch: 67 | Batch_idx: 270 |  Loss_1: (0.4502) | Acc_1: (83.39%) (28927/34688)\n",
      "Epoch: 67 | Batch_idx: 280 |  Loss_1: (0.4511) | Acc_1: (83.35%) (29979/35968)\n",
      "Epoch: 67 | Batch_idx: 290 |  Loss_1: (0.4510) | Acc_1: (83.34%) (31042/37248)\n",
      "Epoch: 67 | Batch_idx: 300 |  Loss_1: (0.4506) | Acc_1: (83.34%) (32110/38528)\n",
      "Epoch: 67 | Batch_idx: 310 |  Loss_1: (0.4506) | Acc_1: (83.33%) (33172/39808)\n",
      "Epoch: 67 | Batch_idx: 320 |  Loss_1: (0.4512) | Acc_1: (83.31%) (34232/41088)\n",
      "Epoch: 67 | Batch_idx: 330 |  Loss_1: (0.4514) | Acc_1: (83.32%) (35300/42368)\n",
      "Epoch: 67 | Batch_idx: 340 |  Loss_1: (0.4527) | Acc_1: (83.27%) (36347/43648)\n",
      "Epoch: 67 | Batch_idx: 350 |  Loss_1: (0.4527) | Acc_1: (83.28%) (37414/44928)\n",
      "Epoch: 67 | Batch_idx: 360 |  Loss_1: (0.4532) | Acc_1: (83.27%) (38478/46208)\n",
      "Epoch: 67 | Batch_idx: 370 |  Loss_1: (0.4537) | Acc_1: (83.26%) (39538/47488)\n",
      "Epoch: 67 | Batch_idx: 380 |  Loss_1: (0.4529) | Acc_1: (83.29%) (40619/48768)\n",
      "Epoch: 67 | Batch_idx: 390 |  Loss_1: (0.4521) | Acc_1: (83.32%) (41658/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4774) | Acc: (87.51%) (8751/10000)\n",
      "Epoch: 68 | Batch_idx: 0 |  Loss_1: (0.5031) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 68 | Batch_idx: 10 |  Loss_1: (0.4291) | Acc_1: (84.66%) (1192/1408)\n",
      "Epoch: 68 | Batch_idx: 20 |  Loss_1: (0.4294) | Acc_1: (84.34%) (2267/2688)\n",
      "Epoch: 68 | Batch_idx: 30 |  Loss_1: (0.4279) | Acc_1: (84.38%) (3348/3968)\n",
      "Epoch: 68 | Batch_idx: 40 |  Loss_1: (0.4308) | Acc_1: (84.30%) (4424/5248)\n",
      "Epoch: 68 | Batch_idx: 50 |  Loss_1: (0.4413) | Acc_1: (83.82%) (5472/6528)\n",
      "Epoch: 68 | Batch_idx: 60 |  Loss_1: (0.4405) | Acc_1: (83.88%) (6549/7808)\n",
      "Epoch: 68 | Batch_idx: 70 |  Loss_1: (0.4477) | Acc_1: (83.66%) (7603/9088)\n",
      "Epoch: 68 | Batch_idx: 80 |  Loss_1: (0.4496) | Acc_1: (83.63%) (8671/10368)\n",
      "Epoch: 68 | Batch_idx: 90 |  Loss_1: (0.4459) | Acc_1: (83.78%) (9759/11648)\n",
      "Epoch: 68 | Batch_idx: 100 |  Loss_1: (0.4463) | Acc_1: (83.82%) (10836/12928)\n",
      "Epoch: 68 | Batch_idx: 110 |  Loss_1: (0.4480) | Acc_1: (83.71%) (11894/14208)\n",
      "Epoch: 68 | Batch_idx: 120 |  Loss_1: (0.4459) | Acc_1: (83.80%) (12979/15488)\n",
      "Epoch: 68 | Batch_idx: 130 |  Loss_1: (0.4456) | Acc_1: (83.83%) (14057/16768)\n",
      "Epoch: 68 | Batch_idx: 140 |  Loss_1: (0.4435) | Acc_1: (83.85%) (15133/18048)\n",
      "Epoch: 68 | Batch_idx: 150 |  Loss_1: (0.4403) | Acc_1: (83.94%) (16224/19328)\n",
      "Epoch: 68 | Batch_idx: 160 |  Loss_1: (0.4452) | Acc_1: (83.73%) (17255/20608)\n",
      "Epoch: 68 | Batch_idx: 170 |  Loss_1: (0.4471) | Acc_1: (83.69%) (18317/21888)\n",
      "Epoch: 68 | Batch_idx: 180 |  Loss_1: (0.4475) | Acc_1: (83.67%) (19384/23168)\n",
      "Epoch: 68 | Batch_idx: 190 |  Loss_1: (0.4466) | Acc_1: (83.70%) (20464/24448)\n",
      "Epoch: 68 | Batch_idx: 200 |  Loss_1: (0.4481) | Acc_1: (83.63%) (21516/25728)\n",
      "Epoch: 68 | Batch_idx: 210 |  Loss_1: (0.4491) | Acc_1: (83.58%) (22574/27008)\n",
      "Epoch: 68 | Batch_idx: 220 |  Loss_1: (0.4476) | Acc_1: (83.64%) (23661/28288)\n",
      "Epoch: 68 | Batch_idx: 230 |  Loss_1: (0.4473) | Acc_1: (83.66%) (24736/29568)\n",
      "Epoch: 68 | Batch_idx: 240 |  Loss_1: (0.4488) | Acc_1: (83.60%) (25789/30848)\n",
      "Epoch: 68 | Batch_idx: 250 |  Loss_1: (0.4486) | Acc_1: (83.60%) (26860/32128)\n",
      "Epoch: 68 | Batch_idx: 260 |  Loss_1: (0.4483) | Acc_1: (83.59%) (27926/33408)\n",
      "Epoch: 68 | Batch_idx: 270 |  Loss_1: (0.4490) | Acc_1: (83.57%) (28988/34688)\n",
      "Epoch: 68 | Batch_idx: 280 |  Loss_1: (0.4495) | Acc_1: (83.52%) (30041/35968)\n",
      "Epoch: 68 | Batch_idx: 290 |  Loss_1: (0.4515) | Acc_1: (83.45%) (31085/37248)\n",
      "Epoch: 68 | Batch_idx: 300 |  Loss_1: (0.4521) | Acc_1: (83.44%) (32147/38528)\n",
      "Epoch: 68 | Batch_idx: 310 |  Loss_1: (0.4519) | Acc_1: (83.43%) (33213/39808)\n",
      "Epoch: 68 | Batch_idx: 320 |  Loss_1: (0.4517) | Acc_1: (83.43%) (34279/41088)\n",
      "Epoch: 68 | Batch_idx: 330 |  Loss_1: (0.4517) | Acc_1: (83.42%) (35344/42368)\n",
      "Epoch: 68 | Batch_idx: 340 |  Loss_1: (0.4522) | Acc_1: (83.41%) (36405/43648)\n",
      "Epoch: 68 | Batch_idx: 350 |  Loss_1: (0.4536) | Acc_1: (83.35%) (37449/44928)\n",
      "Epoch: 68 | Batch_idx: 360 |  Loss_1: (0.4543) | Acc_1: (83.32%) (38501/46208)\n",
      "Epoch: 68 | Batch_idx: 370 |  Loss_1: (0.4548) | Acc_1: (83.30%) (39559/47488)\n",
      "Epoch: 68 | Batch_idx: 380 |  Loss_1: (0.4544) | Acc_1: (83.31%) (40630/48768)\n",
      "Epoch: 68 | Batch_idx: 390 |  Loss_1: (0.4532) | Acc_1: (83.34%) (41672/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4496) | Acc: (88.65%) (8865/10000)\n",
      "Epoch: 69 | Batch_idx: 0 |  Loss_1: (0.5914) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 69 | Batch_idx: 10 |  Loss_1: (0.4736) | Acc_1: (82.81%) (1166/1408)\n",
      "Epoch: 69 | Batch_idx: 20 |  Loss_1: (0.4800) | Acc_1: (82.37%) (2214/2688)\n",
      "Epoch: 69 | Batch_idx: 30 |  Loss_1: (0.4737) | Acc_1: (82.54%) (3275/3968)\n",
      "Epoch: 69 | Batch_idx: 40 |  Loss_1: (0.4783) | Acc_1: (82.36%) (4322/5248)\n",
      "Epoch: 69 | Batch_idx: 50 |  Loss_1: (0.4710) | Acc_1: (82.63%) (5394/6528)\n",
      "Epoch: 69 | Batch_idx: 60 |  Loss_1: (0.4710) | Acc_1: (82.59%) (6449/7808)\n",
      "Epoch: 69 | Batch_idx: 70 |  Loss_1: (0.4710) | Acc_1: (82.70%) (7516/9088)\n",
      "Epoch: 69 | Batch_idx: 80 |  Loss_1: (0.4670) | Acc_1: (82.96%) (8601/10368)\n",
      "Epoch: 69 | Batch_idx: 90 |  Loss_1: (0.4596) | Acc_1: (83.23%) (9695/11648)\n",
      "Epoch: 69 | Batch_idx: 100 |  Loss_1: (0.4608) | Acc_1: (83.15%) (10750/12928)\n",
      "Epoch: 69 | Batch_idx: 110 |  Loss_1: (0.4622) | Acc_1: (83.12%) (11809/14208)\n",
      "Epoch: 69 | Batch_idx: 120 |  Loss_1: (0.4649) | Acc_1: (83.05%) (12863/15488)\n",
      "Epoch: 69 | Batch_idx: 130 |  Loss_1: (0.4686) | Acc_1: (82.87%) (13896/16768)\n",
      "Epoch: 69 | Batch_idx: 140 |  Loss_1: (0.4695) | Acc_1: (82.87%) (14957/18048)\n",
      "Epoch: 69 | Batch_idx: 150 |  Loss_1: (0.4686) | Acc_1: (82.88%) (16020/19328)\n",
      "Epoch: 69 | Batch_idx: 160 |  Loss_1: (0.4679) | Acc_1: (82.89%) (17083/20608)\n",
      "Epoch: 69 | Batch_idx: 170 |  Loss_1: (0.4677) | Acc_1: (82.90%) (18145/21888)\n",
      "Epoch: 69 | Batch_idx: 180 |  Loss_1: (0.4668) | Acc_1: (82.90%) (19207/23168)\n",
      "Epoch: 69 | Batch_idx: 190 |  Loss_1: (0.4663) | Acc_1: (82.92%) (20273/24448)\n",
      "Epoch: 69 | Batch_idx: 200 |  Loss_1: (0.4653) | Acc_1: (82.96%) (21345/25728)\n",
      "Epoch: 69 | Batch_idx: 210 |  Loss_1: (0.4630) | Acc_1: (83.04%) (22428/27008)\n",
      "Epoch: 69 | Batch_idx: 220 |  Loss_1: (0.4628) | Acc_1: (83.03%) (23488/28288)\n",
      "Epoch: 69 | Batch_idx: 230 |  Loss_1: (0.4625) | Acc_1: (83.05%) (24556/29568)\n",
      "Epoch: 69 | Batch_idx: 240 |  Loss_1: (0.4620) | Acc_1: (83.07%) (25624/30848)\n",
      "Epoch: 69 | Batch_idx: 250 |  Loss_1: (0.4608) | Acc_1: (83.12%) (26704/32128)\n",
      "Epoch: 69 | Batch_idx: 260 |  Loss_1: (0.4606) | Acc_1: (83.12%) (27769/33408)\n",
      "Epoch: 69 | Batch_idx: 270 |  Loss_1: (0.4612) | Acc_1: (83.10%) (28825/34688)\n",
      "Epoch: 69 | Batch_idx: 280 |  Loss_1: (0.4610) | Acc_1: (83.10%) (29891/35968)\n",
      "Epoch: 69 | Batch_idx: 290 |  Loss_1: (0.4616) | Acc_1: (83.10%) (30952/37248)\n",
      "Epoch: 69 | Batch_idx: 300 |  Loss_1: (0.4615) | Acc_1: (83.10%) (32018/38528)\n",
      "Epoch: 69 | Batch_idx: 310 |  Loss_1: (0.4617) | Acc_1: (83.09%) (33075/39808)\n",
      "Epoch: 69 | Batch_idx: 320 |  Loss_1: (0.4623) | Acc_1: (83.08%) (34134/41088)\n",
      "Epoch: 69 | Batch_idx: 330 |  Loss_1: (0.4628) | Acc_1: (83.04%) (35184/42368)\n",
      "Epoch: 69 | Batch_idx: 340 |  Loss_1: (0.4629) | Acc_1: (83.03%) (36242/43648)\n",
      "Epoch: 69 | Batch_idx: 350 |  Loss_1: (0.4626) | Acc_1: (83.05%) (37312/44928)\n",
      "Epoch: 69 | Batch_idx: 360 |  Loss_1: (0.4625) | Acc_1: (83.04%) (38372/46208)\n",
      "Epoch: 69 | Batch_idx: 370 |  Loss_1: (0.4617) | Acc_1: (83.08%) (39454/47488)\n",
      "Epoch: 69 | Batch_idx: 380 |  Loss_1: (0.4616) | Acc_1: (83.09%) (40522/48768)\n",
      "Epoch: 69 | Batch_idx: 390 |  Loss_1: (0.4612) | Acc_1: (83.09%) (41544/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4672) | Acc: (88.11%) (8811/10000)\n",
      "Epoch: 70 | Batch_idx: 0 |  Loss_1: (0.3854) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 70 | Batch_idx: 10 |  Loss_1: (0.4653) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 70 | Batch_idx: 20 |  Loss_1: (0.4513) | Acc_1: (83.52%) (2245/2688)\n",
      "Epoch: 70 | Batch_idx: 30 |  Loss_1: (0.4536) | Acc_1: (83.24%) (3303/3968)\n",
      "Epoch: 70 | Batch_idx: 40 |  Loss_1: (0.4507) | Acc_1: (83.33%) (4373/5248)\n",
      "Epoch: 70 | Batch_idx: 50 |  Loss_1: (0.4534) | Acc_1: (83.21%) (5432/6528)\n",
      "Epoch: 70 | Batch_idx: 60 |  Loss_1: (0.4595) | Acc_1: (83.15%) (6492/7808)\n",
      "Epoch: 70 | Batch_idx: 70 |  Loss_1: (0.4623) | Acc_1: (83.04%) (7547/9088)\n",
      "Epoch: 70 | Batch_idx: 80 |  Loss_1: (0.4616) | Acc_1: (83.06%) (8612/10368)\n",
      "Epoch: 70 | Batch_idx: 90 |  Loss_1: (0.4628) | Acc_1: (82.99%) (9667/11648)\n",
      "Epoch: 70 | Batch_idx: 100 |  Loss_1: (0.4624) | Acc_1: (82.97%) (10727/12928)\n",
      "Epoch: 70 | Batch_idx: 110 |  Loss_1: (0.4623) | Acc_1: (82.97%) (11788/14208)\n",
      "Epoch: 70 | Batch_idx: 120 |  Loss_1: (0.4600) | Acc_1: (83.04%) (12861/15488)\n",
      "Epoch: 70 | Batch_idx: 130 |  Loss_1: (0.4619) | Acc_1: (82.96%) (13910/16768)\n",
      "Epoch: 70 | Batch_idx: 140 |  Loss_1: (0.4601) | Acc_1: (83.02%) (14983/18048)\n",
      "Epoch: 70 | Batch_idx: 150 |  Loss_1: (0.4609) | Acc_1: (82.98%) (16039/19328)\n",
      "Epoch: 70 | Batch_idx: 160 |  Loss_1: (0.4584) | Acc_1: (83.07%) (17119/20608)\n",
      "Epoch: 70 | Batch_idx: 170 |  Loss_1: (0.4586) | Acc_1: (83.05%) (18177/21888)\n",
      "Epoch: 70 | Batch_idx: 180 |  Loss_1: (0.4581) | Acc_1: (83.05%) (19242/23168)\n",
      "Epoch: 70 | Batch_idx: 190 |  Loss_1: (0.4587) | Acc_1: (83.03%) (20299/24448)\n",
      "Epoch: 70 | Batch_idx: 200 |  Loss_1: (0.4592) | Acc_1: (83.00%) (21355/25728)\n",
      "Epoch: 70 | Batch_idx: 210 |  Loss_1: (0.4592) | Acc_1: (83.03%) (22425/27008)\n",
      "Epoch: 70 | Batch_idx: 220 |  Loss_1: (0.4604) | Acc_1: (83.00%) (23479/28288)\n",
      "Epoch: 70 | Batch_idx: 230 |  Loss_1: (0.4603) | Acc_1: (83.01%) (24543/29568)\n",
      "Epoch: 70 | Batch_idx: 240 |  Loss_1: (0.4603) | Acc_1: (83.04%) (25616/30848)\n",
      "Epoch: 70 | Batch_idx: 250 |  Loss_1: (0.4596) | Acc_1: (83.07%) (26688/32128)\n",
      "Epoch: 70 | Batch_idx: 260 |  Loss_1: (0.4588) | Acc_1: (83.12%) (27770/33408)\n",
      "Epoch: 70 | Batch_idx: 270 |  Loss_1: (0.4599) | Acc_1: (83.07%) (28815/34688)\n",
      "Epoch: 70 | Batch_idx: 280 |  Loss_1: (0.4594) | Acc_1: (83.08%) (29884/35968)\n",
      "Epoch: 70 | Batch_idx: 290 |  Loss_1: (0.4595) | Acc_1: (83.09%) (30949/37248)\n",
      "Epoch: 70 | Batch_idx: 300 |  Loss_1: (0.4604) | Acc_1: (83.06%) (32002/38528)\n",
      "Epoch: 70 | Batch_idx: 310 |  Loss_1: (0.4578) | Acc_1: (83.15%) (33099/39808)\n",
      "Epoch: 70 | Batch_idx: 320 |  Loss_1: (0.4576) | Acc_1: (83.15%) (34164/41088)\n",
      "Epoch: 70 | Batch_idx: 330 |  Loss_1: (0.4595) | Acc_1: (83.08%) (35201/42368)\n",
      "Epoch: 70 | Batch_idx: 340 |  Loss_1: (0.4592) | Acc_1: (83.09%) (36267/43648)\n",
      "Epoch: 70 | Batch_idx: 350 |  Loss_1: (0.4586) | Acc_1: (83.12%) (37346/44928)\n",
      "Epoch: 70 | Batch_idx: 360 |  Loss_1: (0.4595) | Acc_1: (83.11%) (38402/46208)\n",
      "Epoch: 70 | Batch_idx: 370 |  Loss_1: (0.4592) | Acc_1: (83.12%) (39470/47488)\n",
      "Epoch: 70 | Batch_idx: 380 |  Loss_1: (0.4576) | Acc_1: (83.17%) (40561/48768)\n",
      "Epoch: 70 | Batch_idx: 390 |  Loss_1: (0.4564) | Acc_1: (83.21%) (41605/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5106) | Acc: (87.60%) (8760/10000)\n",
      "Epoch: 71 | Batch_idx: 0 |  Loss_1: (0.4032) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 71 | Batch_idx: 10 |  Loss_1: (0.4423) | Acc_1: (83.95%) (1182/1408)\n",
      "Epoch: 71 | Batch_idx: 20 |  Loss_1: (0.4480) | Acc_1: (83.59%) (2247/2688)\n",
      "Epoch: 71 | Batch_idx: 30 |  Loss_1: (0.4487) | Acc_1: (83.39%) (3309/3968)\n",
      "Epoch: 71 | Batch_idx: 40 |  Loss_1: (0.4572) | Acc_1: (83.23%) (4368/5248)\n",
      "Epoch: 71 | Batch_idx: 50 |  Loss_1: (0.4568) | Acc_1: (83.23%) (5433/6528)\n",
      "Epoch: 71 | Batch_idx: 60 |  Loss_1: (0.4576) | Acc_1: (83.11%) (6489/7808)\n",
      "Epoch: 71 | Batch_idx: 70 |  Loss_1: (0.4579) | Acc_1: (83.02%) (7545/9088)\n",
      "Epoch: 71 | Batch_idx: 80 |  Loss_1: (0.4584) | Acc_1: (83.02%) (8607/10368)\n",
      "Epoch: 71 | Batch_idx: 90 |  Loss_1: (0.4560) | Acc_1: (83.13%) (9683/11648)\n",
      "Epoch: 71 | Batch_idx: 100 |  Loss_1: (0.4531) | Acc_1: (83.23%) (10760/12928)\n",
      "Epoch: 71 | Batch_idx: 110 |  Loss_1: (0.4560) | Acc_1: (83.12%) (11809/14208)\n",
      "Epoch: 71 | Batch_idx: 120 |  Loss_1: (0.4536) | Acc_1: (83.21%) (12888/15488)\n",
      "Epoch: 71 | Batch_idx: 130 |  Loss_1: (0.4523) | Acc_1: (83.30%) (13967/16768)\n",
      "Epoch: 71 | Batch_idx: 140 |  Loss_1: (0.4526) | Acc_1: (83.29%) (15033/18048)\n",
      "Epoch: 71 | Batch_idx: 150 |  Loss_1: (0.4529) | Acc_1: (83.26%) (16093/19328)\n",
      "Epoch: 71 | Batch_idx: 160 |  Loss_1: (0.4537) | Acc_1: (83.25%) (17157/20608)\n",
      "Epoch: 71 | Batch_idx: 170 |  Loss_1: (0.4541) | Acc_1: (83.23%) (18218/21888)\n",
      "Epoch: 71 | Batch_idx: 180 |  Loss_1: (0.4539) | Acc_1: (83.26%) (19289/23168)\n",
      "Epoch: 71 | Batch_idx: 190 |  Loss_1: (0.4534) | Acc_1: (83.32%) (20369/24448)\n",
      "Epoch: 71 | Batch_idx: 200 |  Loss_1: (0.4534) | Acc_1: (83.31%) (21435/25728)\n",
      "Epoch: 71 | Batch_idx: 210 |  Loss_1: (0.4545) | Acc_1: (83.28%) (22491/27008)\n",
      "Epoch: 71 | Batch_idx: 220 |  Loss_1: (0.4551) | Acc_1: (83.23%) (23545/28288)\n",
      "Epoch: 71 | Batch_idx: 230 |  Loss_1: (0.4563) | Acc_1: (83.21%) (24603/29568)\n",
      "Epoch: 71 | Batch_idx: 240 |  Loss_1: (0.4566) | Acc_1: (83.22%) (25671/30848)\n",
      "Epoch: 71 | Batch_idx: 250 |  Loss_1: (0.4587) | Acc_1: (83.13%) (26707/32128)\n",
      "Epoch: 71 | Batch_idx: 260 |  Loss_1: (0.4585) | Acc_1: (83.14%) (27777/33408)\n",
      "Epoch: 71 | Batch_idx: 270 |  Loss_1: (0.4598) | Acc_1: (83.10%) (28824/34688)\n",
      "Epoch: 71 | Batch_idx: 280 |  Loss_1: (0.4605) | Acc_1: (83.08%) (29883/35968)\n",
      "Epoch: 71 | Batch_idx: 290 |  Loss_1: (0.4603) | Acc_1: (83.08%) (30947/37248)\n",
      "Epoch: 71 | Batch_idx: 300 |  Loss_1: (0.4602) | Acc_1: (83.10%) (32018/38528)\n",
      "Epoch: 71 | Batch_idx: 310 |  Loss_1: (0.4606) | Acc_1: (83.09%) (33076/39808)\n",
      "Epoch: 71 | Batch_idx: 320 |  Loss_1: (0.4595) | Acc_1: (83.11%) (34150/41088)\n",
      "Epoch: 71 | Batch_idx: 330 |  Loss_1: (0.4603) | Acc_1: (83.08%) (35200/42368)\n",
      "Epoch: 71 | Batch_idx: 340 |  Loss_1: (0.4602) | Acc_1: (83.09%) (36265/43648)\n",
      "Epoch: 71 | Batch_idx: 350 |  Loss_1: (0.4604) | Acc_1: (83.07%) (37323/44928)\n",
      "Epoch: 71 | Batch_idx: 360 |  Loss_1: (0.4606) | Acc_1: (83.07%) (38386/46208)\n",
      "Epoch: 71 | Batch_idx: 370 |  Loss_1: (0.4609) | Acc_1: (83.04%) (39432/47488)\n",
      "Epoch: 71 | Batch_idx: 380 |  Loss_1: (0.4611) | Acc_1: (83.04%) (40497/48768)\n",
      "Epoch: 71 | Batch_idx: 390 |  Loss_1: (0.4607) | Acc_1: (83.04%) (41520/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4375) | Acc: (88.33%) (8833/10000)\n",
      "Epoch: 72 | Batch_idx: 0 |  Loss_1: (0.3689) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 72 | Batch_idx: 10 |  Loss_1: (0.4073) | Acc_1: (84.94%) (1196/1408)\n",
      "Epoch: 72 | Batch_idx: 20 |  Loss_1: (0.4191) | Acc_1: (84.41%) (2269/2688)\n",
      "Epoch: 72 | Batch_idx: 30 |  Loss_1: (0.4255) | Acc_1: (84.27%) (3344/3968)\n",
      "Epoch: 72 | Batch_idx: 40 |  Loss_1: (0.4206) | Acc_1: (84.51%) (4435/5248)\n",
      "Epoch: 72 | Batch_idx: 50 |  Loss_1: (0.4326) | Acc_1: (84.02%) (5485/6528)\n",
      "Epoch: 72 | Batch_idx: 60 |  Loss_1: (0.4328) | Acc_1: (84.04%) (6562/7808)\n",
      "Epoch: 72 | Batch_idx: 70 |  Loss_1: (0.4394) | Acc_1: (83.88%) (7623/9088)\n",
      "Epoch: 72 | Batch_idx: 80 |  Loss_1: (0.4437) | Acc_1: (83.65%) (8673/10368)\n",
      "Epoch: 72 | Batch_idx: 90 |  Loss_1: (0.4441) | Acc_1: (83.69%) (9748/11648)\n",
      "Epoch: 72 | Batch_idx: 100 |  Loss_1: (0.4471) | Acc_1: (83.56%) (10802/12928)\n",
      "Epoch: 72 | Batch_idx: 110 |  Loss_1: (0.4503) | Acc_1: (83.45%) (11856/14208)\n",
      "Epoch: 72 | Batch_idx: 120 |  Loss_1: (0.4540) | Acc_1: (83.28%) (12899/15488)\n",
      "Epoch: 72 | Batch_idx: 130 |  Loss_1: (0.4543) | Acc_1: (83.25%) (13959/16768)\n",
      "Epoch: 72 | Batch_idx: 140 |  Loss_1: (0.4565) | Acc_1: (83.18%) (15013/18048)\n",
      "Epoch: 72 | Batch_idx: 150 |  Loss_1: (0.4562) | Acc_1: (83.19%) (16078/19328)\n",
      "Epoch: 72 | Batch_idx: 160 |  Loss_1: (0.4568) | Acc_1: (83.15%) (17136/20608)\n",
      "Epoch: 72 | Batch_idx: 170 |  Loss_1: (0.4550) | Acc_1: (83.23%) (18217/21888)\n",
      "Epoch: 72 | Batch_idx: 180 |  Loss_1: (0.4550) | Acc_1: (83.25%) (19288/23168)\n",
      "Epoch: 72 | Batch_idx: 190 |  Loss_1: (0.4562) | Acc_1: (83.22%) (20346/24448)\n",
      "Epoch: 72 | Batch_idx: 200 |  Loss_1: (0.4547) | Acc_1: (83.28%) (21425/25728)\n",
      "Epoch: 72 | Batch_idx: 210 |  Loss_1: (0.4558) | Acc_1: (83.24%) (22482/27008)\n",
      "Epoch: 72 | Batch_idx: 220 |  Loss_1: (0.4548) | Acc_1: (83.29%) (23561/28288)\n",
      "Epoch: 72 | Batch_idx: 230 |  Loss_1: (0.4554) | Acc_1: (83.28%) (24625/29568)\n",
      "Epoch: 72 | Batch_idx: 240 |  Loss_1: (0.4543) | Acc_1: (83.32%) (25702/30848)\n",
      "Epoch: 72 | Batch_idx: 250 |  Loss_1: (0.4561) | Acc_1: (83.25%) (26746/32128)\n",
      "Epoch: 72 | Batch_idx: 260 |  Loss_1: (0.4564) | Acc_1: (83.22%) (27803/33408)\n",
      "Epoch: 72 | Batch_idx: 270 |  Loss_1: (0.4572) | Acc_1: (83.19%) (28856/34688)\n",
      "Epoch: 72 | Batch_idx: 280 |  Loss_1: (0.4596) | Acc_1: (83.13%) (29900/35968)\n",
      "Epoch: 72 | Batch_idx: 290 |  Loss_1: (0.4593) | Acc_1: (83.17%) (30981/37248)\n",
      "Epoch: 72 | Batch_idx: 300 |  Loss_1: (0.4585) | Acc_1: (83.19%) (32052/38528)\n",
      "Epoch: 72 | Batch_idx: 310 |  Loss_1: (0.4574) | Acc_1: (83.23%) (33133/39808)\n",
      "Epoch: 72 | Batch_idx: 320 |  Loss_1: (0.4574) | Acc_1: (83.23%) (34198/41088)\n",
      "Epoch: 72 | Batch_idx: 330 |  Loss_1: (0.4567) | Acc_1: (83.26%) (35274/42368)\n",
      "Epoch: 72 | Batch_idx: 340 |  Loss_1: (0.4580) | Acc_1: (83.21%) (36319/43648)\n",
      "Epoch: 72 | Batch_idx: 350 |  Loss_1: (0.4562) | Acc_1: (83.27%) (37413/44928)\n",
      "Epoch: 72 | Batch_idx: 360 |  Loss_1: (0.4556) | Acc_1: (83.29%) (38485/46208)\n",
      "Epoch: 72 | Batch_idx: 370 |  Loss_1: (0.4551) | Acc_1: (83.31%) (39564/47488)\n",
      "Epoch: 72 | Batch_idx: 380 |  Loss_1: (0.4552) | Acc_1: (83.31%) (40629/48768)\n",
      "Epoch: 72 | Batch_idx: 390 |  Loss_1: (0.4552) | Acc_1: (83.30%) (41650/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4408) | Acc: (89.36%) (8936/10000)\n",
      "Epoch: 73 | Batch_idx: 0 |  Loss_1: (0.4244) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 73 | Batch_idx: 10 |  Loss_1: (0.4695) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 73 | Batch_idx: 20 |  Loss_1: (0.4269) | Acc_1: (84.00%) (2258/2688)\n",
      "Epoch: 73 | Batch_idx: 30 |  Loss_1: (0.4329) | Acc_1: (83.62%) (3318/3968)\n",
      "Epoch: 73 | Batch_idx: 40 |  Loss_1: (0.4335) | Acc_1: (83.63%) (4389/5248)\n",
      "Epoch: 73 | Batch_idx: 50 |  Loss_1: (0.4370) | Acc_1: (83.66%) (5461/6528)\n",
      "Epoch: 73 | Batch_idx: 60 |  Loss_1: (0.4377) | Acc_1: (83.71%) (6536/7808)\n",
      "Epoch: 73 | Batch_idx: 70 |  Loss_1: (0.4425) | Acc_1: (83.60%) (7598/9088)\n",
      "Epoch: 73 | Batch_idx: 80 |  Loss_1: (0.4419) | Acc_1: (83.56%) (8664/10368)\n",
      "Epoch: 73 | Batch_idx: 90 |  Loss_1: (0.4395) | Acc_1: (83.71%) (9750/11648)\n",
      "Epoch: 73 | Batch_idx: 100 |  Loss_1: (0.4451) | Acc_1: (83.46%) (10790/12928)\n",
      "Epoch: 73 | Batch_idx: 110 |  Loss_1: (0.4454) | Acc_1: (83.54%) (11869/14208)\n",
      "Epoch: 73 | Batch_idx: 120 |  Loss_1: (0.4415) | Acc_1: (83.72%) (12966/15488)\n",
      "Epoch: 73 | Batch_idx: 130 |  Loss_1: (0.4431) | Acc_1: (83.68%) (14032/16768)\n",
      "Epoch: 73 | Batch_idx: 140 |  Loss_1: (0.4432) | Acc_1: (83.68%) (15103/18048)\n",
      "Epoch: 73 | Batch_idx: 150 |  Loss_1: (0.4431) | Acc_1: (83.71%) (16180/19328)\n",
      "Epoch: 73 | Batch_idx: 160 |  Loss_1: (0.4446) | Acc_1: (83.65%) (17238/20608)\n",
      "Epoch: 73 | Batch_idx: 170 |  Loss_1: (0.4443) | Acc_1: (83.66%) (18312/21888)\n",
      "Epoch: 73 | Batch_idx: 180 |  Loss_1: (0.4462) | Acc_1: (83.55%) (19358/23168)\n",
      "Epoch: 73 | Batch_idx: 190 |  Loss_1: (0.4454) | Acc_1: (83.57%) (20431/24448)\n",
      "Epoch: 73 | Batch_idx: 200 |  Loss_1: (0.4463) | Acc_1: (83.54%) (21492/25728)\n",
      "Epoch: 73 | Batch_idx: 210 |  Loss_1: (0.4486) | Acc_1: (83.42%) (22531/27008)\n",
      "Epoch: 73 | Batch_idx: 220 |  Loss_1: (0.4482) | Acc_1: (83.42%) (23597/28288)\n",
      "Epoch: 73 | Batch_idx: 230 |  Loss_1: (0.4495) | Acc_1: (83.35%) (24646/29568)\n",
      "Epoch: 73 | Batch_idx: 240 |  Loss_1: (0.4506) | Acc_1: (83.33%) (25705/30848)\n",
      "Epoch: 73 | Batch_idx: 250 |  Loss_1: (0.4524) | Acc_1: (83.24%) (26743/32128)\n",
      "Epoch: 73 | Batch_idx: 260 |  Loss_1: (0.4546) | Acc_1: (83.16%) (27782/33408)\n",
      "Epoch: 73 | Batch_idx: 270 |  Loss_1: (0.4538) | Acc_1: (83.17%) (28850/34688)\n",
      "Epoch: 73 | Batch_idx: 280 |  Loss_1: (0.4542) | Acc_1: (83.15%) (29909/35968)\n",
      "Epoch: 73 | Batch_idx: 290 |  Loss_1: (0.4547) | Acc_1: (83.16%) (30974/37248)\n",
      "Epoch: 73 | Batch_idx: 300 |  Loss_1: (0.4538) | Acc_1: (83.20%) (32057/38528)\n",
      "Epoch: 73 | Batch_idx: 310 |  Loss_1: (0.4528) | Acc_1: (83.23%) (33133/39808)\n",
      "Epoch: 73 | Batch_idx: 320 |  Loss_1: (0.4534) | Acc_1: (83.22%) (34195/41088)\n",
      "Epoch: 73 | Batch_idx: 330 |  Loss_1: (0.4544) | Acc_1: (83.19%) (35247/42368)\n",
      "Epoch: 73 | Batch_idx: 340 |  Loss_1: (0.4544) | Acc_1: (83.19%) (36309/43648)\n",
      "Epoch: 73 | Batch_idx: 350 |  Loss_1: (0.4542) | Acc_1: (83.20%) (37381/44928)\n",
      "Epoch: 73 | Batch_idx: 360 |  Loss_1: (0.4544) | Acc_1: (83.20%) (38447/46208)\n",
      "Epoch: 73 | Batch_idx: 370 |  Loss_1: (0.4556) | Acc_1: (83.17%) (39496/47488)\n",
      "Epoch: 73 | Batch_idx: 380 |  Loss_1: (0.4558) | Acc_1: (83.14%) (40544/48768)\n",
      "Epoch: 73 | Batch_idx: 390 |  Loss_1: (0.4560) | Acc_1: (83.12%) (41562/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4237) | Acc: (88.90%) (8890/10000)\n",
      "Epoch: 74 | Batch_idx: 0 |  Loss_1: (0.4379) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 74 | Batch_idx: 10 |  Loss_1: (0.4596) | Acc_1: (83.17%) (1171/1408)\n",
      "Epoch: 74 | Batch_idx: 20 |  Loss_1: (0.4527) | Acc_1: (83.37%) (2241/2688)\n",
      "Epoch: 74 | Batch_idx: 30 |  Loss_1: (0.4636) | Acc_1: (82.76%) (3284/3968)\n",
      "Epoch: 74 | Batch_idx: 40 |  Loss_1: (0.4714) | Acc_1: (82.60%) (4335/5248)\n",
      "Epoch: 74 | Batch_idx: 50 |  Loss_1: (0.4722) | Acc_1: (82.60%) (5392/6528)\n",
      "Epoch: 74 | Batch_idx: 60 |  Loss_1: (0.4704) | Acc_1: (82.66%) (6454/7808)\n",
      "Epoch: 74 | Batch_idx: 70 |  Loss_1: (0.4692) | Acc_1: (82.72%) (7518/9088)\n",
      "Epoch: 74 | Batch_idx: 80 |  Loss_1: (0.4640) | Acc_1: (82.90%) (8595/10368)\n",
      "Epoch: 74 | Batch_idx: 90 |  Loss_1: (0.4634) | Acc_1: (82.88%) (9654/11648)\n",
      "Epoch: 74 | Batch_idx: 100 |  Loss_1: (0.4629) | Acc_1: (82.92%) (10720/12928)\n",
      "Epoch: 74 | Batch_idx: 110 |  Loss_1: (0.4592) | Acc_1: (83.06%) (11801/14208)\n",
      "Epoch: 74 | Batch_idx: 120 |  Loss_1: (0.4597) | Acc_1: (83.08%) (12868/15488)\n",
      "Epoch: 74 | Batch_idx: 130 |  Loss_1: (0.4581) | Acc_1: (83.13%) (13940/16768)\n",
      "Epoch: 74 | Batch_idx: 140 |  Loss_1: (0.4563) | Acc_1: (83.21%) (15018/18048)\n",
      "Epoch: 74 | Batch_idx: 150 |  Loss_1: (0.4568) | Acc_1: (83.19%) (16079/19328)\n",
      "Epoch: 74 | Batch_idx: 160 |  Loss_1: (0.4581) | Acc_1: (83.16%) (17137/20608)\n",
      "Epoch: 74 | Batch_idx: 170 |  Loss_1: (0.4581) | Acc_1: (83.19%) (18209/21888)\n",
      "Epoch: 74 | Batch_idx: 180 |  Loss_1: (0.4576) | Acc_1: (83.21%) (19279/23168)\n",
      "Epoch: 74 | Batch_idx: 190 |  Loss_1: (0.4585) | Acc_1: (83.17%) (20333/24448)\n",
      "Epoch: 74 | Batch_idx: 200 |  Loss_1: (0.4546) | Acc_1: (83.30%) (21432/25728)\n",
      "Epoch: 74 | Batch_idx: 210 |  Loss_1: (0.4536) | Acc_1: (83.35%) (22512/27008)\n",
      "Epoch: 74 | Batch_idx: 220 |  Loss_1: (0.4543) | Acc_1: (83.32%) (23570/28288)\n",
      "Epoch: 74 | Batch_idx: 230 |  Loss_1: (0.4531) | Acc_1: (83.37%) (24651/29568)\n",
      "Epoch: 74 | Batch_idx: 240 |  Loss_1: (0.4534) | Acc_1: (83.36%) (25715/30848)\n",
      "Epoch: 74 | Batch_idx: 250 |  Loss_1: (0.4532) | Acc_1: (83.36%) (26781/32128)\n",
      "Epoch: 74 | Batch_idx: 260 |  Loss_1: (0.4543) | Acc_1: (83.32%) (27834/33408)\n",
      "Epoch: 74 | Batch_idx: 270 |  Loss_1: (0.4551) | Acc_1: (83.29%) (28891/34688)\n",
      "Epoch: 74 | Batch_idx: 280 |  Loss_1: (0.4561) | Acc_1: (83.24%) (29939/35968)\n",
      "Epoch: 74 | Batch_idx: 290 |  Loss_1: (0.4545) | Acc_1: (83.27%) (31018/37248)\n",
      "Epoch: 74 | Batch_idx: 300 |  Loss_1: (0.4539) | Acc_1: (83.30%) (32093/38528)\n",
      "Epoch: 74 | Batch_idx: 310 |  Loss_1: (0.4531) | Acc_1: (83.33%) (33172/39808)\n",
      "Epoch: 74 | Batch_idx: 320 |  Loss_1: (0.4524) | Acc_1: (83.37%) (34254/41088)\n",
      "Epoch: 74 | Batch_idx: 330 |  Loss_1: (0.4522) | Acc_1: (83.38%) (35328/42368)\n",
      "Epoch: 74 | Batch_idx: 340 |  Loss_1: (0.4523) | Acc_1: (83.36%) (36384/43648)\n",
      "Epoch: 74 | Batch_idx: 350 |  Loss_1: (0.4528) | Acc_1: (83.34%) (37445/44928)\n",
      "Epoch: 74 | Batch_idx: 360 |  Loss_1: (0.4531) | Acc_1: (83.34%) (38509/46208)\n",
      "Epoch: 74 | Batch_idx: 370 |  Loss_1: (0.4530) | Acc_1: (83.32%) (39569/47488)\n",
      "Epoch: 74 | Batch_idx: 380 |  Loss_1: (0.4520) | Acc_1: (83.36%) (40651/48768)\n",
      "Epoch: 74 | Batch_idx: 390 |  Loss_1: (0.4521) | Acc_1: (83.35%) (41677/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4798) | Acc: (88.17%) (8817/10000)\n",
      "Epoch: 75 | Batch_idx: 0 |  Loss_1: (0.4209) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 75 | Batch_idx: 10 |  Loss_1: (0.4141) | Acc_1: (84.87%) (1195/1408)\n",
      "Epoch: 75 | Batch_idx: 20 |  Loss_1: (0.4152) | Acc_1: (84.71%) (2277/2688)\n",
      "Epoch: 75 | Batch_idx: 30 |  Loss_1: (0.4092) | Acc_1: (85.03%) (3374/3968)\n",
      "Epoch: 75 | Batch_idx: 40 |  Loss_1: (0.4290) | Acc_1: (84.45%) (4432/5248)\n",
      "Epoch: 75 | Batch_idx: 50 |  Loss_1: (0.4351) | Acc_1: (84.18%) (5495/6528)\n",
      "Epoch: 75 | Batch_idx: 60 |  Loss_1: (0.4366) | Acc_1: (84.12%) (6568/7808)\n",
      "Epoch: 75 | Batch_idx: 70 |  Loss_1: (0.4424) | Acc_1: (83.88%) (7623/9088)\n",
      "Epoch: 75 | Batch_idx: 80 |  Loss_1: (0.4396) | Acc_1: (83.97%) (8706/10368)\n",
      "Epoch: 75 | Batch_idx: 90 |  Loss_1: (0.4399) | Acc_1: (83.96%) (9780/11648)\n",
      "Epoch: 75 | Batch_idx: 100 |  Loss_1: (0.4421) | Acc_1: (83.86%) (10842/12928)\n",
      "Epoch: 75 | Batch_idx: 110 |  Loss_1: (0.4432) | Acc_1: (83.76%) (11901/14208)\n",
      "Epoch: 75 | Batch_idx: 120 |  Loss_1: (0.4437) | Acc_1: (83.71%) (12965/15488)\n",
      "Epoch: 75 | Batch_idx: 130 |  Loss_1: (0.4448) | Acc_1: (83.67%) (14030/16768)\n",
      "Epoch: 75 | Batch_idx: 140 |  Loss_1: (0.4466) | Acc_1: (83.59%) (15087/18048)\n",
      "Epoch: 75 | Batch_idx: 150 |  Loss_1: (0.4465) | Acc_1: (83.58%) (16154/19328)\n",
      "Epoch: 75 | Batch_idx: 160 |  Loss_1: (0.4476) | Acc_1: (83.53%) (17214/20608)\n",
      "Epoch: 75 | Batch_idx: 170 |  Loss_1: (0.4512) | Acc_1: (83.41%) (18257/21888)\n",
      "Epoch: 75 | Batch_idx: 180 |  Loss_1: (0.4522) | Acc_1: (83.39%) (19319/23168)\n",
      "Epoch: 75 | Batch_idx: 190 |  Loss_1: (0.4535) | Acc_1: (83.32%) (20371/24448)\n",
      "Epoch: 75 | Batch_idx: 200 |  Loss_1: (0.4550) | Acc_1: (83.26%) (21422/25728)\n",
      "Epoch: 75 | Batch_idx: 210 |  Loss_1: (0.4573) | Acc_1: (83.16%) (22459/27008)\n",
      "Epoch: 75 | Batch_idx: 220 |  Loss_1: (0.4565) | Acc_1: (83.20%) (23536/28288)\n",
      "Epoch: 75 | Batch_idx: 230 |  Loss_1: (0.4581) | Acc_1: (83.11%) (24574/29568)\n",
      "Epoch: 75 | Batch_idx: 240 |  Loss_1: (0.4578) | Acc_1: (83.11%) (25638/30848)\n",
      "Epoch: 75 | Batch_idx: 250 |  Loss_1: (0.4556) | Acc_1: (83.22%) (26737/32128)\n",
      "Epoch: 75 | Batch_idx: 260 |  Loss_1: (0.4547) | Acc_1: (83.24%) (27808/33408)\n",
      "Epoch: 75 | Batch_idx: 270 |  Loss_1: (0.4530) | Acc_1: (83.28%) (28887/34688)\n",
      "Epoch: 75 | Batch_idx: 280 |  Loss_1: (0.4522) | Acc_1: (83.30%) (29961/35968)\n",
      "Epoch: 75 | Batch_idx: 290 |  Loss_1: (0.4520) | Acc_1: (83.27%) (31018/37248)\n",
      "Epoch: 75 | Batch_idx: 300 |  Loss_1: (0.4517) | Acc_1: (83.26%) (32080/38528)\n",
      "Epoch: 75 | Batch_idx: 310 |  Loss_1: (0.4524) | Acc_1: (83.23%) (33132/39808)\n",
      "Epoch: 75 | Batch_idx: 320 |  Loss_1: (0.4532) | Acc_1: (83.21%) (34188/41088)\n",
      "Epoch: 75 | Batch_idx: 330 |  Loss_1: (0.4523) | Acc_1: (83.25%) (35273/42368)\n",
      "Epoch: 75 | Batch_idx: 340 |  Loss_1: (0.4528) | Acc_1: (83.24%) (36332/43648)\n",
      "Epoch: 75 | Batch_idx: 350 |  Loss_1: (0.4524) | Acc_1: (83.27%) (37410/44928)\n",
      "Epoch: 75 | Batch_idx: 360 |  Loss_1: (0.4532) | Acc_1: (83.24%) (38462/46208)\n",
      "Epoch: 75 | Batch_idx: 370 |  Loss_1: (0.4532) | Acc_1: (83.23%) (39522/47488)\n",
      "Epoch: 75 | Batch_idx: 380 |  Loss_1: (0.4525) | Acc_1: (83.25%) (40601/48768)\n",
      "Epoch: 75 | Batch_idx: 390 |  Loss_1: (0.4537) | Acc_1: (83.20%) (41602/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5285) | Acc: (88.01%) (8801/10000)\n",
      "Epoch: 76 | Batch_idx: 0 |  Loss_1: (0.6013) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 76 | Batch_idx: 10 |  Loss_1: (0.4943) | Acc_1: (82.10%) (1156/1408)\n",
      "Epoch: 76 | Batch_idx: 20 |  Loss_1: (0.4630) | Acc_1: (82.89%) (2228/2688)\n",
      "Epoch: 76 | Batch_idx: 30 |  Loss_1: (0.4698) | Acc_1: (82.81%) (3286/3968)\n",
      "Epoch: 76 | Batch_idx: 40 |  Loss_1: (0.4651) | Acc_1: (82.93%) (4352/5248)\n",
      "Epoch: 76 | Batch_idx: 50 |  Loss_1: (0.4597) | Acc_1: (83.07%) (5423/6528)\n",
      "Epoch: 76 | Batch_idx: 60 |  Loss_1: (0.4597) | Acc_1: (82.94%) (6476/7808)\n",
      "Epoch: 76 | Batch_idx: 70 |  Loss_1: (0.4599) | Acc_1: (82.90%) (7534/9088)\n",
      "Epoch: 76 | Batch_idx: 80 |  Loss_1: (0.4566) | Acc_1: (83.07%) (8613/10368)\n",
      "Epoch: 76 | Batch_idx: 90 |  Loss_1: (0.4551) | Acc_1: (83.22%) (9693/11648)\n",
      "Epoch: 76 | Batch_idx: 100 |  Loss_1: (0.4549) | Acc_1: (83.28%) (10767/12928)\n",
      "Epoch: 76 | Batch_idx: 110 |  Loss_1: (0.4565) | Acc_1: (83.20%) (11821/14208)\n",
      "Epoch: 76 | Batch_idx: 120 |  Loss_1: (0.4552) | Acc_1: (83.23%) (12890/15488)\n",
      "Epoch: 76 | Batch_idx: 130 |  Loss_1: (0.4550) | Acc_1: (83.22%) (13954/16768)\n",
      "Epoch: 76 | Batch_idx: 140 |  Loss_1: (0.4536) | Acc_1: (83.27%) (15029/18048)\n",
      "Epoch: 76 | Batch_idx: 150 |  Loss_1: (0.4521) | Acc_1: (83.31%) (16103/19328)\n",
      "Epoch: 76 | Batch_idx: 160 |  Loss_1: (0.4518) | Acc_1: (83.35%) (17177/20608)\n",
      "Epoch: 76 | Batch_idx: 170 |  Loss_1: (0.4524) | Acc_1: (83.35%) (18243/21888)\n",
      "Epoch: 76 | Batch_idx: 180 |  Loss_1: (0.4515) | Acc_1: (83.39%) (19320/23168)\n",
      "Epoch: 76 | Batch_idx: 190 |  Loss_1: (0.4563) | Acc_1: (83.21%) (20344/24448)\n",
      "Epoch: 76 | Batch_idx: 200 |  Loss_1: (0.4575) | Acc_1: (83.16%) (21395/25728)\n",
      "Epoch: 76 | Batch_idx: 210 |  Loss_1: (0.4591) | Acc_1: (83.09%) (22442/27008)\n",
      "Epoch: 76 | Batch_idx: 220 |  Loss_1: (0.4607) | Acc_1: (83.04%) (23489/28288)\n",
      "Epoch: 76 | Batch_idx: 230 |  Loss_1: (0.4598) | Acc_1: (83.08%) (24564/29568)\n",
      "Epoch: 76 | Batch_idx: 240 |  Loss_1: (0.4592) | Acc_1: (83.09%) (25632/30848)\n",
      "Epoch: 76 | Batch_idx: 250 |  Loss_1: (0.4564) | Acc_1: (83.20%) (26730/32128)\n",
      "Epoch: 76 | Batch_idx: 260 |  Loss_1: (0.4573) | Acc_1: (83.15%) (27778/33408)\n",
      "Epoch: 76 | Batch_idx: 270 |  Loss_1: (0.4575) | Acc_1: (83.14%) (28841/34688)\n",
      "Epoch: 76 | Batch_idx: 280 |  Loss_1: (0.4582) | Acc_1: (83.12%) (29896/35968)\n",
      "Epoch: 76 | Batch_idx: 290 |  Loss_1: (0.4586) | Acc_1: (83.09%) (30948/37248)\n",
      "Epoch: 76 | Batch_idx: 300 |  Loss_1: (0.4580) | Acc_1: (83.11%) (32021/38528)\n",
      "Epoch: 76 | Batch_idx: 310 |  Loss_1: (0.4590) | Acc_1: (83.10%) (33081/39808)\n",
      "Epoch: 76 | Batch_idx: 320 |  Loss_1: (0.4591) | Acc_1: (83.09%) (34138/41088)\n",
      "Epoch: 76 | Batch_idx: 330 |  Loss_1: (0.4597) | Acc_1: (83.05%) (35185/42368)\n",
      "Epoch: 76 | Batch_idx: 340 |  Loss_1: (0.4602) | Acc_1: (83.03%) (36241/43648)\n",
      "Epoch: 76 | Batch_idx: 350 |  Loss_1: (0.4596) | Acc_1: (83.06%) (37316/44928)\n",
      "Epoch: 76 | Batch_idx: 360 |  Loss_1: (0.4600) | Acc_1: (83.04%) (38372/46208)\n",
      "Epoch: 76 | Batch_idx: 370 |  Loss_1: (0.4595) | Acc_1: (83.05%) (39440/47488)\n",
      "Epoch: 76 | Batch_idx: 380 |  Loss_1: (0.4581) | Acc_1: (83.10%) (40528/48768)\n",
      "Epoch: 76 | Batch_idx: 390 |  Loss_1: (0.4581) | Acc_1: (83.10%) (41549/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4678) | Acc: (88.40%) (8840/10000)\n",
      "Epoch: 77 | Batch_idx: 0 |  Loss_1: (0.4223) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 77 | Batch_idx: 10 |  Loss_1: (0.4661) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 77 | Batch_idx: 20 |  Loss_1: (0.4564) | Acc_1: (83.52%) (2245/2688)\n",
      "Epoch: 77 | Batch_idx: 30 |  Loss_1: (0.4580) | Acc_1: (83.42%) (3310/3968)\n",
      "Epoch: 77 | Batch_idx: 40 |  Loss_1: (0.4648) | Acc_1: (83.00%) (4356/5248)\n",
      "Epoch: 77 | Batch_idx: 50 |  Loss_1: (0.4662) | Acc_1: (83.01%) (5419/6528)\n",
      "Epoch: 77 | Batch_idx: 60 |  Loss_1: (0.4660) | Acc_1: (82.94%) (6476/7808)\n",
      "Epoch: 77 | Batch_idx: 70 |  Loss_1: (0.4609) | Acc_1: (83.10%) (7552/9088)\n",
      "Epoch: 77 | Batch_idx: 80 |  Loss_1: (0.4585) | Acc_1: (83.17%) (8623/10368)\n",
      "Epoch: 77 | Batch_idx: 90 |  Loss_1: (0.4634) | Acc_1: (82.98%) (9665/11648)\n",
      "Epoch: 77 | Batch_idx: 100 |  Loss_1: (0.4585) | Acc_1: (83.15%) (10750/12928)\n",
      "Epoch: 77 | Batch_idx: 110 |  Loss_1: (0.4586) | Acc_1: (83.18%) (11818/14208)\n",
      "Epoch: 77 | Batch_idx: 120 |  Loss_1: (0.4581) | Acc_1: (83.19%) (12884/15488)\n",
      "Epoch: 77 | Batch_idx: 130 |  Loss_1: (0.4582) | Acc_1: (83.18%) (13947/16768)\n",
      "Epoch: 77 | Batch_idx: 140 |  Loss_1: (0.4575) | Acc_1: (83.22%) (15019/18048)\n",
      "Epoch: 77 | Batch_idx: 150 |  Loss_1: (0.4573) | Acc_1: (83.21%) (16082/19328)\n",
      "Epoch: 77 | Batch_idx: 160 |  Loss_1: (0.4587) | Acc_1: (83.12%) (17130/20608)\n",
      "Epoch: 77 | Batch_idx: 170 |  Loss_1: (0.4565) | Acc_1: (83.20%) (18210/21888)\n",
      "Epoch: 77 | Batch_idx: 180 |  Loss_1: (0.4552) | Acc_1: (83.24%) (19284/23168)\n",
      "Epoch: 77 | Batch_idx: 190 |  Loss_1: (0.4549) | Acc_1: (83.23%) (20348/24448)\n",
      "Epoch: 77 | Batch_idx: 200 |  Loss_1: (0.4555) | Acc_1: (83.21%) (21408/25728)\n",
      "Epoch: 77 | Batch_idx: 210 |  Loss_1: (0.4555) | Acc_1: (83.20%) (22470/27008)\n",
      "Epoch: 77 | Batch_idx: 220 |  Loss_1: (0.4544) | Acc_1: (83.24%) (23548/28288)\n",
      "Epoch: 77 | Batch_idx: 230 |  Loss_1: (0.4564) | Acc_1: (83.19%) (24598/29568)\n",
      "Epoch: 77 | Batch_idx: 240 |  Loss_1: (0.4573) | Acc_1: (83.15%) (25649/30848)\n",
      "Epoch: 77 | Batch_idx: 250 |  Loss_1: (0.4572) | Acc_1: (83.15%) (26714/32128)\n",
      "Epoch: 77 | Batch_idx: 260 |  Loss_1: (0.4580) | Acc_1: (83.10%) (27761/33408)\n",
      "Epoch: 77 | Batch_idx: 270 |  Loss_1: (0.4579) | Acc_1: (83.11%) (28828/34688)\n",
      "Epoch: 77 | Batch_idx: 280 |  Loss_1: (0.4563) | Acc_1: (83.16%) (29912/35968)\n",
      "Epoch: 77 | Batch_idx: 290 |  Loss_1: (0.4568) | Acc_1: (83.15%) (30973/37248)\n",
      "Epoch: 77 | Batch_idx: 300 |  Loss_1: (0.4584) | Acc_1: (83.10%) (32018/38528)\n",
      "Epoch: 77 | Batch_idx: 310 |  Loss_1: (0.4583) | Acc_1: (83.08%) (33074/39808)\n",
      "Epoch: 77 | Batch_idx: 320 |  Loss_1: (0.4569) | Acc_1: (83.15%) (34163/41088)\n",
      "Epoch: 77 | Batch_idx: 330 |  Loss_1: (0.4572) | Acc_1: (83.15%) (35228/42368)\n",
      "Epoch: 77 | Batch_idx: 340 |  Loss_1: (0.4585) | Acc_1: (83.10%) (36271/43648)\n",
      "Epoch: 77 | Batch_idx: 350 |  Loss_1: (0.4574) | Acc_1: (83.14%) (37353/44928)\n",
      "Epoch: 77 | Batch_idx: 360 |  Loss_1: (0.4566) | Acc_1: (83.17%) (38429/46208)\n",
      "Epoch: 77 | Batch_idx: 370 |  Loss_1: (0.4565) | Acc_1: (83.17%) (39494/47488)\n",
      "Epoch: 77 | Batch_idx: 380 |  Loss_1: (0.4568) | Acc_1: (83.15%) (40551/48768)\n",
      "Epoch: 77 | Batch_idx: 390 |  Loss_1: (0.4560) | Acc_1: (83.19%) (41594/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4841) | Acc: (89.02%) (8902/10000)\n",
      "Epoch: 78 | Batch_idx: 0 |  Loss_1: (0.4819) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 78 | Batch_idx: 10 |  Loss_1: (0.4248) | Acc_1: (84.23%) (1186/1408)\n",
      "Epoch: 78 | Batch_idx: 20 |  Loss_1: (0.4373) | Acc_1: (83.89%) (2255/2688)\n",
      "Epoch: 78 | Batch_idx: 30 |  Loss_1: (0.4512) | Acc_1: (83.24%) (3303/3968)\n",
      "Epoch: 78 | Batch_idx: 40 |  Loss_1: (0.4501) | Acc_1: (83.25%) (4369/5248)\n",
      "Epoch: 78 | Batch_idx: 50 |  Loss_1: (0.4555) | Acc_1: (82.92%) (5413/6528)\n",
      "Epoch: 78 | Batch_idx: 60 |  Loss_1: (0.4575) | Acc_1: (82.83%) (6467/7808)\n",
      "Epoch: 78 | Batch_idx: 70 |  Loss_1: (0.4569) | Acc_1: (82.89%) (7533/9088)\n",
      "Epoch: 78 | Batch_idx: 80 |  Loss_1: (0.4570) | Acc_1: (82.94%) (8599/10368)\n",
      "Epoch: 78 | Batch_idx: 90 |  Loss_1: (0.4582) | Acc_1: (82.95%) (9662/11648)\n",
      "Epoch: 78 | Batch_idx: 100 |  Loss_1: (0.4624) | Acc_1: (82.81%) (10706/12928)\n",
      "Epoch: 78 | Batch_idx: 110 |  Loss_1: (0.4632) | Acc_1: (82.80%) (11764/14208)\n",
      "Epoch: 78 | Batch_idx: 120 |  Loss_1: (0.4612) | Acc_1: (82.86%) (12834/15488)\n",
      "Epoch: 78 | Batch_idx: 130 |  Loss_1: (0.4605) | Acc_1: (82.90%) (13901/16768)\n",
      "Epoch: 78 | Batch_idx: 140 |  Loss_1: (0.4563) | Acc_1: (83.06%) (14990/18048)\n",
      "Epoch: 78 | Batch_idx: 150 |  Loss_1: (0.4568) | Acc_1: (83.02%) (16047/19328)\n",
      "Epoch: 78 | Batch_idx: 160 |  Loss_1: (0.4557) | Acc_1: (83.08%) (17122/20608)\n",
      "Epoch: 78 | Batch_idx: 170 |  Loss_1: (0.4584) | Acc_1: (82.97%) (18160/21888)\n",
      "Epoch: 78 | Batch_idx: 180 |  Loss_1: (0.4582) | Acc_1: (83.01%) (19231/23168)\n",
      "Epoch: 78 | Batch_idx: 190 |  Loss_1: (0.4579) | Acc_1: (83.03%) (20298/24448)\n",
      "Epoch: 78 | Batch_idx: 200 |  Loss_1: (0.4589) | Acc_1: (83.00%) (21354/25728)\n",
      "Epoch: 78 | Batch_idx: 210 |  Loss_1: (0.4575) | Acc_1: (83.08%) (22438/27008)\n",
      "Epoch: 78 | Batch_idx: 220 |  Loss_1: (0.4567) | Acc_1: (83.11%) (23510/28288)\n",
      "Epoch: 78 | Batch_idx: 230 |  Loss_1: (0.4573) | Acc_1: (83.08%) (24566/29568)\n",
      "Epoch: 78 | Batch_idx: 240 |  Loss_1: (0.4574) | Acc_1: (83.07%) (25625/30848)\n",
      "Epoch: 78 | Batch_idx: 250 |  Loss_1: (0.4576) | Acc_1: (83.07%) (26689/32128)\n",
      "Epoch: 78 | Batch_idx: 260 |  Loss_1: (0.4567) | Acc_1: (83.11%) (27767/33408)\n",
      "Epoch: 78 | Batch_idx: 270 |  Loss_1: (0.4565) | Acc_1: (83.14%) (28839/34688)\n",
      "Epoch: 78 | Batch_idx: 280 |  Loss_1: (0.4562) | Acc_1: (83.15%) (29907/35968)\n",
      "Epoch: 78 | Batch_idx: 290 |  Loss_1: (0.4568) | Acc_1: (83.11%) (30955/37248)\n",
      "Epoch: 78 | Batch_idx: 300 |  Loss_1: (0.4560) | Acc_1: (83.14%) (32031/38528)\n",
      "Epoch: 78 | Batch_idx: 310 |  Loss_1: (0.4572) | Acc_1: (83.09%) (33075/39808)\n",
      "Epoch: 78 | Batch_idx: 320 |  Loss_1: (0.4557) | Acc_1: (83.15%) (34164/41088)\n",
      "Epoch: 78 | Batch_idx: 330 |  Loss_1: (0.4556) | Acc_1: (83.15%) (35228/42368)\n",
      "Epoch: 78 | Batch_idx: 340 |  Loss_1: (0.4542) | Acc_1: (83.21%) (36319/43648)\n",
      "Epoch: 78 | Batch_idx: 350 |  Loss_1: (0.4536) | Acc_1: (83.24%) (37396/44928)\n",
      "Epoch: 78 | Batch_idx: 360 |  Loss_1: (0.4539) | Acc_1: (83.22%) (38455/46208)\n",
      "Epoch: 78 | Batch_idx: 370 |  Loss_1: (0.4534) | Acc_1: (83.25%) (39533/47488)\n",
      "Epoch: 78 | Batch_idx: 380 |  Loss_1: (0.4534) | Acc_1: (83.27%) (40608/48768)\n",
      "Epoch: 78 | Batch_idx: 390 |  Loss_1: (0.4519) | Acc_1: (83.31%) (41657/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4749) | Acc: (89.01%) (8901/10000)\n",
      "Epoch: 79 | Batch_idx: 0 |  Loss_1: (0.4303) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 79 | Batch_idx: 10 |  Loss_1: (0.4457) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 79 | Batch_idx: 20 |  Loss_1: (0.4560) | Acc_1: (83.00%) (2231/2688)\n",
      "Epoch: 79 | Batch_idx: 30 |  Loss_1: (0.4334) | Acc_1: (83.97%) (3332/3968)\n",
      "Epoch: 79 | Batch_idx: 40 |  Loss_1: (0.4370) | Acc_1: (83.84%) (4400/5248)\n",
      "Epoch: 79 | Batch_idx: 50 |  Loss_1: (0.4402) | Acc_1: (83.61%) (5458/6528)\n",
      "Epoch: 79 | Batch_idx: 60 |  Loss_1: (0.4448) | Acc_1: (83.49%) (6519/7808)\n",
      "Epoch: 79 | Batch_idx: 70 |  Loss_1: (0.4472) | Acc_1: (83.36%) (7576/9088)\n",
      "Epoch: 79 | Batch_idx: 80 |  Loss_1: (0.4448) | Acc_1: (83.54%) (8661/10368)\n",
      "Epoch: 79 | Batch_idx: 90 |  Loss_1: (0.4448) | Acc_1: (83.52%) (9728/11648)\n",
      "Epoch: 79 | Batch_idx: 100 |  Loss_1: (0.4439) | Acc_1: (83.56%) (10803/12928)\n",
      "Epoch: 79 | Batch_idx: 110 |  Loss_1: (0.4443) | Acc_1: (83.56%) (11872/14208)\n",
      "Epoch: 79 | Batch_idx: 120 |  Loss_1: (0.4488) | Acc_1: (83.39%) (12915/15488)\n",
      "Epoch: 79 | Batch_idx: 130 |  Loss_1: (0.4528) | Acc_1: (83.25%) (13959/16768)\n",
      "Epoch: 79 | Batch_idx: 140 |  Loss_1: (0.4510) | Acc_1: (83.33%) (15039/18048)\n",
      "Epoch: 79 | Batch_idx: 150 |  Loss_1: (0.4527) | Acc_1: (83.26%) (16093/19328)\n",
      "Epoch: 79 | Batch_idx: 160 |  Loss_1: (0.4540) | Acc_1: (83.22%) (17149/20608)\n",
      "Epoch: 79 | Batch_idx: 170 |  Loss_1: (0.4515) | Acc_1: (83.28%) (18229/21888)\n",
      "Epoch: 79 | Batch_idx: 180 |  Loss_1: (0.4533) | Acc_1: (83.22%) (19280/23168)\n",
      "Epoch: 79 | Batch_idx: 190 |  Loss_1: (0.4527) | Acc_1: (83.25%) (20352/24448)\n",
      "Epoch: 79 | Batch_idx: 200 |  Loss_1: (0.4554) | Acc_1: (83.14%) (21389/25728)\n",
      "Epoch: 79 | Batch_idx: 210 |  Loss_1: (0.4568) | Acc_1: (83.08%) (22439/27008)\n",
      "Epoch: 79 | Batch_idx: 220 |  Loss_1: (0.4573) | Acc_1: (83.05%) (23492/28288)\n",
      "Epoch: 79 | Batch_idx: 230 |  Loss_1: (0.4572) | Acc_1: (83.05%) (24557/29568)\n",
      "Epoch: 79 | Batch_idx: 240 |  Loss_1: (0.4561) | Acc_1: (83.10%) (25635/30848)\n",
      "Epoch: 79 | Batch_idx: 250 |  Loss_1: (0.4546) | Acc_1: (83.17%) (26722/32128)\n",
      "Epoch: 79 | Batch_idx: 260 |  Loss_1: (0.4531) | Acc_1: (83.21%) (27799/33408)\n",
      "Epoch: 79 | Batch_idx: 270 |  Loss_1: (0.4542) | Acc_1: (83.17%) (28851/34688)\n",
      "Epoch: 79 | Batch_idx: 280 |  Loss_1: (0.4561) | Acc_1: (83.10%) (29889/35968)\n",
      "Epoch: 79 | Batch_idx: 290 |  Loss_1: (0.4572) | Acc_1: (83.08%) (30946/37248)\n",
      "Epoch: 79 | Batch_idx: 300 |  Loss_1: (0.4582) | Acc_1: (83.05%) (31997/38528)\n",
      "Epoch: 79 | Batch_idx: 310 |  Loss_1: (0.4572) | Acc_1: (83.10%) (33079/39808)\n",
      "Epoch: 79 | Batch_idx: 320 |  Loss_1: (0.4569) | Acc_1: (83.13%) (34155/41088)\n",
      "Epoch: 79 | Batch_idx: 330 |  Loss_1: (0.4556) | Acc_1: (83.18%) (35242/42368)\n",
      "Epoch: 79 | Batch_idx: 340 |  Loss_1: (0.4553) | Acc_1: (83.20%) (36317/43648)\n",
      "Epoch: 79 | Batch_idx: 350 |  Loss_1: (0.4555) | Acc_1: (83.18%) (37370/44928)\n",
      "Epoch: 79 | Batch_idx: 360 |  Loss_1: (0.4566) | Acc_1: (83.13%) (38414/46208)\n",
      "Epoch: 79 | Batch_idx: 370 |  Loss_1: (0.4567) | Acc_1: (83.14%) (39482/47488)\n",
      "Epoch: 79 | Batch_idx: 380 |  Loss_1: (0.4578) | Acc_1: (83.11%) (40533/48768)\n",
      "Epoch: 79 | Batch_idx: 390 |  Loss_1: (0.4590) | Acc_1: (83.07%) (41537/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4654) | Acc: (88.74%) (8874/10000)\n",
      "Epoch: 80 | Batch_idx: 0 |  Loss_1: (0.4540) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 80 | Batch_idx: 10 |  Loss_1: (0.4358) | Acc_1: (83.95%) (1182/1408)\n",
      "Epoch: 80 | Batch_idx: 20 |  Loss_1: (0.4412) | Acc_1: (83.71%) (2250/2688)\n",
      "Epoch: 80 | Batch_idx: 30 |  Loss_1: (0.4447) | Acc_1: (83.44%) (3311/3968)\n",
      "Epoch: 80 | Batch_idx: 40 |  Loss_1: (0.4433) | Acc_1: (83.50%) (4382/5248)\n",
      "Epoch: 80 | Batch_idx: 50 |  Loss_1: (0.4541) | Acc_1: (83.18%) (5430/6528)\n",
      "Epoch: 80 | Batch_idx: 60 |  Loss_1: (0.4512) | Acc_1: (83.32%) (6506/7808)\n",
      "Epoch: 80 | Batch_idx: 70 |  Loss_1: (0.4584) | Acc_1: (83.12%) (7554/9088)\n",
      "Epoch: 80 | Batch_idx: 80 |  Loss_1: (0.4580) | Acc_1: (83.14%) (8620/10368)\n",
      "Epoch: 80 | Batch_idx: 90 |  Loss_1: (0.4577) | Acc_1: (83.16%) (9687/11648)\n",
      "Epoch: 80 | Batch_idx: 100 |  Loss_1: (0.4581) | Acc_1: (83.15%) (10749/12928)\n",
      "Epoch: 80 | Batch_idx: 110 |  Loss_1: (0.4561) | Acc_1: (83.17%) (11817/14208)\n",
      "Epoch: 80 | Batch_idx: 120 |  Loss_1: (0.4549) | Acc_1: (83.20%) (12886/15488)\n",
      "Epoch: 80 | Batch_idx: 130 |  Loss_1: (0.4566) | Acc_1: (83.11%) (13936/16768)\n",
      "Epoch: 80 | Batch_idx: 140 |  Loss_1: (0.4552) | Acc_1: (83.16%) (15009/18048)\n",
      "Epoch: 80 | Batch_idx: 150 |  Loss_1: (0.4575) | Acc_1: (83.11%) (16063/19328)\n",
      "Epoch: 80 | Batch_idx: 160 |  Loss_1: (0.4585) | Acc_1: (83.09%) (17123/20608)\n",
      "Epoch: 80 | Batch_idx: 170 |  Loss_1: (0.4582) | Acc_1: (83.12%) (18193/21888)\n",
      "Epoch: 80 | Batch_idx: 180 |  Loss_1: (0.4583) | Acc_1: (83.12%) (19258/23168)\n",
      "Epoch: 80 | Batch_idx: 190 |  Loss_1: (0.4576) | Acc_1: (83.14%) (20325/24448)\n",
      "Epoch: 80 | Batch_idx: 200 |  Loss_1: (0.4572) | Acc_1: (83.20%) (21405/25728)\n",
      "Epoch: 80 | Batch_idx: 210 |  Loss_1: (0.4570) | Acc_1: (83.20%) (22470/27008)\n",
      "Epoch: 80 | Batch_idx: 220 |  Loss_1: (0.4559) | Acc_1: (83.23%) (23544/28288)\n",
      "Epoch: 80 | Batch_idx: 230 |  Loss_1: (0.4564) | Acc_1: (83.19%) (24599/29568)\n",
      "Epoch: 80 | Batch_idx: 240 |  Loss_1: (0.4575) | Acc_1: (83.14%) (25648/30848)\n",
      "Epoch: 80 | Batch_idx: 250 |  Loss_1: (0.4579) | Acc_1: (83.15%) (26715/32128)\n",
      "Epoch: 80 | Batch_idx: 260 |  Loss_1: (0.4584) | Acc_1: (83.13%) (27771/33408)\n",
      "Epoch: 80 | Batch_idx: 270 |  Loss_1: (0.4593) | Acc_1: (83.10%) (28824/34688)\n",
      "Epoch: 80 | Batch_idx: 280 |  Loss_1: (0.4583) | Acc_1: (83.12%) (29896/35968)\n",
      "Epoch: 80 | Batch_idx: 290 |  Loss_1: (0.4594) | Acc_1: (83.07%) (30943/37248)\n",
      "Epoch: 80 | Batch_idx: 300 |  Loss_1: (0.4596) | Acc_1: (83.07%) (32006/38528)\n",
      "Epoch: 80 | Batch_idx: 310 |  Loss_1: (0.4595) | Acc_1: (83.07%) (33067/39808)\n",
      "Epoch: 80 | Batch_idx: 320 |  Loss_1: (0.4594) | Acc_1: (83.07%) (34131/41088)\n",
      "Epoch: 80 | Batch_idx: 330 |  Loss_1: (0.4590) | Acc_1: (83.08%) (35198/42368)\n",
      "Epoch: 80 | Batch_idx: 340 |  Loss_1: (0.4595) | Acc_1: (83.05%) (36251/43648)\n",
      "Epoch: 80 | Batch_idx: 350 |  Loss_1: (0.4595) | Acc_1: (83.06%) (37315/44928)\n",
      "Epoch: 80 | Batch_idx: 360 |  Loss_1: (0.4598) | Acc_1: (83.05%) (38377/46208)\n",
      "Epoch: 80 | Batch_idx: 370 |  Loss_1: (0.4606) | Acc_1: (83.01%) (39421/47488)\n",
      "Epoch: 80 | Batch_idx: 380 |  Loss_1: (0.4614) | Acc_1: (82.98%) (40470/48768)\n",
      "Epoch: 80 | Batch_idx: 390 |  Loss_1: (0.4616) | Acc_1: (82.98%) (41490/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4664) | Acc: (88.16%) (8816/10000)\n",
      "Epoch: 81 | Batch_idx: 0 |  Loss_1: (0.5908) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 81 | Batch_idx: 10 |  Loss_1: (0.4679) | Acc_1: (82.81%) (1166/1408)\n",
      "Epoch: 81 | Batch_idx: 20 |  Loss_1: (0.4455) | Acc_1: (83.89%) (2255/2688)\n",
      "Epoch: 81 | Batch_idx: 30 |  Loss_1: (0.4474) | Acc_1: (83.67%) (3320/3968)\n",
      "Epoch: 81 | Batch_idx: 40 |  Loss_1: (0.4603) | Acc_1: (83.21%) (4367/5248)\n",
      "Epoch: 81 | Batch_idx: 50 |  Loss_1: (0.4559) | Acc_1: (83.38%) (5443/6528)\n",
      "Epoch: 81 | Batch_idx: 60 |  Loss_1: (0.4471) | Acc_1: (83.71%) (6536/7808)\n",
      "Epoch: 81 | Batch_idx: 70 |  Loss_1: (0.4466) | Acc_1: (83.68%) (7605/9088)\n",
      "Epoch: 81 | Batch_idx: 80 |  Loss_1: (0.4443) | Acc_1: (83.74%) (8682/10368)\n",
      "Epoch: 81 | Batch_idx: 90 |  Loss_1: (0.4427) | Acc_1: (83.79%) (9760/11648)\n",
      "Epoch: 81 | Batch_idx: 100 |  Loss_1: (0.4433) | Acc_1: (83.78%) (10831/12928)\n",
      "Epoch: 81 | Batch_idx: 110 |  Loss_1: (0.4447) | Acc_1: (83.71%) (11894/14208)\n",
      "Epoch: 81 | Batch_idx: 120 |  Loss_1: (0.4472) | Acc_1: (83.61%) (12949/15488)\n",
      "Epoch: 81 | Batch_idx: 130 |  Loss_1: (0.4518) | Acc_1: (83.43%) (13990/16768)\n",
      "Epoch: 81 | Batch_idx: 140 |  Loss_1: (0.4507) | Acc_1: (83.42%) (15056/18048)\n",
      "Epoch: 81 | Batch_idx: 150 |  Loss_1: (0.4503) | Acc_1: (83.43%) (16126/19328)\n",
      "Epoch: 81 | Batch_idx: 160 |  Loss_1: (0.4510) | Acc_1: (83.41%) (17190/20608)\n",
      "Epoch: 81 | Batch_idx: 170 |  Loss_1: (0.4521) | Acc_1: (83.38%) (18250/21888)\n",
      "Epoch: 81 | Batch_idx: 180 |  Loss_1: (0.4527) | Acc_1: (83.33%) (19307/23168)\n",
      "Epoch: 81 | Batch_idx: 190 |  Loss_1: (0.4521) | Acc_1: (83.36%) (20379/24448)\n",
      "Epoch: 81 | Batch_idx: 200 |  Loss_1: (0.4543) | Acc_1: (83.26%) (21421/25728)\n",
      "Epoch: 81 | Batch_idx: 210 |  Loss_1: (0.4550) | Acc_1: (83.25%) (22483/27008)\n",
      "Epoch: 81 | Batch_idx: 220 |  Loss_1: (0.4555) | Acc_1: (83.25%) (23550/28288)\n",
      "Epoch: 81 | Batch_idx: 230 |  Loss_1: (0.4565) | Acc_1: (83.20%) (24601/29568)\n",
      "Epoch: 81 | Batch_idx: 240 |  Loss_1: (0.4565) | Acc_1: (83.22%) (25672/30848)\n",
      "Epoch: 81 | Batch_idx: 250 |  Loss_1: (0.4558) | Acc_1: (83.23%) (26741/32128)\n",
      "Epoch: 81 | Batch_idx: 260 |  Loss_1: (0.4569) | Acc_1: (83.18%) (27789/33408)\n",
      "Epoch: 81 | Batch_idx: 270 |  Loss_1: (0.4565) | Acc_1: (83.19%) (28857/34688)\n",
      "Epoch: 81 | Batch_idx: 280 |  Loss_1: (0.4553) | Acc_1: (83.23%) (29936/35968)\n",
      "Epoch: 81 | Batch_idx: 290 |  Loss_1: (0.4554) | Acc_1: (83.23%) (31001/37248)\n",
      "Epoch: 81 | Batch_idx: 300 |  Loss_1: (0.4545) | Acc_1: (83.27%) (32082/38528)\n",
      "Epoch: 81 | Batch_idx: 310 |  Loss_1: (0.4541) | Acc_1: (83.29%) (33157/39808)\n",
      "Epoch: 81 | Batch_idx: 320 |  Loss_1: (0.4545) | Acc_1: (83.30%) (34225/41088)\n",
      "Epoch: 81 | Batch_idx: 330 |  Loss_1: (0.4546) | Acc_1: (83.29%) (35289/42368)\n",
      "Epoch: 81 | Batch_idx: 340 |  Loss_1: (0.4542) | Acc_1: (83.30%) (36357/43648)\n",
      "Epoch: 81 | Batch_idx: 350 |  Loss_1: (0.4543) | Acc_1: (83.30%) (37423/44928)\n",
      "Epoch: 81 | Batch_idx: 360 |  Loss_1: (0.4549) | Acc_1: (83.27%) (38479/46208)\n",
      "Epoch: 81 | Batch_idx: 370 |  Loss_1: (0.4558) | Acc_1: (83.24%) (39527/47488)\n",
      "Epoch: 81 | Batch_idx: 380 |  Loss_1: (0.4562) | Acc_1: (83.23%) (40589/48768)\n",
      "Epoch: 81 | Batch_idx: 390 |  Loss_1: (0.4559) | Acc_1: (83.25%) (41624/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4588) | Acc: (88.97%) (8897/10000)\n",
      "Epoch: 82 | Batch_idx: 0 |  Loss_1: (0.3756) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 82 | Batch_idx: 10 |  Loss_1: (0.4265) | Acc_1: (84.59%) (1191/1408)\n",
      "Epoch: 82 | Batch_idx: 20 |  Loss_1: (0.4501) | Acc_1: (83.82%) (2253/2688)\n",
      "Epoch: 82 | Batch_idx: 30 |  Loss_1: (0.4573) | Acc_1: (83.42%) (3310/3968)\n",
      "Epoch: 82 | Batch_idx: 40 |  Loss_1: (0.4562) | Acc_1: (83.50%) (4382/5248)\n",
      "Epoch: 82 | Batch_idx: 50 |  Loss_1: (0.4472) | Acc_1: (83.75%) (5467/6528)\n",
      "Epoch: 82 | Batch_idx: 60 |  Loss_1: (0.4585) | Acc_1: (83.26%) (6501/7808)\n",
      "Epoch: 82 | Batch_idx: 70 |  Loss_1: (0.4609) | Acc_1: (83.11%) (7553/9088)\n",
      "Epoch: 82 | Batch_idx: 80 |  Loss_1: (0.4609) | Acc_1: (83.03%) (8609/10368)\n",
      "Epoch: 82 | Batch_idx: 90 |  Loss_1: (0.4591) | Acc_1: (83.12%) (9682/11648)\n",
      "Epoch: 82 | Batch_idx: 100 |  Loss_1: (0.4564) | Acc_1: (83.18%) (10754/12928)\n",
      "Epoch: 82 | Batch_idx: 110 |  Loss_1: (0.4542) | Acc_1: (83.25%) (11828/14208)\n",
      "Epoch: 82 | Batch_idx: 120 |  Loss_1: (0.4559) | Acc_1: (83.19%) (12885/15488)\n",
      "Epoch: 82 | Batch_idx: 130 |  Loss_1: (0.4535) | Acc_1: (83.28%) (13964/16768)\n",
      "Epoch: 82 | Batch_idx: 140 |  Loss_1: (0.4509) | Acc_1: (83.37%) (15046/18048)\n",
      "Epoch: 82 | Batch_idx: 150 |  Loss_1: (0.4540) | Acc_1: (83.26%) (16093/19328)\n",
      "Epoch: 82 | Batch_idx: 160 |  Loss_1: (0.4546) | Acc_1: (83.22%) (17150/20608)\n",
      "Epoch: 82 | Batch_idx: 170 |  Loss_1: (0.4546) | Acc_1: (83.22%) (18215/21888)\n",
      "Epoch: 82 | Batch_idx: 180 |  Loss_1: (0.4543) | Acc_1: (83.23%) (19282/23168)\n",
      "Epoch: 82 | Batch_idx: 190 |  Loss_1: (0.4545) | Acc_1: (83.23%) (20348/24448)\n",
      "Epoch: 82 | Batch_idx: 200 |  Loss_1: (0.4551) | Acc_1: (83.20%) (21406/25728)\n",
      "Epoch: 82 | Batch_idx: 210 |  Loss_1: (0.4555) | Acc_1: (83.17%) (22463/27008)\n",
      "Epoch: 82 | Batch_idx: 220 |  Loss_1: (0.4553) | Acc_1: (83.18%) (23529/28288)\n",
      "Epoch: 82 | Batch_idx: 230 |  Loss_1: (0.4580) | Acc_1: (83.07%) (24562/29568)\n",
      "Epoch: 82 | Batch_idx: 240 |  Loss_1: (0.4561) | Acc_1: (83.16%) (25653/30848)\n",
      "Epoch: 82 | Batch_idx: 250 |  Loss_1: (0.4566) | Acc_1: (83.14%) (26710/32128)\n",
      "Epoch: 82 | Batch_idx: 260 |  Loss_1: (0.4566) | Acc_1: (83.13%) (27771/33408)\n",
      "Epoch: 82 | Batch_idx: 270 |  Loss_1: (0.4573) | Acc_1: (83.11%) (28830/34688)\n",
      "Epoch: 82 | Batch_idx: 280 |  Loss_1: (0.4571) | Acc_1: (83.13%) (29899/35968)\n",
      "Epoch: 82 | Batch_idx: 290 |  Loss_1: (0.4586) | Acc_1: (83.08%) (30944/37248)\n",
      "Epoch: 82 | Batch_idx: 300 |  Loss_1: (0.4595) | Acc_1: (83.03%) (31991/38528)\n",
      "Epoch: 82 | Batch_idx: 310 |  Loss_1: (0.4588) | Acc_1: (83.06%) (33065/39808)\n",
      "Epoch: 82 | Batch_idx: 320 |  Loss_1: (0.4597) | Acc_1: (83.02%) (34110/41088)\n",
      "Epoch: 82 | Batch_idx: 330 |  Loss_1: (0.4592) | Acc_1: (83.01%) (35171/42368)\n",
      "Epoch: 82 | Batch_idx: 340 |  Loss_1: (0.4588) | Acc_1: (83.03%) (36243/43648)\n",
      "Epoch: 82 | Batch_idx: 350 |  Loss_1: (0.4592) | Acc_1: (83.02%) (37299/44928)\n",
      "Epoch: 82 | Batch_idx: 360 |  Loss_1: (0.4578) | Acc_1: (83.09%) (38392/46208)\n",
      "Epoch: 82 | Batch_idx: 370 |  Loss_1: (0.4583) | Acc_1: (83.04%) (39434/47488)\n",
      "Epoch: 82 | Batch_idx: 380 |  Loss_1: (0.4586) | Acc_1: (83.03%) (40493/48768)\n",
      "Epoch: 82 | Batch_idx: 390 |  Loss_1: (0.4575) | Acc_1: (83.07%) (41537/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4365) | Acc: (88.96%) (8896/10000)\n",
      "Epoch: 83 | Batch_idx: 0 |  Loss_1: (0.3584) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 83 | Batch_idx: 10 |  Loss_1: (0.4369) | Acc_1: (83.81%) (1180/1408)\n",
      "Epoch: 83 | Batch_idx: 20 |  Loss_1: (0.4515) | Acc_1: (83.26%) (2238/2688)\n",
      "Epoch: 83 | Batch_idx: 30 |  Loss_1: (0.4594) | Acc_1: (82.96%) (3292/3968)\n",
      "Epoch: 83 | Batch_idx: 40 |  Loss_1: (0.4605) | Acc_1: (82.87%) (4349/5248)\n",
      "Epoch: 83 | Batch_idx: 50 |  Loss_1: (0.4611) | Acc_1: (82.89%) (5411/6528)\n",
      "Epoch: 83 | Batch_idx: 60 |  Loss_1: (0.4704) | Acc_1: (82.52%) (6443/7808)\n",
      "Epoch: 83 | Batch_idx: 70 |  Loss_1: (0.4661) | Acc_1: (82.66%) (7512/9088)\n",
      "Epoch: 83 | Batch_idx: 80 |  Loss_1: (0.4678) | Acc_1: (82.66%) (8570/10368)\n",
      "Epoch: 83 | Batch_idx: 90 |  Loss_1: (0.4628) | Acc_1: (82.85%) (9650/11648)\n",
      "Epoch: 83 | Batch_idx: 100 |  Loss_1: (0.4598) | Acc_1: (82.99%) (10729/12928)\n",
      "Epoch: 83 | Batch_idx: 110 |  Loss_1: (0.4607) | Acc_1: (82.97%) (11789/14208)\n",
      "Epoch: 83 | Batch_idx: 120 |  Loss_1: (0.4636) | Acc_1: (82.83%) (12828/15488)\n",
      "Epoch: 83 | Batch_idx: 130 |  Loss_1: (0.4625) | Acc_1: (82.87%) (13895/16768)\n",
      "Epoch: 83 | Batch_idx: 140 |  Loss_1: (0.4652) | Acc_1: (82.79%) (14942/18048)\n",
      "Epoch: 83 | Batch_idx: 150 |  Loss_1: (0.4660) | Acc_1: (82.75%) (15994/19328)\n",
      "Epoch: 83 | Batch_idx: 160 |  Loss_1: (0.4676) | Acc_1: (82.71%) (17044/20608)\n",
      "Epoch: 83 | Batch_idx: 170 |  Loss_1: (0.4664) | Acc_1: (82.77%) (18116/21888)\n",
      "Epoch: 83 | Batch_idx: 180 |  Loss_1: (0.4649) | Acc_1: (82.85%) (19194/23168)\n",
      "Epoch: 83 | Batch_idx: 190 |  Loss_1: (0.4659) | Acc_1: (82.80%) (20242/24448)\n",
      "Epoch: 83 | Batch_idx: 200 |  Loss_1: (0.4646) | Acc_1: (82.86%) (21317/25728)\n",
      "Epoch: 83 | Batch_idx: 210 |  Loss_1: (0.4627) | Acc_1: (82.93%) (22398/27008)\n",
      "Epoch: 83 | Batch_idx: 220 |  Loss_1: (0.4639) | Acc_1: (82.87%) (23443/28288)\n",
      "Epoch: 83 | Batch_idx: 230 |  Loss_1: (0.4667) | Acc_1: (82.78%) (24476/29568)\n",
      "Epoch: 83 | Batch_idx: 240 |  Loss_1: (0.4664) | Acc_1: (82.79%) (25538/30848)\n",
      "Epoch: 83 | Batch_idx: 250 |  Loss_1: (0.4658) | Acc_1: (82.78%) (26597/32128)\n",
      "Epoch: 83 | Batch_idx: 260 |  Loss_1: (0.4653) | Acc_1: (82.80%) (27661/33408)\n",
      "Epoch: 83 | Batch_idx: 270 |  Loss_1: (0.4654) | Acc_1: (82.81%) (28724/34688)\n",
      "Epoch: 83 | Batch_idx: 280 |  Loss_1: (0.4658) | Acc_1: (82.81%) (29784/35968)\n",
      "Epoch: 83 | Batch_idx: 290 |  Loss_1: (0.4663) | Acc_1: (82.79%) (30837/37248)\n",
      "Epoch: 83 | Batch_idx: 300 |  Loss_1: (0.4661) | Acc_1: (82.79%) (31899/38528)\n",
      "Epoch: 83 | Batch_idx: 310 |  Loss_1: (0.4647) | Acc_1: (82.86%) (32984/39808)\n",
      "Epoch: 83 | Batch_idx: 320 |  Loss_1: (0.4642) | Acc_1: (82.86%) (34047/41088)\n",
      "Epoch: 83 | Batch_idx: 330 |  Loss_1: (0.4627) | Acc_1: (82.90%) (35124/42368)\n",
      "Epoch: 83 | Batch_idx: 340 |  Loss_1: (0.4627) | Acc_1: (82.93%) (36196/43648)\n",
      "Epoch: 83 | Batch_idx: 350 |  Loss_1: (0.4628) | Acc_1: (82.91%) (37252/44928)\n",
      "Epoch: 83 | Batch_idx: 360 |  Loss_1: (0.4632) | Acc_1: (82.91%) (38309/46208)\n",
      "Epoch: 83 | Batch_idx: 370 |  Loss_1: (0.4619) | Acc_1: (82.94%) (39385/47488)\n",
      "Epoch: 83 | Batch_idx: 380 |  Loss_1: (0.4616) | Acc_1: (82.93%) (40443/48768)\n",
      "Epoch: 83 | Batch_idx: 390 |  Loss_1: (0.4607) | Acc_1: (82.97%) (41484/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4726) | Acc: (89.06%) (8906/10000)\n",
      "Epoch: 84 | Batch_idx: 0 |  Loss_1: (0.4818) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 84 | Batch_idx: 10 |  Loss_1: (0.4994) | Acc_1: (81.39%) (1146/1408)\n",
      "Epoch: 84 | Batch_idx: 20 |  Loss_1: (0.4872) | Acc_1: (81.99%) (2204/2688)\n",
      "Epoch: 84 | Batch_idx: 30 |  Loss_1: (0.4699) | Acc_1: (82.56%) (3276/3968)\n",
      "Epoch: 84 | Batch_idx: 40 |  Loss_1: (0.4622) | Acc_1: (82.85%) (4348/5248)\n",
      "Epoch: 84 | Batch_idx: 50 |  Loss_1: (0.4607) | Acc_1: (82.98%) (5417/6528)\n",
      "Epoch: 84 | Batch_idx: 60 |  Loss_1: (0.4591) | Acc_1: (83.02%) (6482/7808)\n",
      "Epoch: 84 | Batch_idx: 70 |  Loss_1: (0.4556) | Acc_1: (83.15%) (7557/9088)\n",
      "Epoch: 84 | Batch_idx: 80 |  Loss_1: (0.4555) | Acc_1: (83.15%) (8621/10368)\n",
      "Epoch: 84 | Batch_idx: 90 |  Loss_1: (0.4560) | Acc_1: (83.15%) (9685/11648)\n",
      "Epoch: 84 | Batch_idx: 100 |  Loss_1: (0.4545) | Acc_1: (83.23%) (10760/12928)\n",
      "Epoch: 84 | Batch_idx: 110 |  Loss_1: (0.4557) | Acc_1: (83.17%) (11817/14208)\n",
      "Epoch: 84 | Batch_idx: 120 |  Loss_1: (0.4567) | Acc_1: (83.12%) (12874/15488)\n",
      "Epoch: 84 | Batch_idx: 130 |  Loss_1: (0.4541) | Acc_1: (83.24%) (13957/16768)\n",
      "Epoch: 84 | Batch_idx: 140 |  Loss_1: (0.4565) | Acc_1: (83.12%) (15001/18048)\n",
      "Epoch: 84 | Batch_idx: 150 |  Loss_1: (0.4567) | Acc_1: (83.12%) (16065/19328)\n",
      "Epoch: 84 | Batch_idx: 160 |  Loss_1: (0.4570) | Acc_1: (83.10%) (17126/20608)\n",
      "Epoch: 84 | Batch_idx: 170 |  Loss_1: (0.4582) | Acc_1: (83.06%) (18181/21888)\n",
      "Epoch: 84 | Batch_idx: 180 |  Loss_1: (0.4586) | Acc_1: (83.03%) (19237/23168)\n",
      "Epoch: 84 | Batch_idx: 190 |  Loss_1: (0.4594) | Acc_1: (83.01%) (20295/24448)\n",
      "Epoch: 84 | Batch_idx: 200 |  Loss_1: (0.4600) | Acc_1: (82.98%) (21348/25728)\n",
      "Epoch: 84 | Batch_idx: 210 |  Loss_1: (0.4593) | Acc_1: (83.00%) (22417/27008)\n",
      "Epoch: 84 | Batch_idx: 220 |  Loss_1: (0.4584) | Acc_1: (83.05%) (23492/28288)\n",
      "Epoch: 84 | Batch_idx: 230 |  Loss_1: (0.4566) | Acc_1: (83.12%) (24577/29568)\n",
      "Epoch: 84 | Batch_idx: 240 |  Loss_1: (0.4569) | Acc_1: (83.12%) (25642/30848)\n",
      "Epoch: 84 | Batch_idx: 250 |  Loss_1: (0.4551) | Acc_1: (83.20%) (26732/32128)\n",
      "Epoch: 84 | Batch_idx: 260 |  Loss_1: (0.4541) | Acc_1: (83.24%) (27809/33408)\n",
      "Epoch: 84 | Batch_idx: 270 |  Loss_1: (0.4547) | Acc_1: (83.20%) (28860/34688)\n",
      "Epoch: 84 | Batch_idx: 280 |  Loss_1: (0.4540) | Acc_1: (83.22%) (29934/35968)\n",
      "Epoch: 84 | Batch_idx: 290 |  Loss_1: (0.4547) | Acc_1: (83.21%) (30993/37248)\n",
      "Epoch: 84 | Batch_idx: 300 |  Loss_1: (0.4542) | Acc_1: (83.23%) (32065/38528)\n",
      "Epoch: 84 | Batch_idx: 310 |  Loss_1: (0.4550) | Acc_1: (83.20%) (33121/39808)\n",
      "Epoch: 84 | Batch_idx: 320 |  Loss_1: (0.4544) | Acc_1: (83.20%) (34186/41088)\n",
      "Epoch: 84 | Batch_idx: 330 |  Loss_1: (0.4547) | Acc_1: (83.18%) (35241/42368)\n",
      "Epoch: 84 | Batch_idx: 340 |  Loss_1: (0.4536) | Acc_1: (83.23%) (36327/43648)\n",
      "Epoch: 84 | Batch_idx: 350 |  Loss_1: (0.4535) | Acc_1: (83.24%) (37397/44928)\n",
      "Epoch: 84 | Batch_idx: 360 |  Loss_1: (0.4531) | Acc_1: (83.25%) (38466/46208)\n",
      "Epoch: 84 | Batch_idx: 370 |  Loss_1: (0.4525) | Acc_1: (83.28%) (39549/47488)\n",
      "Epoch: 84 | Batch_idx: 380 |  Loss_1: (0.4523) | Acc_1: (83.28%) (40613/48768)\n",
      "Epoch: 84 | Batch_idx: 390 |  Loss_1: (0.4521) | Acc_1: (83.29%) (41643/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4898) | Acc: (88.87%) (8887/10000)\n",
      "Epoch: 85 | Batch_idx: 0 |  Loss_1: (0.5629) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 85 | Batch_idx: 10 |  Loss_1: (0.4889) | Acc_1: (81.39%) (1146/1408)\n",
      "Epoch: 85 | Batch_idx: 20 |  Loss_1: (0.4713) | Acc_1: (82.33%) (2213/2688)\n",
      "Epoch: 85 | Batch_idx: 30 |  Loss_1: (0.4745) | Acc_1: (82.26%) (3264/3968)\n",
      "Epoch: 85 | Batch_idx: 40 |  Loss_1: (0.4597) | Acc_1: (82.89%) (4350/5248)\n",
      "Epoch: 85 | Batch_idx: 50 |  Loss_1: (0.4637) | Acc_1: (82.83%) (5407/6528)\n",
      "Epoch: 85 | Batch_idx: 60 |  Loss_1: (0.4643) | Acc_1: (82.81%) (6466/7808)\n",
      "Epoch: 85 | Batch_idx: 70 |  Loss_1: (0.4607) | Acc_1: (82.97%) (7540/9088)\n",
      "Epoch: 85 | Batch_idx: 80 |  Loss_1: (0.4610) | Acc_1: (82.97%) (8602/10368)\n",
      "Epoch: 85 | Batch_idx: 90 |  Loss_1: (0.4592) | Acc_1: (82.99%) (9667/11648)\n",
      "Epoch: 85 | Batch_idx: 100 |  Loss_1: (0.4540) | Acc_1: (83.20%) (10756/12928)\n",
      "Epoch: 85 | Batch_idx: 110 |  Loss_1: (0.4514) | Acc_1: (83.31%) (11837/14208)\n",
      "Epoch: 85 | Batch_idx: 120 |  Loss_1: (0.4563) | Acc_1: (83.11%) (12872/15488)\n",
      "Epoch: 85 | Batch_idx: 130 |  Loss_1: (0.4602) | Acc_1: (83.00%) (13918/16768)\n",
      "Epoch: 85 | Batch_idx: 140 |  Loss_1: (0.4600) | Acc_1: (83.01%) (14981/18048)\n",
      "Epoch: 85 | Batch_idx: 150 |  Loss_1: (0.4601) | Acc_1: (83.02%) (16047/19328)\n",
      "Epoch: 85 | Batch_idx: 160 |  Loss_1: (0.4604) | Acc_1: (83.03%) (17110/20608)\n",
      "Epoch: 85 | Batch_idx: 170 |  Loss_1: (0.4620) | Acc_1: (82.96%) (18158/21888)\n",
      "Epoch: 85 | Batch_idx: 180 |  Loss_1: (0.4606) | Acc_1: (83.04%) (19239/23168)\n",
      "Epoch: 85 | Batch_idx: 190 |  Loss_1: (0.4580) | Acc_1: (83.16%) (20331/24448)\n",
      "Epoch: 85 | Batch_idx: 200 |  Loss_1: (0.4592) | Acc_1: (83.09%) (21377/25728)\n",
      "Epoch: 85 | Batch_idx: 210 |  Loss_1: (0.4584) | Acc_1: (83.12%) (22448/27008)\n",
      "Epoch: 85 | Batch_idx: 220 |  Loss_1: (0.4598) | Acc_1: (83.07%) (23499/28288)\n",
      "Epoch: 85 | Batch_idx: 230 |  Loss_1: (0.4591) | Acc_1: (83.08%) (24565/29568)\n",
      "Epoch: 85 | Batch_idx: 240 |  Loss_1: (0.4602) | Acc_1: (83.07%) (25624/30848)\n",
      "Epoch: 85 | Batch_idx: 250 |  Loss_1: (0.4599) | Acc_1: (83.08%) (26692/32128)\n",
      "Epoch: 85 | Batch_idx: 260 |  Loss_1: (0.4607) | Acc_1: (83.05%) (27745/33408)\n",
      "Epoch: 85 | Batch_idx: 270 |  Loss_1: (0.4623) | Acc_1: (83.00%) (28791/34688)\n",
      "Epoch: 85 | Batch_idx: 280 |  Loss_1: (0.4620) | Acc_1: (83.01%) (29856/35968)\n",
      "Epoch: 85 | Batch_idx: 290 |  Loss_1: (0.4648) | Acc_1: (82.87%) (30868/37248)\n",
      "Epoch: 85 | Batch_idx: 300 |  Loss_1: (0.4633) | Acc_1: (82.91%) (31945/38528)\n",
      "Epoch: 85 | Batch_idx: 310 |  Loss_1: (0.4628) | Acc_1: (82.93%) (33013/39808)\n",
      "Epoch: 85 | Batch_idx: 320 |  Loss_1: (0.4624) | Acc_1: (82.95%) (34081/41088)\n",
      "Epoch: 85 | Batch_idx: 330 |  Loss_1: (0.4601) | Acc_1: (83.03%) (35180/42368)\n",
      "Epoch: 85 | Batch_idx: 340 |  Loss_1: (0.4592) | Acc_1: (83.08%) (36261/43648)\n",
      "Epoch: 85 | Batch_idx: 350 |  Loss_1: (0.4599) | Acc_1: (83.06%) (37317/44928)\n",
      "Epoch: 85 | Batch_idx: 360 |  Loss_1: (0.4602) | Acc_1: (83.04%) (38372/46208)\n",
      "Epoch: 85 | Batch_idx: 370 |  Loss_1: (0.4605) | Acc_1: (83.04%) (39434/47488)\n",
      "Epoch: 85 | Batch_idx: 380 |  Loss_1: (0.4614) | Acc_1: (83.02%) (40485/48768)\n",
      "Epoch: 85 | Batch_idx: 390 |  Loss_1: (0.4614) | Acc_1: (83.01%) (41506/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4958) | Acc: (88.62%) (8862/10000)\n",
      "Epoch: 86 | Batch_idx: 0 |  Loss_1: (0.5862) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 86 | Batch_idx: 10 |  Loss_1: (0.4631) | Acc_1: (82.67%) (1164/1408)\n",
      "Epoch: 86 | Batch_idx: 20 |  Loss_1: (0.4677) | Acc_1: (82.92%) (2229/2688)\n",
      "Epoch: 86 | Batch_idx: 30 |  Loss_1: (0.4541) | Acc_1: (83.24%) (3303/3968)\n",
      "Epoch: 86 | Batch_idx: 40 |  Loss_1: (0.4473) | Acc_1: (83.42%) (4378/5248)\n",
      "Epoch: 86 | Batch_idx: 50 |  Loss_1: (0.4450) | Acc_1: (83.49%) (5450/6528)\n",
      "Epoch: 86 | Batch_idx: 60 |  Loss_1: (0.4461) | Acc_1: (83.47%) (6517/7808)\n",
      "Epoch: 86 | Batch_idx: 70 |  Loss_1: (0.4439) | Acc_1: (83.60%) (7598/9088)\n",
      "Epoch: 86 | Batch_idx: 80 |  Loss_1: (0.4421) | Acc_1: (83.70%) (8678/10368)\n",
      "Epoch: 86 | Batch_idx: 90 |  Loss_1: (0.4417) | Acc_1: (83.75%) (9755/11648)\n",
      "Epoch: 86 | Batch_idx: 100 |  Loss_1: (0.4427) | Acc_1: (83.74%) (10826/12928)\n",
      "Epoch: 86 | Batch_idx: 110 |  Loss_1: (0.4450) | Acc_1: (83.64%) (11883/14208)\n",
      "Epoch: 86 | Batch_idx: 120 |  Loss_1: (0.4477) | Acc_1: (83.52%) (12936/15488)\n",
      "Epoch: 86 | Batch_idx: 130 |  Loss_1: (0.4468) | Acc_1: (83.52%) (14005/16768)\n",
      "Epoch: 86 | Batch_idx: 140 |  Loss_1: (0.4470) | Acc_1: (83.52%) (15074/18048)\n",
      "Epoch: 86 | Batch_idx: 150 |  Loss_1: (0.4473) | Acc_1: (83.53%) (16144/19328)\n",
      "Epoch: 86 | Batch_idx: 160 |  Loss_1: (0.4485) | Acc_1: (83.46%) (17200/20608)\n",
      "Epoch: 86 | Batch_idx: 170 |  Loss_1: (0.4495) | Acc_1: (83.40%) (18254/21888)\n",
      "Epoch: 86 | Batch_idx: 180 |  Loss_1: (0.4469) | Acc_1: (83.50%) (19345/23168)\n",
      "Epoch: 86 | Batch_idx: 190 |  Loss_1: (0.4503) | Acc_1: (83.37%) (20383/24448)\n",
      "Epoch: 86 | Batch_idx: 200 |  Loss_1: (0.4512) | Acc_1: (83.35%) (21444/25728)\n",
      "Epoch: 86 | Batch_idx: 210 |  Loss_1: (0.4501) | Acc_1: (83.39%) (22522/27008)\n",
      "Epoch: 86 | Batch_idx: 220 |  Loss_1: (0.4498) | Acc_1: (83.42%) (23597/28288)\n",
      "Epoch: 86 | Batch_idx: 230 |  Loss_1: (0.4495) | Acc_1: (83.43%) (24669/29568)\n",
      "Epoch: 86 | Batch_idx: 240 |  Loss_1: (0.4492) | Acc_1: (83.45%) (25742/30848)\n",
      "Epoch: 86 | Batch_idx: 250 |  Loss_1: (0.4485) | Acc_1: (83.49%) (26824/32128)\n",
      "Epoch: 86 | Batch_idx: 260 |  Loss_1: (0.4478) | Acc_1: (83.52%) (27903/33408)\n",
      "Epoch: 86 | Batch_idx: 270 |  Loss_1: (0.4468) | Acc_1: (83.54%) (28979/34688)\n",
      "Epoch: 86 | Batch_idx: 280 |  Loss_1: (0.4494) | Acc_1: (83.45%) (30016/35968)\n",
      "Epoch: 86 | Batch_idx: 290 |  Loss_1: (0.4501) | Acc_1: (83.43%) (31077/37248)\n",
      "Epoch: 86 | Batch_idx: 300 |  Loss_1: (0.4520) | Acc_1: (83.38%) (32124/38528)\n",
      "Epoch: 86 | Batch_idx: 310 |  Loss_1: (0.4525) | Acc_1: (83.36%) (33183/39808)\n",
      "Epoch: 86 | Batch_idx: 320 |  Loss_1: (0.4514) | Acc_1: (83.39%) (34265/41088)\n",
      "Epoch: 86 | Batch_idx: 330 |  Loss_1: (0.4524) | Acc_1: (83.36%) (35317/42368)\n",
      "Epoch: 86 | Batch_idx: 340 |  Loss_1: (0.4521) | Acc_1: (83.37%) (36389/43648)\n",
      "Epoch: 86 | Batch_idx: 350 |  Loss_1: (0.4516) | Acc_1: (83.39%) (37466/44928)\n",
      "Epoch: 86 | Batch_idx: 360 |  Loss_1: (0.4515) | Acc_1: (83.39%) (38531/46208)\n",
      "Epoch: 86 | Batch_idx: 370 |  Loss_1: (0.4520) | Acc_1: (83.35%) (39583/47488)\n",
      "Epoch: 86 | Batch_idx: 380 |  Loss_1: (0.4529) | Acc_1: (83.32%) (40635/48768)\n",
      "Epoch: 86 | Batch_idx: 390 |  Loss_1: (0.4531) | Acc_1: (83.33%) (41663/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5286) | Acc: (88.00%) (8800/10000)\n",
      "Epoch: 87 | Batch_idx: 0 |  Loss_1: (0.4002) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 87 | Batch_idx: 10 |  Loss_1: (0.4447) | Acc_1: (83.45%) (1175/1408)\n",
      "Epoch: 87 | Batch_idx: 20 |  Loss_1: (0.4529) | Acc_1: (83.18%) (2236/2688)\n",
      "Epoch: 87 | Batch_idx: 30 |  Loss_1: (0.4499) | Acc_1: (83.32%) (3306/3968)\n",
      "Epoch: 87 | Batch_idx: 40 |  Loss_1: (0.4535) | Acc_1: (83.19%) (4366/5248)\n",
      "Epoch: 87 | Batch_idx: 50 |  Loss_1: (0.4616) | Acc_1: (82.94%) (5414/6528)\n",
      "Epoch: 87 | Batch_idx: 60 |  Loss_1: (0.4549) | Acc_1: (83.15%) (6492/7808)\n",
      "Epoch: 87 | Batch_idx: 70 |  Loss_1: (0.4621) | Acc_1: (82.83%) (7528/9088)\n",
      "Epoch: 87 | Batch_idx: 80 |  Loss_1: (0.4629) | Acc_1: (82.84%) (8589/10368)\n",
      "Epoch: 87 | Batch_idx: 90 |  Loss_1: (0.4618) | Acc_1: (82.85%) (9650/11648)\n",
      "Epoch: 87 | Batch_idx: 100 |  Loss_1: (0.4605) | Acc_1: (82.89%) (10716/12928)\n",
      "Epoch: 87 | Batch_idx: 110 |  Loss_1: (0.4585) | Acc_1: (82.95%) (11786/14208)\n",
      "Epoch: 87 | Batch_idx: 120 |  Loss_1: (0.4590) | Acc_1: (82.97%) (12850/15488)\n",
      "Epoch: 87 | Batch_idx: 130 |  Loss_1: (0.4570) | Acc_1: (83.10%) (13934/16768)\n",
      "Epoch: 87 | Batch_idx: 140 |  Loss_1: (0.4559) | Acc_1: (83.11%) (15000/18048)\n",
      "Epoch: 87 | Batch_idx: 150 |  Loss_1: (0.4544) | Acc_1: (83.15%) (16072/19328)\n",
      "Epoch: 87 | Batch_idx: 160 |  Loss_1: (0.4548) | Acc_1: (83.15%) (17136/20608)\n",
      "Epoch: 87 | Batch_idx: 170 |  Loss_1: (0.4557) | Acc_1: (83.10%) (18190/21888)\n",
      "Epoch: 87 | Batch_idx: 180 |  Loss_1: (0.4561) | Acc_1: (83.10%) (19253/23168)\n",
      "Epoch: 87 | Batch_idx: 190 |  Loss_1: (0.4549) | Acc_1: (83.15%) (20329/24448)\n",
      "Epoch: 87 | Batch_idx: 200 |  Loss_1: (0.4544) | Acc_1: (83.17%) (21397/25728)\n",
      "Epoch: 87 | Batch_idx: 210 |  Loss_1: (0.4562) | Acc_1: (83.12%) (22450/27008)\n",
      "Epoch: 87 | Batch_idx: 220 |  Loss_1: (0.4558) | Acc_1: (83.13%) (23517/28288)\n",
      "Epoch: 87 | Batch_idx: 230 |  Loss_1: (0.4548) | Acc_1: (83.17%) (24591/29568)\n",
      "Epoch: 87 | Batch_idx: 240 |  Loss_1: (0.4565) | Acc_1: (83.11%) (25639/30848)\n",
      "Epoch: 87 | Batch_idx: 250 |  Loss_1: (0.4565) | Acc_1: (83.13%) (26708/32128)\n",
      "Epoch: 87 | Batch_idx: 260 |  Loss_1: (0.4577) | Acc_1: (83.08%) (27756/33408)\n",
      "Epoch: 87 | Batch_idx: 270 |  Loss_1: (0.4581) | Acc_1: (83.05%) (28810/34688)\n",
      "Epoch: 87 | Batch_idx: 280 |  Loss_1: (0.4590) | Acc_1: (83.04%) (29867/35968)\n",
      "Epoch: 87 | Batch_idx: 290 |  Loss_1: (0.4604) | Acc_1: (82.98%) (30907/37248)\n",
      "Epoch: 87 | Batch_idx: 300 |  Loss_1: (0.4618) | Acc_1: (82.94%) (31955/38528)\n",
      "Epoch: 87 | Batch_idx: 310 |  Loss_1: (0.4610) | Acc_1: (82.97%) (33027/39808)\n",
      "Epoch: 87 | Batch_idx: 320 |  Loss_1: (0.4608) | Acc_1: (82.98%) (34095/41088)\n",
      "Epoch: 87 | Batch_idx: 330 |  Loss_1: (0.4607) | Acc_1: (82.98%) (35157/42368)\n",
      "Epoch: 87 | Batch_idx: 340 |  Loss_1: (0.4599) | Acc_1: (83.01%) (36231/43648)\n",
      "Epoch: 87 | Batch_idx: 350 |  Loss_1: (0.4593) | Acc_1: (83.05%) (37312/44928)\n",
      "Epoch: 87 | Batch_idx: 360 |  Loss_1: (0.4598) | Acc_1: (83.03%) (38365/46208)\n",
      "Epoch: 87 | Batch_idx: 370 |  Loss_1: (0.4600) | Acc_1: (83.01%) (39420/47488)\n",
      "Epoch: 87 | Batch_idx: 380 |  Loss_1: (0.4610) | Acc_1: (82.96%) (40459/48768)\n",
      "Epoch: 87 | Batch_idx: 390 |  Loss_1: (0.4608) | Acc_1: (82.97%) (41484/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4797) | Acc: (89.33%) (8933/10000)\n",
      "Epoch: 88 | Batch_idx: 0 |  Loss_1: (0.4938) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 88 | Batch_idx: 10 |  Loss_1: (0.4582) | Acc_1: (82.88%) (1167/1408)\n",
      "Epoch: 88 | Batch_idx: 20 |  Loss_1: (0.4639) | Acc_1: (82.89%) (2228/2688)\n",
      "Epoch: 88 | Batch_idx: 30 |  Loss_1: (0.4622) | Acc_1: (82.76%) (3284/3968)\n",
      "Epoch: 88 | Batch_idx: 40 |  Loss_1: (0.4644) | Acc_1: (82.76%) (4343/5248)\n",
      "Epoch: 88 | Batch_idx: 50 |  Loss_1: (0.4625) | Acc_1: (82.92%) (5413/6528)\n",
      "Epoch: 88 | Batch_idx: 60 |  Loss_1: (0.4581) | Acc_1: (83.16%) (6493/7808)\n",
      "Epoch: 88 | Batch_idx: 70 |  Loss_1: (0.4617) | Acc_1: (83.04%) (7547/9088)\n",
      "Epoch: 88 | Batch_idx: 80 |  Loss_1: (0.4607) | Acc_1: (83.09%) (8615/10368)\n",
      "Epoch: 88 | Batch_idx: 90 |  Loss_1: (0.4643) | Acc_1: (82.94%) (9661/11648)\n",
      "Epoch: 88 | Batch_idx: 100 |  Loss_1: (0.4591) | Acc_1: (83.14%) (10748/12928)\n",
      "Epoch: 88 | Batch_idx: 110 |  Loss_1: (0.4607) | Acc_1: (83.04%) (11798/14208)\n",
      "Epoch: 88 | Batch_idx: 120 |  Loss_1: (0.4605) | Acc_1: (83.06%) (12865/15488)\n",
      "Epoch: 88 | Batch_idx: 130 |  Loss_1: (0.4618) | Acc_1: (83.03%) (13923/16768)\n",
      "Epoch: 88 | Batch_idx: 140 |  Loss_1: (0.4594) | Acc_1: (83.14%) (15005/18048)\n",
      "Epoch: 88 | Batch_idx: 150 |  Loss_1: (0.4609) | Acc_1: (83.09%) (16060/19328)\n",
      "Epoch: 88 | Batch_idx: 160 |  Loss_1: (0.4612) | Acc_1: (83.04%) (17112/20608)\n",
      "Epoch: 88 | Batch_idx: 170 |  Loss_1: (0.4626) | Acc_1: (83.00%) (18167/21888)\n",
      "Epoch: 88 | Batch_idx: 180 |  Loss_1: (0.4616) | Acc_1: (83.01%) (19232/23168)\n",
      "Epoch: 88 | Batch_idx: 190 |  Loss_1: (0.4624) | Acc_1: (82.97%) (20284/24448)\n",
      "Epoch: 88 | Batch_idx: 200 |  Loss_1: (0.4614) | Acc_1: (82.98%) (21348/25728)\n",
      "Epoch: 88 | Batch_idx: 210 |  Loss_1: (0.4596) | Acc_1: (83.03%) (22426/27008)\n",
      "Epoch: 88 | Batch_idx: 220 |  Loss_1: (0.4602) | Acc_1: (83.04%) (23489/28288)\n",
      "Epoch: 88 | Batch_idx: 230 |  Loss_1: (0.4604) | Acc_1: (83.00%) (24542/29568)\n",
      "Epoch: 88 | Batch_idx: 240 |  Loss_1: (0.4611) | Acc_1: (82.98%) (25598/30848)\n",
      "Epoch: 88 | Batch_idx: 250 |  Loss_1: (0.4623) | Acc_1: (82.92%) (26641/32128)\n",
      "Epoch: 88 | Batch_idx: 260 |  Loss_1: (0.4654) | Acc_1: (82.80%) (27661/33408)\n",
      "Epoch: 88 | Batch_idx: 270 |  Loss_1: (0.4669) | Acc_1: (82.73%) (28697/34688)\n",
      "Epoch: 88 | Batch_idx: 280 |  Loss_1: (0.4680) | Acc_1: (82.68%) (29738/35968)\n",
      "Epoch: 88 | Batch_idx: 290 |  Loss_1: (0.4705) | Acc_1: (82.58%) (30760/37248)\n",
      "Epoch: 88 | Batch_idx: 300 |  Loss_1: (0.4711) | Acc_1: (82.56%) (31810/38528)\n",
      "Epoch: 88 | Batch_idx: 310 |  Loss_1: (0.4713) | Acc_1: (82.54%) (32859/39808)\n",
      "Epoch: 88 | Batch_idx: 320 |  Loss_1: (0.4711) | Acc_1: (82.54%) (33916/41088)\n",
      "Epoch: 88 | Batch_idx: 330 |  Loss_1: (0.4700) | Acc_1: (82.58%) (34989/42368)\n",
      "Epoch: 88 | Batch_idx: 340 |  Loss_1: (0.4684) | Acc_1: (82.64%) (36070/43648)\n",
      "Epoch: 88 | Batch_idx: 350 |  Loss_1: (0.4670) | Acc_1: (82.69%) (37151/44928)\n",
      "Epoch: 88 | Batch_idx: 360 |  Loss_1: (0.4675) | Acc_1: (82.68%) (38204/46208)\n",
      "Epoch: 88 | Batch_idx: 370 |  Loss_1: (0.4658) | Acc_1: (82.73%) (39287/47488)\n",
      "Epoch: 88 | Batch_idx: 380 |  Loss_1: (0.4648) | Acc_1: (82.77%) (40366/48768)\n",
      "Epoch: 88 | Batch_idx: 390 |  Loss_1: (0.4654) | Acc_1: (82.76%) (41379/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4363) | Acc: (89.50%) (8950/10000)\n",
      "Epoch: 89 | Batch_idx: 0 |  Loss_1: (0.4426) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 89 | Batch_idx: 10 |  Loss_1: (0.4724) | Acc_1: (82.24%) (1158/1408)\n",
      "Epoch: 89 | Batch_idx: 20 |  Loss_1: (0.4903) | Acc_1: (81.40%) (2188/2688)\n",
      "Epoch: 89 | Batch_idx: 30 |  Loss_1: (0.4782) | Acc_1: (82.13%) (3259/3968)\n",
      "Epoch: 89 | Batch_idx: 40 |  Loss_1: (0.4631) | Acc_1: (82.74%) (4342/5248)\n",
      "Epoch: 89 | Batch_idx: 50 |  Loss_1: (0.4647) | Acc_1: (82.69%) (5398/6528)\n",
      "Epoch: 89 | Batch_idx: 60 |  Loss_1: (0.4605) | Acc_1: (82.90%) (6473/7808)\n",
      "Epoch: 89 | Batch_idx: 70 |  Loss_1: (0.4623) | Acc_1: (82.91%) (7535/9088)\n",
      "Epoch: 89 | Batch_idx: 80 |  Loss_1: (0.4544) | Acc_1: (83.17%) (8623/10368)\n",
      "Epoch: 89 | Batch_idx: 90 |  Loss_1: (0.4557) | Acc_1: (83.11%) (9681/11648)\n",
      "Epoch: 89 | Batch_idx: 100 |  Loss_1: (0.4562) | Acc_1: (83.11%) (10745/12928)\n",
      "Epoch: 89 | Batch_idx: 110 |  Loss_1: (0.4543) | Acc_1: (83.16%) (11815/14208)\n",
      "Epoch: 89 | Batch_idx: 120 |  Loss_1: (0.4570) | Acc_1: (83.10%) (12870/15488)\n",
      "Epoch: 89 | Batch_idx: 130 |  Loss_1: (0.4590) | Acc_1: (83.00%) (13917/16768)\n",
      "Epoch: 89 | Batch_idx: 140 |  Loss_1: (0.4593) | Acc_1: (83.01%) (14981/18048)\n",
      "Epoch: 89 | Batch_idx: 150 |  Loss_1: (0.4652) | Acc_1: (82.76%) (15996/19328)\n",
      "Epoch: 89 | Batch_idx: 160 |  Loss_1: (0.4653) | Acc_1: (82.74%) (17051/20608)\n",
      "Epoch: 89 | Batch_idx: 170 |  Loss_1: (0.4650) | Acc_1: (82.73%) (18108/21888)\n",
      "Epoch: 89 | Batch_idx: 180 |  Loss_1: (0.4632) | Acc_1: (82.78%) (19178/23168)\n",
      "Epoch: 89 | Batch_idx: 190 |  Loss_1: (0.4643) | Acc_1: (82.69%) (20217/24448)\n",
      "Epoch: 89 | Batch_idx: 200 |  Loss_1: (0.4635) | Acc_1: (82.70%) (21278/25728)\n",
      "Epoch: 89 | Batch_idx: 210 |  Loss_1: (0.4625) | Acc_1: (82.75%) (22348/27008)\n",
      "Epoch: 89 | Batch_idx: 220 |  Loss_1: (0.4618) | Acc_1: (82.78%) (23417/28288)\n",
      "Epoch: 89 | Batch_idx: 230 |  Loss_1: (0.4623) | Acc_1: (82.75%) (24468/29568)\n",
      "Epoch: 89 | Batch_idx: 240 |  Loss_1: (0.4622) | Acc_1: (82.74%) (25524/30848)\n",
      "Epoch: 89 | Batch_idx: 250 |  Loss_1: (0.4626) | Acc_1: (82.74%) (26582/32128)\n",
      "Epoch: 89 | Batch_idx: 260 |  Loss_1: (0.4624) | Acc_1: (82.77%) (27653/33408)\n",
      "Epoch: 89 | Batch_idx: 270 |  Loss_1: (0.4603) | Acc_1: (82.86%) (28742/34688)\n",
      "Epoch: 89 | Batch_idx: 280 |  Loss_1: (0.4617) | Acc_1: (82.80%) (29783/35968)\n",
      "Epoch: 89 | Batch_idx: 290 |  Loss_1: (0.4637) | Acc_1: (82.72%) (30812/37248)\n",
      "Epoch: 89 | Batch_idx: 300 |  Loss_1: (0.4633) | Acc_1: (82.73%) (31876/38528)\n",
      "Epoch: 89 | Batch_idx: 310 |  Loss_1: (0.4626) | Acc_1: (82.77%) (32949/39808)\n",
      "Epoch: 89 | Batch_idx: 320 |  Loss_1: (0.4623) | Acc_1: (82.79%) (34017/41088)\n",
      "Epoch: 89 | Batch_idx: 330 |  Loss_1: (0.4621) | Acc_1: (82.81%) (35087/42368)\n",
      "Epoch: 89 | Batch_idx: 340 |  Loss_1: (0.4620) | Acc_1: (82.82%) (36150/43648)\n",
      "Epoch: 89 | Batch_idx: 350 |  Loss_1: (0.4620) | Acc_1: (82.83%) (37215/44928)\n",
      "Epoch: 89 | Batch_idx: 360 |  Loss_1: (0.4621) | Acc_1: (82.82%) (38271/46208)\n",
      "Epoch: 89 | Batch_idx: 370 |  Loss_1: (0.4629) | Acc_1: (82.81%) (39324/47488)\n",
      "Epoch: 89 | Batch_idx: 380 |  Loss_1: (0.4641) | Acc_1: (82.76%) (40359/48768)\n",
      "Epoch: 89 | Batch_idx: 390 |  Loss_1: (0.4638) | Acc_1: (82.77%) (41383/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4693) | Acc: (88.86%) (8886/10000)\n",
      "Epoch: 90 | Batch_idx: 0 |  Loss_1: (0.4558) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 90 | Batch_idx: 10 |  Loss_1: (0.4568) | Acc_1: (82.88%) (1167/1408)\n",
      "Epoch: 90 | Batch_idx: 20 |  Loss_1: (0.4626) | Acc_1: (82.92%) (2229/2688)\n",
      "Epoch: 90 | Batch_idx: 30 |  Loss_1: (0.4561) | Acc_1: (83.29%) (3305/3968)\n",
      "Epoch: 90 | Batch_idx: 40 |  Loss_1: (0.4461) | Acc_1: (83.65%) (4390/5248)\n",
      "Epoch: 90 | Batch_idx: 50 |  Loss_1: (0.4428) | Acc_1: (83.67%) (5462/6528)\n",
      "Epoch: 90 | Batch_idx: 60 |  Loss_1: (0.4465) | Acc_1: (83.48%) (6518/7808)\n",
      "Epoch: 90 | Batch_idx: 70 |  Loss_1: (0.4479) | Acc_1: (83.42%) (7581/9088)\n",
      "Epoch: 90 | Batch_idx: 80 |  Loss_1: (0.4550) | Acc_1: (83.13%) (8619/10368)\n",
      "Epoch: 90 | Batch_idx: 90 |  Loss_1: (0.4533) | Acc_1: (83.20%) (9691/11648)\n",
      "Epoch: 90 | Batch_idx: 100 |  Loss_1: (0.4495) | Acc_1: (83.38%) (10779/12928)\n",
      "Epoch: 90 | Batch_idx: 110 |  Loss_1: (0.4471) | Acc_1: (83.47%) (11859/14208)\n",
      "Epoch: 90 | Batch_idx: 120 |  Loss_1: (0.4473) | Acc_1: (83.44%) (12923/15488)\n",
      "Epoch: 90 | Batch_idx: 130 |  Loss_1: (0.4476) | Acc_1: (83.39%) (13983/16768)\n",
      "Epoch: 90 | Batch_idx: 140 |  Loss_1: (0.4506) | Acc_1: (83.31%) (15035/18048)\n",
      "Epoch: 90 | Batch_idx: 150 |  Loss_1: (0.4504) | Acc_1: (83.35%) (16109/19328)\n",
      "Epoch: 90 | Batch_idx: 160 |  Loss_1: (0.4490) | Acc_1: (83.38%) (17182/20608)\n",
      "Epoch: 90 | Batch_idx: 170 |  Loss_1: (0.4499) | Acc_1: (83.33%) (18240/21888)\n",
      "Epoch: 90 | Batch_idx: 180 |  Loss_1: (0.4529) | Acc_1: (83.23%) (19282/23168)\n",
      "Epoch: 90 | Batch_idx: 190 |  Loss_1: (0.4520) | Acc_1: (83.27%) (20357/24448)\n",
      "Epoch: 90 | Batch_idx: 200 |  Loss_1: (0.4528) | Acc_1: (83.25%) (21418/25728)\n",
      "Epoch: 90 | Batch_idx: 210 |  Loss_1: (0.4517) | Acc_1: (83.30%) (22499/27008)\n",
      "Epoch: 90 | Batch_idx: 220 |  Loss_1: (0.4511) | Acc_1: (83.31%) (23566/28288)\n",
      "Epoch: 90 | Batch_idx: 230 |  Loss_1: (0.4518) | Acc_1: (83.26%) (24619/29568)\n",
      "Epoch: 90 | Batch_idx: 240 |  Loss_1: (0.4521) | Acc_1: (83.26%) (25683/30848)\n",
      "Epoch: 90 | Batch_idx: 250 |  Loss_1: (0.4516) | Acc_1: (83.28%) (26757/32128)\n",
      "Epoch: 90 | Batch_idx: 260 |  Loss_1: (0.4527) | Acc_1: (83.23%) (27805/33408)\n",
      "Epoch: 90 | Batch_idx: 270 |  Loss_1: (0.4525) | Acc_1: (83.24%) (28873/34688)\n",
      "Epoch: 90 | Batch_idx: 280 |  Loss_1: (0.4522) | Acc_1: (83.25%) (29945/35968)\n",
      "Epoch: 90 | Batch_idx: 290 |  Loss_1: (0.4519) | Acc_1: (83.26%) (31013/37248)\n",
      "Epoch: 90 | Batch_idx: 300 |  Loss_1: (0.4519) | Acc_1: (83.27%) (32082/38528)\n",
      "Epoch: 90 | Batch_idx: 310 |  Loss_1: (0.4531) | Acc_1: (83.23%) (33132/39808)\n",
      "Epoch: 90 | Batch_idx: 320 |  Loss_1: (0.4539) | Acc_1: (83.18%) (34177/41088)\n",
      "Epoch: 90 | Batch_idx: 330 |  Loss_1: (0.4541) | Acc_1: (83.17%) (35238/42368)\n",
      "Epoch: 90 | Batch_idx: 340 |  Loss_1: (0.4548) | Acc_1: (83.15%) (36292/43648)\n",
      "Epoch: 90 | Batch_idx: 350 |  Loss_1: (0.4549) | Acc_1: (83.15%) (37356/44928)\n",
      "Epoch: 90 | Batch_idx: 360 |  Loss_1: (0.4554) | Acc_1: (83.13%) (38413/46208)\n",
      "Epoch: 90 | Batch_idx: 370 |  Loss_1: (0.4557) | Acc_1: (83.12%) (39473/47488)\n",
      "Epoch: 90 | Batch_idx: 380 |  Loss_1: (0.4550) | Acc_1: (83.15%) (40553/48768)\n",
      "Epoch: 90 | Batch_idx: 390 |  Loss_1: (0.4560) | Acc_1: (83.12%) (41562/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4546) | Acc: (88.95%) (8895/10000)\n",
      "Epoch: 91 | Batch_idx: 0 |  Loss_1: (0.4893) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 91 | Batch_idx: 10 |  Loss_1: (0.4124) | Acc_1: (85.01%) (1197/1408)\n",
      "Epoch: 91 | Batch_idx: 20 |  Loss_1: (0.4285) | Acc_1: (84.15%) (2262/2688)\n",
      "Epoch: 91 | Batch_idx: 30 |  Loss_1: (0.4279) | Acc_1: (84.10%) (3337/3968)\n",
      "Epoch: 91 | Batch_idx: 40 |  Loss_1: (0.4219) | Acc_1: (84.32%) (4425/5248)\n",
      "Epoch: 91 | Batch_idx: 50 |  Loss_1: (0.4287) | Acc_1: (84.11%) (5491/6528)\n",
      "Epoch: 91 | Batch_idx: 60 |  Loss_1: (0.4382) | Acc_1: (83.77%) (6541/7808)\n",
      "Epoch: 91 | Batch_idx: 70 |  Loss_1: (0.4363) | Acc_1: (83.87%) (7622/9088)\n",
      "Epoch: 91 | Batch_idx: 80 |  Loss_1: (0.4362) | Acc_1: (83.89%) (8698/10368)\n",
      "Epoch: 91 | Batch_idx: 90 |  Loss_1: (0.4377) | Acc_1: (83.83%) (9764/11648)\n",
      "Epoch: 91 | Batch_idx: 100 |  Loss_1: (0.4371) | Acc_1: (83.83%) (10837/12928)\n",
      "Epoch: 91 | Batch_idx: 110 |  Loss_1: (0.4422) | Acc_1: (83.65%) (11885/14208)\n",
      "Epoch: 91 | Batch_idx: 120 |  Loss_1: (0.4397) | Acc_1: (83.75%) (12971/15488)\n",
      "Epoch: 91 | Batch_idx: 130 |  Loss_1: (0.4434) | Acc_1: (83.59%) (14017/16768)\n",
      "Epoch: 91 | Batch_idx: 140 |  Loss_1: (0.4453) | Acc_1: (83.51%) (15071/18048)\n",
      "Epoch: 91 | Batch_idx: 150 |  Loss_1: (0.4439) | Acc_1: (83.55%) (16148/19328)\n",
      "Epoch: 91 | Batch_idx: 160 |  Loss_1: (0.4436) | Acc_1: (83.58%) (17225/20608)\n",
      "Epoch: 91 | Batch_idx: 170 |  Loss_1: (0.4445) | Acc_1: (83.54%) (18285/21888)\n",
      "Epoch: 91 | Batch_idx: 180 |  Loss_1: (0.4440) | Acc_1: (83.59%) (19365/23168)\n",
      "Epoch: 91 | Batch_idx: 190 |  Loss_1: (0.4474) | Acc_1: (83.43%) (20398/24448)\n",
      "Epoch: 91 | Batch_idx: 200 |  Loss_1: (0.4467) | Acc_1: (83.45%) (21471/25728)\n",
      "Epoch: 91 | Batch_idx: 210 |  Loss_1: (0.4476) | Acc_1: (83.43%) (22532/27008)\n",
      "Epoch: 91 | Batch_idx: 220 |  Loss_1: (0.4475) | Acc_1: (83.45%) (23606/28288)\n",
      "Epoch: 91 | Batch_idx: 230 |  Loss_1: (0.4502) | Acc_1: (83.35%) (24644/29568)\n",
      "Epoch: 91 | Batch_idx: 240 |  Loss_1: (0.4504) | Acc_1: (83.33%) (25706/30848)\n",
      "Epoch: 91 | Batch_idx: 250 |  Loss_1: (0.4522) | Acc_1: (83.27%) (26752/32128)\n",
      "Epoch: 91 | Batch_idx: 260 |  Loss_1: (0.4539) | Acc_1: (83.20%) (27794/33408)\n",
      "Epoch: 91 | Batch_idx: 270 |  Loss_1: (0.4539) | Acc_1: (83.18%) (28854/34688)\n",
      "Epoch: 91 | Batch_idx: 280 |  Loss_1: (0.4530) | Acc_1: (83.22%) (29934/35968)\n",
      "Epoch: 91 | Batch_idx: 290 |  Loss_1: (0.4545) | Acc_1: (83.14%) (30969/37248)\n",
      "Epoch: 91 | Batch_idx: 300 |  Loss_1: (0.4542) | Acc_1: (83.14%) (32032/38528)\n",
      "Epoch: 91 | Batch_idx: 310 |  Loss_1: (0.4541) | Acc_1: (83.14%) (33095/39808)\n",
      "Epoch: 91 | Batch_idx: 320 |  Loss_1: (0.4545) | Acc_1: (83.11%) (34150/41088)\n",
      "Epoch: 91 | Batch_idx: 330 |  Loss_1: (0.4551) | Acc_1: (83.11%) (35210/42368)\n",
      "Epoch: 91 | Batch_idx: 340 |  Loss_1: (0.4548) | Acc_1: (83.12%) (36279/43648)\n",
      "Epoch: 91 | Batch_idx: 350 |  Loss_1: (0.4560) | Acc_1: (83.08%) (37327/44928)\n",
      "Epoch: 91 | Batch_idx: 360 |  Loss_1: (0.4569) | Acc_1: (83.03%) (38365/46208)\n",
      "Epoch: 91 | Batch_idx: 370 |  Loss_1: (0.4571) | Acc_1: (83.02%) (39424/47488)\n",
      "Epoch: 91 | Batch_idx: 380 |  Loss_1: (0.4570) | Acc_1: (83.02%) (40486/48768)\n",
      "Epoch: 91 | Batch_idx: 390 |  Loss_1: (0.4561) | Acc_1: (83.03%) (41517/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4492) | Acc: (89.10%) (8910/10000)\n",
      "Epoch: 92 | Batch_idx: 0 |  Loss_1: (0.4014) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 92 | Batch_idx: 10 |  Loss_1: (0.4609) | Acc_1: (82.95%) (1168/1408)\n",
      "Epoch: 92 | Batch_idx: 20 |  Loss_1: (0.4721) | Acc_1: (82.63%) (2221/2688)\n",
      "Epoch: 92 | Batch_idx: 30 |  Loss_1: (0.4703) | Acc_1: (82.51%) (3274/3968)\n",
      "Epoch: 92 | Batch_idx: 40 |  Loss_1: (0.4717) | Acc_1: (82.56%) (4333/5248)\n",
      "Epoch: 92 | Batch_idx: 50 |  Loss_1: (0.4666) | Acc_1: (82.74%) (5401/6528)\n",
      "Epoch: 92 | Batch_idx: 60 |  Loss_1: (0.4673) | Acc_1: (82.77%) (6463/7808)\n",
      "Epoch: 92 | Batch_idx: 70 |  Loss_1: (0.4648) | Acc_1: (82.89%) (7533/9088)\n",
      "Epoch: 92 | Batch_idx: 80 |  Loss_1: (0.4650) | Acc_1: (82.89%) (8594/10368)\n",
      "Epoch: 92 | Batch_idx: 90 |  Loss_1: (0.4652) | Acc_1: (82.87%) (9653/11648)\n",
      "Epoch: 92 | Batch_idx: 100 |  Loss_1: (0.4660) | Acc_1: (82.79%) (10703/12928)\n",
      "Epoch: 92 | Batch_idx: 110 |  Loss_1: (0.4647) | Acc_1: (82.87%) (11774/14208)\n",
      "Epoch: 92 | Batch_idx: 120 |  Loss_1: (0.4643) | Acc_1: (82.83%) (12828/15488)\n",
      "Epoch: 92 | Batch_idx: 130 |  Loss_1: (0.4660) | Acc_1: (82.78%) (13880/16768)\n",
      "Epoch: 92 | Batch_idx: 140 |  Loss_1: (0.4643) | Acc_1: (82.85%) (14952/18048)\n",
      "Epoch: 92 | Batch_idx: 150 |  Loss_1: (0.4648) | Acc_1: (82.83%) (16009/19328)\n",
      "Epoch: 92 | Batch_idx: 160 |  Loss_1: (0.4629) | Acc_1: (82.88%) (17080/20608)\n",
      "Epoch: 92 | Batch_idx: 170 |  Loss_1: (0.4634) | Acc_1: (82.85%) (18134/21888)\n",
      "Epoch: 92 | Batch_idx: 180 |  Loss_1: (0.4650) | Acc_1: (82.77%) (19177/23168)\n",
      "Epoch: 92 | Batch_idx: 190 |  Loss_1: (0.4649) | Acc_1: (82.76%) (20234/24448)\n",
      "Epoch: 92 | Batch_idx: 200 |  Loss_1: (0.4645) | Acc_1: (82.77%) (21295/25728)\n",
      "Epoch: 92 | Batch_idx: 210 |  Loss_1: (0.4638) | Acc_1: (82.82%) (22368/27008)\n",
      "Epoch: 92 | Batch_idx: 220 |  Loss_1: (0.4651) | Acc_1: (82.76%) (23410/28288)\n",
      "Epoch: 92 | Batch_idx: 230 |  Loss_1: (0.4636) | Acc_1: (82.82%) (24487/29568)\n",
      "Epoch: 92 | Batch_idx: 240 |  Loss_1: (0.4638) | Acc_1: (82.80%) (25541/30848)\n",
      "Epoch: 92 | Batch_idx: 250 |  Loss_1: (0.4643) | Acc_1: (82.79%) (26600/32128)\n",
      "Epoch: 92 | Batch_idx: 260 |  Loss_1: (0.4668) | Acc_1: (82.69%) (27626/33408)\n",
      "Epoch: 92 | Batch_idx: 270 |  Loss_1: (0.4655) | Acc_1: (82.75%) (28705/34688)\n",
      "Epoch: 92 | Batch_idx: 280 |  Loss_1: (0.4653) | Acc_1: (82.76%) (29766/35968)\n",
      "Epoch: 92 | Batch_idx: 290 |  Loss_1: (0.4652) | Acc_1: (82.76%) (30828/37248)\n",
      "Epoch: 92 | Batch_idx: 300 |  Loss_1: (0.4644) | Acc_1: (82.80%) (31901/38528)\n",
      "Epoch: 92 | Batch_idx: 310 |  Loss_1: (0.4641) | Acc_1: (82.80%) (32963/39808)\n",
      "Epoch: 92 | Batch_idx: 320 |  Loss_1: (0.4635) | Acc_1: (82.83%) (34035/41088)\n",
      "Epoch: 92 | Batch_idx: 330 |  Loss_1: (0.4628) | Acc_1: (82.88%) (35113/42368)\n",
      "Epoch: 92 | Batch_idx: 340 |  Loss_1: (0.4625) | Acc_1: (82.88%) (36176/43648)\n",
      "Epoch: 92 | Batch_idx: 350 |  Loss_1: (0.4625) | Acc_1: (82.88%) (37238/44928)\n",
      "Epoch: 92 | Batch_idx: 360 |  Loss_1: (0.4626) | Acc_1: (82.88%) (38297/46208)\n",
      "Epoch: 92 | Batch_idx: 370 |  Loss_1: (0.4619) | Acc_1: (82.92%) (39375/47488)\n",
      "Epoch: 92 | Batch_idx: 380 |  Loss_1: (0.4614) | Acc_1: (82.93%) (40444/48768)\n",
      "Epoch: 92 | Batch_idx: 390 |  Loss_1: (0.4610) | Acc_1: (82.95%) (41476/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5095) | Acc: (89.33%) (8933/10000)\n",
      "Epoch: 93 | Batch_idx: 0 |  Loss_1: (0.3968) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 93 | Batch_idx: 10 |  Loss_1: (0.4535) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 93 | Batch_idx: 20 |  Loss_1: (0.4476) | Acc_1: (83.33%) (2240/2688)\n",
      "Epoch: 93 | Batch_idx: 30 |  Loss_1: (0.4352) | Acc_1: (83.80%) (3325/3968)\n",
      "Epoch: 93 | Batch_idx: 40 |  Loss_1: (0.4536) | Acc_1: (83.25%) (4369/5248)\n",
      "Epoch: 93 | Batch_idx: 50 |  Loss_1: (0.4569) | Acc_1: (83.16%) (5429/6528)\n",
      "Epoch: 93 | Batch_idx: 60 |  Loss_1: (0.4559) | Acc_1: (83.15%) (6492/7808)\n",
      "Epoch: 93 | Batch_idx: 70 |  Loss_1: (0.4596) | Acc_1: (82.97%) (7540/9088)\n",
      "Epoch: 93 | Batch_idx: 80 |  Loss_1: (0.4516) | Acc_1: (83.32%) (8639/10368)\n",
      "Epoch: 93 | Batch_idx: 90 |  Loss_1: (0.4554) | Acc_1: (83.21%) (9692/11648)\n",
      "Epoch: 93 | Batch_idx: 100 |  Loss_1: (0.4537) | Acc_1: (83.21%) (10758/12928)\n",
      "Epoch: 93 | Batch_idx: 110 |  Loss_1: (0.4528) | Acc_1: (83.31%) (11836/14208)\n",
      "Epoch: 93 | Batch_idx: 120 |  Loss_1: (0.4519) | Acc_1: (83.33%) (12906/15488)\n",
      "Epoch: 93 | Batch_idx: 130 |  Loss_1: (0.4546) | Acc_1: (83.21%) (13953/16768)\n",
      "Epoch: 93 | Batch_idx: 140 |  Loss_1: (0.4542) | Acc_1: (83.16%) (15009/18048)\n",
      "Epoch: 93 | Batch_idx: 150 |  Loss_1: (0.4560) | Acc_1: (83.08%) (16057/19328)\n",
      "Epoch: 93 | Batch_idx: 160 |  Loss_1: (0.4587) | Acc_1: (82.97%) (17099/20608)\n",
      "Epoch: 93 | Batch_idx: 170 |  Loss_1: (0.4578) | Acc_1: (83.00%) (18167/21888)\n",
      "Epoch: 93 | Batch_idx: 180 |  Loss_1: (0.4608) | Acc_1: (82.88%) (19202/23168)\n",
      "Epoch: 93 | Batch_idx: 190 |  Loss_1: (0.4600) | Acc_1: (82.93%) (20274/24448)\n",
      "Epoch: 93 | Batch_idx: 200 |  Loss_1: (0.4604) | Acc_1: (82.93%) (21337/25728)\n",
      "Epoch: 93 | Batch_idx: 210 |  Loss_1: (0.4593) | Acc_1: (82.95%) (22403/27008)\n",
      "Epoch: 93 | Batch_idx: 220 |  Loss_1: (0.4576) | Acc_1: (83.01%) (23481/28288)\n",
      "Epoch: 93 | Batch_idx: 230 |  Loss_1: (0.4579) | Acc_1: (82.99%) (24539/29568)\n",
      "Epoch: 93 | Batch_idx: 240 |  Loss_1: (0.4579) | Acc_1: (82.99%) (25601/30848)\n",
      "Epoch: 93 | Batch_idx: 250 |  Loss_1: (0.4564) | Acc_1: (83.05%) (26683/32128)\n",
      "Epoch: 93 | Batch_idx: 260 |  Loss_1: (0.4562) | Acc_1: (83.05%) (27744/33408)\n",
      "Epoch: 93 | Batch_idx: 270 |  Loss_1: (0.4567) | Acc_1: (83.05%) (28810/34688)\n",
      "Epoch: 93 | Batch_idx: 280 |  Loss_1: (0.4573) | Acc_1: (83.03%) (29864/35968)\n",
      "Epoch: 93 | Batch_idx: 290 |  Loss_1: (0.4561) | Acc_1: (83.09%) (30950/37248)\n",
      "Epoch: 93 | Batch_idx: 300 |  Loss_1: (0.4547) | Acc_1: (83.16%) (32039/38528)\n",
      "Epoch: 93 | Batch_idx: 310 |  Loss_1: (0.4538) | Acc_1: (83.18%) (33114/39808)\n",
      "Epoch: 93 | Batch_idx: 320 |  Loss_1: (0.4536) | Acc_1: (83.19%) (34181/41088)\n",
      "Epoch: 93 | Batch_idx: 330 |  Loss_1: (0.4534) | Acc_1: (83.21%) (35253/42368)\n",
      "Epoch: 93 | Batch_idx: 340 |  Loss_1: (0.4521) | Acc_1: (83.24%) (36334/43648)\n",
      "Epoch: 93 | Batch_idx: 350 |  Loss_1: (0.4517) | Acc_1: (83.25%) (37402/44928)\n",
      "Epoch: 93 | Batch_idx: 360 |  Loss_1: (0.4514) | Acc_1: (83.26%) (38475/46208)\n",
      "Epoch: 93 | Batch_idx: 370 |  Loss_1: (0.4497) | Acc_1: (83.32%) (39567/47488)\n",
      "Epoch: 93 | Batch_idx: 380 |  Loss_1: (0.4504) | Acc_1: (83.30%) (40623/48768)\n",
      "Epoch: 93 | Batch_idx: 390 |  Loss_1: (0.4500) | Acc_1: (83.31%) (41654/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4994) | Acc: (88.75%) (8875/10000)\n",
      "Epoch: 94 | Batch_idx: 0 |  Loss_1: (0.4529) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 94 | Batch_idx: 10 |  Loss_1: (0.4586) | Acc_1: (83.38%) (1174/1408)\n",
      "Epoch: 94 | Batch_idx: 20 |  Loss_1: (0.4595) | Acc_1: (82.96%) (2230/2688)\n",
      "Epoch: 94 | Batch_idx: 30 |  Loss_1: (0.4632) | Acc_1: (82.69%) (3281/3968)\n",
      "Epoch: 94 | Batch_idx: 40 |  Loss_1: (0.4525) | Acc_1: (83.04%) (4358/5248)\n",
      "Epoch: 94 | Batch_idx: 50 |  Loss_1: (0.4638) | Acc_1: (82.66%) (5396/6528)\n",
      "Epoch: 94 | Batch_idx: 60 |  Loss_1: (0.4599) | Acc_1: (82.81%) (6466/7808)\n",
      "Epoch: 94 | Batch_idx: 70 |  Loss_1: (0.4588) | Acc_1: (82.82%) (7527/9088)\n",
      "Epoch: 94 | Batch_idx: 80 |  Loss_1: (0.4572) | Acc_1: (82.89%) (8594/10368)\n",
      "Epoch: 94 | Batch_idx: 90 |  Loss_1: (0.4590) | Acc_1: (82.84%) (9649/11648)\n",
      "Epoch: 94 | Batch_idx: 100 |  Loss_1: (0.4613) | Acc_1: (82.75%) (10698/12928)\n",
      "Epoch: 94 | Batch_idx: 110 |  Loss_1: (0.4633) | Acc_1: (82.66%) (11745/14208)\n",
      "Epoch: 94 | Batch_idx: 120 |  Loss_1: (0.4631) | Acc_1: (82.72%) (12811/15488)\n",
      "Epoch: 94 | Batch_idx: 130 |  Loss_1: (0.4641) | Acc_1: (82.68%) (13864/16768)\n",
      "Epoch: 94 | Batch_idx: 140 |  Loss_1: (0.4660) | Acc_1: (82.64%) (14915/18048)\n",
      "Epoch: 94 | Batch_idx: 150 |  Loss_1: (0.4632) | Acc_1: (82.75%) (15994/19328)\n",
      "Epoch: 94 | Batch_idx: 160 |  Loss_1: (0.4601) | Acc_1: (82.86%) (17075/20608)\n",
      "Epoch: 94 | Batch_idx: 170 |  Loss_1: (0.4608) | Acc_1: (82.84%) (18131/21888)\n",
      "Epoch: 94 | Batch_idx: 180 |  Loss_1: (0.4591) | Acc_1: (82.92%) (19211/23168)\n",
      "Epoch: 94 | Batch_idx: 190 |  Loss_1: (0.4604) | Acc_1: (82.87%) (20260/24448)\n",
      "Epoch: 94 | Batch_idx: 200 |  Loss_1: (0.4600) | Acc_1: (82.88%) (21324/25728)\n",
      "Epoch: 94 | Batch_idx: 210 |  Loss_1: (0.4609) | Acc_1: (82.84%) (22373/27008)\n",
      "Epoch: 94 | Batch_idx: 220 |  Loss_1: (0.4603) | Acc_1: (82.86%) (23440/28288)\n",
      "Epoch: 94 | Batch_idx: 230 |  Loss_1: (0.4606) | Acc_1: (82.89%) (24508/29568)\n",
      "Epoch: 94 | Batch_idx: 240 |  Loss_1: (0.4615) | Acc_1: (82.85%) (25558/30848)\n",
      "Epoch: 94 | Batch_idx: 250 |  Loss_1: (0.4616) | Acc_1: (82.87%) (26623/32128)\n",
      "Epoch: 94 | Batch_idx: 260 |  Loss_1: (0.4603) | Acc_1: (82.92%) (27702/33408)\n",
      "Epoch: 94 | Batch_idx: 270 |  Loss_1: (0.4591) | Acc_1: (82.95%) (28773/34688)\n",
      "Epoch: 94 | Batch_idx: 280 |  Loss_1: (0.4588) | Acc_1: (82.97%) (29841/35968)\n",
      "Epoch: 94 | Batch_idx: 290 |  Loss_1: (0.4589) | Acc_1: (82.97%) (30905/37248)\n",
      "Epoch: 94 | Batch_idx: 300 |  Loss_1: (0.4599) | Acc_1: (82.94%) (31956/38528)\n",
      "Epoch: 94 | Batch_idx: 310 |  Loss_1: (0.4603) | Acc_1: (82.92%) (33007/39808)\n",
      "Epoch: 94 | Batch_idx: 320 |  Loss_1: (0.4604) | Acc_1: (82.92%) (34069/41088)\n",
      "Epoch: 94 | Batch_idx: 330 |  Loss_1: (0.4586) | Acc_1: (82.98%) (35157/42368)\n",
      "Epoch: 94 | Batch_idx: 340 |  Loss_1: (0.4592) | Acc_1: (82.96%) (36212/43648)\n",
      "Epoch: 94 | Batch_idx: 350 |  Loss_1: (0.4611) | Acc_1: (82.89%) (37239/44928)\n",
      "Epoch: 94 | Batch_idx: 360 |  Loss_1: (0.4616) | Acc_1: (82.86%) (38286/46208)\n",
      "Epoch: 94 | Batch_idx: 370 |  Loss_1: (0.4628) | Acc_1: (82.81%) (39326/47488)\n",
      "Epoch: 94 | Batch_idx: 380 |  Loss_1: (0.4623) | Acc_1: (82.84%) (40398/48768)\n",
      "Epoch: 94 | Batch_idx: 390 |  Loss_1: (0.4626) | Acc_1: (82.83%) (41415/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4543) | Acc: (89.40%) (8940/10000)\n",
      "Epoch: 95 | Batch_idx: 0 |  Loss_1: (0.4500) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 95 | Batch_idx: 10 |  Loss_1: (0.4692) | Acc_1: (82.60%) (1163/1408)\n",
      "Epoch: 95 | Batch_idx: 20 |  Loss_1: (0.4626) | Acc_1: (82.92%) (2229/2688)\n",
      "Epoch: 95 | Batch_idx: 30 |  Loss_1: (0.4528) | Acc_1: (83.06%) (3296/3968)\n",
      "Epoch: 95 | Batch_idx: 40 |  Loss_1: (0.4544) | Acc_1: (82.95%) (4353/5248)\n",
      "Epoch: 95 | Batch_idx: 50 |  Loss_1: (0.4626) | Acc_1: (82.66%) (5396/6528)\n",
      "Epoch: 95 | Batch_idx: 60 |  Loss_1: (0.4722) | Acc_1: (82.31%) (6427/7808)\n",
      "Epoch: 95 | Batch_idx: 70 |  Loss_1: (0.4753) | Acc_1: (82.26%) (7476/9088)\n",
      "Epoch: 95 | Batch_idx: 80 |  Loss_1: (0.4775) | Acc_1: (82.26%) (8529/10368)\n",
      "Epoch: 95 | Batch_idx: 90 |  Loss_1: (0.4805) | Acc_1: (82.16%) (9570/11648)\n",
      "Epoch: 95 | Batch_idx: 100 |  Loss_1: (0.4785) | Acc_1: (82.24%) (10632/12928)\n",
      "Epoch: 95 | Batch_idx: 110 |  Loss_1: (0.4754) | Acc_1: (82.38%) (11705/14208)\n",
      "Epoch: 95 | Batch_idx: 120 |  Loss_1: (0.4736) | Acc_1: (82.39%) (12761/15488)\n",
      "Epoch: 95 | Batch_idx: 130 |  Loss_1: (0.4686) | Acc_1: (82.58%) (13847/16768)\n",
      "Epoch: 95 | Batch_idx: 140 |  Loss_1: (0.4674) | Acc_1: (82.61%) (14910/18048)\n",
      "Epoch: 95 | Batch_idx: 150 |  Loss_1: (0.4680) | Acc_1: (82.58%) (15962/19328)\n",
      "Epoch: 95 | Batch_idx: 160 |  Loss_1: (0.4676) | Acc_1: (82.63%) (17029/20608)\n",
      "Epoch: 95 | Batch_idx: 170 |  Loss_1: (0.4679) | Acc_1: (82.63%) (18086/21888)\n",
      "Epoch: 95 | Batch_idx: 180 |  Loss_1: (0.4678) | Acc_1: (82.64%) (19145/23168)\n",
      "Epoch: 95 | Batch_idx: 190 |  Loss_1: (0.4663) | Acc_1: (82.71%) (20220/24448)\n",
      "Epoch: 95 | Batch_idx: 200 |  Loss_1: (0.4660) | Acc_1: (82.71%) (21280/25728)\n",
      "Epoch: 95 | Batch_idx: 210 |  Loss_1: (0.4669) | Acc_1: (82.70%) (22335/27008)\n",
      "Epoch: 95 | Batch_idx: 220 |  Loss_1: (0.4678) | Acc_1: (82.70%) (23395/28288)\n",
      "Epoch: 95 | Batch_idx: 230 |  Loss_1: (0.4660) | Acc_1: (82.74%) (24466/29568)\n",
      "Epoch: 95 | Batch_idx: 240 |  Loss_1: (0.4663) | Acc_1: (82.74%) (25524/30848)\n",
      "Epoch: 95 | Batch_idx: 250 |  Loss_1: (0.4669) | Acc_1: (82.73%) (26580/32128)\n",
      "Epoch: 95 | Batch_idx: 260 |  Loss_1: (0.4663) | Acc_1: (82.74%) (27643/33408)\n",
      "Epoch: 95 | Batch_idx: 270 |  Loss_1: (0.4674) | Acc_1: (82.72%) (28695/34688)\n",
      "Epoch: 95 | Batch_idx: 280 |  Loss_1: (0.4673) | Acc_1: (82.73%) (29755/35968)\n",
      "Epoch: 95 | Batch_idx: 290 |  Loss_1: (0.4692) | Acc_1: (82.67%) (30792/37248)\n",
      "Epoch: 95 | Batch_idx: 300 |  Loss_1: (0.4701) | Acc_1: (82.63%) (31837/38528)\n",
      "Epoch: 95 | Batch_idx: 310 |  Loss_1: (0.4701) | Acc_1: (82.63%) (32895/39808)\n",
      "Epoch: 95 | Batch_idx: 320 |  Loss_1: (0.4696) | Acc_1: (82.65%) (33959/41088)\n",
      "Epoch: 95 | Batch_idx: 330 |  Loss_1: (0.4705) | Acc_1: (82.62%) (35005/42368)\n",
      "Epoch: 95 | Batch_idx: 340 |  Loss_1: (0.4698) | Acc_1: (82.64%) (36072/43648)\n",
      "Epoch: 95 | Batch_idx: 350 |  Loss_1: (0.4683) | Acc_1: (82.69%) (37153/44928)\n",
      "Epoch: 95 | Batch_idx: 360 |  Loss_1: (0.4675) | Acc_1: (82.74%) (38232/46208)\n",
      "Epoch: 95 | Batch_idx: 370 |  Loss_1: (0.4685) | Acc_1: (82.69%) (39269/47488)\n",
      "Epoch: 95 | Batch_idx: 380 |  Loss_1: (0.4688) | Acc_1: (82.67%) (40316/48768)\n",
      "Epoch: 95 | Batch_idx: 390 |  Loss_1: (0.4678) | Acc_1: (82.71%) (41353/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4756) | Acc: (89.44%) (8944/10000)\n",
      "Epoch: 96 | Batch_idx: 0 |  Loss_1: (0.4849) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 96 | Batch_idx: 10 |  Loss_1: (0.4638) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 96 | Batch_idx: 20 |  Loss_1: (0.4523) | Acc_1: (83.37%) (2241/2688)\n",
      "Epoch: 96 | Batch_idx: 30 |  Loss_1: (0.4470) | Acc_1: (83.52%) (3314/3968)\n",
      "Epoch: 96 | Batch_idx: 40 |  Loss_1: (0.4602) | Acc_1: (83.00%) (4356/5248)\n",
      "Epoch: 96 | Batch_idx: 50 |  Loss_1: (0.4645) | Acc_1: (82.95%) (5415/6528)\n",
      "Epoch: 96 | Batch_idx: 60 |  Loss_1: (0.4582) | Acc_1: (83.13%) (6491/7808)\n",
      "Epoch: 96 | Batch_idx: 70 |  Loss_1: (0.4632) | Acc_1: (82.88%) (7532/9088)\n",
      "Epoch: 96 | Batch_idx: 80 |  Loss_1: (0.4617) | Acc_1: (82.98%) (8603/10368)\n",
      "Epoch: 96 | Batch_idx: 90 |  Loss_1: (0.4638) | Acc_1: (82.91%) (9657/11648)\n",
      "Epoch: 96 | Batch_idx: 100 |  Loss_1: (0.4633) | Acc_1: (82.90%) (10717/12928)\n",
      "Epoch: 96 | Batch_idx: 110 |  Loss_1: (0.4609) | Acc_1: (83.00%) (11792/14208)\n",
      "Epoch: 96 | Batch_idx: 120 |  Loss_1: (0.4650) | Acc_1: (82.81%) (12826/15488)\n",
      "Epoch: 96 | Batch_idx: 130 |  Loss_1: (0.4636) | Acc_1: (82.89%) (13899/16768)\n",
      "Epoch: 96 | Batch_idx: 140 |  Loss_1: (0.4613) | Acc_1: (82.95%) (14970/18048)\n",
      "Epoch: 96 | Batch_idx: 150 |  Loss_1: (0.4610) | Acc_1: (82.94%) (16030/19328)\n",
      "Epoch: 96 | Batch_idx: 160 |  Loss_1: (0.4584) | Acc_1: (83.03%) (17110/20608)\n",
      "Epoch: 96 | Batch_idx: 170 |  Loss_1: (0.4576) | Acc_1: (83.10%) (18188/21888)\n",
      "Epoch: 96 | Batch_idx: 180 |  Loss_1: (0.4591) | Acc_1: (83.03%) (19237/23168)\n",
      "Epoch: 96 | Batch_idx: 190 |  Loss_1: (0.4612) | Acc_1: (82.96%) (20282/24448)\n",
      "Epoch: 96 | Batch_idx: 200 |  Loss_1: (0.4621) | Acc_1: (82.90%) (21329/25728)\n",
      "Epoch: 96 | Batch_idx: 210 |  Loss_1: (0.4609) | Acc_1: (82.93%) (22399/27008)\n",
      "Epoch: 96 | Batch_idx: 220 |  Loss_1: (0.4593) | Acc_1: (83.01%) (23482/28288)\n",
      "Epoch: 96 | Batch_idx: 230 |  Loss_1: (0.4605) | Acc_1: (82.97%) (24533/29568)\n",
      "Epoch: 96 | Batch_idx: 240 |  Loss_1: (0.4601) | Acc_1: (82.96%) (25591/30848)\n",
      "Epoch: 96 | Batch_idx: 250 |  Loss_1: (0.4589) | Acc_1: (83.00%) (26667/32128)\n",
      "Epoch: 96 | Batch_idx: 260 |  Loss_1: (0.4591) | Acc_1: (83.00%) (27730/33408)\n",
      "Epoch: 96 | Batch_idx: 270 |  Loss_1: (0.4601) | Acc_1: (82.98%) (28785/34688)\n",
      "Epoch: 96 | Batch_idx: 280 |  Loss_1: (0.4599) | Acc_1: (83.01%) (29857/35968)\n",
      "Epoch: 96 | Batch_idx: 290 |  Loss_1: (0.4613) | Acc_1: (82.97%) (30905/37248)\n",
      "Epoch: 96 | Batch_idx: 300 |  Loss_1: (0.4622) | Acc_1: (82.94%) (31955/38528)\n",
      "Epoch: 96 | Batch_idx: 310 |  Loss_1: (0.4609) | Acc_1: (82.98%) (33031/39808)\n",
      "Epoch: 96 | Batch_idx: 320 |  Loss_1: (0.4616) | Acc_1: (82.96%) (34087/41088)\n",
      "Epoch: 96 | Batch_idx: 330 |  Loss_1: (0.4620) | Acc_1: (82.96%) (35149/42368)\n",
      "Epoch: 96 | Batch_idx: 340 |  Loss_1: (0.4636) | Acc_1: (82.90%) (36184/43648)\n",
      "Epoch: 96 | Batch_idx: 350 |  Loss_1: (0.4635) | Acc_1: (82.90%) (37244/44928)\n",
      "Epoch: 96 | Batch_idx: 360 |  Loss_1: (0.4632) | Acc_1: (82.91%) (38313/46208)\n",
      "Epoch: 96 | Batch_idx: 370 |  Loss_1: (0.4635) | Acc_1: (82.90%) (39369/47488)\n",
      "Epoch: 96 | Batch_idx: 380 |  Loss_1: (0.4634) | Acc_1: (82.91%) (40434/48768)\n",
      "Epoch: 96 | Batch_idx: 390 |  Loss_1: (0.4632) | Acc_1: (82.92%) (41461/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4627) | Acc: (89.07%) (8907/10000)\n",
      "Epoch: 97 | Batch_idx: 0 |  Loss_1: (0.4677) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 97 | Batch_idx: 10 |  Loss_1: (0.4419) | Acc_1: (84.02%) (1183/1408)\n",
      "Epoch: 97 | Batch_idx: 20 |  Loss_1: (0.4663) | Acc_1: (83.07%) (2233/2688)\n",
      "Epoch: 97 | Batch_idx: 30 |  Loss_1: (0.4628) | Acc_1: (83.14%) (3299/3968)\n",
      "Epoch: 97 | Batch_idx: 40 |  Loss_1: (0.4510) | Acc_1: (83.48%) (4381/5248)\n",
      "Epoch: 97 | Batch_idx: 50 |  Loss_1: (0.4518) | Acc_1: (83.46%) (5448/6528)\n",
      "Epoch: 97 | Batch_idx: 60 |  Loss_1: (0.4512) | Acc_1: (83.40%) (6512/7808)\n",
      "Epoch: 97 | Batch_idx: 70 |  Loss_1: (0.4543) | Acc_1: (83.31%) (7571/9088)\n",
      "Epoch: 97 | Batch_idx: 80 |  Loss_1: (0.4533) | Acc_1: (83.34%) (8641/10368)\n",
      "Epoch: 97 | Batch_idx: 90 |  Loss_1: (0.4523) | Acc_1: (83.38%) (9712/11648)\n",
      "Epoch: 97 | Batch_idx: 100 |  Loss_1: (0.4474) | Acc_1: (83.55%) (10801/12928)\n",
      "Epoch: 97 | Batch_idx: 110 |  Loss_1: (0.4441) | Acc_1: (83.65%) (11885/14208)\n",
      "Epoch: 97 | Batch_idx: 120 |  Loss_1: (0.4459) | Acc_1: (83.59%) (12946/15488)\n",
      "Epoch: 97 | Batch_idx: 130 |  Loss_1: (0.4448) | Acc_1: (83.60%) (14018/16768)\n",
      "Epoch: 97 | Batch_idx: 140 |  Loss_1: (0.4477) | Acc_1: (83.51%) (15072/18048)\n",
      "Epoch: 97 | Batch_idx: 150 |  Loss_1: (0.4501) | Acc_1: (83.40%) (16119/19328)\n",
      "Epoch: 97 | Batch_idx: 160 |  Loss_1: (0.4514) | Acc_1: (83.33%) (17172/20608)\n",
      "Epoch: 97 | Batch_idx: 170 |  Loss_1: (0.4528) | Acc_1: (83.28%) (18228/21888)\n",
      "Epoch: 97 | Batch_idx: 180 |  Loss_1: (0.4515) | Acc_1: (83.30%) (19299/23168)\n",
      "Epoch: 97 | Batch_idx: 190 |  Loss_1: (0.4529) | Acc_1: (83.26%) (20356/24448)\n",
      "Epoch: 97 | Batch_idx: 200 |  Loss_1: (0.4542) | Acc_1: (83.20%) (21406/25728)\n",
      "Epoch: 97 | Batch_idx: 210 |  Loss_1: (0.4576) | Acc_1: (83.09%) (22440/27008)\n",
      "Epoch: 97 | Batch_idx: 220 |  Loss_1: (0.4587) | Acc_1: (83.05%) (23494/28288)\n",
      "Epoch: 97 | Batch_idx: 230 |  Loss_1: (0.4593) | Acc_1: (83.02%) (24546/29568)\n",
      "Epoch: 97 | Batch_idx: 240 |  Loss_1: (0.4582) | Acc_1: (83.05%) (25620/30848)\n",
      "Epoch: 97 | Batch_idx: 250 |  Loss_1: (0.4603) | Acc_1: (82.98%) (26660/32128)\n",
      "Epoch: 97 | Batch_idx: 260 |  Loss_1: (0.4599) | Acc_1: (83.00%) (27730/33408)\n",
      "Epoch: 97 | Batch_idx: 270 |  Loss_1: (0.4604) | Acc_1: (82.99%) (28786/34688)\n",
      "Epoch: 97 | Batch_idx: 280 |  Loss_1: (0.4614) | Acc_1: (82.95%) (29835/35968)\n",
      "Epoch: 97 | Batch_idx: 290 |  Loss_1: (0.4617) | Acc_1: (82.92%) (30887/37248)\n",
      "Epoch: 97 | Batch_idx: 300 |  Loss_1: (0.4612) | Acc_1: (82.94%) (31955/38528)\n",
      "Epoch: 97 | Batch_idx: 310 |  Loss_1: (0.4612) | Acc_1: (82.93%) (33013/39808)\n",
      "Epoch: 97 | Batch_idx: 320 |  Loss_1: (0.4600) | Acc_1: (82.96%) (34088/41088)\n",
      "Epoch: 97 | Batch_idx: 330 |  Loss_1: (0.4612) | Acc_1: (82.92%) (35133/42368)\n",
      "Epoch: 97 | Batch_idx: 340 |  Loss_1: (0.4608) | Acc_1: (82.93%) (36196/43648)\n",
      "Epoch: 97 | Batch_idx: 350 |  Loss_1: (0.4620) | Acc_1: (82.89%) (37239/44928)\n",
      "Epoch: 97 | Batch_idx: 360 |  Loss_1: (0.4626) | Acc_1: (82.87%) (38293/46208)\n",
      "Epoch: 97 | Batch_idx: 370 |  Loss_1: (0.4628) | Acc_1: (82.86%) (39349/47488)\n",
      "Epoch: 97 | Batch_idx: 380 |  Loss_1: (0.4626) | Acc_1: (82.87%) (40414/48768)\n",
      "Epoch: 97 | Batch_idx: 390 |  Loss_1: (0.4625) | Acc_1: (82.85%) (41427/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4939) | Acc: (88.79%) (8879/10000)\n",
      "Epoch: 98 | Batch_idx: 0 |  Loss_1: (0.4503) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 98 | Batch_idx: 10 |  Loss_1: (0.4135) | Acc_1: (85.09%) (1198/1408)\n",
      "Epoch: 98 | Batch_idx: 20 |  Loss_1: (0.4348) | Acc_1: (84.19%) (2263/2688)\n",
      "Epoch: 98 | Batch_idx: 30 |  Loss_1: (0.4324) | Acc_1: (84.25%) (3343/3968)\n",
      "Epoch: 98 | Batch_idx: 40 |  Loss_1: (0.4454) | Acc_1: (83.56%) (4385/5248)\n",
      "Epoch: 98 | Batch_idx: 50 |  Loss_1: (0.4513) | Acc_1: (83.30%) (5438/6528)\n",
      "Epoch: 98 | Batch_idx: 60 |  Loss_1: (0.4500) | Acc_1: (83.36%) (6509/7808)\n",
      "Epoch: 98 | Batch_idx: 70 |  Loss_1: (0.4539) | Acc_1: (83.23%) (7564/9088)\n",
      "Epoch: 98 | Batch_idx: 80 |  Loss_1: (0.4583) | Acc_1: (83.04%) (8610/10368)\n",
      "Epoch: 98 | Batch_idx: 90 |  Loss_1: (0.4568) | Acc_1: (83.12%) (9682/11648)\n",
      "Epoch: 98 | Batch_idx: 100 |  Loss_1: (0.4631) | Acc_1: (82.91%) (10718/12928)\n",
      "Epoch: 98 | Batch_idx: 110 |  Loss_1: (0.4614) | Acc_1: (82.97%) (11788/14208)\n",
      "Epoch: 98 | Batch_idx: 120 |  Loss_1: (0.4624) | Acc_1: (82.94%) (12845/15488)\n",
      "Epoch: 98 | Batch_idx: 130 |  Loss_1: (0.4653) | Acc_1: (82.79%) (13883/16768)\n",
      "Epoch: 98 | Batch_idx: 140 |  Loss_1: (0.4660) | Acc_1: (82.80%) (14943/18048)\n",
      "Epoch: 98 | Batch_idx: 150 |  Loss_1: (0.4676) | Acc_1: (82.73%) (15990/19328)\n",
      "Epoch: 98 | Batch_idx: 160 |  Loss_1: (0.4680) | Acc_1: (82.71%) (17044/20608)\n",
      "Epoch: 98 | Batch_idx: 170 |  Loss_1: (0.4681) | Acc_1: (82.68%) (18096/21888)\n",
      "Epoch: 98 | Batch_idx: 180 |  Loss_1: (0.4668) | Acc_1: (82.72%) (19165/23168)\n",
      "Epoch: 98 | Batch_idx: 190 |  Loss_1: (0.4687) | Acc_1: (82.66%) (20209/24448)\n",
      "Epoch: 98 | Batch_idx: 200 |  Loss_1: (0.4677) | Acc_1: (82.70%) (21278/25728)\n",
      "Epoch: 98 | Batch_idx: 210 |  Loss_1: (0.4655) | Acc_1: (82.77%) (22354/27008)\n",
      "Epoch: 98 | Batch_idx: 220 |  Loss_1: (0.4650) | Acc_1: (82.77%) (23414/28288)\n",
      "Epoch: 98 | Batch_idx: 230 |  Loss_1: (0.4661) | Acc_1: (82.72%) (24460/29568)\n",
      "Epoch: 98 | Batch_idx: 240 |  Loss_1: (0.4649) | Acc_1: (82.79%) (25539/30848)\n",
      "Epoch: 98 | Batch_idx: 250 |  Loss_1: (0.4651) | Acc_1: (82.79%) (26599/32128)\n",
      "Epoch: 98 | Batch_idx: 260 |  Loss_1: (0.4649) | Acc_1: (82.79%) (27659/33408)\n",
      "Epoch: 98 | Batch_idx: 270 |  Loss_1: (0.4639) | Acc_1: (82.84%) (28734/34688)\n",
      "Epoch: 98 | Batch_idx: 280 |  Loss_1: (0.4640) | Acc_1: (82.82%) (29790/35968)\n",
      "Epoch: 98 | Batch_idx: 290 |  Loss_1: (0.4644) | Acc_1: (82.79%) (30839/37248)\n",
      "Epoch: 98 | Batch_idx: 300 |  Loss_1: (0.4651) | Acc_1: (82.78%) (31892/38528)\n",
      "Epoch: 98 | Batch_idx: 310 |  Loss_1: (0.4660) | Acc_1: (82.75%) (32942/39808)\n",
      "Epoch: 98 | Batch_idx: 320 |  Loss_1: (0.4664) | Acc_1: (82.74%) (33997/41088)\n",
      "Epoch: 98 | Batch_idx: 330 |  Loss_1: (0.4664) | Acc_1: (82.73%) (35052/42368)\n",
      "Epoch: 98 | Batch_idx: 340 |  Loss_1: (0.4664) | Acc_1: (82.73%) (36109/43648)\n",
      "Epoch: 98 | Batch_idx: 350 |  Loss_1: (0.4670) | Acc_1: (82.70%) (37155/44928)\n",
      "Epoch: 98 | Batch_idx: 360 |  Loss_1: (0.4682) | Acc_1: (82.67%) (38199/46208)\n",
      "Epoch: 98 | Batch_idx: 370 |  Loss_1: (0.4685) | Acc_1: (82.65%) (39250/47488)\n",
      "Epoch: 98 | Batch_idx: 380 |  Loss_1: (0.4676) | Acc_1: (82.69%) (40328/48768)\n",
      "Epoch: 98 | Batch_idx: 390 |  Loss_1: (0.4671) | Acc_1: (82.72%) (41360/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4562) | Acc: (89.78%) (8978/10000)\n",
      "Epoch: 99 | Batch_idx: 0 |  Loss_1: (0.5409) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 99 | Batch_idx: 10 |  Loss_1: (0.4782) | Acc_1: (81.82%) (1152/1408)\n",
      "Epoch: 99 | Batch_idx: 20 |  Loss_1: (0.4982) | Acc_1: (81.03%) (2178/2688)\n",
      "Epoch: 99 | Batch_idx: 30 |  Loss_1: (0.4990) | Acc_1: (81.02%) (3215/3968)\n",
      "Epoch: 99 | Batch_idx: 40 |  Loss_1: (0.4870) | Acc_1: (81.52%) (4278/5248)\n",
      "Epoch: 99 | Batch_idx: 50 |  Loss_1: (0.4799) | Acc_1: (81.92%) (5348/6528)\n",
      "Epoch: 99 | Batch_idx: 60 |  Loss_1: (0.4814) | Acc_1: (81.86%) (6392/7808)\n",
      "Epoch: 99 | Batch_idx: 70 |  Loss_1: (0.4829) | Acc_1: (81.82%) (7436/9088)\n",
      "Epoch: 99 | Batch_idx: 80 |  Loss_1: (0.4784) | Acc_1: (82.06%) (8508/10368)\n",
      "Epoch: 99 | Batch_idx: 90 |  Loss_1: (0.4835) | Acc_1: (81.91%) (9541/11648)\n",
      "Epoch: 99 | Batch_idx: 100 |  Loss_1: (0.4813) | Acc_1: (82.03%) (10605/12928)\n",
      "Epoch: 99 | Batch_idx: 110 |  Loss_1: (0.4784) | Acc_1: (82.17%) (11675/14208)\n",
      "Epoch: 99 | Batch_idx: 120 |  Loss_1: (0.4776) | Acc_1: (82.22%) (12734/15488)\n",
      "Epoch: 99 | Batch_idx: 130 |  Loss_1: (0.4763) | Acc_1: (82.25%) (13791/16768)\n",
      "Epoch: 99 | Batch_idx: 140 |  Loss_1: (0.4749) | Acc_1: (82.32%) (14858/18048)\n",
      "Epoch: 99 | Batch_idx: 150 |  Loss_1: (0.4737) | Acc_1: (82.38%) (15922/19328)\n",
      "Epoch: 99 | Batch_idx: 160 |  Loss_1: (0.4732) | Acc_1: (82.39%) (16978/20608)\n",
      "Epoch: 99 | Batch_idx: 170 |  Loss_1: (0.4723) | Acc_1: (82.44%) (18044/21888)\n",
      "Epoch: 99 | Batch_idx: 180 |  Loss_1: (0.4733) | Acc_1: (82.42%) (19094/23168)\n",
      "Epoch: 99 | Batch_idx: 190 |  Loss_1: (0.4735) | Acc_1: (82.42%) (20150/24448)\n",
      "Epoch: 99 | Batch_idx: 200 |  Loss_1: (0.4722) | Acc_1: (82.48%) (21221/25728)\n",
      "Epoch: 99 | Batch_idx: 210 |  Loss_1: (0.4710) | Acc_1: (82.55%) (22294/27008)\n",
      "Epoch: 99 | Batch_idx: 220 |  Loss_1: (0.4701) | Acc_1: (82.56%) (23354/28288)\n",
      "Epoch: 99 | Batch_idx: 230 |  Loss_1: (0.4697) | Acc_1: (82.56%) (24410/29568)\n",
      "Epoch: 99 | Batch_idx: 240 |  Loss_1: (0.4677) | Acc_1: (82.65%) (25495/30848)\n",
      "Epoch: 99 | Batch_idx: 250 |  Loss_1: (0.4666) | Acc_1: (82.70%) (26569/32128)\n",
      "Epoch: 99 | Batch_idx: 260 |  Loss_1: (0.4655) | Acc_1: (82.74%) (27641/33408)\n",
      "Epoch: 99 | Batch_idx: 270 |  Loss_1: (0.4648) | Acc_1: (82.76%) (28708/34688)\n",
      "Epoch: 99 | Batch_idx: 280 |  Loss_1: (0.4654) | Acc_1: (82.75%) (29764/35968)\n",
      "Epoch: 99 | Batch_idx: 290 |  Loss_1: (0.4655) | Acc_1: (82.76%) (30825/37248)\n",
      "Epoch: 99 | Batch_idx: 300 |  Loss_1: (0.4648) | Acc_1: (82.76%) (31887/38528)\n",
      "Epoch: 99 | Batch_idx: 310 |  Loss_1: (0.4656) | Acc_1: (82.73%) (32933/39808)\n",
      "Epoch: 99 | Batch_idx: 320 |  Loss_1: (0.4664) | Acc_1: (82.71%) (33983/41088)\n",
      "Epoch: 99 | Batch_idx: 330 |  Loss_1: (0.4674) | Acc_1: (82.68%) (35028/42368)\n",
      "Epoch: 99 | Batch_idx: 340 |  Loss_1: (0.4692) | Acc_1: (82.62%) (36063/43648)\n",
      "Epoch: 99 | Batch_idx: 350 |  Loss_1: (0.4695) | Acc_1: (82.60%) (37110/44928)\n",
      "Epoch: 99 | Batch_idx: 360 |  Loss_1: (0.4700) | Acc_1: (82.58%) (38158/46208)\n",
      "Epoch: 99 | Batch_idx: 370 |  Loss_1: (0.4715) | Acc_1: (82.52%) (39185/47488)\n",
      "Epoch: 99 | Batch_idx: 380 |  Loss_1: (0.4709) | Acc_1: (82.55%) (40257/48768)\n",
      "Epoch: 99 | Batch_idx: 390 |  Loss_1: (0.4704) | Acc_1: (82.55%) (41274/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5042) | Acc: (89.15%) (8915/10000)\n",
      "Epoch: 100 | Batch_idx: 0 |  Loss_1: (0.5148) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 100 | Batch_idx: 10 |  Loss_1: (0.4853) | Acc_1: (82.46%) (1161/1408)\n",
      "Epoch: 100 | Batch_idx: 20 |  Loss_1: (0.4717) | Acc_1: (82.63%) (2221/2688)\n",
      "Epoch: 100 | Batch_idx: 30 |  Loss_1: (0.4831) | Acc_1: (81.96%) (3252/3968)\n",
      "Epoch: 100 | Batch_idx: 40 |  Loss_1: (0.4709) | Acc_1: (82.45%) (4327/5248)\n",
      "Epoch: 100 | Batch_idx: 50 |  Loss_1: (0.4646) | Acc_1: (82.64%) (5395/6528)\n",
      "Epoch: 100 | Batch_idx: 60 |  Loss_1: (0.4613) | Acc_1: (82.84%) (6468/7808)\n",
      "Epoch: 100 | Batch_idx: 70 |  Loss_1: (0.4555) | Acc_1: (83.07%) (7549/9088)\n",
      "Epoch: 100 | Batch_idx: 80 |  Loss_1: (0.4620) | Acc_1: (82.76%) (8581/10368)\n",
      "Epoch: 100 | Batch_idx: 90 |  Loss_1: (0.4623) | Acc_1: (82.75%) (9639/11648)\n",
      "Epoch: 100 | Batch_idx: 100 |  Loss_1: (0.4631) | Acc_1: (82.77%) (10700/12928)\n",
      "Epoch: 100 | Batch_idx: 110 |  Loss_1: (0.4649) | Acc_1: (82.76%) (11758/14208)\n",
      "Epoch: 100 | Batch_idx: 120 |  Loss_1: (0.4630) | Acc_1: (82.81%) (12825/15488)\n",
      "Epoch: 100 | Batch_idx: 130 |  Loss_1: (0.4599) | Acc_1: (82.92%) (13904/16768)\n",
      "Epoch: 100 | Batch_idx: 140 |  Loss_1: (0.4613) | Acc_1: (82.87%) (14956/18048)\n",
      "Epoch: 100 | Batch_idx: 150 |  Loss_1: (0.4610) | Acc_1: (82.86%) (16015/19328)\n",
      "Epoch: 100 | Batch_idx: 160 |  Loss_1: (0.4618) | Acc_1: (82.84%) (17072/20608)\n",
      "Epoch: 100 | Batch_idx: 170 |  Loss_1: (0.4647) | Acc_1: (82.75%) (18113/21888)\n",
      "Epoch: 100 | Batch_idx: 180 |  Loss_1: (0.4662) | Acc_1: (82.70%) (19159/23168)\n",
      "Epoch: 100 | Batch_idx: 190 |  Loss_1: (0.4692) | Acc_1: (82.58%) (20188/24448)\n",
      "Epoch: 100 | Batch_idx: 200 |  Loss_1: (0.4703) | Acc_1: (82.56%) (21240/25728)\n",
      "Epoch: 100 | Batch_idx: 210 |  Loss_1: (0.4690) | Acc_1: (82.62%) (22313/27008)\n",
      "Epoch: 100 | Batch_idx: 220 |  Loss_1: (0.4689) | Acc_1: (82.64%) (23377/28288)\n",
      "Epoch: 100 | Batch_idx: 230 |  Loss_1: (0.4693) | Acc_1: (82.64%) (24434/29568)\n",
      "Epoch: 100 | Batch_idx: 240 |  Loss_1: (0.4703) | Acc_1: (82.61%) (25482/30848)\n",
      "Epoch: 100 | Batch_idx: 250 |  Loss_1: (0.4685) | Acc_1: (82.68%) (26564/32128)\n",
      "Epoch: 100 | Batch_idx: 260 |  Loss_1: (0.4688) | Acc_1: (82.67%) (27620/33408)\n",
      "Epoch: 100 | Batch_idx: 270 |  Loss_1: (0.4683) | Acc_1: (82.69%) (28685/34688)\n",
      "Epoch: 100 | Batch_idx: 280 |  Loss_1: (0.4685) | Acc_1: (82.69%) (29742/35968)\n",
      "Epoch: 100 | Batch_idx: 290 |  Loss_1: (0.4680) | Acc_1: (82.70%) (30804/37248)\n",
      "Epoch: 100 | Batch_idx: 300 |  Loss_1: (0.4665) | Acc_1: (82.75%) (31882/38528)\n",
      "Epoch: 100 | Batch_idx: 310 |  Loss_1: (0.4661) | Acc_1: (82.75%) (32943/39808)\n",
      "Epoch: 100 | Batch_idx: 320 |  Loss_1: (0.4669) | Acc_1: (82.72%) (33987/41088)\n",
      "Epoch: 100 | Batch_idx: 330 |  Loss_1: (0.4664) | Acc_1: (82.74%) (35056/42368)\n",
      "Epoch: 100 | Batch_idx: 340 |  Loss_1: (0.4661) | Acc_1: (82.76%) (36124/43648)\n",
      "Epoch: 100 | Batch_idx: 350 |  Loss_1: (0.4659) | Acc_1: (82.78%) (37192/44928)\n",
      "Epoch: 100 | Batch_idx: 360 |  Loss_1: (0.4665) | Acc_1: (82.75%) (38239/46208)\n",
      "Epoch: 100 | Batch_idx: 370 |  Loss_1: (0.4655) | Acc_1: (82.79%) (39315/47488)\n",
      "Epoch: 100 | Batch_idx: 380 |  Loss_1: (0.4658) | Acc_1: (82.77%) (40364/48768)\n",
      "Epoch: 100 | Batch_idx: 390 |  Loss_1: (0.4666) | Acc_1: (82.75%) (41374/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4744) | Acc: (89.31%) (8931/10000)\n",
      "Epoch: 101 | Batch_idx: 0 |  Loss_1: (0.4881) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 101 | Batch_idx: 10 |  Loss_1: (0.4322) | Acc_1: (84.30%) (1187/1408)\n",
      "Epoch: 101 | Batch_idx: 20 |  Loss_1: (0.4455) | Acc_1: (83.82%) (2253/2688)\n",
      "Epoch: 101 | Batch_idx: 30 |  Loss_1: (0.4722) | Acc_1: (82.71%) (3282/3968)\n",
      "Epoch: 101 | Batch_idx: 40 |  Loss_1: (0.4785) | Acc_1: (82.41%) (4325/5248)\n",
      "Epoch: 101 | Batch_idx: 50 |  Loss_1: (0.4742) | Acc_1: (82.57%) (5390/6528)\n",
      "Epoch: 101 | Batch_idx: 60 |  Loss_1: (0.4699) | Acc_1: (82.66%) (6454/7808)\n",
      "Epoch: 101 | Batch_idx: 70 |  Loss_1: (0.4721) | Acc_1: (82.58%) (7505/9088)\n",
      "Epoch: 101 | Batch_idx: 80 |  Loss_1: (0.4709) | Acc_1: (82.64%) (8568/10368)\n",
      "Epoch: 101 | Batch_idx: 90 |  Loss_1: (0.4660) | Acc_1: (82.81%) (9646/11648)\n",
      "Epoch: 101 | Batch_idx: 100 |  Loss_1: (0.4650) | Acc_1: (82.89%) (10716/12928)\n",
      "Epoch: 101 | Batch_idx: 110 |  Loss_1: (0.4634) | Acc_1: (82.95%) (11786/14208)\n",
      "Epoch: 101 | Batch_idx: 120 |  Loss_1: (0.4647) | Acc_1: (82.90%) (12840/15488)\n",
      "Epoch: 101 | Batch_idx: 130 |  Loss_1: (0.4631) | Acc_1: (82.96%) (13910/16768)\n",
      "Epoch: 101 | Batch_idx: 140 |  Loss_1: (0.4615) | Acc_1: (83.02%) (14984/18048)\n",
      "Epoch: 101 | Batch_idx: 150 |  Loss_1: (0.4612) | Acc_1: (83.00%) (16042/19328)\n",
      "Epoch: 101 | Batch_idx: 160 |  Loss_1: (0.4605) | Acc_1: (83.00%) (17104/20608)\n",
      "Epoch: 101 | Batch_idx: 170 |  Loss_1: (0.4594) | Acc_1: (83.03%) (18174/21888)\n",
      "Epoch: 101 | Batch_idx: 180 |  Loss_1: (0.4582) | Acc_1: (83.06%) (19243/23168)\n",
      "Epoch: 101 | Batch_idx: 190 |  Loss_1: (0.4590) | Acc_1: (83.03%) (20300/24448)\n",
      "Epoch: 101 | Batch_idx: 200 |  Loss_1: (0.4589) | Acc_1: (83.05%) (21366/25728)\n",
      "Epoch: 101 | Batch_idx: 210 |  Loss_1: (0.4591) | Acc_1: (83.03%) (22424/27008)\n",
      "Epoch: 101 | Batch_idx: 220 |  Loss_1: (0.4591) | Acc_1: (83.02%) (23484/28288)\n",
      "Epoch: 101 | Batch_idx: 230 |  Loss_1: (0.4575) | Acc_1: (83.05%) (24557/29568)\n",
      "Epoch: 101 | Batch_idx: 240 |  Loss_1: (0.4578) | Acc_1: (83.04%) (25616/30848)\n",
      "Epoch: 101 | Batch_idx: 250 |  Loss_1: (0.4592) | Acc_1: (82.98%) (26659/32128)\n",
      "Epoch: 101 | Batch_idx: 260 |  Loss_1: (0.4585) | Acc_1: (83.02%) (27736/33408)\n",
      "Epoch: 101 | Batch_idx: 270 |  Loss_1: (0.4595) | Acc_1: (83.00%) (28791/34688)\n",
      "Epoch: 101 | Batch_idx: 280 |  Loss_1: (0.4622) | Acc_1: (82.89%) (29815/35968)\n",
      "Epoch: 101 | Batch_idx: 290 |  Loss_1: (0.4622) | Acc_1: (82.90%) (30878/37248)\n",
      "Epoch: 101 | Batch_idx: 300 |  Loss_1: (0.4617) | Acc_1: (82.93%) (31952/38528)\n",
      "Epoch: 101 | Batch_idx: 310 |  Loss_1: (0.4614) | Acc_1: (82.93%) (33012/39808)\n",
      "Epoch: 101 | Batch_idx: 320 |  Loss_1: (0.4604) | Acc_1: (82.96%) (34086/41088)\n",
      "Epoch: 101 | Batch_idx: 330 |  Loss_1: (0.4605) | Acc_1: (82.98%) (35156/42368)\n",
      "Epoch: 101 | Batch_idx: 340 |  Loss_1: (0.4608) | Acc_1: (82.94%) (36203/43648)\n",
      "Epoch: 101 | Batch_idx: 350 |  Loss_1: (0.4619) | Acc_1: (82.90%) (37246/44928)\n",
      "Epoch: 101 | Batch_idx: 360 |  Loss_1: (0.4625) | Acc_1: (82.87%) (38294/46208)\n",
      "Epoch: 101 | Batch_idx: 370 |  Loss_1: (0.4624) | Acc_1: (82.88%) (39356/47488)\n",
      "Epoch: 101 | Batch_idx: 380 |  Loss_1: (0.4623) | Acc_1: (82.87%) (40414/48768)\n",
      "Epoch: 101 | Batch_idx: 390 |  Loss_1: (0.4622) | Acc_1: (82.87%) (41434/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4553) | Acc: (90.02%) (9002/10000)\n",
      "Epoch: 102 | Batch_idx: 0 |  Loss_1: (0.5131) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 102 | Batch_idx: 10 |  Loss_1: (0.5001) | Acc_1: (81.18%) (1143/1408)\n",
      "Epoch: 102 | Batch_idx: 20 |  Loss_1: (0.4684) | Acc_1: (82.66%) (2222/2688)\n",
      "Epoch: 102 | Batch_idx: 30 |  Loss_1: (0.4552) | Acc_1: (83.14%) (3299/3968)\n",
      "Epoch: 102 | Batch_idx: 40 |  Loss_1: (0.4593) | Acc_1: (82.87%) (4349/5248)\n",
      "Epoch: 102 | Batch_idx: 50 |  Loss_1: (0.4543) | Acc_1: (83.01%) (5419/6528)\n",
      "Epoch: 102 | Batch_idx: 60 |  Loss_1: (0.4552) | Acc_1: (83.03%) (6483/7808)\n",
      "Epoch: 102 | Batch_idx: 70 |  Loss_1: (0.4493) | Acc_1: (83.31%) (7571/9088)\n",
      "Epoch: 102 | Batch_idx: 80 |  Loss_1: (0.4498) | Acc_1: (83.27%) (8633/10368)\n",
      "Epoch: 102 | Batch_idx: 90 |  Loss_1: (0.4514) | Acc_1: (83.19%) (9690/11648)\n",
      "Epoch: 102 | Batch_idx: 100 |  Loss_1: (0.4558) | Acc_1: (83.03%) (10734/12928)\n",
      "Epoch: 102 | Batch_idx: 110 |  Loss_1: (0.4578) | Acc_1: (82.94%) (11784/14208)\n",
      "Epoch: 102 | Batch_idx: 120 |  Loss_1: (0.4541) | Acc_1: (83.08%) (12868/15488)\n",
      "Epoch: 102 | Batch_idx: 130 |  Loss_1: (0.4526) | Acc_1: (83.16%) (13944/16768)\n",
      "Epoch: 102 | Batch_idx: 140 |  Loss_1: (0.4566) | Acc_1: (83.03%) (14985/18048)\n",
      "Epoch: 102 | Batch_idx: 150 |  Loss_1: (0.4579) | Acc_1: (82.95%) (16032/19328)\n",
      "Epoch: 102 | Batch_idx: 160 |  Loss_1: (0.4603) | Acc_1: (82.82%) (17068/20608)\n",
      "Epoch: 102 | Batch_idx: 170 |  Loss_1: (0.4605) | Acc_1: (82.79%) (18122/21888)\n",
      "Epoch: 102 | Batch_idx: 180 |  Loss_1: (0.4626) | Acc_1: (82.72%) (19164/23168)\n",
      "Epoch: 102 | Batch_idx: 190 |  Loss_1: (0.4617) | Acc_1: (82.73%) (20227/24448)\n",
      "Epoch: 102 | Batch_idx: 200 |  Loss_1: (0.4628) | Acc_1: (82.70%) (21278/25728)\n",
      "Epoch: 102 | Batch_idx: 210 |  Loss_1: (0.4628) | Acc_1: (82.69%) (22334/27008)\n",
      "Epoch: 102 | Batch_idx: 220 |  Loss_1: (0.4638) | Acc_1: (82.67%) (23387/28288)\n",
      "Epoch: 102 | Batch_idx: 230 |  Loss_1: (0.4625) | Acc_1: (82.73%) (24463/29568)\n",
      "Epoch: 102 | Batch_idx: 240 |  Loss_1: (0.4624) | Acc_1: (82.75%) (25526/30848)\n",
      "Epoch: 102 | Batch_idx: 250 |  Loss_1: (0.4620) | Acc_1: (82.78%) (26597/32128)\n",
      "Epoch: 102 | Batch_idx: 260 |  Loss_1: (0.4636) | Acc_1: (82.72%) (27636/33408)\n",
      "Epoch: 102 | Batch_idx: 270 |  Loss_1: (0.4653) | Acc_1: (82.67%) (28676/34688)\n",
      "Epoch: 102 | Batch_idx: 280 |  Loss_1: (0.4656) | Acc_1: (82.66%) (29731/35968)\n",
      "Epoch: 102 | Batch_idx: 290 |  Loss_1: (0.4666) | Acc_1: (82.62%) (30775/37248)\n",
      "Epoch: 102 | Batch_idx: 300 |  Loss_1: (0.4679) | Acc_1: (82.59%) (31820/38528)\n",
      "Epoch: 102 | Batch_idx: 310 |  Loss_1: (0.4679) | Acc_1: (82.58%) (32875/39808)\n",
      "Epoch: 102 | Batch_idx: 320 |  Loss_1: (0.4671) | Acc_1: (82.62%) (33947/41088)\n",
      "Epoch: 102 | Batch_idx: 330 |  Loss_1: (0.4680) | Acc_1: (82.59%) (34990/42368)\n",
      "Epoch: 102 | Batch_idx: 340 |  Loss_1: (0.4682) | Acc_1: (82.57%) (36042/43648)\n",
      "Epoch: 102 | Batch_idx: 350 |  Loss_1: (0.4697) | Acc_1: (82.52%) (37073/44928)\n",
      "Epoch: 102 | Batch_idx: 360 |  Loss_1: (0.4712) | Acc_1: (82.47%) (38106/46208)\n",
      "Epoch: 102 | Batch_idx: 370 |  Loss_1: (0.4715) | Acc_1: (82.47%) (39162/47488)\n",
      "Epoch: 102 | Batch_idx: 380 |  Loss_1: (0.4705) | Acc_1: (82.51%) (40238/48768)\n",
      "Epoch: 102 | Batch_idx: 390 |  Loss_1: (0.4697) | Acc_1: (82.54%) (41269/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4704) | Acc: (89.81%) (8981/10000)\n",
      "Epoch: 103 | Batch_idx: 0 |  Loss_1: (0.5243) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 103 | Batch_idx: 10 |  Loss_1: (0.4501) | Acc_1: (83.59%) (1177/1408)\n",
      "Epoch: 103 | Batch_idx: 20 |  Loss_1: (0.4573) | Acc_1: (83.11%) (2234/2688)\n",
      "Epoch: 103 | Batch_idx: 30 |  Loss_1: (0.4584) | Acc_1: (83.09%) (3297/3968)\n",
      "Epoch: 103 | Batch_idx: 40 |  Loss_1: (0.4606) | Acc_1: (82.87%) (4349/5248)\n",
      "Epoch: 103 | Batch_idx: 50 |  Loss_1: (0.4591) | Acc_1: (83.04%) (5421/6528)\n",
      "Epoch: 103 | Batch_idx: 60 |  Loss_1: (0.4568) | Acc_1: (83.08%) (6487/7808)\n",
      "Epoch: 103 | Batch_idx: 70 |  Loss_1: (0.4557) | Acc_1: (83.14%) (7556/9088)\n",
      "Epoch: 103 | Batch_idx: 80 |  Loss_1: (0.4573) | Acc_1: (83.02%) (8608/10368)\n",
      "Epoch: 103 | Batch_idx: 90 |  Loss_1: (0.4544) | Acc_1: (83.10%) (9680/11648)\n",
      "Epoch: 103 | Batch_idx: 100 |  Loss_1: (0.4566) | Acc_1: (83.02%) (10733/12928)\n",
      "Epoch: 103 | Batch_idx: 110 |  Loss_1: (0.4608) | Acc_1: (82.88%) (11775/14208)\n",
      "Epoch: 103 | Batch_idx: 120 |  Loss_1: (0.4600) | Acc_1: (82.95%) (12847/15488)\n",
      "Epoch: 103 | Batch_idx: 130 |  Loss_1: (0.4641) | Acc_1: (82.80%) (13884/16768)\n",
      "Epoch: 103 | Batch_idx: 140 |  Loss_1: (0.4628) | Acc_1: (82.87%) (14957/18048)\n",
      "Epoch: 103 | Batch_idx: 150 |  Loss_1: (0.4601) | Acc_1: (82.97%) (16037/19328)\n",
      "Epoch: 103 | Batch_idx: 160 |  Loss_1: (0.4616) | Acc_1: (82.89%) (17083/20608)\n",
      "Epoch: 103 | Batch_idx: 170 |  Loss_1: (0.4591) | Acc_1: (83.00%) (18166/21888)\n",
      "Epoch: 103 | Batch_idx: 180 |  Loss_1: (0.4600) | Acc_1: (82.95%) (19217/23168)\n",
      "Epoch: 103 | Batch_idx: 190 |  Loss_1: (0.4623) | Acc_1: (82.85%) (20255/24448)\n",
      "Epoch: 103 | Batch_idx: 200 |  Loss_1: (0.4633) | Acc_1: (82.78%) (21298/25728)\n",
      "Epoch: 103 | Batch_idx: 210 |  Loss_1: (0.4636) | Acc_1: (82.76%) (22351/27008)\n",
      "Epoch: 103 | Batch_idx: 220 |  Loss_1: (0.4655) | Acc_1: (82.68%) (23388/28288)\n",
      "Epoch: 103 | Batch_idx: 230 |  Loss_1: (0.4663) | Acc_1: (82.66%) (24442/29568)\n",
      "Epoch: 103 | Batch_idx: 240 |  Loss_1: (0.4645) | Acc_1: (82.76%) (25530/30848)\n",
      "Epoch: 103 | Batch_idx: 250 |  Loss_1: (0.4650) | Acc_1: (82.74%) (26582/32128)\n",
      "Epoch: 103 | Batch_idx: 260 |  Loss_1: (0.4655) | Acc_1: (82.74%) (27643/33408)\n",
      "Epoch: 103 | Batch_idx: 270 |  Loss_1: (0.4635) | Acc_1: (82.81%) (28726/34688)\n",
      "Epoch: 103 | Batch_idx: 280 |  Loss_1: (0.4612) | Acc_1: (82.90%) (29818/35968)\n",
      "Epoch: 103 | Batch_idx: 290 |  Loss_1: (0.4625) | Acc_1: (82.86%) (30862/37248)\n",
      "Epoch: 103 | Batch_idx: 300 |  Loss_1: (0.4619) | Acc_1: (82.88%) (31932/38528)\n",
      "Epoch: 103 | Batch_idx: 310 |  Loss_1: (0.4623) | Acc_1: (82.87%) (32988/39808)\n",
      "Epoch: 103 | Batch_idx: 320 |  Loss_1: (0.4632) | Acc_1: (82.83%) (34034/41088)\n",
      "Epoch: 103 | Batch_idx: 330 |  Loss_1: (0.4622) | Acc_1: (82.87%) (35109/42368)\n",
      "Epoch: 103 | Batch_idx: 340 |  Loss_1: (0.4629) | Acc_1: (82.84%) (36159/43648)\n",
      "Epoch: 103 | Batch_idx: 350 |  Loss_1: (0.4635) | Acc_1: (82.83%) (37214/44928)\n",
      "Epoch: 103 | Batch_idx: 360 |  Loss_1: (0.4645) | Acc_1: (82.79%) (38254/46208)\n",
      "Epoch: 103 | Batch_idx: 370 |  Loss_1: (0.4639) | Acc_1: (82.79%) (39317/47488)\n",
      "Epoch: 103 | Batch_idx: 380 |  Loss_1: (0.4637) | Acc_1: (82.79%) (40373/48768)\n",
      "Epoch: 103 | Batch_idx: 390 |  Loss_1: (0.4645) | Acc_1: (82.76%) (41378/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4856) | Acc: (89.51%) (8951/10000)\n",
      "Epoch: 104 | Batch_idx: 0 |  Loss_1: (0.2716) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 104 | Batch_idx: 10 |  Loss_1: (0.5040) | Acc_1: (81.39%) (1146/1408)\n",
      "Epoch: 104 | Batch_idx: 20 |  Loss_1: (0.4899) | Acc_1: (81.99%) (2204/2688)\n",
      "Epoch: 104 | Batch_idx: 30 |  Loss_1: (0.4682) | Acc_1: (82.81%) (3286/3968)\n",
      "Epoch: 104 | Batch_idx: 40 |  Loss_1: (0.4777) | Acc_1: (82.45%) (4327/5248)\n",
      "Epoch: 104 | Batch_idx: 50 |  Loss_1: (0.4786) | Acc_1: (82.40%) (5379/6528)\n",
      "Epoch: 104 | Batch_idx: 60 |  Loss_1: (0.4786) | Acc_1: (82.38%) (6432/7808)\n",
      "Epoch: 104 | Batch_idx: 70 |  Loss_1: (0.4796) | Acc_1: (82.28%) (7478/9088)\n",
      "Epoch: 104 | Batch_idx: 80 |  Loss_1: (0.4731) | Acc_1: (82.48%) (8552/10368)\n",
      "Epoch: 104 | Batch_idx: 90 |  Loss_1: (0.4736) | Acc_1: (82.49%) (9608/11648)\n",
      "Epoch: 104 | Batch_idx: 100 |  Loss_1: (0.4744) | Acc_1: (82.46%) (10661/12928)\n",
      "Epoch: 104 | Batch_idx: 110 |  Loss_1: (0.4718) | Acc_1: (82.55%) (11728/14208)\n",
      "Epoch: 104 | Batch_idx: 120 |  Loss_1: (0.4714) | Acc_1: (82.53%) (12783/15488)\n",
      "Epoch: 104 | Batch_idx: 130 |  Loss_1: (0.4729) | Acc_1: (82.46%) (13827/16768)\n",
      "Epoch: 104 | Batch_idx: 140 |  Loss_1: (0.4753) | Acc_1: (82.35%) (14862/18048)\n",
      "Epoch: 104 | Batch_idx: 150 |  Loss_1: (0.4756) | Acc_1: (82.35%) (15917/19328)\n",
      "Epoch: 104 | Batch_idx: 160 |  Loss_1: (0.4763) | Acc_1: (82.35%) (16970/20608)\n",
      "Epoch: 104 | Batch_idx: 170 |  Loss_1: (0.4775) | Acc_1: (82.30%) (18014/21888)\n",
      "Epoch: 104 | Batch_idx: 180 |  Loss_1: (0.4780) | Acc_1: (82.29%) (19064/23168)\n",
      "Epoch: 104 | Batch_idx: 190 |  Loss_1: (0.4779) | Acc_1: (82.28%) (20117/24448)\n",
      "Epoch: 104 | Batch_idx: 200 |  Loss_1: (0.4776) | Acc_1: (82.30%) (21173/25728)\n",
      "Epoch: 104 | Batch_idx: 210 |  Loss_1: (0.4753) | Acc_1: (82.38%) (22250/27008)\n",
      "Epoch: 104 | Batch_idx: 220 |  Loss_1: (0.4754) | Acc_1: (82.39%) (23307/28288)\n",
      "Epoch: 104 | Batch_idx: 230 |  Loss_1: (0.4760) | Acc_1: (82.36%) (24352/29568)\n",
      "Epoch: 104 | Batch_idx: 240 |  Loss_1: (0.4766) | Acc_1: (82.31%) (25392/30848)\n",
      "Epoch: 104 | Batch_idx: 250 |  Loss_1: (0.4753) | Acc_1: (82.35%) (26459/32128)\n",
      "Epoch: 104 | Batch_idx: 260 |  Loss_1: (0.4759) | Acc_1: (82.34%) (27509/33408)\n",
      "Epoch: 104 | Batch_idx: 270 |  Loss_1: (0.4759) | Acc_1: (82.35%) (28564/34688)\n",
      "Epoch: 104 | Batch_idx: 280 |  Loss_1: (0.4760) | Acc_1: (82.34%) (29617/35968)\n",
      "Epoch: 104 | Batch_idx: 290 |  Loss_1: (0.4776) | Acc_1: (82.28%) (30647/37248)\n",
      "Epoch: 104 | Batch_idx: 300 |  Loss_1: (0.4768) | Acc_1: (82.32%) (31715/38528)\n",
      "Epoch: 104 | Batch_idx: 310 |  Loss_1: (0.4780) | Acc_1: (82.27%) (32751/39808)\n",
      "Epoch: 104 | Batch_idx: 320 |  Loss_1: (0.4780) | Acc_1: (82.27%) (33805/41088)\n",
      "Epoch: 104 | Batch_idx: 330 |  Loss_1: (0.4770) | Acc_1: (82.32%) (34876/42368)\n",
      "Epoch: 104 | Batch_idx: 340 |  Loss_1: (0.4766) | Acc_1: (82.32%) (35932/43648)\n",
      "Epoch: 104 | Batch_idx: 350 |  Loss_1: (0.4756) | Acc_1: (82.36%) (37001/44928)\n",
      "Epoch: 104 | Batch_idx: 360 |  Loss_1: (0.4760) | Acc_1: (82.34%) (38049/46208)\n",
      "Epoch: 104 | Batch_idx: 370 |  Loss_1: (0.4769) | Acc_1: (82.31%) (39089/47488)\n",
      "Epoch: 104 | Batch_idx: 380 |  Loss_1: (0.4771) | Acc_1: (82.31%) (40142/48768)\n",
      "Epoch: 104 | Batch_idx: 390 |  Loss_1: (0.4771) | Acc_1: (82.32%) (41161/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4570) | Acc: (89.42%) (8942/10000)\n",
      "Epoch: 105 | Batch_idx: 0 |  Loss_1: (0.5049) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 105 | Batch_idx: 10 |  Loss_1: (0.4466) | Acc_1: (83.59%) (1177/1408)\n",
      "Epoch: 105 | Batch_idx: 20 |  Loss_1: (0.4661) | Acc_1: (82.59%) (2220/2688)\n",
      "Epoch: 105 | Batch_idx: 30 |  Loss_1: (0.4567) | Acc_1: (83.04%) (3295/3968)\n",
      "Epoch: 105 | Batch_idx: 40 |  Loss_1: (0.4574) | Acc_1: (83.12%) (4362/5248)\n",
      "Epoch: 105 | Batch_idx: 50 |  Loss_1: (0.4535) | Acc_1: (83.16%) (5429/6528)\n",
      "Epoch: 105 | Batch_idx: 60 |  Loss_1: (0.4601) | Acc_1: (83.00%) (6481/7808)\n",
      "Epoch: 105 | Batch_idx: 70 |  Loss_1: (0.4614) | Acc_1: (82.98%) (7541/9088)\n",
      "Epoch: 105 | Batch_idx: 80 |  Loss_1: (0.4624) | Acc_1: (82.96%) (8601/10368)\n",
      "Epoch: 105 | Batch_idx: 90 |  Loss_1: (0.4639) | Acc_1: (82.90%) (9656/11648)\n",
      "Epoch: 105 | Batch_idx: 100 |  Loss_1: (0.4619) | Acc_1: (82.96%) (10725/12928)\n",
      "Epoch: 105 | Batch_idx: 110 |  Loss_1: (0.4637) | Acc_1: (82.87%) (11774/14208)\n",
      "Epoch: 105 | Batch_idx: 120 |  Loss_1: (0.4646) | Acc_1: (82.83%) (12829/15488)\n",
      "Epoch: 105 | Batch_idx: 130 |  Loss_1: (0.4677) | Acc_1: (82.70%) (13867/16768)\n",
      "Epoch: 105 | Batch_idx: 140 |  Loss_1: (0.4686) | Acc_1: (82.69%) (14924/18048)\n",
      "Epoch: 105 | Batch_idx: 150 |  Loss_1: (0.4674) | Acc_1: (82.70%) (15984/19328)\n",
      "Epoch: 105 | Batch_idx: 160 |  Loss_1: (0.4663) | Acc_1: (82.72%) (17047/20608)\n",
      "Epoch: 105 | Batch_idx: 170 |  Loss_1: (0.4675) | Acc_1: (82.68%) (18097/21888)\n",
      "Epoch: 105 | Batch_idx: 180 |  Loss_1: (0.4675) | Acc_1: (82.67%) (19152/23168)\n",
      "Epoch: 105 | Batch_idx: 190 |  Loss_1: (0.4693) | Acc_1: (82.63%) (20202/24448)\n",
      "Epoch: 105 | Batch_idx: 200 |  Loss_1: (0.4701) | Acc_1: (82.60%) (21252/25728)\n",
      "Epoch: 105 | Batch_idx: 210 |  Loss_1: (0.4707) | Acc_1: (82.60%) (22309/27008)\n",
      "Epoch: 105 | Batch_idx: 220 |  Loss_1: (0.4704) | Acc_1: (82.63%) (23374/28288)\n",
      "Epoch: 105 | Batch_idx: 230 |  Loss_1: (0.4705) | Acc_1: (82.62%) (24428/29568)\n",
      "Epoch: 105 | Batch_idx: 240 |  Loss_1: (0.4704) | Acc_1: (82.61%) (25485/30848)\n",
      "Epoch: 105 | Batch_idx: 250 |  Loss_1: (0.4690) | Acc_1: (82.67%) (26560/32128)\n",
      "Epoch: 105 | Batch_idx: 260 |  Loss_1: (0.4671) | Acc_1: (82.73%) (27638/33408)\n",
      "Epoch: 105 | Batch_idx: 270 |  Loss_1: (0.4684) | Acc_1: (82.67%) (28676/34688)\n",
      "Epoch: 105 | Batch_idx: 280 |  Loss_1: (0.4681) | Acc_1: (82.69%) (29742/35968)\n",
      "Epoch: 105 | Batch_idx: 290 |  Loss_1: (0.4678) | Acc_1: (82.72%) (30812/37248)\n",
      "Epoch: 105 | Batch_idx: 300 |  Loss_1: (0.4687) | Acc_1: (82.68%) (31855/38528)\n",
      "Epoch: 105 | Batch_idx: 310 |  Loss_1: (0.4679) | Acc_1: (82.72%) (32929/39808)\n",
      "Epoch: 105 | Batch_idx: 320 |  Loss_1: (0.4687) | Acc_1: (82.69%) (33975/41088)\n",
      "Epoch: 105 | Batch_idx: 330 |  Loss_1: (0.4706) | Acc_1: (82.61%) (35000/42368)\n",
      "Epoch: 105 | Batch_idx: 340 |  Loss_1: (0.4711) | Acc_1: (82.59%) (36050/43648)\n",
      "Epoch: 105 | Batch_idx: 350 |  Loss_1: (0.4712) | Acc_1: (82.59%) (37104/44928)\n",
      "Epoch: 105 | Batch_idx: 360 |  Loss_1: (0.4721) | Acc_1: (82.55%) (38146/46208)\n",
      "Epoch: 105 | Batch_idx: 370 |  Loss_1: (0.4710) | Acc_1: (82.60%) (39225/47488)\n",
      "Epoch: 105 | Batch_idx: 380 |  Loss_1: (0.4721) | Acc_1: (82.56%) (40263/48768)\n",
      "Epoch: 105 | Batch_idx: 390 |  Loss_1: (0.4714) | Acc_1: (82.59%) (41295/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4576) | Acc: (89.80%) (8980/10000)\n",
      "Epoch: 106 | Batch_idx: 0 |  Loss_1: (0.5318) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 106 | Batch_idx: 10 |  Loss_1: (0.4948) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 106 | Batch_idx: 20 |  Loss_1: (0.4664) | Acc_1: (83.07%) (2233/2688)\n",
      "Epoch: 106 | Batch_idx: 30 |  Loss_1: (0.4723) | Acc_1: (82.64%) (3279/3968)\n",
      "Epoch: 106 | Batch_idx: 40 |  Loss_1: (0.4759) | Acc_1: (82.49%) (4329/5248)\n",
      "Epoch: 106 | Batch_idx: 50 |  Loss_1: (0.4693) | Acc_1: (82.75%) (5402/6528)\n",
      "Epoch: 106 | Batch_idx: 60 |  Loss_1: (0.4711) | Acc_1: (82.72%) (6459/7808)\n",
      "Epoch: 106 | Batch_idx: 70 |  Loss_1: (0.4661) | Acc_1: (82.96%) (7539/9088)\n",
      "Epoch: 106 | Batch_idx: 80 |  Loss_1: (0.4690) | Acc_1: (82.79%) (8584/10368)\n",
      "Epoch: 106 | Batch_idx: 90 |  Loss_1: (0.4662) | Acc_1: (82.98%) (9666/11648)\n",
      "Epoch: 106 | Batch_idx: 100 |  Loss_1: (0.4637) | Acc_1: (83.11%) (10744/12928)\n",
      "Epoch: 106 | Batch_idx: 110 |  Loss_1: (0.4682) | Acc_1: (82.93%) (11783/14208)\n",
      "Epoch: 106 | Batch_idx: 120 |  Loss_1: (0.4709) | Acc_1: (82.77%) (12820/15488)\n",
      "Epoch: 106 | Batch_idx: 130 |  Loss_1: (0.4750) | Acc_1: (82.59%) (13849/16768)\n",
      "Epoch: 106 | Batch_idx: 140 |  Loss_1: (0.4760) | Acc_1: (82.51%) (14892/18048)\n",
      "Epoch: 106 | Batch_idx: 150 |  Loss_1: (0.4753) | Acc_1: (82.52%) (15949/19328)\n",
      "Epoch: 106 | Batch_idx: 160 |  Loss_1: (0.4752) | Acc_1: (82.48%) (16997/20608)\n",
      "Epoch: 106 | Batch_idx: 170 |  Loss_1: (0.4760) | Acc_1: (82.42%) (18039/21888)\n",
      "Epoch: 106 | Batch_idx: 180 |  Loss_1: (0.4769) | Acc_1: (82.39%) (19089/23168)\n",
      "Epoch: 106 | Batch_idx: 190 |  Loss_1: (0.4755) | Acc_1: (82.44%) (20156/24448)\n",
      "Epoch: 106 | Batch_idx: 200 |  Loss_1: (0.4749) | Acc_1: (82.44%) (21211/25728)\n",
      "Epoch: 106 | Batch_idx: 210 |  Loss_1: (0.4770) | Acc_1: (82.36%) (22244/27008)\n",
      "Epoch: 106 | Batch_idx: 220 |  Loss_1: (0.4762) | Acc_1: (82.38%) (23305/28288)\n",
      "Epoch: 106 | Batch_idx: 230 |  Loss_1: (0.4765) | Acc_1: (82.36%) (24351/29568)\n",
      "Epoch: 106 | Batch_idx: 240 |  Loss_1: (0.4766) | Acc_1: (82.36%) (25406/30848)\n",
      "Epoch: 106 | Batch_idx: 250 |  Loss_1: (0.4763) | Acc_1: (82.38%) (26468/32128)\n",
      "Epoch: 106 | Batch_idx: 260 |  Loss_1: (0.4750) | Acc_1: (82.42%) (27534/33408)\n",
      "Epoch: 106 | Batch_idx: 270 |  Loss_1: (0.4731) | Acc_1: (82.48%) (28610/34688)\n",
      "Epoch: 106 | Batch_idx: 280 |  Loss_1: (0.4725) | Acc_1: (82.49%) (29671/35968)\n",
      "Epoch: 106 | Batch_idx: 290 |  Loss_1: (0.4739) | Acc_1: (82.44%) (30707/37248)\n",
      "Epoch: 106 | Batch_idx: 300 |  Loss_1: (0.4735) | Acc_1: (82.44%) (31761/38528)\n",
      "Epoch: 106 | Batch_idx: 310 |  Loss_1: (0.4743) | Acc_1: (82.42%) (32809/39808)\n",
      "Epoch: 106 | Batch_idx: 320 |  Loss_1: (0.4750) | Acc_1: (82.41%) (33862/41088)\n",
      "Epoch: 106 | Batch_idx: 330 |  Loss_1: (0.4748) | Acc_1: (82.41%) (34917/42368)\n",
      "Epoch: 106 | Batch_idx: 340 |  Loss_1: (0.4758) | Acc_1: (82.37%) (35955/43648)\n",
      "Epoch: 106 | Batch_idx: 350 |  Loss_1: (0.4764) | Acc_1: (82.34%) (36995/44928)\n",
      "Epoch: 106 | Batch_idx: 360 |  Loss_1: (0.4774) | Acc_1: (82.30%) (38031/46208)\n",
      "Epoch: 106 | Batch_idx: 370 |  Loss_1: (0.4766) | Acc_1: (82.33%) (39098/47488)\n",
      "Epoch: 106 | Batch_idx: 380 |  Loss_1: (0.4771) | Acc_1: (82.32%) (40144/48768)\n",
      "Epoch: 106 | Batch_idx: 390 |  Loss_1: (0.4771) | Acc_1: (82.32%) (41160/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4546) | Acc: (89.71%) (8971/10000)\n",
      "Epoch: 107 | Batch_idx: 0 |  Loss_1: (0.4149) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 107 | Batch_idx: 10 |  Loss_1: (0.4762) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 107 | Batch_idx: 20 |  Loss_1: (0.4580) | Acc_1: (82.74%) (2224/2688)\n",
      "Epoch: 107 | Batch_idx: 30 |  Loss_1: (0.4611) | Acc_1: (82.74%) (3283/3968)\n",
      "Epoch: 107 | Batch_idx: 40 |  Loss_1: (0.4686) | Acc_1: (82.39%) (4324/5248)\n",
      "Epoch: 107 | Batch_idx: 50 |  Loss_1: (0.4671) | Acc_1: (82.46%) (5383/6528)\n",
      "Epoch: 107 | Batch_idx: 60 |  Loss_1: (0.4707) | Acc_1: (82.43%) (6436/7808)\n",
      "Epoch: 107 | Batch_idx: 70 |  Loss_1: (0.4684) | Acc_1: (82.56%) (7503/9088)\n",
      "Epoch: 107 | Batch_idx: 80 |  Loss_1: (0.4732) | Acc_1: (82.41%) (8544/10368)\n",
      "Epoch: 107 | Batch_idx: 90 |  Loss_1: (0.4758) | Acc_1: (82.35%) (9592/11648)\n",
      "Epoch: 107 | Batch_idx: 100 |  Loss_1: (0.4760) | Acc_1: (82.29%) (10638/12928)\n",
      "Epoch: 107 | Batch_idx: 110 |  Loss_1: (0.4742) | Acc_1: (82.40%) (11707/14208)\n",
      "Epoch: 107 | Batch_idx: 120 |  Loss_1: (0.4778) | Acc_1: (82.27%) (12742/15488)\n",
      "Epoch: 107 | Batch_idx: 130 |  Loss_1: (0.4748) | Acc_1: (82.37%) (13812/16768)\n",
      "Epoch: 107 | Batch_idx: 140 |  Loss_1: (0.4739) | Acc_1: (82.38%) (14868/18048)\n",
      "Epoch: 107 | Batch_idx: 150 |  Loss_1: (0.4778) | Acc_1: (82.26%) (15900/19328)\n",
      "Epoch: 107 | Batch_idx: 160 |  Loss_1: (0.4768) | Acc_1: (82.29%) (16959/20608)\n",
      "Epoch: 107 | Batch_idx: 170 |  Loss_1: (0.4784) | Acc_1: (82.25%) (18002/21888)\n",
      "Epoch: 107 | Batch_idx: 180 |  Loss_1: (0.4803) | Acc_1: (82.22%) (19049/23168)\n",
      "Epoch: 107 | Batch_idx: 190 |  Loss_1: (0.4781) | Acc_1: (82.30%) (20121/24448)\n",
      "Epoch: 107 | Batch_idx: 200 |  Loss_1: (0.4788) | Acc_1: (82.27%) (21166/25728)\n",
      "Epoch: 107 | Batch_idx: 210 |  Loss_1: (0.4792) | Acc_1: (82.25%) (22213/27008)\n",
      "Epoch: 107 | Batch_idx: 220 |  Loss_1: (0.4796) | Acc_1: (82.23%) (23260/28288)\n",
      "Epoch: 107 | Batch_idx: 230 |  Loss_1: (0.4781) | Acc_1: (82.28%) (24330/29568)\n",
      "Epoch: 107 | Batch_idx: 240 |  Loss_1: (0.4754) | Acc_1: (82.37%) (25409/30848)\n",
      "Epoch: 107 | Batch_idx: 250 |  Loss_1: (0.4759) | Acc_1: (82.33%) (26451/32128)\n",
      "Epoch: 107 | Batch_idx: 260 |  Loss_1: (0.4763) | Acc_1: (82.32%) (27501/33408)\n",
      "Epoch: 107 | Batch_idx: 270 |  Loss_1: (0.4761) | Acc_1: (82.33%) (28560/34688)\n",
      "Epoch: 107 | Batch_idx: 280 |  Loss_1: (0.4770) | Acc_1: (82.31%) (29605/35968)\n",
      "Epoch: 107 | Batch_idx: 290 |  Loss_1: (0.4766) | Acc_1: (82.33%) (30666/37248)\n",
      "Epoch: 107 | Batch_idx: 300 |  Loss_1: (0.4776) | Acc_1: (82.29%) (31705/38528)\n",
      "Epoch: 107 | Batch_idx: 310 |  Loss_1: (0.4781) | Acc_1: (82.27%) (32752/39808)\n",
      "Epoch: 107 | Batch_idx: 320 |  Loss_1: (0.4791) | Acc_1: (82.25%) (33795/41088)\n",
      "Epoch: 107 | Batch_idx: 330 |  Loss_1: (0.4782) | Acc_1: (82.26%) (34852/42368)\n",
      "Epoch: 107 | Batch_idx: 340 |  Loss_1: (0.4775) | Acc_1: (82.28%) (35913/43648)\n",
      "Epoch: 107 | Batch_idx: 350 |  Loss_1: (0.4776) | Acc_1: (82.28%) (36966/44928)\n",
      "Epoch: 107 | Batch_idx: 360 |  Loss_1: (0.4777) | Acc_1: (82.29%) (38024/46208)\n",
      "Epoch: 107 | Batch_idx: 370 |  Loss_1: (0.4773) | Acc_1: (82.30%) (39083/47488)\n",
      "Epoch: 107 | Batch_idx: 380 |  Loss_1: (0.4773) | Acc_1: (82.29%) (40131/48768)\n",
      "Epoch: 107 | Batch_idx: 390 |  Loss_1: (0.4779) | Acc_1: (82.26%) (41128/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4519) | Acc: (89.38%) (8938/10000)\n",
      "Epoch: 108 | Batch_idx: 0 |  Loss_1: (0.5505) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 108 | Batch_idx: 10 |  Loss_1: (0.4897) | Acc_1: (81.61%) (1149/1408)\n",
      "Epoch: 108 | Batch_idx: 20 |  Loss_1: (0.4796) | Acc_1: (82.33%) (2213/2688)\n",
      "Epoch: 108 | Batch_idx: 30 |  Loss_1: (0.4692) | Acc_1: (82.66%) (3280/3968)\n",
      "Epoch: 108 | Batch_idx: 40 |  Loss_1: (0.4713) | Acc_1: (82.53%) (4331/5248)\n",
      "Epoch: 108 | Batch_idx: 50 |  Loss_1: (0.4599) | Acc_1: (82.95%) (5415/6528)\n",
      "Epoch: 108 | Batch_idx: 60 |  Loss_1: (0.4607) | Acc_1: (82.86%) (6470/7808)\n",
      "Epoch: 108 | Batch_idx: 70 |  Loss_1: (0.4578) | Acc_1: (83.02%) (7545/9088)\n",
      "Epoch: 108 | Batch_idx: 80 |  Loss_1: (0.4643) | Acc_1: (82.75%) (8580/10368)\n",
      "Epoch: 108 | Batch_idx: 90 |  Loss_1: (0.4668) | Acc_1: (82.62%) (9624/11648)\n",
      "Epoch: 108 | Batch_idx: 100 |  Loss_1: (0.4700) | Acc_1: (82.55%) (10672/12928)\n",
      "Epoch: 108 | Batch_idx: 110 |  Loss_1: (0.4719) | Acc_1: (82.50%) (11721/14208)\n",
      "Epoch: 108 | Batch_idx: 120 |  Loss_1: (0.4745) | Acc_1: (82.39%) (12760/15488)\n",
      "Epoch: 108 | Batch_idx: 130 |  Loss_1: (0.4751) | Acc_1: (82.41%) (13819/16768)\n",
      "Epoch: 108 | Batch_idx: 140 |  Loss_1: (0.4765) | Acc_1: (82.41%) (14873/18048)\n",
      "Epoch: 108 | Batch_idx: 150 |  Loss_1: (0.4759) | Acc_1: (82.38%) (15922/19328)\n",
      "Epoch: 108 | Batch_idx: 160 |  Loss_1: (0.4719) | Acc_1: (82.52%) (17005/20608)\n",
      "Epoch: 108 | Batch_idx: 170 |  Loss_1: (0.4720) | Acc_1: (82.50%) (18058/21888)\n",
      "Epoch: 108 | Batch_idx: 180 |  Loss_1: (0.4703) | Acc_1: (82.57%) (19130/23168)\n",
      "Epoch: 108 | Batch_idx: 190 |  Loss_1: (0.4713) | Acc_1: (82.55%) (20183/24448)\n",
      "Epoch: 108 | Batch_idx: 200 |  Loss_1: (0.4705) | Acc_1: (82.59%) (21249/25728)\n",
      "Epoch: 108 | Batch_idx: 210 |  Loss_1: (0.4720) | Acc_1: (82.51%) (22284/27008)\n",
      "Epoch: 108 | Batch_idx: 220 |  Loss_1: (0.4707) | Acc_1: (82.55%) (23353/28288)\n",
      "Epoch: 108 | Batch_idx: 230 |  Loss_1: (0.4720) | Acc_1: (82.51%) (24397/29568)\n",
      "Epoch: 108 | Batch_idx: 240 |  Loss_1: (0.4720) | Acc_1: (82.52%) (25456/30848)\n",
      "Epoch: 108 | Batch_idx: 250 |  Loss_1: (0.4740) | Acc_1: (82.42%) (26481/32128)\n",
      "Epoch: 108 | Batch_idx: 260 |  Loss_1: (0.4749) | Acc_1: (82.38%) (27523/33408)\n",
      "Epoch: 108 | Batch_idx: 270 |  Loss_1: (0.4742) | Acc_1: (82.42%) (28590/34688)\n",
      "Epoch: 108 | Batch_idx: 280 |  Loss_1: (0.4740) | Acc_1: (82.43%) (29649/35968)\n",
      "Epoch: 108 | Batch_idx: 290 |  Loss_1: (0.4744) | Acc_1: (82.41%) (30696/37248)\n",
      "Epoch: 108 | Batch_idx: 300 |  Loss_1: (0.4752) | Acc_1: (82.38%) (31739/38528)\n",
      "Epoch: 108 | Batch_idx: 310 |  Loss_1: (0.4766) | Acc_1: (82.33%) (32775/39808)\n",
      "Epoch: 108 | Batch_idx: 320 |  Loss_1: (0.4755) | Acc_1: (82.37%) (33843/41088)\n",
      "Epoch: 108 | Batch_idx: 330 |  Loss_1: (0.4745) | Acc_1: (82.42%) (34920/42368)\n",
      "Epoch: 108 | Batch_idx: 340 |  Loss_1: (0.4747) | Acc_1: (82.40%) (35967/43648)\n",
      "Epoch: 108 | Batch_idx: 350 |  Loss_1: (0.4758) | Acc_1: (82.37%) (37009/44928)\n",
      "Epoch: 108 | Batch_idx: 360 |  Loss_1: (0.4754) | Acc_1: (82.38%) (38066/46208)\n",
      "Epoch: 108 | Batch_idx: 370 |  Loss_1: (0.4770) | Acc_1: (82.32%) (39091/47488)\n",
      "Epoch: 108 | Batch_idx: 380 |  Loss_1: (0.4768) | Acc_1: (82.33%) (40151/48768)\n",
      "Epoch: 108 | Batch_idx: 390 |  Loss_1: (0.4763) | Acc_1: (82.34%) (41171/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4681) | Acc: (89.71%) (8971/10000)\n",
      "Epoch: 109 | Batch_idx: 0 |  Loss_1: (0.4768) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 109 | Batch_idx: 10 |  Loss_1: (0.4667) | Acc_1: (82.88%) (1167/1408)\n",
      "Epoch: 109 | Batch_idx: 20 |  Loss_1: (0.4793) | Acc_1: (82.44%) (2216/2688)\n",
      "Epoch: 109 | Batch_idx: 30 |  Loss_1: (0.4752) | Acc_1: (82.54%) (3275/3968)\n",
      "Epoch: 109 | Batch_idx: 40 |  Loss_1: (0.4673) | Acc_1: (82.87%) (4349/5248)\n",
      "Epoch: 109 | Batch_idx: 50 |  Loss_1: (0.4711) | Acc_1: (82.72%) (5400/6528)\n",
      "Epoch: 109 | Batch_idx: 60 |  Loss_1: (0.4703) | Acc_1: (82.72%) (6459/7808)\n",
      "Epoch: 109 | Batch_idx: 70 |  Loss_1: (0.4684) | Acc_1: (82.74%) (7519/9088)\n",
      "Epoch: 109 | Batch_idx: 80 |  Loss_1: (0.4602) | Acc_1: (82.98%) (8603/10368)\n",
      "Epoch: 109 | Batch_idx: 90 |  Loss_1: (0.4644) | Acc_1: (82.80%) (9645/11648)\n",
      "Epoch: 109 | Batch_idx: 100 |  Loss_1: (0.4663) | Acc_1: (82.73%) (10695/12928)\n",
      "Epoch: 109 | Batch_idx: 110 |  Loss_1: (0.4666) | Acc_1: (82.72%) (11753/14208)\n",
      "Epoch: 109 | Batch_idx: 120 |  Loss_1: (0.4696) | Acc_1: (82.57%) (12788/15488)\n",
      "Epoch: 109 | Batch_idx: 130 |  Loss_1: (0.4680) | Acc_1: (82.60%) (13850/16768)\n",
      "Epoch: 109 | Batch_idx: 140 |  Loss_1: (0.4676) | Acc_1: (82.62%) (14912/18048)\n",
      "Epoch: 109 | Batch_idx: 150 |  Loss_1: (0.4689) | Acc_1: (82.59%) (15963/19328)\n",
      "Epoch: 109 | Batch_idx: 160 |  Loss_1: (0.4687) | Acc_1: (82.58%) (17018/20608)\n",
      "Epoch: 109 | Batch_idx: 170 |  Loss_1: (0.4691) | Acc_1: (82.59%) (18078/21888)\n",
      "Epoch: 109 | Batch_idx: 180 |  Loss_1: (0.4692) | Acc_1: (82.59%) (19134/23168)\n",
      "Epoch: 109 | Batch_idx: 190 |  Loss_1: (0.4707) | Acc_1: (82.54%) (20180/24448)\n",
      "Epoch: 109 | Batch_idx: 200 |  Loss_1: (0.4715) | Acc_1: (82.51%) (21227/25728)\n",
      "Epoch: 109 | Batch_idx: 210 |  Loss_1: (0.4709) | Acc_1: (82.53%) (22289/27008)\n",
      "Epoch: 109 | Batch_idx: 220 |  Loss_1: (0.4698) | Acc_1: (82.55%) (23353/28288)\n",
      "Epoch: 109 | Batch_idx: 230 |  Loss_1: (0.4706) | Acc_1: (82.54%) (24405/29568)\n",
      "Epoch: 109 | Batch_idx: 240 |  Loss_1: (0.4703) | Acc_1: (82.56%) (25468/30848)\n",
      "Epoch: 109 | Batch_idx: 250 |  Loss_1: (0.4705) | Acc_1: (82.55%) (26523/32128)\n",
      "Epoch: 109 | Batch_idx: 260 |  Loss_1: (0.4700) | Acc_1: (82.58%) (27589/33408)\n",
      "Epoch: 109 | Batch_idx: 270 |  Loss_1: (0.4700) | Acc_1: (82.59%) (28649/34688)\n",
      "Epoch: 109 | Batch_idx: 280 |  Loss_1: (0.4696) | Acc_1: (82.61%) (29713/35968)\n",
      "Epoch: 109 | Batch_idx: 290 |  Loss_1: (0.4700) | Acc_1: (82.59%) (30764/37248)\n",
      "Epoch: 109 | Batch_idx: 300 |  Loss_1: (0.4689) | Acc_1: (82.62%) (31833/38528)\n",
      "Epoch: 109 | Batch_idx: 310 |  Loss_1: (0.4695) | Acc_1: (82.60%) (32880/39808)\n",
      "Epoch: 109 | Batch_idx: 320 |  Loss_1: (0.4709) | Acc_1: (82.54%) (33916/41088)\n",
      "Epoch: 109 | Batch_idx: 330 |  Loss_1: (0.4705) | Acc_1: (82.57%) (34984/42368)\n",
      "Epoch: 109 | Batch_idx: 340 |  Loss_1: (0.4714) | Acc_1: (82.55%) (36031/43648)\n",
      "Epoch: 109 | Batch_idx: 350 |  Loss_1: (0.4724) | Acc_1: (82.51%) (37068/44928)\n",
      "Epoch: 109 | Batch_idx: 360 |  Loss_1: (0.4728) | Acc_1: (82.49%) (38119/46208)\n",
      "Epoch: 109 | Batch_idx: 370 |  Loss_1: (0.4728) | Acc_1: (82.49%) (39173/47488)\n",
      "Epoch: 109 | Batch_idx: 380 |  Loss_1: (0.4726) | Acc_1: (82.49%) (40230/48768)\n",
      "Epoch: 109 | Batch_idx: 390 |  Loss_1: (0.4714) | Acc_1: (82.53%) (41266/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4682) | Acc: (89.49%) (8949/10000)\n",
      "Epoch: 110 | Batch_idx: 0 |  Loss_1: (0.5200) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 110 | Batch_idx: 10 |  Loss_1: (0.4911) | Acc_1: (81.46%) (1147/1408)\n",
      "Epoch: 110 | Batch_idx: 20 |  Loss_1: (0.4726) | Acc_1: (82.33%) (2213/2688)\n",
      "Epoch: 110 | Batch_idx: 30 |  Loss_1: (0.4713) | Acc_1: (82.36%) (3268/3968)\n",
      "Epoch: 110 | Batch_idx: 40 |  Loss_1: (0.4633) | Acc_1: (82.58%) (4334/5248)\n",
      "Epoch: 110 | Batch_idx: 50 |  Loss_1: (0.4631) | Acc_1: (82.67%) (5397/6528)\n",
      "Epoch: 110 | Batch_idx: 60 |  Loss_1: (0.4630) | Acc_1: (82.66%) (6454/7808)\n",
      "Epoch: 110 | Batch_idx: 70 |  Loss_1: (0.4638) | Acc_1: (82.65%) (7511/9088)\n",
      "Epoch: 110 | Batch_idx: 80 |  Loss_1: (0.4620) | Acc_1: (82.78%) (8583/10368)\n",
      "Epoch: 110 | Batch_idx: 90 |  Loss_1: (0.4655) | Acc_1: (82.73%) (9636/11648)\n",
      "Epoch: 110 | Batch_idx: 100 |  Loss_1: (0.4674) | Acc_1: (82.63%) (10683/12928)\n",
      "Epoch: 110 | Batch_idx: 110 |  Loss_1: (0.4715) | Acc_1: (82.49%) (11720/14208)\n",
      "Epoch: 110 | Batch_idx: 120 |  Loss_1: (0.4701) | Acc_1: (82.56%) (12787/15488)\n",
      "Epoch: 110 | Batch_idx: 130 |  Loss_1: (0.4757) | Acc_1: (82.35%) (13808/16768)\n",
      "Epoch: 110 | Batch_idx: 140 |  Loss_1: (0.4750) | Acc_1: (82.38%) (14868/18048)\n",
      "Epoch: 110 | Batch_idx: 150 |  Loss_1: (0.4789) | Acc_1: (82.26%) (15900/19328)\n",
      "Epoch: 110 | Batch_idx: 160 |  Loss_1: (0.4769) | Acc_1: (82.33%) (16966/20608)\n",
      "Epoch: 110 | Batch_idx: 170 |  Loss_1: (0.4758) | Acc_1: (82.37%) (18029/21888)\n",
      "Epoch: 110 | Batch_idx: 180 |  Loss_1: (0.4755) | Acc_1: (82.37%) (19083/23168)\n",
      "Epoch: 110 | Batch_idx: 190 |  Loss_1: (0.4762) | Acc_1: (82.32%) (20126/24448)\n",
      "Epoch: 110 | Batch_idx: 200 |  Loss_1: (0.4762) | Acc_1: (82.35%) (21186/25728)\n",
      "Epoch: 110 | Batch_idx: 210 |  Loss_1: (0.4748) | Acc_1: (82.39%) (22253/27008)\n",
      "Epoch: 110 | Batch_idx: 220 |  Loss_1: (0.4734) | Acc_1: (82.44%) (23322/28288)\n",
      "Epoch: 110 | Batch_idx: 230 |  Loss_1: (0.4738) | Acc_1: (82.44%) (24375/29568)\n",
      "Epoch: 110 | Batch_idx: 240 |  Loss_1: (0.4727) | Acc_1: (82.46%) (25438/30848)\n",
      "Epoch: 110 | Batch_idx: 250 |  Loss_1: (0.4718) | Acc_1: (82.49%) (26501/32128)\n",
      "Epoch: 110 | Batch_idx: 260 |  Loss_1: (0.4736) | Acc_1: (82.40%) (27528/33408)\n",
      "Epoch: 110 | Batch_idx: 270 |  Loss_1: (0.4739) | Acc_1: (82.38%) (28577/34688)\n",
      "Epoch: 110 | Batch_idx: 280 |  Loss_1: (0.4738) | Acc_1: (82.38%) (29629/35968)\n",
      "Epoch: 110 | Batch_idx: 290 |  Loss_1: (0.4733) | Acc_1: (82.40%) (30692/37248)\n",
      "Epoch: 110 | Batch_idx: 300 |  Loss_1: (0.4735) | Acc_1: (82.40%) (31746/38528)\n",
      "Epoch: 110 | Batch_idx: 310 |  Loss_1: (0.4734) | Acc_1: (82.41%) (32804/39808)\n",
      "Epoch: 110 | Batch_idx: 320 |  Loss_1: (0.4735) | Acc_1: (82.42%) (33864/41088)\n",
      "Epoch: 110 | Batch_idx: 330 |  Loss_1: (0.4748) | Acc_1: (82.37%) (34897/42368)\n",
      "Epoch: 110 | Batch_idx: 340 |  Loss_1: (0.4749) | Acc_1: (82.35%) (35945/43648)\n",
      "Epoch: 110 | Batch_idx: 350 |  Loss_1: (0.4747) | Acc_1: (82.35%) (36999/44928)\n",
      "Epoch: 110 | Batch_idx: 360 |  Loss_1: (0.4745) | Acc_1: (82.35%) (38052/46208)\n",
      "Epoch: 110 | Batch_idx: 370 |  Loss_1: (0.4746) | Acc_1: (82.34%) (39102/47488)\n",
      "Epoch: 110 | Batch_idx: 380 |  Loss_1: (0.4741) | Acc_1: (82.36%) (40163/48768)\n",
      "Epoch: 110 | Batch_idx: 390 |  Loss_1: (0.4736) | Acc_1: (82.36%) (41182/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4741) | Acc: (89.20%) (8920/10000)\n",
      "Epoch: 111 | Batch_idx: 0 |  Loss_1: (0.4016) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 111 | Batch_idx: 10 |  Loss_1: (0.4834) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 111 | Batch_idx: 20 |  Loss_1: (0.4637) | Acc_1: (82.44%) (2216/2688)\n",
      "Epoch: 111 | Batch_idx: 30 |  Loss_1: (0.4629) | Acc_1: (82.43%) (3271/3968)\n",
      "Epoch: 111 | Batch_idx: 40 |  Loss_1: (0.4631) | Acc_1: (82.55%) (4332/5248)\n",
      "Epoch: 111 | Batch_idx: 50 |  Loss_1: (0.4696) | Acc_1: (82.38%) (5378/6528)\n",
      "Epoch: 111 | Batch_idx: 60 |  Loss_1: (0.4745) | Acc_1: (82.16%) (6415/7808)\n",
      "Epoch: 111 | Batch_idx: 70 |  Loss_1: (0.4699) | Acc_1: (82.37%) (7486/9088)\n",
      "Epoch: 111 | Batch_idx: 80 |  Loss_1: (0.4678) | Acc_1: (82.51%) (8555/10368)\n",
      "Epoch: 111 | Batch_idx: 90 |  Loss_1: (0.4710) | Acc_1: (82.37%) (9595/11648)\n",
      "Epoch: 111 | Batch_idx: 100 |  Loss_1: (0.4729) | Acc_1: (82.31%) (10641/12928)\n",
      "Epoch: 111 | Batch_idx: 110 |  Loss_1: (0.4731) | Acc_1: (82.27%) (11689/14208)\n",
      "Epoch: 111 | Batch_idx: 120 |  Loss_1: (0.4735) | Acc_1: (82.22%) (12734/15488)\n",
      "Epoch: 111 | Batch_idx: 130 |  Loss_1: (0.4716) | Acc_1: (82.26%) (13793/16768)\n",
      "Epoch: 111 | Batch_idx: 140 |  Loss_1: (0.4711) | Acc_1: (82.32%) (14858/18048)\n",
      "Epoch: 111 | Batch_idx: 150 |  Loss_1: (0.4711) | Acc_1: (82.34%) (15915/19328)\n",
      "Epoch: 111 | Batch_idx: 160 |  Loss_1: (0.4717) | Acc_1: (82.35%) (16970/20608)\n",
      "Epoch: 111 | Batch_idx: 170 |  Loss_1: (0.4703) | Acc_1: (82.39%) (18033/21888)\n",
      "Epoch: 111 | Batch_idx: 180 |  Loss_1: (0.4729) | Acc_1: (82.29%) (19066/23168)\n",
      "Epoch: 111 | Batch_idx: 190 |  Loss_1: (0.4720) | Acc_1: (82.36%) (20135/24448)\n",
      "Epoch: 111 | Batch_idx: 200 |  Loss_1: (0.4715) | Acc_1: (82.35%) (21188/25728)\n",
      "Epoch: 111 | Batch_idx: 210 |  Loss_1: (0.4694) | Acc_1: (82.45%) (22268/27008)\n",
      "Epoch: 111 | Batch_idx: 220 |  Loss_1: (0.4698) | Acc_1: (82.45%) (23323/28288)\n",
      "Epoch: 111 | Batch_idx: 230 |  Loss_1: (0.4715) | Acc_1: (82.40%) (24363/29568)\n",
      "Epoch: 111 | Batch_idx: 240 |  Loss_1: (0.4712) | Acc_1: (82.41%) (25423/30848)\n",
      "Epoch: 111 | Batch_idx: 250 |  Loss_1: (0.4712) | Acc_1: (82.41%) (26478/32128)\n",
      "Epoch: 111 | Batch_idx: 260 |  Loss_1: (0.4706) | Acc_1: (82.43%) (27537/33408)\n",
      "Epoch: 111 | Batch_idx: 270 |  Loss_1: (0.4704) | Acc_1: (82.42%) (28591/34688)\n",
      "Epoch: 111 | Batch_idx: 280 |  Loss_1: (0.4687) | Acc_1: (82.48%) (29667/35968)\n",
      "Epoch: 111 | Batch_idx: 290 |  Loss_1: (0.4685) | Acc_1: (82.50%) (30730/37248)\n",
      "Epoch: 111 | Batch_idx: 300 |  Loss_1: (0.4682) | Acc_1: (82.51%) (31791/38528)\n",
      "Epoch: 111 | Batch_idx: 310 |  Loss_1: (0.4691) | Acc_1: (82.47%) (32830/39808)\n",
      "Epoch: 111 | Batch_idx: 320 |  Loss_1: (0.4700) | Acc_1: (82.43%) (33869/41088)\n",
      "Epoch: 111 | Batch_idx: 330 |  Loss_1: (0.4705) | Acc_1: (82.41%) (34915/42368)\n",
      "Epoch: 111 | Batch_idx: 340 |  Loss_1: (0.4712) | Acc_1: (82.38%) (35956/43648)\n",
      "Epoch: 111 | Batch_idx: 350 |  Loss_1: (0.4720) | Acc_1: (82.34%) (36994/44928)\n",
      "Epoch: 111 | Batch_idx: 360 |  Loss_1: (0.4722) | Acc_1: (82.34%) (38046/46208)\n",
      "Epoch: 111 | Batch_idx: 370 |  Loss_1: (0.4718) | Acc_1: (82.35%) (39105/47488)\n",
      "Epoch: 111 | Batch_idx: 380 |  Loss_1: (0.4721) | Acc_1: (82.33%) (40152/48768)\n",
      "Epoch: 111 | Batch_idx: 390 |  Loss_1: (0.4721) | Acc_1: (82.33%) (41167/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4659) | Acc: (89.83%) (8983/10000)\n",
      "Epoch: 112 | Batch_idx: 0 |  Loss_1: (0.5774) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 112 | Batch_idx: 10 |  Loss_1: (0.4682) | Acc_1: (82.95%) (1168/1408)\n",
      "Epoch: 112 | Batch_idx: 20 |  Loss_1: (0.4772) | Acc_1: (82.40%) (2215/2688)\n",
      "Epoch: 112 | Batch_idx: 30 |  Loss_1: (0.4776) | Acc_1: (82.43%) (3271/3968)\n",
      "Epoch: 112 | Batch_idx: 40 |  Loss_1: (0.4687) | Acc_1: (82.70%) (4340/5248)\n",
      "Epoch: 112 | Batch_idx: 50 |  Loss_1: (0.4646) | Acc_1: (82.72%) (5400/6528)\n",
      "Epoch: 112 | Batch_idx: 60 |  Loss_1: (0.4640) | Acc_1: (82.80%) (6465/7808)\n",
      "Epoch: 112 | Batch_idx: 70 |  Loss_1: (0.4640) | Acc_1: (82.77%) (7522/9088)\n",
      "Epoch: 112 | Batch_idx: 80 |  Loss_1: (0.4623) | Acc_1: (82.83%) (8588/10368)\n",
      "Epoch: 112 | Batch_idx: 90 |  Loss_1: (0.4671) | Acc_1: (82.62%) (9624/11648)\n",
      "Epoch: 112 | Batch_idx: 100 |  Loss_1: (0.4723) | Acc_1: (82.47%) (10662/12928)\n",
      "Epoch: 112 | Batch_idx: 110 |  Loss_1: (0.4733) | Acc_1: (82.47%) (11717/14208)\n",
      "Epoch: 112 | Batch_idx: 120 |  Loss_1: (0.4748) | Acc_1: (82.40%) (12762/15488)\n",
      "Epoch: 112 | Batch_idx: 130 |  Loss_1: (0.4775) | Acc_1: (82.27%) (13795/16768)\n",
      "Epoch: 112 | Batch_idx: 140 |  Loss_1: (0.4802) | Acc_1: (82.18%) (14832/18048)\n",
      "Epoch: 112 | Batch_idx: 150 |  Loss_1: (0.4803) | Acc_1: (82.17%) (15882/19328)\n",
      "Epoch: 112 | Batch_idx: 160 |  Loss_1: (0.4795) | Acc_1: (82.20%) (16940/20608)\n",
      "Epoch: 112 | Batch_idx: 170 |  Loss_1: (0.4788) | Acc_1: (82.25%) (18002/21888)\n",
      "Epoch: 112 | Batch_idx: 180 |  Loss_1: (0.4785) | Acc_1: (82.26%) (19059/23168)\n",
      "Epoch: 112 | Batch_idx: 190 |  Loss_1: (0.4794) | Acc_1: (82.24%) (20106/24448)\n",
      "Epoch: 112 | Batch_idx: 200 |  Loss_1: (0.4812) | Acc_1: (82.15%) (21136/25728)\n",
      "Epoch: 112 | Batch_idx: 210 |  Loss_1: (0.4789) | Acc_1: (82.25%) (22214/27008)\n",
      "Epoch: 112 | Batch_idx: 220 |  Loss_1: (0.4796) | Acc_1: (82.23%) (23261/28288)\n",
      "Epoch: 112 | Batch_idx: 230 |  Loss_1: (0.4798) | Acc_1: (82.23%) (24314/29568)\n",
      "Epoch: 112 | Batch_idx: 240 |  Loss_1: (0.4786) | Acc_1: (82.27%) (25380/30848)\n",
      "Epoch: 112 | Batch_idx: 250 |  Loss_1: (0.4783) | Acc_1: (82.29%) (26439/32128)\n",
      "Epoch: 112 | Batch_idx: 260 |  Loss_1: (0.4782) | Acc_1: (82.31%) (27497/33408)\n",
      "Epoch: 112 | Batch_idx: 270 |  Loss_1: (0.4795) | Acc_1: (82.26%) (28535/34688)\n",
      "Epoch: 112 | Batch_idx: 280 |  Loss_1: (0.4799) | Acc_1: (82.24%) (29580/35968)\n",
      "Epoch: 112 | Batch_idx: 290 |  Loss_1: (0.4809) | Acc_1: (82.20%) (30618/37248)\n",
      "Epoch: 112 | Batch_idx: 300 |  Loss_1: (0.4807) | Acc_1: (82.21%) (31672/38528)\n",
      "Epoch: 112 | Batch_idx: 310 |  Loss_1: (0.4783) | Acc_1: (82.30%) (32762/39808)\n",
      "Epoch: 112 | Batch_idx: 320 |  Loss_1: (0.4785) | Acc_1: (82.28%) (33806/41088)\n",
      "Epoch: 112 | Batch_idx: 330 |  Loss_1: (0.4777) | Acc_1: (82.31%) (34873/42368)\n",
      "Epoch: 112 | Batch_idx: 340 |  Loss_1: (0.4769) | Acc_1: (82.34%) (35940/43648)\n",
      "Epoch: 112 | Batch_idx: 350 |  Loss_1: (0.4770) | Acc_1: (82.33%) (36988/44928)\n",
      "Epoch: 112 | Batch_idx: 360 |  Loss_1: (0.4780) | Acc_1: (82.30%) (38028/46208)\n",
      "Epoch: 112 | Batch_idx: 370 |  Loss_1: (0.4782) | Acc_1: (82.30%) (39082/47488)\n",
      "Epoch: 112 | Batch_idx: 380 |  Loss_1: (0.4790) | Acc_1: (82.23%) (40104/48768)\n",
      "Epoch: 112 | Batch_idx: 390 |  Loss_1: (0.4795) | Acc_1: (82.22%) (41110/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5081) | Acc: (89.10%) (8910/10000)\n",
      "Epoch: 113 | Batch_idx: 0 |  Loss_1: (0.5399) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 113 | Batch_idx: 10 |  Loss_1: (0.4602) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 113 | Batch_idx: 20 |  Loss_1: (0.4758) | Acc_1: (82.48%) (2217/2688)\n",
      "Epoch: 113 | Batch_idx: 30 |  Loss_1: (0.4964) | Acc_1: (81.78%) (3245/3968)\n",
      "Epoch: 113 | Batch_idx: 40 |  Loss_1: (0.4968) | Acc_1: (81.78%) (4292/5248)\n",
      "Epoch: 113 | Batch_idx: 50 |  Loss_1: (0.4929) | Acc_1: (81.86%) (5344/6528)\n",
      "Epoch: 113 | Batch_idx: 60 |  Loss_1: (0.4906) | Acc_1: (81.92%) (6396/7808)\n",
      "Epoch: 113 | Batch_idx: 70 |  Loss_1: (0.4860) | Acc_1: (82.10%) (7461/9088)\n",
      "Epoch: 113 | Batch_idx: 80 |  Loss_1: (0.4860) | Acc_1: (82.09%) (8511/10368)\n",
      "Epoch: 113 | Batch_idx: 90 |  Loss_1: (0.4867) | Acc_1: (82.07%) (9559/11648)\n",
      "Epoch: 113 | Batch_idx: 100 |  Loss_1: (0.4844) | Acc_1: (82.12%) (10616/12928)\n",
      "Epoch: 113 | Batch_idx: 110 |  Loss_1: (0.4854) | Acc_1: (82.09%) (11663/14208)\n",
      "Epoch: 113 | Batch_idx: 120 |  Loss_1: (0.4850) | Acc_1: (82.19%) (12729/15488)\n",
      "Epoch: 113 | Batch_idx: 130 |  Loss_1: (0.4846) | Acc_1: (82.17%) (13778/16768)\n",
      "Epoch: 113 | Batch_idx: 140 |  Loss_1: (0.4874) | Acc_1: (82.04%) (14807/18048)\n",
      "Epoch: 113 | Batch_idx: 150 |  Loss_1: (0.4868) | Acc_1: (82.05%) (15858/19328)\n",
      "Epoch: 113 | Batch_idx: 160 |  Loss_1: (0.4853) | Acc_1: (82.07%) (16914/20608)\n",
      "Epoch: 113 | Batch_idx: 170 |  Loss_1: (0.4842) | Acc_1: (82.10%) (17970/21888)\n",
      "Epoch: 113 | Batch_idx: 180 |  Loss_1: (0.4821) | Acc_1: (82.17%) (19036/23168)\n",
      "Epoch: 113 | Batch_idx: 190 |  Loss_1: (0.4833) | Acc_1: (82.11%) (20075/24448)\n",
      "Epoch: 113 | Batch_idx: 200 |  Loss_1: (0.4834) | Acc_1: (82.12%) (21128/25728)\n",
      "Epoch: 113 | Batch_idx: 210 |  Loss_1: (0.4839) | Acc_1: (82.09%) (22170/27008)\n",
      "Epoch: 113 | Batch_idx: 220 |  Loss_1: (0.4830) | Acc_1: (82.08%) (23220/28288)\n",
      "Epoch: 113 | Batch_idx: 230 |  Loss_1: (0.4841) | Acc_1: (82.04%) (24259/29568)\n",
      "Epoch: 113 | Batch_idx: 240 |  Loss_1: (0.4858) | Acc_1: (81.98%) (25288/30848)\n",
      "Epoch: 113 | Batch_idx: 250 |  Loss_1: (0.4867) | Acc_1: (81.94%) (26326/32128)\n",
      "Epoch: 113 | Batch_idx: 260 |  Loss_1: (0.4868) | Acc_1: (81.94%) (27373/33408)\n",
      "Epoch: 113 | Batch_idx: 270 |  Loss_1: (0.4870) | Acc_1: (81.94%) (28424/34688)\n",
      "Epoch: 113 | Batch_idx: 280 |  Loss_1: (0.4870) | Acc_1: (81.94%) (29472/35968)\n",
      "Epoch: 113 | Batch_idx: 290 |  Loss_1: (0.4876) | Acc_1: (81.91%) (30510/37248)\n",
      "Epoch: 113 | Batch_idx: 300 |  Loss_1: (0.4876) | Acc_1: (81.91%) (31557/38528)\n",
      "Epoch: 113 | Batch_idx: 310 |  Loss_1: (0.4880) | Acc_1: (81.88%) (32596/39808)\n",
      "Epoch: 113 | Batch_idx: 320 |  Loss_1: (0.4881) | Acc_1: (81.88%) (33641/41088)\n",
      "Epoch: 113 | Batch_idx: 330 |  Loss_1: (0.4877) | Acc_1: (81.91%) (34702/42368)\n",
      "Epoch: 113 | Batch_idx: 340 |  Loss_1: (0.4874) | Acc_1: (81.91%) (35750/43648)\n",
      "Epoch: 113 | Batch_idx: 350 |  Loss_1: (0.4864) | Acc_1: (81.95%) (36819/44928)\n",
      "Epoch: 113 | Batch_idx: 360 |  Loss_1: (0.4853) | Acc_1: (82.00%) (37891/46208)\n",
      "Epoch: 113 | Batch_idx: 370 |  Loss_1: (0.4848) | Acc_1: (82.03%) (38955/47488)\n",
      "Epoch: 113 | Batch_idx: 380 |  Loss_1: (0.4845) | Acc_1: (82.04%) (40011/48768)\n",
      "Epoch: 113 | Batch_idx: 390 |  Loss_1: (0.4843) | Acc_1: (82.05%) (41023/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4435) | Acc: (89.76%) (8976/10000)\n",
      "Epoch: 114 | Batch_idx: 0 |  Loss_1: (0.6213) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 114 | Batch_idx: 10 |  Loss_1: (0.4616) | Acc_1: (82.95%) (1168/1408)\n",
      "Epoch: 114 | Batch_idx: 20 |  Loss_1: (0.4687) | Acc_1: (82.70%) (2223/2688)\n",
      "Epoch: 114 | Batch_idx: 30 |  Loss_1: (0.4674) | Acc_1: (82.89%) (3289/3968)\n",
      "Epoch: 114 | Batch_idx: 40 |  Loss_1: (0.4720) | Acc_1: (82.68%) (4339/5248)\n",
      "Epoch: 114 | Batch_idx: 50 |  Loss_1: (0.4692) | Acc_1: (82.84%) (5408/6528)\n",
      "Epoch: 114 | Batch_idx: 60 |  Loss_1: (0.4737) | Acc_1: (82.62%) (6451/7808)\n",
      "Epoch: 114 | Batch_idx: 70 |  Loss_1: (0.4785) | Acc_1: (82.39%) (7488/9088)\n",
      "Epoch: 114 | Batch_idx: 80 |  Loss_1: (0.4806) | Acc_1: (82.34%) (8537/10368)\n",
      "Epoch: 114 | Batch_idx: 90 |  Loss_1: (0.4836) | Acc_1: (82.20%) (9575/11648)\n",
      "Epoch: 114 | Batch_idx: 100 |  Loss_1: (0.4860) | Acc_1: (82.12%) (10617/12928)\n",
      "Epoch: 114 | Batch_idx: 110 |  Loss_1: (0.4886) | Acc_1: (82.02%) (11653/14208)\n",
      "Epoch: 114 | Batch_idx: 120 |  Loss_1: (0.4881) | Acc_1: (82.02%) (12703/15488)\n",
      "Epoch: 114 | Batch_idx: 130 |  Loss_1: (0.4906) | Acc_1: (81.91%) (13735/16768)\n",
      "Epoch: 114 | Batch_idx: 140 |  Loss_1: (0.4917) | Acc_1: (81.86%) (14774/18048)\n",
      "Epoch: 114 | Batch_idx: 150 |  Loss_1: (0.4911) | Acc_1: (81.87%) (15823/19328)\n",
      "Epoch: 114 | Batch_idx: 160 |  Loss_1: (0.4901) | Acc_1: (81.91%) (16880/20608)\n",
      "Epoch: 114 | Batch_idx: 170 |  Loss_1: (0.4904) | Acc_1: (81.85%) (17916/21888)\n",
      "Epoch: 114 | Batch_idx: 180 |  Loss_1: (0.4886) | Acc_1: (81.93%) (18981/23168)\n",
      "Epoch: 114 | Batch_idx: 190 |  Loss_1: (0.4885) | Acc_1: (81.92%) (20028/24448)\n",
      "Epoch: 114 | Batch_idx: 200 |  Loss_1: (0.4891) | Acc_1: (81.91%) (21073/25728)\n",
      "Epoch: 114 | Batch_idx: 210 |  Loss_1: (0.4900) | Acc_1: (81.86%) (22108/27008)\n",
      "Epoch: 114 | Batch_idx: 220 |  Loss_1: (0.4878) | Acc_1: (81.93%) (23175/28288)\n",
      "Epoch: 114 | Batch_idx: 230 |  Loss_1: (0.4897) | Acc_1: (81.84%) (24199/29568)\n",
      "Epoch: 114 | Batch_idx: 240 |  Loss_1: (0.4913) | Acc_1: (81.80%) (25234/30848)\n",
      "Epoch: 114 | Batch_idx: 250 |  Loss_1: (0.4912) | Acc_1: (81.80%) (26280/32128)\n",
      "Epoch: 114 | Batch_idx: 260 |  Loss_1: (0.4903) | Acc_1: (81.84%) (27342/33408)\n",
      "Epoch: 114 | Batch_idx: 270 |  Loss_1: (0.4919) | Acc_1: (81.79%) (28370/34688)\n",
      "Epoch: 114 | Batch_idx: 280 |  Loss_1: (0.4916) | Acc_1: (81.81%) (29424/35968)\n",
      "Epoch: 114 | Batch_idx: 290 |  Loss_1: (0.4918) | Acc_1: (81.81%) (30471/37248)\n",
      "Epoch: 114 | Batch_idx: 300 |  Loss_1: (0.4908) | Acc_1: (81.82%) (31524/38528)\n",
      "Epoch: 114 | Batch_idx: 310 |  Loss_1: (0.4908) | Acc_1: (81.82%) (32570/39808)\n",
      "Epoch: 114 | Batch_idx: 320 |  Loss_1: (0.4902) | Acc_1: (81.82%) (33620/41088)\n",
      "Epoch: 114 | Batch_idx: 330 |  Loss_1: (0.4891) | Acc_1: (81.86%) (34683/42368)\n",
      "Epoch: 114 | Batch_idx: 340 |  Loss_1: (0.4900) | Acc_1: (81.82%) (35714/43648)\n",
      "Epoch: 114 | Batch_idx: 350 |  Loss_1: (0.4898) | Acc_1: (81.83%) (36763/44928)\n",
      "Epoch: 114 | Batch_idx: 360 |  Loss_1: (0.4908) | Acc_1: (81.79%) (37793/46208)\n",
      "Epoch: 114 | Batch_idx: 370 |  Loss_1: (0.4916) | Acc_1: (81.77%) (38831/47488)\n",
      "Epoch: 114 | Batch_idx: 380 |  Loss_1: (0.4918) | Acc_1: (81.76%) (39873/48768)\n",
      "Epoch: 114 | Batch_idx: 390 |  Loss_1: (0.4906) | Acc_1: (81.81%) (40906/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4812) | Acc: (89.71%) (8971/10000)\n",
      "Epoch: 115 | Batch_idx: 0 |  Loss_1: (0.5846) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 115 | Batch_idx: 10 |  Loss_1: (0.4805) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 115 | Batch_idx: 20 |  Loss_1: (0.4987) | Acc_1: (81.85%) (2200/2688)\n",
      "Epoch: 115 | Batch_idx: 30 |  Loss_1: (0.4908) | Acc_1: (81.75%) (3244/3968)\n",
      "Epoch: 115 | Batch_idx: 40 |  Loss_1: (0.4755) | Acc_1: (82.30%) (4319/5248)\n",
      "Epoch: 115 | Batch_idx: 50 |  Loss_1: (0.4712) | Acc_1: (82.48%) (5384/6528)\n",
      "Epoch: 115 | Batch_idx: 60 |  Loss_1: (0.4696) | Acc_1: (82.53%) (6444/7808)\n",
      "Epoch: 115 | Batch_idx: 70 |  Loss_1: (0.4690) | Acc_1: (82.59%) (7506/9088)\n",
      "Epoch: 115 | Batch_idx: 80 |  Loss_1: (0.4655) | Acc_1: (82.71%) (8575/10368)\n",
      "Epoch: 115 | Batch_idx: 90 |  Loss_1: (0.4702) | Acc_1: (82.59%) (9620/11648)\n",
      "Epoch: 115 | Batch_idx: 100 |  Loss_1: (0.4655) | Acc_1: (82.78%) (10702/12928)\n",
      "Epoch: 115 | Batch_idx: 110 |  Loss_1: (0.4703) | Acc_1: (82.63%) (11740/14208)\n",
      "Epoch: 115 | Batch_idx: 120 |  Loss_1: (0.4708) | Acc_1: (82.62%) (12796/15488)\n",
      "Epoch: 115 | Batch_idx: 130 |  Loss_1: (0.4708) | Acc_1: (82.59%) (13849/16768)\n",
      "Epoch: 115 | Batch_idx: 140 |  Loss_1: (0.4712) | Acc_1: (82.54%) (14896/18048)\n",
      "Epoch: 115 | Batch_idx: 150 |  Loss_1: (0.4750) | Acc_1: (82.39%) (15925/19328)\n",
      "Epoch: 115 | Batch_idx: 160 |  Loss_1: (0.4748) | Acc_1: (82.40%) (16981/20608)\n",
      "Epoch: 115 | Batch_idx: 170 |  Loss_1: (0.4718) | Acc_1: (82.50%) (18058/21888)\n",
      "Epoch: 115 | Batch_idx: 180 |  Loss_1: (0.4757) | Acc_1: (82.36%) (19081/23168)\n",
      "Epoch: 115 | Batch_idx: 190 |  Loss_1: (0.4757) | Acc_1: (82.37%) (20137/24448)\n",
      "Epoch: 115 | Batch_idx: 200 |  Loss_1: (0.4774) | Acc_1: (82.29%) (21172/25728)\n",
      "Epoch: 115 | Batch_idx: 210 |  Loss_1: (0.4772) | Acc_1: (82.31%) (22230/27008)\n",
      "Epoch: 115 | Batch_idx: 220 |  Loss_1: (0.4778) | Acc_1: (82.29%) (23278/28288)\n",
      "Epoch: 115 | Batch_idx: 230 |  Loss_1: (0.4779) | Acc_1: (82.27%) (24326/29568)\n",
      "Epoch: 115 | Batch_idx: 240 |  Loss_1: (0.4770) | Acc_1: (82.33%) (25396/30848)\n",
      "Epoch: 115 | Batch_idx: 250 |  Loss_1: (0.4773) | Acc_1: (82.29%) (26438/32128)\n",
      "Epoch: 115 | Batch_idx: 260 |  Loss_1: (0.4776) | Acc_1: (82.28%) (27489/33408)\n",
      "Epoch: 115 | Batch_idx: 270 |  Loss_1: (0.4777) | Acc_1: (82.27%) (28538/34688)\n",
      "Epoch: 115 | Batch_idx: 280 |  Loss_1: (0.4784) | Acc_1: (82.24%) (29581/35968)\n",
      "Epoch: 115 | Batch_idx: 290 |  Loss_1: (0.4790) | Acc_1: (82.21%) (30621/37248)\n",
      "Epoch: 115 | Batch_idx: 300 |  Loss_1: (0.4791) | Acc_1: (82.20%) (31670/38528)\n",
      "Epoch: 115 | Batch_idx: 310 |  Loss_1: (0.4795) | Acc_1: (82.16%) (32707/39808)\n",
      "Epoch: 115 | Batch_idx: 320 |  Loss_1: (0.4795) | Acc_1: (82.18%) (33767/41088)\n",
      "Epoch: 115 | Batch_idx: 330 |  Loss_1: (0.4806) | Acc_1: (82.12%) (34793/42368)\n",
      "Epoch: 115 | Batch_idx: 340 |  Loss_1: (0.4804) | Acc_1: (82.12%) (35845/43648)\n",
      "Epoch: 115 | Batch_idx: 350 |  Loss_1: (0.4801) | Acc_1: (82.13%) (36900/44928)\n",
      "Epoch: 115 | Batch_idx: 360 |  Loss_1: (0.4798) | Acc_1: (82.14%) (37954/46208)\n",
      "Epoch: 115 | Batch_idx: 370 |  Loss_1: (0.4807) | Acc_1: (82.10%) (38988/47488)\n",
      "Epoch: 115 | Batch_idx: 380 |  Loss_1: (0.4805) | Acc_1: (82.12%) (40047/48768)\n",
      "Epoch: 115 | Batch_idx: 390 |  Loss_1: (0.4806) | Acc_1: (82.13%) (41063/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4661) | Acc: (89.65%) (8965/10000)\n",
      "Epoch: 116 | Batch_idx: 0 |  Loss_1: (0.4160) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 116 | Batch_idx: 10 |  Loss_1: (0.4716) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 116 | Batch_idx: 20 |  Loss_1: (0.4756) | Acc_1: (82.29%) (2212/2688)\n",
      "Epoch: 116 | Batch_idx: 30 |  Loss_1: (0.4573) | Acc_1: (82.96%) (3292/3968)\n",
      "Epoch: 116 | Batch_idx: 40 |  Loss_1: (0.4593) | Acc_1: (82.85%) (4348/5248)\n",
      "Epoch: 116 | Batch_idx: 50 |  Loss_1: (0.4638) | Acc_1: (82.75%) (5402/6528)\n",
      "Epoch: 116 | Batch_idx: 60 |  Loss_1: (0.4670) | Acc_1: (82.66%) (6454/7808)\n",
      "Epoch: 116 | Batch_idx: 70 |  Loss_1: (0.4666) | Acc_1: (82.63%) (7509/9088)\n",
      "Epoch: 116 | Batch_idx: 80 |  Loss_1: (0.4663) | Acc_1: (82.67%) (8571/10368)\n",
      "Epoch: 116 | Batch_idx: 90 |  Loss_1: (0.4698) | Acc_1: (82.54%) (9614/11648)\n",
      "Epoch: 116 | Batch_idx: 100 |  Loss_1: (0.4697) | Acc_1: (82.54%) (10671/12928)\n",
      "Epoch: 116 | Batch_idx: 110 |  Loss_1: (0.4706) | Acc_1: (82.52%) (11724/14208)\n",
      "Epoch: 116 | Batch_idx: 120 |  Loss_1: (0.4733) | Acc_1: (82.39%) (12761/15488)\n",
      "Epoch: 116 | Batch_idx: 130 |  Loss_1: (0.4761) | Acc_1: (82.29%) (13799/16768)\n",
      "Epoch: 116 | Batch_idx: 140 |  Loss_1: (0.4746) | Acc_1: (82.37%) (14867/18048)\n",
      "Epoch: 116 | Batch_idx: 150 |  Loss_1: (0.4736) | Acc_1: (82.43%) (15933/19328)\n",
      "Epoch: 116 | Batch_idx: 160 |  Loss_1: (0.4759) | Acc_1: (82.36%) (16973/20608)\n",
      "Epoch: 116 | Batch_idx: 170 |  Loss_1: (0.4755) | Acc_1: (82.39%) (18033/21888)\n",
      "Epoch: 116 | Batch_idx: 180 |  Loss_1: (0.4771) | Acc_1: (82.34%) (19076/23168)\n",
      "Epoch: 116 | Batch_idx: 190 |  Loss_1: (0.4760) | Acc_1: (82.35%) (20132/24448)\n",
      "Epoch: 116 | Batch_idx: 200 |  Loss_1: (0.4758) | Acc_1: (82.33%) (21182/25728)\n",
      "Epoch: 116 | Batch_idx: 210 |  Loss_1: (0.4752) | Acc_1: (82.35%) (22240/27008)\n",
      "Epoch: 116 | Batch_idx: 220 |  Loss_1: (0.4744) | Acc_1: (82.36%) (23299/28288)\n",
      "Epoch: 116 | Batch_idx: 230 |  Loss_1: (0.4756) | Acc_1: (82.30%) (24335/29568)\n",
      "Epoch: 116 | Batch_idx: 240 |  Loss_1: (0.4766) | Acc_1: (82.25%) (25374/30848)\n",
      "Epoch: 116 | Batch_idx: 250 |  Loss_1: (0.4770) | Acc_1: (82.24%) (26423/32128)\n",
      "Epoch: 116 | Batch_idx: 260 |  Loss_1: (0.4751) | Acc_1: (82.32%) (27500/33408)\n",
      "Epoch: 116 | Batch_idx: 270 |  Loss_1: (0.4748) | Acc_1: (82.33%) (28557/34688)\n",
      "Epoch: 116 | Batch_idx: 280 |  Loss_1: (0.4767) | Acc_1: (82.27%) (29591/35968)\n",
      "Epoch: 116 | Batch_idx: 290 |  Loss_1: (0.4768) | Acc_1: (82.26%) (30640/37248)\n",
      "Epoch: 116 | Batch_idx: 300 |  Loss_1: (0.4772) | Acc_1: (82.23%) (31683/38528)\n",
      "Epoch: 116 | Batch_idx: 310 |  Loss_1: (0.4786) | Acc_1: (82.19%) (32717/39808)\n",
      "Epoch: 116 | Batch_idx: 320 |  Loss_1: (0.4796) | Acc_1: (82.16%) (33756/41088)\n",
      "Epoch: 116 | Batch_idx: 330 |  Loss_1: (0.4792) | Acc_1: (82.18%) (34820/42368)\n",
      "Epoch: 116 | Batch_idx: 340 |  Loss_1: (0.4779) | Acc_1: (82.24%) (35895/43648)\n",
      "Epoch: 116 | Batch_idx: 350 |  Loss_1: (0.4784) | Acc_1: (82.22%) (36939/44928)\n",
      "Epoch: 116 | Batch_idx: 360 |  Loss_1: (0.4777) | Acc_1: (82.25%) (38004/46208)\n",
      "Epoch: 116 | Batch_idx: 370 |  Loss_1: (0.4777) | Acc_1: (82.23%) (39050/47488)\n",
      "Epoch: 116 | Batch_idx: 380 |  Loss_1: (0.4791) | Acc_1: (82.17%) (40073/48768)\n",
      "Epoch: 116 | Batch_idx: 390 |  Loss_1: (0.4786) | Acc_1: (82.18%) (41088/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4771) | Acc: (90.10%) (9010/10000)\n",
      "Epoch: 117 | Batch_idx: 0 |  Loss_1: (0.5523) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 117 | Batch_idx: 10 |  Loss_1: (0.4986) | Acc_1: (81.32%) (1145/1408)\n",
      "Epoch: 117 | Batch_idx: 20 |  Loss_1: (0.5092) | Acc_1: (80.92%) (2175/2688)\n",
      "Epoch: 117 | Batch_idx: 30 |  Loss_1: (0.4944) | Acc_1: (81.50%) (3234/3968)\n",
      "Epoch: 117 | Batch_idx: 40 |  Loss_1: (0.4811) | Acc_1: (82.01%) (4304/5248)\n",
      "Epoch: 117 | Batch_idx: 50 |  Loss_1: (0.4750) | Acc_1: (82.25%) (5369/6528)\n",
      "Epoch: 117 | Batch_idx: 60 |  Loss_1: (0.4739) | Acc_1: (82.34%) (6429/7808)\n",
      "Epoch: 117 | Batch_idx: 70 |  Loss_1: (0.4724) | Acc_1: (82.41%) (7489/9088)\n",
      "Epoch: 117 | Batch_idx: 80 |  Loss_1: (0.4739) | Acc_1: (82.29%) (8532/10368)\n",
      "Epoch: 117 | Batch_idx: 90 |  Loss_1: (0.4764) | Acc_1: (82.19%) (9574/11648)\n",
      "Epoch: 117 | Batch_idx: 100 |  Loss_1: (0.4741) | Acc_1: (82.28%) (10637/12928)\n",
      "Epoch: 117 | Batch_idx: 110 |  Loss_1: (0.4747) | Acc_1: (82.27%) (11689/14208)\n",
      "Epoch: 117 | Batch_idx: 120 |  Loss_1: (0.4797) | Acc_1: (82.12%) (12719/15488)\n",
      "Epoch: 117 | Batch_idx: 130 |  Loss_1: (0.4782) | Acc_1: (82.19%) (13781/16768)\n",
      "Epoch: 117 | Batch_idx: 140 |  Loss_1: (0.4782) | Acc_1: (82.18%) (14832/18048)\n",
      "Epoch: 117 | Batch_idx: 150 |  Loss_1: (0.4761) | Acc_1: (82.26%) (15900/19328)\n",
      "Epoch: 117 | Batch_idx: 160 |  Loss_1: (0.4758) | Acc_1: (82.28%) (16956/20608)\n",
      "Epoch: 117 | Batch_idx: 170 |  Loss_1: (0.4769) | Acc_1: (82.24%) (18000/21888)\n",
      "Epoch: 117 | Batch_idx: 180 |  Loss_1: (0.4807) | Acc_1: (82.10%) (19022/23168)\n",
      "Epoch: 117 | Batch_idx: 190 |  Loss_1: (0.4805) | Acc_1: (82.13%) (20078/24448)\n",
      "Epoch: 117 | Batch_idx: 200 |  Loss_1: (0.4811) | Acc_1: (82.09%) (21121/25728)\n",
      "Epoch: 117 | Batch_idx: 210 |  Loss_1: (0.4817) | Acc_1: (82.09%) (22171/27008)\n",
      "Epoch: 117 | Batch_idx: 220 |  Loss_1: (0.4823) | Acc_1: (82.08%) (23220/28288)\n",
      "Epoch: 117 | Batch_idx: 230 |  Loss_1: (0.4828) | Acc_1: (82.07%) (24267/29568)\n",
      "Epoch: 117 | Batch_idx: 240 |  Loss_1: (0.4825) | Acc_1: (82.08%) (25319/30848)\n",
      "Epoch: 117 | Batch_idx: 250 |  Loss_1: (0.4809) | Acc_1: (82.12%) (26384/32128)\n",
      "Epoch: 117 | Batch_idx: 260 |  Loss_1: (0.4796) | Acc_1: (82.18%) (27454/33408)\n",
      "Epoch: 117 | Batch_idx: 270 |  Loss_1: (0.4807) | Acc_1: (82.15%) (28497/34688)\n",
      "Epoch: 117 | Batch_idx: 280 |  Loss_1: (0.4830) | Acc_1: (82.05%) (29512/35968)\n",
      "Epoch: 117 | Batch_idx: 290 |  Loss_1: (0.4842) | Acc_1: (81.99%) (30540/37248)\n",
      "Epoch: 117 | Batch_idx: 300 |  Loss_1: (0.4837) | Acc_1: (81.99%) (31589/38528)\n",
      "Epoch: 117 | Batch_idx: 310 |  Loss_1: (0.4836) | Acc_1: (81.99%) (32637/39808)\n",
      "Epoch: 117 | Batch_idx: 320 |  Loss_1: (0.4832) | Acc_1: (82.01%) (33698/41088)\n",
      "Epoch: 117 | Batch_idx: 330 |  Loss_1: (0.4834) | Acc_1: (82.00%) (34741/42368)\n",
      "Epoch: 117 | Batch_idx: 340 |  Loss_1: (0.4835) | Acc_1: (82.00%) (35793/43648)\n",
      "Epoch: 117 | Batch_idx: 350 |  Loss_1: (0.4830) | Acc_1: (82.02%) (36852/44928)\n",
      "Epoch: 117 | Batch_idx: 360 |  Loss_1: (0.4832) | Acc_1: (82.02%) (37899/46208)\n",
      "Epoch: 117 | Batch_idx: 370 |  Loss_1: (0.4833) | Acc_1: (82.01%) (38944/47488)\n",
      "Epoch: 117 | Batch_idx: 380 |  Loss_1: (0.4846) | Acc_1: (81.96%) (39971/48768)\n",
      "Epoch: 117 | Batch_idx: 390 |  Loss_1: (0.4839) | Acc_1: (81.98%) (40989/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4693) | Acc: (89.73%) (8973/10000)\n",
      "Epoch: 118 | Batch_idx: 0 |  Loss_1: (0.4279) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 118 | Batch_idx: 10 |  Loss_1: (0.4856) | Acc_1: (81.18%) (1143/1408)\n",
      "Epoch: 118 | Batch_idx: 20 |  Loss_1: (0.4618) | Acc_1: (82.44%) (2216/2688)\n",
      "Epoch: 118 | Batch_idx: 30 |  Loss_1: (0.4557) | Acc_1: (82.81%) (3286/3968)\n",
      "Epoch: 118 | Batch_idx: 40 |  Loss_1: (0.4635) | Acc_1: (82.58%) (4334/5248)\n",
      "Epoch: 118 | Batch_idx: 50 |  Loss_1: (0.4597) | Acc_1: (82.75%) (5402/6528)\n",
      "Epoch: 118 | Batch_idx: 60 |  Loss_1: (0.4674) | Acc_1: (82.51%) (6442/7808)\n",
      "Epoch: 118 | Batch_idx: 70 |  Loss_1: (0.4664) | Acc_1: (82.56%) (7503/9088)\n",
      "Epoch: 118 | Batch_idx: 80 |  Loss_1: (0.4731) | Acc_1: (82.25%) (8528/10368)\n",
      "Epoch: 118 | Batch_idx: 90 |  Loss_1: (0.4729) | Acc_1: (82.24%) (9579/11648)\n",
      "Epoch: 118 | Batch_idx: 100 |  Loss_1: (0.4757) | Acc_1: (82.12%) (10617/12928)\n",
      "Epoch: 118 | Batch_idx: 110 |  Loss_1: (0.4766) | Acc_1: (82.09%) (11664/14208)\n",
      "Epoch: 118 | Batch_idx: 120 |  Loss_1: (0.4769) | Acc_1: (82.14%) (12722/15488)\n",
      "Epoch: 118 | Batch_idx: 130 |  Loss_1: (0.4763) | Acc_1: (82.17%) (13779/16768)\n",
      "Epoch: 118 | Batch_idx: 140 |  Loss_1: (0.4786) | Acc_1: (82.10%) (14818/18048)\n",
      "Epoch: 118 | Batch_idx: 150 |  Loss_1: (0.4803) | Acc_1: (82.03%) (15855/19328)\n",
      "Epoch: 118 | Batch_idx: 160 |  Loss_1: (0.4802) | Acc_1: (82.05%) (16909/20608)\n",
      "Epoch: 118 | Batch_idx: 170 |  Loss_1: (0.4805) | Acc_1: (82.04%) (17956/21888)\n",
      "Epoch: 118 | Batch_idx: 180 |  Loss_1: (0.4818) | Acc_1: (81.96%) (18988/23168)\n",
      "Epoch: 118 | Batch_idx: 190 |  Loss_1: (0.4818) | Acc_1: (81.97%) (20041/24448)\n",
      "Epoch: 118 | Batch_idx: 200 |  Loss_1: (0.4815) | Acc_1: (82.00%) (21096/25728)\n",
      "Epoch: 118 | Batch_idx: 210 |  Loss_1: (0.4841) | Acc_1: (81.89%) (22117/27008)\n",
      "Epoch: 118 | Batch_idx: 220 |  Loss_1: (0.4816) | Acc_1: (81.98%) (23191/28288)\n",
      "Epoch: 118 | Batch_idx: 230 |  Loss_1: (0.4815) | Acc_1: (82.00%) (24247/29568)\n",
      "Epoch: 118 | Batch_idx: 240 |  Loss_1: (0.4833) | Acc_1: (81.96%) (25282/30848)\n",
      "Epoch: 118 | Batch_idx: 250 |  Loss_1: (0.4816) | Acc_1: (82.04%) (26357/32128)\n",
      "Epoch: 118 | Batch_idx: 260 |  Loss_1: (0.4822) | Acc_1: (82.02%) (27402/33408)\n",
      "Epoch: 118 | Batch_idx: 270 |  Loss_1: (0.4825) | Acc_1: (82.03%) (28454/34688)\n",
      "Epoch: 118 | Batch_idx: 280 |  Loss_1: (0.4832) | Acc_1: (81.99%) (29491/35968)\n",
      "Epoch: 118 | Batch_idx: 290 |  Loss_1: (0.4836) | Acc_1: (81.97%) (30534/37248)\n",
      "Epoch: 118 | Batch_idx: 300 |  Loss_1: (0.4843) | Acc_1: (81.95%) (31574/38528)\n",
      "Epoch: 118 | Batch_idx: 310 |  Loss_1: (0.4836) | Acc_1: (81.98%) (32636/39808)\n",
      "Epoch: 118 | Batch_idx: 320 |  Loss_1: (0.4832) | Acc_1: (81.98%) (33685/41088)\n",
      "Epoch: 118 | Batch_idx: 330 |  Loss_1: (0.4840) | Acc_1: (81.95%) (34722/42368)\n",
      "Epoch: 118 | Batch_idx: 340 |  Loss_1: (0.4816) | Acc_1: (82.05%) (35812/43648)\n",
      "Epoch: 118 | Batch_idx: 350 |  Loss_1: (0.4813) | Acc_1: (82.07%) (36871/44928)\n",
      "Epoch: 118 | Batch_idx: 360 |  Loss_1: (0.4793) | Acc_1: (82.13%) (37952/46208)\n",
      "Epoch: 118 | Batch_idx: 370 |  Loss_1: (0.4795) | Acc_1: (82.13%) (39000/47488)\n",
      "Epoch: 118 | Batch_idx: 380 |  Loss_1: (0.4784) | Acc_1: (82.16%) (40070/48768)\n",
      "Epoch: 118 | Batch_idx: 390 |  Loss_1: (0.4775) | Acc_1: (82.19%) (41093/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4870) | Acc: (89.71%) (8971/10000)\n",
      "Epoch: 119 | Batch_idx: 0 |  Loss_1: (0.4132) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 119 | Batch_idx: 10 |  Loss_1: (0.4535) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 119 | Batch_idx: 20 |  Loss_1: (0.4501) | Acc_1: (83.41%) (2242/2688)\n",
      "Epoch: 119 | Batch_idx: 30 |  Loss_1: (0.4687) | Acc_1: (82.66%) (3280/3968)\n",
      "Epoch: 119 | Batch_idx: 40 |  Loss_1: (0.4758) | Acc_1: (82.26%) (4317/5248)\n",
      "Epoch: 119 | Batch_idx: 50 |  Loss_1: (0.4744) | Acc_1: (82.40%) (5379/6528)\n",
      "Epoch: 119 | Batch_idx: 60 |  Loss_1: (0.4786) | Acc_1: (82.22%) (6420/7808)\n",
      "Epoch: 119 | Batch_idx: 70 |  Loss_1: (0.4809) | Acc_1: (82.09%) (7460/9088)\n",
      "Epoch: 119 | Batch_idx: 80 |  Loss_1: (0.4862) | Acc_1: (81.83%) (8484/10368)\n",
      "Epoch: 119 | Batch_idx: 90 |  Loss_1: (0.4890) | Acc_1: (81.75%) (9522/11648)\n",
      "Epoch: 119 | Batch_idx: 100 |  Loss_1: (0.4911) | Acc_1: (81.64%) (10555/12928)\n",
      "Epoch: 119 | Batch_idx: 110 |  Loss_1: (0.4894) | Acc_1: (81.73%) (11612/14208)\n",
      "Epoch: 119 | Batch_idx: 120 |  Loss_1: (0.4875) | Acc_1: (81.81%) (12670/15488)\n",
      "Epoch: 119 | Batch_idx: 130 |  Loss_1: (0.4847) | Acc_1: (81.92%) (13737/16768)\n",
      "Epoch: 119 | Batch_idx: 140 |  Loss_1: (0.4851) | Acc_1: (81.90%) (14782/18048)\n",
      "Epoch: 119 | Batch_idx: 150 |  Loss_1: (0.4852) | Acc_1: (81.92%) (15833/19328)\n",
      "Epoch: 119 | Batch_idx: 160 |  Loss_1: (0.4842) | Acc_1: (81.95%) (16889/20608)\n",
      "Epoch: 119 | Batch_idx: 170 |  Loss_1: (0.4837) | Acc_1: (81.99%) (17947/21888)\n",
      "Epoch: 119 | Batch_idx: 180 |  Loss_1: (0.4817) | Acc_1: (82.07%) (19013/23168)\n",
      "Epoch: 119 | Batch_idx: 190 |  Loss_1: (0.4829) | Acc_1: (81.99%) (20046/24448)\n",
      "Epoch: 119 | Batch_idx: 200 |  Loss_1: (0.4827) | Acc_1: (82.00%) (21096/25728)\n",
      "Epoch: 119 | Batch_idx: 210 |  Loss_1: (0.4856) | Acc_1: (81.89%) (22117/27008)\n",
      "Epoch: 119 | Batch_idx: 220 |  Loss_1: (0.4829) | Acc_1: (82.01%) (23199/28288)\n",
      "Epoch: 119 | Batch_idx: 230 |  Loss_1: (0.4822) | Acc_1: (82.00%) (24247/29568)\n",
      "Epoch: 119 | Batch_idx: 240 |  Loss_1: (0.4820) | Acc_1: (82.01%) (25299/30848)\n",
      "Epoch: 119 | Batch_idx: 250 |  Loss_1: (0.4807) | Acc_1: (82.07%) (26369/32128)\n",
      "Epoch: 119 | Batch_idx: 260 |  Loss_1: (0.4806) | Acc_1: (82.09%) (27424/33408)\n",
      "Epoch: 119 | Batch_idx: 270 |  Loss_1: (0.4795) | Acc_1: (82.12%) (28486/34688)\n",
      "Epoch: 119 | Batch_idx: 280 |  Loss_1: (0.4794) | Acc_1: (82.13%) (29540/35968)\n",
      "Epoch: 119 | Batch_idx: 290 |  Loss_1: (0.4786) | Acc_1: (82.16%) (30602/37248)\n",
      "Epoch: 119 | Batch_idx: 300 |  Loss_1: (0.4781) | Acc_1: (82.17%) (31660/38528)\n",
      "Epoch: 119 | Batch_idx: 310 |  Loss_1: (0.4791) | Acc_1: (82.15%) (32702/39808)\n",
      "Epoch: 119 | Batch_idx: 320 |  Loss_1: (0.4794) | Acc_1: (82.14%) (33748/41088)\n",
      "Epoch: 119 | Batch_idx: 330 |  Loss_1: (0.4787) | Acc_1: (82.16%) (34809/42368)\n",
      "Epoch: 119 | Batch_idx: 340 |  Loss_1: (0.4787) | Acc_1: (82.17%) (35864/43648)\n",
      "Epoch: 119 | Batch_idx: 350 |  Loss_1: (0.4788) | Acc_1: (82.17%) (36916/44928)\n",
      "Epoch: 119 | Batch_idx: 360 |  Loss_1: (0.4802) | Acc_1: (82.10%) (37939/46208)\n",
      "Epoch: 119 | Batch_idx: 370 |  Loss_1: (0.4806) | Acc_1: (82.09%) (38981/47488)\n",
      "Epoch: 119 | Batch_idx: 380 |  Loss_1: (0.4820) | Acc_1: (82.04%) (40008/48768)\n",
      "Epoch: 119 | Batch_idx: 390 |  Loss_1: (0.4820) | Acc_1: (82.02%) (41011/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4361) | Acc: (90.35%) (9035/10000)\n",
      "Epoch: 120 | Batch_idx: 0 |  Loss_1: (0.4053) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 120 | Batch_idx: 10 |  Loss_1: (0.5080) | Acc_1: (81.18%) (1143/1408)\n",
      "Epoch: 120 | Batch_idx: 20 |  Loss_1: (0.5019) | Acc_1: (81.40%) (2188/2688)\n",
      "Epoch: 120 | Batch_idx: 30 |  Loss_1: (0.4981) | Acc_1: (81.55%) (3236/3968)\n",
      "Epoch: 120 | Batch_idx: 40 |  Loss_1: (0.4893) | Acc_1: (81.80%) (4293/5248)\n",
      "Epoch: 120 | Batch_idx: 50 |  Loss_1: (0.4849) | Acc_1: (81.97%) (5351/6528)\n",
      "Epoch: 120 | Batch_idx: 60 |  Loss_1: (0.4889) | Acc_1: (81.76%) (6384/7808)\n",
      "Epoch: 120 | Batch_idx: 70 |  Loss_1: (0.4864) | Acc_1: (81.87%) (7440/9088)\n",
      "Epoch: 120 | Batch_idx: 80 |  Loss_1: (0.4853) | Acc_1: (81.91%) (8492/10368)\n",
      "Epoch: 120 | Batch_idx: 90 |  Loss_1: (0.4813) | Acc_1: (82.07%) (9560/11648)\n",
      "Epoch: 120 | Batch_idx: 100 |  Loss_1: (0.4811) | Acc_1: (82.09%) (10613/12928)\n",
      "Epoch: 120 | Batch_idx: 110 |  Loss_1: (0.4813) | Acc_1: (82.05%) (11658/14208)\n",
      "Epoch: 120 | Batch_idx: 120 |  Loss_1: (0.4817) | Acc_1: (82.04%) (12706/15488)\n",
      "Epoch: 120 | Batch_idx: 130 |  Loss_1: (0.4818) | Acc_1: (81.99%) (13748/16768)\n",
      "Epoch: 120 | Batch_idx: 140 |  Loss_1: (0.4816) | Acc_1: (82.00%) (14799/18048)\n",
      "Epoch: 120 | Batch_idx: 150 |  Loss_1: (0.4821) | Acc_1: (81.93%) (15836/19328)\n",
      "Epoch: 120 | Batch_idx: 160 |  Loss_1: (0.4788) | Acc_1: (82.06%) (16910/20608)\n",
      "Epoch: 120 | Batch_idx: 170 |  Loss_1: (0.4797) | Acc_1: (82.00%) (17948/21888)\n",
      "Epoch: 120 | Batch_idx: 180 |  Loss_1: (0.4799) | Acc_1: (81.95%) (18987/23168)\n",
      "Epoch: 120 | Batch_idx: 190 |  Loss_1: (0.4792) | Acc_1: (81.97%) (20040/24448)\n",
      "Epoch: 120 | Batch_idx: 200 |  Loss_1: (0.4799) | Acc_1: (81.97%) (21089/25728)\n",
      "Epoch: 120 | Batch_idx: 210 |  Loss_1: (0.4812) | Acc_1: (81.90%) (22119/27008)\n",
      "Epoch: 120 | Batch_idx: 220 |  Loss_1: (0.4805) | Acc_1: (81.94%) (23179/28288)\n",
      "Epoch: 120 | Batch_idx: 230 |  Loss_1: (0.4827) | Acc_1: (81.88%) (24211/29568)\n",
      "Epoch: 120 | Batch_idx: 240 |  Loss_1: (0.4829) | Acc_1: (81.87%) (25256/30848)\n",
      "Epoch: 120 | Batch_idx: 250 |  Loss_1: (0.4845) | Acc_1: (81.82%) (26288/32128)\n",
      "Epoch: 120 | Batch_idx: 260 |  Loss_1: (0.4840) | Acc_1: (81.83%) (27339/33408)\n",
      "Epoch: 120 | Batch_idx: 270 |  Loss_1: (0.4827) | Acc_1: (81.89%) (28407/34688)\n",
      "Epoch: 120 | Batch_idx: 280 |  Loss_1: (0.4815) | Acc_1: (81.92%) (29465/35968)\n",
      "Epoch: 120 | Batch_idx: 290 |  Loss_1: (0.4823) | Acc_1: (81.91%) (30511/37248)\n",
      "Epoch: 120 | Batch_idx: 300 |  Loss_1: (0.4828) | Acc_1: (81.91%) (31559/38528)\n",
      "Epoch: 120 | Batch_idx: 310 |  Loss_1: (0.4820) | Acc_1: (81.95%) (32621/39808)\n",
      "Epoch: 120 | Batch_idx: 320 |  Loss_1: (0.4835) | Acc_1: (81.89%) (33647/41088)\n",
      "Epoch: 120 | Batch_idx: 330 |  Loss_1: (0.4846) | Acc_1: (81.87%) (34686/42368)\n",
      "Epoch: 120 | Batch_idx: 340 |  Loss_1: (0.4852) | Acc_1: (81.83%) (35719/43648)\n",
      "Epoch: 120 | Batch_idx: 350 |  Loss_1: (0.4846) | Acc_1: (81.86%) (36777/44928)\n",
      "Epoch: 120 | Batch_idx: 360 |  Loss_1: (0.4846) | Acc_1: (81.86%) (37828/46208)\n",
      "Epoch: 120 | Batch_idx: 370 |  Loss_1: (0.4852) | Acc_1: (81.85%) (38867/47488)\n",
      "Epoch: 120 | Batch_idx: 380 |  Loss_1: (0.4852) | Acc_1: (81.85%) (39918/48768)\n",
      "Epoch: 120 | Batch_idx: 390 |  Loss_1: (0.4852) | Acc_1: (81.85%) (40925/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4774) | Acc: (89.93%) (8993/10000)\n",
      "Epoch: 121 | Batch_idx: 0 |  Loss_1: (0.5570) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 121 | Batch_idx: 10 |  Loss_1: (0.5065) | Acc_1: (81.18%) (1143/1408)\n",
      "Epoch: 121 | Batch_idx: 20 |  Loss_1: (0.4774) | Acc_1: (82.25%) (2211/2688)\n",
      "Epoch: 121 | Batch_idx: 30 |  Loss_1: (0.4835) | Acc_1: (82.08%) (3257/3968)\n",
      "Epoch: 121 | Batch_idx: 40 |  Loss_1: (0.4758) | Acc_1: (82.32%) (4320/5248)\n",
      "Epoch: 121 | Batch_idx: 50 |  Loss_1: (0.4751) | Acc_1: (82.37%) (5377/6528)\n",
      "Epoch: 121 | Batch_idx: 60 |  Loss_1: (0.4757) | Acc_1: (82.35%) (6430/7808)\n",
      "Epoch: 121 | Batch_idx: 70 |  Loss_1: (0.4841) | Acc_1: (82.01%) (7453/9088)\n",
      "Epoch: 121 | Batch_idx: 80 |  Loss_1: (0.4820) | Acc_1: (82.07%) (8509/10368)\n",
      "Epoch: 121 | Batch_idx: 90 |  Loss_1: (0.4911) | Acc_1: (81.70%) (9516/11648)\n",
      "Epoch: 121 | Batch_idx: 100 |  Loss_1: (0.4953) | Acc_1: (81.54%) (10542/12928)\n",
      "Epoch: 121 | Batch_idx: 110 |  Loss_1: (0.4935) | Acc_1: (81.62%) (11596/14208)\n",
      "Epoch: 121 | Batch_idx: 120 |  Loss_1: (0.4921) | Acc_1: (81.68%) (12651/15488)\n",
      "Epoch: 121 | Batch_idx: 130 |  Loss_1: (0.4913) | Acc_1: (81.73%) (13705/16768)\n",
      "Epoch: 121 | Batch_idx: 140 |  Loss_1: (0.4896) | Acc_1: (81.81%) (14765/18048)\n",
      "Epoch: 121 | Batch_idx: 150 |  Loss_1: (0.4870) | Acc_1: (81.86%) (15822/19328)\n",
      "Epoch: 121 | Batch_idx: 160 |  Loss_1: (0.4857) | Acc_1: (81.91%) (16880/20608)\n",
      "Epoch: 121 | Batch_idx: 170 |  Loss_1: (0.4851) | Acc_1: (81.91%) (17929/21888)\n",
      "Epoch: 121 | Batch_idx: 180 |  Loss_1: (0.4862) | Acc_1: (81.85%) (18963/23168)\n",
      "Epoch: 121 | Batch_idx: 190 |  Loss_1: (0.4858) | Acc_1: (81.88%) (20018/24448)\n",
      "Epoch: 121 | Batch_idx: 200 |  Loss_1: (0.4846) | Acc_1: (81.91%) (21075/25728)\n",
      "Epoch: 121 | Batch_idx: 210 |  Loss_1: (0.4869) | Acc_1: (81.82%) (22099/27008)\n",
      "Epoch: 121 | Batch_idx: 220 |  Loss_1: (0.4855) | Acc_1: (81.87%) (23158/28288)\n",
      "Epoch: 121 | Batch_idx: 230 |  Loss_1: (0.4865) | Acc_1: (81.82%) (24192/29568)\n",
      "Epoch: 121 | Batch_idx: 240 |  Loss_1: (0.4859) | Acc_1: (81.87%) (25256/30848)\n",
      "Epoch: 121 | Batch_idx: 250 |  Loss_1: (0.4846) | Acc_1: (81.91%) (26317/32128)\n",
      "Epoch: 121 | Batch_idx: 260 |  Loss_1: (0.4840) | Acc_1: (81.94%) (27374/33408)\n",
      "Epoch: 121 | Batch_idx: 270 |  Loss_1: (0.4823) | Acc_1: (82.00%) (28445/34688)\n",
      "Epoch: 121 | Batch_idx: 280 |  Loss_1: (0.4831) | Acc_1: (81.96%) (29480/35968)\n",
      "Epoch: 121 | Batch_idx: 290 |  Loss_1: (0.4839) | Acc_1: (81.95%) (30523/37248)\n",
      "Epoch: 121 | Batch_idx: 300 |  Loss_1: (0.4858) | Acc_1: (81.89%) (31549/38528)\n",
      "Epoch: 121 | Batch_idx: 310 |  Loss_1: (0.4854) | Acc_1: (81.90%) (32601/39808)\n",
      "Epoch: 121 | Batch_idx: 320 |  Loss_1: (0.4864) | Acc_1: (81.88%) (33643/41088)\n",
      "Epoch: 121 | Batch_idx: 330 |  Loss_1: (0.4859) | Acc_1: (81.90%) (34698/42368)\n",
      "Epoch: 121 | Batch_idx: 340 |  Loss_1: (0.4849) | Acc_1: (81.92%) (35757/43648)\n",
      "Epoch: 121 | Batch_idx: 350 |  Loss_1: (0.4852) | Acc_1: (81.92%) (36805/44928)\n",
      "Epoch: 121 | Batch_idx: 360 |  Loss_1: (0.4867) | Acc_1: (81.88%) (37835/46208)\n",
      "Epoch: 121 | Batch_idx: 370 |  Loss_1: (0.4857) | Acc_1: (81.91%) (38897/47488)\n",
      "Epoch: 121 | Batch_idx: 380 |  Loss_1: (0.4867) | Acc_1: (81.87%) (39924/48768)\n",
      "Epoch: 121 | Batch_idx: 390 |  Loss_1: (0.4870) | Acc_1: (81.87%) (40933/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4488) | Acc: (90.26%) (9026/10000)\n",
      "Epoch: 122 | Batch_idx: 0 |  Loss_1: (0.4051) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 122 | Batch_idx: 10 |  Loss_1: (0.4762) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 122 | Batch_idx: 20 |  Loss_1: (0.4862) | Acc_1: (81.96%) (2203/2688)\n",
      "Epoch: 122 | Batch_idx: 30 |  Loss_1: (0.4989) | Acc_1: (81.58%) (3237/3968)\n",
      "Epoch: 122 | Batch_idx: 40 |  Loss_1: (0.4927) | Acc_1: (81.84%) (4295/5248)\n",
      "Epoch: 122 | Batch_idx: 50 |  Loss_1: (0.4869) | Acc_1: (82.06%) (5357/6528)\n",
      "Epoch: 122 | Batch_idx: 60 |  Loss_1: (0.4879) | Acc_1: (82.10%) (6410/7808)\n",
      "Epoch: 122 | Batch_idx: 70 |  Loss_1: (0.4859) | Acc_1: (82.14%) (7465/9088)\n",
      "Epoch: 122 | Batch_idx: 80 |  Loss_1: (0.4862) | Acc_1: (82.13%) (8515/10368)\n",
      "Epoch: 122 | Batch_idx: 90 |  Loss_1: (0.4870) | Acc_1: (82.07%) (9559/11648)\n",
      "Epoch: 122 | Batch_idx: 100 |  Loss_1: (0.4853) | Acc_1: (82.06%) (10609/12928)\n",
      "Epoch: 122 | Batch_idx: 110 |  Loss_1: (0.4848) | Acc_1: (82.12%) (11667/14208)\n",
      "Epoch: 122 | Batch_idx: 120 |  Loss_1: (0.4860) | Acc_1: (82.06%) (12709/15488)\n",
      "Epoch: 122 | Batch_idx: 130 |  Loss_1: (0.4862) | Acc_1: (82.07%) (13761/16768)\n",
      "Epoch: 122 | Batch_idx: 140 |  Loss_1: (0.4858) | Acc_1: (82.08%) (14814/18048)\n",
      "Epoch: 122 | Batch_idx: 150 |  Loss_1: (0.4866) | Acc_1: (82.01%) (15850/19328)\n",
      "Epoch: 122 | Batch_idx: 160 |  Loss_1: (0.4869) | Acc_1: (82.00%) (16899/20608)\n",
      "Epoch: 122 | Batch_idx: 170 |  Loss_1: (0.4882) | Acc_1: (81.99%) (17945/21888)\n",
      "Epoch: 122 | Batch_idx: 180 |  Loss_1: (0.4862) | Acc_1: (82.06%) (19011/23168)\n",
      "Epoch: 122 | Batch_idx: 190 |  Loss_1: (0.4850) | Acc_1: (82.08%) (20067/24448)\n",
      "Epoch: 122 | Batch_idx: 200 |  Loss_1: (0.4856) | Acc_1: (82.06%) (21112/25728)\n",
      "Epoch: 122 | Batch_idx: 210 |  Loss_1: (0.4887) | Acc_1: (81.94%) (22131/27008)\n",
      "Epoch: 122 | Batch_idx: 220 |  Loss_1: (0.4886) | Acc_1: (81.93%) (23176/28288)\n",
      "Epoch: 122 | Batch_idx: 230 |  Loss_1: (0.4886) | Acc_1: (81.93%) (24226/29568)\n",
      "Epoch: 122 | Batch_idx: 240 |  Loss_1: (0.4887) | Acc_1: (81.93%) (25275/30848)\n",
      "Epoch: 122 | Batch_idx: 250 |  Loss_1: (0.4901) | Acc_1: (81.88%) (26305/32128)\n",
      "Epoch: 122 | Batch_idx: 260 |  Loss_1: (0.4907) | Acc_1: (81.85%) (27344/33408)\n",
      "Epoch: 122 | Batch_idx: 270 |  Loss_1: (0.4907) | Acc_1: (81.85%) (28391/34688)\n",
      "Epoch: 122 | Batch_idx: 280 |  Loss_1: (0.4889) | Acc_1: (81.91%) (29462/35968)\n",
      "Epoch: 122 | Batch_idx: 290 |  Loss_1: (0.4893) | Acc_1: (81.88%) (30499/37248)\n",
      "Epoch: 122 | Batch_idx: 300 |  Loss_1: (0.4868) | Acc_1: (81.96%) (31579/38528)\n",
      "Epoch: 122 | Batch_idx: 310 |  Loss_1: (0.4867) | Acc_1: (81.95%) (32622/39808)\n",
      "Epoch: 122 | Batch_idx: 320 |  Loss_1: (0.4871) | Acc_1: (81.92%) (33659/41088)\n",
      "Epoch: 122 | Batch_idx: 330 |  Loss_1: (0.4872) | Acc_1: (81.92%) (34709/42368)\n",
      "Epoch: 122 | Batch_idx: 340 |  Loss_1: (0.4873) | Acc_1: (81.91%) (35750/43648)\n",
      "Epoch: 122 | Batch_idx: 350 |  Loss_1: (0.4869) | Acc_1: (81.94%) (36816/44928)\n",
      "Epoch: 122 | Batch_idx: 360 |  Loss_1: (0.4852) | Acc_1: (81.99%) (37888/46208)\n",
      "Epoch: 122 | Batch_idx: 370 |  Loss_1: (0.4863) | Acc_1: (81.94%) (38911/47488)\n",
      "Epoch: 122 | Batch_idx: 380 |  Loss_1: (0.4857) | Acc_1: (81.96%) (39969/48768)\n",
      "Epoch: 122 | Batch_idx: 390 |  Loss_1: (0.4855) | Acc_1: (81.95%) (40976/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4851) | Acc: (90.11%) (9011/10000)\n",
      "Epoch: 123 | Batch_idx: 0 |  Loss_1: (0.6440) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 123 | Batch_idx: 10 |  Loss_1: (0.5048) | Acc_1: (80.40%) (1132/1408)\n",
      "Epoch: 123 | Batch_idx: 20 |  Loss_1: (0.4949) | Acc_1: (81.21%) (2183/2688)\n",
      "Epoch: 123 | Batch_idx: 30 |  Loss_1: (0.5063) | Acc_1: (80.92%) (3211/3968)\n",
      "Epoch: 123 | Batch_idx: 40 |  Loss_1: (0.5050) | Acc_1: (81.10%) (4256/5248)\n",
      "Epoch: 123 | Batch_idx: 50 |  Loss_1: (0.5016) | Acc_1: (81.33%) (5309/6528)\n",
      "Epoch: 123 | Batch_idx: 60 |  Loss_1: (0.4965) | Acc_1: (81.49%) (6363/7808)\n",
      "Epoch: 123 | Batch_idx: 70 |  Loss_1: (0.4906) | Acc_1: (81.68%) (7423/9088)\n",
      "Epoch: 123 | Batch_idx: 80 |  Loss_1: (0.4911) | Acc_1: (81.67%) (8468/10368)\n",
      "Epoch: 123 | Batch_idx: 90 |  Loss_1: (0.4853) | Acc_1: (81.95%) (9546/11648)\n",
      "Epoch: 123 | Batch_idx: 100 |  Loss_1: (0.4836) | Acc_1: (81.99%) (10600/12928)\n",
      "Epoch: 123 | Batch_idx: 110 |  Loss_1: (0.4802) | Acc_1: (82.10%) (11665/14208)\n",
      "Epoch: 123 | Batch_idx: 120 |  Loss_1: (0.4802) | Acc_1: (82.09%) (12714/15488)\n",
      "Epoch: 123 | Batch_idx: 130 |  Loss_1: (0.4778) | Acc_1: (82.19%) (13781/16768)\n",
      "Epoch: 123 | Batch_idx: 140 |  Loss_1: (0.4806) | Acc_1: (82.11%) (14819/18048)\n",
      "Epoch: 123 | Batch_idx: 150 |  Loss_1: (0.4787) | Acc_1: (82.21%) (15889/19328)\n",
      "Epoch: 123 | Batch_idx: 160 |  Loss_1: (0.4794) | Acc_1: (82.19%) (16938/20608)\n",
      "Epoch: 123 | Batch_idx: 170 |  Loss_1: (0.4785) | Acc_1: (82.23%) (17998/21888)\n",
      "Epoch: 123 | Batch_idx: 180 |  Loss_1: (0.4806) | Acc_1: (82.14%) (19030/23168)\n",
      "Epoch: 123 | Batch_idx: 190 |  Loss_1: (0.4823) | Acc_1: (82.07%) (20065/24448)\n",
      "Epoch: 123 | Batch_idx: 200 |  Loss_1: (0.4824) | Acc_1: (82.05%) (21111/25728)\n",
      "Epoch: 123 | Batch_idx: 210 |  Loss_1: (0.4834) | Acc_1: (82.02%) (22153/27008)\n",
      "Epoch: 123 | Batch_idx: 220 |  Loss_1: (0.4838) | Acc_1: (82.01%) (23199/28288)\n",
      "Epoch: 123 | Batch_idx: 230 |  Loss_1: (0.4827) | Acc_1: (82.06%) (24264/29568)\n",
      "Epoch: 123 | Batch_idx: 240 |  Loss_1: (0.4805) | Acc_1: (82.13%) (25335/30848)\n",
      "Epoch: 123 | Batch_idx: 250 |  Loss_1: (0.4832) | Acc_1: (82.02%) (26352/32128)\n",
      "Epoch: 123 | Batch_idx: 260 |  Loss_1: (0.4834) | Acc_1: (82.01%) (27398/33408)\n",
      "Epoch: 123 | Batch_idx: 270 |  Loss_1: (0.4833) | Acc_1: (82.01%) (28449/34688)\n",
      "Epoch: 123 | Batch_idx: 280 |  Loss_1: (0.4845) | Acc_1: (81.98%) (29486/35968)\n",
      "Epoch: 123 | Batch_idx: 290 |  Loss_1: (0.4852) | Acc_1: (81.96%) (30527/37248)\n",
      "Epoch: 123 | Batch_idx: 300 |  Loss_1: (0.4839) | Acc_1: (82.01%) (31596/38528)\n",
      "Epoch: 123 | Batch_idx: 310 |  Loss_1: (0.4851) | Acc_1: (81.98%) (32633/39808)\n",
      "Epoch: 123 | Batch_idx: 320 |  Loss_1: (0.4845) | Acc_1: (81.99%) (33687/41088)\n",
      "Epoch: 123 | Batch_idx: 330 |  Loss_1: (0.4851) | Acc_1: (81.97%) (34730/42368)\n",
      "Epoch: 123 | Batch_idx: 340 |  Loss_1: (0.4847) | Acc_1: (81.98%) (35782/43648)\n",
      "Epoch: 123 | Batch_idx: 350 |  Loss_1: (0.4851) | Acc_1: (81.96%) (36824/44928)\n",
      "Epoch: 123 | Batch_idx: 360 |  Loss_1: (0.4857) | Acc_1: (81.92%) (37855/46208)\n",
      "Epoch: 123 | Batch_idx: 370 |  Loss_1: (0.4855) | Acc_1: (81.92%) (38901/47488)\n",
      "Epoch: 123 | Batch_idx: 380 |  Loss_1: (0.4854) | Acc_1: (81.92%) (39952/48768)\n",
      "Epoch: 123 | Batch_idx: 390 |  Loss_1: (0.4869) | Acc_1: (81.89%) (40943/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4955) | Acc: (89.44%) (8944/10000)\n",
      "Epoch: 124 | Batch_idx: 0 |  Loss_1: (0.5425) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 124 | Batch_idx: 10 |  Loss_1: (0.4706) | Acc_1: (83.10%) (1170/1408)\n",
      "Epoch: 124 | Batch_idx: 20 |  Loss_1: (0.4791) | Acc_1: (82.70%) (2223/2688)\n",
      "Epoch: 124 | Batch_idx: 30 |  Loss_1: (0.4825) | Acc_1: (82.26%) (3264/3968)\n",
      "Epoch: 124 | Batch_idx: 40 |  Loss_1: (0.4789) | Acc_1: (82.37%) (4323/5248)\n",
      "Epoch: 124 | Batch_idx: 50 |  Loss_1: (0.4758) | Acc_1: (82.38%) (5378/6528)\n",
      "Epoch: 124 | Batch_idx: 60 |  Loss_1: (0.4840) | Acc_1: (82.07%) (6408/7808)\n",
      "Epoch: 124 | Batch_idx: 70 |  Loss_1: (0.4894) | Acc_1: (81.88%) (7441/9088)\n",
      "Epoch: 124 | Batch_idx: 80 |  Loss_1: (0.4886) | Acc_1: (81.96%) (8498/10368)\n",
      "Epoch: 124 | Batch_idx: 90 |  Loss_1: (0.4890) | Acc_1: (81.92%) (9542/11648)\n",
      "Epoch: 124 | Batch_idx: 100 |  Loss_1: (0.4944) | Acc_1: (81.72%) (10565/12928)\n",
      "Epoch: 124 | Batch_idx: 110 |  Loss_1: (0.4918) | Acc_1: (81.83%) (11626/14208)\n",
      "Epoch: 124 | Batch_idx: 120 |  Loss_1: (0.4896) | Acc_1: (81.89%) (12683/15488)\n",
      "Epoch: 124 | Batch_idx: 130 |  Loss_1: (0.4889) | Acc_1: (81.95%) (13742/16768)\n",
      "Epoch: 124 | Batch_idx: 140 |  Loss_1: (0.4870) | Acc_1: (82.04%) (14807/18048)\n",
      "Epoch: 124 | Batch_idx: 150 |  Loss_1: (0.4863) | Acc_1: (82.04%) (15856/19328)\n",
      "Epoch: 124 | Batch_idx: 160 |  Loss_1: (0.4851) | Acc_1: (82.09%) (16918/20608)\n",
      "Epoch: 124 | Batch_idx: 170 |  Loss_1: (0.4848) | Acc_1: (82.10%) (17969/21888)\n",
      "Epoch: 124 | Batch_idx: 180 |  Loss_1: (0.4878) | Acc_1: (81.96%) (18989/23168)\n",
      "Epoch: 124 | Batch_idx: 190 |  Loss_1: (0.4901) | Acc_1: (81.87%) (20015/24448)\n",
      "Epoch: 124 | Batch_idx: 200 |  Loss_1: (0.4889) | Acc_1: (81.90%) (21072/25728)\n",
      "Epoch: 124 | Batch_idx: 210 |  Loss_1: (0.4869) | Acc_1: (81.99%) (22143/27008)\n",
      "Epoch: 124 | Batch_idx: 220 |  Loss_1: (0.4865) | Acc_1: (81.97%) (23189/28288)\n",
      "Epoch: 124 | Batch_idx: 230 |  Loss_1: (0.4878) | Acc_1: (81.93%) (24226/29568)\n",
      "Epoch: 124 | Batch_idx: 240 |  Loss_1: (0.4880) | Acc_1: (81.92%) (25272/30848)\n",
      "Epoch: 124 | Batch_idx: 250 |  Loss_1: (0.4878) | Acc_1: (81.92%) (26320/32128)\n",
      "Epoch: 124 | Batch_idx: 260 |  Loss_1: (0.4883) | Acc_1: (81.90%) (27361/33408)\n",
      "Epoch: 124 | Batch_idx: 270 |  Loss_1: (0.4875) | Acc_1: (81.90%) (28409/34688)\n",
      "Epoch: 124 | Batch_idx: 280 |  Loss_1: (0.4886) | Acc_1: (81.87%) (29447/35968)\n",
      "Epoch: 124 | Batch_idx: 290 |  Loss_1: (0.4889) | Acc_1: (81.86%) (30490/37248)\n",
      "Epoch: 124 | Batch_idx: 300 |  Loss_1: (0.4893) | Acc_1: (81.85%) (31534/38528)\n",
      "Epoch: 124 | Batch_idx: 310 |  Loss_1: (0.4897) | Acc_1: (81.83%) (32575/39808)\n",
      "Epoch: 124 | Batch_idx: 320 |  Loss_1: (0.4897) | Acc_1: (81.83%) (33622/41088)\n",
      "Epoch: 124 | Batch_idx: 330 |  Loss_1: (0.4894) | Acc_1: (81.84%) (34676/42368)\n",
      "Epoch: 124 | Batch_idx: 340 |  Loss_1: (0.4888) | Acc_1: (81.87%) (35736/43648)\n",
      "Epoch: 124 | Batch_idx: 350 |  Loss_1: (0.4888) | Acc_1: (81.86%) (36777/44928)\n",
      "Epoch: 124 | Batch_idx: 360 |  Loss_1: (0.4879) | Acc_1: (81.89%) (37842/46208)\n",
      "Epoch: 124 | Batch_idx: 370 |  Loss_1: (0.4881) | Acc_1: (81.88%) (38884/47488)\n",
      "Epoch: 124 | Batch_idx: 380 |  Loss_1: (0.4881) | Acc_1: (81.88%) (39931/48768)\n",
      "Epoch: 124 | Batch_idx: 390 |  Loss_1: (0.4879) | Acc_1: (81.88%) (40940/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5227) | Acc: (90.06%) (9006/10000)\n",
      "Epoch: 125 | Batch_idx: 0 |  Loss_1: (0.5530) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 125 | Batch_idx: 10 |  Loss_1: (0.5020) | Acc_1: (81.25%) (1144/1408)\n",
      "Epoch: 125 | Batch_idx: 20 |  Loss_1: (0.4767) | Acc_1: (82.37%) (2214/2688)\n",
      "Epoch: 125 | Batch_idx: 30 |  Loss_1: (0.4836) | Acc_1: (82.11%) (3258/3968)\n",
      "Epoch: 125 | Batch_idx: 40 |  Loss_1: (0.4933) | Acc_1: (81.73%) (4289/5248)\n",
      "Epoch: 125 | Batch_idx: 50 |  Loss_1: (0.4916) | Acc_1: (81.83%) (5342/6528)\n",
      "Epoch: 125 | Batch_idx: 60 |  Loss_1: (0.4941) | Acc_1: (81.74%) (6382/7808)\n",
      "Epoch: 125 | Batch_idx: 70 |  Loss_1: (0.5029) | Acc_1: (81.37%) (7395/9088)\n",
      "Epoch: 125 | Batch_idx: 80 |  Loss_1: (0.4998) | Acc_1: (81.51%) (8451/10368)\n",
      "Epoch: 125 | Batch_idx: 90 |  Loss_1: (0.4960) | Acc_1: (81.65%) (9511/11648)\n",
      "Epoch: 125 | Batch_idx: 100 |  Loss_1: (0.4936) | Acc_1: (81.74%) (10567/12928)\n",
      "Epoch: 125 | Batch_idx: 110 |  Loss_1: (0.4906) | Acc_1: (81.85%) (11629/14208)\n",
      "Epoch: 125 | Batch_idx: 120 |  Loss_1: (0.4867) | Acc_1: (81.97%) (12696/15488)\n",
      "Epoch: 125 | Batch_idx: 130 |  Loss_1: (0.4882) | Acc_1: (81.92%) (13737/16768)\n",
      "Epoch: 125 | Batch_idx: 140 |  Loss_1: (0.4892) | Acc_1: (81.90%) (14781/18048)\n",
      "Epoch: 125 | Batch_idx: 150 |  Loss_1: (0.4888) | Acc_1: (81.91%) (15831/19328)\n",
      "Epoch: 125 | Batch_idx: 160 |  Loss_1: (0.4845) | Acc_1: (82.06%) (16911/20608)\n",
      "Epoch: 125 | Batch_idx: 170 |  Loss_1: (0.4865) | Acc_1: (81.99%) (17947/21888)\n",
      "Epoch: 125 | Batch_idx: 180 |  Loss_1: (0.4877) | Acc_1: (81.91%) (18978/23168)\n",
      "Epoch: 125 | Batch_idx: 190 |  Loss_1: (0.4880) | Acc_1: (81.94%) (20032/24448)\n",
      "Epoch: 125 | Batch_idx: 200 |  Loss_1: (0.4857) | Acc_1: (82.04%) (21107/25728)\n",
      "Epoch: 125 | Batch_idx: 210 |  Loss_1: (0.4869) | Acc_1: (82.01%) (22149/27008)\n",
      "Epoch: 125 | Batch_idx: 220 |  Loss_1: (0.4853) | Acc_1: (82.07%) (23217/28288)\n",
      "Epoch: 125 | Batch_idx: 230 |  Loss_1: (0.4837) | Acc_1: (82.12%) (24282/29568)\n",
      "Epoch: 125 | Batch_idx: 240 |  Loss_1: (0.4825) | Acc_1: (82.15%) (25342/30848)\n",
      "Epoch: 125 | Batch_idx: 250 |  Loss_1: (0.4823) | Acc_1: (82.16%) (26395/32128)\n",
      "Epoch: 125 | Batch_idx: 260 |  Loss_1: (0.4825) | Acc_1: (82.15%) (27445/33408)\n",
      "Epoch: 125 | Batch_idx: 270 |  Loss_1: (0.4831) | Acc_1: (82.13%) (28488/34688)\n",
      "Epoch: 125 | Batch_idx: 280 |  Loss_1: (0.4825) | Acc_1: (82.14%) (29544/35968)\n",
      "Epoch: 125 | Batch_idx: 290 |  Loss_1: (0.4838) | Acc_1: (82.10%) (30579/37248)\n",
      "Epoch: 125 | Batch_idx: 300 |  Loss_1: (0.4852) | Acc_1: (82.05%) (31612/38528)\n",
      "Epoch: 125 | Batch_idx: 310 |  Loss_1: (0.4870) | Acc_1: (81.98%) (32633/39808)\n",
      "Epoch: 125 | Batch_idx: 320 |  Loss_1: (0.4865) | Acc_1: (82.01%) (33696/41088)\n",
      "Epoch: 125 | Batch_idx: 330 |  Loss_1: (0.4863) | Acc_1: (82.01%) (34747/42368)\n",
      "Epoch: 125 | Batch_idx: 340 |  Loss_1: (0.4871) | Acc_1: (81.98%) (35783/43648)\n",
      "Epoch: 125 | Batch_idx: 350 |  Loss_1: (0.4861) | Acc_1: (82.02%) (36848/44928)\n",
      "Epoch: 125 | Batch_idx: 360 |  Loss_1: (0.4869) | Acc_1: (81.97%) (37879/46208)\n",
      "Epoch: 125 | Batch_idx: 370 |  Loss_1: (0.4866) | Acc_1: (81.98%) (38933/47488)\n",
      "Epoch: 125 | Batch_idx: 380 |  Loss_1: (0.4863) | Acc_1: (82.00%) (39990/48768)\n",
      "Epoch: 125 | Batch_idx: 390 |  Loss_1: (0.4865) | Acc_1: (82.00%) (40998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4733) | Acc: (90.23%) (9023/10000)\n",
      "Epoch: 126 | Batch_idx: 0 |  Loss_1: (0.5159) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 126 | Batch_idx: 10 |  Loss_1: (0.5306) | Acc_1: (80.18%) (1129/1408)\n",
      "Epoch: 126 | Batch_idx: 20 |  Loss_1: (0.5263) | Acc_1: (80.47%) (2163/2688)\n",
      "Epoch: 126 | Batch_idx: 30 |  Loss_1: (0.5157) | Acc_1: (80.90%) (3210/3968)\n",
      "Epoch: 126 | Batch_idx: 40 |  Loss_1: (0.5124) | Acc_1: (80.89%) (4245/5248)\n",
      "Epoch: 126 | Batch_idx: 50 |  Loss_1: (0.5005) | Acc_1: (81.28%) (5306/6528)\n",
      "Epoch: 126 | Batch_idx: 60 |  Loss_1: (0.4987) | Acc_1: (81.39%) (6355/7808)\n",
      "Epoch: 126 | Batch_idx: 70 |  Loss_1: (0.4968) | Acc_1: (81.42%) (7399/9088)\n",
      "Epoch: 126 | Batch_idx: 80 |  Loss_1: (0.5013) | Acc_1: (81.31%) (8430/10368)\n",
      "Epoch: 126 | Batch_idx: 90 |  Loss_1: (0.4982) | Acc_1: (81.44%) (9486/11648)\n",
      "Epoch: 126 | Batch_idx: 100 |  Loss_1: (0.4991) | Acc_1: (81.40%) (10523/12928)\n",
      "Epoch: 126 | Batch_idx: 110 |  Loss_1: (0.4997) | Acc_1: (81.41%) (11567/14208)\n",
      "Epoch: 126 | Batch_idx: 120 |  Loss_1: (0.4974) | Acc_1: (81.48%) (12620/15488)\n",
      "Epoch: 126 | Batch_idx: 130 |  Loss_1: (0.4981) | Acc_1: (81.43%) (13654/16768)\n",
      "Epoch: 126 | Batch_idx: 140 |  Loss_1: (0.5023) | Acc_1: (81.29%) (14671/18048)\n",
      "Epoch: 126 | Batch_idx: 150 |  Loss_1: (0.5023) | Acc_1: (81.31%) (15716/19328)\n",
      "Epoch: 126 | Batch_idx: 160 |  Loss_1: (0.5021) | Acc_1: (81.31%) (16757/20608)\n",
      "Epoch: 126 | Batch_idx: 170 |  Loss_1: (0.5018) | Acc_1: (81.31%) (17798/21888)\n",
      "Epoch: 126 | Batch_idx: 180 |  Loss_1: (0.5016) | Acc_1: (81.31%) (18839/23168)\n",
      "Epoch: 126 | Batch_idx: 190 |  Loss_1: (0.5002) | Acc_1: (81.33%) (19883/24448)\n",
      "Epoch: 126 | Batch_idx: 200 |  Loss_1: (0.4984) | Acc_1: (81.42%) (20947/25728)\n",
      "Epoch: 126 | Batch_idx: 210 |  Loss_1: (0.4972) | Acc_1: (81.47%) (22003/27008)\n",
      "Epoch: 126 | Batch_idx: 220 |  Loss_1: (0.4990) | Acc_1: (81.40%) (23027/28288)\n",
      "Epoch: 126 | Batch_idx: 230 |  Loss_1: (0.4965) | Acc_1: (81.52%) (24104/29568)\n",
      "Epoch: 126 | Batch_idx: 240 |  Loss_1: (0.4955) | Acc_1: (81.57%) (25163/30848)\n",
      "Epoch: 126 | Batch_idx: 250 |  Loss_1: (0.4970) | Acc_1: (81.50%) (26185/32128)\n",
      "Epoch: 126 | Batch_idx: 260 |  Loss_1: (0.4972) | Acc_1: (81.48%) (27222/33408)\n",
      "Epoch: 126 | Batch_idx: 270 |  Loss_1: (0.4992) | Acc_1: (81.39%) (28233/34688)\n",
      "Epoch: 126 | Batch_idx: 280 |  Loss_1: (0.5009) | Acc_1: (81.34%) (29257/35968)\n",
      "Epoch: 126 | Batch_idx: 290 |  Loss_1: (0.5002) | Acc_1: (81.37%) (30310/37248)\n",
      "Epoch: 126 | Batch_idx: 300 |  Loss_1: (0.5003) | Acc_1: (81.37%) (31352/38528)\n",
      "Epoch: 126 | Batch_idx: 310 |  Loss_1: (0.4987) | Acc_1: (81.46%) (32426/39808)\n",
      "Epoch: 126 | Batch_idx: 320 |  Loss_1: (0.4989) | Acc_1: (81.44%) (33464/41088)\n",
      "Epoch: 126 | Batch_idx: 330 |  Loss_1: (0.4999) | Acc_1: (81.42%) (34496/42368)\n",
      "Epoch: 126 | Batch_idx: 340 |  Loss_1: (0.4979) | Acc_1: (81.49%) (35568/43648)\n",
      "Epoch: 126 | Batch_idx: 350 |  Loss_1: (0.4975) | Acc_1: (81.50%) (36617/44928)\n",
      "Epoch: 126 | Batch_idx: 360 |  Loss_1: (0.4966) | Acc_1: (81.54%) (37677/46208)\n",
      "Epoch: 126 | Batch_idx: 370 |  Loss_1: (0.4962) | Acc_1: (81.56%) (38732/47488)\n",
      "Epoch: 126 | Batch_idx: 380 |  Loss_1: (0.4965) | Acc_1: (81.55%) (39771/48768)\n",
      "Epoch: 126 | Batch_idx: 390 |  Loss_1: (0.4968) | Acc_1: (81.54%) (40769/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4856) | Acc: (90.31%) (9031/10000)\n",
      "Epoch: 127 | Batch_idx: 0 |  Loss_1: (0.4970) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 127 | Batch_idx: 10 |  Loss_1: (0.5063) | Acc_1: (80.97%) (1140/1408)\n",
      "Epoch: 127 | Batch_idx: 20 |  Loss_1: (0.4680) | Acc_1: (82.33%) (2213/2688)\n",
      "Epoch: 127 | Batch_idx: 30 |  Loss_1: (0.4661) | Acc_1: (82.48%) (3273/3968)\n",
      "Epoch: 127 | Batch_idx: 40 |  Loss_1: (0.4691) | Acc_1: (82.36%) (4322/5248)\n",
      "Epoch: 127 | Batch_idx: 50 |  Loss_1: (0.4769) | Acc_1: (82.03%) (5355/6528)\n",
      "Epoch: 127 | Batch_idx: 60 |  Loss_1: (0.4833) | Acc_1: (81.90%) (6395/7808)\n",
      "Epoch: 127 | Batch_idx: 70 |  Loss_1: (0.4863) | Acc_1: (81.79%) (7433/9088)\n",
      "Epoch: 127 | Batch_idx: 80 |  Loss_1: (0.4906) | Acc_1: (81.69%) (8470/10368)\n",
      "Epoch: 127 | Batch_idx: 90 |  Loss_1: (0.4911) | Acc_1: (81.68%) (9514/11648)\n",
      "Epoch: 127 | Batch_idx: 100 |  Loss_1: (0.4938) | Acc_1: (81.58%) (10547/12928)\n",
      "Epoch: 127 | Batch_idx: 110 |  Loss_1: (0.4960) | Acc_1: (81.52%) (11583/14208)\n",
      "Epoch: 127 | Batch_idx: 120 |  Loss_1: (0.4953) | Acc_1: (81.55%) (12631/15488)\n",
      "Epoch: 127 | Batch_idx: 130 |  Loss_1: (0.4960) | Acc_1: (81.51%) (13667/16768)\n",
      "Epoch: 127 | Batch_idx: 140 |  Loss_1: (0.4966) | Acc_1: (81.46%) (14701/18048)\n",
      "Epoch: 127 | Batch_idx: 150 |  Loss_1: (0.4966) | Acc_1: (81.41%) (15735/19328)\n",
      "Epoch: 127 | Batch_idx: 160 |  Loss_1: (0.4950) | Acc_1: (81.48%) (16792/20608)\n",
      "Epoch: 127 | Batch_idx: 170 |  Loss_1: (0.4940) | Acc_1: (81.52%) (17843/21888)\n",
      "Epoch: 127 | Batch_idx: 180 |  Loss_1: (0.4954) | Acc_1: (81.47%) (18874/23168)\n",
      "Epoch: 127 | Batch_idx: 190 |  Loss_1: (0.4938) | Acc_1: (81.54%) (19936/24448)\n",
      "Epoch: 127 | Batch_idx: 200 |  Loss_1: (0.4947) | Acc_1: (81.53%) (20975/25728)\n",
      "Epoch: 127 | Batch_idx: 210 |  Loss_1: (0.4960) | Acc_1: (81.48%) (22005/27008)\n",
      "Epoch: 127 | Batch_idx: 220 |  Loss_1: (0.4951) | Acc_1: (81.53%) (23063/28288)\n",
      "Epoch: 127 | Batch_idx: 230 |  Loss_1: (0.4944) | Acc_1: (81.54%) (24110/29568)\n",
      "Epoch: 127 | Batch_idx: 240 |  Loss_1: (0.4934) | Acc_1: (81.58%) (25167/30848)\n",
      "Epoch: 127 | Batch_idx: 250 |  Loss_1: (0.4934) | Acc_1: (81.60%) (26215/32128)\n",
      "Epoch: 127 | Batch_idx: 260 |  Loss_1: (0.4933) | Acc_1: (81.61%) (27265/33408)\n",
      "Epoch: 127 | Batch_idx: 270 |  Loss_1: (0.4948) | Acc_1: (81.56%) (28291/34688)\n",
      "Epoch: 127 | Batch_idx: 280 |  Loss_1: (0.4946) | Acc_1: (81.57%) (29339/35968)\n",
      "Epoch: 127 | Batch_idx: 290 |  Loss_1: (0.4945) | Acc_1: (81.58%) (30388/37248)\n",
      "Epoch: 127 | Batch_idx: 300 |  Loss_1: (0.4935) | Acc_1: (81.61%) (31443/38528)\n",
      "Epoch: 127 | Batch_idx: 310 |  Loss_1: (0.4924) | Acc_1: (81.66%) (32509/39808)\n",
      "Epoch: 127 | Batch_idx: 320 |  Loss_1: (0.4931) | Acc_1: (81.65%) (33547/41088)\n",
      "Epoch: 127 | Batch_idx: 330 |  Loss_1: (0.4942) | Acc_1: (81.60%) (34574/42368)\n",
      "Epoch: 127 | Batch_idx: 340 |  Loss_1: (0.4941) | Acc_1: (81.59%) (35614/43648)\n",
      "Epoch: 127 | Batch_idx: 350 |  Loss_1: (0.4940) | Acc_1: (81.60%) (36662/44928)\n",
      "Epoch: 127 | Batch_idx: 360 |  Loss_1: (0.4945) | Acc_1: (81.57%) (37694/46208)\n",
      "Epoch: 127 | Batch_idx: 370 |  Loss_1: (0.4939) | Acc_1: (81.59%) (38747/47488)\n",
      "Epoch: 127 | Batch_idx: 380 |  Loss_1: (0.4942) | Acc_1: (81.57%) (39781/48768)\n",
      "Epoch: 127 | Batch_idx: 390 |  Loss_1: (0.4941) | Acc_1: (81.57%) (40787/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4652) | Acc: (90.45%) (9045/10000)\n",
      "Epoch: 128 | Batch_idx: 0 |  Loss_1: (0.5326) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 128 | Batch_idx: 10 |  Loss_1: (0.5027) | Acc_1: (81.25%) (1144/1408)\n",
      "Epoch: 128 | Batch_idx: 20 |  Loss_1: (0.4824) | Acc_1: (82.07%) (2206/2688)\n",
      "Epoch: 128 | Batch_idx: 30 |  Loss_1: (0.4917) | Acc_1: (81.63%) (3239/3968)\n",
      "Epoch: 128 | Batch_idx: 40 |  Loss_1: (0.4990) | Acc_1: (81.46%) (4275/5248)\n",
      "Epoch: 128 | Batch_idx: 50 |  Loss_1: (0.4988) | Acc_1: (81.45%) (5317/6528)\n",
      "Epoch: 128 | Batch_idx: 60 |  Loss_1: (0.5030) | Acc_1: (81.26%) (6345/7808)\n",
      "Epoch: 128 | Batch_idx: 70 |  Loss_1: (0.5033) | Acc_1: (81.23%) (7382/9088)\n",
      "Epoch: 128 | Batch_idx: 80 |  Loss_1: (0.5011) | Acc_1: (81.33%) (8432/10368)\n",
      "Epoch: 128 | Batch_idx: 90 |  Loss_1: (0.5005) | Acc_1: (81.34%) (9474/11648)\n",
      "Epoch: 128 | Batch_idx: 100 |  Loss_1: (0.4969) | Acc_1: (81.44%) (10529/12928)\n",
      "Epoch: 128 | Batch_idx: 110 |  Loss_1: (0.4931) | Acc_1: (81.62%) (11597/14208)\n",
      "Epoch: 128 | Batch_idx: 120 |  Loss_1: (0.4941) | Acc_1: (81.59%) (12636/15488)\n",
      "Epoch: 128 | Batch_idx: 130 |  Loss_1: (0.4924) | Acc_1: (81.62%) (13686/16768)\n",
      "Epoch: 128 | Batch_idx: 140 |  Loss_1: (0.4934) | Acc_1: (81.60%) (14728/18048)\n",
      "Epoch: 128 | Batch_idx: 150 |  Loss_1: (0.4936) | Acc_1: (81.61%) (15773/19328)\n",
      "Epoch: 128 | Batch_idx: 160 |  Loss_1: (0.4919) | Acc_1: (81.66%) (16829/20608)\n",
      "Epoch: 128 | Batch_idx: 170 |  Loss_1: (0.4902) | Acc_1: (81.73%) (17888/21888)\n",
      "Epoch: 128 | Batch_idx: 180 |  Loss_1: (0.4907) | Acc_1: (81.72%) (18932/23168)\n",
      "Epoch: 128 | Batch_idx: 190 |  Loss_1: (0.4922) | Acc_1: (81.68%) (19968/24448)\n",
      "Epoch: 128 | Batch_idx: 200 |  Loss_1: (0.4930) | Acc_1: (81.62%) (20999/25728)\n",
      "Epoch: 128 | Batch_idx: 210 |  Loss_1: (0.4918) | Acc_1: (81.67%) (22057/27008)\n",
      "Epoch: 128 | Batch_idx: 220 |  Loss_1: (0.4913) | Acc_1: (81.66%) (23101/28288)\n",
      "Epoch: 128 | Batch_idx: 230 |  Loss_1: (0.4926) | Acc_1: (81.63%) (24135/29568)\n",
      "Epoch: 128 | Batch_idx: 240 |  Loss_1: (0.4938) | Acc_1: (81.59%) (25168/30848)\n",
      "Epoch: 128 | Batch_idx: 250 |  Loss_1: (0.4960) | Acc_1: (81.51%) (26188/32128)\n",
      "Epoch: 128 | Batch_idx: 260 |  Loss_1: (0.4954) | Acc_1: (81.50%) (27229/33408)\n",
      "Epoch: 128 | Batch_idx: 270 |  Loss_1: (0.4936) | Acc_1: (81.56%) (28291/34688)\n",
      "Epoch: 128 | Batch_idx: 280 |  Loss_1: (0.4950) | Acc_1: (81.50%) (29314/35968)\n",
      "Epoch: 128 | Batch_idx: 290 |  Loss_1: (0.4944) | Acc_1: (81.53%) (30368/37248)\n",
      "Epoch: 128 | Batch_idx: 300 |  Loss_1: (0.4926) | Acc_1: (81.59%) (31436/38528)\n",
      "Epoch: 128 | Batch_idx: 310 |  Loss_1: (0.4933) | Acc_1: (81.56%) (32469/39808)\n",
      "Epoch: 128 | Batch_idx: 320 |  Loss_1: (0.4954) | Acc_1: (81.49%) (33483/41088)\n",
      "Epoch: 128 | Batch_idx: 330 |  Loss_1: (0.4936) | Acc_1: (81.54%) (34547/42368)\n",
      "Epoch: 128 | Batch_idx: 340 |  Loss_1: (0.4939) | Acc_1: (81.53%) (35587/43648)\n",
      "Epoch: 128 | Batch_idx: 350 |  Loss_1: (0.4937) | Acc_1: (81.54%) (36636/44928)\n",
      "Epoch: 128 | Batch_idx: 360 |  Loss_1: (0.4925) | Acc_1: (81.58%) (37698/46208)\n",
      "Epoch: 128 | Batch_idx: 370 |  Loss_1: (0.4915) | Acc_1: (81.62%) (38762/47488)\n",
      "Epoch: 128 | Batch_idx: 380 |  Loss_1: (0.4928) | Acc_1: (81.57%) (39782/48768)\n",
      "Epoch: 128 | Batch_idx: 390 |  Loss_1: (0.4932) | Acc_1: (81.57%) (40787/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4630) | Acc: (90.08%) (9008/10000)\n",
      "Epoch: 129 | Batch_idx: 0 |  Loss_1: (0.6533) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 129 | Batch_idx: 10 |  Loss_1: (0.5180) | Acc_1: (80.47%) (1133/1408)\n",
      "Epoch: 129 | Batch_idx: 20 |  Loss_1: (0.5015) | Acc_1: (81.44%) (2189/2688)\n",
      "Epoch: 129 | Batch_idx: 30 |  Loss_1: (0.5003) | Acc_1: (81.45%) (3232/3968)\n",
      "Epoch: 129 | Batch_idx: 40 |  Loss_1: (0.4902) | Acc_1: (81.69%) (4287/5248)\n",
      "Epoch: 129 | Batch_idx: 50 |  Loss_1: (0.4907) | Acc_1: (81.74%) (5336/6528)\n",
      "Epoch: 129 | Batch_idx: 60 |  Loss_1: (0.4920) | Acc_1: (81.76%) (6384/7808)\n",
      "Epoch: 129 | Batch_idx: 70 |  Loss_1: (0.4894) | Acc_1: (81.81%) (7435/9088)\n",
      "Epoch: 129 | Batch_idx: 80 |  Loss_1: (0.4934) | Acc_1: (81.68%) (8469/10368)\n",
      "Epoch: 129 | Batch_idx: 90 |  Loss_1: (0.4926) | Acc_1: (81.71%) (9517/11648)\n",
      "Epoch: 129 | Batch_idx: 100 |  Loss_1: (0.4908) | Acc_1: (81.80%) (10575/12928)\n",
      "Epoch: 129 | Batch_idx: 110 |  Loss_1: (0.4912) | Acc_1: (81.78%) (11619/14208)\n",
      "Epoch: 129 | Batch_idx: 120 |  Loss_1: (0.4877) | Acc_1: (81.88%) (12682/15488)\n",
      "Epoch: 129 | Batch_idx: 130 |  Loss_1: (0.4859) | Acc_1: (81.94%) (13739/16768)\n",
      "Epoch: 129 | Batch_idx: 140 |  Loss_1: (0.4872) | Acc_1: (81.83%) (14769/18048)\n",
      "Epoch: 129 | Batch_idx: 150 |  Loss_1: (0.4871) | Acc_1: (81.85%) (15820/19328)\n",
      "Epoch: 129 | Batch_idx: 160 |  Loss_1: (0.4873) | Acc_1: (81.86%) (16870/20608)\n",
      "Epoch: 129 | Batch_idx: 170 |  Loss_1: (0.4882) | Acc_1: (81.82%) (17909/21888)\n",
      "Epoch: 129 | Batch_idx: 180 |  Loss_1: (0.4868) | Acc_1: (81.85%) (18964/23168)\n",
      "Epoch: 129 | Batch_idx: 190 |  Loss_1: (0.4857) | Acc_1: (81.91%) (20026/24448)\n",
      "Epoch: 129 | Batch_idx: 200 |  Loss_1: (0.4871) | Acc_1: (81.85%) (21059/25728)\n",
      "Epoch: 129 | Batch_idx: 210 |  Loss_1: (0.4886) | Acc_1: (81.79%) (22089/27008)\n",
      "Epoch: 129 | Batch_idx: 220 |  Loss_1: (0.4879) | Acc_1: (81.83%) (23148/28288)\n",
      "Epoch: 129 | Batch_idx: 230 |  Loss_1: (0.4890) | Acc_1: (81.79%) (24183/29568)\n",
      "Epoch: 129 | Batch_idx: 240 |  Loss_1: (0.4898) | Acc_1: (81.75%) (25218/30848)\n",
      "Epoch: 129 | Batch_idx: 250 |  Loss_1: (0.4905) | Acc_1: (81.72%) (26254/32128)\n",
      "Epoch: 129 | Batch_idx: 260 |  Loss_1: (0.4889) | Acc_1: (81.78%) (27322/33408)\n",
      "Epoch: 129 | Batch_idx: 270 |  Loss_1: (0.4887) | Acc_1: (81.78%) (28368/34688)\n",
      "Epoch: 129 | Batch_idx: 280 |  Loss_1: (0.4882) | Acc_1: (81.80%) (29422/35968)\n",
      "Epoch: 129 | Batch_idx: 290 |  Loss_1: (0.4874) | Acc_1: (81.83%) (30481/37248)\n",
      "Epoch: 129 | Batch_idx: 300 |  Loss_1: (0.4875) | Acc_1: (81.81%) (31521/38528)\n",
      "Epoch: 129 | Batch_idx: 310 |  Loss_1: (0.4877) | Acc_1: (81.80%) (32563/39808)\n",
      "Epoch: 129 | Batch_idx: 320 |  Loss_1: (0.4897) | Acc_1: (81.72%) (33579/41088)\n",
      "Epoch: 129 | Batch_idx: 330 |  Loss_1: (0.4886) | Acc_1: (81.77%) (34645/42368)\n",
      "Epoch: 129 | Batch_idx: 340 |  Loss_1: (0.4881) | Acc_1: (81.79%) (35699/43648)\n",
      "Epoch: 129 | Batch_idx: 350 |  Loss_1: (0.4884) | Acc_1: (81.78%) (36742/44928)\n",
      "Epoch: 129 | Batch_idx: 360 |  Loss_1: (0.4883) | Acc_1: (81.77%) (37785/46208)\n",
      "Epoch: 129 | Batch_idx: 370 |  Loss_1: (0.4873) | Acc_1: (81.81%) (38851/47488)\n",
      "Epoch: 129 | Batch_idx: 380 |  Loss_1: (0.4868) | Acc_1: (81.83%) (39905/48768)\n",
      "Epoch: 129 | Batch_idx: 390 |  Loss_1: (0.4874) | Acc_1: (81.81%) (40903/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4812) | Acc: (90.49%) (9049/10000)\n",
      "Epoch: 130 | Batch_idx: 0 |  Loss_1: (0.4986) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 130 | Batch_idx: 10 |  Loss_1: (0.4691) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 130 | Batch_idx: 20 |  Loss_1: (0.4910) | Acc_1: (81.47%) (2190/2688)\n",
      "Epoch: 130 | Batch_idx: 30 |  Loss_1: (0.4758) | Acc_1: (82.21%) (3262/3968)\n",
      "Epoch: 130 | Batch_idx: 40 |  Loss_1: (0.4778) | Acc_1: (82.18%) (4313/5248)\n",
      "Epoch: 130 | Batch_idx: 50 |  Loss_1: (0.4824) | Acc_1: (82.05%) (5356/6528)\n",
      "Epoch: 130 | Batch_idx: 60 |  Loss_1: (0.4864) | Acc_1: (81.95%) (6399/7808)\n",
      "Epoch: 130 | Batch_idx: 70 |  Loss_1: (0.4884) | Acc_1: (81.88%) (7441/9088)\n",
      "Epoch: 130 | Batch_idx: 80 |  Loss_1: (0.4897) | Acc_1: (81.80%) (8481/10368)\n",
      "Epoch: 130 | Batch_idx: 90 |  Loss_1: (0.4859) | Acc_1: (81.96%) (9547/11648)\n",
      "Epoch: 130 | Batch_idx: 100 |  Loss_1: (0.4867) | Acc_1: (81.95%) (10595/12928)\n",
      "Epoch: 130 | Batch_idx: 110 |  Loss_1: (0.4900) | Acc_1: (81.79%) (11621/14208)\n",
      "Epoch: 130 | Batch_idx: 120 |  Loss_1: (0.4893) | Acc_1: (81.82%) (12673/15488)\n",
      "Epoch: 130 | Batch_idx: 130 |  Loss_1: (0.4868) | Acc_1: (81.92%) (13737/16768)\n",
      "Epoch: 130 | Batch_idx: 140 |  Loss_1: (0.4915) | Acc_1: (81.74%) (14752/18048)\n",
      "Epoch: 130 | Batch_idx: 150 |  Loss_1: (0.4910) | Acc_1: (81.77%) (15804/19328)\n",
      "Epoch: 130 | Batch_idx: 160 |  Loss_1: (0.4920) | Acc_1: (81.75%) (16847/20608)\n",
      "Epoch: 130 | Batch_idx: 170 |  Loss_1: (0.4933) | Acc_1: (81.71%) (17884/21888)\n",
      "Epoch: 130 | Batch_idx: 180 |  Loss_1: (0.4910) | Acc_1: (81.80%) (18951/23168)\n",
      "Epoch: 130 | Batch_idx: 190 |  Loss_1: (0.4905) | Acc_1: (81.81%) (20000/24448)\n",
      "Epoch: 130 | Batch_idx: 200 |  Loss_1: (0.4904) | Acc_1: (81.82%) (21050/25728)\n",
      "Epoch: 130 | Batch_idx: 210 |  Loss_1: (0.4892) | Acc_1: (81.85%) (22106/27008)\n",
      "Epoch: 130 | Batch_idx: 220 |  Loss_1: (0.4902) | Acc_1: (81.83%) (23147/28288)\n",
      "Epoch: 130 | Batch_idx: 230 |  Loss_1: (0.4912) | Acc_1: (81.74%) (24169/29568)\n",
      "Epoch: 130 | Batch_idx: 240 |  Loss_1: (0.4917) | Acc_1: (81.73%) (25212/30848)\n",
      "Epoch: 130 | Batch_idx: 250 |  Loss_1: (0.4922) | Acc_1: (81.68%) (26243/32128)\n",
      "Epoch: 130 | Batch_idx: 260 |  Loss_1: (0.4925) | Acc_1: (81.67%) (27285/33408)\n",
      "Epoch: 130 | Batch_idx: 270 |  Loss_1: (0.4926) | Acc_1: (81.67%) (28331/34688)\n",
      "Epoch: 130 | Batch_idx: 280 |  Loss_1: (0.4938) | Acc_1: (81.64%) (29364/35968)\n",
      "Epoch: 130 | Batch_idx: 290 |  Loss_1: (0.4929) | Acc_1: (81.68%) (30423/37248)\n",
      "Epoch: 130 | Batch_idx: 300 |  Loss_1: (0.4924) | Acc_1: (81.69%) (31474/38528)\n",
      "Epoch: 130 | Batch_idx: 310 |  Loss_1: (0.4925) | Acc_1: (81.70%) (32522/39808)\n",
      "Epoch: 130 | Batch_idx: 320 |  Loss_1: (0.4921) | Acc_1: (81.72%) (33577/41088)\n",
      "Epoch: 130 | Batch_idx: 330 |  Loss_1: (0.4912) | Acc_1: (81.75%) (34637/42368)\n",
      "Epoch: 130 | Batch_idx: 340 |  Loss_1: (0.4916) | Acc_1: (81.73%) (35673/43648)\n",
      "Epoch: 130 | Batch_idx: 350 |  Loss_1: (0.4917) | Acc_1: (81.72%) (36716/44928)\n",
      "Epoch: 130 | Batch_idx: 360 |  Loss_1: (0.4932) | Acc_1: (81.67%) (37738/46208)\n",
      "Epoch: 130 | Batch_idx: 370 |  Loss_1: (0.4927) | Acc_1: (81.69%) (38791/47488)\n",
      "Epoch: 130 | Batch_idx: 380 |  Loss_1: (0.4917) | Acc_1: (81.71%) (39850/48768)\n",
      "Epoch: 130 | Batch_idx: 390 |  Loss_1: (0.4929) | Acc_1: (81.68%) (40841/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4842) | Acc: (90.35%) (9035/10000)\n",
      "Epoch: 131 | Batch_idx: 0 |  Loss_1: (0.5353) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 131 | Batch_idx: 10 |  Loss_1: (0.4546) | Acc_1: (82.88%) (1167/1408)\n",
      "Epoch: 131 | Batch_idx: 20 |  Loss_1: (0.4663) | Acc_1: (82.48%) (2217/2688)\n",
      "Epoch: 131 | Batch_idx: 30 |  Loss_1: (0.4709) | Acc_1: (82.38%) (3269/3968)\n",
      "Epoch: 131 | Batch_idx: 40 |  Loss_1: (0.4560) | Acc_1: (83.00%) (4356/5248)\n",
      "Epoch: 131 | Batch_idx: 50 |  Loss_1: (0.4670) | Acc_1: (82.60%) (5392/6528)\n",
      "Epoch: 131 | Batch_idx: 60 |  Loss_1: (0.4627) | Acc_1: (82.67%) (6455/7808)\n",
      "Epoch: 131 | Batch_idx: 70 |  Loss_1: (0.4687) | Acc_1: (82.47%) (7495/9088)\n",
      "Epoch: 131 | Batch_idx: 80 |  Loss_1: (0.4671) | Acc_1: (82.62%) (8566/10368)\n",
      "Epoch: 131 | Batch_idx: 90 |  Loss_1: (0.4646) | Acc_1: (82.75%) (9639/11648)\n",
      "Epoch: 131 | Batch_idx: 100 |  Loss_1: (0.4634) | Acc_1: (82.80%) (10705/12928)\n",
      "Epoch: 131 | Batch_idx: 110 |  Loss_1: (0.4719) | Acc_1: (82.52%) (11725/14208)\n",
      "Epoch: 131 | Batch_idx: 120 |  Loss_1: (0.4707) | Acc_1: (82.57%) (12789/15488)\n",
      "Epoch: 131 | Batch_idx: 130 |  Loss_1: (0.4730) | Acc_1: (82.51%) (13835/16768)\n",
      "Epoch: 131 | Batch_idx: 140 |  Loss_1: (0.4785) | Acc_1: (82.29%) (14851/18048)\n",
      "Epoch: 131 | Batch_idx: 150 |  Loss_1: (0.4778) | Acc_1: (82.29%) (15905/19328)\n",
      "Epoch: 131 | Batch_idx: 160 |  Loss_1: (0.4763) | Acc_1: (82.33%) (16967/20608)\n",
      "Epoch: 131 | Batch_idx: 170 |  Loss_1: (0.4783) | Acc_1: (82.26%) (18004/21888)\n",
      "Epoch: 131 | Batch_idx: 180 |  Loss_1: (0.4810) | Acc_1: (82.16%) (19034/23168)\n",
      "Epoch: 131 | Batch_idx: 190 |  Loss_1: (0.4804) | Acc_1: (82.17%) (20089/24448)\n",
      "Epoch: 131 | Batch_idx: 200 |  Loss_1: (0.4833) | Acc_1: (82.06%) (21113/25728)\n",
      "Epoch: 131 | Batch_idx: 210 |  Loss_1: (0.4849) | Acc_1: (82.00%) (22147/27008)\n",
      "Epoch: 131 | Batch_idx: 220 |  Loss_1: (0.4834) | Acc_1: (82.05%) (23209/28288)\n",
      "Epoch: 131 | Batch_idx: 230 |  Loss_1: (0.4839) | Acc_1: (82.02%) (24251/29568)\n",
      "Epoch: 131 | Batch_idx: 240 |  Loss_1: (0.4862) | Acc_1: (81.92%) (25271/30848)\n",
      "Epoch: 131 | Batch_idx: 250 |  Loss_1: (0.4850) | Acc_1: (81.95%) (26330/32128)\n",
      "Epoch: 131 | Batch_idx: 260 |  Loss_1: (0.4851) | Acc_1: (81.94%) (27375/33408)\n",
      "Epoch: 131 | Batch_idx: 270 |  Loss_1: (0.4859) | Acc_1: (81.90%) (28411/34688)\n",
      "Epoch: 131 | Batch_idx: 280 |  Loss_1: (0.4863) | Acc_1: (81.89%) (29454/35968)\n",
      "Epoch: 131 | Batch_idx: 290 |  Loss_1: (0.4866) | Acc_1: (81.89%) (30503/37248)\n",
      "Epoch: 131 | Batch_idx: 300 |  Loss_1: (0.4868) | Acc_1: (81.85%) (31535/38528)\n",
      "Epoch: 131 | Batch_idx: 310 |  Loss_1: (0.4877) | Acc_1: (81.81%) (32565/39808)\n",
      "Epoch: 131 | Batch_idx: 320 |  Loss_1: (0.4891) | Acc_1: (81.76%) (33594/41088)\n",
      "Epoch: 131 | Batch_idx: 330 |  Loss_1: (0.4890) | Acc_1: (81.76%) (34642/42368)\n",
      "Epoch: 131 | Batch_idx: 340 |  Loss_1: (0.4877) | Acc_1: (81.81%) (35709/43648)\n",
      "Epoch: 131 | Batch_idx: 350 |  Loss_1: (0.4876) | Acc_1: (81.80%) (36753/44928)\n",
      "Epoch: 131 | Batch_idx: 360 |  Loss_1: (0.4879) | Acc_1: (81.78%) (37791/46208)\n",
      "Epoch: 131 | Batch_idx: 370 |  Loss_1: (0.4878) | Acc_1: (81.80%) (38843/47488)\n",
      "Epoch: 131 | Batch_idx: 380 |  Loss_1: (0.4873) | Acc_1: (81.82%) (39901/48768)\n",
      "Epoch: 131 | Batch_idx: 390 |  Loss_1: (0.4870) | Acc_1: (81.84%) (40920/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5027) | Acc: (90.29%) (9029/10000)\n",
      "Epoch: 132 | Batch_idx: 0 |  Loss_1: (0.4949) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 132 | Batch_idx: 10 |  Loss_1: (0.5063) | Acc_1: (80.97%) (1140/1408)\n",
      "Epoch: 132 | Batch_idx: 20 |  Loss_1: (0.5088) | Acc_1: (80.73%) (2170/2688)\n",
      "Epoch: 132 | Batch_idx: 30 |  Loss_1: (0.4875) | Acc_1: (81.75%) (3244/3968)\n",
      "Epoch: 132 | Batch_idx: 40 |  Loss_1: (0.4975) | Acc_1: (81.40%) (4272/5248)\n",
      "Epoch: 132 | Batch_idx: 50 |  Loss_1: (0.5032) | Acc_1: (81.22%) (5302/6528)\n",
      "Epoch: 132 | Batch_idx: 60 |  Loss_1: (0.5061) | Acc_1: (81.16%) (6337/7808)\n",
      "Epoch: 132 | Batch_idx: 70 |  Loss_1: (0.5064) | Acc_1: (81.15%) (7375/9088)\n",
      "Epoch: 132 | Batch_idx: 80 |  Loss_1: (0.5086) | Acc_1: (81.05%) (8403/10368)\n",
      "Epoch: 132 | Batch_idx: 90 |  Loss_1: (0.5005) | Acc_1: (81.39%) (9480/11648)\n",
      "Epoch: 132 | Batch_idx: 100 |  Loss_1: (0.5045) | Acc_1: (81.20%) (10498/12928)\n",
      "Epoch: 132 | Batch_idx: 110 |  Loss_1: (0.5047) | Acc_1: (81.21%) (11539/14208)\n",
      "Epoch: 132 | Batch_idx: 120 |  Loss_1: (0.5050) | Acc_1: (81.24%) (12583/15488)\n",
      "Epoch: 132 | Batch_idx: 130 |  Loss_1: (0.5027) | Acc_1: (81.32%) (13635/16768)\n",
      "Epoch: 132 | Batch_idx: 140 |  Loss_1: (0.5011) | Acc_1: (81.39%) (14689/18048)\n",
      "Epoch: 132 | Batch_idx: 150 |  Loss_1: (0.5033) | Acc_1: (81.33%) (15720/19328)\n",
      "Epoch: 132 | Batch_idx: 160 |  Loss_1: (0.5021) | Acc_1: (81.41%) (16776/20608)\n",
      "Epoch: 132 | Batch_idx: 170 |  Loss_1: (0.5026) | Acc_1: (81.36%) (17807/21888)\n",
      "Epoch: 132 | Batch_idx: 180 |  Loss_1: (0.5029) | Acc_1: (81.34%) (18844/23168)\n",
      "Epoch: 132 | Batch_idx: 190 |  Loss_1: (0.5026) | Acc_1: (81.34%) (19887/24448)\n",
      "Epoch: 132 | Batch_idx: 200 |  Loss_1: (0.5027) | Acc_1: (81.32%) (20923/25728)\n",
      "Epoch: 132 | Batch_idx: 210 |  Loss_1: (0.5010) | Acc_1: (81.39%) (21982/27008)\n",
      "Epoch: 132 | Batch_idx: 220 |  Loss_1: (0.5008) | Acc_1: (81.39%) (23023/28288)\n",
      "Epoch: 132 | Batch_idx: 230 |  Loss_1: (0.5026) | Acc_1: (81.30%) (24040/29568)\n",
      "Epoch: 132 | Batch_idx: 240 |  Loss_1: (0.5034) | Acc_1: (81.27%) (25071/30848)\n",
      "Epoch: 132 | Batch_idx: 250 |  Loss_1: (0.5017) | Acc_1: (81.37%) (26141/32128)\n",
      "Epoch: 132 | Batch_idx: 260 |  Loss_1: (0.5000) | Acc_1: (81.44%) (27207/33408)\n",
      "Epoch: 132 | Batch_idx: 270 |  Loss_1: (0.5006) | Acc_1: (81.43%) (28247/34688)\n",
      "Epoch: 132 | Batch_idx: 280 |  Loss_1: (0.4994) | Acc_1: (81.47%) (29304/35968)\n",
      "Epoch: 132 | Batch_idx: 290 |  Loss_1: (0.5000) | Acc_1: (81.46%) (30342/37248)\n",
      "Epoch: 132 | Batch_idx: 300 |  Loss_1: (0.5007) | Acc_1: (81.43%) (31375/38528)\n",
      "Epoch: 132 | Batch_idx: 310 |  Loss_1: (0.5020) | Acc_1: (81.37%) (32393/39808)\n",
      "Epoch: 132 | Batch_idx: 320 |  Loss_1: (0.5003) | Acc_1: (81.45%) (33465/41088)\n",
      "Epoch: 132 | Batch_idx: 330 |  Loss_1: (0.5029) | Acc_1: (81.34%) (34461/42368)\n",
      "Epoch: 132 | Batch_idx: 340 |  Loss_1: (0.5019) | Acc_1: (81.38%) (35519/43648)\n",
      "Epoch: 132 | Batch_idx: 350 |  Loss_1: (0.5021) | Acc_1: (81.38%) (36561/44928)\n",
      "Epoch: 132 | Batch_idx: 360 |  Loss_1: (0.5019) | Acc_1: (81.38%) (37605/46208)\n",
      "Epoch: 132 | Batch_idx: 370 |  Loss_1: (0.5021) | Acc_1: (81.36%) (38636/47488)\n",
      "Epoch: 132 | Batch_idx: 380 |  Loss_1: (0.5019) | Acc_1: (81.36%) (39679/48768)\n",
      "Epoch: 132 | Batch_idx: 390 |  Loss_1: (0.5025) | Acc_1: (81.34%) (40668/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4768) | Acc: (90.44%) (9044/10000)\n",
      "Epoch: 133 | Batch_idx: 0 |  Loss_1: (0.4966) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 133 | Batch_idx: 10 |  Loss_1: (0.4909) | Acc_1: (81.18%) (1143/1408)\n",
      "Epoch: 133 | Batch_idx: 20 |  Loss_1: (0.4874) | Acc_1: (81.66%) (2195/2688)\n",
      "Epoch: 133 | Batch_idx: 30 |  Loss_1: (0.4808) | Acc_1: (82.18%) (3261/3968)\n",
      "Epoch: 133 | Batch_idx: 40 |  Loss_1: (0.4793) | Acc_1: (82.20%) (4314/5248)\n",
      "Epoch: 133 | Batch_idx: 50 |  Loss_1: (0.4934) | Acc_1: (81.68%) (5332/6528)\n",
      "Epoch: 133 | Batch_idx: 60 |  Loss_1: (0.4816) | Acc_1: (82.11%) (6411/7808)\n",
      "Epoch: 133 | Batch_idx: 70 |  Loss_1: (0.4791) | Acc_1: (82.14%) (7465/9088)\n",
      "Epoch: 133 | Batch_idx: 80 |  Loss_1: (0.4765) | Acc_1: (82.21%) (8524/10368)\n",
      "Epoch: 133 | Batch_idx: 90 |  Loss_1: (0.4788) | Acc_1: (82.13%) (9567/11648)\n",
      "Epoch: 133 | Batch_idx: 100 |  Loss_1: (0.4829) | Acc_1: (82.02%) (10603/12928)\n",
      "Epoch: 133 | Batch_idx: 110 |  Loss_1: (0.4808) | Acc_1: (82.12%) (11667/14208)\n",
      "Epoch: 133 | Batch_idx: 120 |  Loss_1: (0.4802) | Acc_1: (82.15%) (12723/15488)\n",
      "Epoch: 133 | Batch_idx: 130 |  Loss_1: (0.4826) | Acc_1: (82.06%) (13759/16768)\n",
      "Epoch: 133 | Batch_idx: 140 |  Loss_1: (0.4836) | Acc_1: (82.03%) (14805/18048)\n",
      "Epoch: 133 | Batch_idx: 150 |  Loss_1: (0.4850) | Acc_1: (81.99%) (15847/19328)\n",
      "Epoch: 133 | Batch_idx: 160 |  Loss_1: (0.4871) | Acc_1: (81.91%) (16879/20608)\n",
      "Epoch: 133 | Batch_idx: 170 |  Loss_1: (0.4863) | Acc_1: (81.94%) (17935/21888)\n",
      "Epoch: 133 | Batch_idx: 180 |  Loss_1: (0.4847) | Acc_1: (82.01%) (19000/23168)\n",
      "Epoch: 133 | Batch_idx: 190 |  Loss_1: (0.4846) | Acc_1: (82.02%) (20052/24448)\n",
      "Epoch: 133 | Batch_idx: 200 |  Loss_1: (0.4870) | Acc_1: (81.91%) (21074/25728)\n",
      "Epoch: 133 | Batch_idx: 210 |  Loss_1: (0.4865) | Acc_1: (81.92%) (22124/27008)\n",
      "Epoch: 133 | Batch_idx: 220 |  Loss_1: (0.4857) | Acc_1: (81.94%) (23179/28288)\n",
      "Epoch: 133 | Batch_idx: 230 |  Loss_1: (0.4854) | Acc_1: (81.96%) (24234/29568)\n",
      "Epoch: 133 | Batch_idx: 240 |  Loss_1: (0.4849) | Acc_1: (81.99%) (25293/30848)\n",
      "Epoch: 133 | Batch_idx: 250 |  Loss_1: (0.4838) | Acc_1: (82.03%) (26356/32128)\n",
      "Epoch: 133 | Batch_idx: 260 |  Loss_1: (0.4838) | Acc_1: (82.06%) (27413/33408)\n",
      "Epoch: 133 | Batch_idx: 270 |  Loss_1: (0.4835) | Acc_1: (82.07%) (28468/34688)\n",
      "Epoch: 133 | Batch_idx: 280 |  Loss_1: (0.4829) | Acc_1: (82.09%) (29526/35968)\n",
      "Epoch: 133 | Batch_idx: 290 |  Loss_1: (0.4823) | Acc_1: (82.11%) (30584/37248)\n",
      "Epoch: 133 | Batch_idx: 300 |  Loss_1: (0.4823) | Acc_1: (82.12%) (31638/38528)\n",
      "Epoch: 133 | Batch_idx: 310 |  Loss_1: (0.4836) | Acc_1: (82.07%) (32672/39808)\n",
      "Epoch: 133 | Batch_idx: 320 |  Loss_1: (0.4841) | Acc_1: (82.07%) (33720/41088)\n",
      "Epoch: 133 | Batch_idx: 330 |  Loss_1: (0.4835) | Acc_1: (82.08%) (34774/42368)\n",
      "Epoch: 133 | Batch_idx: 340 |  Loss_1: (0.4840) | Acc_1: (82.06%) (35817/43648)\n",
      "Epoch: 133 | Batch_idx: 350 |  Loss_1: (0.4847) | Acc_1: (82.02%) (36849/44928)\n",
      "Epoch: 133 | Batch_idx: 360 |  Loss_1: (0.4836) | Acc_1: (82.05%) (37914/46208)\n",
      "Epoch: 133 | Batch_idx: 370 |  Loss_1: (0.4834) | Acc_1: (82.05%) (38965/47488)\n",
      "Epoch: 133 | Batch_idx: 380 |  Loss_1: (0.4836) | Acc_1: (82.03%) (40006/48768)\n",
      "Epoch: 133 | Batch_idx: 390 |  Loss_1: (0.4837) | Acc_1: (82.03%) (41016/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5236) | Acc: (89.81%) (8981/10000)\n",
      "Epoch: 134 | Batch_idx: 0 |  Loss_1: (0.5321) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 134 | Batch_idx: 10 |  Loss_1: (0.5168) | Acc_1: (80.47%) (1133/1408)\n",
      "Epoch: 134 | Batch_idx: 20 |  Loss_1: (0.4981) | Acc_1: (81.06%) (2179/2688)\n",
      "Epoch: 134 | Batch_idx: 30 |  Loss_1: (0.4967) | Acc_1: (81.15%) (3220/3968)\n",
      "Epoch: 134 | Batch_idx: 40 |  Loss_1: (0.4879) | Acc_1: (81.48%) (4276/5248)\n",
      "Epoch: 134 | Batch_idx: 50 |  Loss_1: (0.4878) | Acc_1: (81.63%) (5329/6528)\n",
      "Epoch: 134 | Batch_idx: 60 |  Loss_1: (0.4932) | Acc_1: (81.49%) (6363/7808)\n",
      "Epoch: 134 | Batch_idx: 70 |  Loss_1: (0.4903) | Acc_1: (81.64%) (7419/9088)\n",
      "Epoch: 134 | Batch_idx: 80 |  Loss_1: (0.4938) | Acc_1: (81.48%) (8448/10368)\n",
      "Epoch: 134 | Batch_idx: 90 |  Loss_1: (0.4923) | Acc_1: (81.53%) (9497/11648)\n",
      "Epoch: 134 | Batch_idx: 100 |  Loss_1: (0.4928) | Acc_1: (81.51%) (10537/12928)\n",
      "Epoch: 134 | Batch_idx: 110 |  Loss_1: (0.4925) | Acc_1: (81.51%) (11581/14208)\n",
      "Epoch: 134 | Batch_idx: 120 |  Loss_1: (0.4892) | Acc_1: (81.65%) (12646/15488)\n",
      "Epoch: 134 | Batch_idx: 130 |  Loss_1: (0.4893) | Acc_1: (81.69%) (13698/16768)\n",
      "Epoch: 134 | Batch_idx: 140 |  Loss_1: (0.4912) | Acc_1: (81.58%) (14724/18048)\n",
      "Epoch: 134 | Batch_idx: 150 |  Loss_1: (0.4916) | Acc_1: (81.55%) (15762/19328)\n",
      "Epoch: 134 | Batch_idx: 160 |  Loss_1: (0.4894) | Acc_1: (81.62%) (16820/20608)\n",
      "Epoch: 134 | Batch_idx: 170 |  Loss_1: (0.4909) | Acc_1: (81.57%) (17853/21888)\n",
      "Epoch: 134 | Batch_idx: 180 |  Loss_1: (0.4907) | Acc_1: (81.58%) (18900/23168)\n",
      "Epoch: 134 | Batch_idx: 190 |  Loss_1: (0.4888) | Acc_1: (81.67%) (19966/24448)\n",
      "Epoch: 134 | Batch_idx: 200 |  Loss_1: (0.4873) | Acc_1: (81.73%) (21028/25728)\n",
      "Epoch: 134 | Batch_idx: 210 |  Loss_1: (0.4850) | Acc_1: (81.82%) (22098/27008)\n",
      "Epoch: 134 | Batch_idx: 220 |  Loss_1: (0.4846) | Acc_1: (81.86%) (23157/28288)\n",
      "Epoch: 134 | Batch_idx: 230 |  Loss_1: (0.4858) | Acc_1: (81.82%) (24193/29568)\n",
      "Epoch: 134 | Batch_idx: 240 |  Loss_1: (0.4854) | Acc_1: (81.84%) (25246/30848)\n",
      "Epoch: 134 | Batch_idx: 250 |  Loss_1: (0.4864) | Acc_1: (81.80%) (26281/32128)\n",
      "Epoch: 134 | Batch_idx: 260 |  Loss_1: (0.4872) | Acc_1: (81.76%) (27316/33408)\n",
      "Epoch: 134 | Batch_idx: 270 |  Loss_1: (0.4873) | Acc_1: (81.75%) (28357/34688)\n",
      "Epoch: 134 | Batch_idx: 280 |  Loss_1: (0.4870) | Acc_1: (81.77%) (29411/35968)\n",
      "Epoch: 134 | Batch_idx: 290 |  Loss_1: (0.4870) | Acc_1: (81.77%) (30456/37248)\n",
      "Epoch: 134 | Batch_idx: 300 |  Loss_1: (0.4862) | Acc_1: (81.79%) (31512/38528)\n",
      "Epoch: 134 | Batch_idx: 310 |  Loss_1: (0.4877) | Acc_1: (81.74%) (32540/39808)\n",
      "Epoch: 134 | Batch_idx: 320 |  Loss_1: (0.4878) | Acc_1: (81.74%) (33587/41088)\n",
      "Epoch: 134 | Batch_idx: 330 |  Loss_1: (0.4877) | Acc_1: (81.74%) (34633/42368)\n",
      "Epoch: 134 | Batch_idx: 340 |  Loss_1: (0.4882) | Acc_1: (81.72%) (35670/43648)\n",
      "Epoch: 134 | Batch_idx: 350 |  Loss_1: (0.4880) | Acc_1: (81.74%) (36723/44928)\n",
      "Epoch: 134 | Batch_idx: 360 |  Loss_1: (0.4873) | Acc_1: (81.77%) (37786/46208)\n",
      "Epoch: 134 | Batch_idx: 370 |  Loss_1: (0.4874) | Acc_1: (81.76%) (38828/47488)\n",
      "Epoch: 134 | Batch_idx: 380 |  Loss_1: (0.4860) | Acc_1: (81.82%) (39902/48768)\n",
      "Epoch: 134 | Batch_idx: 390 |  Loss_1: (0.4859) | Acc_1: (81.82%) (40909/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5868) | Acc: (88.99%) (8899/10000)\n",
      "Epoch: 135 | Batch_idx: 0 |  Loss_1: (0.3882) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 135 | Batch_idx: 10 |  Loss_1: (0.4756) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 135 | Batch_idx: 20 |  Loss_1: (0.5000) | Acc_1: (81.62%) (2194/2688)\n",
      "Epoch: 135 | Batch_idx: 30 |  Loss_1: (0.5055) | Acc_1: (81.40%) (3230/3968)\n",
      "Epoch: 135 | Batch_idx: 40 |  Loss_1: (0.5079) | Acc_1: (81.21%) (4262/5248)\n",
      "Epoch: 135 | Batch_idx: 50 |  Loss_1: (0.5000) | Acc_1: (81.43%) (5316/6528)\n",
      "Epoch: 135 | Batch_idx: 60 |  Loss_1: (0.4923) | Acc_1: (81.72%) (6381/7808)\n",
      "Epoch: 135 | Batch_idx: 70 |  Loss_1: (0.4918) | Acc_1: (81.68%) (7423/9088)\n",
      "Epoch: 135 | Batch_idx: 80 |  Loss_1: (0.4922) | Acc_1: (81.67%) (8468/10368)\n",
      "Epoch: 135 | Batch_idx: 90 |  Loss_1: (0.4908) | Acc_1: (81.77%) (9524/11648)\n",
      "Epoch: 135 | Batch_idx: 100 |  Loss_1: (0.4872) | Acc_1: (81.90%) (10588/12928)\n",
      "Epoch: 135 | Batch_idx: 110 |  Loss_1: (0.4886) | Acc_1: (81.81%) (11624/14208)\n",
      "Epoch: 135 | Batch_idx: 120 |  Loss_1: (0.4895) | Acc_1: (81.78%) (12666/15488)\n",
      "Epoch: 135 | Batch_idx: 130 |  Loss_1: (0.4902) | Acc_1: (81.77%) (13712/16768)\n",
      "Epoch: 135 | Batch_idx: 140 |  Loss_1: (0.4914) | Acc_1: (81.75%) (14755/18048)\n",
      "Epoch: 135 | Batch_idx: 150 |  Loss_1: (0.4902) | Acc_1: (81.82%) (15814/19328)\n",
      "Epoch: 135 | Batch_idx: 160 |  Loss_1: (0.4902) | Acc_1: (81.82%) (16861/20608)\n",
      "Epoch: 135 | Batch_idx: 170 |  Loss_1: (0.4916) | Acc_1: (81.75%) (17894/21888)\n",
      "Epoch: 135 | Batch_idx: 180 |  Loss_1: (0.4935) | Acc_1: (81.67%) (18921/23168)\n",
      "Epoch: 135 | Batch_idx: 190 |  Loss_1: (0.4930) | Acc_1: (81.70%) (19975/24448)\n",
      "Epoch: 135 | Batch_idx: 200 |  Loss_1: (0.4922) | Acc_1: (81.72%) (21026/25728)\n",
      "Epoch: 135 | Batch_idx: 210 |  Loss_1: (0.4935) | Acc_1: (81.66%) (22055/27008)\n",
      "Epoch: 135 | Batch_idx: 220 |  Loss_1: (0.4933) | Acc_1: (81.67%) (23102/28288)\n",
      "Epoch: 135 | Batch_idx: 230 |  Loss_1: (0.4905) | Acc_1: (81.78%) (24181/29568)\n",
      "Epoch: 135 | Batch_idx: 240 |  Loss_1: (0.4903) | Acc_1: (81.77%) (25224/30848)\n",
      "Epoch: 135 | Batch_idx: 250 |  Loss_1: (0.4915) | Acc_1: (81.73%) (26257/32128)\n",
      "Epoch: 135 | Batch_idx: 260 |  Loss_1: (0.4907) | Acc_1: (81.76%) (27315/33408)\n",
      "Epoch: 135 | Batch_idx: 270 |  Loss_1: (0.4905) | Acc_1: (81.76%) (28360/34688)\n",
      "Epoch: 135 | Batch_idx: 280 |  Loss_1: (0.4902) | Acc_1: (81.78%) (29414/35968)\n",
      "Epoch: 135 | Batch_idx: 290 |  Loss_1: (0.4895) | Acc_1: (81.79%) (30466/37248)\n",
      "Epoch: 135 | Batch_idx: 300 |  Loss_1: (0.4896) | Acc_1: (81.79%) (31513/38528)\n",
      "Epoch: 135 | Batch_idx: 310 |  Loss_1: (0.4906) | Acc_1: (81.76%) (32547/39808)\n",
      "Epoch: 135 | Batch_idx: 320 |  Loss_1: (0.4902) | Acc_1: (81.78%) (33603/41088)\n",
      "Epoch: 135 | Batch_idx: 330 |  Loss_1: (0.4901) | Acc_1: (81.79%) (34653/42368)\n",
      "Epoch: 135 | Batch_idx: 340 |  Loss_1: (0.4894) | Acc_1: (81.82%) (35714/43648)\n",
      "Epoch: 135 | Batch_idx: 350 |  Loss_1: (0.4891) | Acc_1: (81.84%) (36771/44928)\n",
      "Epoch: 135 | Batch_idx: 360 |  Loss_1: (0.4910) | Acc_1: (81.78%) (37787/46208)\n",
      "Epoch: 135 | Batch_idx: 370 |  Loss_1: (0.4905) | Acc_1: (81.80%) (38843/47488)\n",
      "Epoch: 135 | Batch_idx: 380 |  Loss_1: (0.4907) | Acc_1: (81.79%) (39887/48768)\n",
      "Epoch: 135 | Batch_idx: 390 |  Loss_1: (0.4909) | Acc_1: (81.79%) (40894/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4479) | Acc: (90.35%) (9035/10000)\n",
      "Epoch: 136 | Batch_idx: 0 |  Loss_1: (0.5045) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 136 | Batch_idx: 10 |  Loss_1: (0.5237) | Acc_1: (80.11%) (1128/1408)\n",
      "Epoch: 136 | Batch_idx: 20 |  Loss_1: (0.5434) | Acc_1: (79.50%) (2137/2688)\n",
      "Epoch: 136 | Batch_idx: 30 |  Loss_1: (0.5422) | Acc_1: (79.44%) (3152/3968)\n",
      "Epoch: 136 | Batch_idx: 40 |  Loss_1: (0.5304) | Acc_1: (80.05%) (4201/5248)\n",
      "Epoch: 136 | Batch_idx: 50 |  Loss_1: (0.5184) | Acc_1: (80.56%) (5259/6528)\n",
      "Epoch: 136 | Batch_idx: 60 |  Loss_1: (0.5162) | Acc_1: (80.71%) (6302/7808)\n",
      "Epoch: 136 | Batch_idx: 70 |  Loss_1: (0.5168) | Acc_1: (80.73%) (7337/9088)\n",
      "Epoch: 136 | Batch_idx: 80 |  Loss_1: (0.5143) | Acc_1: (80.84%) (8382/10368)\n",
      "Epoch: 136 | Batch_idx: 90 |  Loss_1: (0.5134) | Acc_1: (80.87%) (9420/11648)\n",
      "Epoch: 136 | Batch_idx: 100 |  Loss_1: (0.5093) | Acc_1: (81.04%) (10477/12928)\n",
      "Epoch: 136 | Batch_idx: 110 |  Loss_1: (0.5071) | Acc_1: (81.09%) (11521/14208)\n",
      "Epoch: 136 | Batch_idx: 120 |  Loss_1: (0.5063) | Acc_1: (81.09%) (12559/15488)\n",
      "Epoch: 136 | Batch_idx: 130 |  Loss_1: (0.5023) | Acc_1: (81.25%) (13624/16768)\n",
      "Epoch: 136 | Batch_idx: 140 |  Loss_1: (0.5027) | Acc_1: (81.26%) (14665/18048)\n",
      "Epoch: 136 | Batch_idx: 150 |  Loss_1: (0.5006) | Acc_1: (81.35%) (15723/19328)\n",
      "Epoch: 136 | Batch_idx: 160 |  Loss_1: (0.4982) | Acc_1: (81.43%) (16782/20608)\n",
      "Epoch: 136 | Batch_idx: 170 |  Loss_1: (0.4948) | Acc_1: (81.57%) (17854/21888)\n",
      "Epoch: 136 | Batch_idx: 180 |  Loss_1: (0.4951) | Acc_1: (81.54%) (18891/23168)\n",
      "Epoch: 136 | Batch_idx: 190 |  Loss_1: (0.4934) | Acc_1: (81.60%) (19950/24448)\n",
      "Epoch: 136 | Batch_idx: 200 |  Loss_1: (0.4936) | Acc_1: (81.60%) (20993/25728)\n",
      "Epoch: 136 | Batch_idx: 210 |  Loss_1: (0.4913) | Acc_1: (81.67%) (22058/27008)\n",
      "Epoch: 136 | Batch_idx: 220 |  Loss_1: (0.4922) | Acc_1: (81.64%) (23094/28288)\n",
      "Epoch: 136 | Batch_idx: 230 |  Loss_1: (0.4916) | Acc_1: (81.68%) (24150/29568)\n",
      "Epoch: 136 | Batch_idx: 240 |  Loss_1: (0.4916) | Acc_1: (81.69%) (25201/30848)\n",
      "Epoch: 136 | Batch_idx: 250 |  Loss_1: (0.4909) | Acc_1: (81.73%) (26257/32128)\n",
      "Epoch: 136 | Batch_idx: 260 |  Loss_1: (0.4915) | Acc_1: (81.69%) (27292/33408)\n",
      "Epoch: 136 | Batch_idx: 270 |  Loss_1: (0.4904) | Acc_1: (81.73%) (28349/34688)\n",
      "Epoch: 136 | Batch_idx: 280 |  Loss_1: (0.4906) | Acc_1: (81.71%) (29390/35968)\n",
      "Epoch: 136 | Batch_idx: 290 |  Loss_1: (0.4909) | Acc_1: (81.71%) (30435/37248)\n",
      "Epoch: 136 | Batch_idx: 300 |  Loss_1: (0.4917) | Acc_1: (81.67%) (31465/38528)\n",
      "Epoch: 136 | Batch_idx: 310 |  Loss_1: (0.4924) | Acc_1: (81.63%) (32495/39808)\n",
      "Epoch: 136 | Batch_idx: 320 |  Loss_1: (0.4918) | Acc_1: (81.67%) (33558/41088)\n",
      "Epoch: 136 | Batch_idx: 330 |  Loss_1: (0.4915) | Acc_1: (81.69%) (34609/42368)\n",
      "Epoch: 136 | Batch_idx: 340 |  Loss_1: (0.4924) | Acc_1: (81.65%) (35640/43648)\n",
      "Epoch: 136 | Batch_idx: 350 |  Loss_1: (0.4927) | Acc_1: (81.64%) (36679/44928)\n",
      "Epoch: 136 | Batch_idx: 360 |  Loss_1: (0.4930) | Acc_1: (81.62%) (37714/46208)\n",
      "Epoch: 136 | Batch_idx: 370 |  Loss_1: (0.4927) | Acc_1: (81.62%) (38761/47488)\n",
      "Epoch: 136 | Batch_idx: 380 |  Loss_1: (0.4923) | Acc_1: (81.64%) (39814/48768)\n",
      "Epoch: 136 | Batch_idx: 390 |  Loss_1: (0.4918) | Acc_1: (81.67%) (40833/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4742) | Acc: (90.57%) (9057/10000)\n",
      "Epoch: 137 | Batch_idx: 0 |  Loss_1: (0.6040) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 137 | Batch_idx: 10 |  Loss_1: (0.5257) | Acc_1: (80.75%) (1137/1408)\n",
      "Epoch: 137 | Batch_idx: 20 |  Loss_1: (0.5191) | Acc_1: (81.06%) (2179/2688)\n",
      "Epoch: 137 | Batch_idx: 30 |  Loss_1: (0.5061) | Acc_1: (81.40%) (3230/3968)\n",
      "Epoch: 137 | Batch_idx: 40 |  Loss_1: (0.5075) | Acc_1: (81.27%) (4265/5248)\n",
      "Epoch: 137 | Batch_idx: 50 |  Loss_1: (0.5067) | Acc_1: (81.25%) (5304/6528)\n",
      "Epoch: 137 | Batch_idx: 60 |  Loss_1: (0.5060) | Acc_1: (81.26%) (6345/7808)\n",
      "Epoch: 137 | Batch_idx: 70 |  Loss_1: (0.5055) | Acc_1: (81.24%) (7383/9088)\n",
      "Epoch: 137 | Batch_idx: 80 |  Loss_1: (0.5016) | Acc_1: (81.40%) (8440/10368)\n",
      "Epoch: 137 | Batch_idx: 90 |  Loss_1: (0.5011) | Acc_1: (81.45%) (9487/11648)\n",
      "Epoch: 137 | Batch_idx: 100 |  Loss_1: (0.4955) | Acc_1: (81.65%) (10556/12928)\n",
      "Epoch: 137 | Batch_idx: 110 |  Loss_1: (0.4918) | Acc_1: (81.78%) (11619/14208)\n",
      "Epoch: 137 | Batch_idx: 120 |  Loss_1: (0.4930) | Acc_1: (81.72%) (12657/15488)\n",
      "Epoch: 137 | Batch_idx: 130 |  Loss_1: (0.4905) | Acc_1: (81.77%) (13712/16768)\n",
      "Epoch: 137 | Batch_idx: 140 |  Loss_1: (0.4900) | Acc_1: (81.81%) (14765/18048)\n",
      "Epoch: 137 | Batch_idx: 150 |  Loss_1: (0.4898) | Acc_1: (81.82%) (15815/19328)\n",
      "Epoch: 137 | Batch_idx: 160 |  Loss_1: (0.4882) | Acc_1: (81.88%) (16874/20608)\n",
      "Epoch: 137 | Batch_idx: 170 |  Loss_1: (0.4861) | Acc_1: (81.94%) (17936/21888)\n",
      "Epoch: 137 | Batch_idx: 180 |  Loss_1: (0.4877) | Acc_1: (81.88%) (18969/23168)\n",
      "Epoch: 137 | Batch_idx: 190 |  Loss_1: (0.4865) | Acc_1: (81.94%) (20032/24448)\n",
      "Epoch: 137 | Batch_idx: 200 |  Loss_1: (0.4857) | Acc_1: (81.95%) (21084/25728)\n",
      "Epoch: 137 | Batch_idx: 210 |  Loss_1: (0.4849) | Acc_1: (81.98%) (22141/27008)\n",
      "Epoch: 137 | Batch_idx: 220 |  Loss_1: (0.4840) | Acc_1: (82.03%) (23206/28288)\n",
      "Epoch: 137 | Batch_idx: 230 |  Loss_1: (0.4850) | Acc_1: (81.99%) (24244/29568)\n",
      "Epoch: 137 | Batch_idx: 240 |  Loss_1: (0.4856) | Acc_1: (81.95%) (25281/30848)\n",
      "Epoch: 137 | Batch_idx: 250 |  Loss_1: (0.4867) | Acc_1: (81.91%) (26316/32128)\n",
      "Epoch: 137 | Batch_idx: 260 |  Loss_1: (0.4860) | Acc_1: (81.95%) (27379/33408)\n",
      "Epoch: 137 | Batch_idx: 270 |  Loss_1: (0.4858) | Acc_1: (81.95%) (28428/34688)\n",
      "Epoch: 137 | Batch_idx: 280 |  Loss_1: (0.4864) | Acc_1: (81.93%) (29467/35968)\n",
      "Epoch: 137 | Batch_idx: 290 |  Loss_1: (0.4859) | Acc_1: (81.95%) (30525/37248)\n",
      "Epoch: 137 | Batch_idx: 300 |  Loss_1: (0.4856) | Acc_1: (81.96%) (31577/38528)\n",
      "Epoch: 137 | Batch_idx: 310 |  Loss_1: (0.4881) | Acc_1: (81.87%) (32591/39808)\n",
      "Epoch: 137 | Batch_idx: 320 |  Loss_1: (0.4909) | Acc_1: (81.76%) (33595/41088)\n",
      "Epoch: 137 | Batch_idx: 330 |  Loss_1: (0.4907) | Acc_1: (81.78%) (34648/42368)\n",
      "Epoch: 137 | Batch_idx: 340 |  Loss_1: (0.4901) | Acc_1: (81.81%) (35710/43648)\n",
      "Epoch: 137 | Batch_idx: 350 |  Loss_1: (0.4897) | Acc_1: (81.84%) (36767/44928)\n",
      "Epoch: 137 | Batch_idx: 360 |  Loss_1: (0.4905) | Acc_1: (81.80%) (37799/46208)\n",
      "Epoch: 137 | Batch_idx: 370 |  Loss_1: (0.4899) | Acc_1: (81.82%) (38855/47488)\n",
      "Epoch: 137 | Batch_idx: 380 |  Loss_1: (0.4893) | Acc_1: (81.82%) (39904/48768)\n",
      "Epoch: 137 | Batch_idx: 390 |  Loss_1: (0.4886) | Acc_1: (81.84%) (40920/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4830) | Acc: (90.57%) (9057/10000)\n",
      "Epoch: 138 | Batch_idx: 0 |  Loss_1: (0.5083) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 138 | Batch_idx: 10 |  Loss_1: (0.4968) | Acc_1: (81.61%) (1149/1408)\n",
      "Epoch: 138 | Batch_idx: 20 |  Loss_1: (0.4878) | Acc_1: (81.85%) (2200/2688)\n",
      "Epoch: 138 | Batch_idx: 30 |  Loss_1: (0.4691) | Acc_1: (82.54%) (3275/3968)\n",
      "Epoch: 138 | Batch_idx: 40 |  Loss_1: (0.4899) | Acc_1: (81.63%) (4284/5248)\n",
      "Epoch: 138 | Batch_idx: 50 |  Loss_1: (0.4852) | Acc_1: (81.83%) (5342/6528)\n",
      "Epoch: 138 | Batch_idx: 60 |  Loss_1: (0.4869) | Acc_1: (81.79%) (6386/7808)\n",
      "Epoch: 138 | Batch_idx: 70 |  Loss_1: (0.4849) | Acc_1: (81.82%) (7436/9088)\n",
      "Epoch: 138 | Batch_idx: 80 |  Loss_1: (0.4856) | Acc_1: (81.77%) (8478/10368)\n",
      "Epoch: 138 | Batch_idx: 90 |  Loss_1: (0.4877) | Acc_1: (81.70%) (9516/11648)\n",
      "Epoch: 138 | Batch_idx: 100 |  Loss_1: (0.4888) | Acc_1: (81.70%) (10562/12928)\n",
      "Epoch: 138 | Batch_idx: 110 |  Loss_1: (0.4913) | Acc_1: (81.60%) (11594/14208)\n",
      "Epoch: 138 | Batch_idx: 120 |  Loss_1: (0.4912) | Acc_1: (81.57%) (12634/15488)\n",
      "Epoch: 138 | Batch_idx: 130 |  Loss_1: (0.4885) | Acc_1: (81.68%) (13696/16768)\n",
      "Epoch: 138 | Batch_idx: 140 |  Loss_1: (0.4880) | Acc_1: (81.70%) (14746/18048)\n",
      "Epoch: 138 | Batch_idx: 150 |  Loss_1: (0.4890) | Acc_1: (81.67%) (15785/19328)\n",
      "Epoch: 138 | Batch_idx: 160 |  Loss_1: (0.4891) | Acc_1: (81.67%) (16830/20608)\n",
      "Epoch: 138 | Batch_idx: 170 |  Loss_1: (0.4890) | Acc_1: (81.67%) (17877/21888)\n",
      "Epoch: 138 | Batch_idx: 180 |  Loss_1: (0.4882) | Acc_1: (81.70%) (18928/23168)\n",
      "Epoch: 138 | Batch_idx: 190 |  Loss_1: (0.4890) | Acc_1: (81.68%) (19969/24448)\n",
      "Epoch: 138 | Batch_idx: 200 |  Loss_1: (0.4903) | Acc_1: (81.63%) (21003/25728)\n",
      "Epoch: 138 | Batch_idx: 210 |  Loss_1: (0.4917) | Acc_1: (81.61%) (22041/27008)\n",
      "Epoch: 138 | Batch_idx: 220 |  Loss_1: (0.4924) | Acc_1: (81.60%) (23084/28288)\n",
      "Epoch: 138 | Batch_idx: 230 |  Loss_1: (0.4926) | Acc_1: (81.59%) (24124/29568)\n",
      "Epoch: 138 | Batch_idx: 240 |  Loss_1: (0.4915) | Acc_1: (81.63%) (25181/30848)\n",
      "Epoch: 138 | Batch_idx: 250 |  Loss_1: (0.4897) | Acc_1: (81.68%) (26243/32128)\n",
      "Epoch: 138 | Batch_idx: 260 |  Loss_1: (0.4901) | Acc_1: (81.68%) (27286/33408)\n",
      "Epoch: 138 | Batch_idx: 270 |  Loss_1: (0.4907) | Acc_1: (81.67%) (28328/34688)\n",
      "Epoch: 138 | Batch_idx: 280 |  Loss_1: (0.4890) | Acc_1: (81.74%) (29401/35968)\n",
      "Epoch: 138 | Batch_idx: 290 |  Loss_1: (0.4875) | Acc_1: (81.78%) (30461/37248)\n",
      "Epoch: 138 | Batch_idx: 300 |  Loss_1: (0.4886) | Acc_1: (81.74%) (31493/38528)\n",
      "Epoch: 138 | Batch_idx: 310 |  Loss_1: (0.4882) | Acc_1: (81.76%) (32546/39808)\n",
      "Epoch: 138 | Batch_idx: 320 |  Loss_1: (0.4891) | Acc_1: (81.73%) (33580/41088)\n",
      "Epoch: 138 | Batch_idx: 330 |  Loss_1: (0.4884) | Acc_1: (81.76%) (34642/42368)\n",
      "Epoch: 138 | Batch_idx: 340 |  Loss_1: (0.4881) | Acc_1: (81.78%) (35696/43648)\n",
      "Epoch: 138 | Batch_idx: 350 |  Loss_1: (0.4884) | Acc_1: (81.77%) (36738/44928)\n",
      "Epoch: 138 | Batch_idx: 360 |  Loss_1: (0.4892) | Acc_1: (81.75%) (37776/46208)\n",
      "Epoch: 138 | Batch_idx: 370 |  Loss_1: (0.4909) | Acc_1: (81.70%) (38796/47488)\n",
      "Epoch: 138 | Batch_idx: 380 |  Loss_1: (0.4910) | Acc_1: (81.70%) (39844/48768)\n",
      "Epoch: 138 | Batch_idx: 390 |  Loss_1: (0.4894) | Acc_1: (81.75%) (40875/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4850) | Acc: (90.26%) (9026/10000)\n",
      "Epoch: 139 | Batch_idx: 0 |  Loss_1: (0.6091) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 139 | Batch_idx: 10 |  Loss_1: (0.4811) | Acc_1: (82.03%) (1155/1408)\n",
      "Epoch: 139 | Batch_idx: 20 |  Loss_1: (0.5031) | Acc_1: (81.29%) (2185/2688)\n",
      "Epoch: 139 | Batch_idx: 30 |  Loss_1: (0.4813) | Acc_1: (82.13%) (3259/3968)\n",
      "Epoch: 139 | Batch_idx: 40 |  Loss_1: (0.4892) | Acc_1: (81.73%) (4289/5248)\n",
      "Epoch: 139 | Batch_idx: 50 |  Loss_1: (0.4810) | Acc_1: (81.95%) (5350/6528)\n",
      "Epoch: 139 | Batch_idx: 60 |  Loss_1: (0.4816) | Acc_1: (81.93%) (6397/7808)\n",
      "Epoch: 139 | Batch_idx: 70 |  Loss_1: (0.4770) | Acc_1: (82.15%) (7466/9088)\n",
      "Epoch: 139 | Batch_idx: 80 |  Loss_1: (0.4780) | Acc_1: (82.17%) (8519/10368)\n",
      "Epoch: 139 | Batch_idx: 90 |  Loss_1: (0.4752) | Acc_1: (82.27%) (9583/11648)\n",
      "Epoch: 139 | Batch_idx: 100 |  Loss_1: (0.4739) | Acc_1: (82.32%) (10642/12928)\n",
      "Epoch: 139 | Batch_idx: 110 |  Loss_1: (0.4764) | Acc_1: (82.19%) (11678/14208)\n",
      "Epoch: 139 | Batch_idx: 120 |  Loss_1: (0.4823) | Acc_1: (81.99%) (12699/15488)\n",
      "Epoch: 139 | Batch_idx: 130 |  Loss_1: (0.4835) | Acc_1: (81.97%) (13744/16768)\n",
      "Epoch: 139 | Batch_idx: 140 |  Loss_1: (0.4826) | Acc_1: (82.01%) (14802/18048)\n",
      "Epoch: 139 | Batch_idx: 150 |  Loss_1: (0.4804) | Acc_1: (82.11%) (15871/19328)\n",
      "Epoch: 139 | Batch_idx: 160 |  Loss_1: (0.4776) | Acc_1: (82.24%) (16947/20608)\n",
      "Epoch: 139 | Batch_idx: 170 |  Loss_1: (0.4779) | Acc_1: (82.22%) (17996/21888)\n",
      "Epoch: 139 | Batch_idx: 180 |  Loss_1: (0.4796) | Acc_1: (82.16%) (19035/23168)\n",
      "Epoch: 139 | Batch_idx: 190 |  Loss_1: (0.4813) | Acc_1: (82.08%) (20066/24448)\n",
      "Epoch: 139 | Batch_idx: 200 |  Loss_1: (0.4811) | Acc_1: (82.09%) (21121/25728)\n",
      "Epoch: 139 | Batch_idx: 210 |  Loss_1: (0.4834) | Acc_1: (82.00%) (22146/27008)\n",
      "Epoch: 139 | Batch_idx: 220 |  Loss_1: (0.4854) | Acc_1: (81.94%) (23179/28288)\n",
      "Epoch: 139 | Batch_idx: 230 |  Loss_1: (0.4860) | Acc_1: (81.91%) (24220/29568)\n",
      "Epoch: 139 | Batch_idx: 240 |  Loss_1: (0.4870) | Acc_1: (81.88%) (25259/30848)\n",
      "Epoch: 139 | Batch_idx: 250 |  Loss_1: (0.4876) | Acc_1: (81.86%) (26299/32128)\n",
      "Epoch: 139 | Batch_idx: 260 |  Loss_1: (0.4878) | Acc_1: (81.84%) (27342/33408)\n",
      "Epoch: 139 | Batch_idx: 270 |  Loss_1: (0.4873) | Acc_1: (81.87%) (28398/34688)\n",
      "Epoch: 139 | Batch_idx: 280 |  Loss_1: (0.4888) | Acc_1: (81.79%) (29420/35968)\n",
      "Epoch: 139 | Batch_idx: 290 |  Loss_1: (0.4885) | Acc_1: (81.81%) (30471/37248)\n",
      "Epoch: 139 | Batch_idx: 300 |  Loss_1: (0.4901) | Acc_1: (81.73%) (31489/38528)\n",
      "Epoch: 139 | Batch_idx: 310 |  Loss_1: (0.4915) | Acc_1: (81.67%) (32512/39808)\n",
      "Epoch: 139 | Batch_idx: 320 |  Loss_1: (0.4907) | Acc_1: (81.70%) (33568/41088)\n",
      "Epoch: 139 | Batch_idx: 330 |  Loss_1: (0.4900) | Acc_1: (81.73%) (34626/42368)\n",
      "Epoch: 139 | Batch_idx: 340 |  Loss_1: (0.4904) | Acc_1: (81.72%) (35670/43648)\n",
      "Epoch: 139 | Batch_idx: 350 |  Loss_1: (0.4889) | Acc_1: (81.78%) (36740/44928)\n",
      "Epoch: 139 | Batch_idx: 360 |  Loss_1: (0.4896) | Acc_1: (81.76%) (37780/46208)\n",
      "Epoch: 139 | Batch_idx: 370 |  Loss_1: (0.4890) | Acc_1: (81.78%) (38834/47488)\n",
      "Epoch: 139 | Batch_idx: 380 |  Loss_1: (0.4896) | Acc_1: (81.74%) (39862/48768)\n",
      "Epoch: 139 | Batch_idx: 390 |  Loss_1: (0.4886) | Acc_1: (81.77%) (40887/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4795) | Acc: (90.18%) (9018/10000)\n",
      "Epoch: 140 | Batch_idx: 0 |  Loss_1: (0.5818) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 140 | Batch_idx: 10 |  Loss_1: (0.5275) | Acc_1: (80.68%) (1136/1408)\n",
      "Epoch: 140 | Batch_idx: 20 |  Loss_1: (0.4881) | Acc_1: (82.03%) (2205/2688)\n",
      "Epoch: 140 | Batch_idx: 30 |  Loss_1: (0.4848) | Acc_1: (82.13%) (3259/3968)\n",
      "Epoch: 140 | Batch_idx: 40 |  Loss_1: (0.4856) | Acc_1: (81.99%) (4303/5248)\n",
      "Epoch: 140 | Batch_idx: 50 |  Loss_1: (0.4892) | Acc_1: (81.89%) (5346/6528)\n",
      "Epoch: 140 | Batch_idx: 60 |  Loss_1: (0.4876) | Acc_1: (81.95%) (6399/7808)\n",
      "Epoch: 140 | Batch_idx: 70 |  Loss_1: (0.4868) | Acc_1: (82.00%) (7452/9088)\n",
      "Epoch: 140 | Batch_idx: 80 |  Loss_1: (0.4861) | Acc_1: (81.99%) (8501/10368)\n",
      "Epoch: 140 | Batch_idx: 90 |  Loss_1: (0.4874) | Acc_1: (81.92%) (9542/11648)\n",
      "Epoch: 140 | Batch_idx: 100 |  Loss_1: (0.4894) | Acc_1: (81.87%) (10584/12928)\n",
      "Epoch: 140 | Batch_idx: 110 |  Loss_1: (0.4854) | Acc_1: (81.97%) (11646/14208)\n",
      "Epoch: 140 | Batch_idx: 120 |  Loss_1: (0.4894) | Acc_1: (81.82%) (12673/15488)\n",
      "Epoch: 140 | Batch_idx: 130 |  Loss_1: (0.4890) | Acc_1: (81.81%) (13718/16768)\n",
      "Epoch: 140 | Batch_idx: 140 |  Loss_1: (0.4861) | Acc_1: (81.91%) (14784/18048)\n",
      "Epoch: 140 | Batch_idx: 150 |  Loss_1: (0.4864) | Acc_1: (81.89%) (15827/19328)\n",
      "Epoch: 140 | Batch_idx: 160 |  Loss_1: (0.4844) | Acc_1: (81.98%) (16894/20608)\n",
      "Epoch: 140 | Batch_idx: 170 |  Loss_1: (0.4822) | Acc_1: (82.07%) (17963/21888)\n",
      "Epoch: 140 | Batch_idx: 180 |  Loss_1: (0.4815) | Acc_1: (82.11%) (19024/23168)\n",
      "Epoch: 140 | Batch_idx: 190 |  Loss_1: (0.4811) | Acc_1: (82.12%) (20076/24448)\n",
      "Epoch: 140 | Batch_idx: 200 |  Loss_1: (0.4807) | Acc_1: (82.15%) (21135/25728)\n",
      "Epoch: 140 | Batch_idx: 210 |  Loss_1: (0.4824) | Acc_1: (82.09%) (22172/27008)\n",
      "Epoch: 140 | Batch_idx: 220 |  Loss_1: (0.4811) | Acc_1: (82.14%) (23236/28288)\n",
      "Epoch: 140 | Batch_idx: 230 |  Loss_1: (0.4812) | Acc_1: (82.13%) (24284/29568)\n",
      "Epoch: 140 | Batch_idx: 240 |  Loss_1: (0.4809) | Acc_1: (82.12%) (25332/30848)\n",
      "Epoch: 140 | Batch_idx: 250 |  Loss_1: (0.4806) | Acc_1: (82.13%) (26388/32128)\n",
      "Epoch: 140 | Batch_idx: 260 |  Loss_1: (0.4795) | Acc_1: (82.19%) (27457/33408)\n",
      "Epoch: 140 | Batch_idx: 270 |  Loss_1: (0.4800) | Acc_1: (82.16%) (28501/34688)\n",
      "Epoch: 140 | Batch_idx: 280 |  Loss_1: (0.4788) | Acc_1: (82.20%) (29566/35968)\n",
      "Epoch: 140 | Batch_idx: 290 |  Loss_1: (0.4780) | Acc_1: (82.24%) (30631/37248)\n",
      "Epoch: 140 | Batch_idx: 300 |  Loss_1: (0.4787) | Acc_1: (82.20%) (31670/38528)\n",
      "Epoch: 140 | Batch_idx: 310 |  Loss_1: (0.4801) | Acc_1: (82.14%) (32699/39808)\n",
      "Epoch: 140 | Batch_idx: 320 |  Loss_1: (0.4807) | Acc_1: (82.12%) (33741/41088)\n",
      "Epoch: 140 | Batch_idx: 330 |  Loss_1: (0.4802) | Acc_1: (82.12%) (34791/42368)\n",
      "Epoch: 140 | Batch_idx: 340 |  Loss_1: (0.4795) | Acc_1: (82.14%) (35853/43648)\n",
      "Epoch: 140 | Batch_idx: 350 |  Loss_1: (0.4819) | Acc_1: (82.06%) (36867/44928)\n",
      "Epoch: 140 | Batch_idx: 360 |  Loss_1: (0.4832) | Acc_1: (82.01%) (37894/46208)\n",
      "Epoch: 140 | Batch_idx: 370 |  Loss_1: (0.4824) | Acc_1: (82.03%) (38955/47488)\n",
      "Epoch: 140 | Batch_idx: 380 |  Loss_1: (0.4830) | Acc_1: (82.01%) (39995/48768)\n",
      "Epoch: 140 | Batch_idx: 390 |  Loss_1: (0.4827) | Acc_1: (82.01%) (41006/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4651) | Acc: (90.35%) (9035/10000)\n",
      "Epoch: 141 | Batch_idx: 0 |  Loss_1: (0.4729) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 141 | Batch_idx: 10 |  Loss_1: (0.5166) | Acc_1: (80.40%) (1132/1408)\n",
      "Epoch: 141 | Batch_idx: 20 |  Loss_1: (0.4965) | Acc_1: (81.44%) (2189/2688)\n",
      "Epoch: 141 | Batch_idx: 30 |  Loss_1: (0.5157) | Acc_1: (80.80%) (3206/3968)\n",
      "Epoch: 141 | Batch_idx: 40 |  Loss_1: (0.5061) | Acc_1: (81.12%) (4257/5248)\n",
      "Epoch: 141 | Batch_idx: 50 |  Loss_1: (0.4985) | Acc_1: (81.42%) (5315/6528)\n",
      "Epoch: 141 | Batch_idx: 60 |  Loss_1: (0.4969) | Acc_1: (81.43%) (6358/7808)\n",
      "Epoch: 141 | Batch_idx: 70 |  Loss_1: (0.4959) | Acc_1: (81.47%) (7404/9088)\n",
      "Epoch: 141 | Batch_idx: 80 |  Loss_1: (0.4905) | Acc_1: (81.70%) (8471/10368)\n",
      "Epoch: 141 | Batch_idx: 90 |  Loss_1: (0.4934) | Acc_1: (81.53%) (9497/11648)\n",
      "Epoch: 141 | Batch_idx: 100 |  Loss_1: (0.4913) | Acc_1: (81.58%) (10547/12928)\n",
      "Epoch: 141 | Batch_idx: 110 |  Loss_1: (0.4895) | Acc_1: (81.62%) (11596/14208)\n",
      "Epoch: 141 | Batch_idx: 120 |  Loss_1: (0.4885) | Acc_1: (81.64%) (12645/15488)\n",
      "Epoch: 141 | Batch_idx: 130 |  Loss_1: (0.4891) | Acc_1: (81.66%) (13693/16768)\n",
      "Epoch: 141 | Batch_idx: 140 |  Loss_1: (0.4892) | Acc_1: (81.67%) (14740/18048)\n",
      "Epoch: 141 | Batch_idx: 150 |  Loss_1: (0.4921) | Acc_1: (81.56%) (15764/19328)\n",
      "Epoch: 141 | Batch_idx: 160 |  Loss_1: (0.4926) | Acc_1: (81.54%) (16803/20608)\n",
      "Epoch: 141 | Batch_idx: 170 |  Loss_1: (0.4922) | Acc_1: (81.56%) (17852/21888)\n",
      "Epoch: 141 | Batch_idx: 180 |  Loss_1: (0.4920) | Acc_1: (81.59%) (18903/23168)\n",
      "Epoch: 141 | Batch_idx: 190 |  Loss_1: (0.4911) | Acc_1: (81.62%) (19955/24448)\n",
      "Epoch: 141 | Batch_idx: 200 |  Loss_1: (0.4891) | Acc_1: (81.71%) (21023/25728)\n",
      "Epoch: 141 | Batch_idx: 210 |  Loss_1: (0.4884) | Acc_1: (81.75%) (22078/27008)\n",
      "Epoch: 141 | Batch_idx: 220 |  Loss_1: (0.4873) | Acc_1: (81.79%) (23136/28288)\n",
      "Epoch: 141 | Batch_idx: 230 |  Loss_1: (0.4884) | Acc_1: (81.76%) (24174/29568)\n",
      "Epoch: 141 | Batch_idx: 240 |  Loss_1: (0.4882) | Acc_1: (81.77%) (25225/30848)\n",
      "Epoch: 141 | Batch_idx: 250 |  Loss_1: (0.4869) | Acc_1: (81.82%) (26286/32128)\n",
      "Epoch: 141 | Batch_idx: 260 |  Loss_1: (0.4882) | Acc_1: (81.75%) (27311/33408)\n",
      "Epoch: 141 | Batch_idx: 270 |  Loss_1: (0.4913) | Acc_1: (81.63%) (28317/34688)\n",
      "Epoch: 141 | Batch_idx: 280 |  Loss_1: (0.4894) | Acc_1: (81.70%) (29385/35968)\n",
      "Epoch: 141 | Batch_idx: 290 |  Loss_1: (0.4889) | Acc_1: (81.72%) (30440/37248)\n",
      "Epoch: 141 | Batch_idx: 300 |  Loss_1: (0.4895) | Acc_1: (81.70%) (31479/38528)\n",
      "Epoch: 141 | Batch_idx: 310 |  Loss_1: (0.4888) | Acc_1: (81.72%) (32531/39808)\n",
      "Epoch: 141 | Batch_idx: 320 |  Loss_1: (0.4895) | Acc_1: (81.69%) (33564/41088)\n",
      "Epoch: 141 | Batch_idx: 330 |  Loss_1: (0.4885) | Acc_1: (81.71%) (34621/42368)\n",
      "Epoch: 141 | Batch_idx: 340 |  Loss_1: (0.4891) | Acc_1: (81.70%) (35662/43648)\n",
      "Epoch: 141 | Batch_idx: 350 |  Loss_1: (0.4877) | Acc_1: (81.78%) (36740/44928)\n",
      "Epoch: 141 | Batch_idx: 360 |  Loss_1: (0.4879) | Acc_1: (81.78%) (37789/46208)\n",
      "Epoch: 141 | Batch_idx: 370 |  Loss_1: (0.4873) | Acc_1: (81.80%) (38843/47488)\n",
      "Epoch: 141 | Batch_idx: 380 |  Loss_1: (0.4880) | Acc_1: (81.77%) (39880/48768)\n",
      "Epoch: 141 | Batch_idx: 390 |  Loss_1: (0.4884) | Acc_1: (81.76%) (40878/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4762) | Acc: (90.79%) (9079/10000)\n",
      "Epoch: 142 | Batch_idx: 0 |  Loss_1: (0.3532) | Acc_1: (86.72%) (111/128)\n",
      "Epoch: 142 | Batch_idx: 10 |  Loss_1: (0.4890) | Acc_1: (82.17%) (1157/1408)\n",
      "Epoch: 142 | Batch_idx: 20 |  Loss_1: (0.4826) | Acc_1: (82.25%) (2211/2688)\n",
      "Epoch: 142 | Batch_idx: 30 |  Loss_1: (0.4937) | Acc_1: (81.70%) (3242/3968)\n",
      "Epoch: 142 | Batch_idx: 40 |  Loss_1: (0.4888) | Acc_1: (81.90%) (4298/5248)\n",
      "Epoch: 142 | Batch_idx: 50 |  Loss_1: (0.4969) | Acc_1: (81.51%) (5321/6528)\n",
      "Epoch: 142 | Batch_idx: 60 |  Loss_1: (0.4911) | Acc_1: (81.72%) (6381/7808)\n",
      "Epoch: 142 | Batch_idx: 70 |  Loss_1: (0.4944) | Acc_1: (81.60%) (7416/9088)\n",
      "Epoch: 142 | Batch_idx: 80 |  Loss_1: (0.4863) | Acc_1: (81.87%) (8488/10368)\n",
      "Epoch: 142 | Batch_idx: 90 |  Loss_1: (0.4816) | Acc_1: (82.07%) (9559/11648)\n",
      "Epoch: 142 | Batch_idx: 100 |  Loss_1: (0.4802) | Acc_1: (82.12%) (10616/12928)\n",
      "Epoch: 142 | Batch_idx: 110 |  Loss_1: (0.4793) | Acc_1: (82.18%) (11676/14208)\n",
      "Epoch: 142 | Batch_idx: 120 |  Loss_1: (0.4807) | Acc_1: (82.10%) (12716/15488)\n",
      "Epoch: 142 | Batch_idx: 130 |  Loss_1: (0.4825) | Acc_1: (82.06%) (13760/16768)\n",
      "Epoch: 142 | Batch_idx: 140 |  Loss_1: (0.4806) | Acc_1: (82.14%) (14824/18048)\n",
      "Epoch: 142 | Batch_idx: 150 |  Loss_1: (0.4813) | Acc_1: (82.17%) (15882/19328)\n",
      "Epoch: 142 | Batch_idx: 160 |  Loss_1: (0.4793) | Acc_1: (82.27%) (16955/20608)\n",
      "Epoch: 142 | Batch_idx: 170 |  Loss_1: (0.4786) | Acc_1: (82.29%) (18011/21888)\n",
      "Epoch: 142 | Batch_idx: 180 |  Loss_1: (0.4774) | Acc_1: (82.30%) (19068/23168)\n",
      "Epoch: 142 | Batch_idx: 190 |  Loss_1: (0.4767) | Acc_1: (82.32%) (20126/24448)\n",
      "Epoch: 142 | Batch_idx: 200 |  Loss_1: (0.4787) | Acc_1: (82.26%) (21164/25728)\n",
      "Epoch: 142 | Batch_idx: 210 |  Loss_1: (0.4764) | Acc_1: (82.34%) (22239/27008)\n",
      "Epoch: 142 | Batch_idx: 220 |  Loss_1: (0.4774) | Acc_1: (82.31%) (23285/28288)\n",
      "Epoch: 142 | Batch_idx: 230 |  Loss_1: (0.4779) | Acc_1: (82.30%) (24334/29568)\n",
      "Epoch: 142 | Batch_idx: 240 |  Loss_1: (0.4767) | Acc_1: (82.36%) (25406/30848)\n",
      "Epoch: 142 | Batch_idx: 250 |  Loss_1: (0.4771) | Acc_1: (82.32%) (26449/32128)\n",
      "Epoch: 142 | Batch_idx: 260 |  Loss_1: (0.4786) | Acc_1: (82.28%) (27488/33408)\n",
      "Epoch: 142 | Batch_idx: 270 |  Loss_1: (0.4797) | Acc_1: (82.24%) (28527/34688)\n",
      "Epoch: 142 | Batch_idx: 280 |  Loss_1: (0.4792) | Acc_1: (82.24%) (29581/35968)\n",
      "Epoch: 142 | Batch_idx: 290 |  Loss_1: (0.4795) | Acc_1: (82.24%) (30634/37248)\n",
      "Epoch: 142 | Batch_idx: 300 |  Loss_1: (0.4806) | Acc_1: (82.18%) (31663/38528)\n",
      "Epoch: 142 | Batch_idx: 310 |  Loss_1: (0.4808) | Acc_1: (82.17%) (32709/39808)\n",
      "Epoch: 142 | Batch_idx: 320 |  Loss_1: (0.4808) | Acc_1: (82.17%) (33762/41088)\n",
      "Epoch: 142 | Batch_idx: 330 |  Loss_1: (0.4823) | Acc_1: (82.12%) (34794/42368)\n",
      "Epoch: 142 | Batch_idx: 340 |  Loss_1: (0.4809) | Acc_1: (82.18%) (35869/43648)\n",
      "Epoch: 142 | Batch_idx: 350 |  Loss_1: (0.4799) | Acc_1: (82.21%) (36935/44928)\n",
      "Epoch: 142 | Batch_idx: 360 |  Loss_1: (0.4789) | Acc_1: (82.25%) (38005/46208)\n",
      "Epoch: 142 | Batch_idx: 370 |  Loss_1: (0.4814) | Acc_1: (82.15%) (39010/47488)\n",
      "Epoch: 142 | Batch_idx: 380 |  Loss_1: (0.4820) | Acc_1: (82.13%) (40054/48768)\n",
      "Epoch: 142 | Batch_idx: 390 |  Loss_1: (0.4811) | Acc_1: (82.17%) (41084/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4999) | Acc: (90.50%) (9050/10000)\n",
      "Epoch: 143 | Batch_idx: 0 |  Loss_1: (0.5394) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 143 | Batch_idx: 10 |  Loss_1: (0.5092) | Acc_1: (80.89%) (1139/1408)\n",
      "Epoch: 143 | Batch_idx: 20 |  Loss_1: (0.4987) | Acc_1: (81.18%) (2182/2688)\n",
      "Epoch: 143 | Batch_idx: 30 |  Loss_1: (0.4935) | Acc_1: (81.50%) (3234/3968)\n",
      "Epoch: 143 | Batch_idx: 40 |  Loss_1: (0.5026) | Acc_1: (81.15%) (4259/5248)\n",
      "Epoch: 143 | Batch_idx: 50 |  Loss_1: (0.5065) | Acc_1: (81.08%) (5293/6528)\n",
      "Epoch: 143 | Batch_idx: 60 |  Loss_1: (0.5087) | Acc_1: (81.03%) (6327/7808)\n",
      "Epoch: 143 | Batch_idx: 70 |  Loss_1: (0.5049) | Acc_1: (81.13%) (7373/9088)\n",
      "Epoch: 143 | Batch_idx: 80 |  Loss_1: (0.5072) | Acc_1: (81.02%) (8400/10368)\n",
      "Epoch: 143 | Batch_idx: 90 |  Loss_1: (0.5034) | Acc_1: (81.18%) (9456/11648)\n",
      "Epoch: 143 | Batch_idx: 100 |  Loss_1: (0.5027) | Acc_1: (81.24%) (10503/12928)\n",
      "Epoch: 143 | Batch_idx: 110 |  Loss_1: (0.5008) | Acc_1: (81.34%) (11557/14208)\n",
      "Epoch: 143 | Batch_idx: 120 |  Loss_1: (0.4992) | Acc_1: (81.40%) (12608/15488)\n",
      "Epoch: 143 | Batch_idx: 130 |  Loss_1: (0.4976) | Acc_1: (81.48%) (13662/16768)\n",
      "Epoch: 143 | Batch_idx: 140 |  Loss_1: (0.4966) | Acc_1: (81.50%) (14709/18048)\n",
      "Epoch: 143 | Batch_idx: 150 |  Loss_1: (0.4936) | Acc_1: (81.60%) (15772/19328)\n",
      "Epoch: 143 | Batch_idx: 160 |  Loss_1: (0.4902) | Acc_1: (81.69%) (16835/20608)\n",
      "Epoch: 143 | Batch_idx: 170 |  Loss_1: (0.4908) | Acc_1: (81.66%) (17873/21888)\n",
      "Epoch: 143 | Batch_idx: 180 |  Loss_1: (0.4913) | Acc_1: (81.65%) (18917/23168)\n",
      "Epoch: 143 | Batch_idx: 190 |  Loss_1: (0.4940) | Acc_1: (81.55%) (19937/24448)\n",
      "Epoch: 143 | Batch_idx: 200 |  Loss_1: (0.4919) | Acc_1: (81.64%) (21004/25728)\n",
      "Epoch: 143 | Batch_idx: 210 |  Loss_1: (0.4888) | Acc_1: (81.77%) (22085/27008)\n",
      "Epoch: 143 | Batch_idx: 220 |  Loss_1: (0.4882) | Acc_1: (81.81%) (23142/28288)\n",
      "Epoch: 143 | Batch_idx: 230 |  Loss_1: (0.4874) | Acc_1: (81.84%) (24199/29568)\n",
      "Epoch: 143 | Batch_idx: 240 |  Loss_1: (0.4886) | Acc_1: (81.81%) (25237/30848)\n",
      "Epoch: 143 | Batch_idx: 250 |  Loss_1: (0.4882) | Acc_1: (81.82%) (26286/32128)\n",
      "Epoch: 143 | Batch_idx: 260 |  Loss_1: (0.4876) | Acc_1: (81.82%) (27334/33408)\n",
      "Epoch: 143 | Batch_idx: 270 |  Loss_1: (0.4892) | Acc_1: (81.75%) (28357/34688)\n",
      "Epoch: 143 | Batch_idx: 280 |  Loss_1: (0.4905) | Acc_1: (81.71%) (29388/35968)\n",
      "Epoch: 143 | Batch_idx: 290 |  Loss_1: (0.4905) | Acc_1: (81.69%) (30427/37248)\n",
      "Epoch: 143 | Batch_idx: 300 |  Loss_1: (0.4900) | Acc_1: (81.70%) (31478/38528)\n",
      "Epoch: 143 | Batch_idx: 310 |  Loss_1: (0.4915) | Acc_1: (81.65%) (32504/39808)\n",
      "Epoch: 143 | Batch_idx: 320 |  Loss_1: (0.4914) | Acc_1: (81.67%) (33558/41088)\n",
      "Epoch: 143 | Batch_idx: 330 |  Loss_1: (0.4927) | Acc_1: (81.62%) (34582/42368)\n",
      "Epoch: 143 | Batch_idx: 340 |  Loss_1: (0.4926) | Acc_1: (81.62%) (35627/43648)\n",
      "Epoch: 143 | Batch_idx: 350 |  Loss_1: (0.4934) | Acc_1: (81.59%) (36658/44928)\n",
      "Epoch: 143 | Batch_idx: 360 |  Loss_1: (0.4920) | Acc_1: (81.64%) (37725/46208)\n",
      "Epoch: 143 | Batch_idx: 370 |  Loss_1: (0.4924) | Acc_1: (81.63%) (38764/47488)\n",
      "Epoch: 143 | Batch_idx: 380 |  Loss_1: (0.4923) | Acc_1: (81.62%) (39806/48768)\n",
      "Epoch: 143 | Batch_idx: 390 |  Loss_1: (0.4923) | Acc_1: (81.63%) (40813/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4740) | Acc: (90.23%) (9023/10000)\n",
      "Epoch: 144 | Batch_idx: 0 |  Loss_1: (0.3824) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 144 | Batch_idx: 10 |  Loss_1: (0.4274) | Acc_1: (83.95%) (1182/1408)\n",
      "Epoch: 144 | Batch_idx: 20 |  Loss_1: (0.4554) | Acc_1: (82.66%) (2222/2688)\n",
      "Epoch: 144 | Batch_idx: 30 |  Loss_1: (0.4682) | Acc_1: (82.38%) (3269/3968)\n",
      "Epoch: 144 | Batch_idx: 40 |  Loss_1: (0.4634) | Acc_1: (82.62%) (4336/5248)\n",
      "Epoch: 144 | Batch_idx: 50 |  Loss_1: (0.4720) | Acc_1: (82.28%) (5371/6528)\n",
      "Epoch: 144 | Batch_idx: 60 |  Loss_1: (0.4776) | Acc_1: (82.04%) (6406/7808)\n",
      "Epoch: 144 | Batch_idx: 70 |  Loss_1: (0.4798) | Acc_1: (82.03%) (7455/9088)\n",
      "Epoch: 144 | Batch_idx: 80 |  Loss_1: (0.4858) | Acc_1: (81.81%) (8482/10368)\n",
      "Epoch: 144 | Batch_idx: 90 |  Loss_1: (0.4815) | Acc_1: (81.95%) (9546/11648)\n",
      "Epoch: 144 | Batch_idx: 100 |  Loss_1: (0.4814) | Acc_1: (81.97%) (10597/12928)\n",
      "Epoch: 144 | Batch_idx: 110 |  Loss_1: (0.4826) | Acc_1: (81.92%) (11639/14208)\n",
      "Epoch: 144 | Batch_idx: 120 |  Loss_1: (0.4832) | Acc_1: (81.91%) (12686/15488)\n",
      "Epoch: 144 | Batch_idx: 130 |  Loss_1: (0.4862) | Acc_1: (81.80%) (13716/16768)\n",
      "Epoch: 144 | Batch_idx: 140 |  Loss_1: (0.4871) | Acc_1: (81.74%) (14753/18048)\n",
      "Epoch: 144 | Batch_idx: 150 |  Loss_1: (0.4856) | Acc_1: (81.82%) (15814/19328)\n",
      "Epoch: 144 | Batch_idx: 160 |  Loss_1: (0.4867) | Acc_1: (81.79%) (16856/20608)\n",
      "Epoch: 144 | Batch_idx: 170 |  Loss_1: (0.4870) | Acc_1: (81.78%) (17900/21888)\n",
      "Epoch: 144 | Batch_idx: 180 |  Loss_1: (0.4879) | Acc_1: (81.77%) (18944/23168)\n",
      "Epoch: 144 | Batch_idx: 190 |  Loss_1: (0.4844) | Acc_1: (81.88%) (20018/24448)\n",
      "Epoch: 144 | Batch_idx: 200 |  Loss_1: (0.4867) | Acc_1: (81.79%) (21042/25728)\n",
      "Epoch: 144 | Batch_idx: 210 |  Loss_1: (0.4859) | Acc_1: (81.83%) (22100/27008)\n",
      "Epoch: 144 | Batch_idx: 220 |  Loss_1: (0.4874) | Acc_1: (81.76%) (23129/28288)\n",
      "Epoch: 144 | Batch_idx: 230 |  Loss_1: (0.4853) | Acc_1: (81.85%) (24200/29568)\n",
      "Epoch: 144 | Batch_idx: 240 |  Loss_1: (0.4870) | Acc_1: (81.80%) (25234/30848)\n",
      "Epoch: 144 | Batch_idx: 250 |  Loss_1: (0.4880) | Acc_1: (81.79%) (26276/32128)\n",
      "Epoch: 144 | Batch_idx: 260 |  Loss_1: (0.4891) | Acc_1: (81.76%) (27314/33408)\n",
      "Epoch: 144 | Batch_idx: 270 |  Loss_1: (0.4878) | Acc_1: (81.81%) (28378/34688)\n",
      "Epoch: 144 | Batch_idx: 280 |  Loss_1: (0.4866) | Acc_1: (81.85%) (29440/35968)\n",
      "Epoch: 144 | Batch_idx: 290 |  Loss_1: (0.4861) | Acc_1: (81.86%) (30493/37248)\n",
      "Epoch: 144 | Batch_idx: 300 |  Loss_1: (0.4872) | Acc_1: (81.82%) (31525/38528)\n",
      "Epoch: 144 | Batch_idx: 310 |  Loss_1: (0.4856) | Acc_1: (81.86%) (32587/39808)\n",
      "Epoch: 144 | Batch_idx: 320 |  Loss_1: (0.4865) | Acc_1: (81.83%) (33623/41088)\n",
      "Epoch: 144 | Batch_idx: 330 |  Loss_1: (0.4863) | Acc_1: (81.84%) (34674/42368)\n",
      "Epoch: 144 | Batch_idx: 340 |  Loss_1: (0.4856) | Acc_1: (81.87%) (35733/43648)\n",
      "Epoch: 144 | Batch_idx: 350 |  Loss_1: (0.4860) | Acc_1: (81.87%) (36781/44928)\n",
      "Epoch: 144 | Batch_idx: 360 |  Loss_1: (0.4859) | Acc_1: (81.86%) (37827/46208)\n",
      "Epoch: 144 | Batch_idx: 370 |  Loss_1: (0.4868) | Acc_1: (81.83%) (38858/47488)\n",
      "Epoch: 144 | Batch_idx: 380 |  Loss_1: (0.4876) | Acc_1: (81.80%) (39892/48768)\n",
      "Epoch: 144 | Batch_idx: 390 |  Loss_1: (0.4886) | Acc_1: (81.78%) (40889/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4546) | Acc: (90.85%) (9085/10000)\n",
      "Epoch: 145 | Batch_idx: 0 |  Loss_1: (0.3987) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 145 | Batch_idx: 10 |  Loss_1: (0.4692) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 145 | Batch_idx: 20 |  Loss_1: (0.4727) | Acc_1: (82.51%) (2218/2688)\n",
      "Epoch: 145 | Batch_idx: 30 |  Loss_1: (0.4806) | Acc_1: (82.28%) (3265/3968)\n",
      "Epoch: 145 | Batch_idx: 40 |  Loss_1: (0.4835) | Acc_1: (82.26%) (4317/5248)\n",
      "Epoch: 145 | Batch_idx: 50 |  Loss_1: (0.4884) | Acc_1: (81.97%) (5351/6528)\n",
      "Epoch: 145 | Batch_idx: 60 |  Loss_1: (0.4895) | Acc_1: (81.90%) (6395/7808)\n",
      "Epoch: 145 | Batch_idx: 70 |  Loss_1: (0.4836) | Acc_1: (82.06%) (7458/9088)\n",
      "Epoch: 145 | Batch_idx: 80 |  Loss_1: (0.4874) | Acc_1: (81.92%) (8493/10368)\n",
      "Epoch: 145 | Batch_idx: 90 |  Loss_1: (0.4899) | Acc_1: (81.83%) (9531/11648)\n",
      "Epoch: 145 | Batch_idx: 100 |  Loss_1: (0.4916) | Acc_1: (81.78%) (10572/12928)\n",
      "Epoch: 145 | Batch_idx: 110 |  Loss_1: (0.4904) | Acc_1: (81.84%) (11628/14208)\n",
      "Epoch: 145 | Batch_idx: 120 |  Loss_1: (0.4891) | Acc_1: (81.90%) (12684/15488)\n",
      "Epoch: 145 | Batch_idx: 130 |  Loss_1: (0.4850) | Acc_1: (82.05%) (13758/16768)\n",
      "Epoch: 145 | Batch_idx: 140 |  Loss_1: (0.4837) | Acc_1: (82.06%) (14811/18048)\n",
      "Epoch: 145 | Batch_idx: 150 |  Loss_1: (0.4823) | Acc_1: (82.10%) (15869/19328)\n",
      "Epoch: 145 | Batch_idx: 160 |  Loss_1: (0.4823) | Acc_1: (82.11%) (16921/20608)\n",
      "Epoch: 145 | Batch_idx: 170 |  Loss_1: (0.4831) | Acc_1: (82.08%) (17966/21888)\n",
      "Epoch: 145 | Batch_idx: 180 |  Loss_1: (0.4840) | Acc_1: (82.05%) (19009/23168)\n",
      "Epoch: 145 | Batch_idx: 190 |  Loss_1: (0.4839) | Acc_1: (82.04%) (20057/24448)\n",
      "Epoch: 145 | Batch_idx: 200 |  Loss_1: (0.4860) | Acc_1: (81.95%) (21084/25728)\n",
      "Epoch: 145 | Batch_idx: 210 |  Loss_1: (0.4867) | Acc_1: (81.93%) (22127/27008)\n",
      "Epoch: 145 | Batch_idx: 220 |  Loss_1: (0.4868) | Acc_1: (81.94%) (23178/28288)\n",
      "Epoch: 145 | Batch_idx: 230 |  Loss_1: (0.4895) | Acc_1: (81.85%) (24201/29568)\n",
      "Epoch: 145 | Batch_idx: 240 |  Loss_1: (0.4874) | Acc_1: (81.93%) (25274/30848)\n",
      "Epoch: 145 | Batch_idx: 250 |  Loss_1: (0.4847) | Acc_1: (82.01%) (26349/32128)\n",
      "Epoch: 145 | Batch_idx: 260 |  Loss_1: (0.4850) | Acc_1: (82.01%) (27398/33408)\n",
      "Epoch: 145 | Batch_idx: 270 |  Loss_1: (0.4870) | Acc_1: (81.92%) (28418/34688)\n",
      "Epoch: 145 | Batch_idx: 280 |  Loss_1: (0.4884) | Acc_1: (81.86%) (29443/35968)\n",
      "Epoch: 145 | Batch_idx: 290 |  Loss_1: (0.4885) | Acc_1: (81.85%) (30489/37248)\n",
      "Epoch: 145 | Batch_idx: 300 |  Loss_1: (0.4877) | Acc_1: (81.88%) (31547/38528)\n",
      "Epoch: 145 | Batch_idx: 310 |  Loss_1: (0.4880) | Acc_1: (81.86%) (32588/39808)\n",
      "Epoch: 145 | Batch_idx: 320 |  Loss_1: (0.4887) | Acc_1: (81.86%) (33634/41088)\n",
      "Epoch: 145 | Batch_idx: 330 |  Loss_1: (0.4900) | Acc_1: (81.81%) (34661/42368)\n",
      "Epoch: 145 | Batch_idx: 340 |  Loss_1: (0.4892) | Acc_1: (81.84%) (35720/43648)\n",
      "Epoch: 145 | Batch_idx: 350 |  Loss_1: (0.4889) | Acc_1: (81.84%) (36771/44928)\n",
      "Epoch: 145 | Batch_idx: 360 |  Loss_1: (0.4881) | Acc_1: (81.88%) (37834/46208)\n",
      "Epoch: 145 | Batch_idx: 370 |  Loss_1: (0.4890) | Acc_1: (81.84%) (38865/47488)\n",
      "Epoch: 145 | Batch_idx: 380 |  Loss_1: (0.4901) | Acc_1: (81.79%) (39888/48768)\n",
      "Epoch: 145 | Batch_idx: 390 |  Loss_1: (0.4895) | Acc_1: (81.81%) (40907/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4949) | Acc: (90.27%) (9027/10000)\n",
      "Epoch: 146 | Batch_idx: 0 |  Loss_1: (0.4696) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 146 | Batch_idx: 10 |  Loss_1: (0.5118) | Acc_1: (80.68%) (1136/1408)\n",
      "Epoch: 146 | Batch_idx: 20 |  Loss_1: (0.5092) | Acc_1: (81.06%) (2179/2688)\n",
      "Epoch: 146 | Batch_idx: 30 |  Loss_1: (0.5066) | Acc_1: (81.17%) (3221/3968)\n",
      "Epoch: 146 | Batch_idx: 40 |  Loss_1: (0.5002) | Acc_1: (81.52%) (4278/5248)\n",
      "Epoch: 146 | Batch_idx: 50 |  Loss_1: (0.4930) | Acc_1: (81.74%) (5336/6528)\n",
      "Epoch: 146 | Batch_idx: 60 |  Loss_1: (0.4897) | Acc_1: (81.88%) (6393/7808)\n",
      "Epoch: 146 | Batch_idx: 70 |  Loss_1: (0.4901) | Acc_1: (81.83%) (7437/9088)\n",
      "Epoch: 146 | Batch_idx: 80 |  Loss_1: (0.4857) | Acc_1: (81.93%) (8495/10368)\n",
      "Epoch: 146 | Batch_idx: 90 |  Loss_1: (0.4851) | Acc_1: (81.96%) (9547/11648)\n",
      "Epoch: 146 | Batch_idx: 100 |  Loss_1: (0.4849) | Acc_1: (82.02%) (10603/12928)\n",
      "Epoch: 146 | Batch_idx: 110 |  Loss_1: (0.4806) | Acc_1: (82.19%) (11677/14208)\n",
      "Epoch: 146 | Batch_idx: 120 |  Loss_1: (0.4791) | Acc_1: (82.24%) (12737/15488)\n",
      "Epoch: 146 | Batch_idx: 130 |  Loss_1: (0.4796) | Acc_1: (82.21%) (13785/16768)\n",
      "Epoch: 146 | Batch_idx: 140 |  Loss_1: (0.4782) | Acc_1: (82.32%) (14857/18048)\n",
      "Epoch: 146 | Batch_idx: 150 |  Loss_1: (0.4788) | Acc_1: (82.28%) (15903/19328)\n",
      "Epoch: 146 | Batch_idx: 160 |  Loss_1: (0.4815) | Acc_1: (82.15%) (16930/20608)\n",
      "Epoch: 146 | Batch_idx: 170 |  Loss_1: (0.4830) | Acc_1: (82.05%) (17960/21888)\n",
      "Epoch: 146 | Batch_idx: 180 |  Loss_1: (0.4859) | Acc_1: (81.94%) (18984/23168)\n",
      "Epoch: 146 | Batch_idx: 190 |  Loss_1: (0.4843) | Acc_1: (82.01%) (20049/24448)\n",
      "Epoch: 146 | Batch_idx: 200 |  Loss_1: (0.4867) | Acc_1: (81.92%) (21076/25728)\n",
      "Epoch: 146 | Batch_idx: 210 |  Loss_1: (0.4892) | Acc_1: (81.82%) (22098/27008)\n",
      "Epoch: 146 | Batch_idx: 220 |  Loss_1: (0.4872) | Acc_1: (81.88%) (23162/28288)\n",
      "Epoch: 146 | Batch_idx: 230 |  Loss_1: (0.4889) | Acc_1: (81.82%) (24192/29568)\n",
      "Epoch: 146 | Batch_idx: 240 |  Loss_1: (0.4888) | Acc_1: (81.83%) (25242/30848)\n",
      "Epoch: 146 | Batch_idx: 250 |  Loss_1: (0.4879) | Acc_1: (81.84%) (26295/32128)\n",
      "Epoch: 146 | Batch_idx: 260 |  Loss_1: (0.4879) | Acc_1: (81.83%) (27339/33408)\n",
      "Epoch: 146 | Batch_idx: 270 |  Loss_1: (0.4892) | Acc_1: (81.78%) (28367/34688)\n",
      "Epoch: 146 | Batch_idx: 280 |  Loss_1: (0.4884) | Acc_1: (81.81%) (29424/35968)\n",
      "Epoch: 146 | Batch_idx: 290 |  Loss_1: (0.4882) | Acc_1: (81.82%) (30477/37248)\n",
      "Epoch: 146 | Batch_idx: 300 |  Loss_1: (0.4871) | Acc_1: (81.85%) (31537/38528)\n",
      "Epoch: 146 | Batch_idx: 310 |  Loss_1: (0.4862) | Acc_1: (81.91%) (32605/39808)\n",
      "Epoch: 146 | Batch_idx: 320 |  Loss_1: (0.4857) | Acc_1: (81.93%) (33664/41088)\n",
      "Epoch: 146 | Batch_idx: 330 |  Loss_1: (0.4875) | Acc_1: (81.86%) (34684/42368)\n",
      "Epoch: 146 | Batch_idx: 340 |  Loss_1: (0.4870) | Acc_1: (81.89%) (35742/43648)\n",
      "Epoch: 146 | Batch_idx: 350 |  Loss_1: (0.4870) | Acc_1: (81.89%) (36792/44928)\n",
      "Epoch: 146 | Batch_idx: 360 |  Loss_1: (0.4875) | Acc_1: (81.87%) (37832/46208)\n",
      "Epoch: 146 | Batch_idx: 370 |  Loss_1: (0.4879) | Acc_1: (81.85%) (38869/47488)\n",
      "Epoch: 146 | Batch_idx: 380 |  Loss_1: (0.4874) | Acc_1: (81.87%) (39927/48768)\n",
      "Epoch: 146 | Batch_idx: 390 |  Loss_1: (0.4871) | Acc_1: (81.88%) (40940/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4451) | Acc: (90.42%) (9042/10000)\n",
      "Epoch: 147 | Batch_idx: 0 |  Loss_1: (0.4814) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 147 | Batch_idx: 10 |  Loss_1: (0.4634) | Acc_1: (82.81%) (1166/1408)\n",
      "Epoch: 147 | Batch_idx: 20 |  Loss_1: (0.4897) | Acc_1: (81.66%) (2195/2688)\n",
      "Epoch: 147 | Batch_idx: 30 |  Loss_1: (0.4942) | Acc_1: (81.33%) (3227/3968)\n",
      "Epoch: 147 | Batch_idx: 40 |  Loss_1: (0.4970) | Acc_1: (81.35%) (4269/5248)\n",
      "Epoch: 147 | Batch_idx: 50 |  Loss_1: (0.4975) | Acc_1: (81.30%) (5307/6528)\n",
      "Epoch: 147 | Batch_idx: 60 |  Loss_1: (0.5000) | Acc_1: (81.29%) (6347/7808)\n",
      "Epoch: 147 | Batch_idx: 70 |  Loss_1: (0.4988) | Acc_1: (81.40%) (7398/9088)\n",
      "Epoch: 147 | Batch_idx: 80 |  Loss_1: (0.5030) | Acc_1: (81.23%) (8422/10368)\n",
      "Epoch: 147 | Batch_idx: 90 |  Loss_1: (0.5000) | Acc_1: (81.27%) (9466/11648)\n",
      "Epoch: 147 | Batch_idx: 100 |  Loss_1: (0.4927) | Acc_1: (81.54%) (10542/12928)\n",
      "Epoch: 147 | Batch_idx: 110 |  Loss_1: (0.4905) | Acc_1: (81.64%) (11600/14208)\n",
      "Epoch: 147 | Batch_idx: 120 |  Loss_1: (0.4919) | Acc_1: (81.57%) (12634/15488)\n",
      "Epoch: 147 | Batch_idx: 130 |  Loss_1: (0.4924) | Acc_1: (81.57%) (13677/16768)\n",
      "Epoch: 147 | Batch_idx: 140 |  Loss_1: (0.4893) | Acc_1: (81.68%) (14741/18048)\n",
      "Epoch: 147 | Batch_idx: 150 |  Loss_1: (0.4898) | Acc_1: (81.62%) (15776/19328)\n",
      "Epoch: 147 | Batch_idx: 160 |  Loss_1: (0.4938) | Acc_1: (81.51%) (16798/20608)\n",
      "Epoch: 147 | Batch_idx: 170 |  Loss_1: (0.4945) | Acc_1: (81.49%) (17837/21888)\n",
      "Epoch: 147 | Batch_idx: 180 |  Loss_1: (0.4939) | Acc_1: (81.50%) (18883/23168)\n",
      "Epoch: 147 | Batch_idx: 190 |  Loss_1: (0.4929) | Acc_1: (81.54%) (19936/24448)\n",
      "Epoch: 147 | Batch_idx: 200 |  Loss_1: (0.4918) | Acc_1: (81.60%) (20995/25728)\n",
      "Epoch: 147 | Batch_idx: 210 |  Loss_1: (0.4916) | Acc_1: (81.62%) (22044/27008)\n",
      "Epoch: 147 | Batch_idx: 220 |  Loss_1: (0.4898) | Acc_1: (81.64%) (23094/28288)\n",
      "Epoch: 147 | Batch_idx: 230 |  Loss_1: (0.4891) | Acc_1: (81.68%) (24150/29568)\n",
      "Epoch: 147 | Batch_idx: 240 |  Loss_1: (0.4905) | Acc_1: (81.63%) (25181/30848)\n",
      "Epoch: 147 | Batch_idx: 250 |  Loss_1: (0.4896) | Acc_1: (81.68%) (26241/32128)\n",
      "Epoch: 147 | Batch_idx: 260 |  Loss_1: (0.4920) | Acc_1: (81.57%) (27252/33408)\n",
      "Epoch: 147 | Batch_idx: 270 |  Loss_1: (0.4909) | Acc_1: (81.62%) (28311/34688)\n",
      "Epoch: 147 | Batch_idx: 280 |  Loss_1: (0.4905) | Acc_1: (81.63%) (29359/35968)\n",
      "Epoch: 147 | Batch_idx: 290 |  Loss_1: (0.4901) | Acc_1: (81.65%) (30414/37248)\n",
      "Epoch: 147 | Batch_idx: 300 |  Loss_1: (0.4897) | Acc_1: (81.68%) (31469/38528)\n",
      "Epoch: 147 | Batch_idx: 310 |  Loss_1: (0.4897) | Acc_1: (81.68%) (32515/39808)\n",
      "Epoch: 147 | Batch_idx: 320 |  Loss_1: (0.4895) | Acc_1: (81.69%) (33566/41088)\n",
      "Epoch: 147 | Batch_idx: 330 |  Loss_1: (0.4894) | Acc_1: (81.71%) (34617/42368)\n",
      "Epoch: 147 | Batch_idx: 340 |  Loss_1: (0.4891) | Acc_1: (81.73%) (35674/43648)\n",
      "Epoch: 147 | Batch_idx: 350 |  Loss_1: (0.4899) | Acc_1: (81.70%) (36706/44928)\n",
      "Epoch: 147 | Batch_idx: 360 |  Loss_1: (0.4902) | Acc_1: (81.69%) (37747/46208)\n",
      "Epoch: 147 | Batch_idx: 370 |  Loss_1: (0.4908) | Acc_1: (81.67%) (38783/47488)\n",
      "Epoch: 147 | Batch_idx: 380 |  Loss_1: (0.4902) | Acc_1: (81.69%) (39840/48768)\n",
      "Epoch: 147 | Batch_idx: 390 |  Loss_1: (0.4898) | Acc_1: (81.70%) (40851/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4790) | Acc: (90.38%) (9038/10000)\n",
      "Epoch: 148 | Batch_idx: 0 |  Loss_1: (0.5929) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 148 | Batch_idx: 10 |  Loss_1: (0.4764) | Acc_1: (82.10%) (1156/1408)\n",
      "Epoch: 148 | Batch_idx: 20 |  Loss_1: (0.4609) | Acc_1: (82.85%) (2227/2688)\n",
      "Epoch: 148 | Batch_idx: 30 |  Loss_1: (0.4816) | Acc_1: (81.93%) (3251/3968)\n",
      "Epoch: 148 | Batch_idx: 40 |  Loss_1: (0.4829) | Acc_1: (81.92%) (4299/5248)\n",
      "Epoch: 148 | Batch_idx: 50 |  Loss_1: (0.4830) | Acc_1: (81.82%) (5341/6528)\n",
      "Epoch: 148 | Batch_idx: 60 |  Loss_1: (0.4852) | Acc_1: (81.67%) (6377/7808)\n",
      "Epoch: 148 | Batch_idx: 70 |  Loss_1: (0.4826) | Acc_1: (81.81%) (7435/9088)\n",
      "Epoch: 148 | Batch_idx: 80 |  Loss_1: (0.4780) | Acc_1: (82.01%) (8503/10368)\n",
      "Epoch: 148 | Batch_idx: 90 |  Loss_1: (0.4793) | Acc_1: (81.99%) (9550/11648)\n",
      "Epoch: 148 | Batch_idx: 100 |  Loss_1: (0.4798) | Acc_1: (81.93%) (10592/12928)\n",
      "Epoch: 148 | Batch_idx: 110 |  Loss_1: (0.4810) | Acc_1: (81.84%) (11628/14208)\n",
      "Epoch: 148 | Batch_idx: 120 |  Loss_1: (0.4806) | Acc_1: (81.88%) (12681/15488)\n",
      "Epoch: 148 | Batch_idx: 130 |  Loss_1: (0.4769) | Acc_1: (82.01%) (13751/16768)\n",
      "Epoch: 148 | Batch_idx: 140 |  Loss_1: (0.4764) | Acc_1: (82.04%) (14807/18048)\n",
      "Epoch: 148 | Batch_idx: 150 |  Loss_1: (0.4756) | Acc_1: (82.08%) (15864/19328)\n",
      "Epoch: 148 | Batch_idx: 160 |  Loss_1: (0.4770) | Acc_1: (82.04%) (16907/20608)\n",
      "Epoch: 148 | Batch_idx: 170 |  Loss_1: (0.4777) | Acc_1: (82.00%) (17949/21888)\n",
      "Epoch: 148 | Batch_idx: 180 |  Loss_1: (0.4771) | Acc_1: (82.03%) (19004/23168)\n",
      "Epoch: 148 | Batch_idx: 190 |  Loss_1: (0.4772) | Acc_1: (82.01%) (20050/24448)\n",
      "Epoch: 148 | Batch_idx: 200 |  Loss_1: (0.4786) | Acc_1: (81.95%) (21085/25728)\n",
      "Epoch: 148 | Batch_idx: 210 |  Loss_1: (0.4789) | Acc_1: (81.94%) (22130/27008)\n",
      "Epoch: 148 | Batch_idx: 220 |  Loss_1: (0.4794) | Acc_1: (81.91%) (23172/28288)\n",
      "Epoch: 148 | Batch_idx: 230 |  Loss_1: (0.4816) | Acc_1: (81.84%) (24197/29568)\n",
      "Epoch: 148 | Batch_idx: 240 |  Loss_1: (0.4830) | Acc_1: (81.78%) (25229/30848)\n",
      "Epoch: 148 | Batch_idx: 250 |  Loss_1: (0.4821) | Acc_1: (81.81%) (26284/32128)\n",
      "Epoch: 148 | Batch_idx: 260 |  Loss_1: (0.4819) | Acc_1: (81.82%) (27336/33408)\n",
      "Epoch: 148 | Batch_idx: 270 |  Loss_1: (0.4822) | Acc_1: (81.82%) (28380/34688)\n",
      "Epoch: 148 | Batch_idx: 280 |  Loss_1: (0.4836) | Acc_1: (81.77%) (29412/35968)\n",
      "Epoch: 148 | Batch_idx: 290 |  Loss_1: (0.4845) | Acc_1: (81.75%) (30451/37248)\n",
      "Epoch: 148 | Batch_idx: 300 |  Loss_1: (0.4852) | Acc_1: (81.75%) (31496/38528)\n",
      "Epoch: 148 | Batch_idx: 310 |  Loss_1: (0.4856) | Acc_1: (81.73%) (32537/39808)\n",
      "Epoch: 148 | Batch_idx: 320 |  Loss_1: (0.4857) | Acc_1: (81.74%) (33587/41088)\n",
      "Epoch: 148 | Batch_idx: 330 |  Loss_1: (0.4865) | Acc_1: (81.72%) (34622/42368)\n",
      "Epoch: 148 | Batch_idx: 340 |  Loss_1: (0.4874) | Acc_1: (81.68%) (35652/43648)\n",
      "Epoch: 148 | Batch_idx: 350 |  Loss_1: (0.4883) | Acc_1: (81.66%) (36690/44928)\n",
      "Epoch: 148 | Batch_idx: 360 |  Loss_1: (0.4882) | Acc_1: (81.68%) (37741/46208)\n",
      "Epoch: 148 | Batch_idx: 370 |  Loss_1: (0.4883) | Acc_1: (81.69%) (38791/47488)\n",
      "Epoch: 148 | Batch_idx: 380 |  Loss_1: (0.4882) | Acc_1: (81.69%) (39839/48768)\n",
      "Epoch: 148 | Batch_idx: 390 |  Loss_1: (0.4883) | Acc_1: (81.70%) (40850/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4721) | Acc: (90.29%) (9029/10000)\n",
      "Epoch: 149 | Batch_idx: 0 |  Loss_1: (0.6367) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 149 | Batch_idx: 10 |  Loss_1: (0.4857) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 149 | Batch_idx: 20 |  Loss_1: (0.4709) | Acc_1: (82.74%) (2224/2688)\n",
      "Epoch: 149 | Batch_idx: 30 |  Loss_1: (0.4798) | Acc_1: (82.46%) (3272/3968)\n",
      "Epoch: 149 | Batch_idx: 40 |  Loss_1: (0.4774) | Acc_1: (82.56%) (4333/5248)\n",
      "Epoch: 149 | Batch_idx: 50 |  Loss_1: (0.4800) | Acc_1: (82.46%) (5383/6528)\n",
      "Epoch: 149 | Batch_idx: 60 |  Loss_1: (0.4811) | Acc_1: (82.40%) (6434/7808)\n",
      "Epoch: 149 | Batch_idx: 70 |  Loss_1: (0.4844) | Acc_1: (82.24%) (7474/9088)\n",
      "Epoch: 149 | Batch_idx: 80 |  Loss_1: (0.4768) | Acc_1: (82.46%) (8549/10368)\n",
      "Epoch: 149 | Batch_idx: 90 |  Loss_1: (0.4842) | Acc_1: (82.16%) (9570/11648)\n",
      "Epoch: 149 | Batch_idx: 100 |  Loss_1: (0.4852) | Acc_1: (82.16%) (10622/12928)\n",
      "Epoch: 149 | Batch_idx: 110 |  Loss_1: (0.4859) | Acc_1: (82.14%) (11671/14208)\n",
      "Epoch: 149 | Batch_idx: 120 |  Loss_1: (0.4839) | Acc_1: (82.22%) (12734/15488)\n",
      "Epoch: 149 | Batch_idx: 130 |  Loss_1: (0.4856) | Acc_1: (82.13%) (13772/16768)\n",
      "Epoch: 149 | Batch_idx: 140 |  Loss_1: (0.4851) | Acc_1: (82.11%) (14820/18048)\n",
      "Epoch: 149 | Batch_idx: 150 |  Loss_1: (0.4853) | Acc_1: (82.09%) (15866/19328)\n",
      "Epoch: 149 | Batch_idx: 160 |  Loss_1: (0.4826) | Acc_1: (82.16%) (16932/20608)\n",
      "Epoch: 149 | Batch_idx: 170 |  Loss_1: (0.4808) | Acc_1: (82.25%) (18002/21888)\n",
      "Epoch: 149 | Batch_idx: 180 |  Loss_1: (0.4810) | Acc_1: (82.24%) (19053/23168)\n",
      "Epoch: 149 | Batch_idx: 190 |  Loss_1: (0.4803) | Acc_1: (82.24%) (20107/24448)\n",
      "Epoch: 149 | Batch_idx: 200 |  Loss_1: (0.4812) | Acc_1: (82.23%) (21155/25728)\n",
      "Epoch: 149 | Batch_idx: 210 |  Loss_1: (0.4816) | Acc_1: (82.20%) (22200/27008)\n",
      "Epoch: 149 | Batch_idx: 220 |  Loss_1: (0.4821) | Acc_1: (82.15%) (23238/28288)\n",
      "Epoch: 149 | Batch_idx: 230 |  Loss_1: (0.4823) | Acc_1: (82.10%) (24274/29568)\n",
      "Epoch: 149 | Batch_idx: 240 |  Loss_1: (0.4831) | Acc_1: (82.06%) (25313/30848)\n",
      "Epoch: 149 | Batch_idx: 250 |  Loss_1: (0.4824) | Acc_1: (82.06%) (26364/32128)\n",
      "Epoch: 149 | Batch_idx: 260 |  Loss_1: (0.4827) | Acc_1: (82.04%) (27408/33408)\n",
      "Epoch: 149 | Batch_idx: 270 |  Loss_1: (0.4832) | Acc_1: (82.02%) (28452/34688)\n",
      "Epoch: 149 | Batch_idx: 280 |  Loss_1: (0.4833) | Acc_1: (82.02%) (29500/35968)\n",
      "Epoch: 149 | Batch_idx: 290 |  Loss_1: (0.4805) | Acc_1: (82.12%) (30588/37248)\n",
      "Epoch: 149 | Batch_idx: 300 |  Loss_1: (0.4801) | Acc_1: (82.14%) (31647/38528)\n",
      "Epoch: 149 | Batch_idx: 310 |  Loss_1: (0.4809) | Acc_1: (82.11%) (32687/39808)\n",
      "Epoch: 149 | Batch_idx: 320 |  Loss_1: (0.4805) | Acc_1: (82.14%) (33748/41088)\n",
      "Epoch: 149 | Batch_idx: 330 |  Loss_1: (0.4804) | Acc_1: (82.14%) (34803/42368)\n",
      "Epoch: 149 | Batch_idx: 340 |  Loss_1: (0.4806) | Acc_1: (82.13%) (35849/43648)\n",
      "Epoch: 149 | Batch_idx: 350 |  Loss_1: (0.4803) | Acc_1: (82.15%) (36907/44928)\n",
      "Epoch: 149 | Batch_idx: 360 |  Loss_1: (0.4789) | Acc_1: (82.20%) (37981/46208)\n",
      "Epoch: 149 | Batch_idx: 370 |  Loss_1: (0.4791) | Acc_1: (82.20%) (39033/47488)\n",
      "Epoch: 149 | Batch_idx: 380 |  Loss_1: (0.4782) | Acc_1: (82.22%) (40099/48768)\n",
      "Epoch: 149 | Batch_idx: 390 |  Loss_1: (0.4792) | Acc_1: (82.20%) (41099/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4813) | Acc: (89.91%) (8991/10000)\n",
      "Epoch: 150 | Batch_idx: 0 |  Loss_1: (0.4368) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 150 | Batch_idx: 10 |  Loss_1: (0.4796) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 150 | Batch_idx: 20 |  Loss_1: (0.4928) | Acc_1: (81.51%) (2191/2688)\n",
      "Epoch: 150 | Batch_idx: 30 |  Loss_1: (0.4786) | Acc_1: (82.08%) (3257/3968)\n",
      "Epoch: 150 | Batch_idx: 40 |  Loss_1: (0.4804) | Acc_1: (81.96%) (4301/5248)\n",
      "Epoch: 150 | Batch_idx: 50 |  Loss_1: (0.4759) | Acc_1: (82.15%) (5363/6528)\n",
      "Epoch: 150 | Batch_idx: 60 |  Loss_1: (0.4783) | Acc_1: (82.03%) (6405/7808)\n",
      "Epoch: 150 | Batch_idx: 70 |  Loss_1: (0.4794) | Acc_1: (81.97%) (7449/9088)\n",
      "Epoch: 150 | Batch_idx: 80 |  Loss_1: (0.4741) | Acc_1: (82.18%) (8520/10368)\n",
      "Epoch: 150 | Batch_idx: 90 |  Loss_1: (0.4763) | Acc_1: (82.09%) (9562/11648)\n",
      "Epoch: 150 | Batch_idx: 100 |  Loss_1: (0.4779) | Acc_1: (82.05%) (10608/12928)\n",
      "Epoch: 150 | Batch_idx: 110 |  Loss_1: (0.4811) | Acc_1: (81.93%) (11640/14208)\n",
      "Epoch: 150 | Batch_idx: 120 |  Loss_1: (0.4820) | Acc_1: (81.90%) (12685/15488)\n",
      "Epoch: 150 | Batch_idx: 130 |  Loss_1: (0.4813) | Acc_1: (81.97%) (13745/16768)\n",
      "Epoch: 150 | Batch_idx: 140 |  Loss_1: (0.4795) | Acc_1: (82.04%) (14806/18048)\n",
      "Epoch: 150 | Batch_idx: 150 |  Loss_1: (0.4796) | Acc_1: (82.04%) (15856/19328)\n",
      "Epoch: 150 | Batch_idx: 160 |  Loss_1: (0.4797) | Acc_1: (82.05%) (16908/20608)\n",
      "Epoch: 150 | Batch_idx: 170 |  Loss_1: (0.4797) | Acc_1: (82.05%) (17960/21888)\n",
      "Epoch: 150 | Batch_idx: 180 |  Loss_1: (0.4792) | Acc_1: (82.07%) (19014/23168)\n",
      "Epoch: 150 | Batch_idx: 190 |  Loss_1: (0.4782) | Acc_1: (82.10%) (20072/24448)\n",
      "Epoch: 150 | Batch_idx: 200 |  Loss_1: (0.4772) | Acc_1: (82.13%) (21131/25728)\n",
      "Epoch: 150 | Batch_idx: 210 |  Loss_1: (0.4778) | Acc_1: (82.12%) (22178/27008)\n",
      "Epoch: 150 | Batch_idx: 220 |  Loss_1: (0.4786) | Acc_1: (82.11%) (23228/28288)\n",
      "Epoch: 150 | Batch_idx: 230 |  Loss_1: (0.4791) | Acc_1: (82.11%) (24278/29568)\n",
      "Epoch: 150 | Batch_idx: 240 |  Loss_1: (0.4806) | Acc_1: (82.08%) (25320/30848)\n",
      "Epoch: 150 | Batch_idx: 250 |  Loss_1: (0.4814) | Acc_1: (82.04%) (26358/32128)\n",
      "Epoch: 150 | Batch_idx: 260 |  Loss_1: (0.4789) | Acc_1: (82.14%) (27442/33408)\n",
      "Epoch: 150 | Batch_idx: 270 |  Loss_1: (0.4798) | Acc_1: (82.11%) (28482/34688)\n",
      "Epoch: 150 | Batch_idx: 280 |  Loss_1: (0.4811) | Acc_1: (82.07%) (29520/35968)\n",
      "Epoch: 150 | Batch_idx: 290 |  Loss_1: (0.4806) | Acc_1: (82.08%) (30574/37248)\n",
      "Epoch: 150 | Batch_idx: 300 |  Loss_1: (0.4809) | Acc_1: (82.08%) (31622/38528)\n",
      "Epoch: 150 | Batch_idx: 310 |  Loss_1: (0.4794) | Acc_1: (82.13%) (32696/39808)\n",
      "Epoch: 150 | Batch_idx: 320 |  Loss_1: (0.4795) | Acc_1: (82.15%) (33753/41088)\n",
      "Epoch: 150 | Batch_idx: 330 |  Loss_1: (0.4803) | Acc_1: (82.12%) (34793/42368)\n",
      "Epoch: 150 | Batch_idx: 340 |  Loss_1: (0.4806) | Acc_1: (82.11%) (35841/43648)\n",
      "Epoch: 150 | Batch_idx: 350 |  Loss_1: (0.4796) | Acc_1: (82.15%) (36909/44928)\n",
      "Epoch: 150 | Batch_idx: 360 |  Loss_1: (0.4793) | Acc_1: (82.16%) (37964/46208)\n",
      "Epoch: 150 | Batch_idx: 370 |  Loss_1: (0.4786) | Acc_1: (82.18%) (39027/47488)\n",
      "Epoch: 150 | Batch_idx: 380 |  Loss_1: (0.4778) | Acc_1: (82.20%) (40089/48768)\n",
      "Epoch: 150 | Batch_idx: 390 |  Loss_1: (0.4773) | Acc_1: (82.23%) (41113/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4454) | Acc: (90.90%) (9090/10000)\n",
      "Epoch: 151 | Batch_idx: 0 |  Loss_1: (0.5382) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 151 | Batch_idx: 10 |  Loss_1: (0.4855) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 151 | Batch_idx: 20 |  Loss_1: (0.4619) | Acc_1: (82.55%) (2219/2688)\n",
      "Epoch: 151 | Batch_idx: 30 |  Loss_1: (0.4753) | Acc_1: (81.91%) (3250/3968)\n",
      "Epoch: 151 | Batch_idx: 40 |  Loss_1: (0.4812) | Acc_1: (81.76%) (4291/5248)\n",
      "Epoch: 151 | Batch_idx: 50 |  Loss_1: (0.4810) | Acc_1: (81.82%) (5341/6528)\n",
      "Epoch: 151 | Batch_idx: 60 |  Loss_1: (0.4852) | Acc_1: (81.74%) (6382/7808)\n",
      "Epoch: 151 | Batch_idx: 70 |  Loss_1: (0.4882) | Acc_1: (81.62%) (7418/9088)\n",
      "Epoch: 151 | Batch_idx: 80 |  Loss_1: (0.4884) | Acc_1: (81.63%) (8463/10368)\n",
      "Epoch: 151 | Batch_idx: 90 |  Loss_1: (0.4879) | Acc_1: (81.64%) (9510/11648)\n",
      "Epoch: 151 | Batch_idx: 100 |  Loss_1: (0.4915) | Acc_1: (81.51%) (10538/12928)\n",
      "Epoch: 151 | Batch_idx: 110 |  Loss_1: (0.4897) | Acc_1: (81.61%) (11595/14208)\n",
      "Epoch: 151 | Batch_idx: 120 |  Loss_1: (0.4933) | Acc_1: (81.46%) (12616/15488)\n",
      "Epoch: 151 | Batch_idx: 130 |  Loss_1: (0.4933) | Acc_1: (81.46%) (13660/16768)\n",
      "Epoch: 151 | Batch_idx: 140 |  Loss_1: (0.4936) | Acc_1: (81.49%) (14708/18048)\n",
      "Epoch: 151 | Batch_idx: 150 |  Loss_1: (0.4957) | Acc_1: (81.44%) (15740/19328)\n",
      "Epoch: 151 | Batch_idx: 160 |  Loss_1: (0.4935) | Acc_1: (81.55%) (16806/20608)\n",
      "Epoch: 151 | Batch_idx: 170 |  Loss_1: (0.4906) | Acc_1: (81.66%) (17874/21888)\n",
      "Epoch: 151 | Batch_idx: 180 |  Loss_1: (0.4924) | Acc_1: (81.59%) (18902/23168)\n",
      "Epoch: 151 | Batch_idx: 190 |  Loss_1: (0.4899) | Acc_1: (81.68%) (19968/24448)\n",
      "Epoch: 151 | Batch_idx: 200 |  Loss_1: (0.4887) | Acc_1: (81.72%) (21024/25728)\n",
      "Epoch: 151 | Batch_idx: 210 |  Loss_1: (0.4884) | Acc_1: (81.74%) (22076/27008)\n",
      "Epoch: 151 | Batch_idx: 220 |  Loss_1: (0.4906) | Acc_1: (81.64%) (23095/28288)\n",
      "Epoch: 151 | Batch_idx: 230 |  Loss_1: (0.4901) | Acc_1: (81.66%) (24145/29568)\n",
      "Epoch: 151 | Batch_idx: 240 |  Loss_1: (0.4891) | Acc_1: (81.72%) (25208/30848)\n",
      "Epoch: 151 | Batch_idx: 250 |  Loss_1: (0.4888) | Acc_1: (81.72%) (26256/32128)\n",
      "Epoch: 151 | Batch_idx: 260 |  Loss_1: (0.4910) | Acc_1: (81.65%) (27277/33408)\n",
      "Epoch: 151 | Batch_idx: 270 |  Loss_1: (0.4902) | Acc_1: (81.68%) (28334/34688)\n",
      "Epoch: 151 | Batch_idx: 280 |  Loss_1: (0.4897) | Acc_1: (81.70%) (29385/35968)\n",
      "Epoch: 151 | Batch_idx: 290 |  Loss_1: (0.4895) | Acc_1: (81.69%) (30429/37248)\n",
      "Epoch: 151 | Batch_idx: 300 |  Loss_1: (0.4898) | Acc_1: (81.68%) (31471/38528)\n",
      "Epoch: 151 | Batch_idx: 310 |  Loss_1: (0.4901) | Acc_1: (81.65%) (32505/39808)\n",
      "Epoch: 151 | Batch_idx: 320 |  Loss_1: (0.4909) | Acc_1: (81.64%) (33544/41088)\n",
      "Epoch: 151 | Batch_idx: 330 |  Loss_1: (0.4907) | Acc_1: (81.66%) (34598/42368)\n",
      "Epoch: 151 | Batch_idx: 340 |  Loss_1: (0.4917) | Acc_1: (81.62%) (35627/43648)\n",
      "Epoch: 151 | Batch_idx: 350 |  Loss_1: (0.4924) | Acc_1: (81.60%) (36662/44928)\n",
      "Epoch: 151 | Batch_idx: 360 |  Loss_1: (0.4905) | Acc_1: (81.67%) (37739/46208)\n",
      "Epoch: 151 | Batch_idx: 370 |  Loss_1: (0.4902) | Acc_1: (81.68%) (38789/47488)\n",
      "Epoch: 151 | Batch_idx: 380 |  Loss_1: (0.4908) | Acc_1: (81.67%) (39830/48768)\n",
      "Epoch: 151 | Batch_idx: 390 |  Loss_1: (0.4902) | Acc_1: (81.69%) (40843/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4993) | Acc: (90.15%) (9015/10000)\n",
      "Epoch: 152 | Batch_idx: 0 |  Loss_1: (0.4908) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 152 | Batch_idx: 10 |  Loss_1: (0.4706) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 152 | Batch_idx: 20 |  Loss_1: (0.4809) | Acc_1: (82.22%) (2210/2688)\n",
      "Epoch: 152 | Batch_idx: 30 |  Loss_1: (0.4713) | Acc_1: (82.46%) (3272/3968)\n",
      "Epoch: 152 | Batch_idx: 40 |  Loss_1: (0.4781) | Acc_1: (82.20%) (4314/5248)\n",
      "Epoch: 152 | Batch_idx: 50 |  Loss_1: (0.4864) | Acc_1: (81.88%) (5345/6528)\n",
      "Epoch: 152 | Batch_idx: 60 |  Loss_1: (0.4884) | Acc_1: (81.81%) (6388/7808)\n",
      "Epoch: 152 | Batch_idx: 70 |  Loss_1: (0.4835) | Acc_1: (82.01%) (7453/9088)\n",
      "Epoch: 152 | Batch_idx: 80 |  Loss_1: (0.4780) | Acc_1: (82.24%) (8527/10368)\n",
      "Epoch: 152 | Batch_idx: 90 |  Loss_1: (0.4853) | Acc_1: (81.94%) (9544/11648)\n",
      "Epoch: 152 | Batch_idx: 100 |  Loss_1: (0.4878) | Acc_1: (81.86%) (10583/12928)\n",
      "Epoch: 152 | Batch_idx: 110 |  Loss_1: (0.4914) | Acc_1: (81.74%) (11614/14208)\n",
      "Epoch: 152 | Batch_idx: 120 |  Loss_1: (0.4913) | Acc_1: (81.74%) (12660/15488)\n",
      "Epoch: 152 | Batch_idx: 130 |  Loss_1: (0.4868) | Acc_1: (81.91%) (13734/16768)\n",
      "Epoch: 152 | Batch_idx: 140 |  Loss_1: (0.4909) | Acc_1: (81.75%) (14754/18048)\n",
      "Epoch: 152 | Batch_idx: 150 |  Loss_1: (0.4897) | Acc_1: (81.78%) (15806/19328)\n",
      "Epoch: 152 | Batch_idx: 160 |  Loss_1: (0.4903) | Acc_1: (81.75%) (16848/20608)\n",
      "Epoch: 152 | Batch_idx: 170 |  Loss_1: (0.4930) | Acc_1: (81.67%) (17877/21888)\n",
      "Epoch: 152 | Batch_idx: 180 |  Loss_1: (0.4911) | Acc_1: (81.74%) (18937/23168)\n",
      "Epoch: 152 | Batch_idx: 190 |  Loss_1: (0.4928) | Acc_1: (81.68%) (19970/24448)\n",
      "Epoch: 152 | Batch_idx: 200 |  Loss_1: (0.4950) | Acc_1: (81.60%) (20993/25728)\n",
      "Epoch: 152 | Batch_idx: 210 |  Loss_1: (0.4944) | Acc_1: (81.61%) (22042/27008)\n",
      "Epoch: 152 | Batch_idx: 220 |  Loss_1: (0.4928) | Acc_1: (81.68%) (23107/28288)\n",
      "Epoch: 152 | Batch_idx: 230 |  Loss_1: (0.4931) | Acc_1: (81.68%) (24151/29568)\n",
      "Epoch: 152 | Batch_idx: 240 |  Loss_1: (0.4921) | Acc_1: (81.71%) (25206/30848)\n",
      "Epoch: 152 | Batch_idx: 250 |  Loss_1: (0.4923) | Acc_1: (81.70%) (26247/32128)\n",
      "Epoch: 152 | Batch_idx: 260 |  Loss_1: (0.4916) | Acc_1: (81.71%) (27297/33408)\n",
      "Epoch: 152 | Batch_idx: 270 |  Loss_1: (0.4929) | Acc_1: (81.66%) (28327/34688)\n",
      "Epoch: 152 | Batch_idx: 280 |  Loss_1: (0.4926) | Acc_1: (81.66%) (29371/35968)\n",
      "Epoch: 152 | Batch_idx: 290 |  Loss_1: (0.4925) | Acc_1: (81.65%) (30412/37248)\n",
      "Epoch: 152 | Batch_idx: 300 |  Loss_1: (0.4914) | Acc_1: (81.69%) (31472/38528)\n",
      "Epoch: 152 | Batch_idx: 310 |  Loss_1: (0.4916) | Acc_1: (81.67%) (32512/39808)\n",
      "Epoch: 152 | Batch_idx: 320 |  Loss_1: (0.4917) | Acc_1: (81.67%) (33555/41088)\n",
      "Epoch: 152 | Batch_idx: 330 |  Loss_1: (0.4909) | Acc_1: (81.69%) (34612/42368)\n",
      "Epoch: 152 | Batch_idx: 340 |  Loss_1: (0.4911) | Acc_1: (81.68%) (35651/43648)\n",
      "Epoch: 152 | Batch_idx: 350 |  Loss_1: (0.4910) | Acc_1: (81.67%) (36694/44928)\n",
      "Epoch: 152 | Batch_idx: 360 |  Loss_1: (0.4908) | Acc_1: (81.68%) (37745/46208)\n",
      "Epoch: 152 | Batch_idx: 370 |  Loss_1: (0.4911) | Acc_1: (81.67%) (38785/47488)\n",
      "Epoch: 152 | Batch_idx: 380 |  Loss_1: (0.4918) | Acc_1: (81.65%) (39818/48768)\n",
      "Epoch: 152 | Batch_idx: 390 |  Loss_1: (0.4908) | Acc_1: (81.67%) (40835/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4510) | Acc: (90.56%) (9056/10000)\n",
      "Epoch: 153 | Batch_idx: 0 |  Loss_1: (0.5506) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 153 | Batch_idx: 10 |  Loss_1: (0.4672) | Acc_1: (82.39%) (1160/1408)\n",
      "Epoch: 153 | Batch_idx: 20 |  Loss_1: (0.4789) | Acc_1: (82.03%) (2205/2688)\n",
      "Epoch: 153 | Batch_idx: 30 |  Loss_1: (0.4924) | Acc_1: (81.65%) (3240/3968)\n",
      "Epoch: 153 | Batch_idx: 40 |  Loss_1: (0.4726) | Acc_1: (82.47%) (4328/5248)\n",
      "Epoch: 153 | Batch_idx: 50 |  Loss_1: (0.4777) | Acc_1: (82.25%) (5369/6528)\n",
      "Epoch: 153 | Batch_idx: 60 |  Loss_1: (0.4762) | Acc_1: (82.22%) (6420/7808)\n",
      "Epoch: 153 | Batch_idx: 70 |  Loss_1: (0.4736) | Acc_1: (82.37%) (7486/9088)\n",
      "Epoch: 153 | Batch_idx: 80 |  Loss_1: (0.4746) | Acc_1: (82.34%) (8537/10368)\n",
      "Epoch: 153 | Batch_idx: 90 |  Loss_1: (0.4732) | Acc_1: (82.38%) (9596/11648)\n",
      "Epoch: 153 | Batch_idx: 100 |  Loss_1: (0.4767) | Acc_1: (82.24%) (10632/12928)\n",
      "Epoch: 153 | Batch_idx: 110 |  Loss_1: (0.4765) | Acc_1: (82.26%) (11687/14208)\n",
      "Epoch: 153 | Batch_idx: 120 |  Loss_1: (0.4753) | Acc_1: (82.35%) (12754/15488)\n",
      "Epoch: 153 | Batch_idx: 130 |  Loss_1: (0.4755) | Acc_1: (82.31%) (13802/16768)\n",
      "Epoch: 153 | Batch_idx: 140 |  Loss_1: (0.4778) | Acc_1: (82.22%) (14839/18048)\n",
      "Epoch: 153 | Batch_idx: 150 |  Loss_1: (0.4740) | Acc_1: (82.34%) (15915/19328)\n",
      "Epoch: 153 | Batch_idx: 160 |  Loss_1: (0.4716) | Acc_1: (82.42%) (16986/20608)\n",
      "Epoch: 153 | Batch_idx: 170 |  Loss_1: (0.4747) | Acc_1: (82.29%) (18011/21888)\n",
      "Epoch: 153 | Batch_idx: 180 |  Loss_1: (0.4772) | Acc_1: (82.19%) (19041/23168)\n",
      "Epoch: 153 | Batch_idx: 190 |  Loss_1: (0.4773) | Acc_1: (82.21%) (20099/24448)\n",
      "Epoch: 153 | Batch_idx: 200 |  Loss_1: (0.4774) | Acc_1: (82.19%) (21146/25728)\n",
      "Epoch: 153 | Batch_idx: 210 |  Loss_1: (0.4789) | Acc_1: (82.13%) (22181/27008)\n",
      "Epoch: 153 | Batch_idx: 220 |  Loss_1: (0.4787) | Acc_1: (82.15%) (23239/28288)\n",
      "Epoch: 153 | Batch_idx: 230 |  Loss_1: (0.4805) | Acc_1: (82.09%) (24272/29568)\n",
      "Epoch: 153 | Batch_idx: 240 |  Loss_1: (0.4792) | Acc_1: (82.14%) (25340/30848)\n",
      "Epoch: 153 | Batch_idx: 250 |  Loss_1: (0.4783) | Acc_1: (82.19%) (26407/32128)\n",
      "Epoch: 153 | Batch_idx: 260 |  Loss_1: (0.4797) | Acc_1: (82.13%) (27437/33408)\n",
      "Epoch: 153 | Batch_idx: 270 |  Loss_1: (0.4793) | Acc_1: (82.15%) (28497/34688)\n",
      "Epoch: 153 | Batch_idx: 280 |  Loss_1: (0.4795) | Acc_1: (82.15%) (29547/35968)\n",
      "Epoch: 153 | Batch_idx: 290 |  Loss_1: (0.4794) | Acc_1: (82.15%) (30598/37248)\n",
      "Epoch: 153 | Batch_idx: 300 |  Loss_1: (0.4785) | Acc_1: (82.18%) (31663/38528)\n",
      "Epoch: 153 | Batch_idx: 310 |  Loss_1: (0.4793) | Acc_1: (82.15%) (32704/39808)\n",
      "Epoch: 153 | Batch_idx: 320 |  Loss_1: (0.4776) | Acc_1: (82.22%) (33783/41088)\n",
      "Epoch: 153 | Batch_idx: 330 |  Loss_1: (0.4782) | Acc_1: (82.19%) (34824/42368)\n",
      "Epoch: 153 | Batch_idx: 340 |  Loss_1: (0.4773) | Acc_1: (82.23%) (35891/43648)\n",
      "Epoch: 153 | Batch_idx: 350 |  Loss_1: (0.4752) | Acc_1: (82.31%) (36979/44928)\n",
      "Epoch: 153 | Batch_idx: 360 |  Loss_1: (0.4759) | Acc_1: (82.27%) (38015/46208)\n",
      "Epoch: 153 | Batch_idx: 370 |  Loss_1: (0.4756) | Acc_1: (82.28%) (39072/47488)\n",
      "Epoch: 153 | Batch_idx: 380 |  Loss_1: (0.4746) | Acc_1: (82.32%) (40144/48768)\n",
      "Epoch: 153 | Batch_idx: 390 |  Loss_1: (0.4753) | Acc_1: (82.29%) (41144/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4832) | Acc: (90.56%) (9056/10000)\n",
      "Epoch: 154 | Batch_idx: 0 |  Loss_1: (0.3353) | Acc_1: (87.50%) (112/128)\n",
      "Epoch: 154 | Batch_idx: 10 |  Loss_1: (0.4968) | Acc_1: (81.32%) (1145/1408)\n",
      "Epoch: 154 | Batch_idx: 20 |  Loss_1: (0.4891) | Acc_1: (81.70%) (2196/2688)\n",
      "Epoch: 154 | Batch_idx: 30 |  Loss_1: (0.4729) | Acc_1: (82.41%) (3270/3968)\n",
      "Epoch: 154 | Batch_idx: 40 |  Loss_1: (0.4720) | Acc_1: (82.49%) (4329/5248)\n",
      "Epoch: 154 | Batch_idx: 50 |  Loss_1: (0.4736) | Acc_1: (82.40%) (5379/6528)\n",
      "Epoch: 154 | Batch_idx: 60 |  Loss_1: (0.4780) | Acc_1: (82.17%) (6416/7808)\n",
      "Epoch: 154 | Batch_idx: 70 |  Loss_1: (0.4737) | Acc_1: (82.25%) (7475/9088)\n",
      "Epoch: 154 | Batch_idx: 80 |  Loss_1: (0.4759) | Acc_1: (82.22%) (8525/10368)\n",
      "Epoch: 154 | Batch_idx: 90 |  Loss_1: (0.4721) | Acc_1: (82.37%) (9594/11648)\n",
      "Epoch: 154 | Batch_idx: 100 |  Loss_1: (0.4726) | Acc_1: (82.36%) (10647/12928)\n",
      "Epoch: 154 | Batch_idx: 110 |  Loss_1: (0.4753) | Acc_1: (82.27%) (11689/14208)\n",
      "Epoch: 154 | Batch_idx: 120 |  Loss_1: (0.4771) | Acc_1: (82.20%) (12731/15488)\n",
      "Epoch: 154 | Batch_idx: 130 |  Loss_1: (0.4764) | Acc_1: (82.20%) (13783/16768)\n",
      "Epoch: 154 | Batch_idx: 140 |  Loss_1: (0.4782) | Acc_1: (82.11%) (14819/18048)\n",
      "Epoch: 154 | Batch_idx: 150 |  Loss_1: (0.4775) | Acc_1: (82.15%) (15878/19328)\n",
      "Epoch: 154 | Batch_idx: 160 |  Loss_1: (0.4786) | Acc_1: (82.11%) (16921/20608)\n",
      "Epoch: 154 | Batch_idx: 170 |  Loss_1: (0.4787) | Acc_1: (82.14%) (17978/21888)\n",
      "Epoch: 154 | Batch_idx: 180 |  Loss_1: (0.4786) | Acc_1: (82.11%) (19023/23168)\n",
      "Epoch: 154 | Batch_idx: 190 |  Loss_1: (0.4769) | Acc_1: (82.19%) (20093/24448)\n",
      "Epoch: 154 | Batch_idx: 200 |  Loss_1: (0.4775) | Acc_1: (82.14%) (21134/25728)\n",
      "Epoch: 154 | Batch_idx: 210 |  Loss_1: (0.4795) | Acc_1: (82.08%) (22169/27008)\n",
      "Epoch: 154 | Batch_idx: 220 |  Loss_1: (0.4793) | Acc_1: (82.08%) (23218/28288)\n",
      "Epoch: 154 | Batch_idx: 230 |  Loss_1: (0.4801) | Acc_1: (82.04%) (24259/29568)\n",
      "Epoch: 154 | Batch_idx: 240 |  Loss_1: (0.4792) | Acc_1: (82.09%) (25322/30848)\n",
      "Epoch: 154 | Batch_idx: 250 |  Loss_1: (0.4789) | Acc_1: (82.09%) (26374/32128)\n",
      "Epoch: 154 | Batch_idx: 260 |  Loss_1: (0.4769) | Acc_1: (82.17%) (27453/33408)\n",
      "Epoch: 154 | Batch_idx: 270 |  Loss_1: (0.4762) | Acc_1: (82.20%) (28512/34688)\n",
      "Epoch: 154 | Batch_idx: 280 |  Loss_1: (0.4765) | Acc_1: (82.19%) (29561/35968)\n",
      "Epoch: 154 | Batch_idx: 290 |  Loss_1: (0.4777) | Acc_1: (82.14%) (30597/37248)\n",
      "Epoch: 154 | Batch_idx: 300 |  Loss_1: (0.4780) | Acc_1: (82.14%) (31647/38528)\n",
      "Epoch: 154 | Batch_idx: 310 |  Loss_1: (0.4786) | Acc_1: (82.12%) (32689/39808)\n",
      "Epoch: 154 | Batch_idx: 320 |  Loss_1: (0.4772) | Acc_1: (82.16%) (33757/41088)\n",
      "Epoch: 154 | Batch_idx: 330 |  Loss_1: (0.4783) | Acc_1: (82.14%) (34799/42368)\n",
      "Epoch: 154 | Batch_idx: 340 |  Loss_1: (0.4787) | Acc_1: (82.12%) (35844/43648)\n",
      "Epoch: 154 | Batch_idx: 350 |  Loss_1: (0.4802) | Acc_1: (82.06%) (36870/44928)\n",
      "Epoch: 154 | Batch_idx: 360 |  Loss_1: (0.4802) | Acc_1: (82.07%) (37924/46208)\n",
      "Epoch: 154 | Batch_idx: 370 |  Loss_1: (0.4814) | Acc_1: (82.04%) (38959/47488)\n",
      "Epoch: 154 | Batch_idx: 380 |  Loss_1: (0.4809) | Acc_1: (82.08%) (40027/48768)\n",
      "Epoch: 154 | Batch_idx: 390 |  Loss_1: (0.4806) | Acc_1: (82.08%) (41040/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4815) | Acc: (90.28%) (9028/10000)\n",
      "Epoch: 155 | Batch_idx: 0 |  Loss_1: (0.5336) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 155 | Batch_idx: 10 |  Loss_1: (0.4970) | Acc_1: (81.46%) (1147/1408)\n",
      "Epoch: 155 | Batch_idx: 20 |  Loss_1: (0.4935) | Acc_1: (81.29%) (2185/2688)\n",
      "Epoch: 155 | Batch_idx: 30 |  Loss_1: (0.4787) | Acc_1: (81.88%) (3249/3968)\n",
      "Epoch: 155 | Batch_idx: 40 |  Loss_1: (0.4707) | Acc_1: (82.37%) (4323/5248)\n",
      "Epoch: 155 | Batch_idx: 50 |  Loss_1: (0.4783) | Acc_1: (82.09%) (5359/6528)\n",
      "Epoch: 155 | Batch_idx: 60 |  Loss_1: (0.4733) | Acc_1: (82.25%) (6422/7808)\n",
      "Epoch: 155 | Batch_idx: 70 |  Loss_1: (0.4758) | Acc_1: (82.21%) (7471/9088)\n",
      "Epoch: 155 | Batch_idx: 80 |  Loss_1: (0.4808) | Acc_1: (82.07%) (8509/10368)\n",
      "Epoch: 155 | Batch_idx: 90 |  Loss_1: (0.4831) | Acc_1: (81.95%) (9546/11648)\n",
      "Epoch: 155 | Batch_idx: 100 |  Loss_1: (0.4805) | Acc_1: (82.10%) (10614/12928)\n",
      "Epoch: 155 | Batch_idx: 110 |  Loss_1: (0.4788) | Acc_1: (82.19%) (11678/14208)\n",
      "Epoch: 155 | Batch_idx: 120 |  Loss_1: (0.4810) | Acc_1: (82.10%) (12716/15488)\n",
      "Epoch: 155 | Batch_idx: 130 |  Loss_1: (0.4826) | Acc_1: (82.07%) (13762/16768)\n",
      "Epoch: 155 | Batch_idx: 140 |  Loss_1: (0.4844) | Acc_1: (81.99%) (14797/18048)\n",
      "Epoch: 155 | Batch_idx: 150 |  Loss_1: (0.4852) | Acc_1: (81.96%) (15842/19328)\n",
      "Epoch: 155 | Batch_idx: 160 |  Loss_1: (0.4869) | Acc_1: (81.90%) (16877/20608)\n",
      "Epoch: 155 | Batch_idx: 170 |  Loss_1: (0.4839) | Acc_1: (82.00%) (17948/21888)\n",
      "Epoch: 155 | Batch_idx: 180 |  Loss_1: (0.4860) | Acc_1: (81.89%) (18973/23168)\n",
      "Epoch: 155 | Batch_idx: 190 |  Loss_1: (0.4861) | Acc_1: (81.90%) (20022/24448)\n",
      "Epoch: 155 | Batch_idx: 200 |  Loss_1: (0.4880) | Acc_1: (81.84%) (21057/25728)\n",
      "Epoch: 155 | Batch_idx: 210 |  Loss_1: (0.4895) | Acc_1: (81.79%) (22089/27008)\n",
      "Epoch: 155 | Batch_idx: 220 |  Loss_1: (0.4876) | Acc_1: (81.83%) (23148/28288)\n",
      "Epoch: 155 | Batch_idx: 230 |  Loss_1: (0.4872) | Acc_1: (81.84%) (24197/29568)\n",
      "Epoch: 155 | Batch_idx: 240 |  Loss_1: (0.4862) | Acc_1: (81.88%) (25257/30848)\n",
      "Epoch: 155 | Batch_idx: 250 |  Loss_1: (0.4858) | Acc_1: (81.89%) (26311/32128)\n",
      "Epoch: 155 | Batch_idx: 260 |  Loss_1: (0.4863) | Acc_1: (81.86%) (27349/33408)\n",
      "Epoch: 155 | Batch_idx: 270 |  Loss_1: (0.4884) | Acc_1: (81.77%) (28363/34688)\n",
      "Epoch: 155 | Batch_idx: 280 |  Loss_1: (0.4888) | Acc_1: (81.76%) (29409/35968)\n",
      "Epoch: 155 | Batch_idx: 290 |  Loss_1: (0.4891) | Acc_1: (81.74%) (30445/37248)\n",
      "Epoch: 155 | Batch_idx: 300 |  Loss_1: (0.4882) | Acc_1: (81.76%) (31502/38528)\n",
      "Epoch: 155 | Batch_idx: 310 |  Loss_1: (0.4872) | Acc_1: (81.81%) (32566/39808)\n",
      "Epoch: 155 | Batch_idx: 320 |  Loss_1: (0.4883) | Acc_1: (81.77%) (33597/41088)\n",
      "Epoch: 155 | Batch_idx: 330 |  Loss_1: (0.4881) | Acc_1: (81.77%) (34646/42368)\n",
      "Epoch: 155 | Batch_idx: 340 |  Loss_1: (0.4881) | Acc_1: (81.77%) (35690/43648)\n",
      "Epoch: 155 | Batch_idx: 350 |  Loss_1: (0.4885) | Acc_1: (81.74%) (36725/44928)\n",
      "Epoch: 155 | Batch_idx: 360 |  Loss_1: (0.4896) | Acc_1: (81.70%) (37754/46208)\n",
      "Epoch: 155 | Batch_idx: 370 |  Loss_1: (0.4888) | Acc_1: (81.74%) (38816/47488)\n",
      "Epoch: 155 | Batch_idx: 380 |  Loss_1: (0.4877) | Acc_1: (81.79%) (39885/48768)\n",
      "Epoch: 155 | Batch_idx: 390 |  Loss_1: (0.4872) | Acc_1: (81.79%) (40894/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4725) | Acc: (90.13%) (9013/10000)\n",
      "Epoch: 156 | Batch_idx: 0 |  Loss_1: (0.5147) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 156 | Batch_idx: 10 |  Loss_1: (0.4966) | Acc_1: (81.46%) (1147/1408)\n",
      "Epoch: 156 | Batch_idx: 20 |  Loss_1: (0.4850) | Acc_1: (81.73%) (2197/2688)\n",
      "Epoch: 156 | Batch_idx: 30 |  Loss_1: (0.4886) | Acc_1: (81.55%) (3236/3968)\n",
      "Epoch: 156 | Batch_idx: 40 |  Loss_1: (0.4825) | Acc_1: (81.88%) (4297/5248)\n",
      "Epoch: 156 | Batch_idx: 50 |  Loss_1: (0.4829) | Acc_1: (81.91%) (5347/6528)\n",
      "Epoch: 156 | Batch_idx: 60 |  Loss_1: (0.4840) | Acc_1: (81.86%) (6392/7808)\n",
      "Epoch: 156 | Batch_idx: 70 |  Loss_1: (0.4888) | Acc_1: (81.61%) (7417/9088)\n",
      "Epoch: 156 | Batch_idx: 80 |  Loss_1: (0.4912) | Acc_1: (81.49%) (8449/10368)\n",
      "Epoch: 156 | Batch_idx: 90 |  Loss_1: (0.4925) | Acc_1: (81.40%) (9482/11648)\n",
      "Epoch: 156 | Batch_idx: 100 |  Loss_1: (0.4881) | Acc_1: (81.61%) (10551/12928)\n",
      "Epoch: 156 | Batch_idx: 110 |  Loss_1: (0.4864) | Acc_1: (81.71%) (11609/14208)\n",
      "Epoch: 156 | Batch_idx: 120 |  Loss_1: (0.4868) | Acc_1: (81.71%) (12656/15488)\n",
      "Epoch: 156 | Batch_idx: 130 |  Loss_1: (0.4873) | Acc_1: (81.70%) (13700/16768)\n",
      "Epoch: 156 | Batch_idx: 140 |  Loss_1: (0.4907) | Acc_1: (81.59%) (14726/18048)\n",
      "Epoch: 156 | Batch_idx: 150 |  Loss_1: (0.4923) | Acc_1: (81.57%) (15765/19328)\n",
      "Epoch: 156 | Batch_idx: 160 |  Loss_1: (0.4950) | Acc_1: (81.48%) (16791/20608)\n",
      "Epoch: 156 | Batch_idx: 170 |  Loss_1: (0.4954) | Acc_1: (81.45%) (17828/21888)\n",
      "Epoch: 156 | Batch_idx: 180 |  Loss_1: (0.4938) | Acc_1: (81.50%) (18881/23168)\n",
      "Epoch: 156 | Batch_idx: 190 |  Loss_1: (0.4942) | Acc_1: (81.50%) (19925/24448)\n",
      "Epoch: 156 | Batch_idx: 200 |  Loss_1: (0.4932) | Acc_1: (81.55%) (20981/25728)\n",
      "Epoch: 156 | Batch_idx: 210 |  Loss_1: (0.4911) | Acc_1: (81.62%) (22043/27008)\n",
      "Epoch: 156 | Batch_idx: 220 |  Loss_1: (0.4898) | Acc_1: (81.67%) (23103/28288)\n",
      "Epoch: 156 | Batch_idx: 230 |  Loss_1: (0.4877) | Acc_1: (81.73%) (24167/29568)\n",
      "Epoch: 156 | Batch_idx: 240 |  Loss_1: (0.4880) | Acc_1: (81.73%) (25213/30848)\n",
      "Epoch: 156 | Batch_idx: 250 |  Loss_1: (0.4865) | Acc_1: (81.79%) (26278/32128)\n",
      "Epoch: 156 | Batch_idx: 260 |  Loss_1: (0.4877) | Acc_1: (81.75%) (27310/33408)\n",
      "Epoch: 156 | Batch_idx: 270 |  Loss_1: (0.4882) | Acc_1: (81.71%) (28343/34688)\n",
      "Epoch: 156 | Batch_idx: 280 |  Loss_1: (0.4871) | Acc_1: (81.76%) (29406/35968)\n",
      "Epoch: 156 | Batch_idx: 290 |  Loss_1: (0.4871) | Acc_1: (81.76%) (30454/37248)\n",
      "Epoch: 156 | Batch_idx: 300 |  Loss_1: (0.4858) | Acc_1: (81.82%) (31522/38528)\n",
      "Epoch: 156 | Batch_idx: 310 |  Loss_1: (0.4862) | Acc_1: (81.82%) (32571/39808)\n",
      "Epoch: 156 | Batch_idx: 320 |  Loss_1: (0.4868) | Acc_1: (81.80%) (33612/41088)\n",
      "Epoch: 156 | Batch_idx: 330 |  Loss_1: (0.4862) | Acc_1: (81.83%) (34671/42368)\n",
      "Epoch: 156 | Batch_idx: 340 |  Loss_1: (0.4860) | Acc_1: (81.85%) (35726/43648)\n",
      "Epoch: 156 | Batch_idx: 350 |  Loss_1: (0.4857) | Acc_1: (81.87%) (36781/44928)\n",
      "Epoch: 156 | Batch_idx: 360 |  Loss_1: (0.4858) | Acc_1: (81.86%) (37828/46208)\n",
      "Epoch: 156 | Batch_idx: 370 |  Loss_1: (0.4855) | Acc_1: (81.86%) (38876/47488)\n",
      "Epoch: 156 | Batch_idx: 380 |  Loss_1: (0.4855) | Acc_1: (81.86%) (39922/48768)\n",
      "Epoch: 156 | Batch_idx: 390 |  Loss_1: (0.4856) | Acc_1: (81.86%) (40929/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4720) | Acc: (90.40%) (9040/10000)\n",
      "Epoch: 157 | Batch_idx: 0 |  Loss_1: (0.4547) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 157 | Batch_idx: 10 |  Loss_1: (0.5040) | Acc_1: (80.89%) (1139/1408)\n",
      "Epoch: 157 | Batch_idx: 20 |  Loss_1: (0.5184) | Acc_1: (80.92%) (2175/2688)\n",
      "Epoch: 157 | Batch_idx: 30 |  Loss_1: (0.5093) | Acc_1: (81.17%) (3221/3968)\n",
      "Epoch: 157 | Batch_idx: 40 |  Loss_1: (0.5074) | Acc_1: (81.08%) (4255/5248)\n",
      "Epoch: 157 | Batch_idx: 50 |  Loss_1: (0.5068) | Acc_1: (81.08%) (5293/6528)\n",
      "Epoch: 157 | Batch_idx: 60 |  Loss_1: (0.5120) | Acc_1: (80.92%) (6318/7808)\n",
      "Epoch: 157 | Batch_idx: 70 |  Loss_1: (0.5099) | Acc_1: (81.01%) (7362/9088)\n",
      "Epoch: 157 | Batch_idx: 80 |  Loss_1: (0.5040) | Acc_1: (81.23%) (8422/10368)\n",
      "Epoch: 157 | Batch_idx: 90 |  Loss_1: (0.5033) | Acc_1: (81.24%) (9463/11648)\n",
      "Epoch: 157 | Batch_idx: 100 |  Loss_1: (0.5033) | Acc_1: (81.21%) (10499/12928)\n",
      "Epoch: 157 | Batch_idx: 110 |  Loss_1: (0.5011) | Acc_1: (81.38%) (11563/14208)\n",
      "Epoch: 157 | Batch_idx: 120 |  Loss_1: (0.4960) | Acc_1: (81.59%) (12636/15488)\n",
      "Epoch: 157 | Batch_idx: 130 |  Loss_1: (0.4977) | Acc_1: (81.50%) (13666/16768)\n",
      "Epoch: 157 | Batch_idx: 140 |  Loss_1: (0.4971) | Acc_1: (81.52%) (14713/18048)\n",
      "Epoch: 157 | Batch_idx: 150 |  Loss_1: (0.4962) | Acc_1: (81.56%) (15764/19328)\n",
      "Epoch: 157 | Batch_idx: 160 |  Loss_1: (0.4946) | Acc_1: (81.67%) (16830/20608)\n",
      "Epoch: 157 | Batch_idx: 170 |  Loss_1: (0.4956) | Acc_1: (81.64%) (17869/21888)\n",
      "Epoch: 157 | Batch_idx: 180 |  Loss_1: (0.4964) | Acc_1: (81.59%) (18903/23168)\n",
      "Epoch: 157 | Batch_idx: 190 |  Loss_1: (0.4968) | Acc_1: (81.57%) (19942/24448)\n",
      "Epoch: 157 | Batch_idx: 200 |  Loss_1: (0.4954) | Acc_1: (81.62%) (20999/25728)\n",
      "Epoch: 157 | Batch_idx: 210 |  Loss_1: (0.4939) | Acc_1: (81.70%) (22066/27008)\n",
      "Epoch: 157 | Batch_idx: 220 |  Loss_1: (0.4922) | Acc_1: (81.77%) (23131/28288)\n",
      "Epoch: 157 | Batch_idx: 230 |  Loss_1: (0.4904) | Acc_1: (81.80%) (24188/29568)\n",
      "Epoch: 157 | Batch_idx: 240 |  Loss_1: (0.4893) | Acc_1: (81.84%) (25247/30848)\n",
      "Epoch: 157 | Batch_idx: 250 |  Loss_1: (0.4889) | Acc_1: (81.86%) (26299/32128)\n",
      "Epoch: 157 | Batch_idx: 260 |  Loss_1: (0.4898) | Acc_1: (81.84%) (27340/33408)\n",
      "Epoch: 157 | Batch_idx: 270 |  Loss_1: (0.4876) | Acc_1: (81.93%) (28420/34688)\n",
      "Epoch: 157 | Batch_idx: 280 |  Loss_1: (0.4862) | Acc_1: (81.97%) (29484/35968)\n",
      "Epoch: 157 | Batch_idx: 290 |  Loss_1: (0.4864) | Acc_1: (81.96%) (30530/37248)\n",
      "Epoch: 157 | Batch_idx: 300 |  Loss_1: (0.4864) | Acc_1: (81.96%) (31577/38528)\n",
      "Epoch: 157 | Batch_idx: 310 |  Loss_1: (0.4862) | Acc_1: (81.98%) (32636/39808)\n",
      "Epoch: 157 | Batch_idx: 320 |  Loss_1: (0.4850) | Acc_1: (82.02%) (33699/41088)\n",
      "Epoch: 157 | Batch_idx: 330 |  Loss_1: (0.4856) | Acc_1: (82.01%) (34745/42368)\n",
      "Epoch: 157 | Batch_idx: 340 |  Loss_1: (0.4873) | Acc_1: (81.94%) (35767/43648)\n",
      "Epoch: 157 | Batch_idx: 350 |  Loss_1: (0.4869) | Acc_1: (81.95%) (36820/44928)\n",
      "Epoch: 157 | Batch_idx: 360 |  Loss_1: (0.4855) | Acc_1: (81.99%) (37886/46208)\n",
      "Epoch: 157 | Batch_idx: 370 |  Loss_1: (0.4858) | Acc_1: (81.97%) (38924/47488)\n",
      "Epoch: 157 | Batch_idx: 380 |  Loss_1: (0.4862) | Acc_1: (81.95%) (39966/48768)\n",
      "Epoch: 157 | Batch_idx: 390 |  Loss_1: (0.4848) | Acc_1: (81.99%) (40997/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4727) | Acc: (90.47%) (9047/10000)\n",
      "Epoch: 158 | Batch_idx: 0 |  Loss_1: (0.4348) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 158 | Batch_idx: 10 |  Loss_1: (0.4638) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 158 | Batch_idx: 20 |  Loss_1: (0.4747) | Acc_1: (82.22%) (2210/2688)\n",
      "Epoch: 158 | Batch_idx: 30 |  Loss_1: (0.4825) | Acc_1: (82.01%) (3254/3968)\n",
      "Epoch: 158 | Batch_idx: 40 |  Loss_1: (0.4891) | Acc_1: (81.86%) (4296/5248)\n",
      "Epoch: 158 | Batch_idx: 50 |  Loss_1: (0.4917) | Acc_1: (81.65%) (5330/6528)\n",
      "Epoch: 158 | Batch_idx: 60 |  Loss_1: (0.4916) | Acc_1: (81.62%) (6373/7808)\n",
      "Epoch: 158 | Batch_idx: 70 |  Loss_1: (0.4903) | Acc_1: (81.64%) (7419/9088)\n",
      "Epoch: 158 | Batch_idx: 80 |  Loss_1: (0.4837) | Acc_1: (81.89%) (8490/10368)\n",
      "Epoch: 158 | Batch_idx: 90 |  Loss_1: (0.4862) | Acc_1: (81.79%) (9527/11648)\n",
      "Epoch: 158 | Batch_idx: 100 |  Loss_1: (0.4862) | Acc_1: (81.78%) (10572/12928)\n",
      "Epoch: 158 | Batch_idx: 110 |  Loss_1: (0.4861) | Acc_1: (81.82%) (11625/14208)\n",
      "Epoch: 158 | Batch_idx: 120 |  Loss_1: (0.4864) | Acc_1: (81.81%) (12671/15488)\n",
      "Epoch: 158 | Batch_idx: 130 |  Loss_1: (0.4879) | Acc_1: (81.76%) (13710/16768)\n",
      "Epoch: 158 | Batch_idx: 140 |  Loss_1: (0.4882) | Acc_1: (81.75%) (14755/18048)\n",
      "Epoch: 158 | Batch_idx: 150 |  Loss_1: (0.4872) | Acc_1: (81.76%) (15802/19328)\n",
      "Epoch: 158 | Batch_idx: 160 |  Loss_1: (0.4856) | Acc_1: (81.85%) (16867/20608)\n",
      "Epoch: 158 | Batch_idx: 170 |  Loss_1: (0.4823) | Acc_1: (82.02%) (17952/21888)\n",
      "Epoch: 158 | Batch_idx: 180 |  Loss_1: (0.4816) | Acc_1: (82.06%) (19011/23168)\n",
      "Epoch: 158 | Batch_idx: 190 |  Loss_1: (0.4818) | Acc_1: (82.06%) (20062/24448)\n",
      "Epoch: 158 | Batch_idx: 200 |  Loss_1: (0.4824) | Acc_1: (82.03%) (21105/25728)\n",
      "Epoch: 158 | Batch_idx: 210 |  Loss_1: (0.4824) | Acc_1: (82.05%) (22161/27008)\n",
      "Epoch: 158 | Batch_idx: 220 |  Loss_1: (0.4831) | Acc_1: (82.02%) (23201/28288)\n",
      "Epoch: 158 | Batch_idx: 230 |  Loss_1: (0.4839) | Acc_1: (81.99%) (24242/29568)\n",
      "Epoch: 158 | Batch_idx: 240 |  Loss_1: (0.4834) | Acc_1: (82.02%) (25300/30848)\n",
      "Epoch: 158 | Batch_idx: 250 |  Loss_1: (0.4831) | Acc_1: (82.02%) (26352/32128)\n",
      "Epoch: 158 | Batch_idx: 260 |  Loss_1: (0.4834) | Acc_1: (82.01%) (27399/33408)\n",
      "Epoch: 158 | Batch_idx: 270 |  Loss_1: (0.4854) | Acc_1: (81.94%) (28423/34688)\n",
      "Epoch: 158 | Batch_idx: 280 |  Loss_1: (0.4868) | Acc_1: (81.88%) (29451/35968)\n",
      "Epoch: 158 | Batch_idx: 290 |  Loss_1: (0.4881) | Acc_1: (81.84%) (30482/37248)\n",
      "Epoch: 158 | Batch_idx: 300 |  Loss_1: (0.4877) | Acc_1: (81.85%) (31534/38528)\n",
      "Epoch: 158 | Batch_idx: 310 |  Loss_1: (0.4871) | Acc_1: (81.88%) (32595/39808)\n",
      "Epoch: 158 | Batch_idx: 320 |  Loss_1: (0.4869) | Acc_1: (81.88%) (33644/41088)\n",
      "Epoch: 158 | Batch_idx: 330 |  Loss_1: (0.4863) | Acc_1: (81.90%) (34701/42368)\n",
      "Epoch: 158 | Batch_idx: 340 |  Loss_1: (0.4870) | Acc_1: (81.88%) (35738/43648)\n",
      "Epoch: 158 | Batch_idx: 350 |  Loss_1: (0.4898) | Acc_1: (81.77%) (36738/44928)\n",
      "Epoch: 158 | Batch_idx: 360 |  Loss_1: (0.4899) | Acc_1: (81.78%) (37789/46208)\n",
      "Epoch: 158 | Batch_idx: 370 |  Loss_1: (0.4897) | Acc_1: (81.79%) (38839/47488)\n",
      "Epoch: 158 | Batch_idx: 380 |  Loss_1: (0.4894) | Acc_1: (81.79%) (39888/48768)\n",
      "Epoch: 158 | Batch_idx: 390 |  Loss_1: (0.4895) | Acc_1: (81.79%) (40896/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4543) | Acc: (90.60%) (9060/10000)\n",
      "Epoch: 159 | Batch_idx: 0 |  Loss_1: (0.5110) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 159 | Batch_idx: 10 |  Loss_1: (0.4715) | Acc_1: (82.17%) (1157/1408)\n",
      "Epoch: 159 | Batch_idx: 20 |  Loss_1: (0.4730) | Acc_1: (82.25%) (2211/2688)\n",
      "Epoch: 159 | Batch_idx: 30 |  Loss_1: (0.4814) | Acc_1: (81.96%) (3252/3968)\n",
      "Epoch: 159 | Batch_idx: 40 |  Loss_1: (0.4747) | Acc_1: (82.36%) (4322/5248)\n",
      "Epoch: 159 | Batch_idx: 50 |  Loss_1: (0.4770) | Acc_1: (82.40%) (5379/6528)\n",
      "Epoch: 159 | Batch_idx: 60 |  Loss_1: (0.4755) | Acc_1: (82.42%) (6435/7808)\n",
      "Epoch: 159 | Batch_idx: 70 |  Loss_1: (0.4781) | Acc_1: (82.30%) (7479/9088)\n",
      "Epoch: 159 | Batch_idx: 80 |  Loss_1: (0.4815) | Acc_1: (82.17%) (8519/10368)\n",
      "Epoch: 159 | Batch_idx: 90 |  Loss_1: (0.4792) | Acc_1: (82.24%) (9579/11648)\n",
      "Epoch: 159 | Batch_idx: 100 |  Loss_1: (0.4793) | Acc_1: (82.23%) (10631/12928)\n",
      "Epoch: 159 | Batch_idx: 110 |  Loss_1: (0.4813) | Acc_1: (82.21%) (11681/14208)\n",
      "Epoch: 159 | Batch_idx: 120 |  Loss_1: (0.4776) | Acc_1: (82.35%) (12754/15488)\n",
      "Epoch: 159 | Batch_idx: 130 |  Loss_1: (0.4760) | Acc_1: (82.41%) (13819/16768)\n",
      "Epoch: 159 | Batch_idx: 140 |  Loss_1: (0.4767) | Acc_1: (82.38%) (14868/18048)\n",
      "Epoch: 159 | Batch_idx: 150 |  Loss_1: (0.4770) | Acc_1: (82.35%) (15916/19328)\n",
      "Epoch: 159 | Batch_idx: 160 |  Loss_1: (0.4771) | Acc_1: (82.36%) (16973/20608)\n",
      "Epoch: 159 | Batch_idx: 170 |  Loss_1: (0.4788) | Acc_1: (82.28%) (18009/21888)\n",
      "Epoch: 159 | Batch_idx: 180 |  Loss_1: (0.4812) | Acc_1: (82.18%) (19040/23168)\n",
      "Epoch: 159 | Batch_idx: 190 |  Loss_1: (0.4846) | Acc_1: (82.07%) (20064/24448)\n",
      "Epoch: 159 | Batch_idx: 200 |  Loss_1: (0.4845) | Acc_1: (82.05%) (21111/25728)\n",
      "Epoch: 159 | Batch_idx: 210 |  Loss_1: (0.4843) | Acc_1: (82.05%) (22160/27008)\n",
      "Epoch: 159 | Batch_idx: 220 |  Loss_1: (0.4828) | Acc_1: (82.11%) (23226/28288)\n",
      "Epoch: 159 | Batch_idx: 230 |  Loss_1: (0.4820) | Acc_1: (82.14%) (24288/29568)\n",
      "Epoch: 159 | Batch_idx: 240 |  Loss_1: (0.4810) | Acc_1: (82.17%) (25348/30848)\n",
      "Epoch: 159 | Batch_idx: 250 |  Loss_1: (0.4815) | Acc_1: (82.14%) (26390/32128)\n",
      "Epoch: 159 | Batch_idx: 260 |  Loss_1: (0.4807) | Acc_1: (82.16%) (27449/33408)\n",
      "Epoch: 159 | Batch_idx: 270 |  Loss_1: (0.4806) | Acc_1: (82.16%) (28498/34688)\n",
      "Epoch: 159 | Batch_idx: 280 |  Loss_1: (0.4804) | Acc_1: (82.17%) (29554/35968)\n",
      "Epoch: 159 | Batch_idx: 290 |  Loss_1: (0.4808) | Acc_1: (82.13%) (30593/37248)\n",
      "Epoch: 159 | Batch_idx: 300 |  Loss_1: (0.4816) | Acc_1: (82.11%) (31637/38528)\n",
      "Epoch: 159 | Batch_idx: 310 |  Loss_1: (0.4828) | Acc_1: (82.07%) (32671/39808)\n",
      "Epoch: 159 | Batch_idx: 320 |  Loss_1: (0.4830) | Acc_1: (82.06%) (33717/41088)\n",
      "Epoch: 159 | Batch_idx: 330 |  Loss_1: (0.4836) | Acc_1: (82.04%) (34760/42368)\n",
      "Epoch: 159 | Batch_idx: 340 |  Loss_1: (0.4820) | Acc_1: (82.09%) (35832/43648)\n",
      "Epoch: 159 | Batch_idx: 350 |  Loss_1: (0.4821) | Acc_1: (82.09%) (36882/44928)\n",
      "Epoch: 159 | Batch_idx: 360 |  Loss_1: (0.4826) | Acc_1: (82.06%) (37917/46208)\n",
      "Epoch: 159 | Batch_idx: 370 |  Loss_1: (0.4815) | Acc_1: (82.09%) (38983/47488)\n",
      "Epoch: 159 | Batch_idx: 380 |  Loss_1: (0.4813) | Acc_1: (82.10%) (40040/48768)\n",
      "Epoch: 159 | Batch_idx: 390 |  Loss_1: (0.4810) | Acc_1: (82.11%) (41053/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5054) | Acc: (90.10%) (9010/10000)\n",
      "Epoch: 160 | Batch_idx: 0 |  Loss_1: (0.3608) | Acc_1: (85.94%) (110/128)\n",
      "Epoch: 160 | Batch_idx: 10 |  Loss_1: (0.4726) | Acc_1: (82.24%) (1158/1408)\n",
      "Epoch: 160 | Batch_idx: 20 |  Loss_1: (0.4849) | Acc_1: (81.70%) (2196/2688)\n",
      "Epoch: 160 | Batch_idx: 30 |  Loss_1: (0.4855) | Acc_1: (81.83%) (3247/3968)\n",
      "Epoch: 160 | Batch_idx: 40 |  Loss_1: (0.4873) | Acc_1: (81.69%) (4287/5248)\n",
      "Epoch: 160 | Batch_idx: 50 |  Loss_1: (0.4906) | Acc_1: (81.60%) (5327/6528)\n",
      "Epoch: 160 | Batch_idx: 60 |  Loss_1: (0.4916) | Acc_1: (81.56%) (6368/7808)\n",
      "Epoch: 160 | Batch_idx: 70 |  Loss_1: (0.4899) | Acc_1: (81.65%) (7420/9088)\n",
      "Epoch: 160 | Batch_idx: 80 |  Loss_1: (0.4878) | Acc_1: (81.78%) (8479/10368)\n",
      "Epoch: 160 | Batch_idx: 90 |  Loss_1: (0.4813) | Acc_1: (81.98%) (9549/11648)\n",
      "Epoch: 160 | Batch_idx: 100 |  Loss_1: (0.4808) | Acc_1: (81.99%) (10600/12928)\n",
      "Epoch: 160 | Batch_idx: 110 |  Loss_1: (0.4798) | Acc_1: (82.04%) (11656/14208)\n",
      "Epoch: 160 | Batch_idx: 120 |  Loss_1: (0.4773) | Acc_1: (82.14%) (12722/15488)\n",
      "Epoch: 160 | Batch_idx: 130 |  Loss_1: (0.4815) | Acc_1: (82.00%) (13750/16768)\n",
      "Epoch: 160 | Batch_idx: 140 |  Loss_1: (0.4811) | Acc_1: (82.05%) (14809/18048)\n",
      "Epoch: 160 | Batch_idx: 150 |  Loss_1: (0.4809) | Acc_1: (82.04%) (15856/19328)\n",
      "Epoch: 160 | Batch_idx: 160 |  Loss_1: (0.4837) | Acc_1: (81.92%) (16883/20608)\n",
      "Epoch: 160 | Batch_idx: 170 |  Loss_1: (0.4851) | Acc_1: (81.89%) (17924/21888)\n",
      "Epoch: 160 | Batch_idx: 180 |  Loss_1: (0.4855) | Acc_1: (81.88%) (18970/23168)\n",
      "Epoch: 160 | Batch_idx: 190 |  Loss_1: (0.4842) | Acc_1: (81.95%) (20034/24448)\n",
      "Epoch: 160 | Batch_idx: 200 |  Loss_1: (0.4843) | Acc_1: (81.95%) (21085/25728)\n",
      "Epoch: 160 | Batch_idx: 210 |  Loss_1: (0.4854) | Acc_1: (81.94%) (22130/27008)\n",
      "Epoch: 160 | Batch_idx: 220 |  Loss_1: (0.4839) | Acc_1: (82.00%) (23197/28288)\n",
      "Epoch: 160 | Batch_idx: 230 |  Loss_1: (0.4836) | Acc_1: (82.00%) (24245/29568)\n",
      "Epoch: 160 | Batch_idx: 240 |  Loss_1: (0.4836) | Acc_1: (82.00%) (25294/30848)\n",
      "Epoch: 160 | Batch_idx: 250 |  Loss_1: (0.4832) | Acc_1: (82.02%) (26350/32128)\n",
      "Epoch: 160 | Batch_idx: 260 |  Loss_1: (0.4840) | Acc_1: (81.98%) (27389/33408)\n",
      "Epoch: 160 | Batch_idx: 270 |  Loss_1: (0.4844) | Acc_1: (81.97%) (28435/34688)\n",
      "Epoch: 160 | Batch_idx: 280 |  Loss_1: (0.4851) | Acc_1: (81.95%) (29477/35968)\n",
      "Epoch: 160 | Batch_idx: 290 |  Loss_1: (0.4853) | Acc_1: (81.94%) (30520/37248)\n",
      "Epoch: 160 | Batch_idx: 300 |  Loss_1: (0.4859) | Acc_1: (81.91%) (31559/38528)\n",
      "Epoch: 160 | Batch_idx: 310 |  Loss_1: (0.4846) | Acc_1: (81.93%) (32614/39808)\n",
      "Epoch: 160 | Batch_idx: 320 |  Loss_1: (0.4876) | Acc_1: (81.80%) (33610/41088)\n",
      "Epoch: 160 | Batch_idx: 330 |  Loss_1: (0.4866) | Acc_1: (81.84%) (34673/42368)\n",
      "Epoch: 160 | Batch_idx: 340 |  Loss_1: (0.4877) | Acc_1: (81.79%) (35698/43648)\n",
      "Epoch: 160 | Batch_idx: 350 |  Loss_1: (0.4872) | Acc_1: (81.82%) (36758/44928)\n",
      "Epoch: 160 | Batch_idx: 360 |  Loss_1: (0.4875) | Acc_1: (81.81%) (37805/46208)\n",
      "Epoch: 160 | Batch_idx: 370 |  Loss_1: (0.4878) | Acc_1: (81.81%) (38848/47488)\n",
      "Epoch: 160 | Batch_idx: 380 |  Loss_1: (0.4888) | Acc_1: (81.78%) (39882/48768)\n",
      "Epoch: 160 | Batch_idx: 390 |  Loss_1: (0.4892) | Acc_1: (81.77%) (40885/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4625) | Acc: (90.46%) (9046/10000)\n",
      "Epoch: 161 | Batch_idx: 0 |  Loss_1: (0.5426) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 161 | Batch_idx: 10 |  Loss_1: (0.4916) | Acc_1: (81.68%) (1150/1408)\n",
      "Epoch: 161 | Batch_idx: 20 |  Loss_1: (0.4840) | Acc_1: (81.85%) (2200/2688)\n",
      "Epoch: 161 | Batch_idx: 30 |  Loss_1: (0.4967) | Acc_1: (81.45%) (3232/3968)\n",
      "Epoch: 161 | Batch_idx: 40 |  Loss_1: (0.5028) | Acc_1: (81.14%) (4258/5248)\n",
      "Epoch: 161 | Batch_idx: 50 |  Loss_1: (0.4987) | Acc_1: (81.30%) (5307/6528)\n",
      "Epoch: 161 | Batch_idx: 60 |  Loss_1: (0.5021) | Acc_1: (81.19%) (6339/7808)\n",
      "Epoch: 161 | Batch_idx: 70 |  Loss_1: (0.5000) | Acc_1: (81.25%) (7384/9088)\n",
      "Epoch: 161 | Batch_idx: 80 |  Loss_1: (0.4934) | Acc_1: (81.45%) (8445/10368)\n",
      "Epoch: 161 | Batch_idx: 90 |  Loss_1: (0.4926) | Acc_1: (81.47%) (9490/11648)\n",
      "Epoch: 161 | Batch_idx: 100 |  Loss_1: (0.4930) | Acc_1: (81.45%) (10530/12928)\n",
      "Epoch: 161 | Batch_idx: 110 |  Loss_1: (0.4885) | Acc_1: (81.67%) (11603/14208)\n",
      "Epoch: 161 | Batch_idx: 120 |  Loss_1: (0.4874) | Acc_1: (81.71%) (12655/15488)\n",
      "Epoch: 161 | Batch_idx: 130 |  Loss_1: (0.4876) | Acc_1: (81.70%) (13700/16768)\n",
      "Epoch: 161 | Batch_idx: 140 |  Loss_1: (0.4878) | Acc_1: (81.68%) (14742/18048)\n",
      "Epoch: 161 | Batch_idx: 150 |  Loss_1: (0.4890) | Acc_1: (81.61%) (15773/19328)\n",
      "Epoch: 161 | Batch_idx: 160 |  Loss_1: (0.4871) | Acc_1: (81.68%) (16832/20608)\n",
      "Epoch: 161 | Batch_idx: 170 |  Loss_1: (0.4868) | Acc_1: (81.71%) (17884/21888)\n",
      "Epoch: 161 | Batch_idx: 180 |  Loss_1: (0.4860) | Acc_1: (81.74%) (18937/23168)\n",
      "Epoch: 161 | Batch_idx: 190 |  Loss_1: (0.4883) | Acc_1: (81.63%) (19956/24448)\n",
      "Epoch: 161 | Batch_idx: 200 |  Loss_1: (0.4889) | Acc_1: (81.63%) (21001/25728)\n",
      "Epoch: 161 | Batch_idx: 210 |  Loss_1: (0.4874) | Acc_1: (81.69%) (22063/27008)\n",
      "Epoch: 161 | Batch_idx: 220 |  Loss_1: (0.4875) | Acc_1: (81.71%) (23115/28288)\n",
      "Epoch: 161 | Batch_idx: 230 |  Loss_1: (0.4878) | Acc_1: (81.69%) (24155/29568)\n",
      "Epoch: 161 | Batch_idx: 240 |  Loss_1: (0.4898) | Acc_1: (81.62%) (25178/30848)\n",
      "Epoch: 161 | Batch_idx: 250 |  Loss_1: (0.4896) | Acc_1: (81.61%) (26220/32128)\n",
      "Epoch: 161 | Batch_idx: 260 |  Loss_1: (0.4907) | Acc_1: (81.56%) (27247/33408)\n",
      "Epoch: 161 | Batch_idx: 270 |  Loss_1: (0.4908) | Acc_1: (81.56%) (28291/34688)\n",
      "Epoch: 161 | Batch_idx: 280 |  Loss_1: (0.4911) | Acc_1: (81.55%) (29331/35968)\n",
      "Epoch: 161 | Batch_idx: 290 |  Loss_1: (0.4903) | Acc_1: (81.59%) (30390/37248)\n",
      "Epoch: 161 | Batch_idx: 300 |  Loss_1: (0.4886) | Acc_1: (81.64%) (31456/38528)\n",
      "Epoch: 161 | Batch_idx: 310 |  Loss_1: (0.4890) | Acc_1: (81.62%) (32493/39808)\n",
      "Epoch: 161 | Batch_idx: 320 |  Loss_1: (0.4884) | Acc_1: (81.64%) (33545/41088)\n",
      "Epoch: 161 | Batch_idx: 330 |  Loss_1: (0.4892) | Acc_1: (81.60%) (34571/42368)\n",
      "Epoch: 161 | Batch_idx: 340 |  Loss_1: (0.4884) | Acc_1: (81.62%) (35626/43648)\n",
      "Epoch: 161 | Batch_idx: 350 |  Loss_1: (0.4879) | Acc_1: (81.65%) (36682/44928)\n",
      "Epoch: 161 | Batch_idx: 360 |  Loss_1: (0.4878) | Acc_1: (81.66%) (37733/46208)\n",
      "Epoch: 161 | Batch_idx: 370 |  Loss_1: (0.4889) | Acc_1: (81.63%) (38763/47488)\n",
      "Epoch: 161 | Batch_idx: 380 |  Loss_1: (0.4883) | Acc_1: (81.67%) (39828/48768)\n",
      "Epoch: 161 | Batch_idx: 390 |  Loss_1: (0.4893) | Acc_1: (81.63%) (40816/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4834) | Acc: (90.23%) (9023/10000)\n",
      "Epoch: 162 | Batch_idx: 0 |  Loss_1: (0.4869) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 162 | Batch_idx: 10 |  Loss_1: (0.4686) | Acc_1: (82.24%) (1158/1408)\n",
      "Epoch: 162 | Batch_idx: 20 |  Loss_1: (0.4625) | Acc_1: (82.51%) (2218/2688)\n",
      "Epoch: 162 | Batch_idx: 30 |  Loss_1: (0.4675) | Acc_1: (82.56%) (3276/3968)\n",
      "Epoch: 162 | Batch_idx: 40 |  Loss_1: (0.4869) | Acc_1: (81.84%) (4295/5248)\n",
      "Epoch: 162 | Batch_idx: 50 |  Loss_1: (0.4910) | Acc_1: (81.72%) (5335/6528)\n",
      "Epoch: 162 | Batch_idx: 60 |  Loss_1: (0.4877) | Acc_1: (81.80%) (6387/7808)\n",
      "Epoch: 162 | Batch_idx: 70 |  Loss_1: (0.4900) | Acc_1: (81.73%) (7428/9088)\n",
      "Epoch: 162 | Batch_idx: 80 |  Loss_1: (0.4916) | Acc_1: (81.66%) (8466/10368)\n",
      "Epoch: 162 | Batch_idx: 90 |  Loss_1: (0.4929) | Acc_1: (81.66%) (9512/11648)\n",
      "Epoch: 162 | Batch_idx: 100 |  Loss_1: (0.4918) | Acc_1: (81.69%) (10561/12928)\n",
      "Epoch: 162 | Batch_idx: 110 |  Loss_1: (0.4901) | Acc_1: (81.70%) (11608/14208)\n",
      "Epoch: 162 | Batch_idx: 120 |  Loss_1: (0.4880) | Acc_1: (81.77%) (12665/15488)\n",
      "Epoch: 162 | Batch_idx: 130 |  Loss_1: (0.4895) | Acc_1: (81.70%) (13699/16768)\n",
      "Epoch: 162 | Batch_idx: 140 |  Loss_1: (0.4900) | Acc_1: (81.65%) (14736/18048)\n",
      "Epoch: 162 | Batch_idx: 150 |  Loss_1: (0.4915) | Acc_1: (81.61%) (15774/19328)\n",
      "Epoch: 162 | Batch_idx: 160 |  Loss_1: (0.4910) | Acc_1: (81.63%) (16823/20608)\n",
      "Epoch: 162 | Batch_idx: 170 |  Loss_1: (0.4914) | Acc_1: (81.62%) (17864/21888)\n",
      "Epoch: 162 | Batch_idx: 180 |  Loss_1: (0.4935) | Acc_1: (81.56%) (18896/23168)\n",
      "Epoch: 162 | Batch_idx: 190 |  Loss_1: (0.4965) | Acc_1: (81.45%) (19914/24448)\n",
      "Epoch: 162 | Batch_idx: 200 |  Loss_1: (0.4978) | Acc_1: (81.40%) (20943/25728)\n",
      "Epoch: 162 | Batch_idx: 210 |  Loss_1: (0.4968) | Acc_1: (81.44%) (21994/27008)\n",
      "Epoch: 162 | Batch_idx: 220 |  Loss_1: (0.4953) | Acc_1: (81.49%) (23053/28288)\n",
      "Epoch: 162 | Batch_idx: 230 |  Loss_1: (0.4954) | Acc_1: (81.48%) (24093/29568)\n",
      "Epoch: 162 | Batch_idx: 240 |  Loss_1: (0.4960) | Acc_1: (81.47%) (25131/30848)\n",
      "Epoch: 162 | Batch_idx: 250 |  Loss_1: (0.4960) | Acc_1: (81.49%) (26181/32128)\n",
      "Epoch: 162 | Batch_idx: 260 |  Loss_1: (0.4965) | Acc_1: (81.49%) (27223/33408)\n",
      "Epoch: 162 | Batch_idx: 270 |  Loss_1: (0.4950) | Acc_1: (81.54%) (28284/34688)\n",
      "Epoch: 162 | Batch_idx: 280 |  Loss_1: (0.4951) | Acc_1: (81.53%) (29323/35968)\n",
      "Epoch: 162 | Batch_idx: 290 |  Loss_1: (0.4939) | Acc_1: (81.57%) (30384/37248)\n",
      "Epoch: 162 | Batch_idx: 300 |  Loss_1: (0.4935) | Acc_1: (81.61%) (31443/38528)\n",
      "Epoch: 162 | Batch_idx: 310 |  Loss_1: (0.4927) | Acc_1: (81.64%) (32498/39808)\n",
      "Epoch: 162 | Batch_idx: 320 |  Loss_1: (0.4927) | Acc_1: (81.63%) (33542/41088)\n",
      "Epoch: 162 | Batch_idx: 330 |  Loss_1: (0.4923) | Acc_1: (81.65%) (34595/42368)\n",
      "Epoch: 162 | Batch_idx: 340 |  Loss_1: (0.4928) | Acc_1: (81.63%) (35632/43648)\n",
      "Epoch: 162 | Batch_idx: 350 |  Loss_1: (0.4927) | Acc_1: (81.62%) (36671/44928)\n",
      "Epoch: 162 | Batch_idx: 360 |  Loss_1: (0.4919) | Acc_1: (81.65%) (37730/46208)\n",
      "Epoch: 162 | Batch_idx: 370 |  Loss_1: (0.4935) | Acc_1: (81.58%) (38742/47488)\n",
      "Epoch: 162 | Batch_idx: 380 |  Loss_1: (0.4930) | Acc_1: (81.60%) (39796/48768)\n",
      "Epoch: 162 | Batch_idx: 390 |  Loss_1: (0.4930) | Acc_1: (81.59%) (40797/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5064) | Acc: (90.36%) (9036/10000)\n",
      "Epoch: 163 | Batch_idx: 0 |  Loss_1: (0.6520) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 163 | Batch_idx: 10 |  Loss_1: (0.5860) | Acc_1: (78.27%) (1102/1408)\n",
      "Epoch: 163 | Batch_idx: 20 |  Loss_1: (0.5407) | Acc_1: (79.91%) (2148/2688)\n",
      "Epoch: 163 | Batch_idx: 30 |  Loss_1: (0.5212) | Acc_1: (80.49%) (3194/3968)\n",
      "Epoch: 163 | Batch_idx: 40 |  Loss_1: (0.5233) | Acc_1: (80.58%) (4229/5248)\n",
      "Epoch: 163 | Batch_idx: 50 |  Loss_1: (0.5139) | Acc_1: (80.88%) (5280/6528)\n",
      "Epoch: 163 | Batch_idx: 60 |  Loss_1: (0.5074) | Acc_1: (81.15%) (6336/7808)\n",
      "Epoch: 163 | Batch_idx: 70 |  Loss_1: (0.5028) | Acc_1: (81.34%) (7392/9088)\n",
      "Epoch: 163 | Batch_idx: 80 |  Loss_1: (0.4955) | Acc_1: (81.56%) (8456/10368)\n",
      "Epoch: 163 | Batch_idx: 90 |  Loss_1: (0.4937) | Acc_1: (81.65%) (9511/11648)\n",
      "Epoch: 163 | Batch_idx: 100 |  Loss_1: (0.4866) | Acc_1: (81.87%) (10584/12928)\n",
      "Epoch: 163 | Batch_idx: 110 |  Loss_1: (0.4829) | Acc_1: (82.05%) (11657/14208)\n",
      "Epoch: 163 | Batch_idx: 120 |  Loss_1: (0.4881) | Acc_1: (81.82%) (12673/15488)\n",
      "Epoch: 163 | Batch_idx: 130 |  Loss_1: (0.4867) | Acc_1: (81.88%) (13729/16768)\n",
      "Epoch: 163 | Batch_idx: 140 |  Loss_1: (0.4826) | Acc_1: (82.04%) (14807/18048)\n",
      "Epoch: 163 | Batch_idx: 150 |  Loss_1: (0.4799) | Acc_1: (82.14%) (15876/19328)\n",
      "Epoch: 163 | Batch_idx: 160 |  Loss_1: (0.4824) | Acc_1: (82.05%) (16908/20608)\n",
      "Epoch: 163 | Batch_idx: 170 |  Loss_1: (0.4810) | Acc_1: (82.07%) (17963/21888)\n",
      "Epoch: 163 | Batch_idx: 180 |  Loss_1: (0.4794) | Acc_1: (82.10%) (19022/23168)\n",
      "Epoch: 163 | Batch_idx: 190 |  Loss_1: (0.4779) | Acc_1: (82.16%) (20087/24448)\n",
      "Epoch: 163 | Batch_idx: 200 |  Loss_1: (0.4788) | Acc_1: (82.13%) (21130/25728)\n",
      "Epoch: 163 | Batch_idx: 210 |  Loss_1: (0.4750) | Acc_1: (82.26%) (22218/27008)\n",
      "Epoch: 163 | Batch_idx: 220 |  Loss_1: (0.4741) | Acc_1: (82.30%) (23281/28288)\n",
      "Epoch: 163 | Batch_idx: 230 |  Loss_1: (0.4738) | Acc_1: (82.31%) (24338/29568)\n",
      "Epoch: 163 | Batch_idx: 240 |  Loss_1: (0.4737) | Acc_1: (82.33%) (25397/30848)\n",
      "Epoch: 163 | Batch_idx: 250 |  Loss_1: (0.4745) | Acc_1: (82.29%) (26439/32128)\n",
      "Epoch: 163 | Batch_idx: 260 |  Loss_1: (0.4743) | Acc_1: (82.29%) (27491/33408)\n",
      "Epoch: 163 | Batch_idx: 270 |  Loss_1: (0.4765) | Acc_1: (82.22%) (28521/34688)\n",
      "Epoch: 163 | Batch_idx: 280 |  Loss_1: (0.4770) | Acc_1: (82.21%) (29569/35968)\n",
      "Epoch: 163 | Batch_idx: 290 |  Loss_1: (0.4780) | Acc_1: (82.15%) (30598/37248)\n",
      "Epoch: 163 | Batch_idx: 300 |  Loss_1: (0.4780) | Acc_1: (82.14%) (31645/38528)\n",
      "Epoch: 163 | Batch_idx: 310 |  Loss_1: (0.4790) | Acc_1: (82.11%) (32685/39808)\n",
      "Epoch: 163 | Batch_idx: 320 |  Loss_1: (0.4783) | Acc_1: (82.14%) (33748/41088)\n",
      "Epoch: 163 | Batch_idx: 330 |  Loss_1: (0.4790) | Acc_1: (82.11%) (34790/42368)\n",
      "Epoch: 163 | Batch_idx: 340 |  Loss_1: (0.4807) | Acc_1: (82.03%) (35805/43648)\n",
      "Epoch: 163 | Batch_idx: 350 |  Loss_1: (0.4829) | Acc_1: (81.96%) (36822/44928)\n",
      "Epoch: 163 | Batch_idx: 360 |  Loss_1: (0.4834) | Acc_1: (81.94%) (37865/46208)\n",
      "Epoch: 163 | Batch_idx: 370 |  Loss_1: (0.4829) | Acc_1: (81.97%) (38924/47488)\n",
      "Epoch: 163 | Batch_idx: 380 |  Loss_1: (0.4828) | Acc_1: (81.97%) (39977/48768)\n",
      "Epoch: 163 | Batch_idx: 390 |  Loss_1: (0.4824) | Acc_1: (81.99%) (40994/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4640) | Acc: (90.36%) (9036/10000)\n",
      "Epoch: 164 | Batch_idx: 0 |  Loss_1: (0.5618) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 164 | Batch_idx: 10 |  Loss_1: (0.4845) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 164 | Batch_idx: 20 |  Loss_1: (0.4806) | Acc_1: (82.22%) (2210/2688)\n",
      "Epoch: 164 | Batch_idx: 30 |  Loss_1: (0.4856) | Acc_1: (82.03%) (3255/3968)\n",
      "Epoch: 164 | Batch_idx: 40 |  Loss_1: (0.4917) | Acc_1: (81.82%) (4294/5248)\n",
      "Epoch: 164 | Batch_idx: 50 |  Loss_1: (0.4986) | Acc_1: (81.46%) (5318/6528)\n",
      "Epoch: 164 | Batch_idx: 60 |  Loss_1: (0.4905) | Acc_1: (81.63%) (6374/7808)\n",
      "Epoch: 164 | Batch_idx: 70 |  Loss_1: (0.4877) | Acc_1: (81.71%) (7426/9088)\n",
      "Epoch: 164 | Batch_idx: 80 |  Loss_1: (0.4849) | Acc_1: (81.89%) (8490/10368)\n",
      "Epoch: 164 | Batch_idx: 90 |  Loss_1: (0.4850) | Acc_1: (81.87%) (9536/11648)\n",
      "Epoch: 164 | Batch_idx: 100 |  Loss_1: (0.4791) | Acc_1: (82.08%) (10611/12928)\n",
      "Epoch: 164 | Batch_idx: 110 |  Loss_1: (0.4777) | Acc_1: (82.11%) (11666/14208)\n",
      "Epoch: 164 | Batch_idx: 120 |  Loss_1: (0.4779) | Acc_1: (82.10%) (12716/15488)\n",
      "Epoch: 164 | Batch_idx: 130 |  Loss_1: (0.4783) | Acc_1: (82.08%) (13764/16768)\n",
      "Epoch: 164 | Batch_idx: 140 |  Loss_1: (0.4798) | Acc_1: (82.08%) (14813/18048)\n",
      "Epoch: 164 | Batch_idx: 150 |  Loss_1: (0.4790) | Acc_1: (82.13%) (15874/19328)\n",
      "Epoch: 164 | Batch_idx: 160 |  Loss_1: (0.4802) | Acc_1: (82.08%) (16916/20608)\n",
      "Epoch: 164 | Batch_idx: 170 |  Loss_1: (0.4785) | Acc_1: (82.12%) (17975/21888)\n",
      "Epoch: 164 | Batch_idx: 180 |  Loss_1: (0.4786) | Acc_1: (82.12%) (19026/23168)\n",
      "Epoch: 164 | Batch_idx: 190 |  Loss_1: (0.4786) | Acc_1: (82.15%) (20083/24448)\n",
      "Epoch: 164 | Batch_idx: 200 |  Loss_1: (0.4773) | Acc_1: (82.20%) (21148/25728)\n",
      "Epoch: 164 | Batch_idx: 210 |  Loss_1: (0.4764) | Acc_1: (82.23%) (22209/27008)\n",
      "Epoch: 164 | Batch_idx: 220 |  Loss_1: (0.4759) | Acc_1: (82.25%) (23266/28288)\n",
      "Epoch: 164 | Batch_idx: 230 |  Loss_1: (0.4756) | Acc_1: (82.24%) (24317/29568)\n",
      "Epoch: 164 | Batch_idx: 240 |  Loss_1: (0.4769) | Acc_1: (82.19%) (25354/30848)\n",
      "Epoch: 164 | Batch_idx: 250 |  Loss_1: (0.4771) | Acc_1: (82.18%) (26403/32128)\n",
      "Epoch: 164 | Batch_idx: 260 |  Loss_1: (0.4770) | Acc_1: (82.20%) (27463/33408)\n",
      "Epoch: 164 | Batch_idx: 270 |  Loss_1: (0.4770) | Acc_1: (82.20%) (28515/34688)\n",
      "Epoch: 164 | Batch_idx: 280 |  Loss_1: (0.4763) | Acc_1: (82.25%) (29582/35968)\n",
      "Epoch: 164 | Batch_idx: 290 |  Loss_1: (0.4753) | Acc_1: (82.30%) (30656/37248)\n",
      "Epoch: 164 | Batch_idx: 300 |  Loss_1: (0.4741) | Acc_1: (82.37%) (31734/38528)\n",
      "Epoch: 164 | Batch_idx: 310 |  Loss_1: (0.4730) | Acc_1: (82.41%) (32804/39808)\n",
      "Epoch: 164 | Batch_idx: 320 |  Loss_1: (0.4721) | Acc_1: (82.43%) (33870/41088)\n",
      "Epoch: 164 | Batch_idx: 330 |  Loss_1: (0.4715) | Acc_1: (82.45%) (34933/42368)\n",
      "Epoch: 164 | Batch_idx: 340 |  Loss_1: (0.4715) | Acc_1: (82.45%) (35989/43648)\n",
      "Epoch: 164 | Batch_idx: 350 |  Loss_1: (0.4717) | Acc_1: (82.44%) (37040/44928)\n",
      "Epoch: 164 | Batch_idx: 360 |  Loss_1: (0.4719) | Acc_1: (82.43%) (38089/46208)\n",
      "Epoch: 164 | Batch_idx: 370 |  Loss_1: (0.4723) | Acc_1: (82.42%) (39140/47488)\n",
      "Epoch: 164 | Batch_idx: 380 |  Loss_1: (0.4727) | Acc_1: (82.41%) (40192/48768)\n",
      "Epoch: 164 | Batch_idx: 390 |  Loss_1: (0.4732) | Acc_1: (82.39%) (41195/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4887) | Acc: (90.81%) (9081/10000)\n",
      "Epoch: 165 | Batch_idx: 0 |  Loss_1: (0.5808) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 165 | Batch_idx: 10 |  Loss_1: (0.5015) | Acc_1: (81.46%) (1147/1408)\n",
      "Epoch: 165 | Batch_idx: 20 |  Loss_1: (0.5275) | Acc_1: (80.39%) (2161/2688)\n",
      "Epoch: 165 | Batch_idx: 30 |  Loss_1: (0.5088) | Acc_1: (80.92%) (3211/3968)\n",
      "Epoch: 165 | Batch_idx: 40 |  Loss_1: (0.5111) | Acc_1: (80.85%) (4243/5248)\n",
      "Epoch: 165 | Batch_idx: 50 |  Loss_1: (0.5063) | Acc_1: (81.07%) (5292/6528)\n",
      "Epoch: 165 | Batch_idx: 60 |  Loss_1: (0.4989) | Acc_1: (81.35%) (6352/7808)\n",
      "Epoch: 165 | Batch_idx: 70 |  Loss_1: (0.4928) | Acc_1: (81.61%) (7417/9088)\n",
      "Epoch: 165 | Batch_idx: 80 |  Loss_1: (0.4903) | Acc_1: (81.69%) (8470/10368)\n",
      "Epoch: 165 | Batch_idx: 90 |  Loss_1: (0.4912) | Acc_1: (81.68%) (9514/11648)\n",
      "Epoch: 165 | Batch_idx: 100 |  Loss_1: (0.4880) | Acc_1: (81.75%) (10568/12928)\n",
      "Epoch: 165 | Batch_idx: 110 |  Loss_1: (0.4874) | Acc_1: (81.78%) (11620/14208)\n",
      "Epoch: 165 | Batch_idx: 120 |  Loss_1: (0.4830) | Acc_1: (81.96%) (12694/15488)\n",
      "Epoch: 165 | Batch_idx: 130 |  Loss_1: (0.4804) | Acc_1: (82.01%) (13752/16768)\n",
      "Epoch: 165 | Batch_idx: 140 |  Loss_1: (0.4799) | Acc_1: (82.02%) (14803/18048)\n",
      "Epoch: 165 | Batch_idx: 150 |  Loss_1: (0.4813) | Acc_1: (81.98%) (15846/19328)\n",
      "Epoch: 165 | Batch_idx: 160 |  Loss_1: (0.4811) | Acc_1: (81.99%) (16897/20608)\n",
      "Epoch: 165 | Batch_idx: 170 |  Loss_1: (0.4822) | Acc_1: (81.95%) (17938/21888)\n",
      "Epoch: 165 | Batch_idx: 180 |  Loss_1: (0.4829) | Acc_1: (81.91%) (18977/23168)\n",
      "Epoch: 165 | Batch_idx: 190 |  Loss_1: (0.4834) | Acc_1: (81.90%) (20024/24448)\n",
      "Epoch: 165 | Batch_idx: 200 |  Loss_1: (0.4813) | Acc_1: (82.01%) (21100/25728)\n",
      "Epoch: 165 | Batch_idx: 210 |  Loss_1: (0.4800) | Acc_1: (82.06%) (22163/27008)\n",
      "Epoch: 165 | Batch_idx: 220 |  Loss_1: (0.4800) | Acc_1: (82.07%) (23215/28288)\n",
      "Epoch: 165 | Batch_idx: 230 |  Loss_1: (0.4810) | Acc_1: (82.04%) (24257/29568)\n",
      "Epoch: 165 | Batch_idx: 240 |  Loss_1: (0.4787) | Acc_1: (82.12%) (25333/30848)\n",
      "Epoch: 165 | Batch_idx: 250 |  Loss_1: (0.4772) | Acc_1: (82.17%) (26399/32128)\n",
      "Epoch: 165 | Batch_idx: 260 |  Loss_1: (0.4781) | Acc_1: (82.12%) (27436/33408)\n",
      "Epoch: 165 | Batch_idx: 270 |  Loss_1: (0.4782) | Acc_1: (82.13%) (28490/34688)\n",
      "Epoch: 165 | Batch_idx: 280 |  Loss_1: (0.4777) | Acc_1: (82.14%) (29544/35968)\n",
      "Epoch: 165 | Batch_idx: 290 |  Loss_1: (0.4791) | Acc_1: (82.10%) (30580/37248)\n",
      "Epoch: 165 | Batch_idx: 300 |  Loss_1: (0.4794) | Acc_1: (82.10%) (31631/38528)\n",
      "Epoch: 165 | Batch_idx: 310 |  Loss_1: (0.4792) | Acc_1: (82.11%) (32685/39808)\n",
      "Epoch: 165 | Batch_idx: 320 |  Loss_1: (0.4788) | Acc_1: (82.12%) (33743/41088)\n",
      "Epoch: 165 | Batch_idx: 330 |  Loss_1: (0.4803) | Acc_1: (82.07%) (34772/42368)\n",
      "Epoch: 165 | Batch_idx: 340 |  Loss_1: (0.4783) | Acc_1: (82.16%) (35861/43648)\n",
      "Epoch: 165 | Batch_idx: 350 |  Loss_1: (0.4777) | Acc_1: (82.19%) (36926/44928)\n",
      "Epoch: 165 | Batch_idx: 360 |  Loss_1: (0.4766) | Acc_1: (82.22%) (37994/46208)\n",
      "Epoch: 165 | Batch_idx: 370 |  Loss_1: (0.4769) | Acc_1: (82.22%) (39046/47488)\n",
      "Epoch: 165 | Batch_idx: 380 |  Loss_1: (0.4759) | Acc_1: (82.25%) (40112/48768)\n",
      "Epoch: 165 | Batch_idx: 390 |  Loss_1: (0.4748) | Acc_1: (82.29%) (41144/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4957) | Acc: (90.83%) (9083/10000)\n",
      "Epoch: 166 | Batch_idx: 0 |  Loss_1: (0.4105) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 166 | Batch_idx: 10 |  Loss_1: (0.4549) | Acc_1: (82.81%) (1166/1408)\n",
      "Epoch: 166 | Batch_idx: 20 |  Loss_1: (0.4675) | Acc_1: (82.59%) (2220/2688)\n",
      "Epoch: 166 | Batch_idx: 30 |  Loss_1: (0.4619) | Acc_1: (82.81%) (3286/3968)\n",
      "Epoch: 166 | Batch_idx: 40 |  Loss_1: (0.4609) | Acc_1: (82.93%) (4352/5248)\n",
      "Epoch: 166 | Batch_idx: 50 |  Loss_1: (0.4716) | Acc_1: (82.41%) (5380/6528)\n",
      "Epoch: 166 | Batch_idx: 60 |  Loss_1: (0.4775) | Acc_1: (82.16%) (6415/7808)\n",
      "Epoch: 166 | Batch_idx: 70 |  Loss_1: (0.4746) | Acc_1: (82.28%) (7478/9088)\n",
      "Epoch: 166 | Batch_idx: 80 |  Loss_1: (0.4762) | Acc_1: (82.17%) (8519/10368)\n",
      "Epoch: 166 | Batch_idx: 90 |  Loss_1: (0.4776) | Acc_1: (82.11%) (9564/11648)\n",
      "Epoch: 166 | Batch_idx: 100 |  Loss_1: (0.4854) | Acc_1: (81.81%) (10577/12928)\n",
      "Epoch: 166 | Batch_idx: 110 |  Loss_1: (0.4871) | Acc_1: (81.74%) (11613/14208)\n",
      "Epoch: 166 | Batch_idx: 120 |  Loss_1: (0.4898) | Acc_1: (81.63%) (12643/15488)\n",
      "Epoch: 166 | Batch_idx: 130 |  Loss_1: (0.4878) | Acc_1: (81.74%) (13707/16768)\n",
      "Epoch: 166 | Batch_idx: 140 |  Loss_1: (0.4895) | Acc_1: (81.68%) (14741/18048)\n",
      "Epoch: 166 | Batch_idx: 150 |  Loss_1: (0.4883) | Acc_1: (81.77%) (15805/19328)\n",
      "Epoch: 166 | Batch_idx: 160 |  Loss_1: (0.4890) | Acc_1: (81.77%) (16851/20608)\n",
      "Epoch: 166 | Batch_idx: 170 |  Loss_1: (0.4957) | Acc_1: (81.52%) (17844/21888)\n",
      "Epoch: 166 | Batch_idx: 180 |  Loss_1: (0.4934) | Acc_1: (81.61%) (18908/23168)\n",
      "Epoch: 166 | Batch_idx: 190 |  Loss_1: (0.4907) | Acc_1: (81.69%) (19971/24448)\n",
      "Epoch: 166 | Batch_idx: 200 |  Loss_1: (0.4896) | Acc_1: (81.73%) (21028/25728)\n",
      "Epoch: 166 | Batch_idx: 210 |  Loss_1: (0.4878) | Acc_1: (81.79%) (22090/27008)\n",
      "Epoch: 166 | Batch_idx: 220 |  Loss_1: (0.4887) | Acc_1: (81.75%) (23125/28288)\n",
      "Epoch: 166 | Batch_idx: 230 |  Loss_1: (0.4886) | Acc_1: (81.74%) (24169/29568)\n",
      "Epoch: 166 | Batch_idx: 240 |  Loss_1: (0.4879) | Acc_1: (81.77%) (25223/30848)\n",
      "Epoch: 166 | Batch_idx: 250 |  Loss_1: (0.4880) | Acc_1: (81.78%) (26273/32128)\n",
      "Epoch: 166 | Batch_idx: 260 |  Loss_1: (0.4877) | Acc_1: (81.76%) (27313/33408)\n",
      "Epoch: 166 | Batch_idx: 270 |  Loss_1: (0.4870) | Acc_1: (81.79%) (28371/34688)\n",
      "Epoch: 166 | Batch_idx: 280 |  Loss_1: (0.4871) | Acc_1: (81.79%) (29419/35968)\n",
      "Epoch: 166 | Batch_idx: 290 |  Loss_1: (0.4881) | Acc_1: (81.74%) (30447/37248)\n",
      "Epoch: 166 | Batch_idx: 300 |  Loss_1: (0.4869) | Acc_1: (81.79%) (31511/38528)\n",
      "Epoch: 166 | Batch_idx: 310 |  Loss_1: (0.4875) | Acc_1: (81.75%) (32545/39808)\n",
      "Epoch: 166 | Batch_idx: 320 |  Loss_1: (0.4876) | Acc_1: (81.74%) (33585/41088)\n",
      "Epoch: 166 | Batch_idx: 330 |  Loss_1: (0.4875) | Acc_1: (81.74%) (34631/42368)\n",
      "Epoch: 166 | Batch_idx: 340 |  Loss_1: (0.4872) | Acc_1: (81.75%) (35681/43648)\n",
      "Epoch: 166 | Batch_idx: 350 |  Loss_1: (0.4861) | Acc_1: (81.79%) (36748/44928)\n",
      "Epoch: 166 | Batch_idx: 360 |  Loss_1: (0.4854) | Acc_1: (81.81%) (37803/46208)\n",
      "Epoch: 166 | Batch_idx: 370 |  Loss_1: (0.4861) | Acc_1: (81.78%) (38835/47488)\n",
      "Epoch: 166 | Batch_idx: 380 |  Loss_1: (0.4863) | Acc_1: (81.78%) (39883/48768)\n",
      "Epoch: 166 | Batch_idx: 390 |  Loss_1: (0.4856) | Acc_1: (81.80%) (40898/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4804) | Acc: (90.49%) (9049/10000)\n",
      "Epoch: 167 | Batch_idx: 0 |  Loss_1: (0.4940) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 167 | Batch_idx: 10 |  Loss_1: (0.4799) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 167 | Batch_idx: 20 |  Loss_1: (0.4876) | Acc_1: (81.77%) (2198/2688)\n",
      "Epoch: 167 | Batch_idx: 30 |  Loss_1: (0.4854) | Acc_1: (81.80%) (3246/3968)\n",
      "Epoch: 167 | Batch_idx: 40 |  Loss_1: (0.4891) | Acc_1: (81.65%) (4285/5248)\n",
      "Epoch: 167 | Batch_idx: 50 |  Loss_1: (0.4928) | Acc_1: (81.53%) (5322/6528)\n",
      "Epoch: 167 | Batch_idx: 60 |  Loss_1: (0.4935) | Acc_1: (81.43%) (6358/7808)\n",
      "Epoch: 167 | Batch_idx: 70 |  Loss_1: (0.4955) | Acc_1: (81.44%) (7401/9088)\n",
      "Epoch: 167 | Batch_idx: 80 |  Loss_1: (0.4859) | Acc_1: (81.73%) (8474/10368)\n",
      "Epoch: 167 | Batch_idx: 90 |  Loss_1: (0.4893) | Acc_1: (81.60%) (9505/11648)\n",
      "Epoch: 167 | Batch_idx: 100 |  Loss_1: (0.4909) | Acc_1: (81.59%) (10548/12928)\n",
      "Epoch: 167 | Batch_idx: 110 |  Loss_1: (0.4931) | Acc_1: (81.54%) (11585/14208)\n",
      "Epoch: 167 | Batch_idx: 120 |  Loss_1: (0.4899) | Acc_1: (81.62%) (12642/15488)\n",
      "Epoch: 167 | Batch_idx: 130 |  Loss_1: (0.4895) | Acc_1: (81.65%) (13691/16768)\n",
      "Epoch: 167 | Batch_idx: 140 |  Loss_1: (0.4877) | Acc_1: (81.72%) (14748/18048)\n",
      "Epoch: 167 | Batch_idx: 150 |  Loss_1: (0.4913) | Acc_1: (81.60%) (15771/19328)\n",
      "Epoch: 167 | Batch_idx: 160 |  Loss_1: (0.4899) | Acc_1: (81.66%) (16828/20608)\n",
      "Epoch: 167 | Batch_idx: 170 |  Loss_1: (0.4905) | Acc_1: (81.64%) (17869/21888)\n",
      "Epoch: 167 | Batch_idx: 180 |  Loss_1: (0.4903) | Acc_1: (81.65%) (18917/23168)\n",
      "Epoch: 167 | Batch_idx: 190 |  Loss_1: (0.4930) | Acc_1: (81.55%) (19938/24448)\n",
      "Epoch: 167 | Batch_idx: 200 |  Loss_1: (0.4923) | Acc_1: (81.58%) (20990/25728)\n",
      "Epoch: 167 | Batch_idx: 210 |  Loss_1: (0.4923) | Acc_1: (81.59%) (22037/27008)\n",
      "Epoch: 167 | Batch_idx: 220 |  Loss_1: (0.4921) | Acc_1: (81.62%) (23089/28288)\n",
      "Epoch: 167 | Batch_idx: 230 |  Loss_1: (0.4890) | Acc_1: (81.72%) (24163/29568)\n",
      "Epoch: 167 | Batch_idx: 240 |  Loss_1: (0.4901) | Acc_1: (81.66%) (25192/30848)\n",
      "Epoch: 167 | Batch_idx: 250 |  Loss_1: (0.4917) | Acc_1: (81.61%) (26221/32128)\n",
      "Epoch: 167 | Batch_idx: 260 |  Loss_1: (0.4928) | Acc_1: (81.58%) (27253/33408)\n",
      "Epoch: 167 | Batch_idx: 270 |  Loss_1: (0.4928) | Acc_1: (81.59%) (28302/34688)\n",
      "Epoch: 167 | Batch_idx: 280 |  Loss_1: (0.4917) | Acc_1: (81.63%) (29362/35968)\n",
      "Epoch: 167 | Batch_idx: 290 |  Loss_1: (0.4905) | Acc_1: (81.69%) (30429/37248)\n",
      "Epoch: 167 | Batch_idx: 300 |  Loss_1: (0.4888) | Acc_1: (81.75%) (31498/38528)\n",
      "Epoch: 167 | Batch_idx: 310 |  Loss_1: (0.4876) | Acc_1: (81.81%) (32566/39808)\n",
      "Epoch: 167 | Batch_idx: 320 |  Loss_1: (0.4878) | Acc_1: (81.79%) (33607/41088)\n",
      "Epoch: 167 | Batch_idx: 330 |  Loss_1: (0.4879) | Acc_1: (81.79%) (34653/42368)\n",
      "Epoch: 167 | Batch_idx: 340 |  Loss_1: (0.4878) | Acc_1: (81.80%) (35703/43648)\n",
      "Epoch: 167 | Batch_idx: 350 |  Loss_1: (0.4890) | Acc_1: (81.75%) (36729/44928)\n",
      "Epoch: 167 | Batch_idx: 360 |  Loss_1: (0.4899) | Acc_1: (81.71%) (37757/46208)\n",
      "Epoch: 167 | Batch_idx: 370 |  Loss_1: (0.4891) | Acc_1: (81.73%) (38813/47488)\n",
      "Epoch: 167 | Batch_idx: 380 |  Loss_1: (0.4896) | Acc_1: (81.71%) (39847/48768)\n",
      "Epoch: 167 | Batch_idx: 390 |  Loss_1: (0.4894) | Acc_1: (81.73%) (40864/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4780) | Acc: (90.57%) (9057/10000)\n",
      "Epoch: 168 | Batch_idx: 0 |  Loss_1: (0.7104) | Acc_1: (72.66%) (93/128)\n",
      "Epoch: 168 | Batch_idx: 10 |  Loss_1: (0.4880) | Acc_1: (81.25%) (1144/1408)\n",
      "Epoch: 168 | Batch_idx: 20 |  Loss_1: (0.5018) | Acc_1: (81.29%) (2185/2688)\n",
      "Epoch: 168 | Batch_idx: 30 |  Loss_1: (0.4985) | Acc_1: (81.45%) (3232/3968)\n",
      "Epoch: 168 | Batch_idx: 40 |  Loss_1: (0.4969) | Acc_1: (81.36%) (4270/5248)\n",
      "Epoch: 168 | Batch_idx: 50 |  Loss_1: (0.4917) | Acc_1: (81.59%) (5326/6528)\n",
      "Epoch: 168 | Batch_idx: 60 |  Loss_1: (0.4965) | Acc_1: (81.43%) (6358/7808)\n",
      "Epoch: 168 | Batch_idx: 70 |  Loss_1: (0.4971) | Acc_1: (81.32%) (7390/9088)\n",
      "Epoch: 168 | Batch_idx: 80 |  Loss_1: (0.4957) | Acc_1: (81.39%) (8438/10368)\n",
      "Epoch: 168 | Batch_idx: 90 |  Loss_1: (0.4952) | Acc_1: (81.41%) (9483/11648)\n",
      "Epoch: 168 | Batch_idx: 100 |  Loss_1: (0.4975) | Acc_1: (81.31%) (10512/12928)\n",
      "Epoch: 168 | Batch_idx: 110 |  Loss_1: (0.4949) | Acc_1: (81.44%) (11571/14208)\n",
      "Epoch: 168 | Batch_idx: 120 |  Loss_1: (0.4948) | Acc_1: (81.46%) (12617/15488)\n",
      "Epoch: 168 | Batch_idx: 130 |  Loss_1: (0.4925) | Acc_1: (81.58%) (13679/16768)\n",
      "Epoch: 168 | Batch_idx: 140 |  Loss_1: (0.4916) | Acc_1: (81.64%) (14734/18048)\n",
      "Epoch: 168 | Batch_idx: 150 |  Loss_1: (0.4878) | Acc_1: (81.80%) (15811/19328)\n",
      "Epoch: 168 | Batch_idx: 160 |  Loss_1: (0.4883) | Acc_1: (81.78%) (16854/20608)\n",
      "Epoch: 168 | Batch_idx: 170 |  Loss_1: (0.4890) | Acc_1: (81.75%) (17893/21888)\n",
      "Epoch: 168 | Batch_idx: 180 |  Loss_1: (0.4879) | Acc_1: (81.80%) (18951/23168)\n",
      "Epoch: 168 | Batch_idx: 190 |  Loss_1: (0.4862) | Acc_1: (81.86%) (20014/24448)\n",
      "Epoch: 168 | Batch_idx: 200 |  Loss_1: (0.4838) | Acc_1: (81.95%) (21085/25728)\n",
      "Epoch: 168 | Batch_idx: 210 |  Loss_1: (0.4845) | Acc_1: (81.92%) (22124/27008)\n",
      "Epoch: 168 | Batch_idx: 220 |  Loss_1: (0.4840) | Acc_1: (81.94%) (23180/28288)\n",
      "Epoch: 168 | Batch_idx: 230 |  Loss_1: (0.4841) | Acc_1: (81.94%) (24227/29568)\n",
      "Epoch: 168 | Batch_idx: 240 |  Loss_1: (0.4844) | Acc_1: (81.91%) (25267/30848)\n",
      "Epoch: 168 | Batch_idx: 250 |  Loss_1: (0.4830) | Acc_1: (81.95%) (26330/32128)\n",
      "Epoch: 168 | Batch_idx: 260 |  Loss_1: (0.4847) | Acc_1: (81.91%) (27363/33408)\n",
      "Epoch: 168 | Batch_idx: 270 |  Loss_1: (0.4841) | Acc_1: (81.93%) (28421/34688)\n",
      "Epoch: 168 | Batch_idx: 280 |  Loss_1: (0.4834) | Acc_1: (81.97%) (29482/35968)\n",
      "Epoch: 168 | Batch_idx: 290 |  Loss_1: (0.4832) | Acc_1: (81.99%) (30538/37248)\n",
      "Epoch: 168 | Batch_idx: 300 |  Loss_1: (0.4823) | Acc_1: (82.02%) (31600/38528)\n",
      "Epoch: 168 | Batch_idx: 310 |  Loss_1: (0.4828) | Acc_1: (82.00%) (32644/39808)\n",
      "Epoch: 168 | Batch_idx: 320 |  Loss_1: (0.4824) | Acc_1: (82.02%) (33700/41088)\n",
      "Epoch: 168 | Batch_idx: 330 |  Loss_1: (0.4824) | Acc_1: (82.00%) (34742/42368)\n",
      "Epoch: 168 | Batch_idx: 340 |  Loss_1: (0.4829) | Acc_1: (81.98%) (35782/43648)\n",
      "Epoch: 168 | Batch_idx: 350 |  Loss_1: (0.4830) | Acc_1: (81.98%) (36833/44928)\n",
      "Epoch: 168 | Batch_idx: 360 |  Loss_1: (0.4834) | Acc_1: (81.98%) (37881/46208)\n",
      "Epoch: 168 | Batch_idx: 370 |  Loss_1: (0.4828) | Acc_1: (82.00%) (38942/47488)\n",
      "Epoch: 168 | Batch_idx: 380 |  Loss_1: (0.4829) | Acc_1: (82.01%) (39996/48768)\n",
      "Epoch: 168 | Batch_idx: 390 |  Loss_1: (0.4817) | Acc_1: (82.05%) (41025/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4647) | Acc: (90.75%) (9075/10000)\n",
      "Epoch: 169 | Batch_idx: 0 |  Loss_1: (0.4634) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 169 | Batch_idx: 10 |  Loss_1: (0.4701) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 169 | Batch_idx: 20 |  Loss_1: (0.4913) | Acc_1: (81.73%) (2197/2688)\n",
      "Epoch: 169 | Batch_idx: 30 |  Loss_1: (0.4854) | Acc_1: (81.98%) (3253/3968)\n",
      "Epoch: 169 | Batch_idx: 40 |  Loss_1: (0.4900) | Acc_1: (81.92%) (4299/5248)\n",
      "Epoch: 169 | Batch_idx: 50 |  Loss_1: (0.4916) | Acc_1: (81.83%) (5342/6528)\n",
      "Epoch: 169 | Batch_idx: 60 |  Loss_1: (0.4904) | Acc_1: (81.79%) (6386/7808)\n",
      "Epoch: 169 | Batch_idx: 70 |  Loss_1: (0.4913) | Acc_1: (81.71%) (7426/9088)\n",
      "Epoch: 169 | Batch_idx: 80 |  Loss_1: (0.4959) | Acc_1: (81.48%) (8448/10368)\n",
      "Epoch: 169 | Batch_idx: 90 |  Loss_1: (0.5025) | Acc_1: (81.22%) (9460/11648)\n",
      "Epoch: 169 | Batch_idx: 100 |  Loss_1: (0.5025) | Acc_1: (81.18%) (10495/12928)\n",
      "Epoch: 169 | Batch_idx: 110 |  Loss_1: (0.4994) | Acc_1: (81.32%) (11554/14208)\n",
      "Epoch: 169 | Batch_idx: 120 |  Loss_1: (0.4980) | Acc_1: (81.35%) (12599/15488)\n",
      "Epoch: 169 | Batch_idx: 130 |  Loss_1: (0.4932) | Acc_1: (81.54%) (13672/16768)\n",
      "Epoch: 169 | Batch_idx: 140 |  Loss_1: (0.4944) | Acc_1: (81.49%) (14707/18048)\n",
      "Epoch: 169 | Batch_idx: 150 |  Loss_1: (0.4937) | Acc_1: (81.49%) (15751/19328)\n",
      "Epoch: 169 | Batch_idx: 160 |  Loss_1: (0.4928) | Acc_1: (81.54%) (16803/20608)\n",
      "Epoch: 169 | Batch_idx: 170 |  Loss_1: (0.4926) | Acc_1: (81.52%) (17844/21888)\n",
      "Epoch: 169 | Batch_idx: 180 |  Loss_1: (0.4908) | Acc_1: (81.59%) (18902/23168)\n",
      "Epoch: 169 | Batch_idx: 190 |  Loss_1: (0.4876) | Acc_1: (81.70%) (19974/24448)\n",
      "Epoch: 169 | Batch_idx: 200 |  Loss_1: (0.4879) | Acc_1: (81.69%) (21016/25728)\n",
      "Epoch: 169 | Batch_idx: 210 |  Loss_1: (0.4865) | Acc_1: (81.74%) (22076/27008)\n",
      "Epoch: 169 | Batch_idx: 220 |  Loss_1: (0.4861) | Acc_1: (81.78%) (23133/28288)\n",
      "Epoch: 169 | Batch_idx: 230 |  Loss_1: (0.4854) | Acc_1: (81.81%) (24191/29568)\n",
      "Epoch: 169 | Batch_idx: 240 |  Loss_1: (0.4856) | Acc_1: (81.81%) (25236/30848)\n",
      "Epoch: 169 | Batch_idx: 250 |  Loss_1: (0.4832) | Acc_1: (81.88%) (26305/32128)\n",
      "Epoch: 169 | Batch_idx: 260 |  Loss_1: (0.4829) | Acc_1: (81.90%) (27361/33408)\n",
      "Epoch: 169 | Batch_idx: 270 |  Loss_1: (0.4844) | Acc_1: (81.86%) (28397/34688)\n",
      "Epoch: 169 | Batch_idx: 280 |  Loss_1: (0.4857) | Acc_1: (81.83%) (29432/35968)\n",
      "Epoch: 169 | Batch_idx: 290 |  Loss_1: (0.4871) | Acc_1: (81.76%) (30454/37248)\n",
      "Epoch: 169 | Batch_idx: 300 |  Loss_1: (0.4867) | Acc_1: (81.76%) (31500/38528)\n",
      "Epoch: 169 | Batch_idx: 310 |  Loss_1: (0.4854) | Acc_1: (81.81%) (32567/39808)\n",
      "Epoch: 169 | Batch_idx: 320 |  Loss_1: (0.4847) | Acc_1: (81.83%) (33622/41088)\n",
      "Epoch: 169 | Batch_idx: 330 |  Loss_1: (0.4840) | Acc_1: (81.87%) (34685/42368)\n",
      "Epoch: 169 | Batch_idx: 340 |  Loss_1: (0.4838) | Acc_1: (81.86%) (35732/43648)\n",
      "Epoch: 169 | Batch_idx: 350 |  Loss_1: (0.4846) | Acc_1: (81.82%) (36759/44928)\n",
      "Epoch: 169 | Batch_idx: 360 |  Loss_1: (0.4847) | Acc_1: (81.81%) (37805/46208)\n",
      "Epoch: 169 | Batch_idx: 370 |  Loss_1: (0.4846) | Acc_1: (81.82%) (38855/47488)\n",
      "Epoch: 169 | Batch_idx: 380 |  Loss_1: (0.4847) | Acc_1: (81.84%) (39911/48768)\n",
      "Epoch: 169 | Batch_idx: 390 |  Loss_1: (0.4846) | Acc_1: (81.84%) (40919/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4669) | Acc: (90.55%) (9055/10000)\n",
      "Epoch: 170 | Batch_idx: 0 |  Loss_1: (0.4002) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 170 | Batch_idx: 10 |  Loss_1: (0.4640) | Acc_1: (82.95%) (1168/1408)\n",
      "Epoch: 170 | Batch_idx: 20 |  Loss_1: (0.4537) | Acc_1: (83.22%) (2237/2688)\n",
      "Epoch: 170 | Batch_idx: 30 |  Loss_1: (0.4490) | Acc_1: (83.39%) (3309/3968)\n",
      "Epoch: 170 | Batch_idx: 40 |  Loss_1: (0.4529) | Acc_1: (83.19%) (4366/5248)\n",
      "Epoch: 170 | Batch_idx: 50 |  Loss_1: (0.4641) | Acc_1: (82.77%) (5403/6528)\n",
      "Epoch: 170 | Batch_idx: 60 |  Loss_1: (0.4634) | Acc_1: (82.77%) (6463/7808)\n",
      "Epoch: 170 | Batch_idx: 70 |  Loss_1: (0.4676) | Acc_1: (82.57%) (7504/9088)\n",
      "Epoch: 170 | Batch_idx: 80 |  Loss_1: (0.4690) | Acc_1: (82.52%) (8556/10368)\n",
      "Epoch: 170 | Batch_idx: 90 |  Loss_1: (0.4679) | Acc_1: (82.51%) (9611/11648)\n",
      "Epoch: 170 | Batch_idx: 100 |  Loss_1: (0.4727) | Acc_1: (82.37%) (10649/12928)\n",
      "Epoch: 170 | Batch_idx: 110 |  Loss_1: (0.4715) | Acc_1: (82.39%) (11706/14208)\n",
      "Epoch: 170 | Batch_idx: 120 |  Loss_1: (0.4740) | Acc_1: (82.34%) (12753/15488)\n",
      "Epoch: 170 | Batch_idx: 130 |  Loss_1: (0.4779) | Acc_1: (82.22%) (13786/16768)\n",
      "Epoch: 170 | Batch_idx: 140 |  Loss_1: (0.4778) | Acc_1: (82.21%) (14838/18048)\n",
      "Epoch: 170 | Batch_idx: 150 |  Loss_1: (0.4771) | Acc_1: (82.25%) (15897/19328)\n",
      "Epoch: 170 | Batch_idx: 160 |  Loss_1: (0.4780) | Acc_1: (82.23%) (16946/20608)\n",
      "Epoch: 170 | Batch_idx: 170 |  Loss_1: (0.4788) | Acc_1: (82.19%) (17989/21888)\n",
      "Epoch: 170 | Batch_idx: 180 |  Loss_1: (0.4772) | Acc_1: (82.25%) (19055/23168)\n",
      "Epoch: 170 | Batch_idx: 190 |  Loss_1: (0.4745) | Acc_1: (82.36%) (20135/24448)\n",
      "Epoch: 170 | Batch_idx: 200 |  Loss_1: (0.4728) | Acc_1: (82.42%) (21205/25728)\n",
      "Epoch: 170 | Batch_idx: 210 |  Loss_1: (0.4735) | Acc_1: (82.40%) (22255/27008)\n",
      "Epoch: 170 | Batch_idx: 220 |  Loss_1: (0.4746) | Acc_1: (82.36%) (23297/28288)\n",
      "Epoch: 170 | Batch_idx: 230 |  Loss_1: (0.4747) | Acc_1: (82.34%) (24346/29568)\n",
      "Epoch: 170 | Batch_idx: 240 |  Loss_1: (0.4769) | Acc_1: (82.26%) (25375/30848)\n",
      "Epoch: 170 | Batch_idx: 250 |  Loss_1: (0.4771) | Acc_1: (82.25%) (26425/32128)\n",
      "Epoch: 170 | Batch_idx: 260 |  Loss_1: (0.4767) | Acc_1: (82.26%) (27481/33408)\n",
      "Epoch: 170 | Batch_idx: 270 |  Loss_1: (0.4765) | Acc_1: (82.25%) (28532/34688)\n",
      "Epoch: 170 | Batch_idx: 280 |  Loss_1: (0.4758) | Acc_1: (82.28%) (29596/35968)\n",
      "Epoch: 170 | Batch_idx: 290 |  Loss_1: (0.4761) | Acc_1: (82.27%) (30645/37248)\n",
      "Epoch: 170 | Batch_idx: 300 |  Loss_1: (0.4772) | Acc_1: (82.25%) (31689/38528)\n",
      "Epoch: 170 | Batch_idx: 310 |  Loss_1: (0.4763) | Acc_1: (82.28%) (32756/39808)\n",
      "Epoch: 170 | Batch_idx: 320 |  Loss_1: (0.4768) | Acc_1: (82.25%) (33795/41088)\n",
      "Epoch: 170 | Batch_idx: 330 |  Loss_1: (0.4766) | Acc_1: (82.26%) (34850/42368)\n",
      "Epoch: 170 | Batch_idx: 340 |  Loss_1: (0.4779) | Acc_1: (82.21%) (35881/43648)\n",
      "Epoch: 170 | Batch_idx: 350 |  Loss_1: (0.4790) | Acc_1: (82.16%) (36914/44928)\n",
      "Epoch: 170 | Batch_idx: 360 |  Loss_1: (0.4805) | Acc_1: (82.10%) (37939/46208)\n",
      "Epoch: 170 | Batch_idx: 370 |  Loss_1: (0.4803) | Acc_1: (82.10%) (38989/47488)\n",
      "Epoch: 170 | Batch_idx: 380 |  Loss_1: (0.4801) | Acc_1: (82.12%) (40048/48768)\n",
      "Epoch: 170 | Batch_idx: 390 |  Loss_1: (0.4797) | Acc_1: (82.13%) (41065/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5294) | Acc: (90.50%) (9050/10000)\n",
      "Epoch: 171 | Batch_idx: 0 |  Loss_1: (0.6224) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 171 | Batch_idx: 10 |  Loss_1: (0.5177) | Acc_1: (80.97%) (1140/1408)\n",
      "Epoch: 171 | Batch_idx: 20 |  Loss_1: (0.4930) | Acc_1: (81.62%) (2194/2688)\n",
      "Epoch: 171 | Batch_idx: 30 |  Loss_1: (0.4876) | Acc_1: (81.73%) (3243/3968)\n",
      "Epoch: 171 | Batch_idx: 40 |  Loss_1: (0.4902) | Acc_1: (81.59%) (4282/5248)\n",
      "Epoch: 171 | Batch_idx: 50 |  Loss_1: (0.4902) | Acc_1: (81.59%) (5326/6528)\n",
      "Epoch: 171 | Batch_idx: 60 |  Loss_1: (0.4925) | Acc_1: (81.45%) (6360/7808)\n",
      "Epoch: 171 | Batch_idx: 70 |  Loss_1: (0.4910) | Acc_1: (81.54%) (7410/9088)\n",
      "Epoch: 171 | Batch_idx: 80 |  Loss_1: (0.4919) | Acc_1: (81.57%) (8457/10368)\n",
      "Epoch: 171 | Batch_idx: 90 |  Loss_1: (0.4947) | Acc_1: (81.48%) (9491/11648)\n",
      "Epoch: 171 | Batch_idx: 100 |  Loss_1: (0.4935) | Acc_1: (81.52%) (10539/12928)\n",
      "Epoch: 171 | Batch_idx: 110 |  Loss_1: (0.4922) | Acc_1: (81.62%) (11596/14208)\n",
      "Epoch: 171 | Batch_idx: 120 |  Loss_1: (0.4925) | Acc_1: (81.64%) (12644/15488)\n",
      "Epoch: 171 | Batch_idx: 130 |  Loss_1: (0.4897) | Acc_1: (81.71%) (13701/16768)\n",
      "Epoch: 171 | Batch_idx: 140 |  Loss_1: (0.4914) | Acc_1: (81.66%) (14738/18048)\n",
      "Epoch: 171 | Batch_idx: 150 |  Loss_1: (0.4931) | Acc_1: (81.59%) (15769/19328)\n",
      "Epoch: 171 | Batch_idx: 160 |  Loss_1: (0.4917) | Acc_1: (81.66%) (16828/20608)\n",
      "Epoch: 171 | Batch_idx: 170 |  Loss_1: (0.4903) | Acc_1: (81.70%) (17883/21888)\n",
      "Epoch: 171 | Batch_idx: 180 |  Loss_1: (0.4894) | Acc_1: (81.76%) (18943/23168)\n",
      "Epoch: 171 | Batch_idx: 190 |  Loss_1: (0.4861) | Acc_1: (81.88%) (20017/24448)\n",
      "Epoch: 171 | Batch_idx: 200 |  Loss_1: (0.4856) | Acc_1: (81.89%) (21069/25728)\n",
      "Epoch: 171 | Batch_idx: 210 |  Loss_1: (0.4835) | Acc_1: (81.98%) (22141/27008)\n",
      "Epoch: 171 | Batch_idx: 220 |  Loss_1: (0.4828) | Acc_1: (82.00%) (23196/28288)\n",
      "Epoch: 171 | Batch_idx: 230 |  Loss_1: (0.4819) | Acc_1: (82.05%) (24261/29568)\n",
      "Epoch: 171 | Batch_idx: 240 |  Loss_1: (0.4835) | Acc_1: (82.00%) (25295/30848)\n",
      "Epoch: 171 | Batch_idx: 250 |  Loss_1: (0.4844) | Acc_1: (81.97%) (26334/32128)\n",
      "Epoch: 171 | Batch_idx: 260 |  Loss_1: (0.4836) | Acc_1: (82.00%) (27394/33408)\n",
      "Epoch: 171 | Batch_idx: 270 |  Loss_1: (0.4850) | Acc_1: (81.93%) (28419/34688)\n",
      "Epoch: 171 | Batch_idx: 280 |  Loss_1: (0.4835) | Acc_1: (81.99%) (29490/35968)\n",
      "Epoch: 171 | Batch_idx: 290 |  Loss_1: (0.4835) | Acc_1: (81.97%) (30531/37248)\n",
      "Epoch: 171 | Batch_idx: 300 |  Loss_1: (0.4842) | Acc_1: (81.94%) (31571/38528)\n",
      "Epoch: 171 | Batch_idx: 310 |  Loss_1: (0.4829) | Acc_1: (81.99%) (32639/39808)\n",
      "Epoch: 171 | Batch_idx: 320 |  Loss_1: (0.4831) | Acc_1: (82.01%) (33697/41088)\n",
      "Epoch: 171 | Batch_idx: 330 |  Loss_1: (0.4836) | Acc_1: (81.99%) (34738/42368)\n",
      "Epoch: 171 | Batch_idx: 340 |  Loss_1: (0.4835) | Acc_1: (81.99%) (35786/43648)\n",
      "Epoch: 171 | Batch_idx: 350 |  Loss_1: (0.4834) | Acc_1: (81.99%) (36836/44928)\n",
      "Epoch: 171 | Batch_idx: 360 |  Loss_1: (0.4829) | Acc_1: (82.01%) (37897/46208)\n",
      "Epoch: 171 | Batch_idx: 370 |  Loss_1: (0.4824) | Acc_1: (82.03%) (38954/47488)\n",
      "Epoch: 171 | Batch_idx: 380 |  Loss_1: (0.4833) | Acc_1: (82.01%) (39994/48768)\n",
      "Epoch: 171 | Batch_idx: 390 |  Loss_1: (0.4843) | Acc_1: (81.98%) (40990/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4984) | Acc: (90.74%) (9074/10000)\n",
      "Epoch: 172 | Batch_idx: 0 |  Loss_1: (0.4690) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 172 | Batch_idx: 10 |  Loss_1: (0.5045) | Acc_1: (80.75%) (1137/1408)\n",
      "Epoch: 172 | Batch_idx: 20 |  Loss_1: (0.5150) | Acc_1: (80.62%) (2167/2688)\n",
      "Epoch: 172 | Batch_idx: 30 |  Loss_1: (0.5102) | Acc_1: (80.95%) (3212/3968)\n",
      "Epoch: 172 | Batch_idx: 40 |  Loss_1: (0.5057) | Acc_1: (81.21%) (4262/5248)\n",
      "Epoch: 172 | Batch_idx: 50 |  Loss_1: (0.5009) | Acc_1: (81.37%) (5312/6528)\n",
      "Epoch: 172 | Batch_idx: 60 |  Loss_1: (0.4969) | Acc_1: (81.52%) (6365/7808)\n",
      "Epoch: 172 | Batch_idx: 70 |  Loss_1: (0.4950) | Acc_1: (81.57%) (7413/9088)\n",
      "Epoch: 172 | Batch_idx: 80 |  Loss_1: (0.4951) | Acc_1: (81.55%) (8455/10368)\n",
      "Epoch: 172 | Batch_idx: 90 |  Loss_1: (0.4967) | Acc_1: (81.46%) (9489/11648)\n",
      "Epoch: 172 | Batch_idx: 100 |  Loss_1: (0.4949) | Acc_1: (81.54%) (10542/12928)\n",
      "Epoch: 172 | Batch_idx: 110 |  Loss_1: (0.4944) | Acc_1: (81.58%) (11591/14208)\n",
      "Epoch: 172 | Batch_idx: 120 |  Loss_1: (0.4943) | Acc_1: (81.59%) (12637/15488)\n",
      "Epoch: 172 | Batch_idx: 130 |  Loss_1: (0.4947) | Acc_1: (81.58%) (13679/16768)\n",
      "Epoch: 172 | Batch_idx: 140 |  Loss_1: (0.4932) | Acc_1: (81.64%) (14735/18048)\n",
      "Epoch: 172 | Batch_idx: 150 |  Loss_1: (0.4882) | Acc_1: (81.82%) (15815/19328)\n",
      "Epoch: 172 | Batch_idx: 160 |  Loss_1: (0.4872) | Acc_1: (81.88%) (16873/20608)\n",
      "Epoch: 172 | Batch_idx: 170 |  Loss_1: (0.4876) | Acc_1: (81.85%) (17915/21888)\n",
      "Epoch: 172 | Batch_idx: 180 |  Loss_1: (0.4905) | Acc_1: (81.73%) (18935/23168)\n",
      "Epoch: 172 | Batch_idx: 190 |  Loss_1: (0.4910) | Acc_1: (81.70%) (19975/24448)\n",
      "Epoch: 172 | Batch_idx: 200 |  Loss_1: (0.4896) | Acc_1: (81.77%) (21039/25728)\n",
      "Epoch: 172 | Batch_idx: 210 |  Loss_1: (0.4908) | Acc_1: (81.72%) (22072/27008)\n",
      "Epoch: 172 | Batch_idx: 220 |  Loss_1: (0.4900) | Acc_1: (81.77%) (23130/28288)\n",
      "Epoch: 172 | Batch_idx: 230 |  Loss_1: (0.4918) | Acc_1: (81.70%) (24158/29568)\n",
      "Epoch: 172 | Batch_idx: 240 |  Loss_1: (0.4919) | Acc_1: (81.66%) (25191/30848)\n",
      "Epoch: 172 | Batch_idx: 250 |  Loss_1: (0.4926) | Acc_1: (81.63%) (26226/32128)\n",
      "Epoch: 172 | Batch_idx: 260 |  Loss_1: (0.4924) | Acc_1: (81.64%) (27275/33408)\n",
      "Epoch: 172 | Batch_idx: 270 |  Loss_1: (0.4929) | Acc_1: (81.62%) (28314/34688)\n",
      "Epoch: 172 | Batch_idx: 280 |  Loss_1: (0.4916) | Acc_1: (81.68%) (29377/35968)\n",
      "Epoch: 172 | Batch_idx: 290 |  Loss_1: (0.4913) | Acc_1: (81.68%) (30426/37248)\n",
      "Epoch: 172 | Batch_idx: 300 |  Loss_1: (0.4912) | Acc_1: (81.69%) (31475/38528)\n",
      "Epoch: 172 | Batch_idx: 310 |  Loss_1: (0.4912) | Acc_1: (81.70%) (32525/39808)\n",
      "Epoch: 172 | Batch_idx: 320 |  Loss_1: (0.4916) | Acc_1: (81.70%) (33569/41088)\n",
      "Epoch: 172 | Batch_idx: 330 |  Loss_1: (0.4916) | Acc_1: (81.71%) (34618/42368)\n",
      "Epoch: 172 | Batch_idx: 340 |  Loss_1: (0.4927) | Acc_1: (81.66%) (35645/43648)\n",
      "Epoch: 172 | Batch_idx: 350 |  Loss_1: (0.4905) | Acc_1: (81.74%) (36723/44928)\n",
      "Epoch: 172 | Batch_idx: 360 |  Loss_1: (0.4890) | Acc_1: (81.80%) (37797/46208)\n",
      "Epoch: 172 | Batch_idx: 370 |  Loss_1: (0.4892) | Acc_1: (81.79%) (38840/47488)\n",
      "Epoch: 172 | Batch_idx: 380 |  Loss_1: (0.4901) | Acc_1: (81.76%) (39875/48768)\n",
      "Epoch: 172 | Batch_idx: 390 |  Loss_1: (0.4912) | Acc_1: (81.73%) (40864/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4818) | Acc: (90.87%) (9087/10000)\n",
      "Epoch: 173 | Batch_idx: 0 |  Loss_1: (0.5510) | Acc_1: (79.69%) (102/128)\n",
      "Epoch: 173 | Batch_idx: 10 |  Loss_1: (0.4914) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 173 | Batch_idx: 20 |  Loss_1: (0.4647) | Acc_1: (82.74%) (2224/2688)\n",
      "Epoch: 173 | Batch_idx: 30 |  Loss_1: (0.4638) | Acc_1: (82.69%) (3281/3968)\n",
      "Epoch: 173 | Batch_idx: 40 |  Loss_1: (0.4688) | Acc_1: (82.43%) (4326/5248)\n",
      "Epoch: 173 | Batch_idx: 50 |  Loss_1: (0.4755) | Acc_1: (82.17%) (5364/6528)\n",
      "Epoch: 173 | Batch_idx: 60 |  Loss_1: (0.4724) | Acc_1: (82.26%) (6423/7808)\n",
      "Epoch: 173 | Batch_idx: 70 |  Loss_1: (0.4734) | Acc_1: (82.22%) (7472/9088)\n",
      "Epoch: 173 | Batch_idx: 80 |  Loss_1: (0.4722) | Acc_1: (82.27%) (8530/10368)\n",
      "Epoch: 173 | Batch_idx: 90 |  Loss_1: (0.4761) | Acc_1: (82.09%) (9562/11648)\n",
      "Epoch: 173 | Batch_idx: 100 |  Loss_1: (0.4767) | Acc_1: (82.08%) (10611/12928)\n",
      "Epoch: 173 | Batch_idx: 110 |  Loss_1: (0.4830) | Acc_1: (81.83%) (11626/14208)\n",
      "Epoch: 173 | Batch_idx: 120 |  Loss_1: (0.4846) | Acc_1: (81.83%) (12674/15488)\n",
      "Epoch: 173 | Batch_idx: 130 |  Loss_1: (0.4863) | Acc_1: (81.73%) (13704/16768)\n",
      "Epoch: 173 | Batch_idx: 140 |  Loss_1: (0.4885) | Acc_1: (81.66%) (14738/18048)\n",
      "Epoch: 173 | Batch_idx: 150 |  Loss_1: (0.4888) | Acc_1: (81.66%) (15783/19328)\n",
      "Epoch: 173 | Batch_idx: 160 |  Loss_1: (0.4860) | Acc_1: (81.76%) (16850/20608)\n",
      "Epoch: 173 | Batch_idx: 170 |  Loss_1: (0.4873) | Acc_1: (81.73%) (17888/21888)\n",
      "Epoch: 173 | Batch_idx: 180 |  Loss_1: (0.4855) | Acc_1: (81.81%) (18954/23168)\n",
      "Epoch: 173 | Batch_idx: 190 |  Loss_1: (0.4845) | Acc_1: (81.86%) (20014/24448)\n",
      "Epoch: 173 | Batch_idx: 200 |  Loss_1: (0.4861) | Acc_1: (81.83%) (21052/25728)\n",
      "Epoch: 173 | Batch_idx: 210 |  Loss_1: (0.4842) | Acc_1: (81.91%) (22123/27008)\n",
      "Epoch: 173 | Batch_idx: 220 |  Loss_1: (0.4857) | Acc_1: (81.87%) (23159/28288)\n",
      "Epoch: 173 | Batch_idx: 230 |  Loss_1: (0.4840) | Acc_1: (81.93%) (24224/29568)\n",
      "Epoch: 173 | Batch_idx: 240 |  Loss_1: (0.4831) | Acc_1: (81.97%) (25285/30848)\n",
      "Epoch: 173 | Batch_idx: 250 |  Loss_1: (0.4848) | Acc_1: (81.92%) (26318/32128)\n",
      "Epoch: 173 | Batch_idx: 260 |  Loss_1: (0.4857) | Acc_1: (81.89%) (27357/33408)\n",
      "Epoch: 173 | Batch_idx: 270 |  Loss_1: (0.4866) | Acc_1: (81.86%) (28397/34688)\n",
      "Epoch: 173 | Batch_idx: 280 |  Loss_1: (0.4858) | Acc_1: (81.89%) (29454/35968)\n",
      "Epoch: 173 | Batch_idx: 290 |  Loss_1: (0.4853) | Acc_1: (81.89%) (30503/37248)\n",
      "Epoch: 173 | Batch_idx: 300 |  Loss_1: (0.4861) | Acc_1: (81.86%) (31538/38528)\n",
      "Epoch: 173 | Batch_idx: 310 |  Loss_1: (0.4855) | Acc_1: (81.86%) (32586/39808)\n",
      "Epoch: 173 | Batch_idx: 320 |  Loss_1: (0.4870) | Acc_1: (81.80%) (33611/41088)\n",
      "Epoch: 173 | Batch_idx: 330 |  Loss_1: (0.4865) | Acc_1: (81.83%) (34671/42368)\n",
      "Epoch: 173 | Batch_idx: 340 |  Loss_1: (0.4865) | Acc_1: (81.83%) (35715/43648)\n",
      "Epoch: 173 | Batch_idx: 350 |  Loss_1: (0.4872) | Acc_1: (81.80%) (36750/44928)\n",
      "Epoch: 173 | Batch_idx: 360 |  Loss_1: (0.4867) | Acc_1: (81.81%) (37803/46208)\n",
      "Epoch: 173 | Batch_idx: 370 |  Loss_1: (0.4871) | Acc_1: (81.79%) (38839/47488)\n",
      "Epoch: 173 | Batch_idx: 380 |  Loss_1: (0.4871) | Acc_1: (81.78%) (39881/48768)\n",
      "Epoch: 173 | Batch_idx: 390 |  Loss_1: (0.4878) | Acc_1: (81.75%) (40876/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4738) | Acc: (91.19%) (9119/10000)\n",
      "Epoch: 174 | Batch_idx: 0 |  Loss_1: (0.3907) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 174 | Batch_idx: 10 |  Loss_1: (0.4562) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 174 | Batch_idx: 20 |  Loss_1: (0.4826) | Acc_1: (81.81%) (2199/2688)\n",
      "Epoch: 174 | Batch_idx: 30 |  Loss_1: (0.4791) | Acc_1: (82.03%) (3255/3968)\n",
      "Epoch: 174 | Batch_idx: 40 |  Loss_1: (0.4849) | Acc_1: (81.80%) (4293/5248)\n",
      "Epoch: 174 | Batch_idx: 50 |  Loss_1: (0.4872) | Acc_1: (81.82%) (5341/6528)\n",
      "Epoch: 174 | Batch_idx: 60 |  Loss_1: (0.4927) | Acc_1: (81.69%) (6378/7808)\n",
      "Epoch: 174 | Batch_idx: 70 |  Loss_1: (0.4898) | Acc_1: (81.70%) (7425/9088)\n",
      "Epoch: 174 | Batch_idx: 80 |  Loss_1: (0.4849) | Acc_1: (81.97%) (8499/10368)\n",
      "Epoch: 174 | Batch_idx: 90 |  Loss_1: (0.4874) | Acc_1: (81.86%) (9535/11648)\n",
      "Epoch: 174 | Batch_idx: 100 |  Loss_1: (0.4902) | Acc_1: (81.81%) (10576/12928)\n",
      "Epoch: 174 | Batch_idx: 110 |  Loss_1: (0.4910) | Acc_1: (81.80%) (11622/14208)\n",
      "Epoch: 174 | Batch_idx: 120 |  Loss_1: (0.4898) | Acc_1: (81.82%) (12672/15488)\n",
      "Epoch: 174 | Batch_idx: 130 |  Loss_1: (0.4921) | Acc_1: (81.70%) (13700/16768)\n",
      "Epoch: 174 | Batch_idx: 140 |  Loss_1: (0.4961) | Acc_1: (81.58%) (14723/18048)\n",
      "Epoch: 174 | Batch_idx: 150 |  Loss_1: (0.4963) | Acc_1: (81.57%) (15766/19328)\n",
      "Epoch: 174 | Batch_idx: 160 |  Loss_1: (0.4971) | Acc_1: (81.53%) (16802/20608)\n",
      "Epoch: 174 | Batch_idx: 170 |  Loss_1: (0.4976) | Acc_1: (81.56%) (17851/21888)\n",
      "Epoch: 174 | Batch_idx: 180 |  Loss_1: (0.4950) | Acc_1: (81.63%) (18913/23168)\n",
      "Epoch: 174 | Batch_idx: 190 |  Loss_1: (0.4956) | Acc_1: (81.61%) (19951/24448)\n",
      "Epoch: 174 | Batch_idx: 200 |  Loss_1: (0.4968) | Acc_1: (81.58%) (20989/25728)\n",
      "Epoch: 174 | Batch_idx: 210 |  Loss_1: (0.4990) | Acc_1: (81.51%) (22013/27008)\n",
      "Epoch: 174 | Batch_idx: 220 |  Loss_1: (0.4989) | Acc_1: (81.50%) (23055/28288)\n",
      "Epoch: 174 | Batch_idx: 230 |  Loss_1: (0.5003) | Acc_1: (81.45%) (24082/29568)\n",
      "Epoch: 174 | Batch_idx: 240 |  Loss_1: (0.4999) | Acc_1: (81.44%) (25124/30848)\n",
      "Epoch: 174 | Batch_idx: 250 |  Loss_1: (0.4978) | Acc_1: (81.54%) (26198/32128)\n",
      "Epoch: 174 | Batch_idx: 260 |  Loss_1: (0.4968) | Acc_1: (81.58%) (27253/33408)\n",
      "Epoch: 174 | Batch_idx: 270 |  Loss_1: (0.4948) | Acc_1: (81.65%) (28323/34688)\n",
      "Epoch: 174 | Batch_idx: 280 |  Loss_1: (0.4934) | Acc_1: (81.72%) (29392/35968)\n",
      "Epoch: 174 | Batch_idx: 290 |  Loss_1: (0.4950) | Acc_1: (81.66%) (30415/37248)\n",
      "Epoch: 174 | Batch_idx: 300 |  Loss_1: (0.4951) | Acc_1: (81.66%) (31462/38528)\n",
      "Epoch: 174 | Batch_idx: 310 |  Loss_1: (0.4945) | Acc_1: (81.68%) (32516/39808)\n",
      "Epoch: 174 | Batch_idx: 320 |  Loss_1: (0.4920) | Acc_1: (81.77%) (33596/41088)\n",
      "Epoch: 174 | Batch_idx: 330 |  Loss_1: (0.4923) | Acc_1: (81.73%) (34628/42368)\n",
      "Epoch: 174 | Batch_idx: 340 |  Loss_1: (0.4918) | Acc_1: (81.76%) (35685/43648)\n",
      "Epoch: 174 | Batch_idx: 350 |  Loss_1: (0.4926) | Acc_1: (81.73%) (36718/44928)\n",
      "Epoch: 174 | Batch_idx: 360 |  Loss_1: (0.4919) | Acc_1: (81.74%) (37770/46208)\n",
      "Epoch: 174 | Batch_idx: 370 |  Loss_1: (0.4923) | Acc_1: (81.72%) (38807/47488)\n",
      "Epoch: 174 | Batch_idx: 380 |  Loss_1: (0.4939) | Acc_1: (81.66%) (39824/48768)\n",
      "Epoch: 174 | Batch_idx: 390 |  Loss_1: (0.4938) | Acc_1: (81.67%) (40835/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4669) | Acc: (90.93%) (9093/10000)\n",
      "Epoch: 175 | Batch_idx: 0 |  Loss_1: (0.2940) | Acc_1: (89.84%) (115/128)\n",
      "Epoch: 175 | Batch_idx: 10 |  Loss_1: (0.4419) | Acc_1: (84.09%) (1184/1408)\n",
      "Epoch: 175 | Batch_idx: 20 |  Loss_1: (0.4522) | Acc_1: (83.56%) (2246/2688)\n",
      "Epoch: 175 | Batch_idx: 30 |  Loss_1: (0.4633) | Acc_1: (83.09%) (3297/3968)\n",
      "Epoch: 175 | Batch_idx: 40 |  Loss_1: (0.4769) | Acc_1: (82.37%) (4323/5248)\n",
      "Epoch: 175 | Batch_idx: 50 |  Loss_1: (0.4933) | Acc_1: (81.74%) (5336/6528)\n",
      "Epoch: 175 | Batch_idx: 60 |  Loss_1: (0.4971) | Acc_1: (81.57%) (6369/7808)\n",
      "Epoch: 175 | Batch_idx: 70 |  Loss_1: (0.4984) | Acc_1: (81.47%) (7404/9088)\n",
      "Epoch: 175 | Batch_idx: 80 |  Loss_1: (0.5054) | Acc_1: (81.16%) (8415/10368)\n",
      "Epoch: 175 | Batch_idx: 90 |  Loss_1: (0.5056) | Acc_1: (81.14%) (9451/11648)\n",
      "Epoch: 175 | Batch_idx: 100 |  Loss_1: (0.5057) | Acc_1: (81.10%) (10484/12928)\n",
      "Epoch: 175 | Batch_idx: 110 |  Loss_1: (0.5034) | Acc_1: (81.18%) (11534/14208)\n",
      "Epoch: 175 | Batch_idx: 120 |  Loss_1: (0.5062) | Acc_1: (81.02%) (12549/15488)\n",
      "Epoch: 175 | Batch_idx: 130 |  Loss_1: (0.5031) | Acc_1: (81.17%) (13610/16768)\n",
      "Epoch: 175 | Batch_idx: 140 |  Loss_1: (0.4985) | Acc_1: (81.32%) (14676/18048)\n",
      "Epoch: 175 | Batch_idx: 150 |  Loss_1: (0.4967) | Acc_1: (81.42%) (15737/19328)\n",
      "Epoch: 175 | Batch_idx: 160 |  Loss_1: (0.4960) | Acc_1: (81.45%) (16785/20608)\n",
      "Epoch: 175 | Batch_idx: 170 |  Loss_1: (0.4930) | Acc_1: (81.57%) (17855/21888)\n",
      "Epoch: 175 | Batch_idx: 180 |  Loss_1: (0.4930) | Acc_1: (81.55%) (18894/23168)\n",
      "Epoch: 175 | Batch_idx: 190 |  Loss_1: (0.4914) | Acc_1: (81.60%) (19949/24448)\n",
      "Epoch: 175 | Batch_idx: 200 |  Loss_1: (0.4920) | Acc_1: (81.59%) (20992/25728)\n",
      "Epoch: 175 | Batch_idx: 210 |  Loss_1: (0.4916) | Acc_1: (81.60%) (22038/27008)\n",
      "Epoch: 175 | Batch_idx: 220 |  Loss_1: (0.4925) | Acc_1: (81.58%) (23077/28288)\n",
      "Epoch: 175 | Batch_idx: 230 |  Loss_1: (0.4917) | Acc_1: (81.63%) (24135/29568)\n",
      "Epoch: 175 | Batch_idx: 240 |  Loss_1: (0.4890) | Acc_1: (81.73%) (25212/30848)\n",
      "Epoch: 175 | Batch_idx: 250 |  Loss_1: (0.4899) | Acc_1: (81.69%) (26244/32128)\n",
      "Epoch: 175 | Batch_idx: 260 |  Loss_1: (0.4908) | Acc_1: (81.66%) (27281/33408)\n",
      "Epoch: 175 | Batch_idx: 270 |  Loss_1: (0.4896) | Acc_1: (81.71%) (28344/34688)\n",
      "Epoch: 175 | Batch_idx: 280 |  Loss_1: (0.4887) | Acc_1: (81.74%) (29401/35968)\n",
      "Epoch: 175 | Batch_idx: 290 |  Loss_1: (0.4898) | Acc_1: (81.70%) (30431/37248)\n",
      "Epoch: 175 | Batch_idx: 300 |  Loss_1: (0.4897) | Acc_1: (81.71%) (31483/38528)\n",
      "Epoch: 175 | Batch_idx: 310 |  Loss_1: (0.4902) | Acc_1: (81.68%) (32516/39808)\n",
      "Epoch: 175 | Batch_idx: 320 |  Loss_1: (0.4903) | Acc_1: (81.68%) (33562/41088)\n",
      "Epoch: 175 | Batch_idx: 330 |  Loss_1: (0.4901) | Acc_1: (81.70%) (34615/42368)\n",
      "Epoch: 175 | Batch_idx: 340 |  Loss_1: (0.4902) | Acc_1: (81.70%) (35661/43648)\n",
      "Epoch: 175 | Batch_idx: 350 |  Loss_1: (0.4899) | Acc_1: (81.72%) (36713/44928)\n",
      "Epoch: 175 | Batch_idx: 360 |  Loss_1: (0.4908) | Acc_1: (81.67%) (37737/46208)\n",
      "Epoch: 175 | Batch_idx: 370 |  Loss_1: (0.4906) | Acc_1: (81.67%) (38783/47488)\n",
      "Epoch: 175 | Batch_idx: 380 |  Loss_1: (0.4914) | Acc_1: (81.64%) (39814/48768)\n",
      "Epoch: 175 | Batch_idx: 390 |  Loss_1: (0.4913) | Acc_1: (81.64%) (40819/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4818) | Acc: (90.68%) (9068/10000)\n",
      "Epoch: 176 | Batch_idx: 0 |  Loss_1: (0.4341) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 176 | Batch_idx: 10 |  Loss_1: (0.4998) | Acc_1: (81.32%) (1145/1408)\n",
      "Epoch: 176 | Batch_idx: 20 |  Loss_1: (0.4974) | Acc_1: (81.29%) (2185/2688)\n",
      "Epoch: 176 | Batch_idx: 30 |  Loss_1: (0.4988) | Acc_1: (81.28%) (3225/3968)\n",
      "Epoch: 176 | Batch_idx: 40 |  Loss_1: (0.5044) | Acc_1: (80.96%) (4249/5248)\n",
      "Epoch: 176 | Batch_idx: 50 |  Loss_1: (0.5054) | Acc_1: (80.93%) (5283/6528)\n",
      "Epoch: 176 | Batch_idx: 60 |  Loss_1: (0.5010) | Acc_1: (81.11%) (6333/7808)\n",
      "Epoch: 176 | Batch_idx: 70 |  Loss_1: (0.5029) | Acc_1: (81.06%) (7367/9088)\n",
      "Epoch: 176 | Batch_idx: 80 |  Loss_1: (0.5059) | Acc_1: (81.02%) (8400/10368)\n",
      "Epoch: 176 | Batch_idx: 90 |  Loss_1: (0.5094) | Acc_1: (80.91%) (9424/11648)\n",
      "Epoch: 176 | Batch_idx: 100 |  Loss_1: (0.5090) | Acc_1: (80.94%) (10464/12928)\n",
      "Epoch: 176 | Batch_idx: 110 |  Loss_1: (0.5108) | Acc_1: (80.90%) (11494/14208)\n",
      "Epoch: 176 | Batch_idx: 120 |  Loss_1: (0.5057) | Acc_1: (81.06%) (12555/15488)\n",
      "Epoch: 176 | Batch_idx: 130 |  Loss_1: (0.5042) | Acc_1: (81.12%) (13602/16768)\n",
      "Epoch: 176 | Batch_idx: 140 |  Loss_1: (0.5014) | Acc_1: (81.23%) (14661/18048)\n",
      "Epoch: 176 | Batch_idx: 150 |  Loss_1: (0.4994) | Acc_1: (81.32%) (15717/19328)\n",
      "Epoch: 176 | Batch_idx: 160 |  Loss_1: (0.5003) | Acc_1: (81.29%) (16752/20608)\n",
      "Epoch: 176 | Batch_idx: 170 |  Loss_1: (0.4990) | Acc_1: (81.36%) (17807/21888)\n",
      "Epoch: 176 | Batch_idx: 180 |  Loss_1: (0.4977) | Acc_1: (81.41%) (18861/23168)\n",
      "Epoch: 176 | Batch_idx: 190 |  Loss_1: (0.4949) | Acc_1: (81.52%) (19929/24448)\n",
      "Epoch: 176 | Batch_idx: 200 |  Loss_1: (0.4957) | Acc_1: (81.49%) (20966/25728)\n",
      "Epoch: 176 | Batch_idx: 210 |  Loss_1: (0.4946) | Acc_1: (81.51%) (22015/27008)\n",
      "Epoch: 176 | Batch_idx: 220 |  Loss_1: (0.4931) | Acc_1: (81.56%) (23072/28288)\n",
      "Epoch: 176 | Batch_idx: 230 |  Loss_1: (0.4934) | Acc_1: (81.53%) (24106/29568)\n",
      "Epoch: 176 | Batch_idx: 240 |  Loss_1: (0.4937) | Acc_1: (81.52%) (25146/30848)\n",
      "Epoch: 176 | Batch_idx: 250 |  Loss_1: (0.4951) | Acc_1: (81.47%) (26174/32128)\n",
      "Epoch: 176 | Batch_idx: 260 |  Loss_1: (0.4949) | Acc_1: (81.46%) (27215/33408)\n",
      "Epoch: 176 | Batch_idx: 270 |  Loss_1: (0.4931) | Acc_1: (81.53%) (28280/34688)\n",
      "Epoch: 176 | Batch_idx: 280 |  Loss_1: (0.4934) | Acc_1: (81.52%) (29321/35968)\n",
      "Epoch: 176 | Batch_idx: 290 |  Loss_1: (0.4934) | Acc_1: (81.53%) (30370/37248)\n",
      "Epoch: 176 | Batch_idx: 300 |  Loss_1: (0.4945) | Acc_1: (81.49%) (31398/38528)\n",
      "Epoch: 176 | Batch_idx: 310 |  Loss_1: (0.4948) | Acc_1: (81.50%) (32442/39808)\n",
      "Epoch: 176 | Batch_idx: 320 |  Loss_1: (0.4933) | Acc_1: (81.55%) (33506/41088)\n",
      "Epoch: 176 | Batch_idx: 330 |  Loss_1: (0.4934) | Acc_1: (81.55%) (34551/42368)\n",
      "Epoch: 176 | Batch_idx: 340 |  Loss_1: (0.4924) | Acc_1: (81.58%) (35608/43648)\n",
      "Epoch: 176 | Batch_idx: 350 |  Loss_1: (0.4929) | Acc_1: (81.55%) (36641/44928)\n",
      "Epoch: 176 | Batch_idx: 360 |  Loss_1: (0.4935) | Acc_1: (81.55%) (37681/46208)\n",
      "Epoch: 176 | Batch_idx: 370 |  Loss_1: (0.4930) | Acc_1: (81.58%) (38743/47488)\n",
      "Epoch: 176 | Batch_idx: 380 |  Loss_1: (0.4924) | Acc_1: (81.60%) (39795/48768)\n",
      "Epoch: 176 | Batch_idx: 390 |  Loss_1: (0.4923) | Acc_1: (81.61%) (40803/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4452) | Acc: (90.79%) (9079/10000)\n",
      "Epoch: 177 | Batch_idx: 0 |  Loss_1: (0.5301) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 177 | Batch_idx: 10 |  Loss_1: (0.4656) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 177 | Batch_idx: 20 |  Loss_1: (0.4785) | Acc_1: (81.92%) (2202/2688)\n",
      "Epoch: 177 | Batch_idx: 30 |  Loss_1: (0.4742) | Acc_1: (82.16%) (3260/3968)\n",
      "Epoch: 177 | Batch_idx: 40 |  Loss_1: (0.4804) | Acc_1: (81.88%) (4297/5248)\n",
      "Epoch: 177 | Batch_idx: 50 |  Loss_1: (0.4858) | Acc_1: (81.79%) (5339/6528)\n",
      "Epoch: 177 | Batch_idx: 60 |  Loss_1: (0.4868) | Acc_1: (81.80%) (6387/7808)\n",
      "Epoch: 177 | Batch_idx: 70 |  Loss_1: (0.4863) | Acc_1: (81.86%) (7439/9088)\n",
      "Epoch: 177 | Batch_idx: 80 |  Loss_1: (0.4888) | Acc_1: (81.78%) (8479/10368)\n",
      "Epoch: 177 | Batch_idx: 90 |  Loss_1: (0.4894) | Acc_1: (81.75%) (9522/11648)\n",
      "Epoch: 177 | Batch_idx: 100 |  Loss_1: (0.4894) | Acc_1: (81.74%) (10567/12928)\n",
      "Epoch: 177 | Batch_idx: 110 |  Loss_1: (0.4918) | Acc_1: (81.70%) (11608/14208)\n",
      "Epoch: 177 | Batch_idx: 120 |  Loss_1: (0.4898) | Acc_1: (81.77%) (12665/15488)\n",
      "Epoch: 177 | Batch_idx: 130 |  Loss_1: (0.4893) | Acc_1: (81.80%) (13717/16768)\n",
      "Epoch: 177 | Batch_idx: 140 |  Loss_1: (0.4890) | Acc_1: (81.83%) (14769/18048)\n",
      "Epoch: 177 | Batch_idx: 150 |  Loss_1: (0.4931) | Acc_1: (81.71%) (15792/19328)\n",
      "Epoch: 177 | Batch_idx: 160 |  Loss_1: (0.4916) | Acc_1: (81.78%) (16853/20608)\n",
      "Epoch: 177 | Batch_idx: 170 |  Loss_1: (0.4883) | Acc_1: (81.89%) (17925/21888)\n",
      "Epoch: 177 | Batch_idx: 180 |  Loss_1: (0.4895) | Acc_1: (81.85%) (18963/23168)\n",
      "Epoch: 177 | Batch_idx: 190 |  Loss_1: (0.4888) | Acc_1: (81.86%) (20012/24448)\n",
      "Epoch: 177 | Batch_idx: 200 |  Loss_1: (0.4915) | Acc_1: (81.76%) (21035/25728)\n",
      "Epoch: 177 | Batch_idx: 210 |  Loss_1: (0.4904) | Acc_1: (81.80%) (22092/27008)\n",
      "Epoch: 177 | Batch_idx: 220 |  Loss_1: (0.4920) | Acc_1: (81.71%) (23115/28288)\n",
      "Epoch: 177 | Batch_idx: 230 |  Loss_1: (0.4919) | Acc_1: (81.74%) (24169/29568)\n",
      "Epoch: 177 | Batch_idx: 240 |  Loss_1: (0.4919) | Acc_1: (81.76%) (25220/30848)\n",
      "Epoch: 177 | Batch_idx: 250 |  Loss_1: (0.4908) | Acc_1: (81.79%) (26276/32128)\n",
      "Epoch: 177 | Batch_idx: 260 |  Loss_1: (0.4909) | Acc_1: (81.79%) (27324/33408)\n",
      "Epoch: 177 | Batch_idx: 270 |  Loss_1: (0.4905) | Acc_1: (81.78%) (28367/34688)\n",
      "Epoch: 177 | Batch_idx: 280 |  Loss_1: (0.4891) | Acc_1: (81.84%) (29436/35968)\n",
      "Epoch: 177 | Batch_idx: 290 |  Loss_1: (0.4890) | Acc_1: (81.84%) (30482/37248)\n",
      "Epoch: 177 | Batch_idx: 300 |  Loss_1: (0.4873) | Acc_1: (81.90%) (31554/38528)\n",
      "Epoch: 177 | Batch_idx: 310 |  Loss_1: (0.4859) | Acc_1: (81.96%) (32626/39808)\n",
      "Epoch: 177 | Batch_idx: 320 |  Loss_1: (0.4855) | Acc_1: (81.98%) (33685/41088)\n",
      "Epoch: 177 | Batch_idx: 330 |  Loss_1: (0.4861) | Acc_1: (81.97%) (34728/42368)\n",
      "Epoch: 177 | Batch_idx: 340 |  Loss_1: (0.4865) | Acc_1: (81.95%) (35770/43648)\n",
      "Epoch: 177 | Batch_idx: 350 |  Loss_1: (0.4869) | Acc_1: (81.92%) (36806/44928)\n",
      "Epoch: 177 | Batch_idx: 360 |  Loss_1: (0.4885) | Acc_1: (81.86%) (37828/46208)\n",
      "Epoch: 177 | Batch_idx: 370 |  Loss_1: (0.4888) | Acc_1: (81.86%) (38874/47488)\n",
      "Epoch: 177 | Batch_idx: 380 |  Loss_1: (0.4876) | Acc_1: (81.90%) (39941/48768)\n",
      "Epoch: 177 | Batch_idx: 390 |  Loss_1: (0.4879) | Acc_1: (81.90%) (40949/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4646) | Acc: (90.65%) (9065/10000)\n",
      "Epoch: 178 | Batch_idx: 0 |  Loss_1: (0.6290) | Acc_1: (75.78%) (97/128)\n",
      "Epoch: 178 | Batch_idx: 10 |  Loss_1: (0.4885) | Acc_1: (81.82%) (1152/1408)\n",
      "Epoch: 178 | Batch_idx: 20 |  Loss_1: (0.4901) | Acc_1: (81.92%) (2202/2688)\n",
      "Epoch: 178 | Batch_idx: 30 |  Loss_1: (0.4846) | Acc_1: (82.11%) (3258/3968)\n",
      "Epoch: 178 | Batch_idx: 40 |  Loss_1: (0.4852) | Acc_1: (82.03%) (4305/5248)\n",
      "Epoch: 178 | Batch_idx: 50 |  Loss_1: (0.4838) | Acc_1: (82.02%) (5354/6528)\n",
      "Epoch: 178 | Batch_idx: 60 |  Loss_1: (0.4864) | Acc_1: (81.92%) (6396/7808)\n",
      "Epoch: 178 | Batch_idx: 70 |  Loss_1: (0.4850) | Acc_1: (82.05%) (7457/9088)\n",
      "Epoch: 178 | Batch_idx: 80 |  Loss_1: (0.4848) | Acc_1: (82.04%) (8506/10368)\n",
      "Epoch: 178 | Batch_idx: 90 |  Loss_1: (0.4818) | Acc_1: (82.11%) (9564/11648)\n",
      "Epoch: 178 | Batch_idx: 100 |  Loss_1: (0.4820) | Acc_1: (82.07%) (10610/12928)\n",
      "Epoch: 178 | Batch_idx: 110 |  Loss_1: (0.4828) | Acc_1: (82.01%) (11652/14208)\n",
      "Epoch: 178 | Batch_idx: 120 |  Loss_1: (0.4836) | Acc_1: (81.99%) (12699/15488)\n",
      "Epoch: 178 | Batch_idx: 130 |  Loss_1: (0.4817) | Acc_1: (82.06%) (13760/16768)\n",
      "Epoch: 178 | Batch_idx: 140 |  Loss_1: (0.4805) | Acc_1: (82.11%) (14820/18048)\n",
      "Epoch: 178 | Batch_idx: 150 |  Loss_1: (0.4808) | Acc_1: (82.08%) (15864/19328)\n",
      "Epoch: 178 | Batch_idx: 160 |  Loss_1: (0.4799) | Acc_1: (82.10%) (16920/20608)\n",
      "Epoch: 178 | Batch_idx: 170 |  Loss_1: (0.4781) | Acc_1: (82.17%) (17986/21888)\n",
      "Epoch: 178 | Batch_idx: 180 |  Loss_1: (0.4771) | Acc_1: (82.22%) (19049/23168)\n",
      "Epoch: 178 | Batch_idx: 190 |  Loss_1: (0.4777) | Acc_1: (82.19%) (20095/24448)\n",
      "Epoch: 178 | Batch_idx: 200 |  Loss_1: (0.4762) | Acc_1: (82.26%) (21164/25728)\n",
      "Epoch: 178 | Batch_idx: 210 |  Loss_1: (0.4763) | Acc_1: (82.26%) (22217/27008)\n",
      "Epoch: 178 | Batch_idx: 220 |  Loss_1: (0.4748) | Acc_1: (82.26%) (23270/28288)\n",
      "Epoch: 178 | Batch_idx: 230 |  Loss_1: (0.4757) | Acc_1: (82.25%) (24319/29568)\n",
      "Epoch: 178 | Batch_idx: 240 |  Loss_1: (0.4751) | Acc_1: (82.27%) (25379/30848)\n",
      "Epoch: 178 | Batch_idx: 250 |  Loss_1: (0.4752) | Acc_1: (82.27%) (26433/32128)\n",
      "Epoch: 178 | Batch_idx: 260 |  Loss_1: (0.4763) | Acc_1: (82.22%) (27467/33408)\n",
      "Epoch: 178 | Batch_idx: 270 |  Loss_1: (0.4773) | Acc_1: (82.17%) (28504/34688)\n",
      "Epoch: 178 | Batch_idx: 280 |  Loss_1: (0.4775) | Acc_1: (82.15%) (29549/35968)\n",
      "Epoch: 178 | Batch_idx: 290 |  Loss_1: (0.4784) | Acc_1: (82.12%) (30587/37248)\n",
      "Epoch: 178 | Batch_idx: 300 |  Loss_1: (0.4783) | Acc_1: (82.12%) (31638/38528)\n",
      "Epoch: 178 | Batch_idx: 310 |  Loss_1: (0.4777) | Acc_1: (82.14%) (32698/39808)\n",
      "Epoch: 178 | Batch_idx: 320 |  Loss_1: (0.4786) | Acc_1: (82.11%) (33739/41088)\n",
      "Epoch: 178 | Batch_idx: 330 |  Loss_1: (0.4790) | Acc_1: (82.12%) (34791/42368)\n",
      "Epoch: 178 | Batch_idx: 340 |  Loss_1: (0.4794) | Acc_1: (82.10%) (35835/43648)\n",
      "Epoch: 178 | Batch_idx: 350 |  Loss_1: (0.4813) | Acc_1: (82.02%) (36848/44928)\n",
      "Epoch: 178 | Batch_idx: 360 |  Loss_1: (0.4812) | Acc_1: (82.03%) (37904/46208)\n",
      "Epoch: 178 | Batch_idx: 370 |  Loss_1: (0.4827) | Acc_1: (81.99%) (38936/47488)\n",
      "Epoch: 178 | Batch_idx: 380 |  Loss_1: (0.4822) | Acc_1: (82.01%) (39994/48768)\n",
      "Epoch: 178 | Batch_idx: 390 |  Loss_1: (0.4816) | Acc_1: (82.04%) (41020/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4627) | Acc: (90.76%) (9076/10000)\n",
      "Epoch: 179 | Batch_idx: 0 |  Loss_1: (0.5413) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 179 | Batch_idx: 10 |  Loss_1: (0.5138) | Acc_1: (80.68%) (1136/1408)\n",
      "Epoch: 179 | Batch_idx: 20 |  Loss_1: (0.5195) | Acc_1: (80.28%) (2158/2688)\n",
      "Epoch: 179 | Batch_idx: 30 |  Loss_1: (0.5148) | Acc_1: (80.65%) (3200/3968)\n",
      "Epoch: 179 | Batch_idx: 40 |  Loss_1: (0.4982) | Acc_1: (81.29%) (4266/5248)\n",
      "Epoch: 179 | Batch_idx: 50 |  Loss_1: (0.5013) | Acc_1: (81.17%) (5299/6528)\n",
      "Epoch: 179 | Batch_idx: 60 |  Loss_1: (0.4944) | Acc_1: (81.37%) (6353/7808)\n",
      "Epoch: 179 | Batch_idx: 70 |  Loss_1: (0.4963) | Acc_1: (81.34%) (7392/9088)\n",
      "Epoch: 179 | Batch_idx: 80 |  Loss_1: (0.4993) | Acc_1: (81.29%) (8428/10368)\n",
      "Epoch: 179 | Batch_idx: 90 |  Loss_1: (0.4954) | Acc_1: (81.43%) (9485/11648)\n",
      "Epoch: 179 | Batch_idx: 100 |  Loss_1: (0.4970) | Acc_1: (81.36%) (10518/12928)\n",
      "Epoch: 179 | Batch_idx: 110 |  Loss_1: (0.4963) | Acc_1: (81.33%) (11556/14208)\n",
      "Epoch: 179 | Batch_idx: 120 |  Loss_1: (0.5003) | Acc_1: (81.17%) (12571/15488)\n",
      "Epoch: 179 | Batch_idx: 130 |  Loss_1: (0.4981) | Acc_1: (81.25%) (13624/16768)\n",
      "Epoch: 179 | Batch_idx: 140 |  Loss_1: (0.4990) | Acc_1: (81.26%) (14665/18048)\n",
      "Epoch: 179 | Batch_idx: 150 |  Loss_1: (0.5000) | Acc_1: (81.23%) (15701/19328)\n",
      "Epoch: 179 | Batch_idx: 160 |  Loss_1: (0.4990) | Acc_1: (81.30%) (16754/20608)\n",
      "Epoch: 179 | Batch_idx: 170 |  Loss_1: (0.5003) | Acc_1: (81.25%) (17785/21888)\n",
      "Epoch: 179 | Batch_idx: 180 |  Loss_1: (0.4988) | Acc_1: (81.29%) (18833/23168)\n",
      "Epoch: 179 | Batch_idx: 190 |  Loss_1: (0.4985) | Acc_1: (81.31%) (19879/24448)\n",
      "Epoch: 179 | Batch_idx: 200 |  Loss_1: (0.4969) | Acc_1: (81.40%) (20943/25728)\n",
      "Epoch: 179 | Batch_idx: 210 |  Loss_1: (0.4959) | Acc_1: (81.46%) (22001/27008)\n",
      "Epoch: 179 | Batch_idx: 220 |  Loss_1: (0.4973) | Acc_1: (81.37%) (23018/28288)\n",
      "Epoch: 179 | Batch_idx: 230 |  Loss_1: (0.4996) | Acc_1: (81.29%) (24036/29568)\n",
      "Epoch: 179 | Batch_idx: 240 |  Loss_1: (0.4984) | Acc_1: (81.34%) (25093/30848)\n",
      "Epoch: 179 | Batch_idx: 250 |  Loss_1: (0.4974) | Acc_1: (81.39%) (26149/32128)\n",
      "Epoch: 179 | Batch_idx: 260 |  Loss_1: (0.4977) | Acc_1: (81.36%) (27182/33408)\n",
      "Epoch: 179 | Batch_idx: 270 |  Loss_1: (0.4968) | Acc_1: (81.38%) (28229/34688)\n",
      "Epoch: 179 | Batch_idx: 280 |  Loss_1: (0.4959) | Acc_1: (81.43%) (29287/35968)\n",
      "Epoch: 179 | Batch_idx: 290 |  Loss_1: (0.4936) | Acc_1: (81.51%) (30361/37248)\n",
      "Epoch: 179 | Batch_idx: 300 |  Loss_1: (0.4927) | Acc_1: (81.53%) (31413/38528)\n",
      "Epoch: 179 | Batch_idx: 310 |  Loss_1: (0.4919) | Acc_1: (81.55%) (32464/39808)\n",
      "Epoch: 179 | Batch_idx: 320 |  Loss_1: (0.4925) | Acc_1: (81.54%) (33503/41088)\n",
      "Epoch: 179 | Batch_idx: 330 |  Loss_1: (0.4923) | Acc_1: (81.55%) (34552/42368)\n",
      "Epoch: 179 | Batch_idx: 340 |  Loss_1: (0.4918) | Acc_1: (81.58%) (35606/43648)\n",
      "Epoch: 179 | Batch_idx: 350 |  Loss_1: (0.4929) | Acc_1: (81.53%) (36631/44928)\n",
      "Epoch: 179 | Batch_idx: 360 |  Loss_1: (0.4920) | Acc_1: (81.59%) (37700/46208)\n",
      "Epoch: 179 | Batch_idx: 370 |  Loss_1: (0.4902) | Acc_1: (81.66%) (38778/47488)\n",
      "Epoch: 179 | Batch_idx: 380 |  Loss_1: (0.4896) | Acc_1: (81.69%) (39838/48768)\n",
      "Epoch: 179 | Batch_idx: 390 |  Loss_1: (0.4902) | Acc_1: (81.68%) (40838/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4927) | Acc: (90.34%) (9034/10000)\n",
      "Epoch: 180 | Batch_idx: 0 |  Loss_1: (0.5479) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 180 | Batch_idx: 10 |  Loss_1: (0.4643) | Acc_1: (82.60%) (1163/1408)\n",
      "Epoch: 180 | Batch_idx: 20 |  Loss_1: (0.4860) | Acc_1: (81.73%) (2197/2688)\n",
      "Epoch: 180 | Batch_idx: 30 |  Loss_1: (0.4954) | Acc_1: (81.58%) (3237/3968)\n",
      "Epoch: 180 | Batch_idx: 40 |  Loss_1: (0.4943) | Acc_1: (81.50%) (4277/5248)\n",
      "Epoch: 180 | Batch_idx: 50 |  Loss_1: (0.4895) | Acc_1: (81.71%) (5334/6528)\n",
      "Epoch: 180 | Batch_idx: 60 |  Loss_1: (0.4860) | Acc_1: (81.86%) (6392/7808)\n",
      "Epoch: 180 | Batch_idx: 70 |  Loss_1: (0.4781) | Acc_1: (82.21%) (7471/9088)\n",
      "Epoch: 180 | Batch_idx: 80 |  Loss_1: (0.4797) | Acc_1: (82.14%) (8516/10368)\n",
      "Epoch: 180 | Batch_idx: 90 |  Loss_1: (0.4784) | Acc_1: (82.20%) (9575/11648)\n",
      "Epoch: 180 | Batch_idx: 100 |  Loss_1: (0.4801) | Acc_1: (82.16%) (10621/12928)\n",
      "Epoch: 180 | Batch_idx: 110 |  Loss_1: (0.4804) | Acc_1: (82.14%) (11670/14208)\n",
      "Epoch: 180 | Batch_idx: 120 |  Loss_1: (0.4808) | Acc_1: (82.14%) (12722/15488)\n",
      "Epoch: 180 | Batch_idx: 130 |  Loss_1: (0.4811) | Acc_1: (82.12%) (13770/16768)\n",
      "Epoch: 180 | Batch_idx: 140 |  Loss_1: (0.4802) | Acc_1: (82.15%) (14827/18048)\n",
      "Epoch: 180 | Batch_idx: 150 |  Loss_1: (0.4809) | Acc_1: (82.12%) (15873/19328)\n",
      "Epoch: 180 | Batch_idx: 160 |  Loss_1: (0.4807) | Acc_1: (82.15%) (16930/20608)\n",
      "Epoch: 180 | Batch_idx: 170 |  Loss_1: (0.4837) | Acc_1: (82.03%) (17955/21888)\n",
      "Epoch: 180 | Batch_idx: 180 |  Loss_1: (0.4817) | Acc_1: (82.10%) (19021/23168)\n",
      "Epoch: 180 | Batch_idx: 190 |  Loss_1: (0.4806) | Acc_1: (82.13%) (20080/24448)\n",
      "Epoch: 180 | Batch_idx: 200 |  Loss_1: (0.4813) | Acc_1: (82.09%) (21119/25728)\n",
      "Epoch: 180 | Batch_idx: 210 |  Loss_1: (0.4826) | Acc_1: (82.04%) (22158/27008)\n",
      "Epoch: 180 | Batch_idx: 220 |  Loss_1: (0.4822) | Acc_1: (82.07%) (23217/28288)\n",
      "Epoch: 180 | Batch_idx: 230 |  Loss_1: (0.4818) | Acc_1: (82.07%) (24267/29568)\n",
      "Epoch: 180 | Batch_idx: 240 |  Loss_1: (0.4819) | Acc_1: (82.06%) (25315/30848)\n",
      "Epoch: 180 | Batch_idx: 250 |  Loss_1: (0.4828) | Acc_1: (82.05%) (26360/32128)\n",
      "Epoch: 180 | Batch_idx: 260 |  Loss_1: (0.4831) | Acc_1: (82.04%) (27407/33408)\n",
      "Epoch: 180 | Batch_idx: 270 |  Loss_1: (0.4831) | Acc_1: (82.03%) (28455/34688)\n",
      "Epoch: 180 | Batch_idx: 280 |  Loss_1: (0.4827) | Acc_1: (82.05%) (29510/35968)\n",
      "Epoch: 180 | Batch_idx: 290 |  Loss_1: (0.4825) | Acc_1: (82.04%) (30559/37248)\n",
      "Epoch: 180 | Batch_idx: 300 |  Loss_1: (0.4803) | Acc_1: (82.11%) (31635/38528)\n",
      "Epoch: 180 | Batch_idx: 310 |  Loss_1: (0.4802) | Acc_1: (82.10%) (32682/39808)\n",
      "Epoch: 180 | Batch_idx: 320 |  Loss_1: (0.4826) | Acc_1: (82.03%) (33706/41088)\n",
      "Epoch: 180 | Batch_idx: 330 |  Loss_1: (0.4823) | Acc_1: (82.03%) (34755/42368)\n",
      "Epoch: 180 | Batch_idx: 340 |  Loss_1: (0.4835) | Acc_1: (81.99%) (35789/43648)\n",
      "Epoch: 180 | Batch_idx: 350 |  Loss_1: (0.4828) | Acc_1: (82.02%) (36850/44928)\n",
      "Epoch: 180 | Batch_idx: 360 |  Loss_1: (0.4820) | Acc_1: (82.05%) (37913/46208)\n",
      "Epoch: 180 | Batch_idx: 370 |  Loss_1: (0.4833) | Acc_1: (82.00%) (38942/47488)\n",
      "Epoch: 180 | Batch_idx: 380 |  Loss_1: (0.4839) | Acc_1: (81.99%) (39986/48768)\n",
      "Epoch: 180 | Batch_idx: 390 |  Loss_1: (0.4831) | Acc_1: (82.01%) (41007/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4715) | Acc: (90.41%) (9041/10000)\n",
      "Epoch: 181 | Batch_idx: 0 |  Loss_1: (0.6458) | Acc_1: (76.56%) (98/128)\n",
      "Epoch: 181 | Batch_idx: 10 |  Loss_1: (0.5390) | Acc_1: (79.90%) (1125/1408)\n",
      "Epoch: 181 | Batch_idx: 20 |  Loss_1: (0.5228) | Acc_1: (80.47%) (2163/2688)\n",
      "Epoch: 181 | Batch_idx: 30 |  Loss_1: (0.4964) | Acc_1: (81.43%) (3231/3968)\n",
      "Epoch: 181 | Batch_idx: 40 |  Loss_1: (0.4988) | Acc_1: (81.36%) (4270/5248)\n",
      "Epoch: 181 | Batch_idx: 50 |  Loss_1: (0.4940) | Acc_1: (81.60%) (5327/6528)\n",
      "Epoch: 181 | Batch_idx: 60 |  Loss_1: (0.4919) | Acc_1: (81.70%) (6379/7808)\n",
      "Epoch: 181 | Batch_idx: 70 |  Loss_1: (0.4888) | Acc_1: (81.82%) (7436/9088)\n",
      "Epoch: 181 | Batch_idx: 80 |  Loss_1: (0.4825) | Acc_1: (82.00%) (8502/10368)\n",
      "Epoch: 181 | Batch_idx: 90 |  Loss_1: (0.4794) | Acc_1: (82.17%) (9571/11648)\n",
      "Epoch: 181 | Batch_idx: 100 |  Loss_1: (0.4786) | Acc_1: (82.19%) (10626/12928)\n",
      "Epoch: 181 | Batch_idx: 110 |  Loss_1: (0.4821) | Acc_1: (82.07%) (11660/14208)\n",
      "Epoch: 181 | Batch_idx: 120 |  Loss_1: (0.4875) | Acc_1: (81.83%) (12674/15488)\n",
      "Epoch: 181 | Batch_idx: 130 |  Loss_1: (0.4880) | Acc_1: (81.84%) (13723/16768)\n",
      "Epoch: 181 | Batch_idx: 140 |  Loss_1: (0.4883) | Acc_1: (81.80%) (14764/18048)\n",
      "Epoch: 181 | Batch_idx: 150 |  Loss_1: (0.4897) | Acc_1: (81.77%) (15805/19328)\n",
      "Epoch: 181 | Batch_idx: 160 |  Loss_1: (0.4890) | Acc_1: (81.79%) (16856/20608)\n",
      "Epoch: 181 | Batch_idx: 170 |  Loss_1: (0.4897) | Acc_1: (81.78%) (17899/21888)\n",
      "Epoch: 181 | Batch_idx: 180 |  Loss_1: (0.4890) | Acc_1: (81.80%) (18952/23168)\n",
      "Epoch: 181 | Batch_idx: 190 |  Loss_1: (0.4904) | Acc_1: (81.74%) (19984/24448)\n",
      "Epoch: 181 | Batch_idx: 200 |  Loss_1: (0.4901) | Acc_1: (81.74%) (21031/25728)\n",
      "Epoch: 181 | Batch_idx: 210 |  Loss_1: (0.4907) | Acc_1: (81.72%) (22070/27008)\n",
      "Epoch: 181 | Batch_idx: 220 |  Loss_1: (0.4905) | Acc_1: (81.73%) (23120/28288)\n",
      "Epoch: 181 | Batch_idx: 230 |  Loss_1: (0.4906) | Acc_1: (81.74%) (24169/29568)\n",
      "Epoch: 181 | Batch_idx: 240 |  Loss_1: (0.4908) | Acc_1: (81.74%) (25214/30848)\n",
      "Epoch: 181 | Batch_idx: 250 |  Loss_1: (0.4896) | Acc_1: (81.76%) (26269/32128)\n",
      "Epoch: 181 | Batch_idx: 260 |  Loss_1: (0.4871) | Acc_1: (81.87%) (27350/33408)\n",
      "Epoch: 181 | Batch_idx: 270 |  Loss_1: (0.4869) | Acc_1: (81.88%) (28403/34688)\n",
      "Epoch: 181 | Batch_idx: 280 |  Loss_1: (0.4867) | Acc_1: (81.88%) (29452/35968)\n",
      "Epoch: 181 | Batch_idx: 290 |  Loss_1: (0.4879) | Acc_1: (81.84%) (30485/37248)\n",
      "Epoch: 181 | Batch_idx: 300 |  Loss_1: (0.4895) | Acc_1: (81.79%) (31513/38528)\n",
      "Epoch: 181 | Batch_idx: 310 |  Loss_1: (0.4902) | Acc_1: (81.76%) (32547/39808)\n",
      "Epoch: 181 | Batch_idx: 320 |  Loss_1: (0.4891) | Acc_1: (81.81%) (33613/41088)\n",
      "Epoch: 181 | Batch_idx: 330 |  Loss_1: (0.4899) | Acc_1: (81.76%) (34642/42368)\n",
      "Epoch: 181 | Batch_idx: 340 |  Loss_1: (0.4900) | Acc_1: (81.75%) (35681/43648)\n",
      "Epoch: 181 | Batch_idx: 350 |  Loss_1: (0.4915) | Acc_1: (81.69%) (36703/44928)\n",
      "Epoch: 181 | Batch_idx: 360 |  Loss_1: (0.4903) | Acc_1: (81.72%) (37762/46208)\n",
      "Epoch: 181 | Batch_idx: 370 |  Loss_1: (0.4898) | Acc_1: (81.73%) (38814/47488)\n",
      "Epoch: 181 | Batch_idx: 380 |  Loss_1: (0.4904) | Acc_1: (81.71%) (39849/48768)\n",
      "Epoch: 181 | Batch_idx: 390 |  Loss_1: (0.4914) | Acc_1: (81.67%) (40837/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4826) | Acc: (90.81%) (9081/10000)\n",
      "Epoch: 182 | Batch_idx: 0 |  Loss_1: (0.4595) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 182 | Batch_idx: 10 |  Loss_1: (0.4914) | Acc_1: (81.89%) (1153/1408)\n",
      "Epoch: 182 | Batch_idx: 20 |  Loss_1: (0.4678) | Acc_1: (82.44%) (2216/2688)\n",
      "Epoch: 182 | Batch_idx: 30 |  Loss_1: (0.4836) | Acc_1: (81.88%) (3249/3968)\n",
      "Epoch: 182 | Batch_idx: 40 |  Loss_1: (0.4838) | Acc_1: (81.90%) (4298/5248)\n",
      "Epoch: 182 | Batch_idx: 50 |  Loss_1: (0.4805) | Acc_1: (82.12%) (5361/6528)\n",
      "Epoch: 182 | Batch_idx: 60 |  Loss_1: (0.4766) | Acc_1: (82.25%) (6422/7808)\n",
      "Epoch: 182 | Batch_idx: 70 |  Loss_1: (0.4761) | Acc_1: (82.19%) (7469/9088)\n",
      "Epoch: 182 | Batch_idx: 80 |  Loss_1: (0.4841) | Acc_1: (81.88%) (8489/10368)\n",
      "Epoch: 182 | Batch_idx: 90 |  Loss_1: (0.4811) | Acc_1: (81.95%) (9545/11648)\n",
      "Epoch: 182 | Batch_idx: 100 |  Loss_1: (0.4824) | Acc_1: (81.91%) (10589/12928)\n",
      "Epoch: 182 | Batch_idx: 110 |  Loss_1: (0.4871) | Acc_1: (81.69%) (11606/14208)\n",
      "Epoch: 182 | Batch_idx: 120 |  Loss_1: (0.4888) | Acc_1: (81.64%) (12644/15488)\n",
      "Epoch: 182 | Batch_idx: 130 |  Loss_1: (0.4891) | Acc_1: (81.67%) (13695/16768)\n",
      "Epoch: 182 | Batch_idx: 140 |  Loss_1: (0.4925) | Acc_1: (81.58%) (14724/18048)\n",
      "Epoch: 182 | Batch_idx: 150 |  Loss_1: (0.4913) | Acc_1: (81.65%) (15782/19328)\n",
      "Epoch: 182 | Batch_idx: 160 |  Loss_1: (0.4884) | Acc_1: (81.77%) (16852/20608)\n",
      "Epoch: 182 | Batch_idx: 170 |  Loss_1: (0.4885) | Acc_1: (81.75%) (17893/21888)\n",
      "Epoch: 182 | Batch_idx: 180 |  Loss_1: (0.4873) | Acc_1: (81.79%) (18948/23168)\n",
      "Epoch: 182 | Batch_idx: 190 |  Loss_1: (0.4879) | Acc_1: (81.77%) (19990/24448)\n",
      "Epoch: 182 | Batch_idx: 200 |  Loss_1: (0.4870) | Acc_1: (81.81%) (21049/25728)\n",
      "Epoch: 182 | Batch_idx: 210 |  Loss_1: (0.4867) | Acc_1: (81.82%) (22099/27008)\n",
      "Epoch: 182 | Batch_idx: 220 |  Loss_1: (0.4887) | Acc_1: (81.75%) (23126/28288)\n",
      "Epoch: 182 | Batch_idx: 230 |  Loss_1: (0.4872) | Acc_1: (81.81%) (24190/29568)\n",
      "Epoch: 182 | Batch_idx: 240 |  Loss_1: (0.4867) | Acc_1: (81.82%) (25241/30848)\n",
      "Epoch: 182 | Batch_idx: 250 |  Loss_1: (0.4876) | Acc_1: (81.79%) (26278/32128)\n",
      "Epoch: 182 | Batch_idx: 260 |  Loss_1: (0.4864) | Acc_1: (81.84%) (27341/33408)\n",
      "Epoch: 182 | Batch_idx: 270 |  Loss_1: (0.4863) | Acc_1: (81.85%) (28392/34688)\n",
      "Epoch: 182 | Batch_idx: 280 |  Loss_1: (0.4857) | Acc_1: (81.90%) (29458/35968)\n",
      "Epoch: 182 | Batch_idx: 290 |  Loss_1: (0.4874) | Acc_1: (81.86%) (30490/37248)\n",
      "Epoch: 182 | Batch_idx: 300 |  Loss_1: (0.4862) | Acc_1: (81.89%) (31550/38528)\n",
      "Epoch: 182 | Batch_idx: 310 |  Loss_1: (0.4862) | Acc_1: (81.88%) (32596/39808)\n",
      "Epoch: 182 | Batch_idx: 320 |  Loss_1: (0.4864) | Acc_1: (81.88%) (33641/41088)\n",
      "Epoch: 182 | Batch_idx: 330 |  Loss_1: (0.4860) | Acc_1: (81.89%) (34695/42368)\n",
      "Epoch: 182 | Batch_idx: 340 |  Loss_1: (0.4853) | Acc_1: (81.93%) (35760/43648)\n",
      "Epoch: 182 | Batch_idx: 350 |  Loss_1: (0.4852) | Acc_1: (81.92%) (36803/44928)\n",
      "Epoch: 182 | Batch_idx: 360 |  Loss_1: (0.4854) | Acc_1: (81.91%) (37851/46208)\n",
      "Epoch: 182 | Batch_idx: 370 |  Loss_1: (0.4852) | Acc_1: (81.92%) (38900/47488)\n",
      "Epoch: 182 | Batch_idx: 380 |  Loss_1: (0.4854) | Acc_1: (81.90%) (39943/48768)\n",
      "Epoch: 182 | Batch_idx: 390 |  Loss_1: (0.4843) | Acc_1: (81.94%) (40972/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4816) | Acc: (90.73%) (9073/10000)\n",
      "Epoch: 183 | Batch_idx: 0 |  Loss_1: (0.4299) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 183 | Batch_idx: 10 |  Loss_1: (0.4711) | Acc_1: (82.46%) (1161/1408)\n",
      "Epoch: 183 | Batch_idx: 20 |  Loss_1: (0.4801) | Acc_1: (81.99%) (2204/2688)\n",
      "Epoch: 183 | Batch_idx: 30 |  Loss_1: (0.4830) | Acc_1: (81.91%) (3250/3968)\n",
      "Epoch: 183 | Batch_idx: 40 |  Loss_1: (0.4981) | Acc_1: (81.33%) (4268/5248)\n",
      "Epoch: 183 | Batch_idx: 50 |  Loss_1: (0.4949) | Acc_1: (81.48%) (5319/6528)\n",
      "Epoch: 183 | Batch_idx: 60 |  Loss_1: (0.5033) | Acc_1: (81.22%) (6342/7808)\n",
      "Epoch: 183 | Batch_idx: 70 |  Loss_1: (0.5044) | Acc_1: (81.17%) (7377/9088)\n",
      "Epoch: 183 | Batch_idx: 80 |  Loss_1: (0.4994) | Acc_1: (81.39%) (8438/10368)\n",
      "Epoch: 183 | Batch_idx: 90 |  Loss_1: (0.4960) | Acc_1: (81.48%) (9491/11648)\n",
      "Epoch: 183 | Batch_idx: 100 |  Loss_1: (0.4941) | Acc_1: (81.59%) (10548/12928)\n",
      "Epoch: 183 | Batch_idx: 110 |  Loss_1: (0.4949) | Acc_1: (81.60%) (11594/14208)\n",
      "Epoch: 183 | Batch_idx: 120 |  Loss_1: (0.4915) | Acc_1: (81.71%) (12655/15488)\n",
      "Epoch: 183 | Batch_idx: 130 |  Loss_1: (0.4903) | Acc_1: (81.77%) (13712/16768)\n",
      "Epoch: 183 | Batch_idx: 140 |  Loss_1: (0.4866) | Acc_1: (81.88%) (14778/18048)\n",
      "Epoch: 183 | Batch_idx: 150 |  Loss_1: (0.4889) | Acc_1: (81.81%) (15812/19328)\n",
      "Epoch: 183 | Batch_idx: 160 |  Loss_1: (0.4890) | Acc_1: (81.81%) (16860/20608)\n",
      "Epoch: 183 | Batch_idx: 170 |  Loss_1: (0.4887) | Acc_1: (81.82%) (17908/21888)\n",
      "Epoch: 183 | Batch_idx: 180 |  Loss_1: (0.4871) | Acc_1: (81.88%) (18969/23168)\n",
      "Epoch: 183 | Batch_idx: 190 |  Loss_1: (0.4879) | Acc_1: (81.84%) (20009/24448)\n",
      "Epoch: 183 | Batch_idx: 200 |  Loss_1: (0.4872) | Acc_1: (81.86%) (21061/25728)\n",
      "Epoch: 183 | Batch_idx: 210 |  Loss_1: (0.4867) | Acc_1: (81.85%) (22107/27008)\n",
      "Epoch: 183 | Batch_idx: 220 |  Loss_1: (0.4842) | Acc_1: (81.94%) (23180/28288)\n",
      "Epoch: 183 | Batch_idx: 230 |  Loss_1: (0.4833) | Acc_1: (81.96%) (24235/29568)\n",
      "Epoch: 183 | Batch_idx: 240 |  Loss_1: (0.4847) | Acc_1: (81.92%) (25272/30848)\n",
      "Epoch: 183 | Batch_idx: 250 |  Loss_1: (0.4850) | Acc_1: (81.92%) (26319/32128)\n",
      "Epoch: 183 | Batch_idx: 260 |  Loss_1: (0.4847) | Acc_1: (81.92%) (27368/33408)\n",
      "Epoch: 183 | Batch_idx: 270 |  Loss_1: (0.4864) | Acc_1: (81.86%) (28395/34688)\n",
      "Epoch: 183 | Batch_idx: 280 |  Loss_1: (0.4857) | Acc_1: (81.88%) (29450/35968)\n",
      "Epoch: 183 | Batch_idx: 290 |  Loss_1: (0.4856) | Acc_1: (81.86%) (30492/37248)\n",
      "Epoch: 183 | Batch_idx: 300 |  Loss_1: (0.4856) | Acc_1: (81.85%) (31536/38528)\n",
      "Epoch: 183 | Batch_idx: 310 |  Loss_1: (0.4849) | Acc_1: (81.87%) (32591/39808)\n",
      "Epoch: 183 | Batch_idx: 320 |  Loss_1: (0.4845) | Acc_1: (81.89%) (33647/41088)\n",
      "Epoch: 183 | Batch_idx: 330 |  Loss_1: (0.4834) | Acc_1: (81.94%) (34715/42368)\n",
      "Epoch: 183 | Batch_idx: 340 |  Loss_1: (0.4831) | Acc_1: (81.95%) (35769/43648)\n",
      "Epoch: 183 | Batch_idx: 350 |  Loss_1: (0.4834) | Acc_1: (81.94%) (36813/44928)\n",
      "Epoch: 183 | Batch_idx: 360 |  Loss_1: (0.4834) | Acc_1: (81.94%) (37863/46208)\n",
      "Epoch: 183 | Batch_idx: 370 |  Loss_1: (0.4842) | Acc_1: (81.91%) (38898/47488)\n",
      "Epoch: 183 | Batch_idx: 380 |  Loss_1: (0.4847) | Acc_1: (81.90%) (39942/48768)\n",
      "Epoch: 183 | Batch_idx: 390 |  Loss_1: (0.4845) | Acc_1: (81.90%) (40951/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4721) | Acc: (90.96%) (9096/10000)\n",
      "Epoch: 184 | Batch_idx: 0 |  Loss_1: (0.4704) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 184 | Batch_idx: 10 |  Loss_1: (0.4632) | Acc_1: (83.24%) (1172/1408)\n",
      "Epoch: 184 | Batch_idx: 20 |  Loss_1: (0.4714) | Acc_1: (82.81%) (2226/2688)\n",
      "Epoch: 184 | Batch_idx: 30 |  Loss_1: (0.4911) | Acc_1: (81.93%) (3251/3968)\n",
      "Epoch: 184 | Batch_idx: 40 |  Loss_1: (0.4932) | Acc_1: (81.69%) (4287/5248)\n",
      "Epoch: 184 | Batch_idx: 50 |  Loss_1: (0.4898) | Acc_1: (81.77%) (5338/6528)\n",
      "Epoch: 184 | Batch_idx: 60 |  Loss_1: (0.4893) | Acc_1: (81.80%) (6387/7808)\n",
      "Epoch: 184 | Batch_idx: 70 |  Loss_1: (0.4991) | Acc_1: (81.45%) (7402/9088)\n",
      "Epoch: 184 | Batch_idx: 80 |  Loss_1: (0.4971) | Acc_1: (81.56%) (8456/10368)\n",
      "Epoch: 184 | Batch_idx: 90 |  Loss_1: (0.4948) | Acc_1: (81.64%) (9509/11648)\n",
      "Epoch: 184 | Batch_idx: 100 |  Loss_1: (0.4885) | Acc_1: (81.83%) (10579/12928)\n",
      "Epoch: 184 | Batch_idx: 110 |  Loss_1: (0.4892) | Acc_1: (81.85%) (11629/14208)\n",
      "Epoch: 184 | Batch_idx: 120 |  Loss_1: (0.4848) | Acc_1: (81.98%) (12697/15488)\n",
      "Epoch: 184 | Batch_idx: 130 |  Loss_1: (0.4850) | Acc_1: (81.95%) (13741/16768)\n",
      "Epoch: 184 | Batch_idx: 140 |  Loss_1: (0.4878) | Acc_1: (81.84%) (14771/18048)\n",
      "Epoch: 184 | Batch_idx: 150 |  Loss_1: (0.4862) | Acc_1: (81.88%) (15826/19328)\n",
      "Epoch: 184 | Batch_idx: 160 |  Loss_1: (0.4874) | Acc_1: (81.82%) (16861/20608)\n",
      "Epoch: 184 | Batch_idx: 170 |  Loss_1: (0.4874) | Acc_1: (81.83%) (17910/21888)\n",
      "Epoch: 184 | Batch_idx: 180 |  Loss_1: (0.4854) | Acc_1: (81.92%) (18979/23168)\n",
      "Epoch: 184 | Batch_idx: 190 |  Loss_1: (0.4859) | Acc_1: (81.90%) (20022/24448)\n",
      "Epoch: 184 | Batch_idx: 200 |  Loss_1: (0.4869) | Acc_1: (81.87%) (21063/25728)\n",
      "Epoch: 184 | Batch_idx: 210 |  Loss_1: (0.4878) | Acc_1: (81.83%) (22101/27008)\n",
      "Epoch: 184 | Batch_idx: 220 |  Loss_1: (0.4873) | Acc_1: (81.84%) (23150/28288)\n",
      "Epoch: 184 | Batch_idx: 230 |  Loss_1: (0.4854) | Acc_1: (81.92%) (24223/29568)\n",
      "Epoch: 184 | Batch_idx: 240 |  Loss_1: (0.4842) | Acc_1: (81.98%) (25288/30848)\n",
      "Epoch: 184 | Batch_idx: 250 |  Loss_1: (0.4838) | Acc_1: (82.01%) (26347/32128)\n",
      "Epoch: 184 | Batch_idx: 260 |  Loss_1: (0.4817) | Acc_1: (82.08%) (27421/33408)\n",
      "Epoch: 184 | Batch_idx: 270 |  Loss_1: (0.4825) | Acc_1: (82.05%) (28463/34688)\n",
      "Epoch: 184 | Batch_idx: 280 |  Loss_1: (0.4818) | Acc_1: (82.06%) (29516/35968)\n",
      "Epoch: 184 | Batch_idx: 290 |  Loss_1: (0.4812) | Acc_1: (82.09%) (30577/37248)\n",
      "Epoch: 184 | Batch_idx: 300 |  Loss_1: (0.4808) | Acc_1: (82.09%) (31629/38528)\n",
      "Epoch: 184 | Batch_idx: 310 |  Loss_1: (0.4812) | Acc_1: (82.07%) (32669/39808)\n",
      "Epoch: 184 | Batch_idx: 320 |  Loss_1: (0.4811) | Acc_1: (82.05%) (33712/41088)\n",
      "Epoch: 184 | Batch_idx: 330 |  Loss_1: (0.4785) | Acc_1: (82.15%) (34807/42368)\n",
      "Epoch: 184 | Batch_idx: 340 |  Loss_1: (0.4794) | Acc_1: (82.12%) (35845/43648)\n",
      "Epoch: 184 | Batch_idx: 350 |  Loss_1: (0.4786) | Acc_1: (82.15%) (36907/44928)\n",
      "Epoch: 184 | Batch_idx: 360 |  Loss_1: (0.4788) | Acc_1: (82.13%) (37952/46208)\n",
      "Epoch: 184 | Batch_idx: 370 |  Loss_1: (0.4786) | Acc_1: (82.15%) (39010/47488)\n",
      "Epoch: 184 | Batch_idx: 380 |  Loss_1: (0.4795) | Acc_1: (82.12%) (40046/48768)\n",
      "Epoch: 184 | Batch_idx: 390 |  Loss_1: (0.4794) | Acc_1: (82.11%) (41055/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4852) | Acc: (90.56%) (9056/10000)\n",
      "Epoch: 185 | Batch_idx: 0 |  Loss_1: (0.4962) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 185 | Batch_idx: 10 |  Loss_1: (0.4735) | Acc_1: (82.46%) (1161/1408)\n",
      "Epoch: 185 | Batch_idx: 20 |  Loss_1: (0.4826) | Acc_1: (82.03%) (2205/2688)\n",
      "Epoch: 185 | Batch_idx: 30 |  Loss_1: (0.4862) | Acc_1: (81.85%) (3248/3968)\n",
      "Epoch: 185 | Batch_idx: 40 |  Loss_1: (0.4820) | Acc_1: (82.07%) (4307/5248)\n",
      "Epoch: 185 | Batch_idx: 50 |  Loss_1: (0.4804) | Acc_1: (82.14%) (5362/6528)\n",
      "Epoch: 185 | Batch_idx: 60 |  Loss_1: (0.4766) | Acc_1: (82.22%) (6420/7808)\n",
      "Epoch: 185 | Batch_idx: 70 |  Loss_1: (0.4754) | Acc_1: (82.21%) (7471/9088)\n",
      "Epoch: 185 | Batch_idx: 80 |  Loss_1: (0.4777) | Acc_1: (82.10%) (8512/10368)\n",
      "Epoch: 185 | Batch_idx: 90 |  Loss_1: (0.4730) | Acc_1: (82.23%) (9578/11648)\n",
      "Epoch: 185 | Batch_idx: 100 |  Loss_1: (0.4728) | Acc_1: (82.26%) (10635/12928)\n",
      "Epoch: 185 | Batch_idx: 110 |  Loss_1: (0.4771) | Acc_1: (82.14%) (11671/14208)\n",
      "Epoch: 185 | Batch_idx: 120 |  Loss_1: (0.4770) | Acc_1: (82.18%) (12728/15488)\n",
      "Epoch: 185 | Batch_idx: 130 |  Loss_1: (0.4801) | Acc_1: (82.03%) (13755/16768)\n",
      "Epoch: 185 | Batch_idx: 140 |  Loss_1: (0.4808) | Acc_1: (81.99%) (14798/18048)\n",
      "Epoch: 185 | Batch_idx: 150 |  Loss_1: (0.4808) | Acc_1: (82.00%) (15849/19328)\n",
      "Epoch: 185 | Batch_idx: 160 |  Loss_1: (0.4816) | Acc_1: (81.99%) (16896/20608)\n",
      "Epoch: 185 | Batch_idx: 170 |  Loss_1: (0.4845) | Acc_1: (81.88%) (17921/21888)\n",
      "Epoch: 185 | Batch_idx: 180 |  Loss_1: (0.4845) | Acc_1: (81.86%) (18966/23168)\n",
      "Epoch: 185 | Batch_idx: 190 |  Loss_1: (0.4856) | Acc_1: (81.84%) (20009/24448)\n",
      "Epoch: 185 | Batch_idx: 200 |  Loss_1: (0.4878) | Acc_1: (81.76%) (21034/25728)\n",
      "Epoch: 185 | Batch_idx: 210 |  Loss_1: (0.4864) | Acc_1: (81.82%) (22098/27008)\n",
      "Epoch: 185 | Batch_idx: 220 |  Loss_1: (0.4840) | Acc_1: (81.91%) (23170/28288)\n",
      "Epoch: 185 | Batch_idx: 230 |  Loss_1: (0.4838) | Acc_1: (81.93%) (24224/29568)\n",
      "Epoch: 185 | Batch_idx: 240 |  Loss_1: (0.4832) | Acc_1: (81.96%) (25283/30848)\n",
      "Epoch: 185 | Batch_idx: 250 |  Loss_1: (0.4827) | Acc_1: (81.97%) (26335/32128)\n",
      "Epoch: 185 | Batch_idx: 260 |  Loss_1: (0.4810) | Acc_1: (82.02%) (27401/33408)\n",
      "Epoch: 185 | Batch_idx: 270 |  Loss_1: (0.4813) | Acc_1: (82.02%) (28450/34688)\n",
      "Epoch: 185 | Batch_idx: 280 |  Loss_1: (0.4817) | Acc_1: (82.00%) (29494/35968)\n",
      "Epoch: 185 | Batch_idx: 290 |  Loss_1: (0.4794) | Acc_1: (82.10%) (30581/37248)\n",
      "Epoch: 185 | Batch_idx: 300 |  Loss_1: (0.4806) | Acc_1: (82.05%) (31614/38528)\n",
      "Epoch: 185 | Batch_idx: 310 |  Loss_1: (0.4788) | Acc_1: (82.11%) (32687/39808)\n",
      "Epoch: 185 | Batch_idx: 320 |  Loss_1: (0.4789) | Acc_1: (82.10%) (33733/41088)\n",
      "Epoch: 185 | Batch_idx: 330 |  Loss_1: (0.4797) | Acc_1: (82.06%) (34768/42368)\n",
      "Epoch: 185 | Batch_idx: 340 |  Loss_1: (0.4803) | Acc_1: (82.04%) (35808/43648)\n",
      "Epoch: 185 | Batch_idx: 350 |  Loss_1: (0.4804) | Acc_1: (82.03%) (36855/44928)\n",
      "Epoch: 185 | Batch_idx: 360 |  Loss_1: (0.4810) | Acc_1: (82.00%) (37891/46208)\n",
      "Epoch: 185 | Batch_idx: 370 |  Loss_1: (0.4815) | Acc_1: (81.99%) (38936/47488)\n",
      "Epoch: 185 | Batch_idx: 380 |  Loss_1: (0.4812) | Acc_1: (81.99%) (39987/48768)\n",
      "Epoch: 185 | Batch_idx: 390 |  Loss_1: (0.4815) | Acc_1: (82.00%) (40998/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4975) | Acc: (90.32%) (9032/10000)\n",
      "Epoch: 186 | Batch_idx: 0 |  Loss_1: (0.4475) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 186 | Batch_idx: 10 |  Loss_1: (0.4754) | Acc_1: (81.96%) (1154/1408)\n",
      "Epoch: 186 | Batch_idx: 20 |  Loss_1: (0.5175) | Acc_1: (80.69%) (2169/2688)\n",
      "Epoch: 186 | Batch_idx: 30 |  Loss_1: (0.5185) | Acc_1: (80.57%) (3197/3968)\n",
      "Epoch: 186 | Batch_idx: 40 |  Loss_1: (0.5161) | Acc_1: (80.66%) (4233/5248)\n",
      "Epoch: 186 | Batch_idx: 50 |  Loss_1: (0.5084) | Acc_1: (81.08%) (5293/6528)\n",
      "Epoch: 186 | Batch_idx: 60 |  Loss_1: (0.5098) | Acc_1: (80.97%) (6322/7808)\n",
      "Epoch: 186 | Batch_idx: 70 |  Loss_1: (0.5020) | Acc_1: (81.26%) (7385/9088)\n",
      "Epoch: 186 | Batch_idx: 80 |  Loss_1: (0.4961) | Acc_1: (81.49%) (8449/10368)\n",
      "Epoch: 186 | Batch_idx: 90 |  Loss_1: (0.4900) | Acc_1: (81.75%) (9522/11648)\n",
      "Epoch: 186 | Batch_idx: 100 |  Loss_1: (0.4881) | Acc_1: (81.79%) (10574/12928)\n",
      "Epoch: 186 | Batch_idx: 110 |  Loss_1: (0.4835) | Acc_1: (81.96%) (11645/14208)\n",
      "Epoch: 186 | Batch_idx: 120 |  Loss_1: (0.4816) | Acc_1: (82.04%) (12706/15488)\n",
      "Epoch: 186 | Batch_idx: 130 |  Loss_1: (0.4770) | Acc_1: (82.20%) (13783/16768)\n",
      "Epoch: 186 | Batch_idx: 140 |  Loss_1: (0.4781) | Acc_1: (82.18%) (14831/18048)\n",
      "Epoch: 186 | Batch_idx: 150 |  Loss_1: (0.4822) | Acc_1: (82.04%) (15857/19328)\n",
      "Epoch: 186 | Batch_idx: 160 |  Loss_1: (0.4825) | Acc_1: (82.04%) (16907/20608)\n",
      "Epoch: 186 | Batch_idx: 170 |  Loss_1: (0.4808) | Acc_1: (82.10%) (17970/21888)\n",
      "Epoch: 186 | Batch_idx: 180 |  Loss_1: (0.4816) | Acc_1: (82.05%) (19010/23168)\n",
      "Epoch: 186 | Batch_idx: 190 |  Loss_1: (0.4830) | Acc_1: (82.01%) (20049/24448)\n",
      "Epoch: 186 | Batch_idx: 200 |  Loss_1: (0.4821) | Acc_1: (82.06%) (21112/25728)\n",
      "Epoch: 186 | Batch_idx: 210 |  Loss_1: (0.4806) | Acc_1: (82.11%) (22175/27008)\n",
      "Epoch: 186 | Batch_idx: 220 |  Loss_1: (0.4853) | Acc_1: (81.95%) (23181/28288)\n",
      "Epoch: 186 | Batch_idx: 230 |  Loss_1: (0.4837) | Acc_1: (82.00%) (24246/29568)\n",
      "Epoch: 186 | Batch_idx: 240 |  Loss_1: (0.4828) | Acc_1: (82.04%) (25308/30848)\n",
      "Epoch: 186 | Batch_idx: 250 |  Loss_1: (0.4817) | Acc_1: (82.08%) (26370/32128)\n",
      "Epoch: 186 | Batch_idx: 260 |  Loss_1: (0.4829) | Acc_1: (82.01%) (27399/33408)\n",
      "Epoch: 186 | Batch_idx: 270 |  Loss_1: (0.4837) | Acc_1: (81.96%) (28431/34688)\n",
      "Epoch: 186 | Batch_idx: 280 |  Loss_1: (0.4830) | Acc_1: (81.98%) (29488/35968)\n",
      "Epoch: 186 | Batch_idx: 290 |  Loss_1: (0.4829) | Acc_1: (81.98%) (30537/37248)\n",
      "Epoch: 186 | Batch_idx: 300 |  Loss_1: (0.4831) | Acc_1: (81.98%) (31586/38528)\n",
      "Epoch: 186 | Batch_idx: 310 |  Loss_1: (0.4822) | Acc_1: (82.01%) (32647/39808)\n",
      "Epoch: 186 | Batch_idx: 320 |  Loss_1: (0.4833) | Acc_1: (81.98%) (33684/41088)\n",
      "Epoch: 186 | Batch_idx: 330 |  Loss_1: (0.4832) | Acc_1: (81.98%) (34735/42368)\n",
      "Epoch: 186 | Batch_idx: 340 |  Loss_1: (0.4831) | Acc_1: (81.99%) (35789/43648)\n",
      "Epoch: 186 | Batch_idx: 350 |  Loss_1: (0.4829) | Acc_1: (82.01%) (36845/44928)\n",
      "Epoch: 186 | Batch_idx: 360 |  Loss_1: (0.4824) | Acc_1: (82.04%) (37908/46208)\n",
      "Epoch: 186 | Batch_idx: 370 |  Loss_1: (0.4826) | Acc_1: (82.04%) (38958/47488)\n",
      "Epoch: 186 | Batch_idx: 380 |  Loss_1: (0.4822) | Acc_1: (82.05%) (40013/48768)\n",
      "Epoch: 186 | Batch_idx: 390 |  Loss_1: (0.4824) | Acc_1: (82.04%) (41021/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4779) | Acc: (90.93%) (9093/10000)\n",
      "Epoch: 187 | Batch_idx: 0 |  Loss_1: (0.5175) | Acc_1: (81.25%) (104/128)\n",
      "Epoch: 187 | Batch_idx: 10 |  Loss_1: (0.5309) | Acc_1: (80.54%) (1134/1408)\n",
      "Epoch: 187 | Batch_idx: 20 |  Loss_1: (0.5325) | Acc_1: (80.39%) (2161/2688)\n",
      "Epoch: 187 | Batch_idx: 30 |  Loss_1: (0.5136) | Acc_1: (80.92%) (3211/3968)\n",
      "Epoch: 187 | Batch_idx: 40 |  Loss_1: (0.4991) | Acc_1: (81.38%) (4271/5248)\n",
      "Epoch: 187 | Batch_idx: 50 |  Loss_1: (0.4927) | Acc_1: (81.62%) (5328/6528)\n",
      "Epoch: 187 | Batch_idx: 60 |  Loss_1: (0.4884) | Acc_1: (81.81%) (6388/7808)\n",
      "Epoch: 187 | Batch_idx: 70 |  Loss_1: (0.4856) | Acc_1: (81.91%) (7444/9088)\n",
      "Epoch: 187 | Batch_idx: 80 |  Loss_1: (0.4934) | Acc_1: (81.65%) (8465/10368)\n",
      "Epoch: 187 | Batch_idx: 90 |  Loss_1: (0.4926) | Acc_1: (81.67%) (9513/11648)\n",
      "Epoch: 187 | Batch_idx: 100 |  Loss_1: (0.4885) | Acc_1: (81.83%) (10579/12928)\n",
      "Epoch: 187 | Batch_idx: 110 |  Loss_1: (0.4851) | Acc_1: (81.97%) (11647/14208)\n",
      "Epoch: 187 | Batch_idx: 120 |  Loss_1: (0.4872) | Acc_1: (81.86%) (12679/15488)\n",
      "Epoch: 187 | Batch_idx: 130 |  Loss_1: (0.4855) | Acc_1: (81.92%) (13737/16768)\n",
      "Epoch: 187 | Batch_idx: 140 |  Loss_1: (0.4861) | Acc_1: (81.93%) (14787/18048)\n",
      "Epoch: 187 | Batch_idx: 150 |  Loss_1: (0.4857) | Acc_1: (81.94%) (15838/19328)\n",
      "Epoch: 187 | Batch_idx: 160 |  Loss_1: (0.4840) | Acc_1: (82.02%) (16903/20608)\n",
      "Epoch: 187 | Batch_idx: 170 |  Loss_1: (0.4856) | Acc_1: (81.96%) (17940/21888)\n",
      "Epoch: 187 | Batch_idx: 180 |  Loss_1: (0.4861) | Acc_1: (81.94%) (18984/23168)\n",
      "Epoch: 187 | Batch_idx: 190 |  Loss_1: (0.4835) | Acc_1: (82.04%) (20056/24448)\n",
      "Epoch: 187 | Batch_idx: 200 |  Loss_1: (0.4815) | Acc_1: (82.10%) (21122/25728)\n",
      "Epoch: 187 | Batch_idx: 210 |  Loss_1: (0.4803) | Acc_1: (82.13%) (22182/27008)\n",
      "Epoch: 187 | Batch_idx: 220 |  Loss_1: (0.4788) | Acc_1: (82.19%) (23251/28288)\n",
      "Epoch: 187 | Batch_idx: 230 |  Loss_1: (0.4796) | Acc_1: (82.17%) (24297/29568)\n",
      "Epoch: 187 | Batch_idx: 240 |  Loss_1: (0.4795) | Acc_1: (82.16%) (25345/30848)\n",
      "Epoch: 187 | Batch_idx: 250 |  Loss_1: (0.4778) | Acc_1: (82.21%) (26413/32128)\n",
      "Epoch: 187 | Batch_idx: 260 |  Loss_1: (0.4773) | Acc_1: (82.23%) (27471/33408)\n",
      "Epoch: 187 | Batch_idx: 270 |  Loss_1: (0.4772) | Acc_1: (82.22%) (28522/34688)\n",
      "Epoch: 187 | Batch_idx: 280 |  Loss_1: (0.4775) | Acc_1: (82.20%) (29567/35968)\n",
      "Epoch: 187 | Batch_idx: 290 |  Loss_1: (0.4776) | Acc_1: (82.20%) (30617/37248)\n",
      "Epoch: 187 | Batch_idx: 300 |  Loss_1: (0.4784) | Acc_1: (82.17%) (31658/38528)\n",
      "Epoch: 187 | Batch_idx: 310 |  Loss_1: (0.4783) | Acc_1: (82.18%) (32714/39808)\n",
      "Epoch: 187 | Batch_idx: 320 |  Loss_1: (0.4777) | Acc_1: (82.20%) (33774/41088)\n",
      "Epoch: 187 | Batch_idx: 330 |  Loss_1: (0.4783) | Acc_1: (82.19%) (34822/42368)\n",
      "Epoch: 187 | Batch_idx: 340 |  Loss_1: (0.4796) | Acc_1: (82.14%) (35852/43648)\n",
      "Epoch: 187 | Batch_idx: 350 |  Loss_1: (0.4809) | Acc_1: (82.09%) (36883/44928)\n",
      "Epoch: 187 | Batch_idx: 360 |  Loss_1: (0.4821) | Acc_1: (82.06%) (37917/46208)\n",
      "Epoch: 187 | Batch_idx: 370 |  Loss_1: (0.4818) | Acc_1: (82.06%) (38970/47488)\n",
      "Epoch: 187 | Batch_idx: 380 |  Loss_1: (0.4820) | Acc_1: (82.05%) (40016/48768)\n",
      "Epoch: 187 | Batch_idx: 390 |  Loss_1: (0.4824) | Acc_1: (82.04%) (41018/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4681) | Acc: (90.84%) (9084/10000)\n",
      "Epoch: 188 | Batch_idx: 0 |  Loss_1: (0.5468) | Acc_1: (78.91%) (101/128)\n",
      "Epoch: 188 | Batch_idx: 10 |  Loss_1: (0.4704) | Acc_1: (82.24%) (1158/1408)\n",
      "Epoch: 188 | Batch_idx: 20 |  Loss_1: (0.4811) | Acc_1: (81.92%) (2202/2688)\n",
      "Epoch: 188 | Batch_idx: 30 |  Loss_1: (0.4807) | Acc_1: (81.93%) (3251/3968)\n",
      "Epoch: 188 | Batch_idx: 40 |  Loss_1: (0.4767) | Acc_1: (82.20%) (4314/5248)\n",
      "Epoch: 188 | Batch_idx: 50 |  Loss_1: (0.4772) | Acc_1: (82.23%) (5368/6528)\n",
      "Epoch: 188 | Batch_idx: 60 |  Loss_1: (0.4798) | Acc_1: (82.12%) (6412/7808)\n",
      "Epoch: 188 | Batch_idx: 70 |  Loss_1: (0.4830) | Acc_1: (81.94%) (7447/9088)\n",
      "Epoch: 188 | Batch_idx: 80 |  Loss_1: (0.4882) | Acc_1: (81.72%) (8473/10368)\n",
      "Epoch: 188 | Batch_idx: 90 |  Loss_1: (0.4868) | Acc_1: (81.72%) (9519/11648)\n",
      "Epoch: 188 | Batch_idx: 100 |  Loss_1: (0.4856) | Acc_1: (81.81%) (10576/12928)\n",
      "Epoch: 188 | Batch_idx: 110 |  Loss_1: (0.4858) | Acc_1: (81.83%) (11627/14208)\n",
      "Epoch: 188 | Batch_idx: 120 |  Loss_1: (0.4851) | Acc_1: (81.86%) (12678/15488)\n",
      "Epoch: 188 | Batch_idx: 130 |  Loss_1: (0.4849) | Acc_1: (81.85%) (13724/16768)\n",
      "Epoch: 188 | Batch_idx: 140 |  Loss_1: (0.4856) | Acc_1: (81.83%) (14769/18048)\n",
      "Epoch: 188 | Batch_idx: 150 |  Loss_1: (0.4847) | Acc_1: (81.87%) (15823/19328)\n",
      "Epoch: 188 | Batch_idx: 160 |  Loss_1: (0.4817) | Acc_1: (81.95%) (16889/20608)\n",
      "Epoch: 188 | Batch_idx: 170 |  Loss_1: (0.4830) | Acc_1: (81.89%) (17925/21888)\n",
      "Epoch: 188 | Batch_idx: 180 |  Loss_1: (0.4821) | Acc_1: (81.93%) (18981/23168)\n",
      "Epoch: 188 | Batch_idx: 190 |  Loss_1: (0.4828) | Acc_1: (81.89%) (20021/24448)\n",
      "Epoch: 188 | Batch_idx: 200 |  Loss_1: (0.4818) | Acc_1: (81.95%) (21085/25728)\n",
      "Epoch: 188 | Batch_idx: 210 |  Loss_1: (0.4839) | Acc_1: (81.87%) (22111/27008)\n",
      "Epoch: 188 | Batch_idx: 220 |  Loss_1: (0.4850) | Acc_1: (81.84%) (23152/28288)\n",
      "Epoch: 188 | Batch_idx: 230 |  Loss_1: (0.4837) | Acc_1: (81.89%) (24213/29568)\n",
      "Epoch: 188 | Batch_idx: 240 |  Loss_1: (0.4836) | Acc_1: (81.90%) (25264/30848)\n",
      "Epoch: 188 | Batch_idx: 250 |  Loss_1: (0.4852) | Acc_1: (81.86%) (26299/32128)\n",
      "Epoch: 188 | Batch_idx: 260 |  Loss_1: (0.4863) | Acc_1: (81.80%) (27328/33408)\n",
      "Epoch: 188 | Batch_idx: 270 |  Loss_1: (0.4873) | Acc_1: (81.76%) (28361/34688)\n",
      "Epoch: 188 | Batch_idx: 280 |  Loss_1: (0.4863) | Acc_1: (81.81%) (29424/35968)\n",
      "Epoch: 188 | Batch_idx: 290 |  Loss_1: (0.4873) | Acc_1: (81.78%) (30463/37248)\n",
      "Epoch: 188 | Batch_idx: 300 |  Loss_1: (0.4884) | Acc_1: (81.74%) (31493/38528)\n",
      "Epoch: 188 | Batch_idx: 310 |  Loss_1: (0.4885) | Acc_1: (81.74%) (32538/39808)\n",
      "Epoch: 188 | Batch_idx: 320 |  Loss_1: (0.4896) | Acc_1: (81.70%) (33570/41088)\n",
      "Epoch: 188 | Batch_idx: 330 |  Loss_1: (0.4895) | Acc_1: (81.69%) (34610/42368)\n",
      "Epoch: 188 | Batch_idx: 340 |  Loss_1: (0.4900) | Acc_1: (81.68%) (35650/43648)\n",
      "Epoch: 188 | Batch_idx: 350 |  Loss_1: (0.4885) | Acc_1: (81.73%) (36719/44928)\n",
      "Epoch: 188 | Batch_idx: 360 |  Loss_1: (0.4872) | Acc_1: (81.78%) (37788/46208)\n",
      "Epoch: 188 | Batch_idx: 370 |  Loss_1: (0.4875) | Acc_1: (81.77%) (38831/47488)\n",
      "Epoch: 188 | Batch_idx: 380 |  Loss_1: (0.4876) | Acc_1: (81.79%) (39887/48768)\n",
      "Epoch: 188 | Batch_idx: 390 |  Loss_1: (0.4878) | Acc_1: (81.77%) (40886/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4726) | Acc: (90.85%) (9085/10000)\n",
      "Epoch: 189 | Batch_idx: 0 |  Loss_1: (0.4558) | Acc_1: (82.81%) (106/128)\n",
      "Epoch: 189 | Batch_idx: 10 |  Loss_1: (0.5353) | Acc_1: (79.90%) (1125/1408)\n",
      "Epoch: 189 | Batch_idx: 20 |  Loss_1: (0.5244) | Acc_1: (80.36%) (2160/2688)\n",
      "Epoch: 189 | Batch_idx: 30 |  Loss_1: (0.5108) | Acc_1: (81.00%) (3214/3968)\n",
      "Epoch: 189 | Batch_idx: 40 |  Loss_1: (0.5031) | Acc_1: (81.25%) (4264/5248)\n",
      "Epoch: 189 | Batch_idx: 50 |  Loss_1: (0.4942) | Acc_1: (81.48%) (5319/6528)\n",
      "Epoch: 189 | Batch_idx: 60 |  Loss_1: (0.4890) | Acc_1: (81.74%) (6382/7808)\n",
      "Epoch: 189 | Batch_idx: 70 |  Loss_1: (0.4846) | Acc_1: (81.88%) (7441/9088)\n",
      "Epoch: 189 | Batch_idx: 80 |  Loss_1: (0.4803) | Acc_1: (82.01%) (8503/10368)\n",
      "Epoch: 189 | Batch_idx: 90 |  Loss_1: (0.4847) | Acc_1: (81.89%) (9538/11648)\n",
      "Epoch: 189 | Batch_idx: 100 |  Loss_1: (0.4831) | Acc_1: (81.91%) (10589/12928)\n",
      "Epoch: 189 | Batch_idx: 110 |  Loss_1: (0.4827) | Acc_1: (81.94%) (11642/14208)\n",
      "Epoch: 189 | Batch_idx: 120 |  Loss_1: (0.4842) | Acc_1: (81.91%) (12686/15488)\n",
      "Epoch: 189 | Batch_idx: 130 |  Loss_1: (0.4830) | Acc_1: (81.92%) (13737/16768)\n",
      "Epoch: 189 | Batch_idx: 140 |  Loss_1: (0.4830) | Acc_1: (81.92%) (14785/18048)\n",
      "Epoch: 189 | Batch_idx: 150 |  Loss_1: (0.4821) | Acc_1: (81.96%) (15841/19328)\n",
      "Epoch: 189 | Batch_idx: 160 |  Loss_1: (0.4828) | Acc_1: (81.93%) (16884/20608)\n",
      "Epoch: 189 | Batch_idx: 170 |  Loss_1: (0.4845) | Acc_1: (81.88%) (17921/21888)\n",
      "Epoch: 189 | Batch_idx: 180 |  Loss_1: (0.4835) | Acc_1: (81.91%) (18976/23168)\n",
      "Epoch: 189 | Batch_idx: 190 |  Loss_1: (0.4843) | Acc_1: (81.88%) (20018/24448)\n",
      "Epoch: 189 | Batch_idx: 200 |  Loss_1: (0.4836) | Acc_1: (81.91%) (21074/25728)\n",
      "Epoch: 189 | Batch_idx: 210 |  Loss_1: (0.4829) | Acc_1: (81.95%) (22133/27008)\n",
      "Epoch: 189 | Batch_idx: 220 |  Loss_1: (0.4837) | Acc_1: (81.94%) (23179/28288)\n",
      "Epoch: 189 | Batch_idx: 230 |  Loss_1: (0.4855) | Acc_1: (81.88%) (24209/29568)\n",
      "Epoch: 189 | Batch_idx: 240 |  Loss_1: (0.4859) | Acc_1: (81.84%) (25247/30848)\n",
      "Epoch: 189 | Batch_idx: 250 |  Loss_1: (0.4863) | Acc_1: (81.82%) (26286/32128)\n",
      "Epoch: 189 | Batch_idx: 260 |  Loss_1: (0.4864) | Acc_1: (81.81%) (27332/33408)\n",
      "Epoch: 189 | Batch_idx: 270 |  Loss_1: (0.4863) | Acc_1: (81.82%) (28382/34688)\n",
      "Epoch: 189 | Batch_idx: 280 |  Loss_1: (0.4866) | Acc_1: (81.81%) (29424/35968)\n",
      "Epoch: 189 | Batch_idx: 290 |  Loss_1: (0.4865) | Acc_1: (81.81%) (30472/37248)\n",
      "Epoch: 189 | Batch_idx: 300 |  Loss_1: (0.4865) | Acc_1: (81.81%) (31521/38528)\n",
      "Epoch: 189 | Batch_idx: 310 |  Loss_1: (0.4859) | Acc_1: (81.84%) (32577/39808)\n",
      "Epoch: 189 | Batch_idx: 320 |  Loss_1: (0.4843) | Acc_1: (81.90%) (33650/41088)\n",
      "Epoch: 189 | Batch_idx: 330 |  Loss_1: (0.4843) | Acc_1: (81.90%) (34700/42368)\n",
      "Epoch: 189 | Batch_idx: 340 |  Loss_1: (0.4853) | Acc_1: (81.85%) (35727/43648)\n",
      "Epoch: 189 | Batch_idx: 350 |  Loss_1: (0.4848) | Acc_1: (81.88%) (36787/44928)\n",
      "Epoch: 189 | Batch_idx: 360 |  Loss_1: (0.4848) | Acc_1: (81.87%) (37829/46208)\n",
      "Epoch: 189 | Batch_idx: 370 |  Loss_1: (0.4855) | Acc_1: (81.84%) (38863/47488)\n",
      "Epoch: 189 | Batch_idx: 380 |  Loss_1: (0.4858) | Acc_1: (81.81%) (39896/48768)\n",
      "Epoch: 189 | Batch_idx: 390 |  Loss_1: (0.4864) | Acc_1: (81.77%) (40887/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4835) | Acc: (90.86%) (9086/10000)\n",
      "Epoch: 190 | Batch_idx: 0 |  Loss_1: (0.4260) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 190 | Batch_idx: 10 |  Loss_1: (0.4599) | Acc_1: (82.53%) (1162/1408)\n",
      "Epoch: 190 | Batch_idx: 20 |  Loss_1: (0.4750) | Acc_1: (82.18%) (2209/2688)\n",
      "Epoch: 190 | Batch_idx: 30 |  Loss_1: (0.4641) | Acc_1: (82.71%) (3282/3968)\n",
      "Epoch: 190 | Batch_idx: 40 |  Loss_1: (0.4700) | Acc_1: (82.34%) (4321/5248)\n",
      "Epoch: 190 | Batch_idx: 50 |  Loss_1: (0.4747) | Acc_1: (82.06%) (5357/6528)\n",
      "Epoch: 190 | Batch_idx: 60 |  Loss_1: (0.4676) | Acc_1: (82.26%) (6423/7808)\n",
      "Epoch: 190 | Batch_idx: 70 |  Loss_1: (0.4672) | Acc_1: (82.32%) (7481/9088)\n",
      "Epoch: 190 | Batch_idx: 80 |  Loss_1: (0.4695) | Acc_1: (82.31%) (8534/10368)\n",
      "Epoch: 190 | Batch_idx: 90 |  Loss_1: (0.4733) | Acc_1: (82.18%) (9572/11648)\n",
      "Epoch: 190 | Batch_idx: 100 |  Loss_1: (0.4746) | Acc_1: (82.16%) (10621/12928)\n",
      "Epoch: 190 | Batch_idx: 110 |  Loss_1: (0.4800) | Acc_1: (82.01%) (11652/14208)\n",
      "Epoch: 190 | Batch_idx: 120 |  Loss_1: (0.4791) | Acc_1: (82.02%) (12704/15488)\n",
      "Epoch: 190 | Batch_idx: 130 |  Loss_1: (0.4780) | Acc_1: (82.06%) (13760/16768)\n",
      "Epoch: 190 | Batch_idx: 140 |  Loss_1: (0.4810) | Acc_1: (81.91%) (14784/18048)\n",
      "Epoch: 190 | Batch_idx: 150 |  Loss_1: (0.4784) | Acc_1: (82.06%) (15860/19328)\n",
      "Epoch: 190 | Batch_idx: 160 |  Loss_1: (0.4794) | Acc_1: (82.04%) (16907/20608)\n",
      "Epoch: 190 | Batch_idx: 170 |  Loss_1: (0.4793) | Acc_1: (82.04%) (17956/21888)\n",
      "Epoch: 190 | Batch_idx: 180 |  Loss_1: (0.4799) | Acc_1: (82.01%) (19000/23168)\n",
      "Epoch: 190 | Batch_idx: 190 |  Loss_1: (0.4821) | Acc_1: (81.95%) (20036/24448)\n",
      "Epoch: 190 | Batch_idx: 200 |  Loss_1: (0.4817) | Acc_1: (81.96%) (21087/25728)\n",
      "Epoch: 190 | Batch_idx: 210 |  Loss_1: (0.4804) | Acc_1: (82.04%) (22158/27008)\n",
      "Epoch: 190 | Batch_idx: 220 |  Loss_1: (0.4797) | Acc_1: (82.05%) (23209/28288)\n",
      "Epoch: 190 | Batch_idx: 230 |  Loss_1: (0.4766) | Acc_1: (82.17%) (24296/29568)\n",
      "Epoch: 190 | Batch_idx: 240 |  Loss_1: (0.4776) | Acc_1: (82.14%) (25338/30848)\n",
      "Epoch: 190 | Batch_idx: 250 |  Loss_1: (0.4785) | Acc_1: (82.11%) (26381/32128)\n",
      "Epoch: 190 | Batch_idx: 260 |  Loss_1: (0.4792) | Acc_1: (82.10%) (27428/33408)\n",
      "Epoch: 190 | Batch_idx: 270 |  Loss_1: (0.4792) | Acc_1: (82.11%) (28481/34688)\n",
      "Epoch: 190 | Batch_idx: 280 |  Loss_1: (0.4798) | Acc_1: (82.08%) (29523/35968)\n",
      "Epoch: 190 | Batch_idx: 290 |  Loss_1: (0.4774) | Acc_1: (82.15%) (30600/37248)\n",
      "Epoch: 190 | Batch_idx: 300 |  Loss_1: (0.4773) | Acc_1: (82.17%) (31657/38528)\n",
      "Epoch: 190 | Batch_idx: 310 |  Loss_1: (0.4767) | Acc_1: (82.18%) (32715/39808)\n",
      "Epoch: 190 | Batch_idx: 320 |  Loss_1: (0.4780) | Acc_1: (82.13%) (33745/41088)\n",
      "Epoch: 190 | Batch_idx: 330 |  Loss_1: (0.4791) | Acc_1: (82.09%) (34779/42368)\n",
      "Epoch: 190 | Batch_idx: 340 |  Loss_1: (0.4788) | Acc_1: (82.10%) (35834/43648)\n",
      "Epoch: 190 | Batch_idx: 350 |  Loss_1: (0.4777) | Acc_1: (82.13%) (36899/44928)\n",
      "Epoch: 190 | Batch_idx: 360 |  Loss_1: (0.4783) | Acc_1: (82.10%) (37938/46208)\n",
      "Epoch: 190 | Batch_idx: 370 |  Loss_1: (0.4780) | Acc_1: (82.12%) (38999/47488)\n",
      "Epoch: 190 | Batch_idx: 380 |  Loss_1: (0.4768) | Acc_1: (82.18%) (40078/48768)\n",
      "Epoch: 190 | Batch_idx: 390 |  Loss_1: (0.4779) | Acc_1: (82.13%) (41067/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4650) | Acc: (91.07%) (9107/10000)\n",
      "Epoch: 191 | Batch_idx: 0 |  Loss_1: (0.3255) | Acc_1: (89.06%) (114/128)\n",
      "Epoch: 191 | Batch_idx: 10 |  Loss_1: (0.4633) | Acc_1: (83.31%) (1173/1408)\n",
      "Epoch: 191 | Batch_idx: 20 |  Loss_1: (0.4579) | Acc_1: (83.22%) (2237/2688)\n",
      "Epoch: 191 | Batch_idx: 30 |  Loss_1: (0.4554) | Acc_1: (83.11%) (3298/3968)\n",
      "Epoch: 191 | Batch_idx: 40 |  Loss_1: (0.4656) | Acc_1: (82.68%) (4339/5248)\n",
      "Epoch: 191 | Batch_idx: 50 |  Loss_1: (0.4651) | Acc_1: (82.51%) (5386/6528)\n",
      "Epoch: 191 | Batch_idx: 60 |  Loss_1: (0.4671) | Acc_1: (82.36%) (6431/7808)\n",
      "Epoch: 191 | Batch_idx: 70 |  Loss_1: (0.4643) | Acc_1: (82.50%) (7498/9088)\n",
      "Epoch: 191 | Batch_idx: 80 |  Loss_1: (0.4711) | Acc_1: (82.27%) (8530/10368)\n",
      "Epoch: 191 | Batch_idx: 90 |  Loss_1: (0.4741) | Acc_1: (82.15%) (9569/11648)\n",
      "Epoch: 191 | Batch_idx: 100 |  Loss_1: (0.4772) | Acc_1: (82.06%) (10609/12928)\n",
      "Epoch: 191 | Batch_idx: 110 |  Loss_1: (0.4788) | Acc_1: (82.02%) (11653/14208)\n",
      "Epoch: 191 | Batch_idx: 120 |  Loss_1: (0.4795) | Acc_1: (82.00%) (12700/15488)\n",
      "Epoch: 191 | Batch_idx: 130 |  Loss_1: (0.4802) | Acc_1: (81.97%) (13745/16768)\n",
      "Epoch: 191 | Batch_idx: 140 |  Loss_1: (0.4778) | Acc_1: (82.07%) (14812/18048)\n",
      "Epoch: 191 | Batch_idx: 150 |  Loss_1: (0.4756) | Acc_1: (82.15%) (15877/19328)\n",
      "Epoch: 191 | Batch_idx: 160 |  Loss_1: (0.4776) | Acc_1: (82.08%) (16915/20608)\n",
      "Epoch: 191 | Batch_idx: 170 |  Loss_1: (0.4773) | Acc_1: (82.09%) (17968/21888)\n",
      "Epoch: 191 | Batch_idx: 180 |  Loss_1: (0.4792) | Acc_1: (82.03%) (19005/23168)\n",
      "Epoch: 191 | Batch_idx: 190 |  Loss_1: (0.4793) | Acc_1: (82.06%) (20061/24448)\n",
      "Epoch: 191 | Batch_idx: 200 |  Loss_1: (0.4800) | Acc_1: (82.04%) (21106/25728)\n",
      "Epoch: 191 | Batch_idx: 210 |  Loss_1: (0.4798) | Acc_1: (82.04%) (22158/27008)\n",
      "Epoch: 191 | Batch_idx: 220 |  Loss_1: (0.4795) | Acc_1: (82.08%) (23218/28288)\n",
      "Epoch: 191 | Batch_idx: 230 |  Loss_1: (0.4767) | Acc_1: (82.19%) (24301/29568)\n",
      "Epoch: 191 | Batch_idx: 240 |  Loss_1: (0.4765) | Acc_1: (82.19%) (25354/30848)\n",
      "Epoch: 191 | Batch_idx: 250 |  Loss_1: (0.4767) | Acc_1: (82.17%) (26401/32128)\n",
      "Epoch: 191 | Batch_idx: 260 |  Loss_1: (0.4774) | Acc_1: (82.14%) (27441/33408)\n",
      "Epoch: 191 | Batch_idx: 270 |  Loss_1: (0.4787) | Acc_1: (82.08%) (28472/34688)\n",
      "Epoch: 191 | Batch_idx: 280 |  Loss_1: (0.4784) | Acc_1: (82.09%) (29526/35968)\n",
      "Epoch: 191 | Batch_idx: 290 |  Loss_1: (0.4786) | Acc_1: (82.09%) (30576/37248)\n",
      "Epoch: 191 | Batch_idx: 300 |  Loss_1: (0.4787) | Acc_1: (82.08%) (31624/38528)\n",
      "Epoch: 191 | Batch_idx: 310 |  Loss_1: (0.4802) | Acc_1: (82.03%) (32654/39808)\n",
      "Epoch: 191 | Batch_idx: 320 |  Loss_1: (0.4801) | Acc_1: (82.02%) (33699/41088)\n",
      "Epoch: 191 | Batch_idx: 330 |  Loss_1: (0.4798) | Acc_1: (82.03%) (34753/42368)\n",
      "Epoch: 191 | Batch_idx: 340 |  Loss_1: (0.4804) | Acc_1: (82.01%) (35794/43648)\n",
      "Epoch: 191 | Batch_idx: 350 |  Loss_1: (0.4799) | Acc_1: (82.02%) (36849/44928)\n",
      "Epoch: 191 | Batch_idx: 360 |  Loss_1: (0.4790) | Acc_1: (82.05%) (37915/46208)\n",
      "Epoch: 191 | Batch_idx: 370 |  Loss_1: (0.4794) | Acc_1: (82.02%) (38952/47488)\n",
      "Epoch: 191 | Batch_idx: 380 |  Loss_1: (0.4781) | Acc_1: (82.08%) (40027/48768)\n",
      "Epoch: 191 | Batch_idx: 390 |  Loss_1: (0.4774) | Acc_1: (82.09%) (41046/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4953) | Acc: (90.67%) (9067/10000)\n",
      "Epoch: 192 | Batch_idx: 0 |  Loss_1: (0.5509) | Acc_1: (80.47%) (103/128)\n",
      "Epoch: 192 | Batch_idx: 10 |  Loss_1: (0.4430) | Acc_1: (83.74%) (1179/1408)\n",
      "Epoch: 192 | Batch_idx: 20 |  Loss_1: (0.4963) | Acc_1: (81.81%) (2199/2688)\n",
      "Epoch: 192 | Batch_idx: 30 |  Loss_1: (0.5040) | Acc_1: (81.30%) (3226/3968)\n",
      "Epoch: 192 | Batch_idx: 40 |  Loss_1: (0.4969) | Acc_1: (81.52%) (4278/5248)\n",
      "Epoch: 192 | Batch_idx: 50 |  Loss_1: (0.4907) | Acc_1: (81.74%) (5336/6528)\n",
      "Epoch: 192 | Batch_idx: 60 |  Loss_1: (0.4782) | Acc_1: (82.17%) (6416/7808)\n",
      "Epoch: 192 | Batch_idx: 70 |  Loss_1: (0.4820) | Acc_1: (82.04%) (7456/9088)\n",
      "Epoch: 192 | Batch_idx: 80 |  Loss_1: (0.4834) | Acc_1: (81.93%) (8494/10368)\n",
      "Epoch: 192 | Batch_idx: 90 |  Loss_1: (0.4826) | Acc_1: (81.95%) (9546/11648)\n",
      "Epoch: 192 | Batch_idx: 100 |  Loss_1: (0.4817) | Acc_1: (81.94%) (10593/12928)\n",
      "Epoch: 192 | Batch_idx: 110 |  Loss_1: (0.4785) | Acc_1: (82.09%) (11663/14208)\n",
      "Epoch: 192 | Batch_idx: 120 |  Loss_1: (0.4815) | Acc_1: (81.99%) (12699/15488)\n",
      "Epoch: 192 | Batch_idx: 130 |  Loss_1: (0.4858) | Acc_1: (81.86%) (13726/16768)\n",
      "Epoch: 192 | Batch_idx: 140 |  Loss_1: (0.4867) | Acc_1: (81.81%) (14765/18048)\n",
      "Epoch: 192 | Batch_idx: 150 |  Loss_1: (0.4895) | Acc_1: (81.69%) (15789/19328)\n",
      "Epoch: 192 | Batch_idx: 160 |  Loss_1: (0.4910) | Acc_1: (81.62%) (16821/20608)\n",
      "Epoch: 192 | Batch_idx: 170 |  Loss_1: (0.4914) | Acc_1: (81.65%) (17872/21888)\n",
      "Epoch: 192 | Batch_idx: 180 |  Loss_1: (0.4908) | Acc_1: (81.68%) (18924/23168)\n",
      "Epoch: 192 | Batch_idx: 190 |  Loss_1: (0.4893) | Acc_1: (81.75%) (19986/24448)\n",
      "Epoch: 192 | Batch_idx: 200 |  Loss_1: (0.4891) | Acc_1: (81.75%) (21033/25728)\n",
      "Epoch: 192 | Batch_idx: 210 |  Loss_1: (0.4869) | Acc_1: (81.84%) (22103/27008)\n",
      "Epoch: 192 | Batch_idx: 220 |  Loss_1: (0.4869) | Acc_1: (81.84%) (23150/28288)\n",
      "Epoch: 192 | Batch_idx: 230 |  Loss_1: (0.4850) | Acc_1: (81.92%) (24223/29568)\n",
      "Epoch: 192 | Batch_idx: 240 |  Loss_1: (0.4847) | Acc_1: (81.94%) (25276/30848)\n",
      "Epoch: 192 | Batch_idx: 250 |  Loss_1: (0.4844) | Acc_1: (81.93%) (26324/32128)\n",
      "Epoch: 192 | Batch_idx: 260 |  Loss_1: (0.4876) | Acc_1: (81.79%) (27324/33408)\n",
      "Epoch: 192 | Batch_idx: 270 |  Loss_1: (0.4876) | Acc_1: (81.79%) (28371/34688)\n",
      "Epoch: 192 | Batch_idx: 280 |  Loss_1: (0.4887) | Acc_1: (81.76%) (29408/35968)\n",
      "Epoch: 192 | Batch_idx: 290 |  Loss_1: (0.4908) | Acc_1: (81.68%) (30425/37248)\n",
      "Epoch: 192 | Batch_idx: 300 |  Loss_1: (0.4892) | Acc_1: (81.76%) (31500/38528)\n",
      "Epoch: 192 | Batch_idx: 310 |  Loss_1: (0.4887) | Acc_1: (81.78%) (32556/39808)\n",
      "Epoch: 192 | Batch_idx: 320 |  Loss_1: (0.4875) | Acc_1: (81.82%) (33619/41088)\n",
      "Epoch: 192 | Batch_idx: 330 |  Loss_1: (0.4860) | Acc_1: (81.86%) (34684/42368)\n",
      "Epoch: 192 | Batch_idx: 340 |  Loss_1: (0.4850) | Acc_1: (81.91%) (35750/43648)\n",
      "Epoch: 192 | Batch_idx: 350 |  Loss_1: (0.4856) | Acc_1: (81.89%) (36793/44928)\n",
      "Epoch: 192 | Batch_idx: 360 |  Loss_1: (0.4860) | Acc_1: (81.87%) (37831/46208)\n",
      "Epoch: 192 | Batch_idx: 370 |  Loss_1: (0.4864) | Acc_1: (81.86%) (38876/47488)\n",
      "Epoch: 192 | Batch_idx: 380 |  Loss_1: (0.4856) | Acc_1: (81.90%) (39939/48768)\n",
      "Epoch: 192 | Batch_idx: 390 |  Loss_1: (0.4852) | Acc_1: (81.89%) (40947/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4985) | Acc: (90.78%) (9078/10000)\n",
      "Epoch: 193 | Batch_idx: 0 |  Loss_1: (0.4915) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 193 | Batch_idx: 10 |  Loss_1: (0.4946) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 193 | Batch_idx: 20 |  Loss_1: (0.5068) | Acc_1: (81.18%) (2182/2688)\n",
      "Epoch: 193 | Batch_idx: 30 |  Loss_1: (0.5019) | Acc_1: (81.35%) (3228/3968)\n",
      "Epoch: 193 | Batch_idx: 40 |  Loss_1: (0.5033) | Acc_1: (81.21%) (4262/5248)\n",
      "Epoch: 193 | Batch_idx: 50 |  Loss_1: (0.4967) | Acc_1: (81.46%) (5318/6528)\n",
      "Epoch: 193 | Batch_idx: 60 |  Loss_1: (0.4892) | Acc_1: (81.70%) (6379/7808)\n",
      "Epoch: 193 | Batch_idx: 70 |  Loss_1: (0.4924) | Acc_1: (81.55%) (7411/9088)\n",
      "Epoch: 193 | Batch_idx: 80 |  Loss_1: (0.4869) | Acc_1: (81.70%) (8471/10368)\n",
      "Epoch: 193 | Batch_idx: 90 |  Loss_1: (0.4872) | Acc_1: (81.67%) (9513/11648)\n",
      "Epoch: 193 | Batch_idx: 100 |  Loss_1: (0.4856) | Acc_1: (81.72%) (10565/12928)\n",
      "Epoch: 193 | Batch_idx: 110 |  Loss_1: (0.4834) | Acc_1: (81.85%) (11629/14208)\n",
      "Epoch: 193 | Batch_idx: 120 |  Loss_1: (0.4837) | Acc_1: (81.84%) (12676/15488)\n",
      "Epoch: 193 | Batch_idx: 130 |  Loss_1: (0.4886) | Acc_1: (81.70%) (13699/16768)\n",
      "Epoch: 193 | Batch_idx: 140 |  Loss_1: (0.4908) | Acc_1: (81.64%) (14734/18048)\n",
      "Epoch: 193 | Batch_idx: 150 |  Loss_1: (0.4914) | Acc_1: (81.66%) (15783/19328)\n",
      "Epoch: 193 | Batch_idx: 160 |  Loss_1: (0.4877) | Acc_1: (81.80%) (16858/20608)\n",
      "Epoch: 193 | Batch_idx: 170 |  Loss_1: (0.4882) | Acc_1: (81.78%) (17900/21888)\n",
      "Epoch: 193 | Batch_idx: 180 |  Loss_1: (0.4887) | Acc_1: (81.77%) (18945/23168)\n",
      "Epoch: 193 | Batch_idx: 190 |  Loss_1: (0.4882) | Acc_1: (81.80%) (19998/24448)\n",
      "Epoch: 193 | Batch_idx: 200 |  Loss_1: (0.4877) | Acc_1: (81.83%) (21052/25728)\n",
      "Epoch: 193 | Batch_idx: 210 |  Loss_1: (0.4883) | Acc_1: (81.79%) (22090/27008)\n",
      "Epoch: 193 | Batch_idx: 220 |  Loss_1: (0.4880) | Acc_1: (81.81%) (23141/28288)\n",
      "Epoch: 193 | Batch_idx: 230 |  Loss_1: (0.4859) | Acc_1: (81.88%) (24209/29568)\n",
      "Epoch: 193 | Batch_idx: 240 |  Loss_1: (0.4855) | Acc_1: (81.91%) (25267/30848)\n",
      "Epoch: 193 | Batch_idx: 250 |  Loss_1: (0.4869) | Acc_1: (81.85%) (26296/32128)\n",
      "Epoch: 193 | Batch_idx: 260 |  Loss_1: (0.4871) | Acc_1: (81.86%) (27347/33408)\n",
      "Epoch: 193 | Batch_idx: 270 |  Loss_1: (0.4872) | Acc_1: (81.85%) (28391/34688)\n",
      "Epoch: 193 | Batch_idx: 280 |  Loss_1: (0.4861) | Acc_1: (81.89%) (29454/35968)\n",
      "Epoch: 193 | Batch_idx: 290 |  Loss_1: (0.4859) | Acc_1: (81.90%) (30505/37248)\n",
      "Epoch: 193 | Batch_idx: 300 |  Loss_1: (0.4869) | Acc_1: (81.87%) (31541/38528)\n",
      "Epoch: 193 | Batch_idx: 310 |  Loss_1: (0.4874) | Acc_1: (81.84%) (32580/39808)\n",
      "Epoch: 193 | Batch_idx: 320 |  Loss_1: (0.4881) | Acc_1: (81.82%) (33620/41088)\n",
      "Epoch: 193 | Batch_idx: 330 |  Loss_1: (0.4895) | Acc_1: (81.77%) (34645/42368)\n",
      "Epoch: 193 | Batch_idx: 340 |  Loss_1: (0.4889) | Acc_1: (81.79%) (35698/43648)\n",
      "Epoch: 193 | Batch_idx: 350 |  Loss_1: (0.4884) | Acc_1: (81.80%) (36752/44928)\n",
      "Epoch: 193 | Batch_idx: 360 |  Loss_1: (0.4892) | Acc_1: (81.78%) (37788/46208)\n",
      "Epoch: 193 | Batch_idx: 370 |  Loss_1: (0.4897) | Acc_1: (81.77%) (38830/47488)\n",
      "Epoch: 193 | Batch_idx: 380 |  Loss_1: (0.4887) | Acc_1: (81.80%) (39891/48768)\n",
      "Epoch: 193 | Batch_idx: 390 |  Loss_1: (0.4884) | Acc_1: (81.80%) (40898/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4961) | Acc: (90.55%) (9055/10000)\n",
      "Epoch: 194 | Batch_idx: 0 |  Loss_1: (0.4164) | Acc_1: (84.38%) (108/128)\n",
      "Epoch: 194 | Batch_idx: 10 |  Loss_1: (0.4826) | Acc_1: (81.82%) (1152/1408)\n",
      "Epoch: 194 | Batch_idx: 20 |  Loss_1: (0.4797) | Acc_1: (82.03%) (2205/2688)\n",
      "Epoch: 194 | Batch_idx: 30 |  Loss_1: (0.4810) | Acc_1: (81.98%) (3253/3968)\n",
      "Epoch: 194 | Batch_idx: 40 |  Loss_1: (0.4787) | Acc_1: (82.05%) (4306/5248)\n",
      "Epoch: 194 | Batch_idx: 50 |  Loss_1: (0.4876) | Acc_1: (81.71%) (5334/6528)\n",
      "Epoch: 194 | Batch_idx: 60 |  Loss_1: (0.4816) | Acc_1: (81.94%) (6398/7808)\n",
      "Epoch: 194 | Batch_idx: 70 |  Loss_1: (0.4829) | Acc_1: (81.94%) (7447/9088)\n",
      "Epoch: 194 | Batch_idx: 80 |  Loss_1: (0.4812) | Acc_1: (82.03%) (8505/10368)\n",
      "Epoch: 194 | Batch_idx: 90 |  Loss_1: (0.4828) | Acc_1: (81.96%) (9547/11648)\n",
      "Epoch: 194 | Batch_idx: 100 |  Loss_1: (0.4809) | Acc_1: (82.07%) (10610/12928)\n",
      "Epoch: 194 | Batch_idx: 110 |  Loss_1: (0.4812) | Acc_1: (82.06%) (11659/14208)\n",
      "Epoch: 194 | Batch_idx: 120 |  Loss_1: (0.4806) | Acc_1: (82.05%) (12708/15488)\n",
      "Epoch: 194 | Batch_idx: 130 |  Loss_1: (0.4789) | Acc_1: (82.10%) (13767/16768)\n",
      "Epoch: 194 | Batch_idx: 140 |  Loss_1: (0.4812) | Acc_1: (82.03%) (14805/18048)\n",
      "Epoch: 194 | Batch_idx: 150 |  Loss_1: (0.4846) | Acc_1: (81.93%) (15835/19328)\n",
      "Epoch: 194 | Batch_idx: 160 |  Loss_1: (0.4811) | Acc_1: (82.06%) (16910/20608)\n",
      "Epoch: 194 | Batch_idx: 170 |  Loss_1: (0.4814) | Acc_1: (82.06%) (17961/21888)\n",
      "Epoch: 194 | Batch_idx: 180 |  Loss_1: (0.4804) | Acc_1: (82.08%) (19016/23168)\n",
      "Epoch: 194 | Batch_idx: 190 |  Loss_1: (0.4804) | Acc_1: (82.09%) (20069/24448)\n",
      "Epoch: 194 | Batch_idx: 200 |  Loss_1: (0.4810) | Acc_1: (82.09%) (21120/25728)\n",
      "Epoch: 194 | Batch_idx: 210 |  Loss_1: (0.4805) | Acc_1: (82.10%) (22173/27008)\n",
      "Epoch: 194 | Batch_idx: 220 |  Loss_1: (0.4793) | Acc_1: (82.13%) (23233/28288)\n",
      "Epoch: 194 | Batch_idx: 230 |  Loss_1: (0.4789) | Acc_1: (82.17%) (24295/29568)\n",
      "Epoch: 194 | Batch_idx: 240 |  Loss_1: (0.4790) | Acc_1: (82.16%) (25344/30848)\n",
      "Epoch: 194 | Batch_idx: 250 |  Loss_1: (0.4781) | Acc_1: (82.19%) (26405/32128)\n",
      "Epoch: 194 | Batch_idx: 260 |  Loss_1: (0.4796) | Acc_1: (82.13%) (27438/33408)\n",
      "Epoch: 194 | Batch_idx: 270 |  Loss_1: (0.4794) | Acc_1: (82.14%) (28494/34688)\n",
      "Epoch: 194 | Batch_idx: 280 |  Loss_1: (0.4796) | Acc_1: (82.14%) (29543/35968)\n",
      "Epoch: 194 | Batch_idx: 290 |  Loss_1: (0.4790) | Acc_1: (82.16%) (30603/37248)\n",
      "Epoch: 194 | Batch_idx: 300 |  Loss_1: (0.4794) | Acc_1: (82.15%) (31650/38528)\n",
      "Epoch: 194 | Batch_idx: 310 |  Loss_1: (0.4810) | Acc_1: (82.07%) (32672/39808)\n",
      "Epoch: 194 | Batch_idx: 320 |  Loss_1: (0.4810) | Acc_1: (82.07%) (33722/41088)\n",
      "Epoch: 194 | Batch_idx: 330 |  Loss_1: (0.4799) | Acc_1: (82.12%) (34792/42368)\n",
      "Epoch: 194 | Batch_idx: 340 |  Loss_1: (0.4797) | Acc_1: (82.12%) (35845/43648)\n",
      "Epoch: 194 | Batch_idx: 350 |  Loss_1: (0.4794) | Acc_1: (82.14%) (36903/44928)\n",
      "Epoch: 194 | Batch_idx: 360 |  Loss_1: (0.4810) | Acc_1: (82.09%) (37930/46208)\n",
      "Epoch: 194 | Batch_idx: 370 |  Loss_1: (0.4804) | Acc_1: (82.11%) (38991/47488)\n",
      "Epoch: 194 | Batch_idx: 380 |  Loss_1: (0.4818) | Acc_1: (82.06%) (40018/48768)\n",
      "Epoch: 194 | Batch_idx: 390 |  Loss_1: (0.4810) | Acc_1: (82.10%) (41050/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5177) | Acc: (90.00%) (9000/10000)\n",
      "Epoch: 195 | Batch_idx: 0 |  Loss_1: (0.4959) | Acc_1: (82.03%) (105/128)\n",
      "Epoch: 195 | Batch_idx: 10 |  Loss_1: (0.5224) | Acc_1: (81.04%) (1141/1408)\n",
      "Epoch: 195 | Batch_idx: 20 |  Loss_1: (0.5009) | Acc_1: (81.66%) (2195/2688)\n",
      "Epoch: 195 | Batch_idx: 30 |  Loss_1: (0.4910) | Acc_1: (81.75%) (3244/3968)\n",
      "Epoch: 195 | Batch_idx: 40 |  Loss_1: (0.4915) | Acc_1: (81.73%) (4289/5248)\n",
      "Epoch: 195 | Batch_idx: 50 |  Loss_1: (0.4915) | Acc_1: (81.80%) (5340/6528)\n",
      "Epoch: 195 | Batch_idx: 60 |  Loss_1: (0.4823) | Acc_1: (82.16%) (6415/7808)\n",
      "Epoch: 195 | Batch_idx: 70 |  Loss_1: (0.4803) | Acc_1: (82.27%) (7477/9088)\n",
      "Epoch: 195 | Batch_idx: 80 |  Loss_1: (0.4789) | Acc_1: (82.25%) (8528/10368)\n",
      "Epoch: 195 | Batch_idx: 90 |  Loss_1: (0.4775) | Acc_1: (82.26%) (9582/11648)\n",
      "Epoch: 195 | Batch_idx: 100 |  Loss_1: (0.4780) | Acc_1: (82.26%) (10634/12928)\n",
      "Epoch: 195 | Batch_idx: 110 |  Loss_1: (0.4805) | Acc_1: (82.16%) (11674/14208)\n",
      "Epoch: 195 | Batch_idx: 120 |  Loss_1: (0.4813) | Acc_1: (82.11%) (12717/15488)\n",
      "Epoch: 195 | Batch_idx: 130 |  Loss_1: (0.4843) | Acc_1: (81.99%) (13748/16768)\n",
      "Epoch: 195 | Batch_idx: 140 |  Loss_1: (0.4845) | Acc_1: (81.95%) (14791/18048)\n",
      "Epoch: 195 | Batch_idx: 150 |  Loss_1: (0.4826) | Acc_1: (82.02%) (15852/19328)\n",
      "Epoch: 195 | Batch_idx: 160 |  Loss_1: (0.4836) | Acc_1: (81.98%) (16894/20608)\n",
      "Epoch: 195 | Batch_idx: 170 |  Loss_1: (0.4857) | Acc_1: (81.92%) (17930/21888)\n",
      "Epoch: 195 | Batch_idx: 180 |  Loss_1: (0.4847) | Acc_1: (81.92%) (18980/23168)\n",
      "Epoch: 195 | Batch_idx: 190 |  Loss_1: (0.4853) | Acc_1: (81.90%) (20022/24448)\n",
      "Epoch: 195 | Batch_idx: 200 |  Loss_1: (0.4874) | Acc_1: (81.81%) (21047/25728)\n",
      "Epoch: 195 | Batch_idx: 210 |  Loss_1: (0.4887) | Acc_1: (81.77%) (22084/27008)\n",
      "Epoch: 195 | Batch_idx: 220 |  Loss_1: (0.4883) | Acc_1: (81.78%) (23134/28288)\n",
      "Epoch: 195 | Batch_idx: 230 |  Loss_1: (0.4889) | Acc_1: (81.75%) (24173/29568)\n",
      "Epoch: 195 | Batch_idx: 240 |  Loss_1: (0.4910) | Acc_1: (81.69%) (25200/30848)\n",
      "Epoch: 195 | Batch_idx: 250 |  Loss_1: (0.4897) | Acc_1: (81.76%) (26267/32128)\n",
      "Epoch: 195 | Batch_idx: 260 |  Loss_1: (0.4893) | Acc_1: (81.77%) (27319/33408)\n",
      "Epoch: 195 | Batch_idx: 270 |  Loss_1: (0.4897) | Acc_1: (81.75%) (28358/34688)\n",
      "Epoch: 195 | Batch_idx: 280 |  Loss_1: (0.4910) | Acc_1: (81.70%) (29386/35968)\n",
      "Epoch: 195 | Batch_idx: 290 |  Loss_1: (0.4914) | Acc_1: (81.67%) (30422/37248)\n",
      "Epoch: 195 | Batch_idx: 300 |  Loss_1: (0.4906) | Acc_1: (81.71%) (31480/38528)\n",
      "Epoch: 195 | Batch_idx: 310 |  Loss_1: (0.4903) | Acc_1: (81.72%) (32533/39808)\n",
      "Epoch: 195 | Batch_idx: 320 |  Loss_1: (0.4906) | Acc_1: (81.71%) (33574/41088)\n",
      "Epoch: 195 | Batch_idx: 330 |  Loss_1: (0.4884) | Acc_1: (81.79%) (34654/42368)\n",
      "Epoch: 195 | Batch_idx: 340 |  Loss_1: (0.4867) | Acc_1: (81.85%) (35727/43648)\n",
      "Epoch: 195 | Batch_idx: 350 |  Loss_1: (0.4852) | Acc_1: (81.90%) (36795/44928)\n",
      "Epoch: 195 | Batch_idx: 360 |  Loss_1: (0.4853) | Acc_1: (81.89%) (37839/46208)\n",
      "Epoch: 195 | Batch_idx: 370 |  Loss_1: (0.4853) | Acc_1: (81.89%) (38890/47488)\n",
      "Epoch: 195 | Batch_idx: 380 |  Loss_1: (0.4865) | Acc_1: (81.84%) (39913/48768)\n",
      "Epoch: 195 | Batch_idx: 390 |  Loss_1: (0.4870) | Acc_1: (81.83%) (40914/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4642) | Acc: (90.67%) (9067/10000)\n",
      "Epoch: 196 | Batch_idx: 0 |  Loss_1: (0.4040) | Acc_1: (85.16%) (109/128)\n",
      "Epoch: 196 | Batch_idx: 10 |  Loss_1: (0.5122) | Acc_1: (80.82%) (1138/1408)\n",
      "Epoch: 196 | Batch_idx: 20 |  Loss_1: (0.5108) | Acc_1: (81.18%) (2182/2688)\n",
      "Epoch: 196 | Batch_idx: 30 |  Loss_1: (0.5020) | Acc_1: (81.48%) (3233/3968)\n",
      "Epoch: 196 | Batch_idx: 40 |  Loss_1: (0.5041) | Acc_1: (81.36%) (4270/5248)\n",
      "Epoch: 196 | Batch_idx: 50 |  Loss_1: (0.4977) | Acc_1: (81.63%) (5329/6528)\n",
      "Epoch: 196 | Batch_idx: 60 |  Loss_1: (0.4911) | Acc_1: (81.78%) (6385/7808)\n",
      "Epoch: 196 | Batch_idx: 70 |  Loss_1: (0.4980) | Acc_1: (81.47%) (7404/9088)\n",
      "Epoch: 196 | Batch_idx: 80 |  Loss_1: (0.4978) | Acc_1: (81.44%) (8444/10368)\n",
      "Epoch: 196 | Batch_idx: 90 |  Loss_1: (0.4987) | Acc_1: (81.44%) (9486/11648)\n",
      "Epoch: 196 | Batch_idx: 100 |  Loss_1: (0.4949) | Acc_1: (81.55%) (10543/12928)\n",
      "Epoch: 196 | Batch_idx: 110 |  Loss_1: (0.4962) | Acc_1: (81.49%) (11578/14208)\n",
      "Epoch: 196 | Batch_idx: 120 |  Loss_1: (0.4930) | Acc_1: (81.64%) (12645/15488)\n",
      "Epoch: 196 | Batch_idx: 130 |  Loss_1: (0.4932) | Acc_1: (81.62%) (13686/16768)\n",
      "Epoch: 196 | Batch_idx: 140 |  Loss_1: (0.4903) | Acc_1: (81.71%) (14747/18048)\n",
      "Epoch: 196 | Batch_idx: 150 |  Loss_1: (0.4885) | Acc_1: (81.80%) (15810/19328)\n",
      "Epoch: 196 | Batch_idx: 160 |  Loss_1: (0.4883) | Acc_1: (81.80%) (16858/20608)\n",
      "Epoch: 196 | Batch_idx: 170 |  Loss_1: (0.4877) | Acc_1: (81.81%) (17906/21888)\n",
      "Epoch: 196 | Batch_idx: 180 |  Loss_1: (0.4865) | Acc_1: (81.88%) (18970/23168)\n",
      "Epoch: 196 | Batch_idx: 190 |  Loss_1: (0.4865) | Acc_1: (81.89%) (20020/24448)\n",
      "Epoch: 196 | Batch_idx: 200 |  Loss_1: (0.4856) | Acc_1: (81.94%) (21081/25728)\n",
      "Epoch: 196 | Batch_idx: 210 |  Loss_1: (0.4857) | Acc_1: (81.91%) (22122/27008)\n",
      "Epoch: 196 | Batch_idx: 220 |  Loss_1: (0.4852) | Acc_1: (81.92%) (23174/28288)\n",
      "Epoch: 196 | Batch_idx: 230 |  Loss_1: (0.4852) | Acc_1: (81.92%) (24221/29568)\n",
      "Epoch: 196 | Batch_idx: 240 |  Loss_1: (0.4859) | Acc_1: (81.87%) (25256/30848)\n",
      "Epoch: 196 | Batch_idx: 250 |  Loss_1: (0.4850) | Acc_1: (81.90%) (26312/32128)\n",
      "Epoch: 196 | Batch_idx: 260 |  Loss_1: (0.4849) | Acc_1: (81.89%) (27359/33408)\n",
      "Epoch: 196 | Batch_idx: 270 |  Loss_1: (0.4865) | Acc_1: (81.83%) (28385/34688)\n",
      "Epoch: 196 | Batch_idx: 280 |  Loss_1: (0.4870) | Acc_1: (81.81%) (29426/35968)\n",
      "Epoch: 196 | Batch_idx: 290 |  Loss_1: (0.4872) | Acc_1: (81.81%) (30472/37248)\n",
      "Epoch: 196 | Batch_idx: 300 |  Loss_1: (0.4860) | Acc_1: (81.86%) (31539/38528)\n",
      "Epoch: 196 | Batch_idx: 310 |  Loss_1: (0.4873) | Acc_1: (81.79%) (32559/39808)\n",
      "Epoch: 196 | Batch_idx: 320 |  Loss_1: (0.4867) | Acc_1: (81.82%) (33619/41088)\n",
      "Epoch: 196 | Batch_idx: 330 |  Loss_1: (0.4862) | Acc_1: (81.85%) (34679/42368)\n",
      "Epoch: 196 | Batch_idx: 340 |  Loss_1: (0.4862) | Acc_1: (81.84%) (35723/43648)\n",
      "Epoch: 196 | Batch_idx: 350 |  Loss_1: (0.4877) | Acc_1: (81.80%) (36751/44928)\n",
      "Epoch: 196 | Batch_idx: 360 |  Loss_1: (0.4873) | Acc_1: (81.81%) (37805/46208)\n",
      "Epoch: 196 | Batch_idx: 370 |  Loss_1: (0.4875) | Acc_1: (81.81%) (38848/47488)\n",
      "Epoch: 196 | Batch_idx: 380 |  Loss_1: (0.4870) | Acc_1: (81.83%) (39905/48768)\n",
      "Epoch: 196 | Batch_idx: 390 |  Loss_1: (0.4875) | Acc_1: (81.79%) (40895/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4985) | Acc: (90.26%) (9026/10000)\n",
      "Epoch: 197 | Batch_idx: 0 |  Loss_1: (0.5919) | Acc_1: (78.12%) (100/128)\n",
      "Epoch: 197 | Batch_idx: 10 |  Loss_1: (0.4950) | Acc_1: (81.53%) (1148/1408)\n",
      "Epoch: 197 | Batch_idx: 20 |  Loss_1: (0.4984) | Acc_1: (81.51%) (2191/2688)\n",
      "Epoch: 197 | Batch_idx: 30 |  Loss_1: (0.4903) | Acc_1: (81.68%) (3241/3968)\n",
      "Epoch: 197 | Batch_idx: 40 |  Loss_1: (0.5004) | Acc_1: (81.29%) (4266/5248)\n",
      "Epoch: 197 | Batch_idx: 50 |  Loss_1: (0.4930) | Acc_1: (81.53%) (5322/6528)\n",
      "Epoch: 197 | Batch_idx: 60 |  Loss_1: (0.4937) | Acc_1: (81.58%) (6370/7808)\n",
      "Epoch: 197 | Batch_idx: 70 |  Loss_1: (0.4929) | Acc_1: (81.58%) (7414/9088)\n",
      "Epoch: 197 | Batch_idx: 80 |  Loss_1: (0.4895) | Acc_1: (81.71%) (8472/10368)\n",
      "Epoch: 197 | Batch_idx: 90 |  Loss_1: (0.4869) | Acc_1: (81.83%) (9532/11648)\n",
      "Epoch: 197 | Batch_idx: 100 |  Loss_1: (0.4831) | Acc_1: (81.96%) (10596/12928)\n",
      "Epoch: 197 | Batch_idx: 110 |  Loss_1: (0.4852) | Acc_1: (81.84%) (11628/14208)\n",
      "Epoch: 197 | Batch_idx: 120 |  Loss_1: (0.4889) | Acc_1: (81.66%) (12648/15488)\n",
      "Epoch: 197 | Batch_idx: 130 |  Loss_1: (0.4897) | Acc_1: (81.66%) (13693/16768)\n",
      "Epoch: 197 | Batch_idx: 140 |  Loss_1: (0.4876) | Acc_1: (81.74%) (14753/18048)\n",
      "Epoch: 197 | Batch_idx: 150 |  Loss_1: (0.4892) | Acc_1: (81.71%) (15793/19328)\n",
      "Epoch: 197 | Batch_idx: 160 |  Loss_1: (0.4885) | Acc_1: (81.73%) (16842/20608)\n",
      "Epoch: 197 | Batch_idx: 170 |  Loss_1: (0.4876) | Acc_1: (81.76%) (17895/21888)\n",
      "Epoch: 197 | Batch_idx: 180 |  Loss_1: (0.4854) | Acc_1: (81.82%) (18957/23168)\n",
      "Epoch: 197 | Batch_idx: 190 |  Loss_1: (0.4867) | Acc_1: (81.77%) (19990/24448)\n",
      "Epoch: 197 | Batch_idx: 200 |  Loss_1: (0.4848) | Acc_1: (81.84%) (21056/25728)\n",
      "Epoch: 197 | Batch_idx: 210 |  Loss_1: (0.4847) | Acc_1: (81.84%) (22104/27008)\n",
      "Epoch: 197 | Batch_idx: 220 |  Loss_1: (0.4849) | Acc_1: (81.85%) (23153/28288)\n",
      "Epoch: 197 | Batch_idx: 230 |  Loss_1: (0.4850) | Acc_1: (81.86%) (24203/29568)\n",
      "Epoch: 197 | Batch_idx: 240 |  Loss_1: (0.4845) | Acc_1: (81.87%) (25255/30848)\n",
      "Epoch: 197 | Batch_idx: 250 |  Loss_1: (0.4853) | Acc_1: (81.84%) (26293/32128)\n",
      "Epoch: 197 | Batch_idx: 260 |  Loss_1: (0.4854) | Acc_1: (81.85%) (27346/33408)\n",
      "Epoch: 197 | Batch_idx: 270 |  Loss_1: (0.4850) | Acc_1: (81.88%) (28404/34688)\n",
      "Epoch: 197 | Batch_idx: 280 |  Loss_1: (0.4852) | Acc_1: (81.88%) (29450/35968)\n",
      "Epoch: 197 | Batch_idx: 290 |  Loss_1: (0.4853) | Acc_1: (81.87%) (30495/37248)\n",
      "Epoch: 197 | Batch_idx: 300 |  Loss_1: (0.4846) | Acc_1: (81.90%) (31553/38528)\n",
      "Epoch: 197 | Batch_idx: 310 |  Loss_1: (0.4830) | Acc_1: (81.95%) (32622/39808)\n",
      "Epoch: 197 | Batch_idx: 320 |  Loss_1: (0.4848) | Acc_1: (81.88%) (33643/41088)\n",
      "Epoch: 197 | Batch_idx: 330 |  Loss_1: (0.4856) | Acc_1: (81.86%) (34682/42368)\n",
      "Epoch: 197 | Batch_idx: 340 |  Loss_1: (0.4847) | Acc_1: (81.88%) (35740/43648)\n",
      "Epoch: 197 | Batch_idx: 350 |  Loss_1: (0.4839) | Acc_1: (81.91%) (36801/44928)\n",
      "Epoch: 197 | Batch_idx: 360 |  Loss_1: (0.4839) | Acc_1: (81.92%) (37855/46208)\n",
      "Epoch: 197 | Batch_idx: 370 |  Loss_1: (0.4837) | Acc_1: (81.93%) (38907/47488)\n",
      "Epoch: 197 | Batch_idx: 380 |  Loss_1: (0.4838) | Acc_1: (81.92%) (39953/48768)\n",
      "Epoch: 197 | Batch_idx: 390 |  Loss_1: (0.4826) | Acc_1: (81.97%) (40987/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4993) | Acc: (90.31%) (9031/10000)\n",
      "Epoch: 198 | Batch_idx: 0 |  Loss_1: (0.4492) | Acc_1: (83.59%) (107/128)\n",
      "Epoch: 198 | Batch_idx: 10 |  Loss_1: (0.4630) | Acc_1: (82.32%) (1159/1408)\n",
      "Epoch: 198 | Batch_idx: 20 |  Loss_1: (0.4680) | Acc_1: (82.22%) (2210/2688)\n",
      "Epoch: 198 | Batch_idx: 30 |  Loss_1: (0.4672) | Acc_1: (82.48%) (3273/3968)\n",
      "Epoch: 198 | Batch_idx: 40 |  Loss_1: (0.4677) | Acc_1: (82.53%) (4331/5248)\n",
      "Epoch: 198 | Batch_idx: 50 |  Loss_1: (0.4617) | Acc_1: (82.75%) (5402/6528)\n",
      "Epoch: 198 | Batch_idx: 60 |  Loss_1: (0.4676) | Acc_1: (82.56%) (6446/7808)\n",
      "Epoch: 198 | Batch_idx: 70 |  Loss_1: (0.4747) | Acc_1: (82.30%) (7479/9088)\n",
      "Epoch: 198 | Batch_idx: 80 |  Loss_1: (0.4811) | Acc_1: (82.03%) (8505/10368)\n",
      "Epoch: 198 | Batch_idx: 90 |  Loss_1: (0.4828) | Acc_1: (81.93%) (9543/11648)\n",
      "Epoch: 198 | Batch_idx: 100 |  Loss_1: (0.4841) | Acc_1: (81.92%) (10590/12928)\n",
      "Epoch: 198 | Batch_idx: 110 |  Loss_1: (0.4852) | Acc_1: (81.86%) (11631/14208)\n",
      "Epoch: 198 | Batch_idx: 120 |  Loss_1: (0.4875) | Acc_1: (81.77%) (12665/15488)\n",
      "Epoch: 198 | Batch_idx: 130 |  Loss_1: (0.4843) | Acc_1: (81.89%) (13731/16768)\n",
      "Epoch: 198 | Batch_idx: 140 |  Loss_1: (0.4854) | Acc_1: (81.85%) (14773/18048)\n",
      "Epoch: 198 | Batch_idx: 150 |  Loss_1: (0.4860) | Acc_1: (81.86%) (15821/19328)\n",
      "Epoch: 198 | Batch_idx: 160 |  Loss_1: (0.4863) | Acc_1: (81.82%) (16861/20608)\n",
      "Epoch: 198 | Batch_idx: 170 |  Loss_1: (0.4850) | Acc_1: (81.83%) (17911/21888)\n",
      "Epoch: 198 | Batch_idx: 180 |  Loss_1: (0.4843) | Acc_1: (81.85%) (18964/23168)\n",
      "Epoch: 198 | Batch_idx: 190 |  Loss_1: (0.4841) | Acc_1: (81.88%) (20017/24448)\n",
      "Epoch: 198 | Batch_idx: 200 |  Loss_1: (0.4840) | Acc_1: (81.89%) (21069/25728)\n",
      "Epoch: 198 | Batch_idx: 210 |  Loss_1: (0.4817) | Acc_1: (81.98%) (22142/27008)\n",
      "Epoch: 198 | Batch_idx: 220 |  Loss_1: (0.4817) | Acc_1: (81.99%) (23192/28288)\n",
      "Epoch: 198 | Batch_idx: 230 |  Loss_1: (0.4805) | Acc_1: (82.04%) (24258/29568)\n",
      "Epoch: 198 | Batch_idx: 240 |  Loss_1: (0.4778) | Acc_1: (82.14%) (25339/30848)\n",
      "Epoch: 198 | Batch_idx: 250 |  Loss_1: (0.4787) | Acc_1: (82.09%) (26375/32128)\n",
      "Epoch: 198 | Batch_idx: 260 |  Loss_1: (0.4789) | Acc_1: (82.09%) (27424/33408)\n",
      "Epoch: 198 | Batch_idx: 270 |  Loss_1: (0.4808) | Acc_1: (82.03%) (28454/34688)\n",
      "Epoch: 198 | Batch_idx: 280 |  Loss_1: (0.4806) | Acc_1: (82.03%) (29503/35968)\n",
      "Epoch: 198 | Batch_idx: 290 |  Loss_1: (0.4807) | Acc_1: (82.02%) (30551/37248)\n",
      "Epoch: 198 | Batch_idx: 300 |  Loss_1: (0.4807) | Acc_1: (82.02%) (31600/38528)\n",
      "Epoch: 198 | Batch_idx: 310 |  Loss_1: (0.4812) | Acc_1: (82.01%) (32647/39808)\n",
      "Epoch: 198 | Batch_idx: 320 |  Loss_1: (0.4807) | Acc_1: (82.03%) (33703/41088)\n",
      "Epoch: 198 | Batch_idx: 330 |  Loss_1: (0.4815) | Acc_1: (82.01%) (34745/42368)\n",
      "Epoch: 198 | Batch_idx: 340 |  Loss_1: (0.4807) | Acc_1: (82.03%) (35806/43648)\n",
      "Epoch: 198 | Batch_idx: 350 |  Loss_1: (0.4807) | Acc_1: (82.04%) (36860/44928)\n",
      "Epoch: 198 | Batch_idx: 360 |  Loss_1: (0.4808) | Acc_1: (82.03%) (37903/46208)\n",
      "Epoch: 198 | Batch_idx: 370 |  Loss_1: (0.4813) | Acc_1: (82.02%) (38948/47488)\n",
      "Epoch: 198 | Batch_idx: 380 |  Loss_1: (0.4827) | Acc_1: (81.97%) (39975/48768)\n",
      "Epoch: 198 | Batch_idx: 390 |  Loss_1: (0.4811) | Acc_1: (82.03%) (41014/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4885) | Acc: (90.68%) (9068/10000)\n",
      "Epoch: 199 | Batch_idx: 0 |  Loss_1: (0.5052) | Acc_1: (77.34%) (99/128)\n",
      "Epoch: 199 | Batch_idx: 10 |  Loss_1: (0.4812) | Acc_1: (81.75%) (1151/1408)\n",
      "Epoch: 199 | Batch_idx: 20 |  Loss_1: (0.4565) | Acc_1: (82.70%) (2223/2688)\n",
      "Epoch: 199 | Batch_idx: 30 |  Loss_1: (0.4700) | Acc_1: (82.33%) (3267/3968)\n",
      "Epoch: 199 | Batch_idx: 40 |  Loss_1: (0.4767) | Acc_1: (82.13%) (4310/5248)\n",
      "Epoch: 199 | Batch_idx: 50 |  Loss_1: (0.4811) | Acc_1: (81.92%) (5348/6528)\n",
      "Epoch: 199 | Batch_idx: 60 |  Loss_1: (0.4821) | Acc_1: (81.89%) (6394/7808)\n",
      "Epoch: 199 | Batch_idx: 70 |  Loss_1: (0.4902) | Acc_1: (81.57%) (7413/9088)\n",
      "Epoch: 199 | Batch_idx: 80 |  Loss_1: (0.4859) | Acc_1: (81.72%) (8473/10368)\n",
      "Epoch: 199 | Batch_idx: 90 |  Loss_1: (0.4796) | Acc_1: (81.98%) (9549/11648)\n",
      "Epoch: 199 | Batch_idx: 100 |  Loss_1: (0.4855) | Acc_1: (81.74%) (10567/12928)\n",
      "Epoch: 199 | Batch_idx: 110 |  Loss_1: (0.4874) | Acc_1: (81.70%) (11608/14208)\n",
      "Epoch: 199 | Batch_idx: 120 |  Loss_1: (0.4878) | Acc_1: (81.66%) (12648/15488)\n",
      "Epoch: 199 | Batch_idx: 130 |  Loss_1: (0.4885) | Acc_1: (81.67%) (13694/16768)\n",
      "Epoch: 199 | Batch_idx: 140 |  Loss_1: (0.4900) | Acc_1: (81.64%) (14734/18048)\n",
      "Epoch: 199 | Batch_idx: 150 |  Loss_1: (0.4888) | Acc_1: (81.69%) (15789/19328)\n",
      "Epoch: 199 | Batch_idx: 160 |  Loss_1: (0.4876) | Acc_1: (81.75%) (16847/20608)\n",
      "Epoch: 199 | Batch_idx: 170 |  Loss_1: (0.4889) | Acc_1: (81.67%) (17877/21888)\n",
      "Epoch: 199 | Batch_idx: 180 |  Loss_1: (0.4883) | Acc_1: (81.71%) (18931/23168)\n",
      "Epoch: 199 | Batch_idx: 190 |  Loss_1: (0.4868) | Acc_1: (81.79%) (19995/24448)\n",
      "Epoch: 199 | Batch_idx: 200 |  Loss_1: (0.4862) | Acc_1: (81.81%) (21049/25728)\n",
      "Epoch: 199 | Batch_idx: 210 |  Loss_1: (0.4893) | Acc_1: (81.70%) (22066/27008)\n",
      "Epoch: 199 | Batch_idx: 220 |  Loss_1: (0.4877) | Acc_1: (81.77%) (23130/28288)\n",
      "Epoch: 199 | Batch_idx: 230 |  Loss_1: (0.4866) | Acc_1: (81.81%) (24191/29568)\n",
      "Epoch: 199 | Batch_idx: 240 |  Loss_1: (0.4860) | Acc_1: (81.81%) (25238/30848)\n",
      "Epoch: 199 | Batch_idx: 250 |  Loss_1: (0.4858) | Acc_1: (81.82%) (26288/32128)\n",
      "Epoch: 199 | Batch_idx: 260 |  Loss_1: (0.4868) | Acc_1: (81.80%) (27327/33408)\n",
      "Epoch: 199 | Batch_idx: 270 |  Loss_1: (0.4865) | Acc_1: (81.82%) (28380/34688)\n",
      "Epoch: 199 | Batch_idx: 280 |  Loss_1: (0.4887) | Acc_1: (81.70%) (29387/35968)\n",
      "Epoch: 199 | Batch_idx: 290 |  Loss_1: (0.4879) | Acc_1: (81.73%) (30444/37248)\n",
      "Epoch: 199 | Batch_idx: 300 |  Loss_1: (0.4890) | Acc_1: (81.69%) (31472/38528)\n",
      "Epoch: 199 | Batch_idx: 310 |  Loss_1: (0.4869) | Acc_1: (81.76%) (32546/39808)\n",
      "Epoch: 199 | Batch_idx: 320 |  Loss_1: (0.4855) | Acc_1: (81.82%) (33618/41088)\n",
      "Epoch: 199 | Batch_idx: 330 |  Loss_1: (0.4865) | Acc_1: (81.78%) (34649/42368)\n",
      "Epoch: 199 | Batch_idx: 340 |  Loss_1: (0.4862) | Acc_1: (81.79%) (35700/43648)\n",
      "Epoch: 199 | Batch_idx: 350 |  Loss_1: (0.4853) | Acc_1: (81.83%) (36763/44928)\n",
      "Epoch: 199 | Batch_idx: 360 |  Loss_1: (0.4854) | Acc_1: (81.82%) (37809/46208)\n",
      "Epoch: 199 | Batch_idx: 370 |  Loss_1: (0.4845) | Acc_1: (81.85%) (38867/47488)\n",
      "Epoch: 199 | Batch_idx: 380 |  Loss_1: (0.4836) | Acc_1: (81.88%) (39933/48768)\n",
      "Epoch: 199 | Batch_idx: 390 |  Loss_1: (0.4828) | Acc_1: (81.92%) (40961/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5052) | Acc: (90.38%) (9038/10000)\n",
      "1 hours 28 mins 34 secs for training\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory, filename='vgg19_prelu_dropblock_cosine_1.tar.gz')\n",
    "\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 200):\n",
    "\n",
    "    train(epoch)\n",
    "    \n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename='vgg19_prelu_dropblock_cosine_1.tar.gz')\n",
    "    test(epoch)  \n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8766a17f2584f17ae9875767170f3464b2a051bfe2b6423fb227ac503acbc200"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hw2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
